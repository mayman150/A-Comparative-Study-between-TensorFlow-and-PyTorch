Issue Number,Issue Title,Issue Body
23962,[tensorflow/contrib/lite/toco/tflite/export.cc:192] Unsupported operator: MirrorPad,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

System information
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
1.12.0
- Python version:
Python 3.6.5 :: Anaconda, Inc.
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

--------------------------------------
I am trying to convert a graph from .pb to .lite format using toco, but I get this error:
2018-11-25 21:58:56.811762: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: MirrorPad

I think MirrorPad is the basic op, Lite shoud support it.

---------------------------------------
"
23961,Problem installing tensorflow-gpu  (dll load faliure),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary):
- TensorFlow version: tensorflow-1.12.0 (installed via pip3)
- Python version: python 3.6.7
- CUDA/cuDNN version: CUDA==10.0.130 cuDNN==7.4.1
- GPU model and memory: NVIDIA GTX 1050 Ti with Max-Q Design

**Provide the exact sequence of commands / steps that you executed before running into the problem**
After starting python 3.6 in cmd and trying to execute `import tensorflow` I get the following stack trace:

'>>> import tensorflow
Traceback (most recent call last):
  File ""C:\Users\Sunny Nagam\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Sunny Nagam\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Sunny Nagam\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Sunny Nagam\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Sunny Nagam\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Sunny Nagam\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Sunny Nagam\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Sunny Nagam\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Sunny Nagam\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Sunny Nagam\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Sunny Nagam\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Sunny Nagam\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Sunny Nagam\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.'


I have tried installing the CUDA toolkit and cuDNN and adding to my PATH. Here is my current PATH: 

'PATH=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\extras\CUPTI\libx64;C:\cuda\bin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\bin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\libnvvp;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\iCLS\;C:\Program Files\Intel\Intel(R) Management Engine Components\iCLS\;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Users\Sunny Nagam\AppData\Local\Programs\Python\Python36\Scripts\;C:\Users\Sunny Nagam\AppData\Local\Programs\Python\Python36\;C:\Users\Sunny Nagam\AppData\Local\Microsoft\WindowsApps;'

I'm unsure if this infromation is helpful but my \cuda\bin contains ""cudnn_64_7.dll"" and my CUDA\v10.0\bin contains ""cudart64_100.dll"""
23960,Trying to optimize graph with optimize_for_inference too many positional arguments,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**

- OS Platform and Distribution : windows 10 X64
- TensorFlow installed from (source or binary): pip install
- TensorFlow version  : 1.7.1
- Python version: 36
- CUDA/cuDNN version: cuda v9.0
- GPU model and memory: Nvidia 1070


**Describe the current behavior**


I'm doing some image recognition with tensorflow. I retrained a CNN based on Inception V3 with my onw data. 

My results are pretty good, I'm now trying to optimize the protobuf in order to use it with TFmobile.
So I'm using optimize_for_inference like this :
python optimize_for_inference.py 
--input=""C:\Users\project\Documents\peinture\learn\retrained_graphpeinture.pb"" 
--output=""C:\Users\project\Documents\peinture\learn\optimized_graph.pb"" 
--frozen_graph=""True"" --input_names=""Placeholder"" 
--output_names=""final_result""

I have in return a message error : File ""optimize_for_inference.py"", line 92, in main FLAGS.toco_compatible) TypeError: optimize_for_inference() takes 4 positional arguments but 5 were given 

I understand that there are too many arg in the py file. 
I looked at the file line 92, and there is indeed 5 arguments :
output_graph_def = optimize_for_inference_lib.optimize_for_inference(  
input_graph_def,   
FLAGS.input_names.split("",""),   
FLAGS.output_names.split("",""),   
_parse_placeholder_types(FLAGS.placeholder_type_enum),   
FLAGS.toco_compatible) 

Is my file not the good one ? I don't understand why it doesn't work. I'm pretty sure I'm using the good file (clone form Github). I followed step by step the well known Tensorflow for poets 2 tutorial.Best regards
--



Thanks :)"
23959, Commonization of return value's type ​​of  tf.image.resize_image_with_crop_or_pad and tf.image.resize_images,"# system 
tensorflow ver.  tensorflow-gpu == 1.12.0

# rough source code
    
    from PIL import Image
    import tensorflow as tf 

    im = Image.open(""hogehoge.jpeg"")
    w, h = im.size

    raw_image = tf.read_file(""hogehoge.jpeg"")
    image = tf.image.decode_jpeg(raw_image, channels = 3) # rgb color image
    print(image.dtype) # => dtype : uint8
    image = tf.image.resize_image_with_crop_or_pad(image, w, w)
    print(image.dtype) # => dtype : unit8
    image = tf.image.resize_images(image, [128, 128])
    print(image.dtype) # => dtype : float32    ??????????????????????????????????????????


So, I want to commonization of return value's type of them. (I want  tf.image.resize_images will return value of dtype : uint8)
     
"
23958,pip install tf-nightly-gpu Does not work,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS X El Capitan
- Anaconda
- Pip v.18.1
- Python v.3.6.6 

**Describe the problem**
Could not find a version that satisfies the requirement tf-nightly-gpu (from versions: )
No matching distribution found for tf-nightly-gpu
**Provide the exact sequence of commands / steps that you executed before running into the problem**
Following steps on this:
https://www.youtube.com/watch?v=xQ4i6sDt-Tk&t=120s"
23957,Inconsistent results between estimator and graph/session,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10.0.17134
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below):1.12.0
- Python version:3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:9.0
- GPU model and memory: GTX1070, 8Gb


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I implemented the same model (2-layer fully-connected neural network + batch norm to reduce initialization effects) using 
1. Low level APIs (graphs and sessions), and
2. High level APIs (estimators)
I tested the two implementation and tested it on the MNIST dataset. The Estimator shows a much slower training and a significantly lower final recognition error. (Estimator: First epoch~92%, Final~96%,  Sess+Graph: First epoch~96%, Final>98%)
**Describe the expected behavior**
The performance should be similar.
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

The High-Level API
```
import tensorflow as tf
import numpy as np

#Data
from tensorflow.examples.tutorials.mnist import input_data
data = input_data.read_data_sets(""/tmp/data/"", one_hot = True)
n_inputs = 784
n_classes = 10
'''Input Function'''
train_input_fn = tf.estimator.inputs.numpy_input_fn(
    x={""x"":np.array(data.train.images)},
    y=np.array(data.train.labels),
    num_epochs=1,
    shuffle=True)
'''Test Function'''
test_input_fn = tf.estimator.inputs.numpy_input_fn(
    x={""x"":np.array(data.test.images)},
    y=np.array(data.test.labels),
    num_epochs=1,
    shuffle=False)

#Training Parameters
params={""lr_init"" :0.1,
        ""n_neurons"":200}


#Model function
def model_fn(features, labels, mode, params):
    '''Model Graph'''
    x = features[""x""]
    prediction=tf.layers.dense(x, units=params[""n_neurons""], activation=None, use_bias=False)
    prediction=tf.layers.batch_normalization(prediction, training=(mode == tf.estimator.ModeKeys.TRAIN))
    prediction=tf.nn.relu(prediction)
    prediction=tf.layers.dense(prediction, units=n_classes, activation=None)
    
    if(mode==tf.estimator.ModeKeys.PREDICT):
        '''If Evaluation, then simply return predictions'''
        spec = tf.estimator.EstimatorSpec(mode=mode, predictions=prediction)
    else:
        '''If Training or Testing, compute loss and accuracy'''
        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))
        metrics = {""acuracy"":tf.metrics.accuracy(tf.argmax(labels, 1), tf.argmax(prediction, 1))}        
        '''Training Operation'''
        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):
            optimizer = tf.train.MomentumOptimizer(learning_rate=params[""lr_init""], momentum=0.9)
        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())        
        '''Specs for Training and Evaluation'''
        spec=tf.estimator.EstimatorSpec(
            mode=mode,
            loss=loss,
            train_op=train_op,
            eval_metric_ops=metrics)
    
    return spec

model=tf.estimator.Estimator(model_fn=model_fn,
                             params=params,
                             model_dir='./estimator_out/')
for i in range(100):
    model.train(input_fn=train_input_fn)
    result = model.evaluate(input_fn=test_input_fn)
    print(result)

```
The Low-Level API:
```
import tensorflow as tf
import numpy as np

#Data
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(""/tmp/data/"", one_hot = True)
n_inputs=784
n_classes=10

'''Training Parameters'''
params={""lr_init"" :0.1,
        ""n_neurons"":200}
train_batch=128

'''Model graph'''                      
x = tf.placeholder(tf.float32, shape=[None, n_inputs])
y = tf.placeholder(tf.float32, shape=[None, n_classes])
learning_rate = tf.placeholder(tf.float32)
training = tf.placeholder(tf.bool)
prediction=tf.layers.dense(x, units=params[""n_neurons""], activation=None, use_bias=False)
prediction=tf.layers.batch_normalization(prediction, training=training)
prediction=tf.nn.relu(prediction)
prediction=tf.layers.dense(prediction, units=n_classes, activation=None)

'''loss and accuracy'''
loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))
with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):
    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9).minimize(loss)
correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y,1))
accuracy = tf.reduce_mean(tf.cast(correct, 'float'))

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for epoch in range(100):
        for batch in range(int(mnist.train.num_examples/train_batch)):
            epoch_x, epoch_y=mnist.train.next_batch(train_batch, shuffle=True)
            sess.run(optimizer, feed_dict = {x:epoch_x, y:epoch_y, learning_rate:params[""lr_init""], training:True})
        acc = sess.run(accuracy, feed_dict = {x:mnist.test.images, y:mnist.test.labels, learning_rate:0, training:False})
        print('accuracy:',acc)
```"
23956,Tensorflow In Anaconda not found,"Good evening. As I understand tensorflow and keras couldn't be installed in python 3.7 (keras is not used without tensorflow). So I just copy all folders of tensorflow and keras in site-package of anaconda. But system is proceeding with error ""No module named 'tensorflow.python' when I try to import tensorflow (folder tensorflow and folder python exist in directory Local\Continuum\anaconda3\Lib\site-packages\). Is there any other opportunity to use module tensorflow and keras in python 3.7?"
23955,graph_transforms tool link error in windows 10,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
windows 10 pro
- TensorFlow installed from (source or binary):
source
- TensorFlow version:
1.12
- Python version:
3.6
- Installed using virtualenv? pip? conda?:
conda
- Bazel version (if compiling from source):
0.15.2
- GCC/Compiler version (if compiling from source):
vs2015 build tools
- CUDA/cuDNN version:
9.0/7
- GPU model and memory:
2 X pascal6000, 24GB


**Describe the problem**

Hi,
i have installed tf 1.12 from source using bazel according to the instruction on site and the build was successful. now i tried building the graph_transforms tool (the reason why i installed tf from source in the first place) but i get the link error in the logs below. i tried following the suggestions in #23655 but i was still getting link errors.
any help on this would be really appreciated, i really want to start using the graph_transforms tool.
thanks
 
**Provide the exact sequence of commands / steps that you executed before running into the problem**

bazel build tensorflow/tools/graph_transforms:transform_graph

**Any other info / logs**

```
ERROR: C:/deep/tensorflow/tf-1.12/tensorflow/tensorflow/tools/graph_transforms/BUILD:219:1: Linking of rule '//tensorflow/tools/graph_transforms:transform_graph' failed (Exit 1120): link.exe failed: error executing command
  cd C:/users/iariav/_bazel_iariav/6fo6vxbr/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0
    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\winrt;
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\LIB\amd64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17134.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17134.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\Tools;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Team Tools\Performance Tools;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Windows Kits\10\bin\x86;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;;C:\WINDOWS\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/iariav/Anaconda3/envs/tensorflow-v1.12/python.exe
    SET PYTHON_LIB_PATH=C:/Users/iariav/Anaconda3/envs/tensorflow-v1.12/lib/site-packages
    SET TEMP=C:\Users\iariav\AppData\Local\Temp
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1
    SET TF_CUDA_VERSION=9.0
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
    SET TMP=C:\Users\iariav\AppData\Local\Temp
  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/link.exe /nologo /OUT:bazel-out/x64_windows-opt/bin/tensorflow/tools/graph_transforms/transform_graph /SUBSYSTEM:CONSOLE -DEFAULTLIB:advapi32.lib -Wl,-rpath,../local_config_cuda/cuda/lib64 -Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64 /MACHINE:X64 @bazel-out/x64_windows-opt/bin/tensorflow/tools/graph_transforms/transform_graph-2.params
LINK : warning LNK4044: unrecognized option '/Wl,-rpath,../local_config_cuda/cuda/lib64'; ignored
LINK : warning LNK4044: unrecognized option '/Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64'; ignored
LINK : warning LNK4044: unrecognized option '/lpthread'; ignored
   Creating library bazel-out/x64_windows-opt/bin/tensorflow/tools/graph_transforms/transform_graph.lib and object bazel-out/x64_windows-opt/bin/tensorflow/tools/graph_transforms/transform_graph.exp
libbatch_kernels.lo(batch_kernels.o) : warning LNK4217: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported in function ""void __cdecl tensorflow::`dynamic initializer for 'registrar__body__0__object''(void)"" (??__Eregistrar__body__0__object@tensorflow@@YAXXZ)
libcaptured_function.a(captured_function.o) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported
libarithmetic_optimizer.a(arithmetic_optimizer.o) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported
libpin_to_host_optimizer.a(pin_to_host_optimizer.o) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported
libcuda_platform.lo(cuda_dnn.o) : warning LNK4217: locally defined symbol ?ThenBlasGemm@Stream@stream_executor@@QEAAAEAV12@W4Transpose@blas@2@0_K11MAEBV?$DeviceMemory@M@2@H2HMPEAV52@H@Z (public: class stream_executor::Stream & __cdecl stream_executor::Stream::ThenBlasGemm(enum stream_executor::blas::Transpose,enum stream_executor::blas::Transpose,unsigned __int64,unsigned __int64,unsigned __int64,float,class stream_executor::DeviceMemory<float> const &,int,class stream_executor::DeviceMemory<float> const &,int,float,class stream_executor::DeviceMemory<float> *,int)) imported in function ""public: virtual bool __cdecl stream_executor::cuda::CudnnSupport::DoMatMul(class stream_executor::Stream *,class stream_executor::DeviceMemory<float> const &,class stream_executor::DeviceMemory<float> const &,class stream_executor::dnn::BatchDescriptor const &,class stream_executor::dnn::BatchDescriptor const &,class stream_executor::DeviceMemory<float> *)"" (?DoMatMul@CudnnSupport@cuda@stream_executor@@UEAA_NPEAVStream@3@AEBV?$DeviceMemory@M@3@1AEBVBatchDescriptor@dnn@3@2PEAV53@@Z)
libarithmetic_optimizer.a(arithmetic_optimizer.o) : warning LNK4217: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported in function ""private: bool __cdecl tensorflow::grappler::`anonymous namespace'::ReorderCastAndTranspose::NodeIsOnCpuOrGpu(class tensorflow::NodeDef const *)const "" (?NodeIsOnCpuOrGpu@ReorderCastAndTranspose@?A0xbb50fe50@grappler@tensorflow@@AEBA_NPEBVNodeDef@4@@Z)
liblayout_optimizer.a(layout_optimizer.o) : warning LNK4049: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported
libpin_to_host_optimizer.a(pin_to_host_optimizer.o) : warning LNK4049: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported
libunicode_script_op.lo(unicode_script_op.o) : error LNK2019: unresolved external symbol ""__declspec(dllimport) public: virtual __cdecl icu_62::ErrorCode::~ErrorCode(void)"" (__imp_??1ErrorCode@icu_62@@UEAA@XZ) referenced in function ""public: virtual void __cdecl tensorflow::UnicodeScriptOp::Compute(class tensorflow::OpKernelContext *)"" (?Compute@UnicodeScriptOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
libunicode_script_op.lo(unicode_script_op.o) : error LNK2019: unresolved external symbol ""__declspec(dllimport) public: signed char __cdecl icu_62::ErrorCode::isSuccess(void)const "" (__imp_?isSuccess@ErrorCode@icu_62@@QEBACXZ) referenced in function ""public: virtual void __cdecl tensorflow::UnicodeScriptOp::Compute(class tensorflow::OpKernelContext *)"" (?Compute@UnicodeScriptOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
libunicode_script_op.lo(unicode_script_op.o) : error LNK2019: unresolved external symbol ""__declspec(dllimport) public: enum UErrorCode __cdecl icu_62::ErrorCode::reset(void)"" (__imp_?reset@ErrorCode@icu_62@@QEAA?AW4UErrorCode@@XZ) referenced in function ""public: virtual void __cdecl tensorflow::UnicodeScriptOp::Compute(class tensorflow::OpKernelContext *)"" (?Compute@UnicodeScriptOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
libunicode_script_op.lo(unicode_script_op.o) : error LNK2019: unresolved external symbol ""__declspec(dllimport) const icu_62::ErrorCode::`vftable'"" (__imp_??_7ErrorCode@icu_62@@6B@) referenced in function ""public: virtual void __cdecl tensorflow::UnicodeScriptOp::Compute(class tensorflow::OpKernelContext *)"" (?Compute@UnicodeScriptOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
bazel-out/x64_windows-opt/bin/tensorflow/tools/graph_transforms/transform_graph : fatal error LNK1120: 4 unresolved externals
Target //tensorflow/tools/graph_transforms:transform_graph failed to build
INFO: Elapsed time: 782.281s, Critical Path: 295.06s
INFO: 1858 processes: 1858 local.
FAILED: Build did NOT complete successfully
```
"
23954,number of detections in the tflite graph,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12.0
- Python version: 2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA Version 9.0.176

- GPU model and memory:
00:02.0 VGA compatible controller [0300]: Intel Corporation Device [8086:591b] (rev 04) (prog-if 00 [VGA controller])
	DeviceName:  Onboard IGD
	Subsystem: Dell Device [1028:0802]
	Flags: bus master, fast devsel, latency 0, IRQ 130
	Memory at db000000 (64-bit, non-prefetchable) [size=16M]
	Memory at 70000000 (64-bit, prefetchable) [size=256M]
	I/O ports at f000 [size=64]
	[virtual] Expansion ROM at 000c0000 [disabled] [size=128K]
	Capabilities: <access denied>
	Kernel driver in use: i915
	Kernel modules: i915



You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

**Describe the expected behavior**
Hi

I'm trying to detect more than 10 objects in the image ( which is default )
I'm usin the following commands:
bazel run -c opt tensorflow/contrib/lite/toco:toco -- --input_file=$OUTPUT_DIR/tflite_graph.pb --output_file=$OUTPUT_DIR/mobile_net_500.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=FLOAT --max_detections=500 --max_classes_per_detection=1 --allow_custom_ops

I also modified
export_tflite_ssd_graph.py
flags.DEFINE_integer('max_detections', 500 <--- instead of 10,
'Maximum number of detections (boxes) to show.')
flags.DEFINE_integer('max_classes_per_detection', 1,
'Number of classes to display per detection box.')

but still giving 10 objects as output in the android [1,10,4].

any idea?
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
bazel run -c opt tensorflow/contrib/lite/toco:toco -- --input_file=$OUTPUT_DIR/tflite_graph.pb --output_file=$OUTPUT_DIR/mobile_net_500.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=FLOAT --max_detections=500 --max_classes_per_detection=1 --allow_custom_ops

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


"
23953,Runs out of memory after 3 epochs,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): ('v1.11.0-0-gc19e29306c', '1.11.0')

- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
('v1.11.0-0-gc19e29306c', '1.11.0')

**Describe the current behavior**
My model runs out of memory, but just after 3 epochs.
**Describe the expected behavior**
It should run out of memory in the first epoch or never.

**Code to reproduce the issue**
```import numpy as np
import librosa
import tensorflow as tf
wave, sr = librosa.load('/home/viktor/PycharmProjects/Record_Audio/all.wav', mono=True, sr=None)
sr = 44032 / 4
def generator(batch_size):
    while True:
        # start_positions = np.random.randint(0, len(wave) - sr, size=batch_size)
        # batch = []
        batch = []
        for start_position in range(0, len(wave) - sr, 100):
            batch.append(np.expand_dims(np.expand_dims(wave[start_position:start_position + sr], axis=-1), axis=-1))
            # batch1 = np.expand_dims(np.array(batch),axis=-1)
            # batch2 = np.expand_dims(batch1, axis=-1)
            if start_position % batch_size == 0:
                yield np.array(batch), np.array(batch)
def get_model(input_n):
    input1 = tf.keras.layers.Input(shape=(input_n, 1, 1))
    x_a = input1
    for i in [0,1,2,3,3]:#range(7):
        x_a = tf.keras.layers.Conv2D(8 * (2 ** i), (3, 1), padding='same')(x_a)
        # x_a = tf.keras.layers.BatchNormalization()(x_a)
        x_a = tf.keras.layers.Activation('relu')(x_a)
        x_a = tf.keras.layers.MaxPooling2D(pool_size=(2, 1), padding='same')(
            x_a)  # muss same sein, da wird danach GLOBAL pooling nehmen
    latent = x_a
    x_d = latent
    for i in [3,3,2,1,0]:#range(6, -1, -1):
        x_d = tf.keras.layers.Conv2D(8 * (2 ** i), (3, 1), padding='same')(x_d)
        x_d = tf.keras.layers.Activation('relu')(x_d)
        x_d = tf.keras.layers.UpSampling2D(size=(2, 1))(x_d)
    decoded = tf.keras.layers.Conv2D(1, (3, 1), padding='same')(x_d)
    model_encoder = tf.keras.models.Model(inputs=input1, outputs=latent)
    model = tf.keras.models.Model(inputs=input1, outputs=decoded)
    my_loss = tf.keras.losses.mean_squared_error
    my_opt = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
    my_metric1 = tf.keras.metrics.binary_accuracy
    my_metric2 = tf.keras.metrics.mean_squared_error
    model.compile(loss=my_loss,
                  optimizer=my_opt,
                  metrics=[my_metric1, my_metric2])
    return model, model_encoder
callbacks_list = [tf.keras.callbacks.TensorBoard(log_dir='', histogram_freq=0, write_graph=True,
                         write_images=True),
                  tf.keras.callbacks.ModelCheckpoint(monitor='loss',
                                  filepath='weights.{epoch:02d}-{loss:.4f}.hdf5',
                                  save_weights_only=False,
                                  mode='min')]
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
my_gen = generator(batch_size=1)
print 1
with tf.Session(config=config) as sess:
    print 2
    model, model_encoder = get_model(sr)
    print 3
    model.summary()

    model.fit_generator(my_gen, steps_per_epoch=100, epochs=400, verbose=1, callbacks=callbacks_list)

    model_encoder.save('my_encoder_ls_kleiner3_epoch400.h5')
    model.save('my_autocoder_ls_kleiner3_epoch400.h5')
    del model, model_encoder
```

**Other info / logs**

> /home/viktor/venv/bin/python2.7 /home/viktor/PycharmProjects/Autoencoder_Project/autio_autoencoder.py
> /home/viktor/.local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
>   from ._conv import register_converters as _register_converters
> 1
> 2018-11-25 18:59:27.573029: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
> 2018-11-25 18:59:27.650961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2018-11-25 18:59:27.651531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
> name: GeForce 940MX major: 5 minor: 0 memoryClockRate(GHz): 1.189
> pciBusID: 0000:01:00.0
> totalMemory: 1.96GiB freeMemory: 1.74GiB
> 2018-11-25 18:59:27.651542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
> 2018-11-25 18:59:28.207185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
> 2018-11-25 18:59:28.207225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
> 2018-11-25 18:59:28.207229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
> 2018-11-25 18:59:28.207358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1493 MB memory) -> physical GPU (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0)
> 2
> 3
> _________________________________________________________________
> Layer (type)                 Output Shape              Param #   
> =================================================================
> input_1 (InputLayer)         (None, 11008, 1, 1)       0         
> _________________________________________________________________
> conv2d (Conv2D)              (None, 11008, 1, 8)       32        
> _________________________________________________________________
> activation (Activation)      (None, 11008, 1, 8)       0         
> _________________________________________________________________
> max_pooling2d (MaxPooling2D) (None, 5504, 1, 8)        0         
> _________________________________________________________________
> conv2d_1 (Conv2D)            (None, 5504, 1, 16)       400       
> _________________________________________________________________
> activation_1 (Activation)    (None, 5504, 1, 16)       0         
> _________________________________________________________________
> max_pooling2d_1 (MaxPooling2 (None, 2752, 1, 16)       0         
> _________________________________________________________________
> conv2d_2 (Conv2D)            (None, 2752, 1, 32)       1568      
> _________________________________________________________________
> activation_2 (Activation)    (None, 2752, 1, 32)       0         
> _________________________________________________________________
> max_pooling2d_2 (MaxPooling2 (None, 1376, 1, 32)       0         
> _________________________________________________________________
> conv2d_3 (Conv2D)            (None, 1376, 1, 64)       6208      
> _________________________________________________________________
> activation_3 (Activation)    (None, 1376, 1, 64)       0         
> _________________________________________________________________
> max_pooling2d_3 (MaxPooling2 (None, 688, 1, 64)        0         
> _________________________________________________________________
> conv2d_4 (Conv2D)            (None, 688, 1, 64)        12352     
> _________________________________________________________________
> activation_4 (Activation)    (None, 688, 1, 64)        0         
> _________________________________________________________________
> max_pooling2d_4 (MaxPooling2 (None, 344, 1, 64)        0         
> _________________________________________________________________
> conv2d_5 (Conv2D)            (None, 344, 1, 64)        12352     
> _________________________________________________________________
> activation_5 (Activation)    (None, 344, 1, 64)        0         
> _________________________________________________________________
> up_sampling2d (UpSampling2D) (None, 688, 1, 64)        0         
> _________________________________________________________________
> conv2d_6 (Conv2D)            (None, 688, 1, 64)        12352     
> _________________________________________________________________
> activation_6 (Activation)    (None, 688, 1, 64)        0         
> _________________________________________________________________
> up_sampling2d_1 (UpSampling2 (None, 1376, 1, 64)       0         
> _________________________________________________________________
> conv2d_7 (Conv2D)            (None, 1376, 1, 32)       6176      
> _________________________________________________________________
> activation_7 (Activation)    (None, 1376, 1, 32)       0         
> _________________________________________________________________
> up_sampling2d_2 (UpSampling2 (None, 2752, 1, 32)       0         
> _________________________________________________________________
> conv2d_8 (Conv2D)            (None, 2752, 1, 16)       1552      
> _________________________________________________________________
> activation_8 (Activation)    (None, 2752, 1, 16)       0         
> _________________________________________________________________
> up_sampling2d_3 (UpSampling2 (None, 5504, 1, 16)       0         
> _________________________________________________________________
> conv2d_9 (Conv2D)            (None, 5504, 1, 8)        392       
> _________________________________________________________________
> activation_9 (Activation)    (None, 5504, 1, 8)        0         
> _________________________________________________________________
> up_sampling2d_4 (UpSampling2 (None, 11008, 1, 8)       0         
> _________________________________________________________________
> conv2d_10 (Conv2D)           (None, 11008, 1, 1)       25        
> =================================================================
> Total params: 53,409
> Trainable params: 53,409
> Non-trainable params: 0
> _________________________________________________________________
> Epoch 1/400
>   1/100 [..............................] - ETA: 3:01 - loss: 5.6426e-08 - binary_accuracy: 0.0614 - mean_squared_error: 5.6426e-08
>   2/100 [..............................] - ETA: 1:41 - loss: 6.8449e-07 - binary_accuracy: 0.0615 - mean_squared_error: 3.5635e-07
>   3/100 [..............................] - ETA: 1:15 - loss: 6.0687e-06 - binary_accuracy: 0.0616 - mean_squared_error: 2.1083e-06
>   4/100 [>.............................] - ETA: 1:02 - loss: 4.7172e-06 - binary_accuracy: 0.0617 - mean_squared_error: 1.6227e-06
>   5/100 [>.............................] - ETA: 54s - loss: 3.9497e-06 - binary_accuracy: 0.0617 - mean_squared_error: 1.3333e-06 
>   6/100 [>.............................] - ETA: 49s - loss: 3.4188e-06 - binary_accuracy: 0.0618 - mean_squared_error: 1.1323e-06
>   7/100 [=>............................] - ETA: 45s - loss: 3.3364e-06 - binary_accuracy: 0.0619 - mean_squared_error: 1.0286e-06
>   8/100 [=>............................] - ETA: 43s - loss: 3.5565e-06 - binary_accuracy: 0.0620 - mean_squared_error: 9.7965e-07
>   9/100 [=>............................] - ETA: 41s - loss: 3.5066e-06 - binary_accuracy: 0.0621 - mean_squared_error: 9.0916e-07
>  10/100 [==>...........................] - ETA: 39s - loss: 3.2021e-06 - binary_accuracy: 0.0622 - mean_squared_error: 8.2287e-07
>  11/100 [==>...........................] - ETA: 38s - loss: 3.2835e-06 - binary_accuracy: 0.0623 - mean_squared_error: 7.8192e-07
>  12/100 [==>...........................] - ETA: 37s - loss: 3.4868e-06 - binary_accuracy: 0.0623 - mean_squared_error: 7.5651e-07
>  13/100 [==>...........................] - ETA: 36s - loss: 3.3766e-06 - binary_accuracy: 0.0624 - mean_squared_error: 7.1047e-07
>  14/100 [===>..........................] - ETA: 35s - loss: 3.2299e-06 - binary_accuracy: 0.0624 - mean_squared_error: 6.6647e-07
>  15/100 [===>..........................] - ETA: 34s - loss: 3.3325e-06 - binary_accuracy: 0.0625 - mean_squared_error: 6.4324e-07
>  16/100 [===>..........................] - ETA: 34s - loss: 3.2538e-06 - binary_accuracy: 0.0626 - mean_squared_error: 6.1113e-07
>  17/100 [====>.........................] - ETA: 33s - loss: 3.1276e-06 - binary_accuracy: 0.0626 - mean_squared_error: 5.7902e-07
>  18/100 [====>.........................] - ETA: 33s - loss: 3.1739e-06 - binary_accuracy: 0.0627 - mean_squared_error: 5.5907e-07
>  19/100 [====>.........................] - ETA: 33s - loss: 3.1019e-06 - binary_accuracy: 0.0627 - mean_squared_error: 5.3465e-07
>  20/100 [=====>........................] - ETA: 32s - loss: 3.0120e-06 - binary_accuracy: 0.0628 - mean_squared_error: 5.1118e-07
>  21/100 [=====>........................] - ETA: 32s - loss: 3.0295e-06 - binary_accuracy: 0.0628 - mean_squared_error: 4.9450e-07
>  22/100 [=====>........................] - ETA: 32s - loss: 2.9529e-06 - binary_accuracy: 0.0629 - mean_squared_error: 4.7480e-07
>  23/100 [=====>........................] - ETA: 32s - loss: 2.9013e-06 - binary_accuracy: 0.0629 - mean_squared_error: 4.5749e-07
>  24/100 [======>.......................] - ETA: 31s - loss: 2.8980e-06 - binary_accuracy: 0.0630 - mean_squared_error: 4.4333e-07
>  25/100 [======>.......................] - ETA: 31s - loss: 2.8224e-06 - binary_accuracy: 0.0630 - mean_squared_error: 4.2721e-07
>  26/100 [======>.......................] - ETA: 31s - loss: 2.8057e-06 - binary_accuracy: 0.0631 - mean_squared_error: 4.1431e-07
>  27/100 [=======>......................] - ETA: 31s - loss: 2.7728e-06 - binary_accuracy: 0.0631 - mean_squared_error: 4.0160e-07
>  28/100 [=======>......................] - ETA: 30s - loss: 2.7213e-06 - binary_accuracy: 0.0632 - mean_squared_error: 3.8895e-07
>  29/100 [=======>......................] - ETA: 30s - loss: 2.7131e-06 - binary_accuracy: 0.0632 - mean_squared_error: 3.7849e-07
>  30/100 [========>.....................] - ETA: 30s - loss: 2.6618e-06 - binary_accuracy: 0.0633 - mean_squared_error: 3.6718e-07
>  31/100 [========>.....................] - ETA: 30s - loss: 2.6473e-06 - binary_accuracy: 0.0633 - mean_squared_error: 3.5764e-07
>  32/100 [========>.....................] - ETA: 29s - loss: 2.6142e-06 - binary_accuracy: 0.0633 - mean_squared_error: 3.4801e-07
>  33/100 [========>.....................] - ETA: 29s - loss: 2.5874e-06 - binary_accuracy: 0.0634 - mean_squared_error: 3.3906e-07
>  34/100 [=========>....................] - ETA: 29s - loss: 2.5688e-06 - binary_accuracy: 0.0634 - mean_squared_error: 3.3078e-07
>  35/100 [=========>....................] - ETA: 29s - loss: 2.5371e-06 - binary_accuracy: 0.0634 - mean_squared_error: 3.2252e-07
>  36/100 [=========>....................] - ETA: 29s - loss: 2.5250e-06 - binary_accuracy: 0.0635 - mean_squared_error: 3.1518e-07
>  37/100 [==========>...................] - ETA: 29s - loss: 2.4949e-06 - binary_accuracy: 0.0635 - mean_squared_error: 3.0769e-07
>  38/100 [==========>...................] - ETA: 29s - loss: 2.4849e-06 - binary_accuracy: 0.0635 - mean_squared_error: 3.0106e-07
>  39/100 [==========>...................] - ETA: 29s - loss: 2.4585e-06 - binary_accuracy: 0.0635 - mean_squared_error: 2.9430e-07
>  40/100 [===========>..................] - ETA: 28s - loss: 2.4495e-06 - binary_accuracy: 0.0636 - mean_squared_error: 2.8825e-07
>  41/100 [===========>..................] - ETA: 28s - loss: 2.4269e-06 - binary_accuracy: 0.0636 - mean_squared_error: 2.8213e-07
>  42/100 [===========>..................] - ETA: 28s - loss: 2.4186e-06 - binary_accuracy: 0.0636 - mean_squared_error: 2.7659e-07
>  43/100 [===========>..................] - ETA: 28s - loss: 2.3995e-06 - binary_accuracy: 0.0636 - mean_squared_error: 2.7102e-07
>  44/100 [============>.................] - ETA: 28s - loss: 2.3915e-06 - binary_accuracy: 0.0637 - mean_squared_error: 2.6592e-07
>  45/100 [============>.................] - ETA: 28s - loss: 2.3764e-06 - binary_accuracy: 0.0637 - mean_squared_error: 2.6085e-07
>  46/100 [============>.................] - ETA: 27s - loss: 2.3676e-06 - binary_accuracy: 0.0637 - mean_squared_error: 2.5611e-07
>  47/100 [=============>................] - ETA: 27s - loss: 2.3571e-06 - binary_accuracy: 0.0637 - mean_squared_error: 2.5151e-07
>  48/100 [=============>................] - ETA: 27s - loss: 2.3469e-06 - binary_accuracy: 0.0637 - mean_squared_error: 2.4708e-07
>  49/100 [=============>................] - ETA: 26s - loss: 2.3409e-06 - binary_accuracy: 0.0637 - mean_squared_error: 2.4290e-07
>  50/100 [==============>...............] - ETA: 26s - loss: 2.3305e-06 - binary_accuracy: 0.0638 - mean_squared_error: 2.3877e-07
>  51/100 [==============>...............] - ETA: 26s - loss: 2.3262e-06 - binary_accuracy: 0.0638 - mean_squared_error: 2.3490e-07
>  52/100 [==============>...............] - ETA: 25s - loss: 2.3192e-06 - binary_accuracy: 0.0638 - mean_squared_error: 2.3111e-07
>  53/100 [==============>...............] - ETA: 25s - loss: 2.3130e-06 - binary_accuracy: 0.0638 - mean_squared_error: 2.2745e-07
>  54/100 [===============>..............] - ETA: 25s - loss: 2.3104e-06 - binary_accuracy: 0.0638 - mean_squared_error: 2.2399e-07
>  55/100 [===============>..............] - ETA: 24s - loss: 2.3047e-06 - binary_accuracy: 0.0638 - mean_squared_error: 2.2057e-07
>  56/100 [===============>..............] - ETA: 24s - loss: 2.3013e-06 - binary_accuracy: 0.0638 - mean_squared_error: 2.1731e-07
>  57/100 [================>.............] - ETA: 24s - loss: 2.3000e-06 - binary_accuracy: 0.0639 - mean_squared_error: 2.1418e-07
>  58/100 [================>.............] - ETA: 23s - loss: 2.2966e-06 - binary_accuracy: 0.0639 - mean_squared_error: 2.1112e-07
>  59/100 [================>.............] - ETA: 23s - loss: 2.2944e-06 - binary_accuracy: 0.0639 - mean_squared_error: 2.0816e-07
>  60/100 [=================>............] - ETA: 22s - loss: 2.2947e-06 - binary_accuracy: 0.0639 - mean_squared_error: 2.0533e-07
>  61/100 [=================>............] - ETA: 22s - loss: 2.2942e-06 - binary_accuracy: 0.0639 - mean_squared_error: 2.0257e-07
>  62/100 [=================>............] - ETA: 22s - loss: 2.2929e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.9988e-07
>  63/100 [=================>............] - ETA: 21s - loss: 2.2933e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.9729e-07
>  64/100 [==================>...........] - ETA: 21s - loss: 2.2954e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.9480e-07
>  65/100 [==================>...........] - ETA: 20s - loss: 2.2974e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.9238e-07
>  66/100 [==================>...........] - ETA: 20s - loss: 2.2985e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.9001e-07
>  67/100 [===================>..........] - ETA: 20s - loss: 2.2998e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.8771e-07
>  68/100 [===================>..........] - ETA: 19s - loss: 2.3021e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.8548e-07
>  69/100 [===================>..........] - ETA: 19s - loss: 2.3059e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.8333e-07
>  70/100 [====================>.........] - ETA: 18s - loss: 2.3117e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.8126e-07
>  71/100 [====================>.........] - ETA: 18s - loss: 2.3200e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.7928e-07
>  72/100 [====================>.........] - ETA: 17s - loss: 2.3317e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.7740e-07
>  73/100 [====================>.........] - ETA: 17s - loss: 2.3489e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.7565e-07
>  74/100 [=====================>........] - ETA: 16s - loss: 2.3727e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.7402e-07
>  75/100 [=====================>........] - ETA: 16s - loss: 2.4025e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.7252e-07
>  76/100 [=====================>........] - ETA: 15s - loss: 2.4302e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.7103e-07
>  77/100 [======================>.......] - ETA: 15s - loss: 2.4468e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.6944e-07
>  78/100 [======================>.......] - ETA: 14s - loss: 2.4526e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.6774e-07
>  79/100 [======================>.......] - ETA: 14s - loss: 2.4577e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.6608e-07
>  80/100 [=======================>......] - ETA: 13s - loss: 2.4705e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.6454e-07
>  81/100 [=======================>......] - ETA: 13s - loss: 2.4939e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.6318e-07
>  82/100 [=======================>......] - ETA: 12s - loss: 2.5245e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.6193e-07
>  83/100 [=======================>......] - ETA: 12s - loss: 2.5491e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.6064e-07
>  84/100 [========================>.....] - ETA: 11s - loss: 2.5604e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.5923e-07
>  85/100 [========================>.....] - ETA: 10s - loss: 2.5653e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.5777e-07
>  86/100 [========================>.....] - ETA: 10s - loss: 2.5760e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.5640e-07
>  87/100 [=========================>....] - ETA: 9s - loss: 2.5959e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.5517e-07 
>  88/100 [=========================>....] - ETA: 8s - loss: 2.6178e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.5399e-07
>  89/100 [=========================>....] - ETA: 8s - loss: 2.6327e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.5276e-07
>  90/100 [==========================>...] - ETA: 7s - loss: 2.6398e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.5147e-07
>  91/100 [==========================>...] - ETA: 6s - loss: 2.6464e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.5020e-07
>  92/100 [==========================>...] - ETA: 6s - loss: 2.6586e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.4901e-07
>  93/100 [==========================>...] - ETA: 5s - loss: 2.6763e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.4790e-07
>  94/100 [===========================>..] - ETA: 4s - loss: 2.6945e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.4683e-07
>  95/100 [===========================>..] - ETA: 3s - loss: 2.7082e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.4572e-07
>  96/100 [===========================>..] - ETA: 3s - loss: 2.7168e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.4459e-07
>  97/100 [============================>.] - ETA: 2s - loss: 2.7235e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.4346e-07
>  98/100 [============================>.] - ETA: 1s - loss: 2.7323e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.4237e-07
>  99/100 [============================>.] - ETA: 0s - loss: 2.7451e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.4134e-07
> 100/100 [==============================] - 81s 808ms/step - loss: 2.7638e-06 - binary_accuracy: 0.0639 - mean_squared_error: 1.3951e-07
> Epoch 2/400
>   1/100 [..............................] - ETA: 2:13 - loss: 4.6511e-06 - binary_accuracy: 0.0641 - mean_squared_error: 4.6050e-08
>   2/100 [..............................] - ETA: 2:11 - loss: 4.6505e-06 - binary_accuracy: 0.0641 - mean_squared_error: 4.5819e-08
>   3/100 [..............................] - ETA: 2:10 - loss: 4.5782e-06 - binary_accuracy: 0.0641 - mean_squared_error: 4.4894e-08
>   4/100 [>.............................] - ETA: 2:09 - loss: 4.4568e-06 - binary_accuracy: 0.0641 - mean_squared_error: 4.3509e-08
>   5/100 [>.............................] - ETA: 2:09 - loss: 4.3280e-06 - binary_accuracy: 0.0641 - mean_squared_error: 4.2070e-08
>   6/100 [>.............................] - ETA: 2:12 - loss: 4.2212e-06 - binary_accuracy: 0.0641 - mean_squared_error: 4.0855e-08
>   7/100 [=>............................] - ETA: 2:15 - loss: 4.1498e-06 - binary_accuracy: 0.0642 - mean_squared_error: 3.9987e-08
>   8/100 [=>............................] - ETA: 2:16 - loss: 4.1162e-06 - binary_accuracy: 0.0642 - mean_squared_error: 3.9481e-08
>   9/100 [=>............................] - ETA: 2:17 - loss: 4.1209e-06 - binary_accuracy: 0.0642 - mean_squared_error: 3.9333e-08
>  10/100 [==>...........................] - ETA: 2:18 - loss: 4.1703e-06 - binary_accuracy: 0.0642 - mean_squared_error: 3.9595e-08
>  11/100 [==>...........................] - ETA: 2:18 - loss: 4.2791e-06 - binary_accuracy: 0.0642 - mean_squared_error: 4.0391e-08
>  12/100 [==>...........................] - ETA: 2:18 - loss: 4.4724e-06 - binary_accuracy: 0.0642 - mean_squared_error: 4.1935e-08
>  13/100 [==>...........................] - ETA: 2:18 - loss: 4.7454e-06 - binary_accuracy: 0.0643 - mean_squared_error: 4.4170e-08
>  14/100 [===>..........................] - ETA: 2:18 - loss: 5.0277e-06 - binary_accuracy: 0.0643 - mean_squared_error: 4.6464e-08
>  15/100 [===>..........................] - ETA: 2:17 - loss: 5.1614e-06 - binary_accuracy: 0.0643 - mean_squared_error: 4.7444e-08
>  16/100 [===>..........................] - ETA: 2:16 - loss: 5.1270e-06 - binary_accuracy: 0.0643 - mean_squared_error: 4.6963e-08
>  17/100 [====>.........................] - ETA: 2:16 - loss: 5.0722e-06 - binary_accuracy: 0.0643 - mean_squared_error: 4.6310e-08
>  18/100 [====>.........................] - ETA: 2:14 - loss: 5.1136e-06 - binary_accuracy: 0.0643 - mean_squared_error: 4.6476e-08
>  19/100 [====>.........................] - ETA: 2:12 - loss: 5.2091e-06 - binary_accuracy: 0.0643 - mean_squared_error: 4.7094e-08
>  20/100 [=====>........................] - ETA: 2:10 - loss: 5.2402e-06 - binary_accuracy: 0.0644 - mean_squared_error: 4.7169e-08
>  21/100 [=====>........................] - ETA: 2:08 - loss: 5.1967e-06 - binary_accuracy: 0.0644 - mean_squared_error: 4.6626e-08
>  22/100 [=====>........................] - ETA: 2:07 - loss: 5.1680e-06 - binary_accuracy: 0.0644 - mean_squared_error: 4.6207e-08
>  23/100 [=====>........................] - ETA: 2:05 - loss: 5.1954e-06 - binary_accuracy: 0.0644 - mean_squared_error: 4.6248e-08
>  24/100 [======>.......................] - ETA: 2:03 - loss: 5.2299e-06 - binary_accuracy: 0.0644 - mean_squared_error: 4.6345e-08
>  25/100 [======>.......................] - ETA: 2:01 - loss: 5.2190e-06 - binary_accuracy: 0.0644 - mean_squared_error: 4.6077e-08
>  26/100 [======>.......................] - ETA: 2:00 - loss: 5.1851e-06 - binary_accuracy: 0.0644 - mean_squared_error: 4.5629e-08
>  27/100 [=======>......................] - ETA: 1:58 - loss: 5.1754e-06 - binary_accuracy: 0.0645 - mean_squared_error: 4.5375e-08
>  28/100 [=======>......................] - ETA: 1:57 - loss: 5.1923e-06 - binary_accuracy: 0.0645 - mean_squared_error: 4.5330e-08
>  29/100 [=======>......................] - ETA: 1:55 - loss: 5.2030e-06 - binary_accuracy: 0.0645 - mean_squared_error: 4.5239e-08
>  30/100 [========>.....................] - ETA: 1:54 - loss: 5.1889e-06 - binary_accuracy: 0.0645 - mean_squared_error: 4.4956e-08
>  31/100 [========>.....................] - ETA: 1:52 - loss: 5.1672e-06 - binary_accuracy: 0.0645 - mean_squared_error: 4.4618e-08
>  32/100 [========>.....................] - ETA: 1:51 - loss: 5.1605e-06 - binary_accuracy: 0.0645 - mean_squared_error: 4.4396e-08
>  33/100 [========>.....................] - ETA: 1:49 - loss: 5.1693e-06 - binary_accuracy: 0.0645 - mean_squared_error: 4.4292e-08
>  34/100 [=========>....................] - ETA: 1:48 - loss: 5.1775e-06 - binary_accuracy: 0.0646 - mean_squared_error: 4.4185e-08
>  35/100 [=========>....................] - ETA: 1:47 - loss: 5.1730e-06 - binary_accuracy: 0.0646 - mean_squared_error: 4.3986e-08
>  36/100 [=========>....................] - ETA: 1:46 - loss: 5.1600e-06 - binary_accuracy: 0.0646 - mean_squared_error: 4.3725e-08
>  37/100 [==========>...................] - ETA: 1:45 - loss: 5.1500e-06 - binary_accuracy: 0.0646 - mean_squared_error: 4.3488e-08
>  38/100 [==========>...................] - ETA: 1:44 - loss: 5.1495e-06 - binary_accuracy: 0.0646 - mean_squared_error: 4.3322e-08
>  39/100 [==========>...................] - ETA: 1:44 - loss: 5.1570e-06 - binary_accuracy: 0.0646 - mean_squared_error: 4.3215e-08
>  40/100 [===========>..................] - ETA: 1:43 - loss: 5.1657e-06 - binary_accuracy: 0.0646 - mean_squared_error: 4.3117e-08
>  41/100 [===========>..................] - ETA: 1:42 - loss: 5.1702e-06 - binary_accuracy: 0.0646 - mean_squared_error: 4.2992e-08
>  42/100 [===========>..................] - ETA: 1:40 - loss: 5.1690e-06 - binary_accuracy: 0.0647 - mean_squared_error: 4.2827e-08
>  43/100 [===========>..................] - ETA: 1:39 - loss: 5.1643e-06 - binary_accuracy: 0.0647 - mean_squared_error: 4.2638e-08
>  44/100 [============>.................] - ETA: 1:38 - loss: 5.1592e-06 - binary_accuracy: 0.0647 - mean_squared_error: 4.2449e-08
>  45/100 [============>.................] - ETA: 1:37 - loss: 5.1566e-06 - binary_accuracy: 0.0647 - mean_squared_error: 4.2278e-08
>  46/100 [============>.................] - ETA: 1:35 - loss: 5.1580e-06 - binary_accuracy: 0.0647 - mean_squared_error: 4.2137e-08
>  47/100 [=============>................] - ETA: 1:34 - loss: 5.1646e-06 - binary_accuracy: 0.0647 - mean_squared_error: 4.2031e-08
>  48/100 [=============>................] - ETA: 1:32 - loss: 5.1776e-06 - binary_accuracy: 0.0647 - mean_squared_error: 4.1971e-08
>  49/100 [=============>................] - ETA: 1:31 - loss: 5.1990e-06 - binary_accuracy: 0.0648 - mean_squared_error: 4.1967e-08
>  50/100 [==============>...............] - ETA: 1:29 - loss: 5.2322e-06 - binary_accuracy: 0.0648 - mean_squared_error: 4.2042e-08
>  51/100 [==============>...............] - ETA: 1:27 - loss: 5.2805e-06 - binary_accuracy: 0.0648 - mean_squared_error: 4.2217e-08
>  52/100 [==============>...............] - ETA: 1:26 - loss: 5.3460e-06 - binary_accuracy: 0.0648 - mean_squared_error: 4.2504e-08
>  53/100 [==============>...............] - ETA: 1:24 - loss: 5.4203e-06 - binary_accuracy: 0.0648 - mean_squared_error: 4.2847e-08
>  54/100 [===============>..............] - ETA: 1:22 - loss: 5.4860e-06 - binary_accuracy: 0.0648 - mean_squared_error: 4.3132e-08
>  55/100 [===============>..............] - ETA: 1:21 - loss: 5.5213e-06 - binary_accuracy: 0.0648 - mean_squared_error: 4.3219e-08
>  56/100 [===============>..............] - ETA: 1:19 - loss: 5.5272e-06 - binary_accuracy: 0.0648 - mean_squared_error: 4.3117e-08
>  57/100 [================>.............] - ETA: 1:17 - loss: 5.5249e-06 - binary_accuracy: 0.0649 - mean_squared_error: 4.2964e-08
>  58/100 [================>.............] - ETA: 1:16 - loss: 5.5362e-06 - binary_accuracy: 0.0649 - mean_squared_error: 4.2897e-08
>  59/100 [================>.............] - ETA: 1:14 - loss: 5.5663e-06 - binary_accuracy: 0.0649 - mean_squared_error: 4.2950e-08
>  60/100 [=================>............] - ETA: 1:13 - loss: 5.6027e-06 - binary_accuracy: 0.0649 - mean_squared_error: 4.3042e-08
>  61/100 [=================>............] - ETA: 1:11 - loss: 5.6289e-06 - binary_accuracy: 0.0649 - mean_squared_error: 4.3069e-08
>  62/100 [=================>............] - ETA: 1:10 - loss: 5.6372e-06 - binary_accuracy: 0.0649 - mean_squared_error: 4.2986e-08
>  63/100 [=================>............] - ETA: 1:08 - loss: 5.6366e-06 - binary_accuracy: 0.0649 - mean_squared_error: 4.2849e-08
>  64/100 [==================>...........] - ETA: 1:07 - loss: 5.6406e-06 - binary_accuracy: 0.0649 - mean_squared_error: 4.2741e-08
>  65/100 [==================>...........] - ETA: 1:05 - loss: 5.6555e-06 - binary_accuracy: 0.0649 - mean_squared_error: 4.2699e-08
>  66/100 [==================>...........] - ETA: 1:03 - loss: 5.6778e-06 - binary_accuracy: 0.0650 - mean_squared_error: 4.2703e-08
>  67/100 [===================>..........] - ETA: 1:02 - loss: 5.6985e-06 - binary_accuracy: 0.0650 - mean_squared_error: 4.2697e-08
>  68/100 [===================>..........] - ETA: 1:00 - loss: 5.7110e-06 - binary_accuracy: 0.0650 - mean_squared_error: 4.2642e-08
>  69/100 [===================>..........] - ETA: 59s - loss: 5.7153e-06 - binary_accuracy: 0.0650 - mean_squared_error: 4.2540e-08 
>  70/100 [====================>.........] - ETA: 57s - loss: 5.7170e-06 - binary_accuracy: 0.0650 - mean_squared_error: 4.2422e-08
>  71/100 [====================>.........] - ETA: 55s - loss: 5.7216e-06 - binary_accuracy: 0.0650 - mean_squared_error: 4.2323e-08
>  72/100 [====================>.........] - ETA: 54s - loss: 5.7319e-06 - binary_accuracy: 0.0650 - mean_squared_error: 4.2257e-08
>  73/100 [====================>.........] - ETA: 52s - loss: 5.7473e-06 - binary_accuracy: 0.0650 - mean_squared_error: 4.2221e-08
>  74/100 [=====================>........] - ETA: 50s - loss: 5.7650e-06 - binary_accuracy: 0.0650 - mean_squared_error: 4.2198e-08
>  75/100 [=====================>........] - ETA: 48s - loss: 5.7820e-06 - binary_accuracy: 0.0650 - mean_squared_error: 4.2172e-08
>  76/100 [=====================>........] - ETA: 46s - loss: 5.7958e-06 - binary_accuracy: 0.0651 - mean_squared_error: 4.2128e-08
>  77/100 [======================>.......] - ETA: 45s - loss: 5.8058e-06 - binary_accuracy: 0.0651 - mean_squared_error: 4.2062e-08
>  78/100 [======================>.......] - ETA: 43s - loss: 5.8125e-06 - binary_accuracy: 0.0651 - mean_squared_error: 4.1979e-08
>  79/100 [======================>.......] - ETA: 41s - loss: 5.8174e-06 - binary_accuracy: 0.0651 - mean_squared_error: 4.1886e-08
>  80/100 [=======================>......] - ETA: 39s - loss: 5.8219e-06 - binary_accuracy: 0.0651 - mean_squared_error: 4.1791e-08
>  81/100 [=======================>......] - ETA: 37s - loss: 5.8271e-06 - binary_accuracy: 0.0651 - mean_squared_error: 4.1701e-08
>  82/100 [=======================>......] - ETA: 35s - loss: 5.8339e-06 - binary_accuracy: 0.0651 - mean_squared_error: 4.1620e-08
>  83/100 [=======================>......] - ETA: 33s - loss: 5.8430e-06 - binary_accuracy: 0.0651 - mean_squared_error: 4.1553e-08
>  84/100 [========================>.....] - ETA: 31s - loss: 5.8556e-06 - binary_accuracy: 0.0651 - mean_squared_error: 4.1504e-08
>  85/100 [========================>.....] - ETA: 29s - loss: 5.8737e-06 - binary_accuracy: 0.0652 - mean_squared_error: 4.1486e-08
>  86/100 [========================>.....] - ETA: 27s - loss: 5.9012e-06 - binary_accuracy: 0.0652 - mean_squared_error: 4.1519e-08
>  87/100 [=========================>....] - ETA: 26s - loss: 5.9436e-06 - binary_accuracy: 0.0652 - mean_squared_error: 4.1631e-08
>  88/100 [=========================>....] - ETA: 24s - loss: 6.0074e-06 - binary_accuracy: 0.0652 - mean_squared_error: 4.1857e-08
>  89/100 [=========================>....] - ETA: 22s - loss: 6.0908e-06 - binary_accuracy: 0.0652 - mean_squared_error: 4.2185e-08
>  90/100 [==========================>...] - ETA: 20s - loss: 6.1760e-06 - binary_accuracy: 0.0652 - mean_squared_error: 4.2521e-08
>  91/100 [==========================>...] - ETA: 18s - loss: 6.2324e-06 - binary_accuracy: 0.0652 - mean_squared_error: 4.2704e-08
>  92/100 [==========================>...] - ETA: 16s - loss: 6.2507e-06 - binary_accuracy: 0.0652 - mean_squared_error: 4.2688e-08
>  93/100 [==========================>...] - ETA: 14s - loss: 6.2549e-06 - binary_accuracy: 0.0652 - mean_squared_error: 4.2599e-08
>  94/100 [===========================>..] - ETA: 12s - loss: 6.2718e-06 - binary_accuracy: 0.0652 - mean_squared_error: 4.2576e-08
>  95/100 [===========================>..] - ETA: 10s - loss: 6.3854e-06 - binary_accuracy: 0.0652 - mean_squared_error: 4.3049e-08
>  96/100 [===========================>..] - ETA: 8s - loss: 6.8662e-06 - binary_accuracy: 0.0653 - mean_squared_error: 4.5393e-08 
>  97/100 [============================>.] - ETA: 6s - loss: 7.3806e-06 - binary_accuracy: 0.0653 - mean_squared_error: 4.7896e-08
>  98/100 [============================>.] - ETA: 4s - loss: 7.3750e-06 - binary_accuracy: 0.0653 - mean_squared_error: 4.7759e-08
>  99/100 [============================>.] - ETA: 2s - loss: 7.8195e-06 - binary_accuracy: 0.0653 - mean_squared_error: 4.9885e-08
> 100/100 [==============================] - 207s 2s/step - loss: 7.9570e-06 - binary_accuracy: 0.0653 - mean_squared_error: 5.0452e-08
> Epoch 3/400
>   1/100 [..............................] - ETA: 4:18 - loss: 2.4469e-05 - binary_accuracy: 0.0661 - mean_squared_error: 1.2174e-07
>   2/100 [..............................] - ETA: 4:14 - loss: 2.7106e-05 - binary_accuracy: 0.0661 - mean_squared_error: 1.3449e-07
>   3/100 [..............................] - ETA: 4:13 - loss: 2.1926e-05 - binary_accuracy: 0.0661 - mean_squared_error: 1.0865e-07
>   4/100 [>.............................] - ETA: 4:12 - loss: 2.3737e-05 - binary_accuracy: 0.0661 - mean_squared_error: 1.1723e-07
>   5/100 [>.............................] - ETA: 4:10 - loss: 2.0573e-05 - binary_accuracy: 0.0661 - mean_squared_error: 1.0151e-07
>   6/100 [>.............................] - ETA: 4:10 - loss: 2.2641e-05 - binary_accuracy: 0.0661 - mean_squared_error: 1.1128e-07
>   7/100 [=>............................] - ETA: 4:09 - loss: 2.0556e-05 - binary_accuracy: 0.0661 - mean_squared_error: 1.0094e-07
>   8/100 [=>............................] - ETA: 4:07 - loss: 2.0933e-05 - binary_accuracy: 0.0661 - mean_squared_error: 1.0248e-07
>   9/100 [=>............................] - ETA: 4:04 - loss: 1.9599e-05 - binary_accuracy: 0.0661 - mean_squared_error: 9.5842e-08
>  10/100 [==>...........................] - ETA: 4:02 - loss: 1.9594e-05 - binary_accuracy: 0.0661 - mean_squared_error: 9.5566e-08
>  11/100 [==>...........................] - ETA: 4:00 - loss: 1.8655e-05 - binary_accuracy: 0.0661 - mean_squared_error: 9.0869e-08
>  12/100 [==>...........................] - ETA: 3:59 - loss: 1.8528e-05 - binary_accuracy: 0.0661 - mean_squared_error: 9.0033e-08
>  13/100 [==>...........................] - ETA: 3:56 - loss: 1.7757e-05 - binary_accuracy: 0.0661 - mean_squared_error: 8.6177e-08
>  14/100 [===>..........................] - ETA: 3:54 - loss: 1.7610e-05 - binary_accuracy: 0.0661 - mean_squared_error: 8.5263e-08
>  15/100 [===>..........................] - ETA: 3:55 - loss: 1.6979e-05 - binary_accuracy: 0.0661 - mean_squared_error: 8.2105e-082018-11-25 19:04:59.501703: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 217.69MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
> 
>  16/100 [===>..........................] - ETA: 4:00 - loss: 1.6821e-05 - binary_accuracy: 0.0661 - mean_squared_error: 8.1152e-082018-11-25 19:05:02.936137: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 218.70MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
> 
>  17/100 [====>.........................] - ETA: 4:00 - loss: 1.6288e-05 - binary_accuracy: 0.0661 - mean_squared_error: 7.8482e-082018-11-25 19:05:06.861676: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 219.70MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
> 
>  18/100 [====>.........................] - ETA: 4:04 - loss: 1.6132e-05 - binary_accuracy: 0.0661 - mean_squared_error: 7.7558e-082018-11-25 19:05:10.940344: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 220.71MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
> 
>  19/100 [====>.........................] - ETA: 4:04 - loss: 1.5687e-05 - binary_accuracy: 0.0661 - mean_squared_error: 7.5320e-082018-11-25 19:05:14.366659: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 221.72MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
> 
>  20/100 [=====>........................] - ETA: 4:00 - loss: 1.5526e-05 - binary_accuracy: 0.0661 - mean_squared_error: 7.4390e-082018-11-25 19:05:17.314344: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 222.73MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
> 
>  21/100 [=====>........................] - ETA: 3:57 - loss: 1.5154e-05 - binary_accuracy: 0.0661 - mean_squared_error: 7.2510e-082018-11-25 19:05:20.200250: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 223.73MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
> 
>  22/100 [=====>........................] - ETA: 3:53 - loss: 1.4995e-05 - binary_accuracy: 0.0661 - mean_squared_error: 7.1600e-082018-11-25 19:05:23.086283: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 224.74MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
> 
>  23/100 [=====>........................] - ETA: 3:50 - loss: 1.4684e-05 - binary_accuracy: 0.0661 - mean_squared_error: 7.0018e-082018-11-25 19:05:26.003008: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 225.75MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
> 
>  24/100 [======>.......................] - ETA: 3:48 - loss: 1.4525e-05 - binary_accuracy: 0.0661 - mean_squared_error: 6.9119e-082018-11-25 19:05:29.164767: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 226.76MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
> 
>  25/100 [======>.......................] - ETA: 3:45 - loss: 1.4266e-05 - binary_accuracy: 0.0661 - mean_squared_error: 6.7787e-08
>  26/100 [======>.......................] - ETA: 3:42 - loss: 1.4110e-05 - binary_accuracy: 0.0661 - mean_squared_error: 6.6917e-08
>  27/100 [=======>......................] - ETA: 3:38 - loss: 1.3893e-05 - binary_accuracy: 0.0661 - mean_squared_error: 6.5783e-08
>  28/100 [=======>......................] - ETA: 3:35 - loss: 1.3742e-05 - binary_accuracy: 0.0661 - mean_squared_error: 6.4949e-08
>  29/100 [=======>......................] - ETA: 3:32 - loss: 1.3558e-05 - binary_accuracy: 0.0661 - mean_squared_error: 6.3977e-08
>  30/100 [========>.....................] - ETA: 3:29 - loss: 1.3416e-05 - binary_accuracy: 0.0661 - mean_squared_error: 6.3188e-08
>  31/100 [========>.....................] - ETA: 3:26 - loss: 1.3258e-05 - binary_accuracy: 0.0661 - mean_squared_error: 6.2341e-08
>  32/100 [========>.....................] - ETA: 3:24 - loss: 1.3125e-05 - binary_accuracy: 0.0661 - mean_squared_error: 6.1603e-08
>  33/100 [========>.....................] - ETA: 3:21 - loss: 1.2988e-05 - binary_accuracy: 0.0661 - mean_squared_error: 6.0857e-08
>  34/100 [=========>....................] - ETA: 3:18 - loss: 1.2865e-05 - binary_accuracy: 0.0661 - mean_squared_error: 6.0173e-08
>  35/100 [=========>....................] - ETA: 3:15 - loss: 1.2745e-05 - binary_accuracy: 0.0661 - mean_squared_error: 5.9507e-08
>  36/100 [=========>....................] - ETA: 3:13 - loss: 1.2632e-05 - binary_accuracy: 0.0661 - mean_squared_error: 5.8876e-08
>  37/100 [==========>...................] - ETA: 3:10 - loss: 1.2526e-05 - binary_accuracy: 0.0661 - mean_squared_error: 5.8277e-08
>  38/100 [==========>...................] - ETA: 3:08 - loss: 1.2423e-05 - binary_accuracy: 0.0661 - mean_squared_error: 5.7697e-08
>  39/100 [==========>...................] - ETA: 3:05 - loss: 1.2328e-05 - binary_accuracy: 0.0661 - mean_squared_error: 5.7152e-08
>  40/100 [===========>..................] - ETA: 3:03 - loss: 1.2235e-05 - binary_accuracy: 0.0661 - mean_squared_error: 5.6621e-08
>  41/100 [===========>..................] - ETA: 3:00 - loss: 1.2149e-05 - binary_accuracy: 0.0661 - mean_squared_error: 5.6122e-08
>  42/100 [===========>..................] - ETA: 2:57 - loss: 1.2065e-05 - binary_accuracy: 0.0661 - mean_squared_error: 5.5634e-08
>  43/100 [===========>..................] - ETA: 2:55 - loss: 1.1988e-05 - binary_accuracy: 0.0661 - mean_squared_error: 5.5176e-08
>  44/100 [============>.................] - ETA: 2:52 - loss: 1.1912e-05 - binary_accuracy: 0.0661 - mean_squared_error: 5.4728e-08
>  45/100 [============>.................] - ETA: 2:49 - loss: 1.1842e-05 - binary_accuracy: 0.0661 - mean_squared_error: 5.4305e-08
>  46/100 [============>.................] - ETA: 2:46 - loss: 1.1773e-05 - binary_accuracy: 0.0661 - mean_squared_error: 5.3894e-08
>  47/100 [=============>................] - ETA: 2:43 - loss: 1.1709e-05 - binary_accuracy: 0.0661 - mean_squared_error: 5.3502e-08
>  48/100 [=============>................] - ETA: 2:40 - loss: 1.1648e-05 - binary_accuracy: 0.0661 - mean_squared_error: 5.3123e-08
>  49/100 [=============>................] - ETA: 2:38 - loss: 1.1589e-05 - binary_accuracy: 0.0661 - mean_squared_error: 5.2759e-08
>  50/100 [==============>...............] - ETA: 2:35 - loss: 1.1534e-05 - binary_accuracy: 0.0661 - mean_squared_error: 5.2409e-08
>  51/100 [==============>...............] - ETA: 2:32 - loss: 1.1480e-05 - binary_accuracy: 0.0661 - mean_squared_error: 5.2070e-08
>  52/100 [==============>...............] - ETA: 2:29 - loss: 1.1430e-05 - binary_accuracy: 0.0661 - mean_squared_error: 5.1746e-08
>  53/100 [==============>...............] - ETA: 2:26 - loss: 1.1382e-05 - binary_accuracy: 0.0661 - mean_squared_error: 5.1429e-08
>  54/100 [===============>..............] - ETA: 2:23 - loss: 1.1337e-05 - binary_accuracy: 0.0661 - mean_squared_error: 5.1129e-082018-11-25 19:07:16.149663: W tensorflow/core/common_runtime/bfc_allocator.cc:271] Allocator (GPU_0_bfc) ran out of memory trying to allocate 85.66MiB.  Current allocation summary follows.
> 2018-11-25 19:07:16.149714: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (256): 	Total Chunks: 162, Chunks in use: 161. 40.5KiB allocated for chunks. 40.2KiB in use in bin. 10.6KiB client-requested in use in bin.
> 2018-11-25 19:07:16.149724: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (512): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
> 2018-11-25 19:07:16.149731: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (1024): 	Total Chunks: 10, Chunks in use: 10. 15.2KiB allocated for chunks. 15.2KiB in use in bin. 14.5KiB client-requested in use in bin.
> 2018-11-25 19:07:16.149735: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (2048): 	Total Chunks: 3, Chunks in use: 3. 7.5KiB allocated for chunks. 7.5KiB in use in bin. 4.5KiB client-requested in use in bin.
> 2018-11-25 19:07:16.149739: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (4096): 	Total Chunks: 10, Chunks in use: 10. 63.2KiB allocated for chunks. 63.2KiB in use in bin. 60.0KiB client-requested in use in bin.
> 2018-11-25 19:07:16.149743: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (8192): 	Total Chunks: 2, Chunks in use: 2. 18.5KiB allocated for chunks. 18.5KiB in use in bin. 12.0KiB client-requested in use in bin.
> 2018-11-25 19:07:16.149747: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (16384): 	Total Chunks: 12, Chunks in use: 11. 302.0KiB allocated for chunks. 273.0KiB in use in bin. 264.0KiB client-requested in use in bin.
> 2018-11-25 19:07:16.149752: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (32768): 	Total Chunks: 14, Chunks in use: 13. 677.2KiB allocated for chunks. 624.8KiB in use in bin. 600.0KiB client-requested in use in bin.
> 2018-11-25 19:07:16.149756: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (65536): 	Total Chunks: 4, Chunks in use: 4. 314.2KiB allocated for chunks. 314.2KiB in use in bin. 192.0KiB client-requested in use in bin.
> 2018-11-25 19:07:16.149759: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (131072): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
> 2018-11-25 19:07:16.149763: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (262144): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
> 2018-11-25 19:07:16.149766: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (524288): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
> 2018-11-25 19:07:16.149769: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (1048576): 	Total Chunks: 1, Chunks in use: 0. 1.59MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
> 2018-11-25 19:07:16.149772: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (2097152): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
> 2018-11-25 19:07:16.149776: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (4194304): 	Total Chunks: 1, Chunks in use: 0. 4.00MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
> 2018-11-25 19:07:16.149780: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (8388608): 	Total Chunks: 2, Chunks in use: 1. 18.71MiB allocated for chunks. 10.71MiB in use in bin. 10.71MiB client-requested in use in bin.
> 2018-11-25 19:07:16.149784: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (16777216): 	Total Chunks: 5, Chunks in use: 3. 101.54MiB allocated for chunks. 58.83MiB in use in bin. 53.54MiB client-requested in use in bin.
> 2018-11-25 19:07:16.149789: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (33554432): 	Total Chunks: 10, Chunks in use: 9. 448.83MiB allocated for chunks. 406.00MiB in use in bin. 385.49MiB client-requested in use in bin.
> 2018-11-25 19:07:16.149794: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (67108864): 	Total Chunks: 7, Chunks in use: 7. 577.49MiB allocated for chunks. 577.49MiB in use in bin. 513.98MiB client-requested in use in bin.
> 2018-11-25 19:07:16.149798: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (134217728): 	Total Chunks: 2, Chunks in use: 2. 339.68MiB allocated for chunks. 339.68MiB in use in bin. 171.33MiB client-requested in use in bin.
> 2018-11-25 19:07:16.149801: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (268435456): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
> 2018-11-25 19:07:16.149805: I tensorflow/core/common_runtime/bfc_allocator.cc:626] Bin for 85.66MiB was 64.00MiB, Chunk State: 
> 2018-11-25 19:07:16.149810: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c0000 of size 1280
> 2018-11-25 19:07:16.149813: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c0500 of size 256
> 2018-11-25 19:07:16.149815: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c0600 of size 256
> 2018-11-25 19:07:16.149817: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c0700 of size 256
> 2018-11-25 19:07:16.149820: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c0800 of size 256
> 2018-11-25 19:07:16.149822: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c0900 of size 256
> 2018-11-25 19:07:16.149825: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c0a00 of size 256
> 2018-11-25 19:07:16.149827: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c0b00 of size 256
> 2018-11-25 19:07:16.149830: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c0c00 of size 256
> 2018-11-25 19:07:16.149832: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c0d00 of size 256
> 2018-11-25 19:07:16.149835: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c0e00 of size 256
> 2018-11-25 19:07:16.149837: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c0f00 of size 256
> 2018-11-25 19:07:16.149840: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c1000 of size 256
> 2018-11-25 19:07:16.149842: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c1100 of size 256
> 2018-11-25 19:07:16.149845: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c1200 of size 256
> 2018-11-25 19:07:16.149847: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c1300 of size 256
> 2018-11-25 19:07:16.149849: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c1400 of size 256
> 2018-11-25 19:07:16.149852: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c1500 of size 256
> 2018-11-25 19:07:16.149854: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c1600 of size 256
> 2018-11-25 19:07:16.149857: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c1700 of size 256
> 2018-11-25 19:07:16.149859: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c1800 of size 256
> 2018-11-25 19:07:16.149861: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c1900 of size 256
> 2018-11-25 19:07:16.149864: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c1a00 of size 256
> 2018-11-25 19:07:16.149866: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c1b00 of size 256
> 2018-11-25 19:07:16.149868: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c1c00 of size 256
> 2018-11-25 19:07:16.149871: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c1d00 of size 256
> 2018-11-25 19:07:16.149874: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c1e00 of size 256
> 2018-11-25 19:07:16.149879: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c1f00 of size 256
> 2018-11-25 19:07:16.149882: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c2000 of size 256
> 2018-11-25 19:07:16.149886: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c2100 of size 256
> 2018-11-25 19:07:16.149890: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c2200 of size 256
> 2018-11-25 19:07:16.149895: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c2300 of size 256
> 2018-11-25 19:07:16.149899: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c2400 of size 256
> 2018-11-25 19:07:16.149903: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c2500 of size 256
> 2018-11-25 19:07:16.149907: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c2600 of size 256
> 2018-11-25 19:07:16.149911: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020c2700 of size 49152
> 2018-11-25 19:07:16.149915: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020ce700 of size 49152
> 2018-11-25 19:07:16.149918: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7020da700 of size 53760
> 2018-11-25 19:07:16.149920: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020e7900 of size 256
> 2018-11-25 19:07:16.149923: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020e7a00 of size 256
> 2018-11-25 19:07:16.149926: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020e7b00 of size 1536
> 2018-11-25 19:07:16.149928: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020e8100 of size 1536
> 2018-11-25 19:07:16.149931: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020e8700 of size 256
> 2018-11-25 19:07:16.149933: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020e8800 of size 6400
> 2018-11-25 19:07:16.149936: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020ea100 of size 256
> 2018-11-25 19:07:16.149938: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020ea200 of size 256
> 2018-11-25 19:07:16.149941: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020ea300 of size 24832
> 2018-11-25 19:07:16.149943: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f0400 of size 256
> 2018-11-25 19:07:16.149946: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f0500 of size 256
> 2018-11-25 19:07:16.149949: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f0600 of size 256
> 2018-11-25 19:07:16.149951: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f0700 of size 256
> 2018-11-25 19:07:16.149953: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f0800 of size 256
> 2018-11-25 19:07:16.149956: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f0900 of size 256
> 2018-11-25 19:07:16.149958: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f0a00 of size 256
> 2018-11-25 19:07:16.149961: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f0b00 of size 256
> 2018-11-25 19:07:16.149963: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f0c00 of size 256
> 2018-11-25 19:07:16.149965: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f0d00 of size 256
> 2018-11-25 19:07:16.149968: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f0e00 of size 6144
> 2018-11-25 19:07:16.149970: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7020f2600 of size 256
> 2018-11-25 19:07:16.149973: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f2700 of size 256
> 2018-11-25 19:07:16.149975: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f2800 of size 256
> 2018-11-25 19:07:16.149978: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f2900 of size 6144
> 2018-11-25 19:07:16.149980: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f4100 of size 256
> 2018-11-25 19:07:16.149983: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f4200 of size 256
> 2018-11-25 19:07:16.149985: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f4300 of size 256
> 2018-11-25 19:07:16.149988: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f4400 of size 256
> 2018-11-25 19:07:16.149990: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f4500 of size 256
> 2018-11-25 19:07:16.149992: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f4600 of size 256
> 2018-11-25 19:07:16.149995: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f4700 of size 256
> 2018-11-25 19:07:16.149997: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f4800 of size 256
> 2018-11-25 19:07:16.149999: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f4900 of size 256
> 2018-11-25 19:07:16.150002: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f4a00 of size 256
> 2018-11-25 19:07:16.150004: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f4b00 of size 256
> 2018-11-25 19:07:16.150007: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f4c00 of size 256
> 2018-11-25 19:07:16.150009: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f4d00 of size 256
> 2018-11-25 19:07:16.150011: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f4e00 of size 256
> 2018-11-25 19:07:16.150014: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f4f00 of size 256
> 2018-11-25 19:07:16.150016: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f5000 of size 256
> 2018-11-25 19:07:16.150019: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f5100 of size 1536
> 2018-11-25 19:07:16.150021: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f5700 of size 256
> 2018-11-25 19:07:16.150023: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f5800 of size 256
> 2018-11-25 19:07:16.150026: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f5900 of size 256
> 2018-11-25 19:07:16.150028: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f5a00 of size 256
> 2018-11-25 19:07:16.150030: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f5b00 of size 256
> 2018-11-25 19:07:16.150033: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f5c00 of size 256
> 2018-11-25 19:07:16.150035: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f5d00 of size 256
> 2018-11-25 19:07:16.150037: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f5e00 of size 256
> 2018-11-25 19:07:16.150040: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f5f00 of size 256
> 2018-11-25 19:07:16.150042: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f6000 of size 256
> 2018-11-25 19:07:16.150045: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f6100 of size 256
> 2018-11-25 19:07:16.150047: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f6200 of size 256
> 2018-11-25 19:07:16.150049: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f6300 of size 256
> 2018-11-25 19:07:16.150052: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f6400 of size 1536
> 2018-11-25 19:07:16.150054: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020f6a00 of size 24576
> 2018-11-25 19:07:16.150057: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020fca00 of size 6144
> 2018-11-25 19:07:16.150059: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020fe200 of size 256
> 2018-11-25 19:07:16.150062: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020fe300 of size 6144
> 2018-11-25 19:07:16.150064: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7020ffb00 of size 1536
> 2018-11-25 19:07:16.150067: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702100100 of size 256
> 2018-11-25 19:07:16.150069: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702100200 of size 256
> 2018-11-25 19:07:16.150072: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702100300 of size 256
> 2018-11-25 19:07:16.150074: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702100400 of size 24576
> 2018-11-25 19:07:16.150076: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702106400 of size 256
> 2018-11-25 19:07:16.150079: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702106500 of size 49152
> 2018-11-25 19:07:16.150081: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702112500 of size 256
> 2018-11-25 19:07:16.150084: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702112600 of size 256
> 2018-11-25 19:07:16.150086: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702112700 of size 256
> 2018-11-25 19:07:16.150088: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702112800 of size 256
> 2018-11-25 19:07:16.150091: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702112900 of size 256
> 2018-11-25 19:07:16.150093: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702112a00 of size 256
> 2018-11-25 19:07:16.150096: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702112b00 of size 256
> 2018-11-25 19:07:16.150098: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702112c00 of size 256
> 2018-11-25 19:07:16.150101: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702112d00 of size 256
> 2018-11-25 19:07:16.150103: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702112e00 of size 256
> 2018-11-25 19:07:16.150105: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702112f00 of size 1536
> 2018-11-25 19:07:16.150108: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702113500 of size 24576
> 2018-11-25 19:07:16.150110: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702119500 of size 6144
> 2018-11-25 19:07:16.150113: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70211ad00 of size 24576
> 2018-11-25 19:07:16.150115: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702120d00 of size 256
> 2018-11-25 19:07:16.150118: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702120e00 of size 256
> 2018-11-25 19:07:16.150120: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702120f00 of size 256
> 2018-11-25 19:07:16.150122: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702121000 of size 256
> 2018-11-25 19:07:16.150125: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702121100 of size 256
> 2018-11-25 19:07:16.150127: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702121200 of size 256
> 2018-11-25 19:07:16.150130: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x702121300 of size 29696
> 2018-11-25 19:07:16.150132: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702128700 of size 256
> 2018-11-25 19:07:16.150134: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702128800 of size 49408
> 2018-11-25 19:07:16.150137: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702134900 of size 51200
> 2018-11-25 19:07:16.150139: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702141100 of size 256
> 2018-11-25 19:07:16.150142: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702141200 of size 27904
> 2018-11-25 19:07:16.150145: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702147f00 of size 24576
> 2018-11-25 19:07:16.150147: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70214df00 of size 29184
> 2018-11-25 19:07:16.150150: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702155100 of size 1536
> 2018-11-25 19:07:16.150152: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702155700 of size 256
> 2018-11-25 19:07:16.150155: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702155800 of size 256
> 2018-11-25 19:07:16.150157: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702155900 of size 256
> 2018-11-25 19:07:16.150159: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702155a00 of size 256
> 2018-11-25 19:07:16.150162: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702155b00 of size 256
> 2018-11-25 19:07:16.150164: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702155c00 of size 256
> 2018-11-25 19:07:16.150167: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702155d00 of size 1792
> 2018-11-25 19:07:16.150169: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702156400 of size 256
> 2018-11-25 19:07:16.150171: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702156500 of size 1792
> 2018-11-25 19:07:16.150175: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702156c00 of size 256
> 2018-11-25 19:07:16.150179: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702156d00 of size 6144
> 2018-11-25 19:07:16.150183: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702158500 of size 10496
> 2018-11-25 19:07:16.150187: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70215ae00 of size 256
> 2018-11-25 19:07:16.150191: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70215af00 of size 256
> 2018-11-25 19:07:16.150195: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70215b000 of size 256
> 2018-11-25 19:07:16.150200: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70215b100 of size 256
> 2018-11-25 19:07:16.150204: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70215b200 of size 2816
> 2018-11-25 19:07:16.150208: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70215bd00 of size 7424
> 2018-11-25 19:07:16.150212: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70215da00 of size 49152
> 2018-11-25 19:07:16.150216: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702169a00 of size 24576
> 2018-11-25 19:07:16.150220: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70216fa00 of size 24576
> 2018-11-25 19:07:16.150225: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702175a00 of size 49152
> 2018-11-25 19:07:16.150229: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702181a00 of size 95488
> 2018-11-25 19:07:16.150233: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702198f00 of size 49152
> 2018-11-25 19:07:16.150237: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7021a4f00 of size 49152
> 2018-11-25 19:07:16.150241: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7021b0f00 of size 61696
> 2018-11-25 19:07:16.150246: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7021c0000 of size 49920
> 2018-11-25 19:07:16.150250: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7021cc300 of size 74496
> 2018-11-25 19:07:16.150255: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7021de600 of size 25600
> 2018-11-25 19:07:16.150259: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7021e4a00 of size 34048
> 2018-11-25 19:07:16.150263: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7021ecf00 of size 49408
> 2018-11-25 19:07:16.150267: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7021f9000 of size 69888
> 2018-11-25 19:07:16.150271: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70220a100 of size 256
> 2018-11-25 19:07:16.150274: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70220a200 of size 81920
> 2018-11-25 19:07:16.150278: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70221e200 of size 256
> 2018-11-25 19:07:16.150282: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70221e300 of size 256
> 2018-11-25 19:07:16.150286: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70221e400 of size 256
> 2018-11-25 19:07:16.150290: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70221e500 of size 256
> 2018-11-25 19:07:16.150294: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70221e600 of size 256
> 2018-11-25 19:07:16.150299: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70221e700 of size 256
> 2018-11-25 19:07:16.150302: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70221e800 of size 256
> 2018-11-25 19:07:16.150304: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70221e900 of size 256
> 2018-11-25 19:07:16.150307: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70221ea00 of size 256
> 2018-11-25 19:07:16.150309: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70221eb00 of size 256
> 2018-11-25 19:07:16.150311: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70221ec00 of size 256
> 2018-11-25 19:07:16.150314: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70221ed00 of size 256
> 2018-11-25 19:07:16.150316: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70221ee00 of size 256
> 2018-11-25 19:07:16.150319: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70221ef00 of size 256
> 2018-11-25 19:07:16.150321: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70221f000 of size 256
> 2018-11-25 19:07:16.150324: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70221f100 of size 256
> 2018-11-25 19:07:16.150326: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70221f200 of size 256
> 2018-11-25 19:07:16.150329: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70221f300 of size 256
> 2018-11-25 19:07:16.150331: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70221f400 of size 256
> 2018-11-25 19:07:16.150333: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70221f500 of size 256
> 2018-11-25 19:07:16.150336: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70221f600 of size 256
> 2018-11-25 19:07:16.150338: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70221f700 of size 256
> 2018-11-25 19:07:16.150340: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70221f800 of size 256
> 2018-11-25 19:07:16.150343: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70221f900 of size 256
> 2018-11-25 19:07:16.150345: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70221fa00 of size 256
> 2018-11-25 19:07:16.150347: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70221fb00 of size 256
> 2018-11-25 19:07:16.150350: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70221fc00 of size 256
> 2018-11-25 19:07:16.150352: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70221fd00 of size 256
> 2018-11-25 19:07:16.150354: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70221fe00 of size 256
> 2018-11-25 19:07:16.150357: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70221ff00 of size 256
> 2018-11-25 19:07:16.150359: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702220000 of size 2816
> 2018-11-25 19:07:16.150362: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702220b00 of size 256
> 2018-11-25 19:07:16.150364: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702220c00 of size 256
> 2018-11-25 19:07:16.150366: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702220d00 of size 256
> 2018-11-25 19:07:16.150369: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702220e00 of size 256
> 2018-11-25 19:07:16.150371: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702220f00 of size 256
> 2018-11-25 19:07:16.150373: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702221000 of size 256
> 2018-11-25 19:07:16.150376: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702221100 of size 256
> 2018-11-25 19:07:16.150378: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702221200 of size 256
> 2018-11-25 19:07:16.150380: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702221300 of size 256
> 2018-11-25 19:07:16.150383: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702221400 of size 256
> 2018-11-25 19:07:16.150385: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702221500 of size 256
> 2018-11-25 19:07:16.150388: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702221600 of size 256
> 2018-11-25 19:07:16.150390: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702221700 of size 256
> 2018-11-25 19:07:16.150392: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702221800 of size 256
> 2018-11-25 19:07:16.150395: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702221900 of size 2048
> 2018-11-25 19:07:16.150398: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702222100 of size 8448
> 2018-11-25 19:07:16.150400: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702224200 of size 6144
> 2018-11-25 19:07:16.150403: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702225a00 of size 7936
> 2018-11-25 19:07:16.150405: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x702227900 of size 256
> 2018-11-25 19:07:16.150407: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x702227a00 of size 1672704
> 2018-11-25 19:07:16.150410: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7028c0000 of size 4194304
> 2018-11-25 19:07:16.150412: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x702cc0000 of size 8388608
> 2018-11-25 19:07:16.150415: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7034c0000 of size 16777216
> 2018-11-25 19:07:16.150418: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7044c0000 of size 11228160
> 2018-11-25 19:07:16.150420: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x704f75400 of size 22326272
> 2018-11-25 19:07:16.150422: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7064c0000 of size 67108864
> 2018-11-25 19:07:16.150425: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70a4c0000 of size 44912640
> 2018-11-25 19:07:16.150428: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x70cf95000 of size 89305088
> 2018-11-25 19:07:16.150430: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7124c0000 of size 89825280
> 2018-11-25 19:07:16.150433: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x717a6a000 of size 178610176
> 2018-11-25 19:07:16.150435: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7224c0000 of size 44912640
> 2018-11-25 19:07:16.150438: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x724f95000 of size 44912640
> 2018-11-25 19:07:16.150440: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x727a6a000 of size 89825280
> 2018-11-25 19:07:16.150443: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x72d014000 of size 44912640
> 2018-11-25 19:07:16.150445: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x72fae9000 of size 44912640
> 2018-11-25 19:07:16.150447: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7325be000 of size 89825280
> 2018-11-25 19:07:16.150450: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x737b68000 of size 177569792
> 2018-11-25 19:07:16.150452: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7424c0000 of size 44912640
> 2018-11-25 19:07:16.150455: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x744f95000 of size 22456320
> 2018-11-25 19:07:16.150457: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7464ff800 of size 22456320
> 2018-11-25 19:07:16.150460: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x747a6a000 of size 89825280
> 2018-11-25 19:07:16.150462: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x74d014000 of size 89825280
> 2018-11-25 19:07:16.150465: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7525be000 of size 44912640
> 2018-11-25 19:07:16.150467: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x755093000 of size 22456320
> 2018-11-25 19:07:16.150470: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7565fd800 of size 44912640
> 2018-11-25 19:07:16.150472: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7590d2800 of size 44912640
> 2018-11-25 19:07:16.150474: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x75bba7800 of size 66422784
> 2018-11-25 19:07:16.150478: I tensorflow/core/common_runtime/bfc_allocator.cc:651]      Summary of in-use Chunks by size: 
> 2018-11-25 19:07:16.150485: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 161 Chunks of size 256 totalling 40.2KiB
> 2018-11-25 19:07:16.150491: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 1280 totalling 1.2KiB
> 2018-11-25 19:07:16.150497: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 7 Chunks of size 1536 totalling 10.5KiB
> 2018-11-25 19:07:16.150502: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 2 Chunks of size 1792 totalling 3.5KiB
> 2018-11-25 19:07:16.150507: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 2048 totalling 2.0KiB
> 2018-11-25 19:07:16.150512: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 2 Chunks of size 2816 totalling 5.5KiB
> 2018-11-25 19:07:16.150517: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 7 Chunks of size 6144 totalling 42.0KiB
> 2018-11-25 19:07:16.150522: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 6400 totalling 6.2KiB
> 2018-11-25 19:07:16.150527: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 7424 totalling 7.2KiB
> 2018-11-25 19:07:16.150533: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 7936 totalling 7.8KiB
> 2018-11-25 19:07:16.150538: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 8448 totalling 8.2KiB
> 2018-11-25 19:07:16.150543: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 10496 totalling 10.2KiB
> 2018-11-25 19:07:16.150548: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 7 Chunks of size 24576 totalling 168.0KiB
> 2018-11-25 19:07:16.150554: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 24832 totalling 24.2KiB
> 2018-11-25 19:07:16.150557: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 25600 totalling 25.0KiB
> 2018-11-25 19:07:16.150560: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 27904 totalling 27.2KiB
> 2018-11-25 19:07:16.150563: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 29184 totalling 28.5KiB
> 2018-11-25 19:07:16.150566: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 34048 totalling 33.2KiB
> 2018-11-25 19:07:16.150569: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 7 Chunks of size 49152 totalling 336.0KiB
> 2018-11-25 19:07:16.150572: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 2 Chunks of size 49408 totalling 96.5KiB
> 2018-11-25 19:07:16.150575: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 49920 totalling 48.8KiB
> 2018-11-25 19:07:16.150578: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 51200 totalling 50.0KiB
> 2018-11-25 19:07:16.150581: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 61696 totalling 60.2KiB
> 2018-11-25 19:07:16.150584: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 69888 totalling 68.2KiB
> 2018-11-25 19:07:16.150587: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 74496 totalling 72.8KiB
> 2018-11-25 19:07:16.150590: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 81920 totalling 80.0KiB
> 2018-11-25 19:07:16.150593: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 95488 totalling 93.2KiB
> 2018-11-25 19:07:16.150596: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 11228160 totalling 10.71MiB
> 2018-11-25 19:07:16.150599: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 16777216 totalling 16.00MiB
> 2018-11-25 19:07:16.150602: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 2 Chunks of size 22456320 totalling 42.83MiB
> 2018-11-25 19:07:16.150605: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 8 Chunks of size 44912640 totalling 342.66MiB
> 2018-11-25 19:07:16.150608: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 66422784 totalling 63.35MiB
> 2018-11-25 19:07:16.150611: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 67108864 totalling 64.00MiB
> 2018-11-25 19:07:16.150614: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 89305088 totalling 85.17MiB
> 2018-11-25 19:07:16.150618: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 5 Chunks of size 89825280 totalling 428.32MiB
> 2018-11-25 19:07:16.150621: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 177569792 totalling 169.34MiB
> 2018-11-25 19:07:16.150624: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 178610176 totalling 170.34MiB
> 2018-11-25 19:07:16.150627: I tensorflow/core/common_runtime/bfc_allocator.cc:658] Sum Total of in-use chunks: 1.36GiB
> 2018-11-25 19:07:16.150632: I tensorflow/core/common_runtime/bfc_allocator.cc:660] Stats: 
> Limit:                  1565786112
> InUse:                  1461751552
> MaxInUse:               1494473216
> NumAllocs:                  110326
> MaxAllocSize:            375537664
> 
> 2018-11-25 19:07:16.150646: W tensorflow/core/common_runtime/bfc_allocator.cc:275] ***_***********xx************xxxxx***************__************xxxxx*****_*************************x
> 2018-11-25 19:07:16.150663: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at image_resizer_state.h:115 : Resource exhausted: OOM when allocating tensor with shape[255,11008,1,8] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
> Traceback (most recent call last):
>   File ""/home/viktor/PycharmProjects/Autoencoder_Project/autio_autoencoder.py"", line 98, in <module>
>     model.fit_generator(my_gen, steps_per_epoch=100, epochs=400, verbose=1, callbacks=callbacks_list)
>   File ""/home/viktor/venv/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/training.py"", line 2065, in fit_generator
>     initial_epoch=initial_epoch)
>   File ""/home/viktor/venv/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/training_generator.py"", line 171, in fit_generator
>     x, y, sample_weight=sample_weight, class_weight=class_weight)
>   File ""/home/viktor/venv/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/training.py"", line 1828, in train_on_batch
>     outputs = self.train_function(ins)
>   File ""/home/viktor/venv/local/lib/python2.7/site-packages/tensorflow/python/keras/backend.py"", line 2978, in __call__
>     run_metadata=self.run_metadata)
>   File ""/home/viktor/venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1399, in __call__
>     run_metadata_ptr)
>   File ""/home/viktor/venv/local/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py"", line 526, in __exit__
>     c_api.TF_GetCode(self.status.status))
> tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[255,11008,1,8] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
> 	 [[{{node up_sampling2d_4/ResizeNearestNeighbor}} = ResizeNearestNeighbor[T=DT_FLOAT, _class=[""loc:@train...ighborGrad""], align_corners=false, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](activation_9/Relu-0-0-TransposeNCHWToNHWC-LayoutOptimizer, up_sampling2d_4/mul)]]
> Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
> 
> 	 [[{{node metrics/mean_squared_error/Mean_1/_215}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_1731_metrics/mean_squared_error/Mean_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
> Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
> 
> 
> Process finished with exit code 1
> 
"
23952,Tensorflow dense_image_warp Failed to convert object of type <type 'list'> to Tensor,"Using up-to-date Keras and Tensorflow r1.12 on Ubuntu 16.04 and 14.04. For the following code:
   
    img2D = Input(shape=(100, 100, 3))
    refPosX = Input(shape=(100, 100, 1))
    refPosY = Input(shape=(100, 100, 1))

    depth_map = depth_net(dFeatures)
    curX = tf.multiply(depth_map, refPosX)
    curY = tf.multiply(depth_map, refPosY)
    dMove = tf.concat([curX, curY], axis=3)

    warped = tfc.image.dense_image_warp(img2D, dMove)

I'm receiving the error output:

    Using TensorFlow backend.
    Traceback (most recent call last):
    File ""/home/carson/ws/dla/test_network_5/depth_and_color_nets.py"", line 89, in <module>
    File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/image/python/ops/dense_image_warp.py"", line 195, in dense_image_warp
    [batch_size, height * width, 2])
     File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 6482, in reshape
    ""Reshape"", tensor=tensor, shape=shape, name=name)
    File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 513, in _apply_op_helper
    raise err
    TypeError: Failed to convert object of type <type 'list'> to Tensor. Contents: [None, 10000, 2]. Consider casting elements to a supported type.

This seems to only be happening with this function, so unless I've missed something in the documentation, I'm not sure what's going on here :/"
23951,"During config, please provide an error or warning if too low of a computer capability is set.","During config, we can request the required computer capabilties to be compiled to. For the newer tensorflows >1.10 we can request using cuda computer capabilities 3.0, but after tensorflow is built it is not usable unless there is a compute capabilties >=3.5.

Seems like a warning would be appropriate especially considering the 4+ hours of compiling afterwards will succeed, but fail when actually attempting to use the built packages."
23950,tensorflow 1.10.0 build error,"when i use ""bazel build --config=opt --config=cuda //tensorflow:libtensorflow_cc.so"" to build tensorflow c++ API， I got this error: tensorflow/cc/BUILD:477:1: Linking of rule '//tensorflow/cc:ops/random_ops_gen_cc' failed (Exit 1)"
23949,Feature Request: Derivative-Free Optimization,"**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): Yes
Yes but I'm not sure exactly how to get it done right now.

**Describe the feature and the current behavior/state.**
Often we need to optimize dense nets without derivatives/gradients. It would be excellent to have some built-in derivative-free optimization tools!
https://en.wikipedia.org/wiki/Derivative-free_optimization

**Will this change the current api? How?**
Yes, we could use derivative free optimization without gradients, gradient tape.
There's one example in Tensorflow Probability but the docs aren't super clear on how to implement this:
https://github.com/tensorflow/probability/blob/master/tensorflow_probability/python/optimizer/nelder_mead.py

**Who will benefit with this feature?**
Users who want to do RL can use this if the objective function is difficult / impossible to differentiate, or other situations where gradient calculation doesn't work.

**Any Other info.**
Eager + tf.keras.Model support would be excellent"
23948,UpSampling1D takes long. UpSampling2D is fast.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): virtual env, pip
- TensorFlow version (use command below): **('v1.11.0-0-gc19e29306c', '1.11.0')**
- Python version: 2.7
- CUDA/cuDNN version: Yes, 9
- GPU model and memory: Yes, 2GB


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
**('v1.11.0-0-gc19e29306c', '1.11.0')**

**Describe the current behavior**
When I run this code, it is super slow. it takes minutes to generate the model
```
input1 = tf.keras.layers.Input(shape=(input_n,1))
 x_a = input1
for i in range(6):
    x_a = tf.keras.layers.Conv1D(8 * (2 ** i), (3), padding='same')(x_a)
    x_a = Activation('relu')(x_a)
    x_a = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same')(x_a)
latent = x_a
x_d = latent
for i in range(5,-1,-1):
    x_d = tf.keras.layers.Conv1D(8 * (2 ** i), (3), padding='same')(x_d)
    x_d = Activation('relu')(x_d)
    x_d = tf.keras.layers.UpSampling1D(size=2)(x_d)
decoded = x_d
model = tf.keras.models.Model(inputs=input1, outputs=decoded)
```

**Describe the expected behavior**
But the UpSampling2D is much faster
```
input1 = tf.keras.layers.Input(shape=(input_n,1))
 x_a = input1
for i in range(6):
    x_a = tf.keras.layers.Conv2D(8 * (2 ** i), (3), padding='same')(x_a)
    x_a = Activation('relu')(x_a)
    x_a = tf.keras.layers.MaxPooling2D(pool_size=2, padding='same')(x_a)
latent = x_a
x_d = latent
for i in range(5,-1,-1):
    x_d = tf.keras.layers.Conv2D(8 * (2 ** i), (3), padding='same')(x_d)
    x_d = Activation('relu')(x_d)
    x_d = tf.keras.layers.UpSampling2D(size=2)(x_d)
decoded = x_d
model = tf.keras.models.Model(inputs=input1, outputs=decoded)
```

I tested every component. The reason is UpSampling1D
"
23947,Network Bandwidth Un-expected Behavior ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution: Linux Ubuntu
- TensorFlow installed from (source or binary): Yes
- TensorFlow version (use command below): 1.11.0
- Python version: 3.5
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version: None
- GPU model and memory: No GPU only CPU.


I have a simple model 2.8 MB. I wrote a distributed Tensorflow code with one parameter server and one worker. I measured the network bandwidth for in and out from worker.
I found that worker utilized bandwidth-in is 53 MB and worker utilized bandwidth-out is 26 MB. 
My dataset is MNIST dataset which is 46 MB. 
The total worker and server bandwidth are 92.5 MB.

My question is that I thought I will see only the size of my model in and out from to worker but where these 53, 26 data come from.

This is my code:

https://github.com/salemmohammed/TensorflowDistributedExample/blob/master/PS_Dist1.py

"
23946,[BUG] `accumulate_n` fatal  in `InteractiveSession`,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary, Anaconda
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.2/7.2.1
- GPU model and memory: GTX 1050 mobile, 4GB

**Describe the current behavior**
Failure when using `accumulate_n` (`add_n` works) with tensors (also with `tf.constant(array_here)`) with an `InteractiveSession` (works for normal session).

**Describe the expected behavior**
Same result as `add_n` (and no error)
**Code to reproduce the issue**
```
import tensorflow as tf

t1 = tf.constant(1.)
t2 = tf.constant(42.)
tensor_list = [t1, t2]
add_n_op = tf.add_n(tensor_list)   # for comparison only
accumulate_n_op = tf.accumulate_n(tensor_list) 
sess = tf.InteractiveSession()
# sess = tf.Session()  # this would work
print(sess.run(add_n_op))  # works (print to show it works)
print(sess.run(accumulate_n_op))  # does not work
```
**Other info / logs**
Error message:  
2018-11-24 11:11:27.877871: F tensorflow/compiler/jit/deadness_analysis.cc:639] Check failed: it != predicate_map_.end() AccumulateNV2/Internal/_3
"
23945,Automatic new type of layer creation mechanism for TensorFlow,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

IDEA: I want to offer you a method that put production of a new type of layers on an industrial scale.
STATUS: Idea came to my mind I am thinking about implementation and I want to know your opinion.


**Will this change the current api? How?**

API may change as to provide compatibility to a new feature.

**Who will benefit with this feature?**

APPLICATION:
1. Creation of new type of layers suited for particular industries and application.
2. Research work can be enhanced. We are turning a neural network into the classic mathematical language.

**Any Other info.**

PROBLEM: How to turn a neural network in mathematical formula suited for a creation of an absolutely new type of layer for TensorFlow. It might be used in TensorFlow, Keras,  PyTorch or anything. 

HOW: 
1, You trained your network for particular industry application and invested a lot of computational power to make it work. And this neural network work for solving a particular problem in a particular industry very well.
2. You freeze all layers in your neural network.
3. Let's suppose we have an MSE loss for a simplicity.
4. Then you start to training target(label) looking for INTEGRAL of a function of your neural network. It will produce a mathematical formula describing a neural network. Let's call ANTI GRAD algorithm.
5. After it's done you have a mathematical formula that can easily be turned into a new type of layer for particular industry application with a simple(you can tune simplicity) formula for Chemistry, Economics, Medicine.

COMMENTS:
It would require a team of programmers to make it work. So, I am looking for new opportunities and the team to make it happen.


"
23944,"Installation with python 2.7 on MacOS,  attribute __doc__ not writable","As shown in title, I was compiling from source with python 2.7 on MacOS, and found failure described as ""AttributeError: attribute '__doc__' of 'type' objects is not writable""

""* `ONLY_FIRST_TOWER`: Deprecated alias for `ONLY_FIRST_REPLICA'.\n""

Then I changed this file ""variables.py"" and recompile only to found the same bug in other files."
23943,Build Options for Older CPU w/o AVX,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04
- TensorFlow installed from (source or binary):
source
- TensorFlow version:
tensorflow-1.12.0rc0
- Python version:
3.6
- Installed using virtualenv? pip? conda?:
conda
- Bazel version (if compiling from source):
0.17.2
- GCC/Compiler version (if compiling from source):
gcc version 5.4.0
- CUDA/cuDNN version:
no
- GPU model and memory:
no

**Describe the problem**
I need to turn off AVX during build. However, the tf_configure.bazelrc doesn't have an explicit AVX flag. Since the build takes hours to complete, could you tell me what the flag is? Here is my tf_configure.bazelrc:

import %workspace%/tools/bazel.rc
build --action_env PYTHON_BIN_PATH=""/home/me/anaconda3/bin/python""
build --action_env PYTHON_LIB_PATH=""/home/me/anaconda3/lib/python3.6/site-packages""
build --python_path=""/home/me/anaconda3/bin/python""
build:xla --define with_xla_support=true
build --action_env TF_NEED_OPENCL_SYCL=""0""
build --action_env TF_NEED_ROCM=""0""
build --action_env TF_NEED_CUDA=""0""
build --action_env TF_DOWNLOAD_CLANG=""0""
build:opt --copt=-march=native
build:opt --copt=-Wno-sign-compare
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
build:v2 --define=tf_api_version=2


**Provide the exact sequence of commands / steps that you executed before running into the problem**

2018-11-23 16:35:33.993987: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.
Aborted (core dumped)


"
23942,Question about example programs,"Hi, I just started using the regression portion of tensorflow. However, I can not find a program that creates a multi variable regression MODEL and a programs that uses that MODEL to make a prediction.
Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Window 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:NONE
- **TensorFlow installed from (source or binary)**:Binary(installed with pip, non gpu)
- **TensorFlow version (use command below)**: 1.12
- **Python version**:3.5.2
- **Bazel version (if compiling from source)**:none
- **GCC/Compiler version (if compiling from source)**:none
- **CUDA/cuDNN version**:none
- **GPU model and memory**: intel hd(can not be used with tensorflow, irrevelant)
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
23941,Tensorflow Linux Crash,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Both, have tried using stock code and non-stock code.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): I believe it was binary, installed it a while ago
- TensorFlow version (use command below):  ('v1.7.0-3-g024aecf414', '1.7.0')
- Python version: Python 2.7.12
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: CUDA = CUDA Version 9.0.176, cuDNN = Not sure, I do not see cudnn.h in the ""/usr/local/cuda/include/cudnn.h"" directory
- GPU model and memory: Titan Xp, 12GB

**Describe the current behavior**
A while back (~6 months ago) we got a new computer with a Titan Xp GPU.  I installed Tensorflow and CUDA/cuDNN per the instructions on the Tensorflow website.  I was running into some strange behavior where running code with Tensorflow would sometimes work OK with no issue, but sometimes it would cause the entire computer to crash, where the only way to recover was through a hard reset.  I am not so experienced with Tensorflow/Linux/GPU computing, this is all relatively new to me.  So I tried wiping the computer and starting over from scratch to repeat the procedure from the beginning, and was still seeing the same behavior.  There is another GPU computer that I've been able to use that has not had this issue, so I put troubleshooting this issue onto the ""backburner"" and have not looked at it for a couple months - so I am trying to get back to resolving this.

Note that there seems to be on correlation to what code causes this, it will happen with either a stock example or a custom code written for our data.  There is code that will run OK on a different machine with GPU, but will sometimes cause this behavior on this machine without changing the code.  Note that this seems to only occur when running Python+Tensorflow on the GPU, it makes me think somehow related to the GPU - maybe some driver was not configured correctly?

**Describe the expected behavior**
Expecting the machine to run without crashing, would expect it to give an error and stop running code rather than the entire machine crashing.

**Code to reproduce the issue**
Seems like running almost any code using Tensorflow will cause this behavior, does not correlated with certain code.

**Other info / logs**
Please let me know what other information would be helpful.
"
23940,"tflite - Tensorflow lite : Output tensor dimension are not changeable - Adding/removing labels(classes) gives a  crash like Cannot copy between a TensorFlowLite tensor with shape [1, 1001] and a Java object with shape [1, 1000].","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23939, fatal error: cuda/include/cublas_v2.h: No such file or directory,"Following instructions building tensorflow from source by using bazel, I got error when 
 
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package  --define framework_shared_object=false tensorflow:libtensorflow_cc.so --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0""


**System information**
- Linux Ubuntu 16.04
- TensorFlow installed from source
- TensorFlow version: r1.8
- Python version: 3.5
- Installed using virtualenv :
- Bazel version (if compiling from source): 0.18.1
- GCC/Compiler version (if compiling from source): 5.4
- CUDA/cuDNN version: 9.0/7.0
- GPU model and memory: TITAN XP 


my configuration file:
build --action_env PYTHON_BIN_PATH=""/home/kx/segmappyenv/bin/python""
build --action_env PYTHON_LIB_PATH=""/home/kx/segmappyenv/lib/python3.5/site-packages""
build --force_python=py3
build --host_force_python=py3
build --python_path=""/home/kx/segmappyenv/bin/python""
build --define with_jemalloc=true
build --define with_gcp_support=true
build --define with_hdfs_support=true
build --define with_s3_support=true
build --define with_kafka_support=true
build:xla --define with_xla_support=true
build:gdr --define with_gdr_support=true
build:verbs --define with_verbs_support=true
build --action_env TF_NEED_OPENCL_SYCL=""0""
build --action_env TF_NEED_CUDA=""1""
build --action_env CUDA_TOOLKIT_PATH=""/usr/local/cuda-9.0""
build --action_env TF_CUDA_VERSION=""9.0""
build --action_env CUDNN_INSTALL_PATH=""/usr/local/cuda-9.0""
build --action_env TF_CUDNN_VERSION=""7""
build --action_env TF_NCCL_VERSION=""1""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""6.1""
build --action_env LD_LIBRARY_PATH=""/opt/ros/kinetic/lib:/opt/ros/kinetic/lib/x86_64-linux-gnu:/usr/local/cuda-9.0/lib64:/usr/local/cuda-9.0/extras/CUPTI/lib64""
build --action_env TF_CUDA_CLANG=""0""
build --action_env GCC_HOST_COMPILER_PATH=""/usr/bin/gcc""
build --config=cuda
test --config=cuda
build --define grpc_no_ares=true
build:opt --copt=-march=native
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
build --copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK
build --host_copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK

**Any other info / logs**
build logs:


INFO: From Compiling tensorflow/core/distributed_runtime/master.cc [for host]:
In file included from ./tensorflow/core/platform/default/logging.h:24:0,
                 from ./tensorflow/core/platform/logging.h:25,
                 from ./tensorflow/core/lib/core/status.h:25,
                 from ./tensorflow/core/framework/variant.h:29,
                 from ./tensorflow/core/framework/allocator.h:26,
                 from ./tensorflow/core/common_runtime/device.h:35,
                 from ./tensorflow/core/distributed_runtime/master.h:21,
                 from tensorflow/core/distributed_runtime/master.cc:32:
./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetTensorDim(tensorflow::gtl::ArraySlice<T>, tensorflow::TensorFormat, char) [with T = long long int]':
./tensorflow/core/util/tensor_format.h:372:47:   required from here
./tensorflow/core/util/tensor_format.h:340:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(index >= 0 && index < dimension_attributes.size())
                             ^
./tensorflow/core/platform/macros.h:82:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/core/util/tensor_format.h:340:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attributes.size())
   ^
./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int]':
./tensorflow/core/util/tensor_format.h:381:54:   required from here
./tensorflow/core/util/tensor_format.h:355:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(index >= 0 && index < dimension_attribute.size())
                             ^
./tensorflow/core/platform/macros.h:82:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/core/util/tensor_format.h:355:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attribute.size())
   ^
ERROR: /home/kx/segmappyenv/tensorflow/tensorflow/core/kernels/BUILD:2547:1: C++ compilation of rule '//tensorflow/core/kernels:self_adjoint_eig_v2_op' failed (Exit 1)
In file included from tensorflow/core/kernels/self_adjoint_eig_v2_op_gpu.cc:29:0:
./tensorflow/core/kernels/cuda_solvers.h:26:36: fatal error: cuda/include/cublas_v2.h: No such file or directory
compilation terminated.
INFO: Elapsed time: 383.691s, Critical Path: 52.31s
INFO: 2759 processes: 2759 local.
FAILED: Build did NOT complete successfully
"
23938,docker cannot open web,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

sudo nvidia-docker run -it -p 8888:8888 tensorflow/tensorflow:1.12.0-devel-gpu-py3
root@82bdd6461185:~# exit

the chrome is not open jupyter notebook 

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23937,"TF-lite not supported Abs, ResizeNearestNeighbor？？","super@super-virtual-machine:~/Downloads/Onet-TF_to_caffe$ tflite_convert \
>   --output_file=/home/super/Downloads/Onet-TF_to_caffe/align.tflite \
>   --graph_def_file=/home/super/Downloads/Onet-TF_to_caffe/frozen_model.pb \
>   --input_arrays=alignmentnet/inputs \
>   --output_arrays=alignmentnet/add
2018-11-23 18:02:52.499590: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Traceback (most recent call last):
  File ""/home/super/.local/bin/tflite_convert"", line 11, in <module>
    sys.exit(main())
  File ""/home/super/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 412, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/home/super/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/home/super/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 408, in run_main
    _convert_model(tflite_flags)
  File ""/home/super/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 162, in _convert_model
    output_data = converter.convert()
  File ""/home/super/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/lite.py"", line 453, in convert
    **converter_kwargs)
  File ""/home/super/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py"", line 342, in toco_convert_impl
    input_data.SerializeToString())
  File ""/home/super/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py"", line 135, in toco_convert_protos
    (stdout, stderr))
RuntimeError: TOCO failed see console for info.
2018-11-23 18:02:54.114501: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Abs
2018-11-23 18:02:54.114680: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Abs
2018-11-23 18:02:54.114763: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Abs
2018-11-23 18:02:54.114819: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Abs
2018-11-23 18:02:54.114875: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Abs
2018-11-23 18:02:54.114933: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Abs
2018-11-23 18:02:54.115020: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Abs
2018-11-23 18:02:54.115154: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Abs
2018-11-23 18:02:54.115213: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ResizeNearestNeighbor
2018-11-23 18:02:54.115265: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Abs
2018-11-23 18:02:54.115308: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ResizeNearestNeighbor
2018-11-23 18:02:54.115341: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Abs
2018-11-23 18:02:54.115377: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Abs
2018-11-23 18:02:54.123507: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Abs
2018-11-23 18:02:54.125172: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 162 operators, 227 arrays (0 quantized)
2018-11-23 18:02:54.126147: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 162 operators, 227 arrays (0 quantized)
2018-11-23 18:02:54.128320: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 101 operators, 163 arrays (0 quantized)
2018-11-23 18:02:54.149382: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 100 operators, 162 arrays (0 quantized)
2018-11-23 18:02:54.150311: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 99 operators, 160 arrays (0 quantized)
2018-11-23 18:02:54.151169: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 99 operators, 160 arrays (0 quantized)
2018-11-23 18:02:54.152134: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 2032128 bytes, theoretical optimal value: 2032128 bytes.
2018-11-23 18:02:54.152627: F tensorflow/contrib/lite/toco/tflite/export.cc:386] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.contrib.lite.TFLiteConverter(). Here is a list of operators for which  you will need custom implementations: Abs, ResizeNearestNeighbor.
Aborted (core dumped)
"
23935,Can only utilize GPU after being run as root,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 10 Buster Fresh Install (Testing)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6/2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 9.0.176 / cuDNN 9.0
- GPU model and memory: GeForce GTX 1060 6GB

**What is happening**
When the system is first booted Tensorflow is not able to recognize the GPU instead returning `CUDA_ERROR_UNKNOWN: unknown error` but after running a version of Tensorflow as root all other instances start working.

**Describe the expected behavior**
In the example below the first instance of Tensorflow in the virtualenv should have been able to see the GPU. 

**Code to reproduce the issue**
Below is edited/commented version of an experiment I ran showcasing this bug. I have removed most of the verbose logging for brevity but the original can be found [here](https://pastebin.com/B7vHcyjn).
```bash
# Activate Virtualenv with Tensorflow 1.12.0
kyle@Debian:~/Code/ML$ source env/bin/activate
# Export the paths to CUDA
(env) kyle@Debian:~/Code/ML$ export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64""
(env) kyle@Debian:~/Code/ML$ export CUDA_HOME=/usr/local/cuda
# Launch Python Interpreter
(env) kyle@Debian:~/Code/ML$ python
Python 3.6.7 (default, Oct 21 2018, 08:08:16) 
# Import the TensorFlow installed in Virtualenv
>>> import tensorflow as tf
# Test for GPUs
>>> tf.test.is_gpu_available()
# None are found
False
# Let's exit the Python interpreter
>>> exit()
# and deactivate the Virtualenv
(env) kyle@Debian:~/Code/ML$ deactivate

# Login as superuser
kyle@Debian:~/Code/ML$ sudo su
[sudo] password for kyle: 
# Export paths to CUDA (again)
root@Debian:/home/kyle/Code/ML# export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64""
root@Debian:/home/kyle/Code/ML# export CUDA_HOME=/usr/local/cuda
# Launch Global Python Interpreter (as root)
root@Debian:/home/kyle/Code/ML# python
Python 2.7.15+ (default, Aug 31 2018, 11:56:52) 
# Import a global install of TensorFlow 1.12.0
>>> import tensorflow as tf
# Test for GPUs
>>> tf.test.is_gpu_available()
# It found one! :D
True
# Exit Python interpreter
>>> exit()
# Exit root
root@Debian:/home/kyle/Code/ML# exit

# We are now logged in as user 'kyle'
# Activate the same virtualenv as before
kyle@Debian:~/Code/ML$ source env/bin/activate
# Enter the Virtualenv's Python interpreter
(env) kyle@Debian:~/Code/ML$ python
Python 3.6.7 (default, Oct 21 2018, 08:08:16) 
# Import Tensorflow from within Virtualenv
>>> import tensorflow as tf
# Test for GPUs
>>> tf.test.is_gpu_available()
# Now it found one? wtf.
True
```

**Other info / logs**
The package `nvidia-modprobe` is already installed. 
Nvidia Driver: 390.87
```bash
kyle@Debian:~$ nvidia-smi
Fri Nov 23 02:46:11 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 390.87                 Driver Version: 390.87                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 106...  Off  | 00000000:01:00.0  On |                  N/A |
|  3%   49C    P8     9W / 120W |    275MiB /  6070MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1007      G   /usr/lib/xorg/Xorg                           152MiB |
|    0      1485      G   /usr/bin/kwin_x11                             36MiB |
|    0      1489      G   /usr/bin/krunner                               1MiB |
|    0      1491      G   /usr/bin/plasmashell                          30MiB |
|    0      2729      G   ...quest-channel-token=5720462904883144478    51MiB |
+-----------------------------------------------------------------------------+
```"
23934,transform_graph build  error,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):Source
- TensorFlow version: Github Master version
- Python version:2.7
- Installed using virtualenv? pip? conda?: None. 
- Bazel version (if compiling from source): 0.18.1
- GCC/Compiler version (if compiling from source):7.3.0
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
Install transform_graph tools from the source using Bazel on my VMware VM Ubuntu 18.04
**Provide the exact sequence of commands / steps that you executed before running into the problem**
cd ~/tensorflow
bazel build tensorflow/tools/graph_transforms:transform_graph

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
And i got random error such as this:
ERROR: /home/zpluo/tensorflow/tensorflow/core/kernels/BUILD:2829:1: C++ compilation of rule '//tensorflow/core/kernels:matrix_square_root_op' failed (Exit 4)
gcc: internal compiler error: Killed (program cc1plus)

PS:Sometime it's the other file can't be built."
23933,38c9b12 stucks gdr in ResNet50_v1.5 official benchmark,"With 38c9b12464b23aac132fbc8005cb74de86eee241, grpc+gdr server protocol gets stuck in ResNet50_v1.5 official benchmark. 

This issue is a marker to other users of GDR. Hopefully I could find some time to fix it next week."
23932,lite.TFLiteConverter.from_session not supported constant shape placeholder as input,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

System information
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
1.12.0
- Python version:
Python 3.6.5 :: Anaconda, Inc.
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
When I tried to convert scalar shape placeholder tensor( shape = () ) as an input, converter triggers ""IndexError: list index( shape list idx ) out of range""

**Describe the expected behavior**
I think converter should be support scalar shape placeholder, because TF users usually need scalar shape placeholder when they set dynamic params (hyper params, dropout, ect...) in neural network.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

--------------------------------------
 import tensorflow as tf
 import tensorflow.contrib.lite as lite

 scalar_input = tf.placeholder(shape=(), name='input', dtype=tf.float32)
 b = tf.constant(1.0)
 val = scalar_input + b
 out = tf.identity(val, name=""out"")

 with tf.Session() as sess:
     converter = lite.TFLiteConverter.from_session(sess, [scalar_input], [out])
     tflite_model = converter.convert()
     open(""converted_model.tflite"", ""wb"").write(tflite_model)

---------------------------------------
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23931,Flatten Op is nil when accessed via exported graph,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
1.12.0
- Python version:
Python 3.6.5 :: Anaconda, Inc.
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
v1.12.0-rc2-3-ga6d8ffae09 1.12.0

**Describe the current behavior**
My use case is the build the graph using Python interface for TF and export that graph as a pb file. This graph is then imported in Go environment using Go interface for TF. It seems tf.layers.flatten is inaccessible using the name, when name is provided as part of tf.layers.flatten's argument.

However, assigning name to it using tf.identity works. Pl. see code below.
**Describe the expected behavior**
`tf.layers.flatten(my_input, name=""flatten"")` should allow access to flatten operation
from the exported graph using name `flatten` and should not require naming it separately.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Below is python code to export the graph:
```
import tensorflow as tf

print(tf.__version__)

my_input = tf.placeholder(dtype=tf.uint8, shape=[1, None, None, 3], name=""myInput"")

tf.layers.flatten(my_input, name=""flatten1"")

f = tf.layers.flatten(my_input)
tf.identity(f, name=""flatten2"")

# finally save the graph to be used in Go code
graph = tf.Session().graph_def
tf.io.write_graph(graph, ""./"", ""flatten.pb"", as_text=False)
```
Below is Go code to access flatten operation:
```
package main

import (
	""fmt""
	""io/ioutil""
	""log""

	tf ""github.com/tensorflow/tensorflow/tensorflow/go""
)

func main() {
	def, err := ioutil.ReadFile(""flatten.pb"")
	if err != nil {
		log.Fatal(err)
	}

	graph := tf.NewGraph()
	if err := graph.Import(def, """"); err != nil {
		log.Fatal(err)
	}

	fmt.Println(""Access via name=flatten1 fetched a nil?:"", graph.Operation(""flatten1"") == nil)
	fmt.Println(""Access via name=flatten2 fetched a nil?:"", graph.Operation(""flatten2"") == nil)
}
```
Executing above produces following output:
```
$ go run flatten.go
Access via name=flatten1 fetched a nil?: true
Access via name=flatten2 fetched a nil?: false
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23930,Failed to load the native TensorFlow runtime.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>


Hi!
I worked with Tensorflow on without GPU. No I'm trying to run it on a device with following specs.
 I have get the follow error message, how I can solve it?

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 Home
- Mobile device : ThinkPad X1 Yoga 1st, Intel(R)Core(TM)i7-6500U CPU@2.50GHz 2.59GHz
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.9.0
- Python version: 3.7.0

**Describe the problem**

ImportError: Traceback (most recent call last):
  File ""C:\Users\y_pas\python\Anaconda3\envs\tensor\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\y_pas\python\Anaconda3\envs\tensor\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\y_pas\python\Anaconda3\envs\tensor\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\y_pas\python\Anaconda3\envs\tensor\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\y_pas\python\Anaconda3\envs\tensor\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 指定されたモジュールが見つかりません。


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.



"
23928,C++ compilation of rule '//tensorflow/python:cost_analyzer_lib' failed (Exit 2): msvc_wrapper_for_nvcc.bat failed: error executing command,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10 Professional
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
Trying to compile to CUDA 10
- **TensorFlow version (use command below)**:
1.12
- **Python version**:
3.6 64-bits
- **Bazel version (if compiling from source)**:
0.19.2
- **GCC/Compiler version (if compiling from source)**:
Microsoft (R) C/C++ Optimizing Compiler Version 19.16.27024.1 for x86
- **CUDA/cuDNN version**:
CUDA 10
- **GPU model and memory**:
GeForce GTX 1080 Ti 
11 GB GDDR5X
- **Exact command to reproduce**:
When buiilding from source:
bazel --output_base=C:\Temp\finalbuild build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
After many atempts to compile this for use with CUDA 10 I get this error in the end of a really long compile. Tried everything but I still get this annoying error.
I use VS 2017 Enterprise and CUDA 10

### Source code / logs
ERROR: C:/temp/tensorflow/tensorflow/python/BUILD:247:1: C++ compilation of rule '//tensorflow/python:cost_analyzer_lib' failed (Exit 2): msvc_wrapper_for_nvcc.bat failed: error executing command
  cd C:/temp/finalbuild/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\include;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\lib\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.16.27023\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\MSBuild\15.0\bin\Roslyn;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;C:\Program Files (x86)\Windows Kits\10\bin\10.0.17763.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\\MSBuild\15.0\bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\Common7\Tools\;;C:\WINDOWS\system32;C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Program Files (x86)/Microsoft Visual Studio/Shared/Python36_64/python.exe
    SET PYTHON_LIB_PATH=C:/Users/<USERNAME>/AppData/Roaming/Python/Python36/site-packages
    SET TEMP=C:\Users\<USERNAME>~1\AppData\Local\Temp
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1
    SET TF_CUDA_VERSION=10.0
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
    SET TMP=C:\Users\<USERNAME>~1\AppData\Local\Temp


Driver Version | 416.94
-- | --
Driver Model | WDDM
CUDA Device Index | 0
GPU Family | GP102-A
Compute Capability | 6.1
Number of SMs | 28
Frame Buffer Physical Size (MB) | 11264
Frame Buffer Bandwidth (GB/s) | 484,44
Frame Buffer Bus Width (bits) | 352
Frame Buffer Location | Dedicated
Graphics Clock (Mhz) | 803
Memory Clock (Mhz) | 5505
Processor Clock (Mhz) | 1607
RAM Type | GDDR5X
Attached Monitors | 2


Name | AMD Ryzen Threadripper 1950X 16-Core Processor
-- | --
Architecture | x64
Frequency | 3 750 MHz
Number of Cores | 32
Page Size | 4 096
Total Physical Memory | 16 258,00 MB
Available Physical Memory | 11 372,00 MB
Hybrid Graphics Enabled | False
Version Name | Windows 10 Pro
Version Number | 10.0.17134
Nsight Version | 6.0.0.18227
Nsight Edition | Standard
Visual Studio Version | 15.0


"
23926,Makefile for TFLite under Linux,"
**System information**
- Linux Ubuntu 16.04
- TensorFlow Lite compilation from source
- TensorFlow version 1.12
- g++ 5.4.0
- GPU / CUDA/cuDNN version - not used

I came across interesting thing in implementation of Makefile for Linux version of TFLite. In particular:
list of compiled sources does not include directory 'tensorflow/lite/core', only some sub-directories from it. This make the compiled 'libtensorflow-lite.a' binary unusable - linker always gives error even on the simplest examples, as e.g. give on this page - https://www.tensorflow.org/lite/apis

By adding line 
$(wildcard tensorflow/lite/core/*.cc) \
I managed to fix this - newly compiled binary now contained all symbols that were previously absent in the binary.

My question is - is this made on purpose for some reason that I've missed or this is just a bug? Could someone from the developers give better explanation? Thanks! 
"
23925,libtensorflow_framework.so: undefined symbol: cuDevicePrimaryCtxGetState???,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
23924,Memory leak when using tf.contrib.data.unbatch(),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0
- Python version: Python 3.6.7 :: Anaconda, Inc.
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.0
- GPU model and memory: 1080ti

**Describe the current behavior**
Memory usage continuously increase when using `tf.contrib.data.unbatch()`.

**Describe the expected behavior**
Memory usage should not increase.

**Code to reproduce the issue**

```
from absl import app
from absl import flags
from absl import logging
import tensorflow as tf


FLAGS = flags.FLAGS
flags.DEFINE_integer('epochs', 1000, '')
flags.DEFINE_boolean('use_unbatch', False, '')


def create_dataset(input_holder):
    dataset = tf.data.Dataset.from_tensor_slices((input_holder,))

    def generate_random_tensor(size):
        return tf.random_uniform([5, size, size], dtype=tf.float32)

    dataset = dataset.map(generate_random_tensor)
    if FLAGS.use_unbatch:
        dataset = dataset.apply(tf.contrib.data.unbatch())
    else:
        dataset = dataset.flat_map(
            lambda x: tf.data.Dataset.from_tensor_slices((x,)))
        # The output of the dataset becomes a single-element tuple w/o this.
        dataset = dataset.map(lambda x: x)
    return dataset


def main(_):
    with tf.Session() as sess:
        size_holder = tf.placeholder(tf.int32, shape=[None])
        dataset = create_dataset(size_holder)

        iterator = dataset.make_initializable_iterator()
        get_next = iterator.get_next()

        for i in range(FLAGS.epochs):
            logging.info('Epoch #%d', i)
            sess.run(iterator.initializer, feed_dict={
                size_holder: [1000 + (i % 100)],
            })
            try:
                while True:
                    array = sess.run(get_next)
                    logging.info('  Generated: %s', array.shape)
            except tf.errors.OutOfRangeError:
                pass


if __name__ == '__main__':
    app.run(main)
```

Memory usage will increase with `--use_unbatch`, while with `--nouse_unbatch`, memory usage does not increase.

**Other info / logs**

It seems like `input_->Unref()` call is missing in [`UnbatchDatasetOp`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/unbatch_dataset_op.cc#L42)."
23923,Inconsistant naming in Estimator documentation,"- Doc Link: https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator

In the section for `export_savedmodel`, it mentioned `export_to_savedmodel` which seems should be `export_savedmodel`. Maybe worth a fix."
23922,Tensorflow uses only one from eight gpus,"Hey,
i have written an neural chatbot with tensorflow. But my problem is, that TF only uses one GPU.
I am training on an NVIDIA DGX Station with 8x Tesla V100, so I want to use alle memory and cores.
Where is my mistake in the code? Who could I solve my problem?

I would be very thankful about an answer! :D


My code:


import time

import data
import click
import numpy as np
import tensorflow as tf
import tensorlayer as tl

from tqdm import tqdm
from sklearn.utils import shuffle
from tensorlayer.layers import DenseLayer, EmbeddingInputlayer, Seq2Seq, retrieve_seq_length_op2

sessionConfig = tf.ConfigProto(allow_soft_placement = True, log_device_placement = False)

@click.command()
@click.option('-dc', '--data-corpus', default = 'twitter')
@click.option('-bs', '--batch-size', default = 32)
@click.option('-e', '--epochs', default = 100000)
@click.option('-lr', '--learning-rate', default = 0.001)
@click.option('-m', '--mode', is_flag = True)
def train(data_corpus, batch_size, epochs, learning_rate, mode):

    metadata, trainX, trainY, testX, testY, validX, validY = setup(data_corpus)

    srcLength = len(trainX)
    targetLength = len(trainY)

    assert srcLength == targetLength

    nStep = srcLength // batch_size
    srcVocabSize = len(metadata['index2Word'])
    embeddingDim = 1024

    word2Index = metadata['word2Index']  
    index2Word = metadata['index2Word']  

    unk_id = word2Index['unk']  
    pad_id = word2Index['_']    

    start_id = srcVocabSize  
    end_id = srcVocabSize + 1 

    word2Index.update({'start_id': start_id})
    word2Index.update({'end_id': end_id})
    index2Word = index2Word + ['start_id', 'end_id']

    srcVocabSize = tgt_vocab_size = srcVocabSize + 2

    target_seqs = tl.prepro.sequences_add_end_id([trainY[100]], end_id=end_id)[0]
    decode_seqs = tl.prepro.sequences_add_start_id([trainY[100]], start_id=start_id, remove_last=False)[0]
    target_mask = tl.prepro.sequences_get_mask([target_seqs])[0]
    if not mode:
        print(""encode_seqs"", [index2Word[id] for id in trainX[100]])
        print(""target_seqs"", [index2Word[id] for id in target_seqs])
        print(""decode_seqs"", [index2Word[id] for id in decode_seqs])
        print(""target_mask"", target_mask)
        print(len(target_seqs), len(decode_seqs), len(target_mask))
		
    tf.reset_default_graph()
    
    session = tf.Session(config=sessionConfig)
	
    encode_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=""encode_seqs"")
    decode_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=""decode_seqs"")
    target_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=""target_seqs"")
    target_mask = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=""target_mask"") 
        
    outputNetwork, _ = createModel(encode_seqs, decode_seqs, srcVocabSize, embeddingDim, isTrain=True, reuse=False)
    outputNetwork.print_params(False)
        
    encode_seqs2 = tf.placeholder(dtype=tf.int64, shape=[1, None], name=""encode_seqs"")
    decode_seqs2 = tf.placeholder(dtype=tf.int64, shape=[1, None], name=""decode_seqs"")
        
    network, RNNnetwork = createModel(encode_seqs2, decode_seqs2, srcVocabSize, embeddingDim, isTrain=False, reuse=True)
    y = tf.nn.softmax(network.outputs)
        
    loss = tl.cost.cross_entropy_seq_with_mask(logits=outputNetwork.outputs, target_seqs=target_seqs, input_mask=target_mask, return_details=False, name='cost')
        
    trainOp = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)

    session.run(tf.global_variables_initializer())

    tl.files.load_and_assign_npz(sess=session, name='model.npz', network=network)	

    def inference(seed):
        seed_id = [word2Index.get(w, unk_id) for w in seed.split("" "")]
        
        state = session.run(RNNnetwork.final_state_encode, {encode_seqs2: [seed_id]})
        o, state = session.run([y, RNNnetwork.final_state_decode], {RNNnetwork.initial_state_decode: state, decode_seqs2: [[start_id]]})
        w_id = tl.nlp.sample_top(o[0], top_k=3)
        w = index2Word[w_id]
        sentence = [w]
        for _ in range(200): # 30 output length
            o, state = session.run([y, RNNnetwork.final_state_decode], {RNNnetwork.initial_state_decode: state, decode_seqs2: [[w_id]]})
            w_id = tl.nlp.sample_top(o[0], top_k=2)
            w = index2Word[w_id]
            if w_id == end_id:
                break
            sentence = sentence + [w]
        return sentence

    if mode:
        while True:
            input_seq = input('Input >> ')
            sentence = inference(input_seq)
            print(""Output >>"", ' '.join(sentence))
    else:
        seeds = [""happy birthday have a nice day"",
                 ""donald trump won last nights presidential debate according to snap online polls""]
        for epoch in range(epochs):
            trainX, trainY = shuffle(trainX, trainY, random_state=0)
            total_loss, n_iter = 0, 0
            for X, Y in tqdm(tl.iterate.minibatches(inputs=trainX, targets=trainY, batch_size=batch_size, shuffle=False), total=nStep, desc='Epoch[{}/{}]'.format(epoch + 1, epochs), leave=False):
                X = tl.prepro.pad_sequences(X)
                _target_seqs = tl.prepro.sequences_add_end_id(Y, end_id=end_id)
                _target_seqs = tl.prepro.pad_sequences(_target_seqs)
                _decode_seqs = tl.prepro.sequences_add_start_id(Y, start_id=start_id, remove_last=False)
                _decode_seqs = tl.prepro.pad_sequences(_decode_seqs)
                _target_mask = tl.prepro.sequences_get_mask(_target_seqs)
				
                _, loss_iter = session.run([trainOp, loss], {encode_seqs: X, decode_seqs: _decode_seqs, target_seqs: _target_seqs, target_mask: _target_mask})
                total_loss += loss_iter
                n_iter += 1

            print('Epoch [{}/{}]: loss {:.4f}'.format(epoch + 1, epochs, total_loss / n_iter))
            
            for seed in seeds:
                print(""Input >>"", seed)
                for _ in range(5):
                    sentence = inference(seed)
                    print(""Output >>"", ' '.join(sentence))
            
            tl.files.save_npz(network.all_params, name='model.npz', sess=session)
    
    session.close()
    
    
def createModel(encodeSeqs, decodeSeqs, vocabSize, embeddingDim, isTrain = True, reuse = False):
    with tf.variable_scope(""model"", reuse = reuse):
        with tf.variable_scope(""embedding"") as vs:
            networkworkEncoder = EmbeddingInputlayer(inputs = encodeSeqs, 
                                                 vocabulary_size = vocabSize, 
                                                 embedding_size = embeddingDim, 
                                                 name = 'sequenceEmbedding')
            vs.reuse_variables()
            networkworkDecoder = EmbeddingInputlayer(inputs = decodeSeqs, 
                                                 vocabulary_size = vocabSize, 
                                                 embedding_size = embeddingDim, 
                                                 name = 'sequenceEmbedding')
        networkworkRNN = Seq2Seq(networkworkEncoder, networkworkDecoder, 
                             cell_fn = tf.nn.rnn_cell.LSTMCell, 
                             n_hidden = embeddingDim, 
                             initializer = tf.random_uniform_initializer(-0.1, 0.1),
                             encode_sequence_length = retrieve_seq_length_op2(encodeSeqs),
                             decode_sequence_length = retrieve_seq_length_op2(decodeSeqs),
                             initial_state_encode = None,
                             dropout = (0.5 if isTrain else None),
                             n_layer = 3,
                             return_seq_2d = True,
                             name = 'seq2seq')
        networkworkOut = DenseLayer(networkworkRNN, n_units = vocabSize, act = tf.identity, name = 'output')
    return networkworkOut, networkworkRNN

def setup(data_corpus):
    metaData, indexQ, indexA = data.load_data(Path=''.format(data_corpus))
    (trainX, trainY), (testX, testY), (validX, validY) = data.split_dataset(indexQ, indexA)
    trainX = tl.prepro.remove_pad_sequences(trainX.tolist())
    trainY = tl.prepro.remove_pad_sequences(trainY.tolist())
    testX = tl.prepro.remove_pad_sequences(testX.tolist())
    testY = tl.prepro.remove_pad_sequences(testY.tolist())
    validX = tl.prepro.remove_pad_sequences(validX.tolist())
    validY = tl.prepro.remove_pad_sequences(validY.tolist())
    return metaData, trainX, trainY, testX, testY, validX, validY

def main():
    try:
        train()
    except KeyboardInterrupt:
        print('Aborted!')

if __name__ == '__main__':
    main()
"
23920,tf.estimator package not installed appears in ops_to_register.h when bazel-bin/tensorflow/python/tools/print_selective_registration_header is executed,"
**System information**
OS Platform and Distribution: maxOS High Sierra 10.13.6
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.12.0
- Python version: 2.7
- Installed using virtualenv? pip? conda?:conda
- Bazel version:0..18.1
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:   Not supported when confugurate
"
23919,front camera bounding box tracking issue,"
Thanks for clicking  in and reading my questions first :)
I want to detect human face by front camera .
I have changed my camera to front, and changed location like this.
` location.left = 288f - result.getLocation().left;`
  ` location.right = 288f - result.getLocation().right;`
      
I have 2 questions.
       1.My code can detect human correctly, but the bounding box tracks in opposite direction(left-            >right. right->left ). Maybe I miss to  edit somewhere's location ?
       2.Another question is , sometimes when target moves, a new bounding box occurs, but the old one    is still there ,too. However,if preview changes a lot (like target leaves quickly, or new target in), the old box will disappear .
If any one have any idea I will appreciate.
I recorded the detection scene and put it below .
[https://reurl.cc/EZ4pv](url)
 "
23918,tf.dynamic_partition may cause NaN loss when use it with multi gpus and it performs normally with single gpu,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **_no_**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): _**Ubuntu 16.04.5 LTS**_
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:**_no_**
- TensorFlow installed from (source or binary):**_binary_**
- TensorFlow version (use command below):**_1.12.0_**
- Python version: **_Python 3.5.2_**
- Bazel version (if compiling from source): _**None**_
- GCC/Compiler version (if compiling from source):  **_5.4.0_**
- CUDA/cuDNN version: **_cuda-9.0.176/cudnn-7.2.1_**
- GPU model and memory: **_2 same GTX Titan X (Pascal), 12GB_** 


I just use the offical docker image of the tensorflow, the tag is `1.12.0-devel-gpu-py3`

**Describe the problem**

**The api `tf.dynamic_partition` may cause NaN loss when use it with multi gpus and it is normal with single gpu.**

The code to reproduce the problem:
```
#coding=utf-8

import os
import time
import tensorflow as tf

os.environ[""CUDA_DEVICE_ORDER""]=""PCI_BUS_ID""  
os.environ[""CUDA_VISIBLE_DEVICES""]=""0""
num_gpus = len(os.environ[""CUDA_VISIBLE_DEVICES""].split(','))

def dp_and_bm_api_test(argv=None):
	a = tf.constant([[1, 2, 3],
					[4, 5, 6],
					[7, 8, 9],
					[10, 11, 12]])
	
	mask = tf.constant([1, 0, 1, 0])
	res_01 = tf.boolean_mask(a, mask)
	res_02 = tf.dynamic_partition(a, mask, num_partitions=2)[1]

	with tf.Session() as sess:
		r_1, r_2 = sess.run([res_01, res_02])
		print(""r_1\n"", r_1)
		print(""r_2\n"", r_2)


def tower_model_fn(features, labels, api_sel=0):
	""""""a test tower fn""""""
	num_classes = 10
	net = tf.layers.conv2d(features, num_classes * 2, [3,3], padding=""same"")
	net = tf.layers.batch_normalization(net)
	net = tf.nn.relu(net)
	net = tf.layers.conv2d(net, num_classes, [3,3], padding=""same"")
	net = tf.layers.batch_normalization(net)
	logits = tf.nn.relu(net)

	labels = tf.squeeze(labels, axis=3)  # reduce the channel dimension.
	logits_by_num_classes = tf.reshape(logits, [-1, num_classes])
	labels_flat = tf.reshape(labels, [-1, ])
	valid_indices = tf.to_int32(labels_flat <= num_classes - 1)

	if api_sel == 0:
		valid_logits = tf.boolean_mask(logits_by_num_classes, valid_indices)
		valid_labels = tf.boolean_mask(labels_flat, valid_indices)
	else:
		valid_logits = tf.dynamic_partition(logits_by_num_classes, valid_indices, num_partitions=2)[1]
		valid_labels = tf.dynamic_partition(labels_flat, valid_indices, num_partitions=2)[1]


	cross_entropy = tf.losses.sparse_softmax_cross_entropy(logits=valid_logits, labels=valid_labels, 
							loss_collection=None)
	tf.add_to_collection(tf.GraphKeys.LOSSES, cross_entropy)
	return None

def train(api_sel=0):
	## variable strategy
	variable_strategy = 'CPU'
	input_device = '/cpu:0'
	#var_device = '/gpu:0'
	var_device = '/cpu:0'
	num_devices = num_gpus
	device_type = 'gpu'
	
	with tf.Graph().as_default() as graph:
		with tf.device(var_device):
			## some collector
			tower_ce_loss = []

			global_step = tf.train.get_or_create_global_step()	

			name_scopes = ['tower_%d' % i for i in range(num_devices)]
			for i in range(num_devices):
				with tf.variable_scope(tf.get_variable_scope(), reuse=bool(i > 0)):
					worker_device = '/{0}:{1}'.format(device_type, i)
					with tf.name_scope(name_scopes[i]) as name_scope:
						with tf.device(worker_device):	

							images = tf.ones([8, 321, 321, 3])
							labels = tf.zeros([8, 321, 321, 1], dtype=tf.int32)
							tower_model_fn(images, labels, api_sel=api_sel)
							ce_now = tf.get_collection(tf.GraphKeys.LOSSES, scope=name_scope)
							tower_ce_loss.append(tf.add_n(ce_now))

		
		with tf.device(var_device):
			update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, name_scopes[0])	

			ce_loss = tf.add_n(tower_ce_loss)
			session_config = tf.ConfigProto(allow_soft_placement=True,
											log_device_placement=False
											)
			# Build an initialization operation to run below.
			init_op = tf.global_variables_initializer()
			max_steps = 30
			step_gap_init_time = 0.0
			with tf.Session(config=session_config) as sess:
				sess.run(init_op)
				for steps in range(1, max_steps + 1, 1):
					step_gap_init_time = time.time()
					c_l = sess.run([ce_loss])
					if steps % 10 == 0:
						gap_time = (time.time() - step_gap_init_time) / 10
						print(""ce loss{0}, {1:1.4f}s per steps"".format(c_l, gap_time))
	tf.keras.backend.clear_session()
	tf.reset_default_graph()

if __name__ == '__main__':
	tf.logging.set_verbosity(tf.logging.INFO)
	print(""\nTest the two apis"")
	dp_and_bm_api_test()
	print(""The number of gpus is {0}"".format(num_gpus))
	print(""\nUsing tf.boolean_mask"")
	train(api_sel=0)
	print(""\nUsing tf.dynamic_partition"")
	train(api_sel=1)
```

When I use two gpus, I set `os.environ[""CUDA_VISIBLE_DEVICES""]=""0, 1""`


```
2018-11-22 07:37:36.070340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1
2018-11-22 07:37:36.478438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-22 07:37:36.478486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1
2018-11-22 07:37:36.478498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y
2018-11-22 07:37:36.478511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N
2018-11-22 07:37:36.478930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11421 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:17:00.0, compute capability: 5.2)
2018-11-22 07:37:36.479432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 11421 MB memory) -> physical GPU (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:18:00.0, compute capability: 5.2)
r_1
 [[1 2 3]
 [7 8 9]]
r_2
 [[1 2 3]
 [7 8 9]]
The number of gpus is 2

Using tf.boolean_mask

ce loss[5.088441], 0.0010s per steps
ce loss[5.088441], 0.0010s per steps
ce loss[5.088441], 0.0010s per steps


Using tf.dynamic_partition
ce loss[nan], 0.0010s per steps
ce loss[nan], 0.0011s per steps
ce loss[nan], 0.0011s per steps
```


When I use one gpus, I set `os.environ[""CUDA_VISIBLE_DEVICES""]=""0""`
Then I get
```

Test the two apis
2018-11-22 07:41:32.112369: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2018-11-22 07:41:34.385976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076
pciBusID: 0000:17:00.0
totalMemory: 11.92GiB freeMemory: 11.80GiB
2018-11-22 07:41:34.386009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-11-22 07:41:34.586971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-22 07:41:34.587016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0
2018-11-22 07:41:34.587024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N
2018-11-22 07:41:34.587249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11421 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:17:00.0, compute capability: 5.2)
r_1
 [[1 2 3]
 [7 8 9]]
r_2
 [[1 2 3]
 [7 8 9]]
The number of gpus is 1


Using tf.boolean_mask

ce loss[2.3863728], 0.0009s per steps
ce loss[2.3863728], 0.0009s per steps
ce loss[2.3863728], 0.0009s per steps



Using tf.dynamic_partition
ce loss[2.1562214], 0.0010s per steps
ce loss[2.1562214], 0.0013s per steps
ce loss[2.1562214], 0.0010s per steps

```

I think the implementation of the  api `tf.dynamic_partition` is really terrible, 
I also reported that this api may cuase memory leak under certain situation
https://github.com/tensorflow/tensorflow/issues/22464

It also cost me lots of time to find that it is not suitable with mulit gpus.....
I think there may other potential issues about the  `tf.dynamic_partition`.
And I don't figure out why I still use it......

"
23917,error raised when convert srgan model to .tflite,"Hi! Recently, I've tried a lot to convert a SRGan model(.ckpt) to .tflite version.However,things dont go that smoothly.When i called the converted model(.tflite) in android studio, it ran just collapsed. Finally , i find it is two modules contained in the code which is pixelShuffler() that  account for the raised error. Because when i set input of that module as output node in the frozen graph,it works.But  i dived into the problematic module and found no operator not supported by tensorflow lite.And the code is @https://github.com/brade31919/SRGAN-tensorflow. So Has anyone converted GAN to .tflite successfully?"
23916,tensorflow1.2/1.4 gpu with dbg version Eigen::GpuDevice::memory fail,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 14.04 or 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source code build with gpu and dbg
- TensorFlow version (use command below): 1.2 or 1.4
- Python version: 2.7
- Bazel version (if compiling from source): 0.45 or 0.54
- GCC/Compiler version (if compiling from source): 4.8
- CUDA/cuDNN version: 8.0/5.1 or 8.0/6.0
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

I build the tensorflow1.4 with gpu and -c dbg version, but when run test code or some example, meet cuda memcpy failed, but if you build with -c opt and work normally and no issue found.
I have already test with ubu14+ tensorflow1.4, or ubu16 with tensorflow1.2/1.4, both of them are failed, the faild log as below, can you give me some help.
------
jensen @ubu16 alexnet_imagenet$ python myalexnet_forward.py 
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
2018-11-22 13:01:42.430751: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-11-22 13:01:42.430780: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-11-22 13:01:42.430787: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-11-22 13:01:42.851315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-11-22 13:01:42.851638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: 
name: GeForce GTX 750 Ti
major: 5 minor: 0 memoryClockRate (GHz) 1.1105
pciBusID 0000:01:00.0
Total memory: 1.96GiB
Free memory: 1.92GiB
2018-11-22 13:01:42.851683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 
2018-11-22 13:01:42.851698: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y 
2018-11-22 13:01:42.851725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 750 Ti, pci bus id: 0000:01:00.0)
tenpython: external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceCuda.h:218: void Eigen::GpuDevice::memcpy(void*, const void*, std::size_t) const: Assertion `err == cudaSuccess' failed.
Aborted (core dumped)

------

**Describe the expected behavior**

work normally with bazel -c dbg and bazel -c opt.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23915,tensorflow gpu with dbg version Eigen::GpuDevice::memcpy assert faile ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23914,Weird behavior of tf.math.reduce_max,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0
- Python version: 3.6.6
- Bazel version (if compiling from source): None
- GCC/Compiler version (if compiling from source): None
- CUDA/cuDNN version: 9.0/7
- GPU model and memory: GeForce GTX 1080 Ti, 11G

**Describe the current behavior**
```python
import numpy as np
import tensorflow as tf

# tf.enable_eager_execution()


def test_bug():
    input = tf.convert_to_tensor(np.ones((1, 1, 1, 1), dtype=np.float32))
    x = tf.layers.conv2d(input, 1, (1, 1), activation=""tanh"",
                         kernel_initializer=tf.initializers.constant(2),
                         use_bias=False)
    max_out = tf.math.reduce_max(x, [1, 2])

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        m = sess.run(max_out)
        m1, _ = sess.run([max_out, x])
        print('m =', m)
        print('m1 =', m1)
        assert np.allclose(m, m1)

test_bug()
```
I'd suppose that the assertion should pass given above code. However, `tf.math.reduce_max` looks very strange:
```
m = [[2.]]
m1 = [[0.9640276]]
```

**Describe the expected behavior**
`m` and `m1` should have the same value and the assertion should pass.
```
m = [[0.9640276]]
m1 = [[0.9640276]]
```

**Code to reproduce the issue**
Listed above.

**Other info / logs**
If eager execution is enabled, then the value of `x` and `max_out` is the same.
"
23912,Building TensorFlow 1.12 with CUDA 10,"I get the error below when I try to build TensorFlow on a Windows 2016 server.
I followed this guide step-by-step: https://mc.ai/install-tensorflow-gpu-with-cuda-10-0-and-cudnn-7-4-for-python-on-windows-10/

It is my first time trying to something like this - so please elaborate your answers :-)

# ERROR: C:/tensorflow/tensorflow/contrib/seq2seq/BUILD:68:1: undeclared inclusion(s) in rule '//tensorflow/contrib/seq2seq:python/ops/_beam_search_ops_gpu':
this rule is missing dependency declarations for the following files included by 'tensorflow/contrib/seq2seq/kernels/beam_search_ops_gpu.cu.cc':
  'C:/users/administrator/appdata/local/temp/2/nvcc_inter_files_tmp_dir/beam_search_ops_gpu.cu.compute_70.cudafe1.stub.c'
  'C:/users/administrator/appdata/local/temp/2/nvcc_inter_files_tmp_dir/beam_search_ops_gpu.cu.fatbin.c'
c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\eigen\src/Core/util/Memory.h(164): warning: calling a __host__ function from a __host__ __device__ function is not allowed

c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\eigen\src/Core/util/Memory.h(179): warning: calling a __host__ function from a __host__ __device__ function is not allowed

c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\eigen\src/Core/products/Parallelizer.h(20): warning: variable ""m_maxThreads"" was set but never used

c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\eigen\src/Core/ArrayWrapper.h(94): warning: __declspec attributes ignored

external/com_google_absl\absl/strings/string_view.h(496): warning: expression has no effect

external/protobuf_archive/src\google/protobuf/arena_impl.h(55): warning: integer conversion resulted in a change of sign
external/protobuf_archive/src\google/protobuf/arena_impl.h(309): warning: integer conversion resulted in a change of sign

external/protobuf_archive/src\google/protobuf/arena_impl.h(310): warning: integer conversion resulted in a change of sign

external/protobuf_archive/src\google/protobuf/map.h(1025): warning: invalid friend declaration

.\tensorflow/core/lib/gtl/flatmap.h(157): warning: invalid friend declaration

c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function ""Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::VALUE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::VALUE]""
(855): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::VALUE]""
(2096): here
            instantiation of ""Eigen::internal::igamma_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma(const Scalar &, const Scalar &) [with Scalar=float]""
c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsHalf.h(34): here

c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function ""Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::VALUE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::VALUE]""
(863): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::VALUE]""
(2096): here
            instantiation of ""Eigen::internal::igamma_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma(const Scalar &, const Scalar &) [with Scalar=float]""
c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsHalf.h(34): here

c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function ""Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::DERIVATIVE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::DERIVATIVE]""
(855): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::DERIVATIVE]""
(2102): here
            instantiation of ""Eigen::internal::igamma_der_a_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma_der_a(const Scalar &, const Scalar &) [with Scalar=float]""
c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsHalf.h(38): here

c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function ""Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::DERIVATIVE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::DERIVATIVE]""
(863): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::DERIVATIVE]""
(2102): here
            instantiation of ""Eigen::internal::igamma_der_a_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma_der_a(const Scalar &, const Scalar &) [with Scalar=float]""
c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsHalf.h(38): here

c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function ""Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
(855): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
(2108): here
            instantiation of ""Eigen::internal::gamma_sample_der_alpha_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::gamma_sample_der_alpha(const Scalar &, const Scalar &) [with Scalar=float]""
c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsHalf.h(42): here

c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function ""Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
(863): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
(2108): here
            instantiation of ""Eigen::internal::gamma_sample_der_alpha_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::gamma_sample_der_alpha(const Scalar &, const Scalar &) [with Scalar=float]""
c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsHalf.h(42): here

c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function ""Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::VALUE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::VALUE]""
(855): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::VALUE]""
(2096): here
            instantiation of ""Eigen::internal::igamma_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma(const Scalar &, const Scalar &) [with Scalar=double]""
c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(120): here

c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function ""Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::VALUE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::VALUE]""
(863): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::VALUE]""
(2096): here
            instantiation of ""Eigen::internal::igamma_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma(const Scalar &, const Scalar &) [with Scalar=double]""
c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(120): here

c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function ""Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::DERIVATIVE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::DERIVATIVE]""
(855): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::DERIVATIVE]""
(2102): here
            instantiation of ""Eigen::internal::igamma_der_a_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma_der_a(const Scalar &, const Scalar &) [with Scalar=double]""
c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(135): here

c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function ""Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::DERIVATIVE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::DERIVATIVE]""
(863): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::DERIVATIVE]""
(2102): here
            instantiation of ""Eigen::internal::igamma_der_a_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma_der_a(const Scalar &, const Scalar &) [with Scalar=double]""
c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(135): here

c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function ""Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
(855): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
(2108): here
            instantiation of ""Eigen::internal::gamma_sample_der_alpha_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::gamma_sample_der_alpha(const Scalar &, const Scalar &) [with Scalar=double]""
c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(154): here

c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function ""Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
(863): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
(2108): here
            instantiation of ""Eigen::internal::gamma_sample_der_alpha_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::gamma_sample_der_alpha(const Scalar &, const Scalar &) [with Scalar=double]""
c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(154): here

c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\eigen\src/Core/util/Memory.h(164): warning: calling a __host__ function from a __host__ __device__ function is not allowed

c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\eigen\src/Core/util/Memory.h(179): warning: calling a __host__ function from a __host__ __device__ function is not allowed

c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\eigen\src/Core/products/Parallelizer.h(20): warning: variable ""m_maxThreads"" was set but never used

c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\eigen\src/Core/ArrayWrapper.h(94): warning: __declspec attributes ignored

external/com_google_absl\absl/strings/string_view.h(496): warning: expression has no effect

external/protobuf_archive/src\google/protobuf/arena_impl.h(55): warning: integer conversion resulted in a change of sign
external/protobuf_archive/src\google/protobuf/arena_impl.h(309): warning: integer conversion resulted in a change of sign

external/protobuf_archive/src\google/protobuf/arena_impl.h(310): warning: integer conversion resulted in a change of sign

external/protobuf_archive/src\google/protobuf/map.h(1025): warning: invalid friend declaration

.\tensorflow/core/lib/gtl/flatmap.h(157): warning: invalid friend declaration

c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function ""Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::VALUE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::VALUE]""
(855): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::VALUE]""
(2096): here
            instantiation of ""Eigen::internal::igamma_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma(const Scalar &, const Scalar &) [with Scalar=float]""
c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsHalf.h(34): here

c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function ""Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::VALUE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::VALUE]""
(863): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::VALUE]""
(2096): here
            instantiation of ""Eigen::internal::igamma_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma(const Scalar &, const Scalar &) [with Scalar=float]""
c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsHalf.h(34): here

c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function ""Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::DERIVATIVE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::DERIVATIVE]""
(855): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::DERIVATIVE]""
(2102): here
            instantiation of ""Eigen::internal::igamma_der_a_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma_der_a(const Scalar &, const Scalar &) [with Scalar=float]""
c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsHalf.h(38): here

c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function ""Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::DERIVATIVE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::DERIVATIVE]""
(863): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::DERIVATIVE]""
(2102): here
            instantiation of ""Eigen::internal::igamma_der_a_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma_der_a(const Scalar &, const Scalar &) [with Scalar=float]""
c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsHalf.h(38): here

c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function ""Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
(855): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
(2108): here
            instantiation of ""Eigen::internal::gamma_sample_der_alpha_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::gamma_sample_der_alpha(const Scalar &, const Scalar &) [with Scalar=float]""
c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsHalf.h(42): here

c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function ""Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
(863): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
(2108): here
            instantiation of ""Eigen::internal::gamma_sample_der_alpha_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::gamma_sample_der_alpha(const Scalar &, const Scalar &) [with Scalar=float]""
c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsHalf.h(42): here

c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function ""Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::VALUE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::VALUE]""
(855): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::VALUE]""
(2096): here
            instantiation of ""Eigen::internal::igamma_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma(const Scalar &, const Scalar &) [with Scalar=double]""
c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(120): here

c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function ""Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::VALUE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::VALUE]""
(863): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::VALUE]""
(2096): here
            instantiation of ""Eigen::internal::igamma_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma(const Scalar &, const Scalar &) [with Scalar=double]""
c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(120): here

c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function ""Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::DERIVATIVE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::DERIVATIVE]""
(855): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::DERIVATIVE]""
(2102): here
            instantiation of ""Eigen::internal::igamma_der_a_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma_der_a(const Scalar &, const Scalar &) [with Scalar=double]""
c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(135): here

c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function ""Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::DERIVATIVE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::DERIVATIVE]""
(863): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::DERIVATIVE]""
(2102): here
            instantiation of ""Eigen::internal::igamma_der_a_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma_der_a(const Scalar &, const Scalar &) [with Scalar=double]""
c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(135): here

c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function ""Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
(855): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
(2108): here
            instantiation of ""Eigen::internal::gamma_sample_der_alpha_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::gamma_sample_der_alpha(const Scalar &, const Scalar &) [with Scalar=double]""
c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(154): here

c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function ""Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
(863): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
(2108): here
            instantiation of ""Eigen::internal::gamma_sample_der_alpha_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::gamma_sample_der_alpha(const Scalar &, const Scalar &) [with Scalar=double]""
c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(154): here

c:\users\administrator\_bazel_administrator\xv6zejqw\execroot\org_tensorflow\external\eigen_archive\eigen\src/Core/ArrayWrapper.h(94): warning: __declspec attributes ignored

external/com_google_absl\absl/strings/string_view.h(496): warning: expression has no effect

external/protobuf_archive/src\google/protobuf/arena_impl.h(55): warning: integer conversion resulted in a change of sign
external/protobuf_archive/src\google/protobuf/arena_impl.h(309): warning: integer conversion resulted in a change of sign

external/protobuf_archive/src\google/protobuf/arena_impl.h(310): warning: integer conversion resulted in a change of sign

external/protobuf_archive/src\google/protobuf/map.h(1025): warning: invalid friend declaration

.\tensorflow/core/lib/gtl/flatmap.h(157): warning: invalid friend declaration

host_defines.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 860.118s, Critical Path: 96.34s
INFO: 2217 processes: 2217 local.
FAILED: Build did NOT complete successfully
"
23906,Copy value of trainable variable to another trainable variable.,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.4.1
- Are you willing to contribute it (Yes/No): No.



**Describe the feature and the current behavior/state.**
Now Tensorflow doesn't allow to copy value of trainable variable to another one. Operations like tf.assight, tf.identity, tf.Variable(source_variable.initialized_value()) or tf.contrib.copy_graph.copy_variable_to_graph(var, self.graph) DO NOT copy just value. After such operations the destination variable referes to the source variable and I can't pass this variable to optimizer for computing gradients: ""ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, ... ""
**Will this change the current api? How?**
This change won't change the existing functions, just will add new opportunity for trainable variables.
**Who will benefit with this feature?**
It will make more flexible structure that can be good for GANs, or when we have two networks and it is neccessary to copy values of one network parameters to second network parameters, and still train two networks independently.
**Any Other info.**
"
23905,"Error after correct (or so i suppose) installation, but nothing seems to work when i try to run ""import tensorflow as tf""","**System information**
- Windows 7 64bits (service pack 1) on AMD FX-8320E
- TensorFlow 1.12.0 (https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.12.0-cp36-cp36m-win_amd64.whl)
- Python 3.7.1 (v3.7.1:260ec2c36a, Oct 20 2018, 14:57:15) [MSC v.1915 64 bit (AMD64)] on win32
- Installed using pip
- GTX 750 ti 2GB (not relevant cause i'm using the CPU only version)


After a fresh install of tensorflow after getting other problems for a few times, i get this log when running a simple.py with just the line ""import tensorflow as tf"", the console log is the following:

`Traceback (most recent call last):
  File ""C:\Users\marco\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\marco\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\marco\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\marco\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\marco\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Impossibile trovare il modulo specificato.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\marco\Desktop\This is a test!.py"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\marco\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\marco\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\marco\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\marco\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\marco\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\marco\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\marco\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\marco\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Impossibile trovare il modulo specificato.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.`"
23904,New instances of iterator.make_initializer do not release previously allocated memory,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0
- Python version: 3.5.2
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 9.0.176 / 7.3.0.29
- GPU model and memory: GeForce GTX 1080 Ti

**Describe the current behavior**
Consider the following setup (see the code below)
1. Create a dataset using `tf.data.Dataset.from_tensor_slices`
2. Use it to initialize a `tf.data.Iterator.from_structure` iterator
3. Repeat the previous step several times in a single session

Observed behavior: memory consumption grows linearly with number of iterations

**Describe the expected behavior**
Memory consumption should be bounded for any number of iterations.

**Code to reproduce the issue**
```python
import tensorflow as tf
import numpy as np
import os
import psutil

n_samples = 1000*1000
dim = 100
batch_size = 50

raw_data = np.zeros((n_samples, dim)).astype(np.float32)
dataset = tf.data.Dataset.from_tensor_slices(raw_data).batch(batch_size)
iterator = tf.data.Iterator.from_structure(tf.float32, [None, dim])

process = psutil.Process(os.getpid())
def mem():
    return process.memory_info().rss / 1024 ** 3.

sess = tf.Session()
for i in range(20):
    sess.run(iterator.make_initializer(dataset))
    print('Epoch {}, mem.: {:.2f}Gb'.format( i, mem() ))
```

Output:
```
Epoch 0, mem.: 1.34Gb
Epoch 1, mem.: 1.71Gb
Epoch 2, mem.: 2.09Gb
Epoch 3, mem.: 2.46Gb
Epoch 4, mem.: 2.83Gb
Epoch 5, mem.: 3.20Gb
Epoch 6, mem.: 3.58Gb
Epoch 7, mem.: 3.95Gb
Epoch 8, mem.: 4.32Gb
Epoch 9, mem.: 4.69Gb
Epoch 10, mem.: 5.07Gb
Epoch 11, mem.: 5.44Gb
Epoch 12, mem.: 5.81Gb
Epoch 13, mem.: 6.18Gb
Epoch 14, mem.: 6.56Gb
Epoch 15, mem.: 6.93Gb
Epoch 16, mem.: 7.30Gb
Epoch 17, mem.: 7.67Gb
Epoch 18, mem.: 8.05Gb
Epoch 19, mem.: 8.42Gb
```"
23903,Poor performance when scaling up data pipeline in multi CPU environments,"**System information**
AWS Deep Learning AMI (ubuntu 16.04, tensorflow 12) on AWS p3.8 and p3.16 instances.

**Describe the current behavior**
I am trying to optimize the speed of my code when training on Imagenet and started by looking at the data pipeline. I am using TFRecordDataset(), and use both prefetch() and num_parallel_calls for mappings. Here I just load in the batches without performing any action on them.

I seem to get reasonably good results on AWS p3.8 (32 vCPU, 4 Nvidia Tesla V100 GPUs), although it's still a bit below the ~3000 images/second that I've seen mentioned elsewhere. However, if I run exactly the same code on AWS p3.16 (64 vCPU, 8 GPUs), throughput *decreases* significantly, even while CPU use is higher. 

*The two systems perform as expected on other tasks.*  I did some very elementary benchmarking of both systems:
- calculating the number of primes, spawning a number of threads using multiprocessing.Process(). As expected, performance between p3.8 and p3.16 appears to be identical when n_threads < 32, and 3.16 performs better when n_threads = 128.  
- reading a file with 1,000,000 lines (and repeat 100 times): identical performance

This suggests to me the problem is with the TFRecordDataset, but if there is any other benchmarking that makes sense, please let me know.

Both systems allow two threads for each CPU. I launched another p3.16 instance with a single thread per CPU, so that the number of virtual CPUs is identical to the p3.8 instance. This did not appear to make any difference.

**Describe the expected behavior**
Performance should improve, or at least remain the same, on p3.16 as compared to p3.8, as there are more CPUs available.

**Code to reproduce the issue**
Taking only the data pipeline part and skipping all shuffling, preprocessing, etcetera I am left with this as a minimal working example.

(Note that I use multiple prefetch statements; this significantly improves performance on both systems for me, but removing them does not change their relative performance. Similarly, using 128 parallel calls in the mapping function gives better results than 32 or 64, but changing that or making it dependent on the actual number of vCPUs doesn't solve the observed behaviour.)

    import tensorflow as tf
    import os
    import time

    DATA_FOLDER = '/home/ubuntu/datasets/imagenet-data'
    BATCH_SIZE = 256
    N_TRIALS = 10
    STEPS_PER_TRIAL = 100

    def decode_imagenet(serialized_example):
        feature_map = {
            'image/encoded': tf.FixedLenFeature([], dtype=tf.string, default_value=''),
            'image/class/label': tf.FixedLenFeature([1], dtype=tf.int64, default_value=-1)
        }
        features = tf.parse_single_example(serialized_example, feature_map)

        image = features['image/encoded']
        image = tf.image.decode_jpeg(image, channels=3)
        image = tf.image.convert_image_dtype(image, tf.float32)
        image = tf.image.resize_images(image, [224, 224])

        label = tf.squeeze(tf.cast(features['image/class/label'], tf.int32))
        
        return image, label

    training_files = [os.path.join(DATA_FOLDER, fn) for fn in os.listdir(DATA_FOLDER) if 'train' in fn]

    dataset = tf.data.TFRecordDataset(training_files)
    dataset = dataset.repeat()
    dataset = dataset.prefetch(BATCH_SIZE*16)
    dataset = dataset.map(decode_imagenet, num_parallel_calls=128)
    dataset = dataset.prefetch(BATCH_SIZE*4)
    dataset = dataset.batch(BATCH_SIZE)
    dataset = dataset.prefetch(1)


    iterator = dataset.make_initializable_iterator()

    batch_x, batch_y = iterator.get_next()

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        sess.run(iterator.initializer)

        res = []
        for trial in range(N_TRIALS):
            t0 = time.time()
            for _ in range(STEPS_PER_TRIAL):
                sess.run([batch_x, batch_y])
            t1 = time.time()
            images_per_second = BATCH_SIZE*STEPS_PER_TRIAL/(t1-t0)
            res.append(images_per_second)
            print('Trial %i: %f ips' % (trial, images_per_second))
        print('Overall average: %f ips' % (sum(res)/N_TRIALS))



**Other info / logs**

Output of above code on p3.8:

    Trial 0: 2687.172262 ips
    Trial 1: 2861.731855 ips
    Trial 2: 2832.192681 ips
    Trial 3: 2826.211587 ips
    Trial 4: 2851.585431 ips
    Trial 5: 2809.069081 ips
    Trial 6: 2869.325024 ips
    Trial 7: 2858.402548 ips
    Trial 8: 2833.678433 ips
    Trial 9: 2859.654147 ips
    Overall average: 2828.902305 ips

Meanwhile, %CPU (observed using the top command) is 2300-2400.

On p3.16:

    Trial 0: 1585.289254 ips
    Trial 1: 2077.237178 ips
    Trial 2: 2165.624391 ips
    Trial 3: 2183.211334 ips
    Trial 4: 2226.638648 ips
    Trial 5: 2217.900892 ips
    Trial 6: 2217.662143 ips
    Trial 7: 2232.781854 ips
    Trial 8: 2178.928816 ips
    Trial 9: 2222.856626 ips
    Overall average: 2130.813114 ips

%CPU is between 3700-4000.

"
23902,tensor.op.outputs is self-referential,"To be honest, I don't know if this is a ""verified"" bug. But I've tried many things, narrowed it down to a really simple reproducible case, read the documentation, [asked on SO](https://stackoverflow.com/questions/53409455/how-do-i-find-the-output-of-a-tensor-and-or-op-in-tensorflow-or-tensorflow-op), and at this point think it's either a bug, or something so unapparent that it might as well be fixed as though it _were_ a bug...So I'm thinking this belongs here as well, even if I do get an answer on SO.  
The issue is as follows (practically replicated from the SO version of my question):
I have a tensor, and I'd like to find ""where it leads"" for various reasons. The way to theoretically do this would be to just look at my_tensor.op.outputs, as per documentation and such, but this always seems to point back to my_tensor itself!
I've easily gone the other way before, meaning I can get the input tensor by using my_tensor.op.inputs, but for some reason ""outputs"" isn't doing the expected.
Here's a simple example:

    import tensorflow as tf
    
    a = tf.placeholder(tf.uint8, name='a')
    b = tf.placeholder(tf.uint8, name='b')
    my_sum = tf.identity(a + b, name='my_sum')
    graph = tf.get_default_graph()
    
    # I should have 4 ops at this point, as validated by:
    print(graph.get_operations())
    >> [<tf.Operation 'a' type=Placeholder>, <tf.Operation 'b' type=Placeholder>, <tf.Operation 'add' type=Add>, <tf.Operation 'my_sum' type=Identity>]
    
    # So let's try get the output of 'a':
    print(list(a.op.outputs))
    >> [<tf.Tensor 'a:0' shape=<unknown> dtype=uint8>]
If you tried the above, you'll see you got back to 'a'...
Again, running my_sum.op.inputs gives the 'add' op, and running even further back get's us to 'a' and 'b' as expected:

    input_to_my_sum = list(my_sum.op.inputs)[0]
    print(input_to_my_sum)
    >> Tensor(""add:0"", dtype=uint8)
    
    print(list(input_to_my_sum.op.inputs))
    >> [<tf.Tensor 'a:0' shape=<unknown> dtype=uint8>, <tf.Tensor 'b:0' shape=<unknown> dtype=uint8>]
But the other way round? No such luck:

    print(list(input_to_my_sum.op.outputs))
    >> [<tf.Tensor 'add:0' shape=<unknown> dtype=uint8>]
    
    print('This is no fun at all')
    >> This is no fun at all
So what am I doing wrong?
I've also tried using the (deprecated) op.values() with no success, and I'm confused because the documentation explicitly states that this should give me the outputs of the op (from [https://www.tensorflow.org/api_docs/python/tf/Operation]()):

>outputs
>The list of Tensor objects representing the outputs of this op.

(I checked that a.op.__class__ is the right class and that I'm reading the correct documentation).

And just to wrap things up, the node_def of the ops also shows no signs of an output field....
Thanks in advance for any advice!
 
BTW, this is my first post and I might be doing something wrong. Let me know if that's the case and I'll try and fix it.

P.S: Using tf 1.8.0 for what it's worth.
"
23901,reduce_prod EXTREMELY slow,"I have a custom operation running tf.reduce_sum over an axis of a Rank 5 tensor, doing an NN training.
As soon as i switch to tf.reduce_prod I get a performance drop of about 20x slower training time.

This was somewhat unexpected since reduce_max or other reductions run at the same speed of reduce_sum.

Is this expected behaviour?

Tested on windows & linux, tensorflow versions tested from 1.5 to 1.11"
23900,No clear_devices in BestExporter,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes, please see this [Colab](https://colab.research.google.com/drive/1GKAqEo7qSr6kMAgxPrOrudA5eLndQ6Ub)

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): n/a
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): n/a
- TensorFlow version (use command below): 1.10+
- Python version: 3.5
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: whatever is on Colab


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

One can not train a distributed-`Estimator` and export the model, then load it on a non-distributive device because if we look at the TensorFlow docs for BestExporter

```
__init__(
    name='best_exporter',
    serving_input_receiver_fn=None,
    event_file_pattern='eval/*.tfevents.*',
    compare_fn=_loss_smaller,
    assets_extra=None,
    as_text=False,
    exports_to_keep=5
)
```
it is apparent that `clear_devices` is not an option.

**Describe the expected behavior**

Let me easily export and import `Estimators`

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

 see this [Colab](https://colab.research.google.com/drive/1GKAqEo7qSr6kMAgxPrOrudA5eLndQ6Ub)
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23899,tflite_simple_example  VerifyVector too few arguments to function call,"

**System information**
- iOS 12.1 :
- iPhone7Plus
- TensorFlowLite 1.10.1(pod install):
- 1.10.1:




**Describe the problem**

Pod install in the lite/examples/ios/simple . Pod automatically installed tensorflow lite 1.10.1.  error occurred, say ""tensorflow/lite/kernels/register.h"" file not found. After I add paths to those header files, I encounter issue ""verifier.VerifyVector(min()) too few arguments to function call, expeced 3, have 1"".

Any guys have any clue ? Thanks in advance"
23898,Error occurs when importing metagraph that contains cudnnRNN cells,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7.4
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install .whl
- TensorFlow version (use command below): 1.12.0
- Python version: 2.7.14
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source): c++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)
- CUDA/cuDNN version: cuda: 9.0 cuDNN: 7.1.3
- GPU model and memory: GTX1050Ti/4GB
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/2603030/tf_env.txt)


**Describe the current behavior**
I try to import graph from a MetaGraphDef proto that contains a CudnnRNN cell, the error below raises:

```
 Traceback (most recent call last):
  File ""test.py"", line 21, in <module>
    saver = tf.train.import_meta_graph(mgd)
  File ""/root/anaconda2/envs/tf12/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1674, in import_meta_graph
    meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]
  File ""/root/anaconda2/envs/tf12/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1696, in _import_meta_graph_with_return_elements
    **kwargs))
  File ""/root/anaconda2/envs/tf12/lib/python2.7/site-packages/tensorflow/python/framework/meta_graph.py"", line 852, in import_scoped_meta_graph_with_return_elements
    ops.prepend_name_scope(value, scope_to_prepend_to_names))
  File ""/root/anaconda2/envs/tf12/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3490, in as_graph_element
    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
  File ""/root/anaconda2/envs/tf12/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3550, in _as_graph_element_locked
    ""graph."" % repr(name))
KeyError: ""The name 'cudnn_lstm/opaque_kernel_saveable' refers to an Operation not in the graph.""
```

What I've done:
I've debuged the souce code and found out that a CudnnLSTMSaveable instance is created when cudnnRNN cell gets called and corresponding op was saved in tf.GraphKeys.SAVEABLE_OBJECTS collections with the name ""cudnn_lstm/opaque_kernel_saveable"".
when executing saver.import_meta_graph, in saver.save_op(line 189 of saver.py) function the saveable object mentioned above is somehow transfered into ""cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel"" and ""cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias"" and gets saved. But the op named ""cudnn_lstm/opaque_kernel_saveable"" didn't gets saved. I'm wandering if this is a bug.

**Describe the expected behavior**
graph gets imported

**Code to reproduce the issue**
```
import tensorflow as tf
import numpy as np
num_layers = 1
num_units = 128
direction = ""unidirectional""
inputs    = tf.placeholder(tf.float32, [None, None, 32], name=""inputs"")
convolved = tf.transpose(inputs, [1, 0, 2])

lstm      =  tf.contrib.cudnn_rnn.CudnnLSTM(num_layers, num_units, direction=direction)
outputs, output_states = lstm(convolved, training=True)
enc_output = tf.transpose(outputs, [1, 0, 2])
optimizer  = tf.train.AdamOptimizer(0.001)
optim = optimizer.minimize(enc_output)
saver     = tf.train.Saver()
init      = tf.global_variables_initializer()
with tf.Session() as session:
    session.run(init)
    # export MetaGraphDef proto to mgd
    mgd=saver.export_meta_graph()
with tf.Session(graph=tf.Graph()) as session:
    # import graph from mgd on another graph
    saver = tf.train.import_meta_graph(mgd)
```

"
23897," 'LSTMBlockCell' error when loading a model using saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)","in the issue below they had the same problem,but there's no much information. btw this is the only issue I found about this.
I hope I can receive more  help by opening this issue. 

_Originally posted by @AbdelsalamHaa in https://github.com/tensorflow/tensorflow/issues/20732#issuecomment-440546353_"
23896,Looking for Tensorflow lite C++ Android NDK install instructions,"Can someone provide me documentation or examples of using Tensorflow lite + Android NDK C++.

The only project example is in Java, but I need to use Tensorflow lite with the Android NDK in C++.

I'm also using a Windows 10 computer and have the latest version of Bazel installed.

Any steps, documentation, and example projects for Tensorflow lite + Android NDK C++ would be very beneficial to me and the community..

Thank you for any support"
23894,Anaconda jupyter noteboob  'Kernel Restarting(The kenel appears to have died.It will restart automatically)'when i used tensorflow-gpu,"this is the code
import os
import tensorflow as tf 
from PIL import Image
from nets import nets_factory
import numpy as np
import matplotlib.pyplot as plt  
CHAR_SET_LEN = 10

IMAGE_HEIGHT = 60 

IMAGE_WIDTH = 160  

BATCH_SIZE = 1

TFRECORD_FILE = ""D:/Tensorflow/captcha/test.tfrecords""

# placeholder
x = tf.placeholder(tf.float32, [None, 224, 224])  


def read_and_decode(filename):
    
    filename_queue = tf.train.string_input_producer([filename])
    reader = tf.TFRecordReader()
   
    _, serialized_example = reader.read(filename_queue)   
    features = tf.parse_single_example(serialized_example,
                                       features={
                                           'image' : tf.FixedLenFeature([], tf.string),
                                           'label0': tf.FixedLenFeature([], tf.int64),
                                           'label1': tf.FixedLenFeature([], tf.int64),
                                           'label2': tf.FixedLenFeature([], tf.int64),
                                           'label3': tf.FixedLenFeature([], tf.int64),
                                       })

    image = tf.decode_raw(features['image'], tf.uint8)

    image_raw = tf.reshape(image, [224, 224])
    # tf.train.shuffle_batch必须确定shape
    image = tf.reshape(image, [224, 224])

    image = tf.cast(image, tf.float32) / 255.0
    image = tf.subtract(image, 0.5)
    image = tf.multiply(image, 2.0)

    label0 = tf.cast(features['label0'], tf.int32)
    label1 = tf.cast(features['label1'], tf.int32)
    label2 = tf.cast(features['label2'], tf.int32)
    label3 = tf.cast(features['label3'], tf.int32)

    return image, image_raw, label0, label1, label2, label3

mage, image_raw, label0, label1, label2, label3 = read_and_decode(TFRECORD_FILE)

#使用shuffle_batch可以随机打乱
image_batch, image_raw_batch, label_batch0, label_batch1, label_batch2, label_batch3 = tf.train.shuffle_batch(
        [image, image_raw, label0, label1, label2, label3], batch_size = BATCH_SIZE,
        capacity = 50000, min_after_dequeue=10000, num_threads=1)


train_network_fn = nets_factory.get_network_fn(
    'alexnet_v2',
    num_classes=CHAR_SET_LEN,
    weight_decay=0.0005,
    is_training=False)
gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)  
with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
    # inputs: a tensor of size [batch_size, height, width, channels]
    X = tf.reshape(x, [BATCH_SIZE, 224, 224, 1])
    
    logits0,logits1,logits2,logits3,end_points = train_network_fn(X)
    
    
    predict0 = tf.reshape(logits0, [-1, CHAR_SET_LEN])  
    predict0 = tf.argmax(predict0, 1)  

    predict1 = tf.reshape(logits1, [-1, CHAR_SET_LEN])  
    predict1 = tf.argmax(predict1, 1)  

    predict2 = tf.reshape(logits2, [-1, CHAR_SET_LEN])  
    predict2 = tf.argmax(predict2, 1)  

    predict3 = tf.reshape(logits3, [-1, CHAR_SET_LEN])  
    predict3 = tf.argmax(predict3, 1)  

    
    sess.run(tf.global_variables_initializer())
    
    saver = tf.train.Saver()
    saver.restore(sess,'./captcha/models/crack_captcha.model-6000')

    
    coord = tf.train.Coordinator()
    
    threads = tf.train.start_queue_runners(sess=sess, coord=coord)

    for i in range(10):
        
        b_image, b_image_raw, b_label0, b_label1 ,b_label2 ,b_label3 = sess.run([image_batch, 
                                                                    image_raw_batch, 
                                                                    label_batch0, 
                                                                    label_batch1, 
                                                                    label_batch2, 
                                                                    label_batch3])
       
        img=Image.fromarray(b_image_raw[0],'L')
        plt.imshow(img)
        plt.axis('off')
        plt.show()
        
        print('label:',b_label0, b_label1 ,b_label2 ,b_label3)
       
        label0,label1,label2,label3 = sess.run([predict0,predict1,predict2,predict3], feed_dict={x: b_image})
        
        print('predict:',label0,label1,label2,label3) 
                
    
    coord.request_stop()
    
    coord.join(threads)"
23893,StructuralEnsembleRegressor gives warning: Converting sparse IndexedSlices to a dense Tensor of unknown shape,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
I break down the stock example and run it on debug mode. I used my own sample data which has 1000 rows x 15 cols.  here's the code:
#----
    reader = tf.contrib.timeseries.CSVReader(csv_file_name,column_names=((tf.contrib.timeseries.TrainEvalFeatures.TIMES,)+ (tf.contrib.timeseries.TrainEvalFeatures.VALUES,) * N_COLS))
    train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(reader, batch_size=N_COLS, window_size = 15)
    training_steps=150
    structural = tf.contrib.timeseries.StructuralEnsembleRegressor(periodicities=[7,100], num_features=N_COLS, cycle_num_latent_values=3)
    ff = structural.train(input_fn=train_input_fn, steps=training_steps)
#----
- OS Platform and Distribution: Windows 10, Anaconda, Python3.6
- TensorFlow installed from (source or binary): 1.12.0
- GPU model and memory: Quadro K2200

Checked related SO posts and tf source code (gradients_impl.py), looks to me this was due to that TensorFlow automatically densifies the tf.IndexedSlices for unknown shape.  This also makes it throw OOM error for slightly larger dataset (1000rows x 25cols). I also saw a note from gradient_impl.py that reads:
  # TODO(mrry): Consider adding static shape information to
  # IndexedSlices, to avoid using numpy here.

"
23892,TensorBoard Callback write_images,"I want to use the TensorBoard callback to visualize my conv layer kernels. But i can only see the first conv layer kernel in TensorBoard and my Dense layers at the end. For the other conv layers i can just see the bias values and not the kernels.

Here is my sample code for the Keras model.

# Imports
import tensorflow as tf
import numpy as np
import os
from os import makedirs
from os.path import exists, join
from keras.datasets import mnist
import time

from keras.layers import *
from keras.activations import *
from keras.models import *
from keras.optimizers import *
from keras.initializers import *
from keras.callbacks import TensorBoard
from keras.callbacks import ModelCheckpoint
from keras.utils.np_utils import to_categorical

from plotting import *

log_dir = '""./""

# Load MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

batch_size = 128
epochs = 10
width = 28
height = 28
depth = 1
num_classes = 10
train_size = x_train.shape[0]
test_size = x_test.shape[0]

x_train = x_train.reshape(train_size, width, height, depth)
y_train = to_categorical(y_train, num_classes=num_classes)
x_test = x_test.reshape(test_size, width, height, depth)
y_test = to_categorical(y_test, num_classes=num_classes)

tb = TensorBoard(
    log_dir=log_dir, 
    histogram_freq=1, 
    write_graph=True, 
    write_images=True)

# Define the DNN
model = Sequential()
model.add(Conv2D(filters=16, kernel_size=3, input_shape=(width, height, depth), name=""conv1""))
model.add(Activation(""relu""))
model.add(Conv2D(filters=20, kernel_size=3, name=""conv2""))
model.add(Activation(""relu""))
model.add(MaxPool2D())

model.add(Conv2D(filters=24, kernel_size=3, name=""conv3""))
model.add(Activation(""relu""))
model.add(Conv2D(filters=28, kernel_size=3, name=""conv4""))
model.add(Activation(""relu""))
model.add(MaxPool2D())

model.add(Flatten())
model.add(Dense(128))
model.add(Activation(""relu""))
model.add(Dense(num_classes, name=""features""))
model.add(Activation(""softmax""))

# Print the DNN layers
model.summary()

# Train the DNN
lr = 1e-3
optimizer = Adam(lr=lr)
model.compile(loss=""categorical_crossentropy"", optimizer=optimizer, metrics=[""accuracy""])
model.fit(x_train, y_train, verbose=1, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), callbacks=[tb])

# Test the DNN
score = model.evaluate(x_test, y_test, batch_size=batch_size)
print(""Test performance: "", score)

![ktb](https://user-images.githubusercontent.com/42785357/48809367-29508000-ecd9-11e8-93e8-dd73713c4515.png)
"
23891,Can we have working example with assets in the documentation for the simple_save?,"**System information**
- TensorFlow version: 1.12
- Doc Link: https://www.tensorflow.org/api_docs/python/tf/saved_model/simple_save 

Simple_save seems to be a newer and more standardized saving approach. Since it can also use assets to store vocabularies, I wonder if it is possible to have a working example in document based on keras sentiment analysis system's data? So there is place to look up how to really integrate the entire model (including vocabulary, word embedding, etc) in a single simple_save."
23890,Failed to allocate tensors : Xcode 10 ,"
**System information**
- OS Platform and Distribution:  macOS Mojave 10.14.2 Beta
- Mobile device : iPhone 8P / IPhone XR
- TensorFlow installed from (source or binary):  SOURCE
- TensorFlow version:  Newest 
- Python version: 3.7
- Installed using virtualenv? pip? conda?: Unknown
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the problem**
Xcode Crashed when I import my models into the TF-Lite example project. I don't know what to do or try to get he models to work.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. Import the TFlite and txt files 
2. copy the path if needle ( Xcode asks this )
3. rename model_file_name to imported tflite model
4. rename label_file_name to imported txt file

**Any other info / logs**
 Loaded model 1resolved reportertensorflow/contrib/lite/kernels/squeeze.cc:64 current >= 0 && current < input_num_dims && input_dims->data[current] == 1 was not true.
Node 66 failed to prepare.

Failed to allocate tensors!2018-11-20 13:08:26.664470-0600 tflite_camera_example[9610:2085000] [MC] System group container for systemgroup.com.apple.configurationprofiles path is /private/var/containers/Shared/SystemGroup/systemgroup.com.apple.configurationprofiles
2018-11-20 13:08:26.664721-0600 tflite_camera_example[9610:2085000] [MC] Reading from public effective user settings.
(lldb) 
"
23888,Error: ...tensorflow/core/common_runtime/bfc_allocator.cc:373] tried to deallocate nullptr,"**System information**
Ubuntu 18.04 (bionic);
g++ (Ubuntu 7.3.0-16ubuntu3) 7.3.0;
bazel release 0.18.1
Python 2.7.15rc1
- CUDA/cuDNN version: none
- GPU model and memory: none
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/2601035/tf_env.txt)

**Current behavior**
I've built TensorFlow from commit
`git log -1`
`commit a6d8ffae097d0132989ae4688d224121ec6d8f35 (HEAD, tag: v1.12.0, origin/r1.12)`
`Author: Todd Wang <toddwang@gmail.com>`
`Date:   Thu Nov 1 18:35:10 2018 -0700`
`Fix a bug in tpu.py and xla.py that while creating an identity node for control input edges under rewrite context, the parent control flow context is lost. (#23446)
PiperOrigin-RevId: 219724472`
with command line
`bazel build --config=mkl -c opt --copt=-mavx --copt=-mavx2 //tensorflow/tools/pip_package:build_pip_package`
and got the error, repeatedly appearing in terminal:
`E tensorflow/core/common_runtime/bfc_allocator.cc:373] tried to deallocate nullptr`
while attempting to run the example [Text generation using a RNN with eager execution](https://www.tensorflow.org/tutorials/sequences/text_generation).
Also can be reproduced with the package, installed with `pip install https://storage.googleapis.com/intel-optimized-tensorflow/tensorflow-1.11.0-cp27-cp27mu-linux_x86_64.whl `.


**Expected behavior**
Running without error messages.
"
23887,Hessian diagonal computation,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.10.0-0-g656e7a2b34 1.10.0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

I'm computing the full Hessian before extracting the diagonal.

**Describe the expected behavior**

I want to only compute the diagonal to avoid the overhead of computing the full Hessian.

**Code to reproduce the issue**

```python
import numpy as np
import tensorflow as tf

y_true = np.array([
    [1, 0, 0, 0, 0],
    [0, 1, 0, 0, 0],
    [0, 0, 0, 1, 0],
    [0, 0, 1, 0, 0],
    [0, 0, 0, 0, 1],
    [0, 0, 0, 0, 1],
    [0, 0, 0, 0, 1]
], dtype=float)

y_pred = np.array([
    [1, 0, 0, 0, 0],
    [0, 1, 0, 0, 0],
    [0, 0, 0, 1, 0],
    [0, 0, 1, 0, 0],
    [0, 0, 0, 0, 1],
    [0, 0, 0, 0, 1],
    [0, 0, 0, 0, 1]
], dtype=float)

weights = np.array([1, 1, 1, 1, 1], dtype=float)

with tf.Session():

    # We first convert the numpy arrays to Tensorflow tensors
    y_true = tf.convert_to_tensor(y_true)
    y_pred = tf.convert_to_tensor(y_pred)
    weights = tf.convert_to_tensor(weights)

    # The following code block is a custom loss 
    ys = tf.reduce_sum(y_true, axis=0)
    y_true = y_true / ys
    ln_p = tf.nn.log_softmax(y_pred)
    wll = tf.reduce_sum(y_true * ln_p, axis=0)
    loss = -tf.tensordot(weights, wll, axes=1)

    grad = tf.gradients(loss, y_pred)[0]

    hess = tf.hessians(loss, y_pred)[0]
    hess = tf.diag_part(hess)

    print(hess.eval())
```

This outputs:

```python
[[0.24090069 0.12669198 0.12669198 0.12669198 0.12669198]
 [0.12669198 0.24090069 0.12669198 0.12669198 0.12669198]
 [0.12669198 0.12669198 0.12669198 0.24090069 0.12669198]
 [0.12669198 0.12669198 0.24090069 0.12669198 0.12669198]
 [0.04223066 0.04223066 0.04223066 0.04223066 0.08030023]
 [0.04223066 0.04223066 0.04223066 0.04223066 0.08030023]
 [0.04223066 0.04223066 0.04223066 0.04223066 0.08030023]]
```

which is exactly what I want. The problem is that the full Hessian is computed before actually `tf.diag_part`.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23885,tensorflow/core/framework/op_kernel.cc:1261] Invalid argument: ValueError,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, I also tested the code in Examples
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.12.0
- Python version: 3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version: 9.0
- GPU model and memory: TITAN Xp (12196MiB)
--


**Describe the current behavior**
I have been testing your MirroredStrategy() function with your example script and my own script. 
- It worked well on the sample script. 
- When I adapted my own script, it kept emitting the following error I don't know how to interpret/search.

```
WARNING:tensorflow:Not all devices in distribute strategy are visible by TensorFlow sessions.
WARNING:tensorflow:You are accessing attribute optimizerof the 
DistributedCallbackModel that may not have been set correctly.
WARNING:tensorflow:You are accessing attribute 
_unconditional_checkpoint_dependenciesof the DistributedCallbackModel 
that may not have been set correctly.
Epoch 1/1
2018-11-20 16:41:15.195119: W tensorflow/core/framework/op_kernel.cc:1261] 
Invalid argument: ValueError: `generator` yielded an element of shape (1, 1) 
where an element of shape (1,) was expected.
Traceback (most recent call last):

  File ""xxxxx/lib/python3.5/site-packages/tensorflow/python/ops/script_ops.py"", 
line 206, in __call__
    ret = func(*args)

  File ""xxxxx/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py"", 
line 487, in generator_py_func
    ""of shape %s was expected."" % (ret_array.shape, expected_shape))

ValueError: `generator` yielded an element of shape (1, 1) 
where an element of shape (1,) was expected.


[[{{node PyFunc}} = PyFunc[Tin=[DT_INT64], Tout=[DT_INT32, DT_INT64], 
token=""pyfunc_1""](arg0)]]                                                   
[[{{node IteratorGetNext}} = IteratorGetNext[output_shapes=[[?,500], [?,1]], 
output_types=[DT_INT32, DT_INT64]](IteratorFromStringHandle V2)]] 
[[{{node ExperimentalFunctionBufferingResourceGetNext}} = 
ExperimentalFunctionBufferingResourceGetNext[output_types=[DT_INT32, DT_INT64] , 
_device=""/job:localhost/replica:0/task:0/device:CPU:0""]
(ExperimentalFunctionBufferingResource)]]
Exception ignored in:  6e80>>
--



```
The script without 	```distribution = tf.contrib.distribute.MirroredStrategy()``` ran well. Since I need to populate the network with a huge set, I plan to parallize the work on various GPUs. 


**Describe the expected behavior**
Please kindly let me know how should I interpret the errors and what the solutions are. Thank you.

**Code to reproduce the issue**
My network architecture looks like this (a hierarchical one): 
```
config = tf.ConfigProto(allow_soft_placement = True, log_device_placement= False)
	sess = tf.Session(config = config)
	with tf.Session(config = config) as sess:
		sess.run(tf.global_variables_initializer())
                # load in data
	         #######################DNN Level 1########################
	        if options.L1_model == 0:
		        model = BuildModel()
		        model.fit(X_train, y_train[:, 0],
				  validation_data=(X_test, y_test[:, 0]),
				  epochs=options.epochs,
				  verbose=2,
				  batch_size=options.batch_size_L1)
          #######################DNN Level 1########################
	         if options.L2_model == 0:
 
		         model = BuildModel()
		          model.fit(..., batch_size=options.batch_size_L2)

```

The model stopped already at the first level. And I don't think the failure has something to do with a hierarchical structure. 

`BuildModel()` is defined this way:
```
def buildModel_DNN(word_index, embeddings_index, nClasses, 
MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, gpusno, nLayers=3,Number_Node=100, dropout=0.5):
	
	embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))
	for word, i in word_index.items():
		embedding_vector = embeddings_index.get(word)
		if embedding_vector is not None:
			embedding_matrix[i] = embedding_vector
			
	model = tf.keras.models.Sequential()
	
	model.add(tf.keras.layers.Embedding(len(word_index) + 1,
								EMBEDDING_DIM,
								weights=[embedding_matrix],
								input_length=MAX_SEQUENCE_LENGTH,
								trainable=True))
	model.add(tf.keras.layers.Flatten())
	
	for i in range(0,nLayers):
		model.add(tf.keras.layers.Dense(Number_Node, activation='relu'))
		model.add(tf.keras.layers.Dropout(dropout))
	
	model.add(tf.keras.layers.Dense(nClasses, activation='softmax'))

	
	distribution = tf.contrib.distribute.MirroredStrategy(
                               ['/device:GPU:0', '/device:GPU:1'], prefetch_on_device=True)
	
	model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,
				  optimizer=tf.train.AdamOptimizer(),
				  metrics=['accuracy'], 
				  # distribute=distribution,
				  options = run_opts)
				  
	print('model summary:') 
	model.summary()

	return model
```"
23883,pip install tensorflow not working for py version 3.7.0 and 3.7.1,"
**System information**
- OS : Win 10
- TensorFlow installed from : pip
- TensorFlow version: 1.12.0
- Python version: 3.7.0 and 3.7.1
- Installed using: pip



`$ python --version`
`Python 3.7.1`
`pip --version`
`pip 18.1`
`pip install tensorflow`"
23882,The same op seed gives different results in eager execution,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux openSUSE Leap 42.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: none
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0
- Python version: 3.6.4
- Bazel version (if compiling from source): none
- GCC/Compiler version (if compiling from source): none
- CUDA/cuDNN version: none
- GPU model and memory: none


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

```
import tensorflow as tf

tf.enable_eager_execution()

a = tf.random_uniform((3, ), seed=0)
b = tf.random_uniform((3, ), seed=0)
print(a)
print(b)
```

prints

```
tf.Tensor([0.10086262 0.9701668  0.8487642 ], shape=(3,), dtype=float32)
tf.Tensor([0.5689162  0.31256282 0.09009469], shape=(3,), dtype=float32)
```

When run in a graph, the output is the same.

**Describe the expected behavior**

I would have expected the output to be same in eager mode as well, just like in graph mode.
"
23881,tensorflow1.8 cuda 9.0 cuDNN ubuntu16.04 install wrong,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:1.8.0-gpu
- Python version:3.5.2
- Installed using virtualenv? pip? conda?:pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):4.4
- CUDA/cuDNN version:9/7
- GPU model and memory:1050 Ti  4g

when i used python to import tensorflow as tf ,there is a problem :ImportError: libcusolver.so.8.0: cannot open shared object file: No such file or directory. but when i searched the file in the /usr/local/cuda/lib64, there existed a file named libcusolver.so.9.0.

"
23879,"1 error detected in the compilation of ""C:/Users/dorp/AppData/Local/Temp/nvcc_in ter_files_tmp_dir/reduce_slice_ops_gpu.cu.compute_70.cpp1.ii"".","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.11/1.12
- Python version:2.6
- Bazel version (if compiling from source):1.18
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:9/9.2
- GPU model and memory:gtx1070 8gb

Used the command:

> C:\Users\dorp\Desktop\tensorflow-1.11.0>bazel build --config=opt --config=cuda -
-define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_pack
age

And got the error
> 1 error detected in the compilation of ""C:/Users/dorp/AppData/Local/Temp/nvcc_in
ter_files_tmp_dir/reduce_slice_ops_gpu.cu.compute_70.cpp1.ii"".
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 285.274s, Critical Path: 58.63s
INFO: 1550 processes: 1550 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully


"
23878,Bug: instantiating dynamic_rnn with tf.int32 in input and state raises TypeError,"**System information**
- OS Platform: OS X 10.13.3
- Custom code
- tensorflow version: 1.12.0
- python version: 3.6.5

**Describe the current behavior**
Tensorflow raises a TypeError when creating a dynamic_rnn with tf.int32 type in its input and state. When changing the type to tf.float32 the error is not raised.

**Describe the expected behavior**
Ideally, a dynamic_rnn should support tf.in32 types. If there's any reason why instantiating a dynamic_rnn with tf.int32 type in its input and state should not be allowed, a custom error should be raised.  

**Code to reproduce the issue**
The code below reproduces the error:
```
import tensorflow as tf

X = tf.placeholder(tf.int32, [None, 10, 1])
cell = tf.nn.rnn_cell.LSTMCell(1, dtype=tf.int32)
output, state = tf.nn.dynamic_rnn(cell=cell, inputs=X, dtype=tf.int32)

```
The code below doesn't:
```

import tensorflow as tf

X = tf.placeholder(tf.float32, [None, 10, 1])
cell = tf.nn.rnn_cell.LSTMCell(1, dtype=tf.float32)
output, state = tf.nn.dynamic_rnn(cell=cell, inputs=X, dtype=tf.float32)

```

Note the change in dtype.

**Other info / logs**
TRACEBACK:
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-3-1d83a30f7748> in <module>()
      2 X = tf.placeholder(tf.int32, [None, 10, 1])
      3 cell = tf.nn.rnn_cell.LSTMCell(1, dtype=tf.int32)
----> 4 output, state = tf.nn.dynamic_rnn(cell=cell, inputs=X, dtype=tf.int32)#, initial_state=state)
      5 

~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py in dynamic_rnn(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)
    662         swap_memory=swap_memory,
    663         sequence_length=sequence_length,
--> 664         dtype=dtype)
    665 
    666     # Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].

~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py in _dynamic_rnn_loop(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)
    870       parallel_iterations=parallel_iterations,
    871       maximum_iterations=time_steps,
--> 872       swap_memory=swap_memory)
    873 
    874   # Unpack final output if not using output tuples.

~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)
   3289       ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, loop_context)
   3290     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants,
-> 3291                                     return_same_structure)
   3292     if maximum_iterations is not None:
   3293       return result[1]

~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants, return_same_structure)
   3002       with ops.get_default_graph()._mutation_lock():  # pylint: disable=protected-access
   3003         original_body_result, exit_vars = self._BuildLoop(
-> 3004             pred, body, original_loop_vars, loop_vars, shape_invariants)
   3005     finally:
   3006       self.Exit()

~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)
   2937         flat_sequence=vars_for_body_with_tensor_arrays)
   2938     pre_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access
-> 2939     body_result = body(*packed_vars_for_body)
   2940     post_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access
   2941     if not nest.is_sequence(body_result):

~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in <lambda>(i, lv)
   3258         cond = lambda i, lv: (  # pylint: disable=g-long-lambda
   3259             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))
-> 3260         body = lambda i, lv: (i + 1, orig_body(*lv))
   3261 
   3262     if context.executing_eagerly():

~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py in _time_step(time, output_ta_t, state)
    838           skip_conditionals=True)
    839     else:
--> 840       (output, new_state) = call_cell()
    841 
    842     # Keras cells always wrap state as list, even if it's a single tensor.

~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py in <lambda>()
    824     if is_keras_rnn_cell and not nest.is_sequence(state):
    825       state = [state]
--> 826     call_cell = lambda: cell(input_t, state)
    827 
    828     if sequence_length is not None:

~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py in __call__(self, inputs, state, scope, *args, **kwargs)
    368     # method.  See the class docstring for more details.
    369     return base_layer.Layer.__call__(self, inputs, state, scope=scope,
--> 370                                      *args, **kwargs)
    371 
    372 

~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)
    372 
    373       # Actually call layer
--> 374       outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
    375 
    376     if not context.executing_eagerly():

~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    755       if not in_deferred_mode:
    756         self._in_call = True
--> 757         outputs = self.call(inputs, *args, **kwargs)
    758         self._in_call = False
    759         if outputs is None:

~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py in call(self, inputs, state)
   1003            sigmoid(i + self._w_i_diag * c_prev) * self._activation(j))
   1004     else:
-> 1005       c = (sigmoid(f + self._forget_bias) * c_prev + sigmoid(i) *
   1006            self._activation(j))
   1007 

TypeError: unsupported operand type(s) for +: 'Tensor' and 'float'"
23876,How to use tf.contrib.periodic_resample,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 1.6.0
- Doc Link: https://www.tensorflow.org/api_docs/python/tf/contrib/periodic_resample/periodic_resample


**Describe the documentation issue**
There is no example on how to use the operation at all. Specifically, most graphs that this operation is useful for (DCGANs) use 4D tensors (batch size, height, width, channels), with the batch size being unknown already (``None``) and the operation should decrease a single dimension specified as ``None``. Since we usually don't want to decrease the batch size with this operation, I have no idea how to actually use it in this case.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
If I understand how to actually use this function, yes.
"
23875,DCGAN implementation differs between GitHub and Colab,"There's a difference between the [DCGAN](https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/contrib/eager/python/examples/generative_examples/dcgan.ipynb) implementation present on GitHub and the one present in Colab.
Also, they both suffer the same problem as #19643 #23873 "
23874,"warning ""using ptxas 10.0.145, which is older than 9.2.88""","""*** WARNING *** You are using ptxas 10.0.145, which is older than 9.2.88""
https://storage.googleapis.com/tf-performance/tf_binary/tensorflow-1.12.0.a6d8ffa.AVX2.CUDA10-cp27-cp27mu-linux_x86_64.whl

https://github.com/tensorflow/tensorflow/blob/f827bad24efc949e657e141464ea1010c18d812c/tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc#L427 
"
23873,"Keras layers: no update ops added even when used as a ""simplified interface to Tensorflow""","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Archlinux
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12
- Python version: 3.6
- CUDA/cuDNN version: cuda 9.0, cudnn 7.1
- GPU model and memory: nvidia 1080ti

**Describe the current behavior**

I'm migrating my codebase to a Tensorflow 2.0 compatible version, thus I'm replacing all the `tf.layers` call with their `tf.keras.layers` counterpart.

I want to continue using static graphs and MonitoredSession to train my models (since I do have all the input pipelines defined with tf.data.Dataset and all the training script defined to work in this way).

However, there's a huge problem when a layer adds some update operation (like the BatchNormalization layer).

The actual behavior is that the update operations (of moving mean and variance) are not called when training the model.

This can be OK in a full-keras solution, where the connection among models, the update operations and so on, are managed by the `train_on_batch` + `model.compile` + `model.fit` call (that do some magic in the background).
In fact, @fchollet said this is by design: https://github.com/tensorflow/tensorflow/issues/19643#issuecomment-394527486

But since I don't want to migrate to a full keras-based solution, how can I handle the updates?

I found some theoretical workaround (collected the update operations `model.updates` + collect the inputs `model.inputs`, loop over the inputs, feed the correct input and execute with sess.run the updates), but those are really ugly and they don't work: I can trigger the parameter update, but the time-step of the update execution is wrong and the solution does not converge: moreover, when the model becomes complex (like in the example of BiGAN below) it can be a real mess and the code become unmaintenable.

**Describe the expected behavior**

When I'm using keras layers to define the models and I'm not calling `train_on_batch` or `compile` or any other method to train the model that's pure keras, the update operations should be placed into the graph (having thus the same behavior of the batch normalization layer present in `tf.layers`) and executed when model `.trainable` is `True`.

Moreover, there's another strange behavour: when I define a model output passing a new input (hence I invoke the `Model.call` method) the update operation have no idea of this new input.

Probably, the `.call` method shouln't just return the output tensor, but it should return a new `Model` that shared the same parameters of the orignial one, but defined with a new input (and thus with its update ops aware of the new input).

**Code to reproduce the issue**
A BiGAN implementation.

```python
from typing import Tuple, Callable, Any, Optional
import multiprocessing
import sys
import numpy as np
import tensorflow as tf
from tensorflow import keras as k
from tensorflow.python.training.summary_io import SummaryWriterCache


def dataset(
    shape: Tuple[int, int],
    batch_size: int = 32,
    epochs: int = None,
    train: bool = False,
    _batch=True,
) -> tf.data.Dataset:
    """"""Returns the dataset correcly batched and resized
    Args:
        shape: The output shape of the images in the Dataset
        batch_size: The size of the batch to return at each invocation
        epochs: The the number of times that the dataset will repeat itself
                before throwing an exception
        train: when True, returns the shuffled train dataset, when False returns
               the test, not shuffled, dataset
        _batch: private, do not use
    Returns:
        The dataset
    """"""

    def _process(image, label):
        """"""The function used to resize the image to the specified shape.
        Used in the tf.data.Dataset.map function
        Args:
            image: the input image
            label: the input label
        Return:
            resized_image, label
        """"""
        nonlocal shape
        image = tf.image.resize_images(
            tf.expand_dims(image, axis=0), shape, tf.image.ResizeMethod.NEAREST_NEIGHBOR
        )
        image = tf.cast(image, tf.float32)
        image = tf.squeeze(image, axis=[0])
        return image, label

    (train_images, train_labels), (
        test_images,
        test_labels,
    ) = k.datasets.cifar10.load_data()
    if not train:
        train_images = test_images
        train_labels = test_labels
    train_images = (train_images - 127.5) / 127.5

    def _generator():
        r""""""The generator that returns the pair image,label
        This must be used in order to don't use tf.data.Dataset.from_tensor_slices.abs
        In this way, we can build a dataset from a generator and solve the problem of huge
        graphs created by from_tensor_slices (it creates a constant in the graph :\)
        """"""
        for image, label in zip(train_images, train_labels):
            yield image, label

    def _set_shape(image, label):
        """"""Set the static + dynamic shape of the image, in order to correctly build the
        input pipeline in both phases
        Args:
            image: the input image
            label: the input label
        Return:
            image, label
        """"""
        image.set_shape((32, 32, 3))  # static
        image = tf.reshape(image, (32, 32, 3))  # dynamic
        return image, label

    _dataset = tf.data.Dataset.from_generator(
        _generator, (tf.float32, tf.int32)
    )  # unkown shape
    _dataset = _dataset.map(
        _set_shape, num_parallel_calls=multiprocessing.cpu_count()
    )  # known static chsape

    _dataset = _dataset.map(
        _process, num_parallel_calls=multiprocessing.cpu_count()
    )  # resize to desired input shape

    if _batch:
        _dataset = _dataset.batch(batch_size, drop_remainder=True)
        if epochs:
            _dataset = _dataset.repeat(epochs)
    elif epochs:
        _dataset = _dataset.repeat(epochs)

    _dataset = _dataset.prefetch(1)
    return _dataset


KERNEL_INITIALIZER = k.initializers.RandomNormal(mean=0.0, stddev=0.02)
ALMOST_ONE = k.initializers.RandomNormal(mean=1.0, stddev=0.02)


def discriminator(
    visual_shape: tf.TensorShape,
    encoding_shape: tf.TensorShape,
    conditioning: Optional[Any] = None,
    l2_penalty: float = 0.0,
) -> k.Model:
    """"""
    Build the Discriminator model.

    Returns a k.Model with 2 inputs and a single output.
    The inputs are an image and its encoded/latent representation.

    Args:
        visual_shape: The shape of the visual input, 3D tensor
        encoding_shape: The shape of the latent/encoded representation
        # NOT IMPLEMENTED: Conditioning: data used as GAN conditioning
        # UNUSED: l2_penalty: l2 regularization strength

    Returns:
        The discriminator model.

    """"""
    kernel_size = (5, 5)

    # Inputs
    # (64, 64, C)
    # (Latent Dimension, )
    input_visual = k.layers.Input(shape=visual_shape)
    input_encoding = k.layers.Input(shape=encoding_shape)

    # Data
    # ### Layer 0
    # (64, 64, 32)
    visual = k.layers.Conv2D(
        filters=32,
        kernel_size=kernel_size,
        strides=(1, 1),
        padding=""same"",
        kernel_initializer=KERNEL_INITIALIZER,
        kernel_regularizer=k.regularizers.l2(l2_penalty),
    )(input_visual)
    visual = k.layers.LeakyReLU(alpha=0.1)(visual)

    # Data
    # ### Layer 1
    # (32, 32, 32)
    visual = k.layers.Conv2D(
        filters=32,
        kernel_size=kernel_size,
        strides=(2, 2),
        padding=""same"",
        kernel_initializer=KERNEL_INITIALIZER,
        kernel_regularizer=k.regularizers.l2(l2_penalty),
    )(visual)
    visual = k.layers.BatchNormalization()(visual)
    visual = k.layers.LeakyReLU(alpha=0.1)(visual)
    visual = k.layers.Dropout(rate=0.5)(visual)

    # ### Layer 2
    # (16, 16, 64)
    visual = k.layers.Conv2D(
        filters=64,
        kernel_size=kernel_size,
        strides=(2, 2),
        padding=""same"",
        kernel_initializer=KERNEL_INITIALIZER,
    )(visual)
    visual = k.layers.BatchNormalization()(visual)
    visual = k.layers.LeakyReLU(alpha=0.1)(visual)
    visual = k.layers.Dropout(rate=0.5)(visual)

    # Flatten
    visual = k.layers.Flatten()(visual)

    # Encoding
    # ### Layer 5 D(z)
    # (512,)
    encoding = k.layers.Dense(units=512, kernel_initializer=KERNEL_INITIALIZER)(
        input_encoding
    )
    encoding = k.layers.LeakyReLU(alpha=0.1)(encoding)
    encoding = k.layers.Dropout(rate=0.5)(encoding)

    # Data + Encoding
    # ### Layer 6 D(x, z)
    # (4608,)
    mixed = k.layers.Concatenate()([visual, encoding])
    mixed = k.layers.Dense(units=1024, kernel_initializer=KERNEL_INITIALIZER)(mixed)
    mixed = k.layers.LeakyReLU(alpha=0.1)(mixed)
    mixed = k.layers.Dropout(rate=0.5)(mixed)
    features = mixed

    # Final Step
    # ### Layer 7
    # (1)
    out = k.layers.Dense(1, kernel_initializer=KERNEL_INITIALIZER)(mixed)

    # Use the functional interface
    model = k.Model(inputs=[input_visual, input_encoding], outputs=[out, features])
    model.summary()
    return model


def generator(
    input_shape: int,
    output_depth: int = 3,
    conditioning: Optional[Any] = None,
    l2_penalty: float = 0.0,
) -> k.Model:
    """"""
    Build the Generator model.

    Given a latent representation, generates a meaningful image.
    The input shape must be in the form of a vector 1x1xD.

    Args:
        input_shape: The shape of the noise prior
        output_depth: The number of channels of the generated image
        # NOT IMPLEMENTED: Conditioning: data used as GAN conditioning
        l2_penalty: l2 regularization strength

    Returns:
        The Generator model.

    """"""
    kernel_size = (5, 5)
    model = k.Sequential(name=""generator"")

    # Project and Reshape the latent space
    # ### Layer 1
    # (4*4*64,)
    model.add(
        k.layers.Dense(
            units=4 * 4 * 64,
            kernel_initializer=KERNEL_INITIALIZER,
            input_shape=input_shape,
            kernel_regularizer=k.regularizers.l2(l2_penalty),
        )
    )
    model.add(k.layers.Activation(k.activations.relu))
    model.add(k.layers.Reshape((4, 4, 64)))

    # Starting Deconvolutions
    # ### Layer 2
    # (8, 8, 64)
    model.add(
        k.layers.Conv2DTranspose(
            filters=64,
            kernel_size=kernel_size,
            strides=(2, 2),
            padding=""same"",
            kernel_initializer=KERNEL_INITIALIZER,
        )
    )
    model.add(k.layers.BatchNormalization())
    model.add(k.layers.Activation(k.activations.relu))

    # Starting Deconvolutions
    # ### Layer 3
    # (16, 16, 128)
    model.add(
        k.layers.Conv2DTranspose(
            filters=128,
            kernel_size=kernel_size,
            strides=(2, 2),
            padding=""same"",
            kernel_initializer=KERNEL_INITIALIZER,
        )
    )
    model.add(k.layers.BatchNormalization())
    model.add(k.layers.Activation(k.activations.relu))

    # ### Layer 4
    # (32, 32, 256)
    model.add(
        k.layers.Conv2DTranspose(
            filters=256,
            kernel_size=kernel_size,
            strides=(2, 2),
            padding=""same"",
            kernel_initializer=KERNEL_INITIALIZER,
        )
    )
    model.add(k.layers.BatchNormalization())
    model.add(k.layers.Activation(k.activations.relu))

    # ### Layer 5
    # (64, 64, C)
    model.add(
        k.layers.Conv2DTranspose(
            filters=output_depth,
            kernel_size=kernel_size,
            strides=(2, 2),
            padding=""same"",
            kernel_initializer=KERNEL_INITIALIZER,
        )
    )
    model.add(k.layers.Activation(k.activations.tanh))  # G(z) in [-1,1]

    model.summary()
    return model


def encoder(
    visual_shape: int, latent_dimension: int, l2_penalty: float = 0.0
) -> k.Model:
    """"""
    Build the Encoder model.

    The encoder encodes the input in a vector with shape 1x1xlatent_dimension.

    Args:
        visual_shape: The shape of the input to encode
        latent_dimension: The number of dimensions (along the depth) to use.
        # NOT IMPLEMENTED: conditioning: Data used as GAN conditioning
        l2_penalty: l2 regularization strength

    Returns:
        The Encoder model.

    """"""

    kernel_size = (5, 5)

    # Inputs
    # (64, 64, C)
    # (Latent Dimension, )
    input_visual = k.layers.Input(shape=visual_shape)

    # Data
    # ### Layer 0
    # (64, 64, 32)
    visual = k.layers.Conv2D(
        filters=32,
        kernel_size=kernel_size,
        strides=(1, 1),
        padding=""same"",
        kernel_initializer=KERNEL_INITIALIZER,
        kernel_regularizer=k.regularizers.l2(l2_penalty),
    )(input_visual)
    visual = k.layers.LeakyReLU(alpha=0.1)(visual)

    # Data
    # ### Layer 1
    # (32, 32, 32)
    visual = k.layers.Conv2D(
        filters=32,
        kernel_size=kernel_size,
        strides=(2, 2),
        padding=""same"",
        kernel_initializer=KERNEL_INITIALIZER,
        kernel_regularizer=k.regularizers.l2(l2_penalty),
    )(visual)
    visual = k.layers.BatchNormalization()(visual)
    visual = k.layers.LeakyReLU(alpha=0.1)(visual)
    visual = k.layers.Dropout(rate=0.5)(visual)

    # ### Layer 2
    # (16, 16, 64)
    visual = k.layers.Conv2D(
        filters=128,
        kernel_size=kernel_size,
        strides=(2, 2),
        padding=""same"",
        kernel_initializer=KERNEL_INITIALIZER,
    )(visual)
    visual = k.layers.BatchNormalization()(visual)
    visual = k.layers.LeakyReLU(alpha=0.1)(visual)
    visual = k.layers.Dropout(rate=0.5)(visual)

    # Flatten
    visual = k.layers.Flatten()(visual)

    # Encoding
    # (Latent space, )
    # ### Layer 5
    visual = k.layers.Dense(
        units=latent_dimension, kernel_initializer=KERNEL_INITIALIZER
    )(visual)

    model = k.Model(inputs=input_visual, outputs=visual)
    model.summary()
    return model


def bce(x: tf.Tensor, label: tf.Tensor) -> tf.Tensor:
    """"""Returns the discrete binary cross entropy between x and the discrete label
    Args:
        x: a 2D tensor
        label: the discrite label, aka, the distribution to match

    Returns:
        The binary cros entropy
    """"""
    assert len(x.shape) == 2 and len(label.shape) == 0

    return tf.losses.sigmoid_cross_entropy(tf.ones_like(x) * label, x)


def min_max(positive: tf.Tensor, negative: tf.Tensor) -> tf.Tensor:
    """"""Returns the discriminator (min max) loss
    Args:
        positive: the discriminator output for the positive class: 2D tensor
        negative: the discriminator output for the negative class: 2D tensor
    Returns:
        The sum of 2 BCE
    """"""
    one = tf.constant(1.0)
    zero = tf.constant(0.0)
    d_loss = bce(positive, one) + bce(negative, zero)
    return d_loss


def train():
    """"""Train the GAN.""""""
    batch_size = 32
    epochs = 100
    latent_dimension = 100
    l2_penalty = 0.0

    x_, y_ = dataset((64, 64), batch_size, epochs).make_one_shot_iterator().get_next()

    x = tf.placeholder(tf.float32, list(x_.shape))
    tf.summary.image(""x"", x, max_outputs=3)

    # Define the Models
    E = encoder(x.shape[1:], latent_dimension, l2_penalty)

    z_ = tf.random_normal([batch_size, latent_dimension], mean=0.0, stddev=1.0)

    z = tf.placeholder(tf.float32, list(z_.shape))
    G = generator(z.shape[1:], x.shape[-1].value, l2_penalty)
    D = discriminator(x.shape[1:], E.output.shape[1:], l2_penalty)

    # Generate from latent representation z
    G_z = G(z)
    tf.summary.image(""G(z)"", G_z, max_outputs=3)

    # encode x to a latent representation
    E_x = E(x)

    G_Ex = G(E_x)
    tf.summary.image(""G(E(x))"", G_Ex, max_outputs=3)

    # plot image difference
    tf.summary.image(
        ""G(E(x)) - x"", tf.norm(G_Ex - x_, axis=3, keepdims=True), max_outputs=3
    )

    # The output of the discriminator is a bs,n,n,value
    # hence flatten all the values of the map and compute
    # the loss element wise
    D_Gz, F_Gz = D(inputs=[G_z, z])
    D_x, F_x = D(inputs=[x, E_x])
    D_Gz = k.layers.Flatten()(D_Gz)
    F_Gz = k.layers.Flatten()(F_Gz)
    D_x = k.layers.Flatten()(D_x)
    F_x = k.layers.Flatten()(F_x)

    ## Discriminator
    d_loss = min_max(D_x, D_Gz)

    ## Generator
    g_loss = bce(D_Gz, tf.constant(1.0))
    # Encoder
    e_loss = bce(D_x, tf.constant(0.0))

    # add regularizations defined in the keras layers
    d_loss += tf.add_n(D.losses)
    e_loss += tf.add_n(E.losses)
    g_loss += tf.add_n(G.losses)

    tf.summary.scalar(""d_loss"", d_loss)
    tf.summary.scalar(""g_loss"", g_loss)
    tf.summary.scalar(""e_loss"", e_loss)

    global_step = tf.train.get_or_create_global_step()

    lr = 1e-4
    tf.summary.scalar(""lr"", lr)

    # Define the D train op
    train_d = tf.train.AdamOptimizer(lr, beta1=0.5).minimize(
        d_loss, var_list=D.trainable_variables
    )

    # Define the G + E train ops (the models can be trained
    # the same step, but separately)
    train_g = tf.train.AdamOptimizer(lr, beta1=0.5).minimize(
        g_loss, global_step=global_step, var_list=G.trainable_variables
    )

    train_e = tf.train.AdamOptimizer(lr, beta1=0.5).minimize(
        e_loss, var_list=E.trainable_variables
    )

    log_dir = f""logs/test""
    summary_op = tf.summary.merge_all()

    scaffold = tf.train.Scaffold()
    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True
    session_creator = tf.train.ChiefSessionCreator(
        config=config, scaffold=scaffold, checkpoint_dir=log_dir
    )

    def _loop(func: Callable) -> None:
        """"""
        Execute func for the specified number of epochs or max_steps.

        Args:
            func: callable to loop

        Returns:
            None.
        """"""
        try:
            while True:
                func()
        except tf.errors.OutOfRangeError:
            pass

    with tf.train.MonitoredSession(
        session_creator=session_creator,
        hooks=[
            tf.train.CheckpointSaverHook(log_dir, save_steps=100, scaffold=scaffold)
            # tf.train.ProfilerHook(save_steps=1000, output_dir=log_dir),
        ],
    ) as sess:
        # Get the summary writer.
        # The rational behind using the writer (from the scaffold)
        # and not using the SummarySaverHook is that we want to log
        # every X steps the output of G, G(E(x)) and x
        # But since we need to use placeholders to feed the same data
        # to G, D and E, we can't use the Hook, because the first
        # sess.run on the data, will trigger the summary save op
        # and the summary save op needs the data from the placeholder
        writer = SummaryWriterCache.get(log_dir)

        def _train():
            # First create the input, that must be shared between the 2
            # training iteration
            real, noise = sess.run([x_, z_])
            feed_dict = {x: real, z: noise}

            # train D
            d_gz_, d_x, _, d_loss_value = sess.run(
                [D_Gz, D_x, train_d, d_loss], feed_dict
            )

            # train G+E
            _, g_loss_value, _, e_loss_value, step = sess.run(
                [train_g, g_loss, train_e, e_loss, global_step], feed_dict
            )

            if step % 100 == 0:
                summaries = sess.run(summary_op, feed_dict)
                print(
                    f""[{step}] d: {d_loss_value} - g: {g_loss_value} - e: {e_loss_value}""
                )
                print(np.mean(d_gz_), np.mean(d_x))
                writer.add_summary(summaries, step)
                writer.flush()

        _loop(_train)
    return 0


if __name__ == ""__main__"":
    sys.exit(train())
```
"
23872,SparseTensor multiplication in tf.keras.layers.Dense,"**System information**
- TensorFlow version (you are using): 1.12.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
In `keras.layers.Dense`, `tf.sparse_tensor_dense_matmul` is called for SparseTensor as [this](https://github.com/keras-team/keras/blob/master/keras/backend/tensorflow_backend.py#L1093-L1094).
But in `tf.keras`, only `tf.tensordot` (which can not treats SparseTensor) is called as [this](https://github.com/tensorflow/tensorflow/blob/f827bad24efc949e657e141464ea1010c18d812c/tensorflow/python/keras/layers/core.py#L955-L972).

Do you have any plan to treat SparseTensor in `tf.keras`?

**Will this change the current api? How?**
We can fix it simply using `tf.sparse_tensor_dense_matmul`.
If needed, I can create PR.
"
23871,[Feature Request] Make some parameter untrainable when training on estimator,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>

**System information**
- TensorFlow version (you are using): 1.10
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
1. In estimator, I am wondering if there are way to make some parameter untrainable when training on estimator.
In session approaches, it's just load tf.trainable_variable() and remove parameters from list before
optimizer process, but I cannot see any features related to those in estimator.

**Will this change the current api? How?**

This is basic features and already implemented on the session method. So, the estimator should support this."
23870,"In implementation of ConvLSTM,I  want to know where is the w_ci dot C_t-1 in paper formula (3)","```

  def call(self, inputs, state, scope=None):
    cell, hidden = state
    new_hidden = _conv([inputs, hidden], self._kernel_shape,
                       4 * self._output_channels, self._use_bias)
    gates = array_ops.split(
        value=new_hidden, num_or_size_splits=4, axis=self._conv_ndims + 1)

    input_gate, new_input, forget_gate, output_gate = gates
    new_cell = math_ops.sigmoid(forget_gate + self._forget_bias) * cell
    new_cell += math_ops.sigmoid(input_gate) * math_ops.tanh(new_input)
    output = math_ops.tanh(new_cell) * math_ops.sigmoid(output_gate)

    if self._skip_connection:
      output = array_ops.concat([output, inputs], axis=-1)
    new_state = rnn_cell_impl.LSTMStateTuple(new_cell, output)
return output, new_state
```

In this implementation, I wonder where is  w_ci dot C_t-1 appearing in paper formula (3)"
23868,Exporting GraphDef from File and using the resulting TFLite model in the TFLite Android App doesn't work.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS High Sierra 10.13.4
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Lenovo Tab 10
- TensorFlow installed from (source or binary): source I think
- TensorFlow version (use command below): 1.11.0
- Python version: 3.6.5
- Bazel version (if compiling from source): 0.16.1
- GCC/Compiler version (if compiling from source): 4.2.1
- CUDA/cuDNN version: I don't have CUDA
- GPU model and memory: I'm not running on a GPU


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

I have downloaded the tensorflow Release 1.12.0.
Using Android Studio, I imported the Tensorflow-Lite sample for Android located in tensorflow/tensorflow/lite/examples/android folder. I edited the Manifest so that only the DetectActivity would be installed and would run. I deployed it in my Lenovo Tab 10 Tablet and it ran well.

I wanted to use my own model, and while checking [this TensorFlow Guide](https://www.tensorflow.org/lite/convert/python_api#exporting_a_graphdef_from_file_), I downloaded the Mobilenet_1.0_224 to see how the conversion would go. As I was using TensorFlow 1.11, my Python Script looked like:

	import tensorflow as tf

	graph_def_file = ""mobilenet_v1_1.0_224/frozen_graph.pb""
	input_arrays = [""input""]
	output_arrays = [""MobilenetV1/Predictions/Softmax""]

	converter = tf.contrib.lite.TocoConverter.from_frozen_graph(
	  graph_def_file, 
	  input_arrays, 
	  output_arrays)
	tflite_model = converter.convert()
	open(""converted_model.tflite"", ""wb"").write(tflite_model)

I got the converted_model.tflite and the labels.txt and copied it to the assets folder. I ran the app again, expecting it to run like before, but I hit an error:

    java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 602112 bytes and a ByteBuffer with 270000 bytes.

The line at fault is this line from TFLiteObejctDetectionAPIModel:
    tfLite.runForMultipleInputsOutputs(inputArray, outputMap);

I had expected the app to run again and use my new model since I'm feeding the converted model from what TensorFlow showed in their example. But it seems that this is not the case.

I also changed the variable TF_OD_API_IS_QUANTIZED to false.

**Code to reproduce the issue**
To reproduce the issue I had encountered, download the TensorFlow-Lite example. Check the manifest and comment out the other activities and just leave the DetectorActivity. Run it and it should run as expected. Now, use the guide linked above to convert a GraphDef to a TFLite example, and then copy the resulting tflite model to the assets folder of the Android Sample code. Update the value of TF_OD_API_IS_QUANTIZED to false, and then run it. 


**Other info / logs**
    java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 602112 bytes and a ByteBuffer with 270000 bytes.
        at org.tensorflow.lite.Tensor.throwExceptionIfTypeIsIncompatible(Tensor.java:175)
        at org.tensorflow.lite.Tensor.setTo(Tensor.java:65)
        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:126)
        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:168)
        at org.tensorflow.demo.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:216)
        at org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:249)
        at android.os.Handler.handleCallback(Handler.java:751)
        at android.os.Handler.dispatchMessage(Handler.java:95)
        at android.os.Looper.loop(Looper.java:154)
        at android.os.HandlerThread.run(HandlerThread.java:61)
"
23867,Document about using custom op to build graph in C++,"**System information**
- TensorFlow version: ALL
- Doc Link: 
Custom op: https://www.tensorflow.org/guide/extend/op
Build graph in c++: https://www.tensorflow.org/guide/extend/cc


**Describe the documentation issue**
Please provide documentation about how to build graph using custom OP.
Related StackOverflow question: https://stackoverflow.com/questions/53384454/how-to-use-custom-op-to-build-tensorflow-graph-in-c
"
23864,I want ot know where is the w_ci dot C_t-1 in paper formulation (3),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23863,SimpleRNN + batch_normalization causes inexplicable InvalidArgumentError,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 22
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0
- Python version: 3.6.5
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**
The test program throws an exception `tensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value` in session run. However, it completes normally if `EPISODE_LENGTH` is halved, the `SimpleRNN` layer is commented out or the `batch_normalization` layer is commented out. The problem also goes away if I replace the `RandomDataset`-derived dataset with one built with `from_tensor_slices` from constant data.

**Describe the expected behavior**
The test program should complete normally. I don't think my program has a bug that is responsible for the exception but even if it does, the throwing of the exception should be consistent and not depend on `EPISODE_LENGTH` being big enough.

**Code to reproduce the issue**
```
#!/usr/bin/env python3

import numpy as np
import tensorflow as tf
import tensorflow.keras.layers as kl

NUM_FEATURES = 2
EPISODE_LENGTH = 32
BATCH_SIZE = 2

class GraphBuilder:
    def __init__(self):
        g = tf.Graph()
        with g.as_default():
            tf.set_random_seed(0)

            with tf.variable_scope('input'):
                ds = tf.data.Dataset.from_tensors(
                    (np.zeros(shape=(BATCH_SIZE, EPISODE_LENGTH, NUM_FEATURES), dtype=np.float32),
                     np.zeros(shape=(BATCH_SIZE, EPISODE_LENGTH), dtype=np.float32)))
                it = ds.make_one_shot_iterator()
                batch = it.get_next(name='batch')
                batch_x, batch_y = batch
            with tf.variable_scope('nn'):
                layer = batch_x
                layer = kl.SimpleRNN(1, return_sequences=True)(layer, training=True)
                layer = tf.layers.batch_normalization(layer, axis=2, training=True)
                batch_pred = layer[:, :, 0]

            with tf.variable_scope('loss'):
                num_rows = tf.reduce_prod(tf.shape(batch_y))
                e = tf.subtract(batch_y, batch_pred, name='error')
                sse = tf.reduce_sum(tf.square(e), name='sse')
                mse = tf.divide(sse, tf.cast(num_rows, tf.float32), name='mse')
                rmse = tf.sqrt(mse, name='rmse')

            with tf.variable_scope('train'):
                optimizer = tf.train.AdamOptimizer(0.001)
                update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
                with tf.control_dependencies(update_ops):
                    train_op = optimizer.minimize(mse)

        self.rmse = rmse
        self.train_op = train_op
        self.graph = g


def main():
    gb = GraphBuilder()
    config = tf.ConfigProto(intra_op_parallelism_threads=2, inter_op_parallelism_threads=2)
    config.device_count['GPU'] = 0
    with tf.Session(graph=gb.graph, config=config) as sess:
        sess.run(tf.global_variables_initializer())
        sess.run([gb.rmse, gb.train_op])

main()
```


**Other info / logs**
```
2018-11-20 13:31:56.998664: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Traceback (most recent call last):
  File ""/data/jchia/venv/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    return fn(*args)
  File ""/data/jchia/venv/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/data/jchia/venv/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""py3/train2.py"", line 56, in <module>
    main()
  File ""py3/train2.py"", line 54, in main
    sess.run([gb.rmse, gb.train_op])
  File ""/data/jchia/venv/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/data/jchia/venv/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/data/jchia/venv/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/data/jchia/venv/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value
```"
23861,"matmul with large matrices fails with float16, but succeeds with float32","- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes. see below

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04

- **TensorFlow installed from (source or binary)**:
This bug occurs with every version of GPU tensorflow I've tried or built, including several binaries and currently building from source.

- **TensorFlow version (use command below)**:
v1.12.0-rc0-2836-gd63e3ea

- **Python version**:
2.7

- **Bazel version (if compiling from source)**:
0.19.1

- **GCC/Compiler version (if compiling from source)**:
5.4.0

- **CUDA/cuDNN version**:
Currently building from source with CUDA 10.0 and cuDNN 7.4, but previously the bug occurred with binaries using CUDA 9.0 and cuDNN 7.4 and cuDNN 7.0

- **GPU model and memory**:
GeForce GTX 1070 8GB

- **Exact command to reproduce**:
Run the following python script:
```
import numpy as np
import tensorflow as tf

xvar = tf.placeholder (tf.float16, name=""x"",shape=[None,2])

weight_init = tf.uniform_unit_scaling_initializer (factor=1,dtype=tf.float16)
W = tf.get_variable (""W"",[2,2],initializer=weight_init,dtype=tf.float16)

output = tf.matmul (xvar,W)

init = tf.global_variables_initializer ()

with tf.Session () as sess:
    sess.run (init)
    o = sess.run (output, feed_dict = {xvar:np.zeros ((8500000,2),dtype=np.float16)})
```

### Describe the problem
Matmul fails with large float16 matrices. If you reduce the size of the matrix to 8e6, the code runs. If you keep the size of the matrix the same, but change all the data types to float32, it runs. The limiting factor seems to be the single largest dimension. For example, doing the matmul with sizes (8e6,100) and (100,100) works in float16

Perhaps this is not a Tensorflow issue, but an internal CUDA issue?

### Source code / logs
```
WARNING:tensorflow:From float16_demo.py:6: __init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.
WARNING:tensorflow:From /home/ilia/tensorflow/venv-source-nightly-2018nov19/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2018-11-19 15:49:01.787213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:993] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-11-19 15:49:01.787660: I tensorflow/compiler/xla/service/service.cc:149] XLA service 0x4863200 executing computations on platform CUDA. Devices:
2018-11-19 15:49:01.787676: I tensorflow/compiler/xla/service/service.cc:157]   StreamExecutor device (0): GeForce GTX 1070, Compute Capability 6.1
2018-11-19 15:49:01.787882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.7465
pciBusID: 0000:01:00.0
totalMemory: 7.92GiB freeMemory: 7.52GiB
2018-11-19 15:49:01.787894: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2018-11-19 15:49:01.788392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-19 15:49:01.788401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2018-11-19 15:49:01.788405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2018-11-19 15:49:01.788537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7316 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-11-19 15:49:03.017490: E tensorflow/stream_executor/cuda/cuda_blas.cc:652] failed to run cuBLAS routine cublasSgemmEx: CUBLAS_STATUS_EXECUTION_FAILED
Traceback (most recent call last):
  File ""float16_demo.py"", line 15, in <module>
    o = sess.run (output, feed_dict = {xvar:np.zeros ((8500000,2),dtype=np.float16)})
  File ""/home/ilia/tensorflow/venv-source-nightly-2018nov19/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/home/ilia/tensorflow/venv-source-nightly-2018nov19/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/ilia/tensorflow/venv-source-nightly-2018nov19/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/home/ilia/tensorflow/venv-source-nightly-2018nov19/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(8500000, 2), b.shape=(2, 2), m=8500000, n=2, k=2
	 [[node MatMul (defined at float16_demo.py:9) ]]

Caused by op u'MatMul', defined at:
  File ""float16_demo.py"", line 9, in <module>
    output = tf.matmul (xvar,W)
  File ""/home/ilia/tensorflow/venv-source-nightly-2018nov19/local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py"", line 2272, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File ""/home/ilia/tensorflow/venv-source-nightly-2018nov19/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 4763, in mat_mul
    name=name)
  File ""/home/ilia/tensorflow/venv-source-nightly-2018nov19/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/home/ilia/tensorflow/venv-source-nightly-2018nov19/local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 512, in new_func
    return func(*args, **kwargs)
  File ""/home/ilia/tensorflow/venv-source-nightly-2018nov19/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3268, in create_op
    op_def=op_def)
  File ""/home/ilia/tensorflow/venv-source-nightly-2018nov19/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1831, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(8500000, 2), b.shape=(2, 2), m=8500000, n=2, k=2
	 [[node MatMul (defined at float16_demo.py:9) ]]
```"
23860,Poor memory performance of K.batch_dot under tensorflow backend relative to batched tf.matmul ,"[ x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

[ x] Check that your version of TensorFlow is up-to-date. The installation instructions can be found here.

[x ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

I am performing batch matrix multiplies of two tensors of size (batch, N, M) and (batch, M, K) to get a tensor of size (batch, N, K), with the matrix products. This behavior can be done with both tf.matmul and K.batch_dot with the default axis arguments.

However in K.batch_dot, the elementwise multiplication in the line

keras/keras/backend/tensorflow_backend.py

Line 1248 in 75a3503

 result = tf.reduce_sum(x * y, 1) 
eats up a lot of memory. The elementwise multiplication followed by summing over an axis is of course mathematically equivalent to the matrix multiply, but in the two-step implementation, Tensorflow assigns memory to the intermediate very large tensor.
In this simple example, my small GPU (Nvidia 970) is able to perform the calculation using tf.matmul, but using K.batch_dot Tensorflow fails with an OOM error.

import numpy as np
import tensorflow as tf
from keras import backend as K

a = np.random.normal(size=(100, 500, 10000)).astype(np.float32)
b = np.random.normal(size=(100, 10000, 32)).astype(np.float32)

a_t = K.placeholder(a.shape)
b_t = K.placeholder(b.shape)

td = tf.matmul(a_t, b_t)
bd = K.batch_dot(a_t, b_t)

sess = K.get_session()
sess.run(td, feed_dict={a_t: a, b_t: b})
sess.run(bd, feed_dict={a_t: a, b_t: b})
This fails when it tries to assign a tensor of size (100, 10000, 500, 32) in the elementwise multiply in batch_dot (the dimension of 10000 not being strictly necessary in this case since we are only interested in the sum)."
23859,Android Bazel Build Error ,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: 
- TensorFlow installed from (source or binary): source
- TensorFlow version: master (5c60fb7e9b90d6641c2b5848773ef49956ed54e3)
- Python version: Python 2.7.15rc1
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.19.1
- GCC/Compiler version (if compiling from source): gcc 7.3.0
- CUDA/cuDNN version:
- GPU model and memory:

I am trying to build `//tensorflow/contrib/android:libtensorflow_inference.so` so I can use some new ops (cos specifically) in my graph, but am getting build errors:
```
ERROR: /home/iwsmith/.cache/bazel/_bazel_iwsmith/df6342feda2c8ba7bff417a851a0ca71/external/llvm/BUILD.bazel:1941:1: C++ compilation of rule '@llvm//:support' failed (Exit 1)
In file included from external/llvm/lib/Support/Path.cpp:1111:                        
external/llvm/lib/Support/Unix/Path.inc:633:9: error: no member named 'futimens' in the global namespace
  if (::futimens(FD, Times))                                                                                                     
      ~~^                                                                                                                                                                             1 error generated.                                                    
Target //tensorflow/contrib/android:libtensorflow_inference.so failed to build
```
NDK: r17c
SDK: 28
```iwsmith@bob:~/sdk/tools$ bin/sdkmanager --list                                                                                                       
Warning: File /home/iwsmith/.android/repositories.cfg could not be loaded.                                                                                           
Installed packages:=====================] 100% Computing updates...                                                                                  
  Path                 | Version | Description                       | Location                                                                      
  -------              | ------- | -------                           | -------                                                                        
  build-tools;28.0.3   | 28.0.3  | Android SDK Build-Tools 28.0.3    | build-tools/28.0.3/                                                          
  platform-tools       | 28.0.1  | Android SDK Platform-Tools 28.0.1 | platform-tools/                                                                
  platforms;android-28 | 6       | Android SDK Platform 28           | platforms/android-28/                                                        
  tools                | 26.1.1  | Android SDK Tools 26.1.1          | tools/    
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. `git clone --recurse-submodules https://github.com/tensorflow/tensorflow.git`
1. `./configure`
1. `bazel clean`
1. `bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cxxopt=-std=c++11    --cpu=armeabi-v7a`

**Any other info / logs**
I have tried with a variety of NDK versions (18b, 14b) and get different build errors each time. I also tried with `--cpu=arm64-v8a` and got build errors. I am unsure which cpu architecture to target for the Pixel.
"
23858,"Key Variable not found in checkpoint      [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_INT32], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]","How should I fix this issue? --the checkpoint is correct.

```
def image_filling(image, mask, args):
    with tf.Graph().as_default():
        dummy = tf.Variable(0)  # dummy variable !!!
        with tf.Session() as sess:
            sess.run(tf.global_variables_initializer())
            saver = tf.train.Saver()
            saver.restore(sess, ""image_completion/gan.ckpt"")

```

Error is:

```
$ python completion_DCGAN.py 
2018-11-19 03:36:55.472699: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-11-19 03:36:55.641138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: 
name: Tesla P100-PCIE-12GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:02:00.0
totalMemory: 11.91GiB freeMemory: 11.63GiB
2018-11-19 03:36:55.641170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-11-19 03:36:55.928819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-19 03:36:55.928849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 
2018-11-19 03:36:55.928855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N 
2018-11-19 03:36:55.929099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11251 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-12GB, pci bus id: 0000:02:00.0, compute capability: 6.0)
2018-11-19 03:36:56.139821: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 28. Tune using inter_op_parallelism_threads for best performance.
2018-11-19 03:36:56.162713: W tensorflow/core/framework/op_kernel.cc:1275] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key Variable not found in checkpoint
Traceback (most recent call last):
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1278, in _do_call
    return fn(*args)
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1263, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1350, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.NotFoundError: Key Variable not found in checkpoint
     [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_INT32], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1725, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 877, in run
    run_metadata_ptr)
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1272, in _do_run
    run_metadata)
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Key Variable not found in checkpoint
     [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_INT32], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

Caused by op 'save/RestoreV2', defined at:
  File ""completion_DCGAN.py"", line 299, in <module>
    image_filling(real_batch, mask, args)
  File ""completion_DCGAN.py"", line 267, in image_filling
    saver = tf.train.Saver()
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1281, in __init__
    self.build()
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1293, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1330, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 778, in _build_internal
    restore_sequentially, reshape)
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 397, in _AddRestoreOps
    restore_sequentially)
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 829, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1463, in restore_v2
    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 454, in new_func
    return func(*args, **kwargs)
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3155, in create_op
    op_def=op_def)
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

NotFoundError (see above for traceback): Key Variable not found in checkpoint
     [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_INT32], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1737, in restore
    checkpointable.OBJECT_GRAPH_PROTO_KEY)
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 351, in get_tensor
    status)
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 519, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""completion_DCGAN.py"", line 299, in <module>
    image_filling(real_batch, mask, args)
  File ""completion_DCGAN.py"", line 268, in image_filling
    saver.restore(sess, ""image_completion/gan.ckpt"")
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1743, in restore
    err, ""a Variable name or other graph key that is missing"")
tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

Key Variable not found in checkpoint
     [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_INT32], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

Caused by op 'save/RestoreV2', defined at:
  File ""completion_DCGAN.py"", line 299, in <module>
    image_filling(real_batch, mask, args)
  File ""completion_DCGAN.py"", line 267, in image_filling
    saver = tf.train.Saver()
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1281, in __init__
    self.build()
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1293, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1330, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 778, in _build_internal
    restore_sequentially, reshape)
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 397, in _AddRestoreOps
    restore_sequentially)
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 829, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1463, in restore_v2
    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 454, in new_func
    return func(*args, **kwargs)
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3155, in create_op
    op_def=op_def)
  File ""/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

NotFoundError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

Key Variable not found in checkpoint
     [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_INT32], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
```

I have the following:
```
Tensorflow 1.10
Python3.6.2
$ uname -a
Linux scc1 2.6.32-696.28.1.el6.x86_64 #1 SMP Wed May 9 23:09:02 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
$ lsb_release -a
LSB Version:	:base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch
Distributor ID:	CentOS
Description:	CentOS release 6.4 (Final)
Release:	6.4
Codename:	Final
```
"
23856,GPU there still training on CPU,"./deviceQuery Starting...

 CUDA Device Query (Runtime API) version (CUDART static linking)

Detected 1 CUDA Capable device(s)

Device 0: ""GeForce 940MX""
  CUDA Driver Version / Runtime Version          9.0 / 9.0
  CUDA Capability Major/Minor version number:    5.0
  Total amount of global memory:                 2003 MBytes (2100232192 bytes)
  ( 3) Multiprocessors, (128) CUDA Cores/MP:     384 CUDA Cores
  GPU Max Clock rate:                            1242 MHz (1.24 GHz)
  Memory Clock rate:                             1001 Mhz
  Memory Bus Width:                              64-bit
  L2 Cache Size:                                 1048576 bytes
  Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)
  Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  2048
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
  Maximum memory pitch:                          2147483647 bytes
  Texture alignment:                             512 bytes
  Concurrent copy and kernel execution:          Yes with 1 copy engine(s)
  Run time limit on kernels:                     Yes
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Disabled
  Device supports Unified Addressing (UVA):      Yes
  Supports Cooperative Kernel Launch:            No
  Supports MultiDevice Co-op Kernel Launch:      No
  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0
  Compute Mode:
     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >

deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 9.0, CUDA Runtime Version = 9.0, NumDevs = 1
Result = PASS

**this shows my cuda is ok**

cudnnGetVersion() : 7401 , CUDNN_VERSION from cudnn.h : 7401 (7.4.1)
Host compiler version : GCC 5.4.0
There are 1 CUDA capable devices on your machine :
device 0 : sms  3  Capabilities 5.0, SmClock 1241.5 Mhz, MemSize (Mb) 2002, MemClock 1001.0 Mhz, Ecc=0, boardGroupID=0
Using device 0

Testing single precision
Loading image data/one_28x28.pgm
Performing forward propagation ...
Testing cudnnGetConvolutionForwardAlgorithm ...
Fastest algorithm is Algo 1
Testing cudnnFindConvolutionForwardAlgorithm ...
^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.028768 time requiring 0 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.044288 time requiring 3464 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.054624 time requiring 57600 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.197760 time requiring 207360 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.501696 time requiring 2057744 memory
Resulting weights from Softmax:
0.0000000 0.9999399 0.0000000 0.0000000 0.0000561 0.0000000 0.0000012 0.0000017 0.0000010 0.0000000 
Loading image data/three_28x28.pgm
Performing forward propagation ...
Resulting weights from Softmax:
0.0000000 0.0000000 0.0000000 0.9999288 0.0000000 0.0000711 0.0000000 0.0000000 0.0000000 0.0000000 
Loading image data/five_28x28.pgm
Performing forward propagation ...
Resulting weights from Softmax:
0.0000000 0.0000008 0.0000000 0.0000002 0.0000000 0.9999820 0.0000154 0.0000000 0.0000012 0.0000006 

Result of classification: 1 3 5

Test passed!

Testing half precision (math in single precision)
Loading image data/one_28x28.pgm
Performing forward propagation ...
Testing cudnnGetConvolutionForwardAlgorithm ...
Fastest algorithm is Algo 1
Testing cudnnFindConvolutionForwardAlgorithm ...
^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.028288 time requiring 0 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.038432 time requiring 3464 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.057536 time requiring 28800 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.199968 time requiring 207360 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.498144 time requiring 2057744 memory
Resulting weights from Softmax:
0.0000001 1.0000000 0.0000001 0.0000000 0.0000563 0.0000001 0.0000012 0.0000017 0.0000010 0.0000001 
Loading image data/three_28x28.pgm
Performing forward propagation ...
Resulting weights from Softmax:
0.0000000 0.0000000 0.0000000 1.0000000 0.0000000 0.0000714 0.0000000 0.0000000 0.0000000 0.0000000 
Loading image data/five_28x28.pgm
Performing forward propagation ...
Resulting weights from Softmax:
0.0000000 0.0000008 0.0000000 0.0000002 0.0000000 1.0000000 0.0000154 0.0000000 0.0000012 0.0000006 

Result of classification: 1 3 5

Test passed!
 **this shows my cudnn is ok**


[0] GeForce 940MX    | 51'C,  92 % |  1904 /  2002 MB | yuvraj(1699M) root(94M) yuvraj(55M) yuvraj(43M)
 this is command gpustat during training 

 nvidia-smi
Mon Nov 19 21:54:09 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.130                Driver Version: 384.130                   |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce 940MX       Off  | 00000000:01:00.0 Off |                  N/A |
| N/A   61C    P0    N/A /  N/A |   1927MiB /  2002MiB |     91%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0       942      G   /usr/lib/xorg/Xorg                           103MiB |
|    0      1749      G   compiz                                        57MiB |
|    0      2278      G   ...uest-channel-token=13477521418036853922    45MiB |
|    0      3726      C   /home/yuvraj/anaconda3/envs/tf/bin/python   1699MiB |
|    0      3873      C   /usr/lib/libreoffice/program/soffice.bin      17MiB |
+-----------------------------------------------------------------------------+ 

this is nvidia-smi during training as we can see the load is 90% but training is on CPU as CPU load is also 90% and the training time was similar to that of cpu like before i installed GPU version i had CPU version and the training time was similar i was doing it on mnist dataset with batch size 32 and it was taking 40-45sec per epoch so idk where i have done wrong please help !!!





"
23855,Tf Lite only support 4D(or less) tensors stack op?,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, that is a custom model
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below):1.12-gpu
- Python version:3.6
- Bazel version (if compiling from source):0.19
- GCC/Compiler version (if compiling from source):5.4
- CUDA/cuDNN version:9.0/7.3.1
- GPU model and memory:GTX 1080 ti

**Describe the current behavior**
I created a custom TensorFlow model and converted it tflite with 
`converter = tf.lite.TFLiteConverter.from_session(sess, [img], [out])`
`tflite_model = converter.convert()`
`open(""converted_model.tflite"", ""wb"").write(tflite_model).`
The process was fine. Then I tried to test the file with interpreter. But when I run
`interpreter = tf.lite.Interpreter(model_path=""converted_model.tflite"")`
 `interpreter.allocate_tensors()`
I get the following runtime error:
`tensorflow/contrib/lite/kernels/pack.cc:38 NumDimensions(input0) < 4 was not true.Node number 14 (PACK) failed to prepare.`

**Code to reproduce the issue**
`z = tf.stack([x, y], axis=3), where x and y both are 4D tensor.` 
What can I do to get the interpreter run? Any help will be appreciated.
"
23854,Error using Estimator export_saved_model (Not found: Key global_step not found in checkpoint),"Hello,

I am trying to prepare a pre-trained model for google cloud ML. I am trying to use an estimator to export the model. During the loading of the checkpoints by the estimator I get the following error:

```
2018-11-19 13:28:57.526564: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key global_step not found in checkpoint
Traceback (most recent call last):
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1334, in _do_call
    return fn(*args)
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.NotFoundError: Key global_step not found in checkpoint
         [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]
```

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: Tesla K80

**Code to reproduce the issue**

```
MODEL_DIR='model/'
def decode_image(image_bytes):
    image = tf.image.decode_image(image_bytes)
    image = tf.cast(image, dtype=tf.uint8)
    return image

def serving_input_fn():
    createmodel()
    inputs = {'image_bytes': tf.placeholder(tf.string, shape=(), name=""image_bytes"")}
    imagebytes = tf.squeeze(inputs['image_bytes']) # make it a scalar
    image = decode_image(imagebytes)
    # make the outer dimension unknown (and not 1)
    image = tf.placeholder_with_default(image, shape=[None, None, None, 3])

    features = {'image_bytes' : image}
    return tf.estimator.export.ServingInputReceiver(features, inputs)

def model_fn(features, labels, mode, params):
    pred = tf.get_default_graph().get_tensor_by_name(""fc1_voc12:0"")
    return tf.estimator.EstimatorSpec(
        mode=tf.estimator.ModeKeys.PREDICT,
        predictions=pred,
        export_outputs={'pred':tf.estimator.export.PredictOutput(pred)}
        )

estimator = tf.estimator.Estimator(
    model_fn=model_fn,
    model_dir=MODEL_DIR)

estimator.export_savedmodel('deployment_gcp_1', serving_input_fn, strip_default_attrs=True)
```

I have searched this issue quite a bit. There was one bug report for an older version of tensorflow (I think 1.2.0 but I am not sure now). I am able to load and save this model using tf.saved_model.simple_save, and it works when I run predictions on it.

I am not sure if this is a bug or if I am missing something really simple.
"
23853,Graph optimized using tf.contrib.tensorrt is not loadable with TF_GraphImportGraphDef,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source.
- TensorFlow version (use command below): v1.12
- Python version: 2.7.12

- Bazel version (if compiling from source): 0.19.0
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version: 9.0/7.0.5
- GPU model and memory: 1080 Ti

**Describe the current behavior**

I optimize a TensorFlow graph with
```
    precision_mode = 'FP32'  # ""FP32"",""FP16"" or ""INT8""
    graph_def = trt.create_inference_graph(
        input_graph_def=graph_def,
        outputs=output_node_names,
        max_batch_size=num_cameras,
        max_workspace_size_bytes=4*10**9,
        precision_mode=precision_mode,
        minimum_segment_size=10,  # minimum number of nodes in an engine,
    )
```

, save the resulting graph, and try to load it in a C++ program using C API.

First, I call

```
TF_LoadLibrary(""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tensorrt/python/ops/_trt_engine_op.so"", status)
```

and call `TF_GraphImportGraphDef` with the optimized graph.

I get the following error:

```
TF_GraphImportGraphDef: No shape inference function exists for op 'TRTEngineOp', did you forget to define it?
```

**Describe the expected behavior**

The call to `TF_GraphImportGraphDef` must succeed.

**Code to reproduce the issue**

It seems that the issue, although not in the bug tracker, should be already known to the authors: https://github.com/tensorflow/tensorflow/blob/v1.12.0/tensorflow/contrib/tensorrt/ops/trt_engine_op.cc#L46
However, I can make a minimal example to reproduce the problem on demand.

**Other info / logs**

It is a pain that a TRT-optimized graph cannot be used outside of python now.
I would be happy to know about a workaround, in case one exists.
"
23852,reduce_max returns wrong shape,"**Problem**
**tf.reduce_max returns tensor with wrong shape**

    ...
    left_expr = tf.reduce_sum(left_encs, axis=1)
    left_expr = tf.Print(left_expr,  [tf.shape(left_encs), tf.shape(left_expr)], message=""Shapes Are: "")
    ...

and the output is:

    Shapes Are: [46 21 512][1 512]

the tensor `left_encs` is the output of transformer encoder, the meaning of the shape is [batch_size, sequence_length, hidden_dim]

I have tried to construct a random tensor with same shape as `left_encs` and the call `tf.reduce_sum(left_encs, axis=1)` and the output is as expected `[46, 512]`.

but I cannot get it right from the real code.

is there any operation should I do before calling `tf.reduce_sum`?

**System information**
- TensorFlow installed from: Anaconda
- TensorFlow version: v1.11
- Python version: 3.6
- CUDA/cuDNN version: v9.0
- GPU model and memory: Tesla K40

"
23851,Error in running pre-compiled binary on tensorflow,"
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 (64-bit)
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
TensorFlow installed from (source or binary): Source
TensorFlow version:1.7 - 1.9
Python version:3.6
Installed using virtualenv? pip? conda?: Git
Bazel version (if compiling from source):No
GCC/Compiler version (if compiling from source): CMake : 3.10.1
CUDA/cuDNN version: No
GPU model and memory: No
Describe the problem
I am trying to run the pre-compiled tensor flow binaries and run the tensorflow example.
I know the support for cmake have been depreciated but i am using  an old version of tensorflow
so need some help in regards.

Provide the exact sequence of commands / steps that you executed before running into the problem
1. Downloading the pre-compiled binary for windows.
2. Included the project in qt-creator
3. Build the project

Any other info / logs:
main.obj : error LNK2019: unresolved external symbol ""public: virtual __cdecl tensorflow::ConfigProto::~ConfigProto(void)"" (??1ConfigProto@tensorflow@@UEAA@XZ) referenced in function ""public: __cdecl tensorflow::SessionOptions::~SessionOptions(void)"" (??1SessionOptions@tensorflow@@QEAA@XZ)
main.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::SessionOptions::SessionOptions(void)"" (??0SessionOptions@tensorflow@@QEAA@XZ) referenced in function main
main.obj : error LNK2019: unresolved external symbol ""class tensorflow::Status __cdecl tensorflow::NewSession(struct tensorflow::SessionOptions const &,class tensorflow::Session * *)"" (?NewSession@tensorflow@@YA?AVStatus@1@AEBUSessionOptions@1@PEAPEAVSession@1@@Z) referenced in function main
debug\tensorflowcppNew.exe : fatal error LNK1120: 4 unresolved externals"
23849, How to get coordinates on the screen frame ？,"I ran the Android example of TensorFlow successfully, but I want to know that there is a blue box in the TFL Detect application. I want to know which method is used to get the coordinates of the box. Which class or method is the specific Android code? Who can help me? Thank you."
23848,Object detection example for TFLite,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.12.0
- Are you willing to contribute it (Yes/No):Yes



**Describe the feature and the current behavior/state.**
I want to add an example for converting an object detection model say, ssd_mobilenet_v1 to TFLite. The code will have a Makefile for setting up everything and a jupyter notebook for showing step by step procedure for conversion.
**Will this change the current api? How?**
No
**Who will benefit with this feature?**
I see that a lot of people ask questions around this as they don't follow the correct procedure. Even I have invested a lot of time in it and I think that a working example can help them in saving a lot of time.
**Any Other info.**
After that, I can even write script for running it on Raspberry Pi 3."
23847,~40% performance decrease since Tensorflow 1.9 when training large models,"**System information**
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.9.0-0-g25c197e 1.9.0
- Python version: 3
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: 9.0.176/7.1.4.18
- GPU model and memory: 8 x Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53

**Describe the current behavior**
I train a model using slim in tensorflow models with the flowers dataset and the large nasnet. This were run on the official tensorflow docker images.
On my Tesla V100 for tensorflow 1.8.0-devel-gpu-py3 I got around 
`INFO:tensorflow:global_step/sec: 1.33334`
With tensorflow 1.9.0-devel-gpu-py3 a maximum at
`INFO:tensorflow:global_step/sec: 0.89946`
With the current tensorflow 1.12.0-devel-gpu-py3 a maximum at
`INFO:tensorflow:global_step/sec: 0.900003`

**Describe the expected behavior**
There should not be an performance decrease in this drastic 

**Code to reproduce the issue**
```shell
#!/usr/bin/env bash
[ ! -d ""models"" ] && git clone https://github.com/tensorflow/models.git
cd models/research/slim/
MODEL_NAME=""nasnet_large""
TRAIN_DIR=/tmp/flowers-models/${MODEL_NAME}
DATASET_DIR=/tmp/flowers
rm -rf ""$TRAIN_DIR""
export CUDA_DEVICE_ORDER=""PCI_BUS_ID""
export CUDA_VISIBLE_DEVICES=""0,1""

python download_and_convert_data.py \
    --dataset_name=flowers \
    --dataset_dir=${DATASET_DIR}

python train_image_classifier.py \
    --train_dir=${TRAIN_DIR}/all \
    --dataset_name=flowers \
    --dataset_split_name=train \
    --dataset_dir=${DATASET_DIR} \
    --model_name=${MODEL_NAME} \
    --max_number_of_steps=500 \
    --batch_size=16 \
    --learning_rate=0.015 \
    --save_interval_secs=60000 \
    --save_summaries_secs=60 \
    --log_every_n_steps=10 \
    --optimizer=rmsprop \
    --num_preprocessing_threads 4 \
    --num_readers 8 \
    --moving_average_decay=0.9999 \
    --weight_decay=0.00005 \
    --learning_rate_decay_type=exponential \
    --learning_rate_decay_factor=0.97 \
    --label_smoothing=0.1
```

**Other info / logs**
I thought this issue #20843 is related to the problem but I have done a git bisect between the `v1.8.0` and the `v1.9.0` tag and running the script above for every commit and found the one that introduce the largest drop in performance: d4976f7. This commit enabled the layout optimizer by default for all gpu cluster. As a workaround and a proof I disabled the layout optimizer with the following config:
```python
rewrite_options = rewriter_config_pb2.RewriterConfig(layout_optimizer=rewriter_config_pb2.RewriterConfig.OFF)
graph_options = tf.GraphOptions(rewrite_options=rewrite_options)
config = tf.ConfigProto(graph_options=graph_options) 
```  
And most of the performance were back again in `tensorflow 1.9.0-devel-gpu-py3`:
`INFO:tensorflow:global_step/sec: 1.18333`
And in the latest release `tensorflow 1.12.0-devel-gpu-py3`:
`INFO:tensorflow:global_step/sec: 1.23337`
[Here](https://gist.github.com/DavidWiesner/c6af3238a90b3a8b1209543a9f8d7127#file-git-bisect-log) is a full log of my bisect script. [One another log](https://gist.github.com/DavidWiesner/c6af3238a90b3a8b1209543a9f8d7127#file-git-bisect-layout_optimizer-on-off-log) shows the influence of disabling the layout-optimizer in commit d4976f7 (that introducing the drop in performance) and some release versions of tensorflow. 
Maybe @yaozhang can answer why the layout optimizer was disabled for some architectures until 1.8.0 in f0d1abbf2 and why @zhangyaobit enabled for all architectures since 1.9.0 in d4976f7
"
23846,Distributed TensorFlow hangs at the end of one iteration when using DataSet api,"Training epoch by epoch, the program will hangs on some random nodes forever at the end of one iteration, namely the end of one epoch. It appears certainly when using `DataSet` api, while it is fine when using old `Queue` based data api. I have set `inter_op_parallelism_threads=1, intra_op_parallelism_threads=1`. @mrry 

Env:
  Distributed running on Yarn.
  Node information: CentOS, linux kernel 3.10.0. Only CPU.
  TensorFlow: r1.12, built from source.

Demo(It's independent of the model. It's easier to reproduce by using more worker nodes. Here, I use 50 worker nodes and 20 ps nodes.)
```python
flags.DEFINE_string(""ps_hosts"", """", ""Comma-separated list of hostname:port pairs"")
flags.DEFINE_string(""worker_hosts"", """", ""Comma-separated list of hostname:port pairs"")
flags.DEFINE_string(""job_name"", """", ""One of 'ps', 'worker'"")
flags.DEFINE_integer(""task_index"", 0, ""Index of task within the job"")

FLAGS(sys.argv)

class Job(object):
    def __init__(self):
        self.ps_hosts = FLAGS.ps_hosts.split(',')
        self.worker_hosts = FLAGS.worker_hosts.split(',')
        self.job_name = FLAGS.job_name
        self.task_index = FLAGS.task_index
        self.cluster = tf.train.ClusterSpec({'ps': self.ps_hosts, 'worker': self.worker_hosts})
        self.server = tf.train.Server(self.cluster, job_name=self.job_name, task_index=self.task_index)
        self.is_chief = (self.task_index == 0 and self.job_name == 'worker')
        worker_prefix = '/job:worker/task:%s' % self.task_index
        self.cpu_device = '%s/cpu:0' % worker_prefix
        self.param_server_device = tf.train.replica_device_setter(
            worker_device=self.cpu_device, cluster=self.cluster,
            ps_strategy=tf.contrib.training.GreedyLoadBalancingStrategy(len(self.ps_hosts), tf.contrib.training.byte_size_load_fn))
        self.num_ps = self.cluster.num_tasks('ps')
        self.num_worker = self.cluster.num_tasks('worker')

    def data_iter(self, batch_size=1000, file_pattern='./input/part-*'):
        def _parse_function(examples):
            features = {}
            features['label'] = tf.FixedLenFeature([], tf.float32)
            features['user_id'] = tf.FixedLenFeature([1], tf.int64)
            features['item_id'] = tf.FixedLenFeature([1], tf.int64)
            instance = tf.parse_example(examples, features)
            return instance['label'], instance['user_id'], instance['item_id']

        with tf.name_scope('input'):
            files = tf.data.Dataset.list_files(file_pattern)
            dataset = files.apply(tf.contrib.data.parallel_interleave(
                        lambda file: tf.data.TFRecordDataset(file),
                        cycle_length=1, sloppy=True))
            dataset = dataset.prefetch(buffer_size=batch_size*2)
            dataset = dataset.batch(batch_size)
            dataset = dataset.map(_parse_function, num_parallel_calls=1)
            iterator = dataset.make_initializable_iterator()
            return iterator

    def model(self, user_id, item_id):
        user_embedding_variable = tf.get_variable('user_emb_var', [1000000, 32], initializer=tf.random_uniform_initializer(minval=-0.5, maxval=0.5, dtype=tf.float32))
        item_embedding_variable = tf.get_variable('user_emb_var', [500000, 32], initializer=tf.random_uniform_initializer(minval=-0.5, maxval=0.5, dtype=tf.float32))
        user_embedding = tf.nn.embedding_lookup(user_embedding_variable, user_id)
        item_embedding = tf.nn.embedding_lookup(item_embedding_variable, item_id)
        user_embedding = tf.reshape(user_embedding, [-1, 32])
        item_embedding = tf.reshape(item_embedding, [-1, 32])
        cross = tf.reduce_sum(user_embedding * item_embedding, 1, keep_dims=True)
        bias = tf.get_variable('bias', initializer=tf.constant(np.zeros((1), dtype=np.float32)), dtype=tf.float32)
        layer = cross + bias
        weight_np = np.zeros((1, 2), dtype=np.float32)
        weight_np[:, 1] = 1
        weight = tf.get_variable('weight', initializer=tf.constant(weight_np), dtype=tf.float32, trainable=False)
        logits = tf.matmul(layer, weight)
        return logits

    def train(self):
        if self.job_name == 'ps':
            with tf.device('/cpu:0'):
                self.server.join()
        elif self.job_name == 'worker':
            with tf.Graph().as_default():
                with tf.device(self.param_server_device):
                    train_iterator = self.data_iter()
                    train_label, train_user_id, train_item_id = train_iterator.get_next()
                    train_logit = self.model(train_user_id, train_item_id)
                    train_label = tf.to_int64(train_label)
                    train_cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=train_logit, labels=train_label)
                    train_loss = tf.reduce_mean(train_cross_entropy, name='loss')
                    opt = tf.train.AdamOptimizer(learning_rate=0.001)
                    train_op = opt.minimize(train_loss)
                    saver = tf.train.Saver()

                    sess_config = tf.ConfigProto(allow_soft_placement=True,
                        log_device_placement=False,
                        device_filters=[""/job:ps"", ""/job:%s/task:%d"" % (self.job_name, self.task_index)],
                        operation_timeout_in_ms=60000,
                        inter_op_parallelism_threads=1,
                        intra_op_parallelism_threads=1)
                    with tf.train.MonitoredTrainingSession(master=self.server.target,
                                                           is_chief=self.is_chief,
                                                           config=sess_config) as sess:
                        epoch_num = 0
                        while epoch_num < 10:
                            epoch_num += 1
                            sess.run(train_iterator.initializer)
                            while True:
                                try:
                                    sess.run(train_op)
                                except tf.errors.OutOfRangeError:
                                    saver.save(sess=sess._sess._sess._sess._sess,
                                            save_path=""some_hdfs_path/model.checkpoint.""+str(epoch_num),
                                            latest_filename='checkpoint.'+str(epoch_num))
                                    break

def main(_):
    job = Job()
    job.train()

if __name__ == '__main__':
    tf.app.run()
```

Fully pstack: 
  https://github.com/formath/TensorFlow-Bugs/blob/master/39024.pstack

"
23845,Some confusion about Gather OP implement,"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/gather_functor.h#L137
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/gather_functor.h#L139

In TensorFlow master branch, gather_functor::GatherFunctorCPU template specialization when slice_size==10 or slice_size==20, why this two magic number?"
23844,ERROR: missing input file '@local_config_nccl//:nccl/NCCL-SLA.txt' on Ubuntu18.4LTS,"<em>This is a build/installation issue.</em>


**_System information_**
 OS Platform and Distribution (Linux Ubuntu 18.04):
-
- TensorFlow installed from source 
- TensorFlow version: 1.8.0
- Python version:3.6
- Installed using virtualenv? pip? conda?: PIP
- Bazel version (if compiling from source):0.14
- GCC/Compiler version (if compiling from source):7.3.0
- CUDA/cuDNN version:7.1.4
- GPU model and memory:GeForce GT 710B-


**_Problem description_**

I tried installing Tensorflow on Ubuntu 18.4 -LTS following instructions from [(https://medium.com/@Oysiyl/install-tensorflow-1-8-0-with-gpu-from-source-on-ubuntu-18-04-bionic-beaver-35cfa9df3600 )] (https://medium.com/@Oysiyl/install-tensorflow-1-8-0-with-gpu-from-source-on-ubuntu-18-04-bionic-beaver-35cfa9df3600 ) BUT BUILD NOT SUCCESSFUL and WITH ERRORS

_**Exact sequence of commands / steps that I executed before running into the problem**_
-
**Step 1:** Updated and upgraded the system : 
`
    sudo apt-get update 
    sudo apt-get upgrade
`

**Step 2 :** Verified having CUDA capable GPU : 
`    lspci | grep -i nvidia`

**Step 3:** Verified 64bit version of lnux : 
   ` uname -m && cat /etc/*release`

**Step 4:**  Installed dependencies : 
   ` sudo apt-get install build-essential 
    sudo apt-get install cmake git unzip zip
    sudo add-apt-repository ppa:deadsnakes/ppa
    sudo apt-get update
    sudo apt-get install python2.7-dev python3.5-dev python3.6-dev pylint`

**Step 5:** Installed Linux 4.16 kernel : 
    `uname -r
    wget http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.16/linux-headers-4.16.0-041600_4.16.0-041600.201804012230_all.deb
    wget http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.16/linux-headers-4.16.0-041600-generic_4.16.0-041600.201804012230_amd64.deb
    wget http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.16/linux-image-4.16.0-041600-generic_4.16.0-041600.201804012230_amd64.deb
    sudo dpkg -i *.deb
    uname -sr`

**Step 6:**  Installed nVidia CUDA 9.2 : 
    `sudo apt-get purge nvidia*
    sudo apt-get autoremove
    sudo apt-get autoclean
    sudo rm -rf /usr/local/cuda*`

    sudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1710/x86_64/7fa2af80.pub
    echo ""deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1710/x86_64 /"" | sudo tee /etc/apt/sources.list.d/cuda.list
    sudo apt-get update 
    sudo apt-get -o Dpkg::Options::=""--force-overwrite"" install cuda-9-2 cuda-drivers
`
**Step 7 & 8 :** After rebooting, 

    `echo 'export PATH=/usr/local/cuda-9.2/bin${PATH:+:${PATH}}' >> ~/.bashrc
     echo 'export LD_LIBRARY_PATH=/usr/local/cuda-9.2/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}' >> ~/.bashrc
    source ~/.bashrc
    sudo ldconfig
    nvidia-smi`
  
    Created xorg file for nvidia resolution settings

    `sudo nvidia-xconfig
     nvidia-settings`

**Step 9 :** Installed CuDNN 7.1.4 

   ` tar -xf cudnn-9.2-linux-x64-v7.1.tgz
    sudo cp -R cuda/include/* /usr/local/cuda-9.2/include
    sudo cp -R cuda/lib64/* /usr/local/cuda-9.2/lib64`

**Step 10 :** Installed NCCL 2.2.13

   ` tar -xf nccl_2.2.13-1+cuda9.2_x86_64.txz
    cd nccl_2.2.13-1+cuda9.2_x86_64
    sudo cp -R * /usr/local/cuda-9.2/targets/x86_64-linux/
    sudo ldconfig`

**Step 11 :** Installed dependencies 

    `sudo apt-get install libcupti-dev
    echo 'export LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH' >> 
    ~/.bashrc
    sudo apt-get install python3-numpy python3-dev python3-pip python3-wheel`

**Step 12 :** Configured Tensorflow from source 

   ` cd ~/
    wget https://github.com/bazelbuild/bazel/releases/download/0.14.0/bazel-0.14.0-installer-linux-x86_64.sh
    chmod +x bazel-0.14.0-installer-linux-x86_64.sh
    ./bazel-0.14.0-installer-linux-x86_64.sh --user
    echo 'export PATH=""$PATH:$HOME/bin""' >> ~/.bashrc`
(Reloading environment variables) : 
   ` source ~/.bashrc
    sudo ldconfig`

(Built Tensorflow 1.8.0 )
    `cd ~/
    git clone https://github.com/tensorflow/tensorflow.git
    cd tensorflow
    git pull
    git checkout r1.8
    ./configure`
**Step 13 :** Building Tensorflow with Bazel : 

  ->upto this point everything went fine.

   `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package  --config-monolothic`

->It took nearly 5 hours for this but finally failed to build successfully.-


**_Errors :_** 

1)` ERROR: missing input file '@local_config_nccl//:nccl/NCCL-SLA.txt'`

2) `ERROR: /home/vikranth/tensorflow/tensorflow/tools/pip_package/BUILD:166:1: //tensorflow/tools/pip_package:build_pip_package: missing input file '@local_config_nccl//:nccl/NCCL-SLA.txt'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.`

3) `ERROR: /home/vikranth/tensorflow/tensorflow/tools/pip_package/BUILD:166:1 1 input file(s) do not exist
INFO: Elapsed time: 222.836s, Critical Path: 25.63s
INFO: 386 processes, local.`

4)` FAILED: Build did NOT complete successfully`




**

> 

- **Other info / logs**

**

Updated log(re-running the command : 
   `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package  --config-monolothic`
)


**_Terminal Output :_** 
___________________________________________________________________________________

`vikranth@MySys:~$ cd tensorflow`

`vikranth@MySys:~/tensorflow$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --config=monolithic`

`Starting local Bazel server and connecting to it...`
`................`
WARNING: `/home/vikranth/.cache/bazel/_bazel_vikranth/31056e930c01d2cff6d6b0198442019b/external/grpc/BUILD:1943:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/vikranth/.cache/bazel/_bazel_vikranth/31056e930c01d2cff6d6b0198442019b/external/grpc/bazel/grpc_build_system.bzl:172:12`
WARNING: `/home/vikranth/.cache/bazel/_bazel_vikranth/31056e930c01d2cff6d6b0198442019b/external/grpc/BUILD:1943:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/vikranth/.cache/bazel/_bazel_vikranth/31056e930c01d2cff6d6b0198442019b/external/grpc/bazel/grpc_build_system.bzl:172:12`
WARNING: `/home/vikranth/.cache/bazel/_bazel_vikranth/31056e930c01d2cff6d6b0198442019b/external/grpc/BUILD:1943:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/vikranth/.cache/bazel/_bazel_vikranth/31056e930c01d2cff6d6b0198442019b/external/grpc/bazel/grpc_build_system.bzl:172:12`
WARNING: 
`/home/vikranth/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.`
WARNING: 
`/home/vikranth/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (286 packages loaded).
INFO: Found 1 target...
INFO: From Compiling tensorflow/contrib/lite/util.cc [for host]:
tensorflow/contrib/lite/util.cc: In function 'TfLiteIntArray* tflite::ConvertArrayToTfLiteIntArray(int, const int*)':`
`tensorflow/contrib/lite/util.cc:25:24: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (size_t i = 0; i < rank; i++) {
                      ~~^~~~~~`
`INFO: From Compiling tensorflow/contrib/lite/kernels/gemm_support.cc [for host]:`
`In file included from external/gemmlowp/public/../internal/../fixedpoint/fixedpoint.h:871:0,
                 from external/gemmlowp/public/../internal/output.h:26,
                 from external/gemmlowp/public/../internal/unpack.h:23,
                 from external/gemmlowp/public/../internal/single_thread_gemm.h:29,
                 from external/gemmlowp/public/../internal/multi_thread_gemm.h:24,
                 from external/gemmlowp/public/../internal/dispatch_gemm_shape.h:23,
                 from external/gemmlowp/public/gemmlowp.h:19,
                 from ./tensorflow/contrib/lite/kernels/gemm_support.h:18,
                 from tensorflow/contrib/lite/kernels/gemm_support.cc:15:
external/gemmlowp/public/../internal/../fixedpoint/./fixedpoint_sse.h:43:39: warning: ignoring attributes on template argument '__m128i {aka __vector(2) long long int}' [-Wignored-attributes]
 struct FixedPointRawTypeTraits<__m128i> {`
                                       ^
`In file included from external/gemmlowp/public/../internal/simd_wrappers.h:509:0,
                 from external/gemmlowp/public/../internal/output.h:28,
                 from external/gemmlowp/public/../internal/unpack.h:23,
                 from external/gemmlowp/public/../internal/single_thread_gemm.h:29,
                 from external/gemmlowp/public/../internal/multi_thread_gemm.h:24,
                 from external/gemmlowp/public/../internal/dispatch_gemm_shape.h:23,
                 from external/gemmlowp/public/gemmlowp.h:19,
                 from ./tensorflow/contrib/lite/kernels/gemm_support.h:18,
                 from tensorflow/contrib/lite/kernels/gemm_support.cc:15:
external/gemmlowp/public/../internal/simd_wrappers_sse.h:31:72: warning: ignoring attributes on template argument 'gemmlowp::Int32x4 {aka __vector(2) long long int}' [-Wignored-attributes]
       typename std::conditional<ScalarCount >= 4, Int32x4, std::int32_t>::type;`
                                                                        ^
`external/gemmlowp/public/../internal/simd_wrappers_sse.h:37:72: warning: ignoring attributes on template argument 'gemmlowp::Int16x8 {aka __vector(2) long long int}' [-Wignored-attributes]
       typename std::conditional<ScalarCount >= 8, Int16x8, std::int16_t>::type;`
                                                                        ^
`external/gemmlowp/public/../internal/simd_wrappers_sse.h:45:52: warning: ignoring attributes on template argument 'gemmlowp::Uint8x16 {aka __vector(2) long long int}' [-Wignored-attributes]
                                 std::uint8_t>::type>::type;`
                                                    ^
`INFO: From Compiling tensorflow/contrib/lite/arena_planner.cc [for host]:
tensorflow/contrib/lite/arena_planner.cc: In member function 'virtual TfLiteStatus tflite::ArenaPlanner::PlanAllocations()':
tensorflow/contrib/lite/arena_planner.cc:82:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < graph_info_->num_nodes(); ++i) {`
                   ~~^~~~~~~~~~~~~~~~~~~~~~~~~~
`tensorflow/contrib/lite/arena_planner.cc:101:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < graph_info_->num_nodes(); ++i) {`
                   ~~^~~~~~~~~~~~~~~~~~~~~~~~~~
`tensorflow/contrib/lite/arena_planner.cc: In member function 'virtual TfLiteStatus tflite::ArenaPlanner::ExecuteAllocations(int, int)':
tensorflow/contrib/lite/arena_planner.cc:139:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < graph_info_->num_tensors(); ++i) {`
                   ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~
`tensorflow/contrib/lite/arena_planner.cc: In member function 'TfLiteStatus tflite::ArenaPlanner::CalculateAllocationOfInternalTensors(int)':
tensorflow/contrib/lite/arena_planner.cc:232:18: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   if (node_index < graph_info_->num_nodes()) {`
       ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~
`tensorflow/contrib/lite/arena_planner.cc: In member function 'TfLiteStatus tflite::ArenaPlanner::CalculateDeallocationOfInternalTensors(int)':
tensorflow/contrib/lite/arena_planner.cc:245:18: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   if (node_index < graph_info_->num_nodes()) {`
       ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~
`ERROR: missing input file '@local_config_nccl//:nccl/NCCL-SLA.txt'
ERROR: /home/vikranth/tensorflow/tensorflow/tools/pip_package/BUILD:166:1: //tensorflow/tools/pip_package:build_pip_package: missing input file '@local_config_nccl//:nccl/NCCL-SLA.txt'`
`Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /home/vikranth/tensorflow/tensorflow/tools/pip_package/BUILD:166:1 1 input file(s) do not exist
INFO: Elapsed time: 222.836s, Critical Path: 25.63s
INFO: 386 processes, local.
FAILED: Build did NOT complete successfully`

`vikranth@MySys:~/tensorflow$ tensorflow -V
tensorflow: command not found`
"
23842,Run time GPU resources allocation in Tensorflow,"Hi
I have developed a model which is  Real time vehicle and person detection using tensor flow as back end in python and have  ten jobs (ten video feed from different cctv camera ) or more than that at run time some time jobs are more than ten or less than ten  jobs ,I want to run ten  jobs at a same time on one  gpu(GT X 1080 memory 8118MiB) using tensor flow  but if i have ten jobs then GPU resources equally distribute to every jobs and if jobs are five then resource equally distributed to five jobs i.e run time GPU resources allocation on the basis of number of available jobs(may be more than ten jobs or less than ten jobs) like cpu use its intelligence when one job/process complete then it release resources and cpu assign these resources to other process so  I want to utilize my whole resources of my GPU at every time
suggest me any approach
Thanks"
23840,"RuntimeWarning: compiletime version 3.4 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5; uiltins.type size changed, may indicate binary incompatibility. Expected 432, got 412","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template </em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian 9
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary, I think? (_pip_ installed)
- TensorFlow version (use command below): 1.11.0
- Python version: 3.5.3
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A, no GPU
- GPU model and memory: N/A, no GPU


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
As you know with the title, when I import tensorflow in shell window, I get a warning. (See code)
**Describe the expected behavior**
I recently saw some issues, and they says that it is OK to use tensorflow with this. But my question is: <strong>how can I get rid of this?</strong>
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```

>>> import tensorflow as tf

Warning (from warnings module):
  File ""/usr/lib/python3.5/importlib/_bootstrap.py"", line 222
    return f(*args, **kwds)
RuntimeWarning: compiletime version 3.4 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5

Warning (from warnings module):
  File ""/usr/lib/python3.5/importlib/_bootstrap.py"", line 222
    return f(*args, **kwds)
RuntimeWarning: builtins.type size changed, may indicate binary incompatibility. Expected 432, got 412
>>> 
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23839,Documentaion section comparing_compiler_optimizations has been removed from performance overview,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version:
- Doc Link: https://www.tensorflow.org/guide/performance/overview#comparing_compiler_optimizations


**Describe the documentation issue**
The following section has been removed from official documentation: 
https://www.tensorflow.org/guide/performance/overview#comparing_compiler_optimizations
I was able to find the benchmark after long search here:
https://github.com/tensorflow/tensorflow/issues/13050#issuecomment-330699371
I was wondering if the section was removed deliberately or by accident?

Regards, Anton

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
23837,The accuracy of the CNN I made is 0.1. It seems to be a matter of function used,"It's a cnn I made, and no matter how much I learn or test, the accuracy is 0.1. My guess is, I've probably correct 1/10, so the accuracy is 0.1. What do you think? Where do i have to modify it to get like this ""100% accuracy""

i think The cause seems to be this part.

onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)
loss = tf.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=logits)
(Note, training accuracy increases with execution.)

def cnn_model_fn(features, labels, mode):
""""""Model function for CNN.""""""
# Input Layer
input_layer = tf.reshape(features[""image""], [-1, 28, 28, 3])

# Convolutional Layer #1
conv1 = tf.layers.conv2d(
    inputs=input_layer,
    filters=32,
    kernel_size=[5, 5],
    padding=""SAME"",
    activation=tf.nn.relu)

# Pooling Layer #1
pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)

# Convolutional Layer #2 and Pooling Layer #2
conv2 = tf.layers.conv2d(
    inputs=pool1,
    filters=64,
    kernel_size=[5, 5],
    padding=""SAME"",
    activation=tf.nn.relu)
pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)

# Dense Layer
pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])
dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)
dropout = tf.layers.dropout(inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)

# Logits Layer
logits = tf.layers.dense(inputs=dropout, units=10)

predictions = {
    # Generate predictions (for PREDICT and EVAL mode)
    ""classes"": tf.argmax(input=logits, axis=1),
    # Add `softmax_tensor` to the graph. It is used for PREDICT and by the
    # `logging_hook`.
    ""probabilities"": tf.nn.softmax(logits, name=""softmax_tensor"")
}

if mode == tf.estimator.ModeKeys.PREDICT:
    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)

onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)
loss = tf.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=logits)

# Configure the Training Op (for TRAIN mode)
if mode == tf.estimator.ModeKeys.TRAIN:
    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)
    train_op = optimizer.minimize(
        loss=loss,
        global_step=tf.train.get_global_step())
    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)

# Add evaluation metrics (for EVAL mode)
eval_metric_ops = {
    ""accuracy"": tf.metrics.accuracy(
        labels=labels, predictions=predictions[""classes""])}
return tf.estimator.EstimatorSpec(
    mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
23836,ImportError with Win10,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10 build 17134
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): `pip install tensorflow-gpu`
- TensorFlow version (use command below): 1.12.0
- Python version: 3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.0/7.4.1.5
- GPU model and memory: GTX1060 6GB


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import numpy as np
import os
import six.moves.urllib as urllib
import sys
import tarfile
import tensorflow as tf
import zipfile

from distutils.version import StrictVersion
from collections import defaultdict
from io import StringIO
from matplotlib import pyplot as plt
from PIL import Image

# This is needed since the notebook is stored in the object_detection folder.
sys.path.append("".."")
from object_detection.utils import ops as utils_ops

if StrictVersion(tf.__version__) < StrictVersion('1.9.0'):
  raise ImportError('Please upgrade your TensorFlow installation to v1.9.* or later!') 
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
ImportError                               Traceback (most recent call last)
c:\users\khvlo\anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

c:\users\khvlo\anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

c:\users\khvlo\anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

c:\users\khvlo\anaconda3\envs\tensorflow1\lib\imp.py in load_module(name, file, filename, details)
    242         else:
--> 243             return load_dynamic(name, filename, file)
    244     elif type_ == PKG_DIRECTORY:

c:\users\khvlo\anaconda3\envs\tensorflow1\lib\imp.py in load_dynamic(name, path, file)
    342             name=name, loader=loader, origin=path)
--> 343         return _load(spec)
    344 

ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-2-1e9eee4e6961> in <module>
      4 import sys
      5 import tarfile
----> 6 import tensorflow as tf
      7 import zipfile
      8 

c:\users\khvlo\anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\__init__.py in <module>
     22 
     23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     25 
     26 try:

c:\users\khvlo\anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\__init__.py in <module>
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 from tensorflow.python.tools import component_api_helper

c:\users\khvlo\anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""c:\users\khvlo\anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""c:\users\khvlo\anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""c:\users\khvlo\anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""c:\users\khvlo\anaconda3\envs\tensorflow1\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""c:\users\khvlo\anaconda3\envs\tensorflow1\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```"
23834,Relative Device Placement,"**System information**
- TensorFlow version (you are using): 1.10+
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

From the [Using GPUs] Guide (under [multiple gpus][Using Multiple GPUs])  there is a part about using multiple GPUs in a ""multi-tower fashion"":

```
for d in ['/device:GPU:2', '/device:GPU:3']:
  with tf.device(d):
```

Seeing this, one might be tempted to leverage this style for multiple GPU training in a custom Estimator to indicate to the model that it can be distributed across multiple GPUs efficiently.

To my knowledge, if manual device placement is absent TensorFlow does not have some form of optimal device mapping (expect perhaps if you have the GPU version installed and a GPU is available, using it over the CPU). So what other choice do you have?

Anyway, you carry on with training your estimator and export it to a `SavedModel` via `estimator.export_savedmodel(...)` and wish to use this `SavedModel` later... perhaps on a different machine, one which may not have as many GPUs as the device on which the model was trained (or maybe no GPUs)

so when you run

```
from tensorflow.contrib import predictor
predict_fn = predictor.from_saved_model(model_dir)
```

you get

```
Cannot assign a device for operation <OP-NAME>. Operation was 
explicitly assigned to <DEVICE-NAME> but available devices are 
[<AVAILABLE-DEVICE-0>,...]
```

Now if you share a GPU cluster and are specifying TensorFlow to run your model on `GPU:3` and `GPU:6`, then go to run your model on your local machine, which may have 2 GPUs, you get the same problem.

## Putative Solution

I propose either:

1. machine device mapping

2. introducing relative device placement with automatic fallback. 


For the former, this means that I can tell TensorFlow that at the beginning to treat the manual placement of devices in the graph to another via a dictionary:

```
# map all GPUs to single core CPU
machine_device_map = {""device:GPU:{}"".format(i): ""device/CPU:0"" for i in range(8)}

# map odd GPUs to GPU 0 and even GPUs to GPU 1 on smaller GPU machine
machine_device_map = {
    ""device/GPU:{}"".format(i): ""device:GPU:{}"".format(i % 2)
    for i in range(8)
}


# then something like this goes after importing TensorFlow
tf.devices.map_device(machine_device_map)
```

For the latter if I train my model on GPUs 1,4,6,7  and my machine only has GPUs 0 and 1 then  device placement 1,4 falls back to 0 and 6,7 falls back to 1. If my device only has a single CPU

This would be done with something like:

```
with tf.relative_device('GPU:0'):
```

where, given 'device/relative/GPU:0' relative device calls

```
from TensorFlow's.python.client import device_lib
devices = device_lib.list_local_devices()

relative_device_num = 0
relative_device_type = 'GPU'

cpus = [d for d in devices if d.device_type == 'CPU']
gpus = [d for d in devices if d.device_type == 'GPU']

if relative_device_type=='GPU':
    if gpus:
        return gpus[relative_device_num % len(gpus)] 

return cpus[relative_device_num % len(cpus)]

```



[Using GPUs]: https://www.tensorflow.org/guide/using_gpu#manual_device_placement
[Using Multiple GPUs]: https://www.tensorflow.org/guide/using_gpu#using_multiple_gpus

**Will this change the current api? How?**

No. It would add the ability to change device placement for distributed models after training

**Who will benefit with this feature?**

All TF users

**Any Other info.**
"
23833,The same compilation target results in different compilation results,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Linux Ubuntu 16.04
- TensorFlow version:1.8.0
- Python version:3.5
- Bazel version (if compiling from source):0.17.2
- GCC/Compiler version (if compiling from source):gcc 5.4

**Describe the problem**

Initially, I used tensorflow1.8.0 as my external dependency for development, mainly using core:lib core:all_kernels core:core_cpu as the compilation target, and I wanted to use these three library files to generate my.so dynamic library files

It does works and Everything seems normal, but I find that if I compile the same goal in tensorflow's self workspace, The generated binaries will be much smaller than the ones I compiled using external dependencies.

In the beginning, I thought this was a problem caused by bazel compilation, so I consulted on bazle's community. Here is the issue I initiated

https://github.com/bazelbuild/bazel/issues/6677

But after analysis by the developers, it is likely that tensorflow controls itself.

This problem has been bothering me for a long time. I hope you can give me some ideas or Suggestions to solve the problem. Thank you very much!

Here are the issues and validation process I mentioned in issue.


The situation became clear, and I simplified my work the most, remove BUILD file and modified WORKSPACE file to the following：
```
local_repository(
        name=""org_tensorflow"",
        path=""../tensorflow"",
)
# TensorFlow depends on ""io_bazel_rules_closure"" so we need this here.
# Needs to be kept in sync with the same target in TensorFlow's WORKSPACE file.
http_archive(
    name = ""io_bazel_rules_closure"",
    sha256 = ""6691c58a2cd30a86776dd9bb34898b041e37136f2dc7e24cadaeaf599c95c657"",
    strip_prefix = ""rules_closure-08039ba8ca59f64248bb3b6ae016460fe9c9914f"",
    urls = [
        ""https://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/08039ba8ca59f64248bb3b6ae016460fe9c9914f.tar.gz"",
        ""https://github.com/bazelbuild/rules_closure/archive/08039ba8ca59f64248bb3b6ae016460fe9c9914f.tar.gz"",  # 2018-01-16
    ],
)
load('@org_tensorflow//tensorflow:workspace.bzl', 'tf_workspace')
tf_workspace(path_prefix = """", tf_repo_name = ""org_davinci"")
```

I then modified the BUILD file in the tensorflow source directory （../tensorflow），
```
exports_files(
    [
        ""LICENSE"",
        ""ACKNOWLEDGEMENTS"",
    ],
)
#bazel build //tensorflow/contrib/davinci_adpt_2:libtf_kernels.so
#By default, packages of “_impl” are not visible，Here I changed the package visibility in core/framework/BUILD
## 
cc_binary(
    name = ""libtf_kernels.so"",
    visibility = [""//visibility:public""],
    deps = [
        "":op_interface"",
        ""//tensorflow/core:framework_internal_impl"",
        ""//tensorflow/core:lib_internal_impl"",
        ""//tensorflow/core:core_cpu_impl"",
    ],
    linkstatic = True,
    linkshared = True,
)
cc_library(
    name = ""op_interface"",
    visibility = [""//visibility:public""],
    srcs = glob([],exclude=[]),
    hdrs = glob([],exclude=[]),
    deps = [
        ""//tensorflow/core:all_kernels"",
        ""//tensorflow/core:lib"",
        ""//tensorflow/core:core_cpu"",
    ],
)
```
I tested three scenarios：
- In my workspace dir
```
bazel build @org_tensorflow//:libtf_kernels.so
```
This creates a 700M .so file

- in the tensorflow source directory
```
bazel build //:libtf_kernels.so
```
This creates a 125M .so file

- in the tensorflow source directory
```
bazel build @org_tensorflow//:libtf_kernels.so
```
This also creates a 125M .so file

### Here if I don't add the three ""_impl"" dependencies and run `bazel build //:libtf_kernels.so` in the source directory of tensorflow, I will fail in the link phase!  But even without the three ""_impl"" dependencies, I can still compile successfully in my own workspace!

I don't know what the problem is there, it looks like the content is similar when linked, but when I executed under the source directory of tensorflow, bazel didn't compile enough targets?
"
23832,Tensorflow conda-forge installation issues,"**System information**
- OS Platform and Distribution: MacOSX version 10.11.6
- TensorFlow version: 1.10
- Python version: 3.6
- Installed using conda

Just to clarify, I'm trying to perform an out-of-the-box installation of tensorflow.  Below is the tensorflow installation procedure that I used, in addition to the error that I get

```
conda create -n tf-test tensorflow -c conda-forge
source activate tf-test
python -c ""import tensorflow""

Traceback (most recent call last):
  File ""/Users/mortonjt/miniconda3/envs/tf-test/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/Users/mortonjt/miniconda3/envs/tf-test/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/Users/mortonjt/miniconda3/envs/tf-test/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/Users/mortonjt/miniconda3/envs/tf-test/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/Users/mortonjt/miniconda3/envs/tf-test/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/Users/mortonjt/miniconda3/envs/tf-test/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation
  Referenced from: /Users/mortonjt/miniconda3/envs/tf-test/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so (which was built for Mac OS X 10.12)
  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security
 in /Users/mortonjt/miniconda3/envs/tf-test/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/mortonjt/miniconda3/envs/tf-test/lib/python3.6/site-packages/tensorflow/__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/Users/mortonjt/miniconda3/envs/tf-test/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/Users/mortonjt/miniconda3/envs/tf-test/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/Users/mortonjt/miniconda3/envs/tf-test/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/Users/mortonjt/miniconda3/envs/tf-test/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/Users/mortonjt/miniconda3/envs/tf-test/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/Users/mortonjt/miniconda3/envs/tf-test/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/Users/mortonjt/miniconda3/envs/tf-test/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/Users/mortonjt/miniconda3/envs/tf-test/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation
  Referenced from: /Users/mortonjt/miniconda3/envs/tf-test/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so (which was built for Mac OS X 10.12)
  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security
 in /Users/mortonjt/miniconda3/envs/tf-test/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

The full information about the conda environment can be found below

<br/>

Environment (<code>conda list</code>):
<details>

```
$ conda list
# packages in environment at /Users/mortonjt/miniconda3/envs/tf-test:
#
# Name                    Version                   Build  Channel
absl-py                   0.6.1                 py36_1000    conda-forge
astor                     0.7.1                      py_0    conda-forge
blas                      1.0                         mkl  
bzip2                     1.0.6                         1    conda-forge
c-ares                    1.15.0               h470a237_1    conda-forge
ca-certificates           2018.10.15           ha4d7672_0    conda-forge
certifi                   2018.10.15            py36_1000    conda-forge
gast                      0.2.0                      py_0    conda-forge
grpcio                    1.16.0           py36hd60e7a3_0    conda-forge
intel-openmp              2019.0                      118  
libffi                    3.2.1                hfc679d8_5    conda-forge
libgfortran               3.0.1                h93005f0_2  
libprotobuf               3.6.1                hd28b015_0    conda-forge
markdown                  2.6.11                     py_0    conda-forge
mkl                       2019.0                      118  
mkl_fft                   1.0.6                    py36_0    conda-forge
mkl_random                1.0.2                    py36_0    conda-forge
ncurses                   6.1                  hfc679d8_1    conda-forge
numpy                     1.15.4           py36h6a91979_0  
numpy-base                1.15.4           py36h8a80b8c_0  
openssl                   1.0.2p               h470a237_1    conda-forge
pip                       18.1                  py36_1000    conda-forge
protobuf                  3.6.1            py36hfc679d8_1    conda-forge
python                    3.6.6                h5001a0f_0    conda-forge
readline                  7.0                  haf1bffa_1    conda-forge
setuptools                40.6.2                   py36_0    conda-forge
six                       1.11.0                py36_1001    conda-forge
sqlite                    3.25.3               hb1c47c0_0    conda-forge
tensorboard               1.10.0                   py36_0    conda-forge
tensorflow                1.10.0                   py36_0    conda-forge
termcolor                 1.1.0                      py_2    conda-forge
tk                        8.6.9                ha92aebf_0    conda-forge
werkzeug                  0.14.1                     py_0    conda-forge
wheel                     0.32.2                   py36_0    conda-forge
xz                        5.2.4                h470a237_1    conda-forge
zlib                      1.2.11               h470a237_3    conda-forge
```

</details>

Details about  <code>conda</code> and system ( <code>conda info</code> ):
<details>

```
$ conda info
     active environment : tf-test
    active env location : /Users/mortonjt/miniconda3/envs/tf-test
            shell level : 1
       user config file : /Users/mortonjt/.condarc
 populated config files : 
          conda version : 4.5.11
    conda-build version : not installed
         python version : 3.7.0.final.0
       base environment : /Users/mortonjt/miniconda3  (writable)
           channel URLs : https://repo.anaconda.com/pkgs/main/osx-64
                          https://repo.anaconda.com/pkgs/main/noarch
                          https://repo.anaconda.com/pkgs/free/osx-64
                          https://repo.anaconda.com/pkgs/free/noarch
                          https://repo.anaconda.com/pkgs/r/osx-64
                          https://repo.anaconda.com/pkgs/r/noarch
                          https://repo.anaconda.com/pkgs/pro/osx-64
                          https://repo.anaconda.com/pkgs/pro/noarch
          package cache : /Users/mortonjt/miniconda3/pkgs
                          /Users/mortonjt/.conda/pkgs
       envs directories : /Users/mortonjt/miniconda3/envs
                          /Users/mortonjt/.conda/envs
               platform : osx-64
             user-agent : conda/4.5.11 requests/2.19.1 CPython/3.7.0 Darwin/15.6.0 OSX/10.11.6
                UID:GID : 501:20
             netrc file : None
           offline mode : False
```
</details>

Not sure about the connection between conda-forge and the main TF branch, but this is an awesome resource and having these sorts of issues resolved in the conda recipe would be extremely helpful, particularly for software that heavily depend on conda.

This issue has been referenced here:
https://github.com/conda-forge/tensorflow-feedstock/issues/64

Edit: the tensorflow installation from the anaconda channel seems to be working

```
conda create -n tf-test2 tensorflow
source activate tf-test2
python -c ""import tensorflow"" 
```"
23831,Documentation link broken,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 1.12
- Doc Link: https://www.tensorflow.org/guide/extend/model_files


**Describe the documentation issue**
Linked file in https://www.tensorflow.org/guide/extend/model_files#text_or_binary was already removed from tensorboard repo by https://github.com/tensorflow/tensorboard/pull/1512.
**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
Can not."
23830,Not compatible with tf.float16,"### System information
- **OS Platform - Windows10 (64-bit OS and x64-based processor)**:
- **TensorFlow installed from Anaconda**:
- **TensorFlow version - 1.11.0**:
- **Python version - 3.6.5**:

float16 not in the list of allowed values.
**Command:**
```
a = tf.placeholder(tf.float16, shape=[2, 2])
gen_linalg_ops.qr(a)
```
TypeError                                 Traceback (most recent call last)
<ipython-input-11-9aca9b273d31> in <module>
      1 a = tf.placeholder(tf.float16, shape=[2, 2])
----> 2 gen_linalg_ops.qr(a)

G:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\gen_linalg_ops.py in qr(input, full_matrices, name)
   1486     full_matrices = _execute.make_bool(full_matrices, ""full_matrices"")
   1487     _, _, _op = _op_def_lib._apply_op_helper(
-> 1488         ""Qr"", input=input, full_matrices=full_matrices, name=name)
   1489     _result = _op.outputs[:]
   1490     _inputs_flat = _op.inputs

G:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    607               _SatisfiesTypeConstraint(base_type,
    608                                        _Attr(op_def, input_arg.type_attr),
--> 609                                        param_name=input_name)
    610             attrs[input_arg.type_attr] = attr_value
    611             inferred_from[input_arg.type_attr] = input_name

G:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\op_def_library.py in _SatisfiesTypeConstraint(dtype, attr_def, param_name)
     58           ""allowed values: %s"" %
     59           (param_name, dtypes.as_dtype(dtype).name,
---> 60            "", "".join(dtypes.as_dtype(x).name for x in allowed_list)))
     61 
     62 

TypeError: Value passed to parameter 'input' has DataType float16 not in list of allowed values: float64, float32, complex64, complex128
"
23827,Tensor cpu need cuda?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): W7 64 bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version: 1.12.0
- Python version: 3.6.5
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: not supported by GPU
- GPU model and memory: Geforce GT 330m



**Describe the problem**
i have cpu tensor,couse my video card don't support cuda acceleration.
I have correctly installed tensorflow by pip command,but i got error when import.
some one told me about cuda driver etc,but why i need it if i want use cpu version?
i tryed to use cuda and cudnn,but error persist
i got this error using keras too

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Xxx\AppData\Local\Programs\Python\Python36\lib\site-packages\
tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\xxx\AppData\Local\Programs\Python\Python36\lib\site-packages\
tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper

    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript
ion)
  File ""C:\Users\xxx\AppData\Local\Programs\Python\Python36\lib\imp.py"", line
243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\xxxx\AppData\Local\Programs\Python\Python36\lib\imp.py"", line
343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed with error code -1073741795
343, in load_dynamic
   

Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors
"
23826,Why is bias is added by 0.5,"
![biaserr](https://user-images.githubusercontent.com/5864369/48667949-7ae2ea00-eae2-11e8-9a5a-a24e1049d54f.png)
Hello I'm wondering why is the predicted bias term is always 0.5 greater than the real bias term, I know that I'm adding noise but in the example in cognitive classes they dont have this problem. Thoughts ?"
23824,Load SavedModel for Estimator API,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.10+
- Are you willing to contribute it (Yes/No): yes



**Describe the feature and the current behavior/state.**

TensorFlow current touts several high level APIs including:

> - Estimators, a high-level API that provides fully-packaged models ready for large-scale training and production.

Focusing on what Estimator's encapsulate:

> Estimators encapsulate the following actions:
> - training
> - evaluation
> - prediction
> - export for serving


there seems to be a glaring omission that straddles prediction and serving.

Where is the import?

Preparing a custom estimator for serving is an arduous journey, so once one has a `saved_model`, it would be nice to be able to load the saved model via the estimator class, rather than the `contrib` module:

```
from tensorflow.contrib import predictor
predict_fn = predictor.from_saved_model('<model-dir>')
```

e.g.

```
est = tf.estimator.Estimator.from_saved_model(model_dir)

# retrain new dataset or whatever, using estimator api rather than SavedModelPredictor
```

perhaps this was left out as it was the goal to only used exported models in the serving context. 

If that is the case, however, then the serving documentation needs critical updates. In addition it (the interface to serving) would benefit from a higher level api centered around estimators rather than any saved model.   

If I have a saved model and it is this easy to predict with it:

```
from tensorflow.contrib import predictor
predict_fn = predictor.from_saved_model('<model-dir>')
predict_fn(<pred_features>)
```

then it should be this easy to get a restful api to my model

```
> tf-serve <model-dir> --host=localhost --port=8000
```

so that I can ""GET"" my prediction


Now, I could be misinformed. It is very likely I do not understand the tensorflow/serving.git to the fullest and it might very well be that easy. If that is the case, then the documentation needs to be updated to clarify how one can just serve their model. 


**Will this change the current api? How?**

Add a method to the `tf.estimator.Estimator` class `from_saved_model` or `import_saved_model` which allows one to recover the estimator. 


Also if the Estimator API is the high level API that the TF documentation leads me to believe it to be, perhaps make it directly compatible with TF.JS

**Who will benefit with this feature?**

People using the estimator api.

People confused by the obtuse protocol to serve a custom estimator

**Any Other info.**
"
23823, tf.estimator.export.ServingInputReceiver very rigid implementation,"**System information**
- I have written custom code.
- OS Platform and Distribution: Ubuntu 16.04
- TensorFlow installed from:  binary
- TensorFlow version: tensorflow-1.12.0
- Python version: 3.6
- CUDA/cuDNN version: 9.0
- GPU model and memory: GeForce MX150 2GB

**Describe the current behavior**

Very rigid implementation for tf.estimator.export.ServingInputReceiver. I use nested dictionaries for train and eval features, such as: features={}; features['static'] = tf.Tensor(); features['dynamic']['a'] = tf.Tensor(); features['dynamic']['b'] = tf.Tensor(), which works fine for train and eval input fns but it is not supported for tf.estimator.export.ServingInputReceiver. It raises value_error at _check_tensor:

ValueError: feature dynamic must be a Tensor or SparseTensor.

Since the function _wrap_and_check_input_tensors only allows plain dictionaries containing string keys and tensor values. 

**Describe the expected behavior**

At least one should be able to disable the option check_tensors. Actually, tf.estimator is expected to provide a more flexible implementation.

**Other info / logs**

  File ""/home/edu/.local/lib/python3.6/site-packages/tensorflow/python/estimator/export/export.py"", line 137, in __new__ 
    features = _wrap_and_check_input_tensors(features, 'feature')
  File ""/home/edu/.local/lib/python3.6/site-packages/tensorflow/python/estimator/export/export.py"", line 73, in _wrap_and_check_input_tensors
    _check_tensor(tensor, name, error_label=field_name)
  File ""/home/edu/.local/lib/python3.6/site-packages/tensorflow/python/estimator/export/export.py"", line 97, in _check_tensor
    raise value_error
ValueError: feature dynamic must be a Tensor or SparseTensor.
"
23822,Event files created by estimator are not closed after training/evaluation,"I'm using the estimator API and a custom implementation of several hyperparameter search methods (random search, hyperband, etc.) to train multiple models and select the best hyperparameters. At the end of the algorithm I would like to delete all the files created during the trainings/evaluations (e.g., the checkpoint files, event files, etc.). I'm trying this with shutil.rmtree; however, I am not able to delete the event files as it appears they are still in use.

Here is an example code - mostly pulled from the Iris example scripts - along with the error to illustrate the issue. The actual code uses a custom estimator implementation instead of the DNNClassifier, but this has the same problem.

```
import tensorflow as tf
import pandas as pd
import os
import shutil

def load_data(y_name='Species'):
    train_url = ""http://download.tensorflow.org/data/iris_training.csv""
    test_url = ""http://download.tensorflow.org/data/iris_test.csv""

    train_data = pd.read_csv(train_url)
    train_data.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']
    test_data = pd.read_csv(test_url)
    test_data.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']
    
    train_x = train_data.iloc[:, 0:4]
    train_y = train_data.iloc[:, 4]
    
    test_x = test_data.iloc[:, 0:4]
    test_y = test_data.iloc[:, 4]
    
    return (train_x, train_y), (test_x, test_y)

def train_input_fn(features, labels, batch_size):
    """"""An input function for training""""""
    # Convert the inputs to a Dataset.
    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))

    # Shuffle, repeat, and batch the examples.
    dataset = dataset.shuffle(1000).repeat().batch(batch_size)

    # Return the dataset.
    return dataset

def eval_input_fn(features, labels, batch_size):
    """"""An input function for evaluation or prediction""""""
    features=dict(features)
    if labels is None:
        # No labels, use only features.
        inputs = features
    else:
        inputs = (features, labels)

    # Convert the inputs to a Dataset.
    dataset = tf.data.Dataset.from_tensor_slices(inputs)

    # Batch the examples
    assert batch_size is not None, ""batch_size must not be None""
    dataset = dataset.batch(batch_size)

    # Return the dataset.
    return dataset

model_dir = os.path.join(os.getcwd(), *['estimator_test', 'test_1'])

(train_x, train_y), (test_x, test_y) = load_data()

# Feature columns describe how to use the input.
my_feature_columns = []
for key in train_x.keys():
    my_feature_columns.append(tf.feature_column.numeric_column(key=key))

# Build 2 hidden layer DNN with 10, 10 units respectively.
classifier = tf.estimator.DNNClassifier(
    feature_columns=my_feature_columns,
    # Two hidden layers of 10 nodes each.
    hidden_units=[10, 10],
    # The model must choose between 3 classes.
    n_classes=3,
    model_dir = model_dir)

# Train the Model.
classifier.train(
    input_fn=lambda: train_input_fn(train_x, train_y, 100),
    steps=100)

# Evaluate the model.
eval_result = classifier.evaluate(
    input_fn=lambda: eval_input_fn(test_x, test_y, 100))

print('\nTest set accuracy: {accuracy:0.3f}\n'.format(**eval_result))

shutil.rmtree(model_dir)
```
Here is the error:
```OSError                                   Traceback (most recent call last)
<ipython-input-40-beccd7cbb56e> in <module>()
----> 1 shutil.rmtree(os.path.join(os.getcwd(), *['estimator_test', 'delete_test_1']))

~\AppData\Local\Continuum\anaconda3\lib\shutil.py in rmtree(path, ignore_errors, onerror)
    492             os.close(fd)
    493     else:
--> 494         return _rmtree_unsafe(path, onerror)
    495 
    496 # Allow introspection of whether or not the hardening against symlink

~\AppData\Local\Continuum\anaconda3\lib\shutil.py in _rmtree_unsafe(path, onerror)
    391         os.rmdir(path)
    392     except OSError:
--> 393         onerror(os.rmdir, path, sys.exc_info())
    394 
    395 # Version using fd-based APIs to protect against races

~\AppData\Local\Continuum\anaconda3\lib\shutil.py in _rmtree_unsafe(path, onerror)
    389                 onerror(os.unlink, fullname, sys.exc_info())
    390     try:
--> 391         os.rmdir(path)
    392     except OSError:
    393         onerror(os.rmdir, path, sys.exc_info())

OSError: [WinError 145] The directory is not empty:
```
I looked at all the files that were open by Python processes, and it's only the event files that are still in use.  I would expect that all files would be closed after training/evaluation are complete. Unfortunately, I cannot find a way to manually close these after training.

On a related note (and this is more of a feature request), it would be great to be able to disable writing summaries in the estimator configuration as the files are pretty large. This would remove the need to delete them after training altogether. 

System information
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 1.11 (CPU version)
- Python version: 3.6
"
23820,"When tf.Session(), got error cuDevicePrimaryCtxRetain: CUDA_ERROR_NOT_SUPPORTED","When try to create session by tf.Session(), it gave error as below. Not sure how to solve it.

Python 3.6.5 (default, Apr  1 2018, 05:46:30) 
[GCC 7.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> tf.Session()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/eliyart/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1511, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File ""/home/eliyart/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 634, in __init__
    self._session = tf_session.TF_NewSessionRef(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_NOT_SUPPORTED: operation not supported

+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.48                 Driver Version: 410.48                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|=====+======================+======================|
|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |
| N/A   27C    P8    27W / 149W |     16MiB / 11441MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|==============================================|
|    0      2075      G   /usr/lib/xorg/Xorg                             9MiB |
|    0      2128      G   /usr/bin/gnome-shell                           6MiB |
+-----------------------------------------------------------------------------+

Any suggestion, thanks. 
alex
"
23819,"Broken link in ""A Tool Developer's Guide..."" for graph_run_run2.pbtxt","<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- Doc Link:
   i.  https://www.tensorflow.org/guide/extend/model_files
   ii. https://github.com/tensorflow/docs/blob/master/site/en/guide/extend/model_files.md


**Describe the documentation issue**
As in the previously closed #5978 and #12042, the link for `graph_run_run2.pbtxt` is currently broken."
23817,Build from source r1.12 : tensorflow/tensorflow/contrib/kafka/BUILD:36:1: no such package '@kafka//':,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: r1.12
- Python version: python2.7.14
- Installed using virtualenv? pip? conda?: source
- Bazel version (if compiling from source): 0.18.0
- GCC/Compiler version (if compiling from source): gcc4.8.5
- CUDA/cuDNN version: cuda9.0
- GPU model and memory: p40 20g



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
./configure
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

After bazel build:
```
WARNING: Duplicate rc file: /home/adamzhangchao/bin/tensorflow/tools/bazel.rc is read multiple times, most recently imported from /home/adamzhangchao/bin/tensorflow/.bazelrc
WARNING: Processed legacy workspace file /home/adamzhangchao/bin/tensorflow/tools/bazel.rc. This file will not be processed in the next release of Bazel. Please read https://github.com/bazelbuild/bazel/issues/6319 for further information, including how to upgrade.
Starting local Bazel server and connecting to it...
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
DEBUG: /home/adamzhangchao/.cache/bazel/_bazel_adamzhangchao/6492ced5c6ed3f5961e75357254e4c4a/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default
ERROR: /home/adamzhangchao/bin/tensorflow/tensorflow/contrib/kafka/BUILD:36:1: no such package '@kafka//': Traceback (most recent call last):
	File ""/home/adamzhangchao/bin/tensorflow/third_party/repo.bzl"", line 106
		_apply_patch(ctx, ctx.attr.patch_file)
	File ""/home/adamzhangchao/bin/tensorflow/third_party/repo.bzl"", line 68, in _apply_patch
		fail(""patch command is not found, ple..."")
patch command is not found, please install it and referenced by '//tensorflow/contrib/kafka:dataset_kernels'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@kafka//': Traceback (most recent call last):
	File ""/home/adamzhangchao/bin/tensorflow/third_party/repo.bzl"", line 106
		_apply_patch(ctx, ctx.attr.patch_file)
	File ""/home/adamzhangchao/bin/tensorflow/third_party/repo.bzl"", line 68, in _apply_patch
		fail(""patch command is not found, ple..."")
patch command is not found, please install it
INFO: Elapsed time: 7.860s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (251 packages loaded)
    currently loading: tensorflow/core/kernels
```"
23816,entry point not found ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
23815,TensorFlow restore model Key Variable not found in checkpoint,"OSX10.14.1  py3.6  tf1.8

I'm a tensorflow beginner.I try to train a model for mnist. When I restore the model I get Error.

    from datetime import datetime
    
    import tensorflow as tf
    from tensorflow.examples.tutorials.mnist import input_data
    
    mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)
    save_model_path = 'mnist_model/model.ckpt'
    
    
    def train():
        learning_rate = 0.05
        batch_size = 100
        max_epochs = 100
        num_of_batch = int(mnist.train.num_examples / batch_size)
        now = datetime.utcnow().strftime(""%Y%m%d%H%M%S"")
    
        X = tf.placeholder(tf.float32, shape=[None, 784], name='X')
        y = tf.placeholder(tf.float32, shape=[None, 10], name='y')
        print(X.name, y.name)
    
        W = tf.get_variable(shape=[784, 10], name='weight')
        b = tf.get_variable(initializer=tf.zeros([10]), name='bais')
        tf.summary.histogram(""weights"", W)
        tf.summary.histogram(""biases"", b)
    
        with tf.name_scope('pred'):
            y_pred = tf.nn.softmax(tf.matmul(X, W) + b, name='predict')
            print(y_pred.name)
    
        with tf.name_scope('loss'):
            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_pred))
            tf.summary.scalar('loss', loss)
            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)
    
        with tf.name_scope('acc'):
            correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))
            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='acc')
            print(accuracy.name)
    
        merged_summary_op = tf.summary.merge_all()
        init_op = tf.global_variables_initializer()
    
        saver = tf.train.Saver()
    
        with tf.Session() as sess:
            sess.run(init_op)
    
            loss_avg = 0
            writer = tf.summary.FileWriter('mnist/{}'.format(now), sess.graph)
            for epoch in range(max_epochs):
                for i in range(num_of_batch):
                    batch_x, batch_y = mnist.train.next_batch(batch_size)
                    summary_str, _, l = sess.run([merged_summary_op, optimizer, loss], feed_dict={X: batch_x, y: batch_y})
                    loss_avg += l
                    global_step = epoch * num_of_batch + i
                    writer.add_summary(summary_str, global_step)
    
                    if global_step % 100 == 0:
                        print('Epoch {}: {} save model'.format(epoch, i))
                        # save model in halfway
                        saver.save(sess, save_model_path, global_step=global_step)
    
                loss_avg /= num_of_batch
                print('Epoch {}: Loss {}'.format(epoch, loss_avg))
    
            print(sess.run(accuracy, feed_dict={X: mnist.test.images, y: mnist.test.labels}))
            saver.save(sess, save_model_path)
    
    
    def predict(import_from_meta=False):
        if import_from_meta:
            meta_path = 'mnist_model/model.ckpt.meta'
            checkpoint_path = 'mnist_model'
        else:
            # stupid var WTF ValueError: No variables to save
            _ = tf.Variable(0)
            saver = tf.train.Saver()
    
        with tf.Session() as sess:
            sess.run(tf.global_variables_initializer())
            if import_from_meta:
                saver = tf.train.import_meta_graph(meta_path)
                saver.restore(sess, tf.train.latest_checkpoint(checkpoint_path))
            else:
                saver.restore(sess, save_model_path)
            graph = tf.get_default_graph()
            X = graph.get_tensor_by_name('X:0')
            y = graph.get_tensor_by_name('y:0')
            accuracy = graph.get_tensor_by_name('acc/acc:0')
            print(sess.run(accuracy, feed_dict={X: mnist.test.images, y: mnist.test.labels}))
    
            pred = graph.get_tensor_by_name('pred/predict:0')
            import matplotlib.pyplot as plt
            i = 90
            img_orign = mnist.train.images[i]
            img = img_orign.reshape((28, 28))
            plt.imshow(img, cmap='gray')
            plt.title(mnist.train.labels[i])
            plt.show()
            a = sess.run(pred, feed_dict={X: img_orign.reshape(-1, 784)})
            print(a.shape)
            import numpy as np
            print(np.argmax(a))
    
    
    def check_ckpt():
        from tensorflow.python.tools import inspect_checkpoint as chkp
        chkp.print_tensors_in_checkpoint_file(save_model_path, tensor_name='', all_tensors=True)
    
    
    if __name__ == '__main__':
        # train()
        predict(import_from_meta=False)
        # check_ckpt()


use `predict(import_from_meta=False)`

Error:


    WARNING:tensorflow:From /Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
    Instructions for updating:
    Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
    2018-11-08 16:53:40.482921: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key Variable not found in checkpoint
    Traceback (most recent call last):
      File ""/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1322, in _do_call
        return fn(*args)
      File ""/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1307, in _run_fn
        options, feed_dict, fetch_list, target_list, run_metadata)
      File ""/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1409, in _call_tf_sessionrun
        run_metadata)
    tensorflow.python.framework.errors_impl.NotFoundError: Key Variable not found in checkpoint
    	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_INT32], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File ""/Users/wyx/project/learn-sktf/tf/mnist_clf.py"", line 115, in <module>
        predict(import_from_meta=False)
      File ""/Users/wyx/project/learn-sktf/tf/mnist_clf.py"", line 92, in predict
        saver.restore(sess, save_model_path)
      File ""/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1802, in restore
        {self.saver_def.filename_tensor_name: save_path})
      File ""/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 900, in run
        run_metadata_ptr)
      File ""/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1135, in _run
        feed_dict_tensor, options, run_metadata)
      File ""/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
        run_metadata)
      File ""/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
        raise type(e)(node_def, op, message)
    tensorflow.python.framework.errors_impl.NotFoundError: Key Variable not found in checkpoint
    	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_INT32], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
    
    Caused by op 'save/RestoreV2', defined at:
      File ""/Users/wyx/project/learn-sktf/tf/mnist_clf.py"", line 115, in <module>
        predict(import_from_meta=False)
      File ""/Users/wyx/project/learn-sktf/tf/mnist_clf.py"", line 84, in predict
        saver = tf.train.Saver()
      File ""/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1338, in __init__
        self.build()
      File ""/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1347, in build
        self._build(self._filename, build_save=True, build_restore=True)
      File ""/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1384, in _build
        build_save=build_save, build_restore=build_restore)
      File ""/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 835, in _build_internal
        restore_sequentially, reshape)
      File ""/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 472, in _AddRestoreOps
        restore_sequentially)
      File ""/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 886, in bulk_restore
        return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
      File ""/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1463, in restore_v2
        shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)
      File ""/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
        op_def=op_def)
      File ""/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3392, in create_op
        op_def=op_def)
      File ""/Users/wyx/project/learn-sktf/.env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1718, in __init__
        self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access
    
    NotFoundError (see above for traceback): Key Variable not found in checkpoint
    	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_INT32], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

It's very strange that when I use `predict(import_from_meta=True)` I can get a **right answer**.

Then I try `check_ckpt()` to [inspect variables in a checkpoint](https://www.tensorflow.org/guide/saved_model#inspect_variables_in_a_checkpoint). I can't find any right tensor_name . That's so funny and `name_scope` just like a joke.

    tensor_name:  bais
    [-24.933702    1.7660792   3.697866  -14.221888    8.967291   42.149403
      -3.2693458  23.876926  -30.643892   -3.7861202]
    tensor_name:  bais/Adam
    [ 3.1726879e-07 -5.2043208e-07  3.4227469e-05  2.5119303e-07
     -2.0110610e-04  1.8493415e-04 -3.6275055e-06 -1.4343520e-04
     -7.2765622e-05  2.0172486e-04]
    tensor_name:  bais/Adam_1
    [5.2586905e-08 8.9204484e-08 1.5440051e-07 2.9412612e-07 2.4380788e-07
     3.4676964e-07 8.7062219e-08 1.8839150e-07 4.3878950e-07 4.2466107e-07]
    tensor_name:  loss/beta1_power
    0.0
    tensor_name:  loss/beta2_power
    1.2639432e-24
    tensor_name:  weight
    [[-0.03386476  0.03485525 -0.03267809 ... -0.08548199  0.00565728
      -0.01887459]
     [ 0.00370622  0.08523928  0.05811391 ... -0.07838921  0.05987743
       0.074329  ]
     [ 0.0180116   0.04400793 -0.0260816  ...  0.00807328  0.06537797
      -0.07446742]
     ...
     [-0.00665552 -0.03390152 -0.03889231 ... -0.01871967 -0.05968629
       0.07207178]
     [ 0.01317277  0.03459686 -0.03268962 ...  0.07082433  0.03290742
       0.03172391]
     [-0.04514085 -0.03013236  0.01006595 ...  0.01906221  0.02611361
       0.04348358]]
    tensor_name:  weight/Adam
    [[0. 0. 0. ... 0. 0. 0.]
     [0. 0. 0. ... 0. 0. 0.]
     [0. 0. 0. ... 0. 0. 0.]
     ...
     [0. 0. 0. ... 0. 0. 0.]
     [0. 0. 0. ... 0. 0. 0.]
     [0. 0. 0. ... 0. 0. 0.]]
    tensor_name:  weight/Adam_1
    [[0. 0. 0. ... 0. 0. 0.]
     [0. 0. 0. ... 0. 0. 0.]
     [0. 0. 0. ... 0. 0. 0.]
     ...
     [0. 0. 0. ... 0. 0. 0.]
     [0. 0. 0. ... 0. 0. 0.]
     [0. 0. 0. ... 0. 0. 0.]]
    
    Process finished with exit code 0


So what's wrong in my code?  and why must create variables before `tf.train.Saver` when we just want to restore a model?"
23814,[DeepLabV3+] How to get class probabilities in C++ using frozen graph?,"It looks like I should get the output from layer ResizeBilinear_3? This is the layer before argmax, according to tensorflow/models#4045. @aquariusjay
However, the numbers I got do not look like probabilities as they contained negative numbers and numbers much bigger than 1.
![image](https://user-images.githubusercontent.com/2267796/48654874-4af1f480-e9c5-11e8-9b20-215e144c3566.png)
FYI, the model I'm using is **deeplabv3_xception_ade20k_train** from the model zoo. It has 150 object classes + 1 unknown class so in total 151 classes.

Thank you!"
23813,deleted,deleted
23810,"tfjs-node, can't compile using tsc / Missing type definitions. ","**System information**
- OS Platform and Distribution : Linux Ubuntu 16.04 / Docker
- TensorFlow installed from (source or binary): Binary
- TensorFlow version: N/A
- Python version: N/A
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the problem**
Compiling a index.ts file with something simple as  :

```typescript
import * as tf from '@tensorflow/tfjs';
// Load the binding
import '@tensorflow/tfjs-node';
```
raise this : 
```
> tsc

node_modules/@tensorflow/tfjs-converter/dist/src/executor/frozen_model.d.ts:16:78 - error TS2304: Cannot find name 'RequestInit'.

16     constructor(modelUrl: string, weightManifestUrl: string, requestOption?: RequestInit);
                                                                                ~~~~~~~~~~~

node_modules/@tensorflow/tfjs-converter/dist/src/executor/frozen_model.d.ts:27:103 - error TS2304: Cannot find name 'RequestInit'.

27 export declare function loadFrozenModel(modelUrl: string, weightsManifestUrl: string, requestOption?: RequestInit): Promise<FrozenModel>;
                                                                                                         ~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/engine.d.ts:94:24 - error TS2304: Cannot find name 'ImageData'.

94     fromPixels(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels: number): Tensor3D;
                          ~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/engine.d.ts:94:36 - error TS2304: Cannot find name 'HTMLImageElement'.

94     fromPixels(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels: number): Tensor3D;
                                      ~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/engine.d.ts:94:75 - error TS2304: Cannot find name 'HTMLVideoElement'.

94     fromPixels(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels: number): Tensor3D;
                                                                             ~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/io/browser_files.d.ts:14:45 - error TS2304: Cannot find name 'File'.

14 export declare function browserFiles(files: File[]): IOHandler;
                                               ~~~~

node_modules/@tensorflow/tfjs-core/dist/io/browser_http.d.ts:6:37 - error TS2304: Cannot find name 'RequestInit'.

6     protected readonly requestInit: RequestInit;
                                      ~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/io/browser_http.d.ts:9:56 - error TS2304: Cannot find name 'RequestInit'.

9     constructor(path: string | string[], requestInit?: RequestInit, weightPathPrefix?: string);
                                                         ~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/io/browser_http.d.ts:19:83 - error TS2304: Cannot find name 'RequestInit'.

19 export declare function browserHTTPRequest(path: string | string[], requestInit?: RequestInit, weightPathPrefix?: string): IOHandler;
                                                                                     ~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/io/indexed_db.d.ts:5:35 - error TS2304: Cannot find name 'IDBFactory'.

5     protected readonly indexedDB: IDBFactory;
                                    ~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/io/local_storage.d.ts:5:28 - error TS2304: Cannot find name 'Storage'.

5     protected readonly LS: Storage;
                             ~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/io/types.d.ts:24:17 - error TS2304: Cannot find name 'Response'.

24     responses?: Response[];
                   ~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/io/weights_loader.d.ts:3:88 - error TS2304: Cannot find name 'RequestInit'.

3 export declare function loadWeightsAsArrayBuffer(fetchURLs: string[], requestOptions?: RequestInit): Promise<ArrayBuffer[]>;
                                                                                         ~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/io/weights_loader.d.ts:4:136 - error TS2304: Cannot find name 'RequestInit'.

4 export declare function loadWeights(manifest: WeightsManifestConfig, filePathPrefix?: string, weightNames?: string[], requestOptions?: RequestInit): Promise<NamedTensorMap>;
                                                                                                                                         ~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/backend.d.ts:13:24 - error TS2304: Cannot find name 'ImageData'.

13     fromPixels(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels: number): Tensor3D;
                          ~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/backend.d.ts:13:36 - error TS2304: Cannot find name 'HTMLImageElement'.

13     fromPixels(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels: number): Tensor3D;
                                      ~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/backend.d.ts:13:75 - error TS2304: Cannot find name 'HTMLVideoElement'.

13     fromPixels(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels: number): Tensor3D;
                                                                             ~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/backend.d.ts:40:24 - error TS2304: Cannot find name 'ImageData'.

40     fromPixels(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels: number): Tensor<Rank.R3>;
                          ~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/backend.d.ts:40:36 - error TS2304: Cannot find name 'HTMLImageElement'.

40     fromPixels(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels: number): Tensor<Rank.R3>;
                                      ~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/backend.d.ts:40:75 - error TS2304: Cannot find name 'HTMLVideoElement'.

40     fromPixels(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels: number): Tensor<Rank.R3>;
                                                                             ~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/backend_cpu.d.ts:14:24 - error TS2304: Cannot find name 'ImageData'.

14     fromPixels(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels: number): Tensor3D;
                          ~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/backend_cpu.d.ts:14:36 - error TS2304: Cannot find name 'HTMLImageElement'.

14     fromPixels(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels: number): Tensor3D;
                                      ~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/backend_cpu.d.ts:14:75 - error TS2304: Cannot find name 'HTMLVideoElement'.

14     fromPixels(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels: number): Tensor3D;
                                                                             ~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/backend_webgl.d.ts:51:24 - error TS2304: Cannot find name 'ImageData'.

51     fromPixels(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels: number): Tensor3D;
                          ~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/backend_webgl.d.ts:51:36 - error TS2304: Cannot find name 'HTMLImageElement'.

51     fromPixels(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels: number): Tensor3D;
                                      ~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/backend_webgl.d.ts:51:75 - error TS2304: Cannot find name 'HTMLVideoElement'.

51     fromPixels(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels: number): Tensor3D;
                                                                             ~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_context.d.ts:8:9 - error TS2304: Cannot find name 'WebGLRenderingContext'.

8     gl: WebGLRenderingContext;
          ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_context.d.ts:24:22 - error TS2304: Cannot find name 'WebGLRenderingContext'.

24     constructor(gl?: WebGLRenderingContext);
                        ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_context.d.ts:30:61 - error TS2304: Cannot find name 'ImageData'.

30     uploadPixelDataToTexture(texture: WebGLTexture, pixels: ImageData | HTMLImageElement | HTMLCanvasElement): void;
                                                               ~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_context.d.ts:30:73 - error TS2304: Cannot find name 'HTMLImageElement'.

30     uploadPixelDataToTexture(texture: WebGLTexture, pixels: ImageData | HTMLImageElement | HTMLCanvasElement): void;
                                                                           ~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:11:48 - error TS2304: Cannot find name 'WebGLRenderingContext'.

11 export declare function createVertexShader(gl: WebGLRenderingContext): WebGLShader;
                                                  ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:12:48 - error TS2304: Cannot find name 'WebGLRenderingContext'.

12 export declare function createVertexBuffer(gl: WebGLRenderingContext): WebGLBuffer;
                                                  ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:13:47 - error TS2304: Cannot find name 'WebGLRenderingContext'.

13 export declare function createIndexBuffer(gl: WebGLRenderingContext): WebGLBuffer;
                                                 ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:14:46 - error TS2304: Cannot find name 'WebGLRenderingContext'.

14 export declare function getTextureConfig(gl: WebGLRenderingContext, textureHalfFloatExtension?: any): TextureConfig;
                                                ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:15:56 - error TS2304: Cannot find name 'WebGLRenderingContext'.

15 export declare function createFloat32MatrixTexture(gl: WebGLRenderingContext, rows: number, columns: number, textureConfig: TextureConfig): WebGLTexture;
                                                          ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:16:56 - error TS2304: Cannot find name 'WebGLRenderingContext'.

16 export declare function createFloat16MatrixTexture(gl: WebGLRenderingContext, rows: number, columns: number, textureConfig: TextureConfig): WebGLTexture;
                                                          ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:17:62 - error TS2304: Cannot find name 'WebGLRenderingContext'.

17 export declare function createUnsignedBytesMatrixTexture(gl: WebGLRenderingContext, rows: number, columns: number, textureConfig: TextureConfig): WebGLTexture;
                                                                ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:18:55 - error TS2304: Cannot find name 'WebGLRenderingContext'.

18 export declare function createPackedMatrixTexture(gl: WebGLRenderingContext, rows: number, columns: number, textureConfig: TextureConfig): WebGLTexture;
                                                         ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:19:62 - error TS2304: Cannot find name 'WebGLRenderingContext'.

19 export declare function createFloat16PackedMatrixTexture(gl: WebGLRenderingContext, rows: number, columns: number, textureConfig: TextureConfig): WebGLTexture;
                                                                ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:20:63 - error TS2304: Cannot find name 'WebGLRenderingContext'.

20 export declare function bindVertexProgramAttributeStreams(gl: WebGLRenderingContext, program: WebGLProgram, vertexBuffer: WebGLBuffer): boolean;
                                                                 ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:21:54 - error TS2304: Cannot find name 'WebGLRenderingContext'.

21 export declare function uploadPixelDataToTexture(gl: WebGLRenderingContext, texture: WebGLTexture, pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void;
                                                        ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:21:108 - error TS2304: Cannot find name 'ImageData'.

21 export declare function uploadPixelDataToTexture(gl: WebGLRenderingContext, texture: WebGLTexture, pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void;
                                                                                                              ~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:21:120 - error TS2304: Cannot find name 'HTMLImageElement'.

21 export declare function uploadPixelDataToTexture(gl: WebGLRenderingContext, texture: WebGLTexture, pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void;
                                                                                                                          ~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:21:159 - error TS2304: Cannot find name 'HTMLVideoElement'.

21 export declare function uploadPixelDataToTexture(gl: WebGLRenderingContext, texture: WebGLTexture, pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void;
                                                                                                                                                                 ~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:22:51 - error TS2304: Cannot find name 'WebGLRenderingContext'.

22 export declare function uploadMatrixToTexture(gl: WebGLRenderingContext, texture: WebGLTexture, rows: number, columns: number, matrix: Float32Array, numChannels: number, textureConfig: TextureConfig): void;
                                                     ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:23:57 - error TS2304: Cannot find name 'WebGLRenderingContext'.

23 export declare function uploadMatrixToPackedTexture(gl: WebGLRenderingContext, texture: WebGLTexture, batch: number, rows: number, columns: number, matrix: Float32Array, textureConfig: TextureConfig): void;
                                                           ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:24:64 - error TS2304: Cannot find name 'WebGLRenderingContext'.

24 export declare function maybeCreateBufferFromOutputTexture(gl: WebGLRenderingContext, texture: WebGLTexture, rows: number, columns: number, textureConfig: TextureConfig): WebGLBuffer | WebGLTexture;
                                                                  ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:25:61 - error TS2304: Cannot find name 'WebGLRenderingContext'.

25 export declare function downloadFloat32MatrixFromBuffer(gl: WebGLRenderingContext, buffer: WebGLBuffer, rows: number, columns: number, textureConfig: TextureConfig): Float32Array;
                                                               ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:26:68 - error TS2304: Cannot find name 'WebGLRenderingContext'.

26 export declare function downloadFloat32MatrixFromOutputTexture(gl: WebGLRenderingContext, rows: number, columns: number, textureConfig: TextureConfig): Float32Array;
                                                                      ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:27:77 - error TS2304: Cannot find name 'WebGLRenderingContext'.

27 export declare function downloadByteEncodedFloatMatrixFromOutputTexture(gl: WebGLRenderingContext, rows: number, columns: number, textureConfig: TextureConfig): Float32Array;
                                                                               ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/gpgpu_util.d.ts:28:67 - error TS2304: Cannot find name 'WebGLRenderingContext'.

28 export declare function downloadMatrixFromPackedOutputTexture(gl: WebGLRenderingContext, batch: number, rows: number, cols: number, physicalRows: number, physicalCols: number, textureConfig: TextureConfig): Float32Array;
                                                                     ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:1:45 - error TS2304: Cannot find name 'WebGLRenderingContext'.

1 export declare function callAndCheck<T>(gl: WebGLRenderingContext, func: () => T): T;
                                              ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:3:45 - error TS2304: Cannot find name 'WebGLRenderingContext'.

3 export declare function checkWebGLError(gl: WebGLRenderingContext): void;
                                              ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:4:50 - error TS2304: Cannot find name 'WebGLRenderingContext'.

4 export declare function getWebGLErrorMessage(gl: WebGLRenderingContext, status: number): string;
                                                   ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:5:49 - error TS2304: Cannot find name 'WebGLRenderingContext'.

5 export declare function getExtensionOrThrow(gl: WebGLRenderingContext, extensionName: string): {};
                                                  ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:6:48 - error TS2304: Cannot find name 'WebGLRenderingContext'.

6 export declare function createVertexShader(gl: WebGLRenderingContext, vertexShaderSource: string): WebGLShader;
                                                 ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:7:50 - error TS2304: Cannot find name 'WebGLRenderingContext'.

7 export declare function createFragmentShader(gl: WebGLRenderingContext, fragmentShaderSource: string): WebGLShader;
                                                   ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:8:43 - error TS2304: Cannot find name 'WebGLRenderingContext'.

8 export declare function createProgram(gl: WebGLRenderingContext): WebGLProgram;
                                            ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:9:41 - error TS2304: Cannot find name 'WebGLRenderingContext'.

9 export declare function linkProgram(gl: WebGLRenderingContext, program: WebGLProgram): void;
                                          ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:10:45 - error TS2304: Cannot find name 'WebGLRenderingContext'.

10 export declare function validateProgram(gl: WebGLRenderingContext, program: WebGLProgram): void;
                                               ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:11:54 - error TS2304: Cannot find name 'WebGLRenderingContext'.

11 export declare function createStaticVertexBuffer(gl: WebGLRenderingContext, data: Float32Array): WebGLBuffer;
                                                        ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:12:53 - error TS2304: Cannot find name 'WebGLRenderingContext'.

12 export declare function createStaticIndexBuffer(gl: WebGLRenderingContext, data: Uint16Array): WebGLBuffer;
                                                       ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:14:43 - error TS2304: Cannot find name 'WebGLRenderingContext'.

14 export declare function createTexture(gl: WebGLRenderingContext): WebGLTexture;
                                             ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:16:47 - error TS2304: Cannot find name 'WebGLRenderingContext'.

16 export declare function createFramebuffer(gl: WebGLRenderingContext): WebGLFramebuffer;
                                                 ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:17:64 - error TS2304: Cannot find name 'WebGLRenderingContext'.

17 export declare function bindVertexBufferToProgramAttribute(gl: WebGLRenderingContext, program: WebGLProgram, attribute: string, buffer: WebGLBuffer, arrayEntriesPerItem: number, itemStrideInBytes: number, itemOffsetInBytes: number): boolean;
                                                                  ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:18:45 - error TS2304: Cannot find name 'WebGLRenderingContext'.

18 export declare function bindTextureUnit(gl: WebGLRenderingContext, texture: WebGLTexture, textureUnit: number): void;
                                               ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:19:47 - error TS2304: Cannot find name 'WebGLRenderingContext'.

19 export declare function unbindTextureUnit(gl: WebGLRenderingContext, textureUnit: number): void;
                                                 ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:20:62 - error TS2304: Cannot find name 'WebGLRenderingContext'.

20 export declare function getProgramUniformLocationOrThrow(gl: WebGLRenderingContext, program: WebGLProgram, uniformName: string): WebGLUniformLocation;
                                                                ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:21:55 - error TS2304: Cannot find name 'WebGLRenderingContext'.

21 export declare function getProgramUniformLocation(gl: WebGLRenderingContext, program: WebGLProgram, uniformName: string): WebGLUniformLocation;
                                                         ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:22:64 - error TS2304: Cannot find name 'WebGLRenderingContext'.

22 export declare function bindTextureToProgramUniformSampler(gl: WebGLRenderingContext, program: WebGLProgram, texture: WebGLTexture, uniformSamplerLocation: WebGLUniformLocation, textureUnit: number): void;
                                                                  ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:23:53 - error TS2304: Cannot find name 'WebGLRenderingContext'.

23 export declare function bindCanvasToFramebuffer(gl: WebGLRenderingContext): void;
                                                       ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:24:59 - error TS2304: Cannot find name 'WebGLRenderingContext'.

24 export declare function bindColorTextureToFramebuffer(gl: WebGLRenderingContext, texture: WebGLTexture, framebuffer: WebGLFramebuffer): void;
                                                             ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:25:63 - error TS2304: Cannot find name 'WebGLRenderingContext'.

25 export declare function unbindColorTextureFromFramebuffer(gl: WebGLRenderingContext, framebuffer: WebGLFramebuffer): void;
                                                                 ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:26:49 - error TS2304: Cannot find name 'WebGLRenderingContext'.

26 export declare function validateFramebuffer(gl: WebGLRenderingContext): void;
                                                   ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/kernels/webgl/webgl_util.d.ts:27:56 - error TS2304: Cannot find name 'WebGLRenderingContext'.

27 export declare function getFramebufferErrorMessage(gl: WebGLRenderingContext, status: number): string;
                                                          ~~~~~~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/ops/array_ops.d.ts:11:38 - error TS2304: Cannot find name 'ImageData'.

11 declare function fromPixels_(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels?: number): Tensor3D;
                                        ~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/ops/array_ops.d.ts:11:50 - error TS2304: Cannot find name 'HTMLImageElement'.

11 declare function fromPixels_(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels?: number): Tensor3D;
                                                    ~~~~~~~~~~~~~~~~

node_modules/@tensorflow/tfjs-core/dist/ops/array_ops.d.ts:11:89 - error TS2304: Cannot find name 'HTMLVideoElement'.

11 declare function fromPixels_(pixels: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement, numChannels?: number): Tensor3D;
                                                                                           ~~~~~~~~~~~~~~~~

node_modules/@types/gl-texture2d/index.d.ts:11:18 - error TS2304: Cannot find name 'ImageData'.

11 type InputType = ImageData | HTMLCanvasElement | HTMLImageElement | HTMLVideoElement;
                    ~~~~~~~~~

node_modules/@types/gl-texture2d/index.d.ts:11:50 - error TS2304: Cannot find name 'HTMLImageElement'.

11 type InputType = ImageData | HTMLCanvasElement | HTMLImageElement | HTMLVideoElement;
                                                    ~~~~~~~~~~~~~~~~

node_modules/@types/gl-texture2d/index.d.ts:11:69 - error TS2304: Cannot find name 'HTMLVideoElement'.

11 type InputType = ImageData | HTMLCanvasElement | HTMLImageElement | HTMLVideoElement;
                                                                       ~~~~~~~~~~~~~~~~

node_modules/@types/gl-texture2d/index.d.ts:26:18 - error TS2304: Cannot find name 'WebGLRenderingContext'.

26     readonly gl: WebGLRenderingContext;
                    ~~~~~~~~~~~~~~~~~~~~~

node_modules/@types/gl-texture2d/index.d.ts:37:32 - error TS2304: Cannot find name 'WebGLRenderingContext'.

37 declare function texture2d(gl: WebGLRenderingContext, array: ndarray): Texture;
                                  ~~~~~~~~~~~~~~~~~~~~~

node_modules/@types/gl-texture2d/index.d.ts:40:9 - error TS2304: Cannot find name 'WebGLRenderingContext'.

40     gl: WebGLRenderingContext,
           ~~~~~~~~~~~~~~~~~~~~~

node_modules/@types/webgl-ext/index.d.ts:11:76 - error TS2304: Cannot find name 'WebGLContextAttributes'.

11  getContext(contextId: ""webgl"" | ""experimental-webgl"", contextAttributes?: WebGLContextAttributes): (WebGLRenderingContext & WebGL1Extensions) | null;
                                                                              ~~~~~~~~~~~~~~~~~~~~~~

node_modules/@types/webgl-ext/index.d.ts:11:102 - error TS2304: Cannot find name 'WebGLRenderingContext'.

11  getContext(contextId: ""webgl"" | ""experimental-webgl"", contextAttributes?: WebGLContextAttributes): (WebGLRenderingContext & WebGL1Extensions) | null;
                                                                                                        ~~~~~~~~~~~~~~~~~~~~~

node_modules/@types/webgl-ext/index.d.ts:22:63 - error TS2304: Cannot find name 'EXT_texture_filter_anisotropic'.

22  getExtension(name: ""WEBKIT_EXT_texture_filter_anisotropic""): EXT_texture_filter_anisotropic; // Chrome
                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

node_modules/@types/webgl-ext/index.d.ts:25:62 - error TS2304: Cannot find name 'WEBGL_compressed_texture_s3tc'.

25  getExtension(name: ""WEBKIT_WEBGL_compressed_texture_s3tc""): WEBGL_compressed_texture_s3tc; // Chrome
                                                                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

node_modules/@types/webgl-ext/index.d.ts:26:52 - error TS2304: Cannot find name 'WEBGL_depth_texture'.

26  getExtension(name: ""WEBKIT_WEBGL_depth_texture""): WEBGL_depth_texture; // Chrome
                                                      ~~~~~~~~~~~~~~~~~~~

node_modules/@types/webgl-ext/index.d.ts:27:51 - error TS2304: Cannot find name 'WEBGL_lose_context'.

27  getExtension(name: ""WEBKIT_WEBGL_lose_context""): WEBGL_lose_context; // Chrome
                                                     ~~~~~~~~~~~~~~~~~~

node_modules/@types/webgl2/index.d.ts:7:37 - error TS2304: Cannot find name 'HTMLElement'.

7 interface HTMLCanvasElement extends HTMLElement {
                                      ~~~~~~~~~~~

node_modules/@types/webgl2/index.d.ts:8:81 - error TS2304: Cannot find name 'WebGLContextAttributes'.

8     getContext(contextId: ""webgl2"" | ""experimental-webgl2"", contextAttributes?: WebGLContextAttributes): WebGL2RenderingContext | null;
                                                                                  ~~~~~~~~~~~~~~~~~~~~~~

node_modules/@types/webgl2/index.d.ts:17:42 - error TS2304: Cannot find name 'WebGLRenderingContext'.

17 interface WebGL2RenderingContext extends WebGLRenderingContext {
                                            ~~~~~~~~~~~~~~~~~~~~~

node_modules/@types/webgl2/index.d.ts:337:47 - error TS2304: Cannot find name 'ImageData'.

337         format: number, type: number, source: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException
                                                  ~~~~~~~~~

node_modules/@types/webgl2/index.d.ts:337:59 - error TS2304: Cannot find name 'HTMLImageElement'.

337         format: number, type: number, source: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException
                                                              ~~~~~~~~~~~~~~~~

node_modules/@types/webgl2/index.d.ts:337:98 - error TS2304: Cannot find name 'HTMLVideoElement'.

337         format: number, type: number, source: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException
                                                                                                     ~~~~~~~~~~~~~~~~

node_modules/@types/webgl2/index.d.ts:339:61 - error TS2304: Cannot find name 'ImageData'.

339         format: number, type: number, source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException
                                                                ~~~~~~~~~

node_modules/@types/webgl2/index.d.ts:339:73 - error TS2304: Cannot find name 'HTMLImageElement'.

339         format: number, type: number, source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException
                                                                            ~~~~~~~~~~~~~~~~

node_modules/@types/webgl2/index.d.ts:339:112 - error TS2304: Cannot find name 'HTMLVideoElement'.

339         format: number, type: number, source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException
                                                                                                                   ~~~~~~~~~~~~~~~~

node_modules/@types/webgl2/index.d.ts:345:47 - error TS2304: Cannot find name 'ImageData'.

345         format: number, type: number, source: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException
                                                  ~~~~~~~~~

node_modules/@types/webgl2/index.d.ts:345:59 - error TS2304: Cannot find name 'HTMLImageElement'.

345         format: number, type: number, source: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException
                                                              ~~~~~~~~~~~~~~~~

node_modules/@types/webgl2/index.d.ts:345:98 - error TS2304: Cannot find name 'HTMLVideoElement'.

345         format: number, type: number, source: ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException
                                                                                                     ~~~~~~~~~~~~~~~~

node_modules/@types/webgl2/index.d.ts:347:61 - error TS2304: Cannot find name 'ImageData'.

347         format: number, type: number, source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException
                                                                ~~~~~~~~~

node_modules/@types/webgl2/index.d.ts:347:73 - error TS2304: Cannot find name 'HTMLImageElement'.

347         format: number, type: number, source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException
                                                                            ~~~~~~~~~~~~~~~~

node_modules/@types/webgl2/index.d.ts:347:112 - error TS2304: Cannot find name 'HTMLVideoElement'.

347         format: number, type: number, source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException
                                                                                                                   ~~~~~~~~~~~~~~~~

node_modules/@types/webgl2/index.d.ts:354:31 - error TS2304: Cannot find name 'ImageData'.

354         source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException
                                  ~~~~~~~~~

node_modules/@types/webgl2/index.d.ts:354:43 - error TS2304: Cannot find name 'HTMLImageElement'.

354         source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException
                                              ~~~~~~~~~~~~~~~~

node_modules/@types/webgl2/index.d.ts:354:82 - error TS2304: Cannot find name 'HTMLVideoElement'.

354         source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException
                                                                                     ~~~~~~~~~~~~~~~~

node_modules/@types/webgl2/index.d.ts:363:31 - error TS2304: Cannot find name 'ImageData'.

363         source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException
                                  ~~~~~~~~~

node_modules/@types/webgl2/index.d.ts:363:43 - error TS2304: Cannot find name 'HTMLImageElement'.

363         source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException
                                              ~~~~~~~~~~~~~~~~

node_modules/@types/webgl2/index.d.ts:363:82 - error TS2304: Cannot find name 'HTMLVideoElement'.

363         source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException
                                                                                     ~~~~~~~~~~~~~~~~

node_modules/@types/webgl2/index.d.ts:374:31 - error TS2304: Cannot find name 'ImageData'.

374         source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException
                                  ~~~~~~~~~

node_modules/@types/webgl2/index.d.ts:374:43 - error TS2304: Cannot find name 'HTMLImageElement'.

374         source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException
                                              ~~~~~~~~~~~~~~~~

node_modules/@types/webgl2/index.d.ts:374:82 - error TS2304: Cannot find name 'HTMLVideoElement'.

374         source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException
                                                                                     ~~~~~~~~~~~~~~~~

node_modules/@types/webgl2/index.d.ts:384:31 - error TS2304: Cannot find name 'ImageData'.

384         source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException
                                  ~~~~~~~~~

node_modules/@types/webgl2/index.d.ts:384:43 - error TS2304: Cannot find name 'HTMLImageElement'.

384         source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException
                                              ~~~~~~~~~~~~~~~~

node_modules/@types/webgl2/index.d.ts:384:82 - error TS2304: Cannot find name 'HTMLVideoElement'.

384         source: ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): void; // May throw DOMException
                                                                                     ~~~~~~~~~~~~~~~~

node_modules/@types/webgl2/index.d.ts:560:72 - error TS2304: Cannot find name 'WebGLActiveInfo'.
```


**Provide the exact sequence of commands / steps that you executed before running into the problem**

My tsconfig : 
```
{
  ""compilerOptions"": {
    ""strict"": true,
    ""noImplicitAny"": false,
    ""target"": ""es2017"",
    ""module"": ""commonjs"",
    ""sourceMap"": true,
    ""skipLibCheck"": true,
    ""lib"": [
      ""esnext""
    ],
    ""moduleResolution"": ""node"",
    ""outDir"": ""dist"",
    ""baseUrl"": ""dist"",
    ""noEmitOnError"": true
  },
  ""include"": [
    ""src""
  ],
  ""exclude"": [
    ""node_modules"",
    ""dist"",
    ""eth-contracts""
  ]
}
```

Any advices ? 
"
23809,Cannot deploy trained model: google.protobuf.message.DecodeError: Error parsing message,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS High Sierra 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 1.18
- Python version: 3.6.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: No GPU


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
When I try to deploy my trained model using this function:
def load_graph(frozen_graph_filename):
	with tf.gfile.GFile(frozen_graph_filename, ""rb"") as f:
		graph_def = tf.GraphDef()
		graph_def.ParseFromString(f.read())

	with tf.Graph().as_default() as graph:
		tf.import_graph_def(graph_def, name='prefix')
	return graph
I get this error: google.protobuf.message.DecodeError: Error parsing message
at the line that goes:
graph_def.ParseFromString(f.read())

**Describe the expected behavior**
For all my other trained models, this error doesn't pop up, and this method does work. However, this model is one I trained myself. I got it from the export folder the TF Object Detection API made in my folder.

**Code to reproduce the issue**
def load_graph(frozen_graph_filename):
	with tf.gfile.GFile(frozen_graph_filename, ""rb"") as f:
		graph_def = tf.GraphDef()
		graph_def.ParseFromString(f.read())

	with tf.Graph().as_default() as graph:
		tf.import_graph_def(graph_def, name='prefix')
	return graph

Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Traceback (most recent call last):
  File ""/Users/spencerkraisler/Desktop/raccoon_tutorial/model_deploy.py"", line 27, in <module>
    detection_graph = load_graph(GRAPH_PATH)
  File ""/Users/spencerkraisler/Desktop/raccoon_tutorial/model_deploy.py"", line 10, in load_graph
    graph_def.ParseFromString(f.read())
google.protobuf.message.DecodeError: Error parsing message"
23807,Convert pb model to tflite model failed in docker environment,"I m using the docker image pulled from here:
https://hub.docker.com/r/tensorflow/tensorflow/tags
the tag is nightly-devel-py3.
And then I upgrade the tf-nightly to  tf-nightly 1.13.0.dev20181116 using pip.

**System information**
- i7-8650U+16G+ 1060
- Windows Pro Docker Environment(CPU mode)
- tf-nightly 1.13.0.dev20181116
- Python version:

It failed when I trying to convert pb model to tflite model.
The error info says that some operators doesn't supported in the TFLiteConverter.
Can anyone help me to fix it?
Thanks.

**My Command Line**
tflite_convert --output_file='/data/fast-neural-style-train&test/models/result.tflite' --graph_def_file='/data/fast-neural-style-train&test/models/test_model.pb' --input_arrays=input --output_arrays=output_new --input_shapes=256,256,3

**Error Info**
2018-11-16 03:45:33.142433: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Traceback (most recent call last):
  File ""/usr/local/bin/tflite_convert"", line 11, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/lite/python/tflite_convert.py"", line 421, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/lite/python/tflite_convert.py"", line 417, in run_main
    _convert_model(tflite_flags)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/lite/python/tflite_convert.py"", line 170, in _convert_model
    output_data = converter.convert()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/lite/python/lite.py"", line 456, in convert
    **converter_kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/lite/python/convert.py"", line 397, in toco_convert_impl
    input_data.SerializeToString())
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/lite/python/convert.py"", line 172, in toco_convert_protos
    ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
2018-11-16 03:45:34.088574: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: Round
2018-11-16 03:45:34.095390: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: Round
2018-11-16 03:45:34.095586: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2018-11-16 03:45:34.095621: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2018-11-16 03:45:34.095652: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2018-11-16 03:45:34.095684: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2018-11-16 03:45:34.095801: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2018-11-16 03:45:34.095848: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2018-11-16 03:45:34.095925: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.096229: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.096281: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: SquaredDifference
2018-11-16 03:45:34.096329: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.096374: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: SquaredDifference
2018-11-16 03:45:34.096455: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.096501: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: SquaredDifference
2018-11-16 03:45:34.096631: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.096783: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.096957: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.097208: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.097403: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.097626: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.097862: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.098174: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.098459: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.098733: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.098940: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.098991: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: SquaredDifference
2018-11-16 03:45:34.099099: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.099149: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: SquaredDifference
2018-11-16 03:45:34.099194: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.099242: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: SquaredDifference
2018-11-16 03:45:34.100307: F tensorflow/lite/toco/tooling_util.cc:1020] Check failed: array->has_shape()
Aborted
"
23806,Register.h not found,"For the Xcode iOS project, I follow the steps, however during the build for device process, Xcode gives me an error stating that it cannot find the register.h file.

**System information**
- OS Platform and Distribution: macOS Mojave 10.14.2 and High Sierra 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): clones repo, followed instructions on website
- TensorFlow version: 1.12
- Python version: 3.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23804,remove op Save also in remove_training_nodes method,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): master branch
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
In current `remove_training_nodes` method, we just remove `CheckNumerics` and `Identity` (conditional).
As `Save` is a training nodes, I think we should remove it also.

**Will this change the current api? How?**
No.

**Who will benefit with this feature?**
`remove_training_nodes` users.
"
23803,TF1.12 building w/ py3.6.4 on HPC,"Hello, 
I am trying to (re-)build Tensorflow on the HPC I use. I already managed to build once TF 1.9 but with Python 3.4 and probably another compiler. For the rest I tried to keep everything the same, such as bazel with the suggested version. Unfortunately I was able to build only GCC 5.5.0 instead of 4.8.0, but I don't think it is the problem here as I am using the flag --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"". 

In this case I am trying to build TF 1.12 with Py3.6.4, but I am experiencing problems (a bit different) with TF 1.9 as well. I am not sure of the path to the Python Library that I give at the beginning.
Thanks.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 3.16.51-3
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.12
- Python version: 3.6.4
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): 1.5.0
- GCC/Compiler version (if compiling from source): 5.5.0
- CUDA/cuDNN version: 9.1.85/7.1.2
- GPU model and memory: K80

**Problem Description**
ERROR: /mnt/gaiagpfs/users/homedirs/ccimarelli/.cache/bazel/_bazel_ccimarelli/9669602cb65c53050fa19def8832a149/external/nasm/BUILD.bazel:8:1: undeclared
inclusion(s) in rule '@nasm//:nasm':
this rule is missing dependency declarations for the following files included by 'external/nasm/x86/regflags.c':
  '/home/users/ccimarelli/opt/gcc/GCC-5.5.0/lib/gcc/x86_64-unknown-linux-gnu/5.5.0/include-fixed/limits.h'
  '/home/users/ccimarelli/opt/gcc/GCC-5.5.0/lib/gcc/x86_64-unknown-linux-gnu/5.5.0/include-fixed/syslimits.h'
  '/home/users/ccimarelli/opt/gcc/GCC-5.5.0/lib/gcc/x86_64-unknown-linux-gnu/5.5.0/include/stddef.h'
  '/home/users/ccimarelli/opt/gcc/GCC-5.5.0/lib/gcc/x86_64-unknown-linux-gnu/5.5.0/include/stdarg.h'

**Sequence of commands / steps executed before running into the problem**
$ ./cofigure
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.15.0 installed.
Please specify the location of python. [Default is /home/users/.../virtualenvs/ml-full/bin/python]:
Please input the desired Python library path to use.  Default is [/home/users/.../virtualenvs/ml-full/lib/python3.6/site-packages]

Do you wish to build TensorFlow with Apache Ignite support? [Y/n]: n
No Apache Ignite support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [Y/n]: n
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.
 Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: 9.1

Please specify the location where CUDA 9.1 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /opt/apps/resif/data/production/v1.1-20180718/default/software/system/CUDA/9.1.85

Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 7.1

Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /opt/apps/resif/data/production/v1.1-20180718/default/software/system/CUDA/9.1.85]: /opt/apps/resif/data/production/v1.1-20180718/default/software/numlib/cuDNN/7.1.2-CUDA-9.1.85

Do you wish to build TensorFlow with TensorRT support? [y/N]:
No TensorRT support will be enabled for TensorFlow.

Please specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may
have worse performance with multiple GPUs. [Default is 2.2]: 1.3

Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.7,3.7,3.7,3.7,3.7,3.7,3.7,3.7]: 3.5,3.7,7.0

Do you want to use clang as CUDA compiler? [y/N]:
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /home/users/ccimarelli/opt/gcc/GCC-5.5.0/bin/gcc]:

$ bazel build --config=opt --config=cuda --spawn_strategy=standalone --action_env=TMP=/home/users/ccimarelli/tmp --verbose_failures --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/tools/pip_package:build_pip_package



"
23802,Reduced redundancy in Estimator api,"


**System information**
- TensorFlow version (you are using): 1.10+
- Are you willing to contribute it (Yes/No): yes



**Describe the feature and the current behavior/state.**

The TensorFlow estimator api, while a welcomed addition, seems to add a lot of redundancy to the workflow as well as unnecessary complexity. While a prominent goal of the estimator API is to decouple the input pipelines from the model, unfortunately this is not fully realized due to this redundancy and complexity.

# Example

Suppose we wish to make a custom estimator that has an `input_fn` which reads from TF `Record` files via `tf.data.TFRecordDataset(files)` and further, we wish to save our trained model so we can use it later (i.e. we need a `serving_input_reciever_fn`).  

For simplicity lets assume for a single ""example"" that after our input pipeline we are left with a tensor with shape [2,3], perhaps:

    [[0,0,1],[1,0,0]]

and let us say that contextually we will refer to this feature as ""main_input"".

Then we now need to wrap this into a `(Sequence)Example` (*NOTE*: see this [S.O. post](https://stackoverflow.com/questions/52035692/tensorflow-v1-10-store-images-as-byte-strings-or-per-channel) for the lack of clarity the documentation provides on how to best do this)



```
# all of input pipeline here

# processed data
main_input = np.array([
    [0, 0, 1],
    [1, 0, 0],
])

# wrap each channel in Feature then wrap all in FeatureList
main_input_feat_list = tf.train.FeatureList(
    feature=[
        tf.train.Feature(float_list=tf.train.FloatList(value=value)) 
        for value in main_input
    ]
)

# wrap in SequenceExample
example = tf.train.SequenceExample(
    context={
        # other stuff could go here
    }, 
    feature_lists=tf.train.FeatureLists(feature_list={
        'main_input': main_input_feat_list,
        # other stuff could go here
    })
)


# write to record
with tf.python_io.TFRecordWriter('demo_example.tfrecord') as writer:
    writer.write(example.SerializeToString())
```

from here, we now need to unwrap this from TF `Record` in our `Estimator`'s `input_fn` using a _different_ api:
```

def parse_record(record):
    return tf.parse_single_sequence_example(
        record, 
        context_features={
            # get context features here
        }, 
        sequence_features={
            'main_input': tf.FixedLenSequenceFeature((3, ), tf.float32), # <---- REDUNDANT
            # get other sequence features here
        }
    )


def input_fn(files:list=['demo_example.tfrecord'], params:dict):
    dataset = tf.data.TFRecordDataset(files).map(lambda record: parse_record(record))
    # reformat dataset to return feature, label pairs
    return dataset


```

Now if we want to use `tf.estimator.BestExporter` we need a `serving_input_receiver_fn` so lets go ahead and get that written:


```
def serving_input_receiver_fn():
    batch_size = None
    main_input = tf.placeholder(tf.float32, (batch_size, 2, 3), name='main_input') # <---- REDUNDANT

   
    features = {'main_input': main_input}
    
    # unclear what the difference is between features and receiver tensors
    receiver_tensors = features
    return  tf.estimator.export.ServingInputReceiver(features, receiver_tensors)


exporter = tf.estimator.BestExporter(
    name='best_exporter', 
    serving_input_receiver_fn= serving_input_receiver_fn,
    exports_to_keep=3
)
```

So while the input pipeline should be decoupled from the model, the model's first layer at the very least is defined by the input. Therefore I should not have to three different times, in three different ways tell TensorFlow what my input features are.

```

        # 1st to write to TF Records 
        feature_lists=tf.train.FeatureLists(feature_list={
            'main_input':  tf.train.FeatureList(
                feature=[
                    tf.train.Feature(float_list=tf.train.FloatList(value=value)) 
                    for value in main_input
                ]
            ),
            # other stuff could go here
        })


        # then to get it out of TF Records with an asymmetric api
        #...
        'main_input': tf.FixedLenSequenceFeature((3, ), tf.float32)
        #...

        # and once more I have to define placeholders for the features in the serving_input_receiver_fn
        main_input = tf.placeholder(tf.float32, (batch_size, 2, 3), name='main_input') # <---- REDUNDANT
        

```

# Proposal to solve

In this [colab](https://colab.research.google.com/drive/1HrSYF1I7rBGaNQ7388Ss3epWPLTloEC6) I demonstrate the fledgling idea of how to solve this, named [""FIO"" (Feature Input / Output)](https://pypi.org/project/fio/) whereby instead of having to redefine everything many times over with different apis depending on the case, you can define everything once in a data ""schema"" e.g.


```
# features here are of an actual example (not batched)
features = {
    'my-feature': 'hi',
    'seq': np.array([
        # ch1, ch2, ch3
        [   1,   1,  1], # element 1
        [   2,   2,  2], # element 2
        [   3,   3,  3], # element 3
        [   4,   5,  6]  # element 4
    ])
}


# here we specify what is needed to encode and decode from `(Sequence)Example` and `TF Records`

SCHEMA = {
    'my-feature': {'length': 'fixed', 'dtype': tf.string,  'shape': []},
    'seq': {
        'length': 'fixed',
        'dtype': tf.int64,
        'shape': [4, 3],
        'encode': 'channels',
        'channel_names': ['A', 'B', 'C'],
        'data_format': 'channels_last'
    }
}
```

then
```

# define our converter
fio = FIO(SCHEMA, etype='example')

# write to example
file = 'example.tfrecord'

example = fio.to_example(features)
with tf.python_io.TFRecordWriter(file) as writer:
    writer.write(example.SerializeToString())

# use a sequence example instead
fio = FIO(SCHEMA, etype='sequence_example', sequence_features=['seq'])
file = 'sequence_example.tfrecord'
with tf.python_io.TFRecordWriter(file) as writer:
    writer.write(example.SerializeToString())

```

where in both cases we can read from records with

```
tf.data.TFRecordDataset(DATASET_FILENAMES).map(lambda r: fio.from_record(r))
```

Thus I propose that TensorFlow add's a new component to the estimator class, which would be the bride between the input pipelines / serving input functions and the `model_fn`. This would be something like the data schema. 

A user can define a data schema instance once and import it into the input pipeline to write their inputs to TF Record, import the same schema into their `Estimator`'s `input_fn` to read from TF `Record`s (or change the TF Record file format slightly so that you do not have define how it should be read, like how I do not have to define the keys in a json file to read a json file) and then once more it can be passed to (or automatically used by the `Estimator`) for the `serving_input_receiver_fn`.  

I believe letting users declaratively, upfront, define what the `Estimator` accepts in terms of data and then not forcing them to, at least trice, re-write what the data is, would be a great simplifier and clean up the `Estimator` api


**Will this change the current api? How?**

Technically, everything ""under the hood"" could remain the same. The changes would be made to `tf.data` to introduce a `Schema` class which would work as a unifier across the multiple ways of encoding / decoding `tf`/`np` tensors to / from TF Records. In addition, the `Estimator` class would be updated to accept a `tf.data.Schema` instance, from which it would automate the redundancy described above.

**Who will benefit with this feature?**
Anyone who uses the `Estimator` api, anyone who uses `TF Records`, anyone who acknowledges that TensorFlow's documentation is far from complete and examples are needed to bridge how the ""high"" level apis quite often interface directly with lower (or even the lowest) level apis (e.g. `Estimators` which deal with `Records` or users who want to have an exporter which requires a `ServingInputReceiver`)

**Any Other info.**
Please look at [FIO](https://pypi.org/project/fio/) and generalize this to work as described above.


See these S.O. Posts and their linked Colab's to follow my journey in trying to decipher what should be a high level api, but it turns out to be fairly tethered to lower features.


[optimal way to store tensors at TF Records](https://stackoverflow.com/questions/52035692/tensorflow-v1-10-store-images-as-byte-strings-or-per-channel)

[recovering TF Records](https://stackoverflow.com/questions/52064866/tensorflow-1-10-tfrecorddataset-recovering-tfrecords)

[early stopping](https://stackoverflow.com/questions/52641737/tensorflow-1-10-custom-estimator-early-stopping-with-train-and-evaluate)

[what are serving_input_receiver_fn](https://stackoverflow.com/questions/52874647/tensorflow-v1-10-why-is-an-input-serving-receiver-function-needed-when-checkpoi)

[using an estimator after training](https://stackoverflow.com/questions/53307954/tensorflow-custom-estimator-predict-throwing-value-error)

[defining estimator spec throws error](https://stackoverflow.com/questions/53317235/tensorflow-custom-estimators-defining-estimator-spec-triggers-error)"
23801,How to train my dataset(audio samples) in tensorflow ( simple audio recognition).,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No , i am using simple audio recognition example 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): https://www.tensorflow.org
- TensorFlow version (use command below):r1.12
- Python version:3.6
- Bazel version (if compiling from source):N.A
- GCC/Compiler version (if compiling from source):N.A
- CUDA/cuDNN version:N.A
- GPU model and memory:N.A


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
When i want to train my own dataset (without url). i already commented --data_url  but i got positional argument error.


**Code to reproduce the issue**
'''
(base) H:\tensorflow-r1.10>python tensorflow/examples/speech_commands/train.py --data_dir=tensorflow/examples/speech_commands/dataset
H:\Anaconda\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of
the second argument of issubdtype from `float` to `np.floating` is deprecated. I
n future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Traceback (most recent call last):
  File ""tensorflow/examples/speech_commands/train.py"", line 463, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""H:\Anaconda\lib\site-packages\tensorflow\python\platform\app.py"", line 1
25, in run
    _sys.exit(main(argv))
  File ""tensorflow/examples/speech_commands/train.py"", line 107, in main
    FLAGS.testing_percentage, model_settings, FLAGS.summaries_dir)
TypeError: __init__() missing 1 required positional argument: 'summaries_dir'
'''
Where i was wrong and what i needs to do?"
23800,TFLite:  numeric precision issue in Conv int8 implementation,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No, but dump the runtime data
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Null
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.11.0 and 1.12.0
- Python version: 3.6
- Bazel version (if compiling from source): 0.16.1
- GCC/Compiler version (if compiling from source): 5.5.0
- CUDA/cuDNN version: No
- GPU model and memory: No

**Describe the current behavior**
When running mobiletnet int8 model, starting from the third Conv(`MobilenetV1/MobilenetV1/Conv2d_2_pointwise/Relu6`), TFLite's output mismatches with another framework.

I looked into it and found that [GetQuantizedConvolutionMultipler()](https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/contrib/lite/kernels/kernel_util.cc#L25) generates unexpected results with the calculation `multiplier = input_scale * weight_scale / output_scale`. By *unexpected* I mean the math result is wrong, on my machine at least.

Log example of parameter of `MobilenetV1/MobilenetV1/Conv2d_2_pointwise/Relu6` is as below. Since `input_scale` and `output_scale` are the same, the `real_mulpliter` shall be same as `filter_scale` - more even, these parameters shall be the same for most Conv ops in quantized MobileNetV1.
```conv pre computed args --------------------------
            input scale : 0.023528477177023888
            filter scale : 0.015148180536925793
            output scale : 0.023528477177023888
            real_multiplier: 0.015148180864416702
            output_multiplier: 2081950125
```

**Describe the expected behavior**
Well, the expected behavior is that generates correct result.

**Code to reproduce the issue**
To generates the log in r1.11, try https://github.com/jackwish/tensorflow/tree/d/lite/qconv-r1.11 . Note that, I have already used **[another code style](https://github.com/jackwish/tensorflow/commit/f3677c512ebe06a2dd0dda129ba9ef71de639d87#diff-ca0f46c80fd3cf1f2040be6147a2d8cfR47) such that the output multiplier is correct**.
You can also try [r1.12](https://github.com/jackwish/tensorflow/tree/d/lite/qconv-r1.12) (there is bug in dumping conv outputs here, but the `Prepare()` still has multiplier log).

**Other info / logs**
I was running quantized MobileNetV1  with TFLite on a x86 server. By chance, i noticed that some output of Conv int8 is unexpected. TFLite was invoked through python interfaces, the model comes from [TF model zoo](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md) (the TFLite model inside [MobileNet_v1_1.0_224_quant](http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_1.0_224_quant.tgz)). Nothing else special.

Note sure if there is similar behavior on other machine/devices."
23799,TFLite can not set input shape and input_stats with 1.0/128,"```
converter = tf.contrib.lite.TFLiteConverter.from_saved_model(
  saved_model_dir=sys.argv[1],
  input_shapes={'input' :[5, 3000, 40, 3]}
  )
converter.dump_graphviz_dir='./lite_dump'
converter.inference_type=tf.contrib.lite.constants.QUANTIZED_UINT8
converter.quantized_input_stats={'inputs': (127, 1.0/128)}
converter.default_ranges_stats=(0, 6)
tflite_model = converter.convert()
```
```
inputs
Type: Uint8 [1×3000×40×3] 
MinMax: [-16256, 16384]
Quantization:
128 * (x - 127)
```

The batch_size is 1 , not 5.
the input var is 128 not 1.0/128

```
interpreter.resize_tensor_input(input_details[0]['index'], [2, 3000, 40, 3])
interpreter.allocate_tensors()
interpreter.invoke()
```
```
Inputs [{'name': 'inputs', 'index': 17, 'shape': array([   1, 3000,   40,    3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]
Outputs [{'name': 'softmax', 'index': 24, 'shape': array([1, 2], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]
Inputs [{'name': 'inputs', 'index': 17, 'shape': array([   2, 3000,   40,    3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]
Traceback (most recent call last):
  File ""tflite_run.py"", line 15, in <module>
    interpreter.allocate_tensors()
  File ""/nfs/project/tools/anaconda3/envs/tf1.12_py3.5/lib/python3.6/site-packages/tensorflow/contrib/lite/python/interpreter.py"", line 71, in allocate_tensors
    return self._interpreter.AllocateTensors()
  File ""/nfs/project/tools/anaconda3/envs/tf1.12_py3.5/lib/python3.6/site-packages/tensorflow/contrib/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py"", line 106, in AllocateTensors
    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)
RuntimeError: tensorflow/contrib/lite/kernels/reshape.cc:58 num_input_elements != num_output_elements (1500 != 750)Node number 14 (RESHAPE) failed to prepare.
```"
23798,Docs Needed,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 1.10+
- Doc Link: https://www.tensorflow.org/api_docs/python/tf/estimator/export/ServingInputReceiver


**Describe the documentation issue**

It is unclear:

1. what a `serving_input_receiver_fn` is
2. why one is needed
3. how one should be written (e.g. what arguments are required in the function definition)
4. what should be returned 
5. what the difference is between the `features` argument of `tf.estimator.export.ServingInputReceiver` and the `receiver_tensors` argument is and why they are needed.


See this [S.O. post](https://stackoverflow.com/questions/52874647/tensorflow-v1-10-why-is-an-input-serving-receiver-function-needed-when-checkpoi) for an example of why it is unclear

"
23797,tf lite armeabi version is much slower than the nightly aar ,"      At first I was using  'org.tensorflow:tensorflow-lite:0.0.0-nightly'  aar in my own project. The tf lite model costs 900ms.
      But in my company project, the ndk abiFilters is armeabi.  I changed abiFilters from armeabi-v7a to armeabi. However,  I found the aar not support armeabi.
![image](https://user-images.githubusercontent.com/10142633/48612953-5f35f180-e9c5-11e8-9fde-9faddba7884e.png)
      I compile the armeabi version with the following command, and import so and jar file, but my model costs 4000ms. Is there some wrong with the command ??
bazel build --cxxopt='--std=c++11' //tensorflow/lite/java:tensorflowlite \
--crosstool_top=//external:android/crosstool \
--host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
--cpu=armeabi
     "
23796,Improve using of graph transform tool ,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 1.12.0
- Doc Link: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms


**Describe the documentation issue**
As newbie to Tensorflow it quite confusing seeing that you need to `bazel build` something, all I had to do before this point is `pip install`. To be more specific, what is bazel? Why do I need it to use GTT? Why is GTT not available from pip installation? Why when I'm going to _Build from source page_ it encourages me that you providing pre-built packages, where is GTT then?
"
23795,Windows 10 Bazel Build failed ,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.10
- Python version: 3.5
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): not compiled from source(bazel version: 0.18.1
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.0/7.0
- GPU model and memory: Zotac Gtx 1080ti mini



**Describe the problem**
Build fails with 1 error
initially i tried with cmake, it was giving few errors so started with bazel..no luck since many days! 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
i followed all the steps mentioned [here](https://www.tensorflow.org/install/source) 

`bazel build --copt=-nvcc_options=disable-warnings --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`

The reason i'm trying to build from source is because ssd_mobilnet retraining is taking longer than expected (3seconds per step)
resizing of image is happening on cpu and training on gpu.
if both happens on gpu then it should take around half second.

i have attached full build output 
[Build output.txt](https://github.com/tensorflow/tensorflow/files/2588467/Build.output.txt)

i've notived too many `warning LNK4044` in build process

below is the gist of build 

> build --copt=-nvcc_options=disable-warnings --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
> WARNING: Processed legacy workspace file e:\git_projects\tensorflow-experimental\tensorflow/tools/bazel.rc. This file will not be processed in the next release of Bazel. Please read https://github.com/bazelbuild/bazel/issues/6319 for further information, including how to upgrade.
> WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
> .
> .
> .
> .
> 
>    Creating library bazel-out/host/bin/tensorflow/cc/ops/image_ops_gen_cc.lib and object bazel-out/host/bin/tensorflow/cc/ops/image_ops_gen_cc.exp
> ERROR: E:/git_projects/tensorflow-experimental/tensorflow/tensorflow/core/kernels/BUILD:2123:1: C++ compilation of rule '//tensorflow/core/kernels:colorspace_op_gpu' failed (Exit 1): msvc_wrapper_for_nvcc.bat failed: error executing command
>   cd C:/users/neuroflares/_bazel_neuroflares/twu4fovw/execroot/org_tensorflow
> .
> .
> .
> .
> 
> 1 error detected in the compilation of ""C:/Users/user~1/AppData/Local/Temp/nvcc_inter_files_tmp_dir/colorspace_op_gpu.cu.cpp1.ii"".
> Target //tensorflow/tools/pip_package:build_pip_package failed to build
> INFO: Elapsed time: 1530.120s, Critical Path: 558.89s
> INFO: 878 processes: 878 local.
> FAILED: Build did NOT complete successfully


What could have gone wrong?
Thanks & Regards
"
23794,RuntimeError : about tf.Gradient() in eager mode,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):   Linux Ubuntu 16.04
- TensorFlow installed from (source or binary):  pip installed
- TensorFlow version (use command below):   1.10.0
- Python version: 3.5
- CUDA/cuDNN version: cuda-9.0, cudnn-7.2
- GPU model and memory:   GTX 1080 ti - 11G

**Describe the current behavior**
I have a huge model with large parameter, before train code, cost GPU memory is 7859MB, and  my local memory unused is 234M free(total is 32G)
![image](https://user-images.githubusercontent.com/10268274/48605840-dca73500-e9b8-11e8-84ac-8a7a8bb617ba.png)
![image](https://user-images.githubusercontent.com/10268274/48605858-ea5cba80-e9b8-11e8-92b9-3bc184829bba.png)

when I bug the code ,occur this warning:
```
2018-11-16 15:52:30.386249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-11-16 15:52:30.571530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-16 15:52:30.571559: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0
2018-11-16 15:52:30.571565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N
2018-11-16 15:52:30.571742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9772 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-11-16 15:52:34.410871: W tensorflow/core/framework/allocator.cc:108] Allocation of 423405000 exceeds 10% of system memory.
2018-11-16 15:52:34.602313: W tensorflow/core/framework/allocator.cc:108] Allocation of 372480000 exceeds 10% of system memory.

```
I will update my discriminator network so I compute gradient first and in eager mode, I used tf.GradientTape() for it.
But Exception here: I already used tf.GradientTape() for gradients but, it also warning tf.gradients is not supported when eager execution is enabled.  I did not use tf.gradient anywhere!!!!
![image](https://user-images.githubusercontent.com/10268274/48606113-a9b17100-e9b9-11e8-856b-e88a5bbed34e.png)

and I test demo code like line:1508-1511 in pictures, it worked well. But line:1557 it did not work with same code!!!!  I am sure that I did not use tf.gradients() in my code!!

**Describe the expected behavior**

It should be work 



**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
![image](https://user-images.githubusercontent.com/10268274/48606265-30664e00-e9ba-11e8-9688-553d27c0ab45.png)
"
23793,Tensorflow exception no module found with pyspark,"When I run a program with tensorflow an pyspark together with spark-submit sample.py it says no module named tensorflow but when I run activate tensorflow and run with python sample.py it says no module named pyspark.

import tensorflow as tf

import pyspark # only run after findspark.init()

from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

df = spark.sql('''select 'spark' as hello ''')

df.show()"
23792,How can I use tf.assign under mirroredstrategy,"When I used tf.assign under MirroredStrategy, I got valueError(you must specify an aggregation method to update a mirroredVariavle in towercontexy) 
How can I fix it? "
23791,TFLite support float input with uint8 inference,
23789,Dataset.map() with random_shuffle() and num_parallel_calls=1 has non-deterministic result,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 22
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.12
- Python version: 3.6.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
`tf.data.Dataset.range(100).batch(2).map(lambda x: tf.random_shuffle(x), num_parallel_calls=1)` is non-deterministic. Different runs of the test program produce different output. It's as if the `random_shuffle` is ignoring the random seed. Although adding a seed argument to random_shuffle makes the problem go away, random_shuffle should still use the graph-level random seed when the seed argument is unspecified.

**Describe the expected behavior**
Two different runs should always have the same output.

**Code to reproduce the issue**
```
#!/usr/bin/env python3

import tensorflow as tf

tf.random.set_random_seed(0)
rds = tf.data.Dataset.range(100).batch(2).map(
    lambda x: tf.random_shuffle(x), num_parallel_calls=1)
r = rds.make_one_shot_iterator().get_next()
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(4):
        x, = sess.run([r])
        print(x)
```

**Other info / logs**
```
$ py3/rds.py
2018-11-16 15:11:02.272589: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[1 0]
[3 2]
[4 5]
[7 6]
$ py3/rds.py
2018-11-16 15:11:04.833767: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[0 1]
[2 3]
[5 4]
[6 7]
```"
23785,Infinite loop when printing CSV data in eager execution mode ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac OS X Mojave
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No
- TensorFlow installed from (source or binary):
binary via pip (1.12.0)
- TensorFlow version (use command below):
v1.12.0-rc2-3-ga6d8ffae09 1.12.0
- Python version:
Python 3.6.5
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
Following the docs at: https://www.tensorflow.org/tutorials/eager/eager_basics
When using `ds_file = tf.data.TextLineDataset(filename)` the docs works as expected, but when using `tf.data.experimental.make_csv_dataset` function then then the first statement in the print loop is repeated infinitely.

Example output
```
1.12.0

Elements in ds_file:
OrderedDict([('Line 1', ), ('1', )])
OrderedDict([('Line 1', ), ('1', )])
OrderedDict([('Line 1', ), ('1', )])
OrderedDict([('Line 1', ), ('1', )])
OrderedDict([('Line 1', ), ('1', )])
OrderedDict([('Line 1', ), ('1', )])
OrderedDict([('Line 1', ), ('1', )])
OrderedDict([('Line 1', ), ('1', )])
... etc
```

**Describe the expected behavior**
I expect the data API to allow me exploring data in eager execution mode even if I use the CsvDataset class. I can't use `TextLineDataset` as my CSV file contain multi line values in quotation marks. 

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```python
import tempfile
import tensorflow as tf

print(tf.__version__)
tf.enable_eager_execution()
_, filename = tempfile.mkstemp()

with open(filename, 'w') as f:
  f.write(""""""""Line 1"",1
""Line 2"",2
""Line 3"",3"""""")
#ds_file = tf.data.TextLineDataset(filename)
ds_file = tf.data.experimental.make_csv_dataset(filename, 1)

print('\nElements in ds_file:')
for x in ds_file:
  print(x)
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
See above."
23784,"Bazel 0.19.1 fails: Toolchain identifier 'local' was not found, valid identifiers are [local_linux, local_darwin, local_windows]","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2226a4d599224d4759844db5c80460fafd87145f
- Python version: 2.7.12
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): 0.19.1
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0
- CUDA/cuDNN version: 9.0/7.2.1.38-1+cuda9.0
- GPU model and memory: V100-PCIE-32GB

**Describe the problem**

```
$ bazel clean
Starting local Bazel server and connecting to it...
INFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.
$ ./configure
WARNING: Running Bazel server needs to be killed, because the startup options are different.
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.19.1 installed.
Please specify the location of python. [Default is /home/byronyi/tf/bin/python]:


Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
AttributeError: 'module' object has no attribute 'getsitepackages'
Found possible Python library paths:
  /home/byronyi/tf/lib/python2.7/site-packages
Please input the desired Python library path to use.  Default is [/home/byronyi/tf/lib/python2.7/site-packages]

Do you wish to build TensorFlow with XLA JIT support? [Y/n]:
XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]:
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]:


Please specify the location where CUDA 9.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]:


Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:


Do you wish to build TensorFlow with TensorRT support? [y/N]:
No TensorRT support will be enabled for TensorFlow.

Please specify the locally installed NCCL version you want to use. [Default is to use https://github.com/nvidia/nccl]:


Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]: 7.0


Do you want to use clang as CUDA compiler? [y/N]:
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:


Do you wish to build TensorFlow with MPI support? [y/N]:
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]:


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]:
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=gdr         	# Build with GDR support.
	--config=verbs       	# Build with libverbs support.
	--config=ngraph      	# Build with Intel nGraph support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=noaws       	# Disable AWS S3 filesystem support.
	--config=nogcp       	# Disable GCP support.
	--config=nohdfs      	# Disable HDFS support.
	--config=noignite    	# Disable Apacha Ignite support.
	--config=nokafka     	# Disable Apache Kafka support.
Configuration finished
$ bazel build -c opt --config=opt //tensorflow/tools/pip_package:build_pip_package
Starting local Bazel server and connecting to it...
ERROR: /home/byronyi/.cache/bazel/_bazel_byronyi/fe336f47afec8b033fb4b29a6628548f/external/local_config_cc/BUILD:57:1: in cc_toolchain rule @local_config_cc//:cc-compiler-k8: Error while selecting cc_toolchain: Toolchain identifier 'local' was not found, valid identifiers are [local_linux, local_darwin, local_windows]
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis of target '@local_config_cc//:cc-compiler-k8' failed; build aborted
INFO: Elapsed time: 2.701s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (228 packages loaded, 1224 targets configured)
    currently loading: tensorflow/core ... (6 packages)
```

**Any other info / logs**
Manually changing 

```/home/byronyi/.cache/bazel/_bazel_byronyi/fe336f47afec8b033fb4b29a6628548f/external/local_config_cc/BUILD:57``` 

from `local` to `local_linux` fix the problem."
23783,CudaRoot() during compilation should not be used ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):v1.12.0-0-ga6d8ffae09 1.12.0
- Python version:3.6
- Bazel version (if compiling from source):n/a
- GCC/Compiler version (if compiling from source):n/a
- CUDA/cuDNN version:9.0/7.2
- GPU model and memory:n/a


**Describe the current behavior**
It can be seen from https://github.com/tensorflow/tensorflow/search?q=CudaRoot&unscoped_q=CudaRoot, that the `CudaRoot()` function is used by tensorflow to:

1. Get the path to libdevice library
2. Get the path to ptxas binary 

However, the return value of `CudaRoot()` function is a constant that's determined during compilation.
As a result, tensorflow's XLA cannot work properly, if it is used on a machine where the path to CUDA is different from the compilation machine.

**Describe the expected behavior**

The path to cuda should be dynamically obtained (by configuration or environment variables).

**Code to reproduce the issue**
Using the command in the end of https://medium.com/tensorflow/pushing-the-limits-of-gpu-performance-with-xla-53559db8e473, posted yesterday by @tfboyd .

**Other info / logs**
With XLA, saw errors like:
```
2018-11-15 14:07:40.696214: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:794] Failed to compile ptx to cubin.  Will attempt to let GPU driver compile the ptx. Not found: /usr/local/cuda
-9.0/bin/ptxas not found
```
"
23782,Object detection time increases as the image resolution increase.,"<em> The Object detection inference time  for different Object detection techniques increases as  the increase in Image size(resolution). I did bit of research and came to know that detection api resizes image to certain resolution and pass the resized image to detection process. In case of SSD the images are resized to 300*300 and in case of Faster R-CNN image are resize  between resolution 600 and 1024. However, if this is the case, then such increase in processing time with respect to image size should not make sense as any sized image  will be eventually processed at the standard size  which is 300*300 in case of SSD and  between 600 & 1024 in case of Faster R-CNN. I have experimental result that shows the increase in inference with respect to image size. I am not able to put conclusion for this behavior in words. I know the reason behind this for Yolo but not for other detection framework</em>
**This is my experimental result for SSD, Faster RCNN and Yolo is as follows**
![result](https://user-images.githubusercontent.com/41212548/48574000-3cbfad80-e8cb-11e8-9332-816435401420.png)



"
23780,"Repeated ""Already exists: Resource"" errors when I increase my model size","**System information**
- Custom model built by creating convolutional network tf graph with SGD optimizer and running
- Windows 10
- TensorFlow installed from pip
- TensorFlow version (use command below): 1.12
- Python version: 3.6
- Running on cpu

I've created a convolutional network by making a custom graph for feed forward and using a tensorflow optimizer to train it (I've tried with SGD and Adam). 

```
`sess = tf.Session()
optimizer = tf.train.GradientDescentOptimizer(learning_rate=.01)

input_size = 16
input_matrix = tf.placeholder(tf.float32, shape=[1,input_size,input_size])
label_ph = tf.placeholder(tf.float32, shape=[28*28])
model_out = Model.feed_forward(input_matrix)

init = tf.global_variables_initializer()
sess.run(init)

loss = tf.losses.mean_squared_error([tf.convert_to_tensor(label_ph, tf.float32)], [model_out])
train = optimizer.minimize(loss)
sess.graph.finalize()`
```

This is my graph definition. I know Model.feed_forward is opaque but it's basically a series of functions that get variable weights and feed input through layers. Just to give you an idea here's an example of a convolutional feed forward:

```
`def feed_input(input_tensors, filters, biases, stride, zero_pad=True):
        if zero_pad:
            pad_size = int((int(filters[0].shape[0]) + 1)/2 - 1)
            input_tensors = tf.map_fn(lambda inp:
                tf.pad(inp, [[pad_size, pad_size], [pad_size, pad_size]]), input_tensors)

        _, shape_x, _ = input_tensors.shape

        unflattened_output = tf.map_fn(lambda input_tensor: tf.convert_to_tensor([[[tf.add(tf.tensordot(curr_filter,
            ConvLayer.__get_input_slice__(input_tensor, x, y, tf.shape(curr_filter)[0]), 2), biases[filter_ind])
         for x in range(0, shape_x - curr_filter.shape[0] + 1, stride)]
         for y in range(0, shape_x - curr_filter.shape[0] + 1, stride)]
         for filter_ind, curr_filter in enumerate(filters)], dtype=tf.float32), input_tensors)

        shape_1, shape_2, shape_3, shape_4 = unflattened_output.shape

        return tf.reshape(unflattened_output, [shape_1 * shape_2, shape_3, shape_4])`

```
After defining my graph I am running a loop that gets some input and runs sess.run(train, feed_dict={input_matrix:img, label_ph:label_image}). If I make input_size smaller (e.g. 4) or if I reduce the number of layers in my network, everything runs fine and it does exactly what I expect it to do. However, when my network grows in the ways I just mentioned it consistently spits out these errors during sess.run(train, feed_dict={input_matrix:img, label_ph:label_image}):

> 2018-11-15 00:46:31.069913: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.069914: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.069914: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_20/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.069927: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_20/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.070138: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.078467: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.080434: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.081575: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.083125: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.085370: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.089281: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.090932: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.091080: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.091398: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.091916: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.092735: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.098503: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.094920: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.099036: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.101236: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.101988: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.101966: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_20/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.103264: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_20/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.102945: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.106092: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.106858: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.108494: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.109495: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_20/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.109987: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.112326: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.112525: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_20/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.113595: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.113831: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.115342: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.115418: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.120685: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.122569: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.122708: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.123904: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.124832: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.126926: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.127336: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_20/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.129575: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.128498: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.115477: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.130180: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.130601: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.131469: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_20/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.135938: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.132834: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.139111: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_20/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.139728: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.140827: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.141429: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.142202: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.140004: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.142621: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.144039: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.147757: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.145043: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.149502: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.146723: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.148971: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.145861: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.149850: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.151642: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.152947: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.154227: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.155666: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.157249: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.157877: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.159295: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.160204: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.163481: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.163750: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.164518: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.166533: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.172400: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.174768: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
2018-11-15 00:46:31.174793: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
Traceback (most recent call last):
  File ""C:\Users\Aidan\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1334, in _do_call
    return fn(*args)
  File ""C:\Users\Aidan\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""C:\Users\Aidan\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.AlreadyExistsError: Resource __per_step_4/gradients/map_24/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
	 [[{{node gradients/map_24/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var}} = TemporaryVariable[dtype=DT_FLOAT, shape=[5,5], var_name=""gradients/...dd/tmp_var"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^gradients/map_24/while/Tensordot_580/Reshape_grad/Reshape)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Users/Aidan/Documents/PycharmProjects/SketchIt/src/network/FeedImages.py"", line 57, in <module>
    sess.run(train, feed_dict={input_matrix:img, label_ph:label_image})
  File ""C:\Users\Aidan\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 929, in run
    run_metadata_ptr)
  File ""C:\Users\Aidan\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Users\Aidan\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1328, in _do_run
    run_metadata)
  File ""C:\Users\Aidan\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.AlreadyExistsError: Resource __per_step_4/gradients/map_24/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar
	 [[{{node gradients/map_24/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var}} = TemporaryVariable[dtype=DT_FLOAT, shape=[5,5], var_name=""gradients/...dd/tmp_var"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^gradients/map_24/while/Tensordot_580/Reshape_grad/Reshape)]]

Anyone know what might be the cause? Maybe some concurrency issue since its telling me things that shouldn't exist have already been created? Help would be much appreciated, I'm close to making this thing work! Thanks for your time."
23779,Error on Label's datatype when enable eager execution in Colab ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.14.1 - Colab
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: None
- **TensorFlow installed from (source or binary)**: Colab build in
- **TensorFlow version (use command below)**: 1.12.0
- **Python version**: Colab Python2
- **Bazel version (if compiling from source)**: None
- **GCC/Compiler version (if compiling from source)**: None
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**: None


### Describe the problem
When I run the simple DNN on MNIST without eager execution, it works fine. However, when I enable eager execution then fit the model, I got the error said that my `train_labels` and `test_labels` has to be `int64` tensor which are originally `uint8` tensor. The error is shown as below:

`InvalidArgumentError: cannot compute Equal as input #0(zero-based) was expected to be a int64 tensor but is a uint8 tensor [Op:Equal]`

Although, I can fix it by using `train_labels = train_labels.astype(np.int64)` ,It looks like the error should be fixed in the source code.



### Source code to reproduce the issue in Colab

```python

import tensorflow as tf
import numpy as np

tf.enable_eager_execution()

mnist = tf.keras.datasets.mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

train_images = train_images / 255.0
test_images = test_images / 255.0

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28,28)),
  tf.keras.layers.Dense(512, activation=tf.nn.relu),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(128, activation=tf.nn.relu),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])

model.compile(optimizer = tf.train.AdamOptimizer(0.001),loss='sparse_categorical_crossentropy',metrics=['accuracy'])

model.summary()

history = model.fit(x = train_images,y = train_labels,validation_split=0.1,batch_size = 256,verbose=2,shuffle = True,epochs=10)

```


### Other info / logs


```Python traceback

InvalidArgumentErrorTraceback (most recent call last)
<ipython-input-6-7bdf2a937371> in <module>()
      5                     verbose=2,
      6                     shuffle = True,
----> 7                     epochs=10)
      8 ###########################################
 .
 .
 .
 .
 .
 .     
      
/usr/local/lib/python2.7/dist-packages/six.pyc in raise_from(value, from_value)
    735 else:
    736     def raise_from(value, from_value):
--> 737         raise value
    738 
    739 

InvalidArgumentError: cannot compute Equal as input #0(zero-based) was expected to be a int64 tensor but is a uint8 tensor [Op:Equal]

```


		"
23778,cannot conver mobilenet pb to tflite,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.12.0
- Python version:2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

Im trying to convert pb to tflite, i run my model using Mobilenet v2, 
when trying to convert im getting the following error.

bazel-bin/tensorflow/contrib/lite/toco/toco --input_file=/home/dell/TFObject-detection/tensorflow/models/research/object_detection/Output/frozen_inference_graph.pb --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=frozen_inference_graph.lite --inference_type=FLOAT --input_type=FLOAT --input_arrays=ImageTensor --output_arrays=SemanticPredictions --input_shapes=1,512,512,3 --allow_nonexistent_arrays


2018-11-15 09:54:38.341689: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 0 operators, 2 arrays (0 quantized)
2018-11-15 09:54:38.341711: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:135] Model is empty!!!
2018-11-15 09:54:38.341737: W tensorflow/contrib/lite/toco/tooling_util.cc:1246] Fixing constant output array SemanticPredictions by inserting a copy. This is not optimal.
2018-11-15 09:54:38.341749: F ./tensorflow/contrib/lite/toco/model.h:1984] Check failed: has_shape() 
Aborted (core dumped)


is that related to this issue? any hint?"
23774,"""ValueError: list.remove(x): x not in list"" in eager mode for decorated keras method","The below runs without the decorator but when the decorator is added then it fails with the above error.

tf.__version__, keras.__version__ = ('1.12.0', '2.1.6-tf')
ubuntu 16.04
```
import tensorflow.keras.layers as KL
from functools import wraps

def decorator(f):
    @wraps(f)
    def wrapper(*args, **kwargs):
        return f(*args, **kwargs)
    return wrapper

class MyLayer(KL.Layer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    @decorator
    def call(self, inputs):
        pass
a=MyLayer()
```"
23773,ERRoR in running with Gpu,"How to solve this Error: 
InvalidArgumentError (see above for traceback): Cannot assign a device for operation init: Operation was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device."
23772,Failed to convert .pb to .tflite,"When I convert a pb file to tflite, I meet a error as follow.  what's wrong?

(tensorflow) XXXX :~/tensorflow/tensorflow-master$ bazel-bin/tensorflow/contrib/lite/toco/toco  \
>   --input_file=./headpose.pb \
>   --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE \
>   --output_file=./headpose.tflite --inference_type=FLOAT \
>   --input_type=FLOAT --input_arrays=input \
>   --output_arrays=output –input_shapes=1,416,416,3
2018-11-15 21:19:52.503499: W tensorflow/contrib/lite/toco/toco_cmdline_flags.cc:282] --input_type is deprecated. It was an ambiguous flag that set both --input_data_types and --inference_input_type. If you are trying to complement the input file with information about the type of input arrays, use --input_data_type. If you are trying to control the quantization/dequantization of real-numbers input arrays in the output file, use --inference_input_type.
2018-11-15 21:19:52.616030: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: RefSwitch
2018-11-15 21:19:52.633837: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: AssignSub
2018-11-15 21:19:52.633907: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: RefSwitch
2018-11-15 21:19:52.633920: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: AssignSub
2018-11-15 21:19:52.634233: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: RefSwitch
2018-11-15 21:19:52.634249: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: AssignSub
2018-11-15 21:19:52.634269: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: RefSwitch
2018-11-15 21:19:52.634280: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: AssignSub
2018-11-15 21:19:52.634410: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: RefSwitch
2018-11-15 21:19:52.634424: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: AssignSub
2018-11-15 21:19:52.634444: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: RefSwitch
2018-11-15 21:19:52.634454: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: AssignSub
2018-11-15 21:19:52.634767: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: RefSwitch
2018-11-15 21:19:52.634782: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: AssignSub
2018-11-15 21:19:52.634801: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: RefSwitch
2018-11-15 21:19:52.634811: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: AssignSub
2018-11-15 21:19:52.635636: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: RefSwitch
2018-11-15 21:19:52.635656: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: AssignSub
2018-11-15 21:19:52.635686: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: RefSwitch
2018-11-15 21:19:52.635705: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: AssignSub
2018-11-15 21:19:52.639048: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: RefSwitch
2018-11-15 21:19:52.639092: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: AssignSub
2018-11-15 21:19:52.639121: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: RefSwitch
2018-11-15 21:19:52.639131: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: AssignSub
2018-11-15 21:19:52.652115: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: RefSwitch
2018-11-15 21:19:52.652212: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: AssignSub
2018-11-15 21:19:52.652237: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: RefSwitch
2018-11-15 21:19:52.652246: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: AssignSub
2018-11-15 21:19:52.682724: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: RefSwitch
2018-11-15 21:19:52.682769: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: AssignSub
2018-11-15 21:19:52.682804: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: RefSwitch
2018-11-15 21:19:52.682826: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: AssignSub
2018-11-15 21:19:52.691380: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 437 operators, 737 arrays (0 quantized)
2018-11-15 21:19:52.699329: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 293 operators, 513 arrays (0 quantized)
2018-11-15 21:19:52.706308: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 293 operators, 513 arrays (0 quantized)
2018-11-15 21:19:52.706484: F tensorflow/contrib/lite/toco/graph_transformations/resolve_tensorflow_switch.cc:98] Check failed: other_op->type == OperatorType::kMerge Found BatchNormalization as non-selected output from Switch, but only Merge supported.
Aborted (core dumped)

"
23771,Build failure on TensorFlow v1.12.0 on s390x,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04, 18.04 s390x
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary):  source 
- TensorFlow version: v1.12.0
- Python version:  2.7.x
- Installed using virtualenv? pip? conda?:  Building from source 
- Bazel version (if compiling from source): v0.15.0
- GCC/Compiler version (if compiling from source): 7.0.0
- CUDA/cuDNN version: NA
- GPU model and memory: NA


**Describe the problem**

We are building TensorFlow v1.12.0 from source on Ubuntu 16.04, 18.04 on s390x platform .
Build fails with an error: ""`Unknown Target CPU`"" 

Bulid command: 
  `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package `

@gunan We haven't observed this Boringssl error with current master build and it's building successfully on s390x. CI link: https://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_CI/

However while building v1.12.0 (since it is latest stable) , we came across this issue.  
Looks like some code changes made in master which resolves issues related to Boringssl w.r.t. s390x are missing in v1.12.0.

Will the Tensorflow v1.13.0 have the required changes to skip Boringssl for s390x?  Or is there a way to fix build issue in v1.12.0 ? Please let us know.




"
23770,The distribute strategy is not compatible with tf.contrib.slim.batch_norm() and tf.contrib.layers.batch_norm(),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **_no_**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): _**Ubuntu 16.04.5 LTS**_
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:**_no_**
- TensorFlow installed from (source or binary):**_binary_**
- TensorFlow version (use command below):**_1.12.0_**
- Python version: **_Python 3.5.2_**
- Bazel version (if compiling from source): _**None**_
- GCC/Compiler version (if compiling from source):  **_5.4.0_**
- CUDA/cuDNN version: **_cuda-9.0.176/cudnn-7.2.1_**
- GPU model and memory: **_2 same GTX Titan X (Pascal), 12GB_** 

I just use the offical docker image of the tensorflow, the tag is `1.12.0-devel-gpu-py3`

**Problem**

The code is shown below
```
#coding=utf-8

import tensorflow as tf
import numpy as np
import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--num_gpus', type=int, default=2,
						help='The number of the GPUs.')
FLAGS, unparsed = parser.parse_known_args()


def input_fn():
    return (
        tf.data.Dataset.from_tensor_slices([0])
        .map(lambda _: tf.random_uniform([1], 0, np.pi * 2))
        .map(lambda x: (x, tf.sin(x)))
        .repeat()
        .batch(10)
    )

def model_fn(features, labels, mode):
    net = tf.layers.dense(features, units=20)
    net = tf.nn.tanh(net)
    
    #net = tf.contrib.layers.batch_norm(net)
    net = tf.contrib.slim.batch_norm(net)
    #net = tf.keras.layers.BatchNormalization()(net)
    #net = tf.layers.batch_normalization(inputs=net)

    net = tf.layers.dense(net, units=20)
    net = tf.nn.tanh(net)
    output = tf.layers.dense(net, units=1)

    if mode == tf.estimator.ModeKeys.TRAIN:
        loss = tf.reduce_mean(tf.pow(output - labels, 2))
        loss = tf.identity(loss, name='loss')
        train_op = tf.train.GradientDescentOptimizer(0.1).minimize(loss, global_step=tf.train.get_global_step())
        return tf.estimator.EstimatorSpec(tf.estimator.ModeKeys.TRAIN, loss=loss, train_op=train_op)

session_config = tf.ConfigProto(allow_soft_placement=True,
                                    gpu_options=tf.GPUOptions(allow_growth=True, 
                                                force_gpu_compatible=True))
distribution = tf.contrib.distribute.MirroredStrategy(num_gpus=FLAGS.num_gpus)
config = tf.estimator.RunConfig(keep_checkpoint_max=0, 
								train_distribute=distribution)
estimator = tf.estimator.Estimator(model_fn=model_fn, config=config)

tf.logging.set_verbosity(tf.logging.INFO)
tensors_to_log = {'loss':'loss'}
logging_hook = tf.train.LoggingTensorHook(
        			tensors=tensors_to_log, every_n_iter=100)
train_hooks=[logging_hook]
print('tensorflow version: %s' % tf.__version__)
estimator.train(input_fn=input_fn, steps=1000, hooks=train_hooks)
```

When use `tf.contrib.layers.batch_norm`  and `tf.contrib.slim.batch_norm` will raise error
```
 name: GeForce GTX TITAN X, pci bus id: 0000:18:00.0, compute capability: 5.2)
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Error reported to Coordinator:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 795, in run
    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 1195, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""estimator_multi_gpu.py"", line 27, in model_fn
    net = tf.contrib.slim.batch_norm(net)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 182, in func_with_args
    return func(*args, **current_args)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 596, in batch_norm
    scope=scope)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 416, in _fused_batch_norm
    is_training, _delay_updates, moving_vars_fn)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/utils.py"", line 214, in smart_cond
    return static_cond(pred_value, fn1, fn2)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/utils.py"", line 192, in static_cond
    return fn1()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 410, in _delay_updates
    moving_mean, mean, decay, zero_debias=zero_debias_moving_mean)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/moving_averages.py"", line 84, in assign_moving_average
    with ops.colocate_with(variable):
  File ""/usr/lib/python3.5/contextlib.py"", line 59, in __enter__
    return next(self.gen)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 4094, in _colocate_with_for_gradient
    with self.colocate_with(op, ignore_existing):
  File ""/usr/lib/python3.5/contextlib.py"", line 59, in __enter__
    return next(self.gen)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 4146, in colocate_with
    op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 1307, in internal_convert_to_tensor_or_indexed_slices
    value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 1146, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py"", line 439, in _tensor_conversion_mirrored
    assert not as_ref
AssertionError
```

But if I use `tf.keras.layers.BatchNormalization()(net)` or   `tf.layers.batch_normalization(inputs=net)`,
it is totally OK.

I see the source code of the `tf.layers.batch_normalization` and find it is inherited form `keras.layers.BatchNormalization`
```
from tensorflow.python.keras import layers as keras_layers
from tensorflow.python.layers import base
from tensorflow.python.ops import init_ops
from tensorflow.python.util.tf_export import tf_export

@tf_export('layers.BatchNormalization')
class BatchNormalization(keras_layers.BatchNormalization, base.Layer):
```
And I also find some similar issues
https://github.com/tensorflow/tensorflow/issues/20509
https://github.com/tensorflow/tensorflow/issues/20874
https://github.com/tensorflow/tensorflow/issues/21968
https://github.com/tensorflow/tensorflow/issues/22399

_**There are many pretrained models in the model zoo of `slim`, the bn layer are all not compatible with the distribute stratege. It is not convinient for us to use them in other tasks with `tf.estimator`.**_


**Other info / logs**
There are also two samll problems in the docker images....

**_ONE:_**
```
__new__() got an unexpected keyword argument 'serialized_options'
```
I solvd it by 
```
RUN pip3 install --upgrade pip
```
when build the image.

The `pip install -U protobuf ` did not work.

**_TWO:_**
```

INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpggc6_3sr/model.ckpt.
INFO:tensorflow:model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
Unexpected end of /proc/mounts line `overlay / overlay rw,relatime,lowerdir=/var/lib/docker/overlay2/l/GFPVUUB6DIRSEA556PJ7UZSB2U:/var/lib/docker/overlay2/l/Z7PKFCBJ2NCRWLDUMR7HOGXHY5:/var/lib/docker/overlay2/l/XJZAHQOGE7RXOAHTAKELNSYBZ5:/var/lib/docker/overlay2/l/ESG4TTNOTM5ZINQ5O4JCYNFODB:/var/lib/docker/overlay2/l/6ZQZQCRBLVJTIVAKYNIA2VNCNX:/var/lib/docker/overlay2/l/ZIWP422R2WQJXJQWA6BWTKSVP2:/var/lib/docker/overlay2/l/EZJIAHDCQ5P5GASJZR3GCECEE6:/var/lib/docker/overlay2/l/OA2WSFXRNJQI36B7ZILMTVTD5V:/var/lib/docker/overlay2/l/XITXCQNT5YIY7'
Unexpected end of /proc/mounts line `YSSGYLTAEXMQX:/var/lib/docker/overlay2/l/EHORMZU5A6H3H6NAL34XTIYBEA:/var/lib/docker/overlay2/l/E34DRW27EHWOG6HEQCYM2LKYJX:/var/lib/docker/overlay2/l/Y4SABJDSTVMRHZ2SC6QYSK4NP6:/var/lib/docker/overlay2/l/BWHO3NPMOZYCZ33PIH4XXQH3Z6:/var/lib/docker/overlay2/l/YLLBXOSS6TVCINXJNKTZX7J6PQ:/var/lib/docker/overlay2/l/QQ2FBR7QT4NFNYR4UZUXOBD2K3:/var/lib/docker/overlay2/l/SSP6MK4IQCUV2N5DSY5BO4EZO7:/var/lib/docker/overlay2/l/XUONJPPZG5ANA2ELQECHOIY7KU:/var/lib/docker/overlay2/l/ZOCEMSU7RIV6OH6QZNTKKIGACB:/var/lib/do'
```
It seems that this is harmless.
https://stackoverflow.com/questions/46138549/docker-openmpi-and-unexpected-end-of-proc-mounts-line"
23769,Error when feeding tf.keras model in custom loss function,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux openSUSE Leap 42.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: none
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0
- Python version: 3.4.6
- Bazel version (if compiling from source): none
- GCC/Compiler version (if compiling from source): none
- CUDA/cuDNN version: none
- GPU model and memory: none


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

I am trying to add adversarial training to my `tf.keras` model. For this, I added a custom loss function, which feeds the model the adversarially perturbed input and adds the cross-entropy to the loss. 

The problem is that feeding the model a tensor in the custom loss function leads to a `TypeError: argument of type 'NoDependency' is not iterable`. I have reduced it to a minimal example that simply feeds the model zeros below. 

If I call `model(model.input)` before the custom loss function is called, it surprisingly works.

**Describe the expected behavior**

The expected behavior would be the model compiling and fitting as it does with the `model(model.input)` line uncommented. The example below would also work when defining the `y_pred_zeros = model(tf.zeros((32, 20)))` outside of the `loss` function. But for adversarial training, I need to call the model inside of the loss function.

**Code to reproduce the issue**
```python
import numpy as np
import tensorflow as tf

model = tf.keras.Sequential([
    tf.keras.layers.Dense(10, activation='relu', input_shape=(20,)),
    tf.keras.layers.Dense(5)
])

# With the following line uncommented, it works.
# model(model.input)

def loss(y_true, y_pred):
    loss = tf.keras.losses.mean_squared_error(y_true, y_pred)

    # Feed zeros to the model and add the mean of the output to the loss
    y_pred_zeros = model(tf.zeros((32, 20)))

    return loss + tf.reduce_mean(y_pred_zeros)

model.compile('sgd', loss=loss)

x_train, y_train = np.random.randn(100, 20), np.random.randn(100, 5)
model.fit(x_train, y_train)
```

**Other info / logs**
```python
/home/kilian/Desktop/venv/bin/python /home/kilian/Desktop/tmp.py
Traceback (most recent call last):
  File ""/home/kilian/Desktop/tmp.py"", line 20, in <module>
    model.compile('sgd', loss=loss)
  File ""/home/kilian/Desktop/venv/lib/python3.4/site-packages/tensorflow/python/training/checkpointable/base.py"", line 474, in _method_wrapper
    method(self, *args, **kwargs)
  File ""/home/kilian/Desktop/venv/lib/python3.4/site-packages/tensorflow/python/keras/engine/training.py"", line 617, in compile
    output_loss = weighted_loss(y_true, y_pred, sample_weight, mask)
  File ""/home/kilian/Desktop/venv/lib/python3.4/site-packages/tensorflow/python/keras/engine/training_utils.py"", line 598, in weighted
    score_array = fn(y_true, y_pred)
  File ""/home/kilian/Desktop/tmp.py"", line 16, in loss
    y_pred_zeros = model(tf.zeros((32, 20)))
  File ""/home/kilian/Desktop/venv/lib/python3.4/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 704, in __call__
    if ('mask' in self._call_fn_args and 'mask' not in kwargs and
TypeError: argument of type 'NoDependency' is not iterable

Process finished with exit code 1
```
"
23768,TFLite not support variable batch_size of input,"I use `tf.estimator.export.build_raw_serving_input_receiver_fn` api with `default_batch_size=None` to export a savedModel, which input shape maybe [None, 3000, 40, 3].  But after convert the model to tflite model , the input shape is [1, 3000, 40, 3].  How can I using tflite with variable batch_size of input."
23767,Tensorflow calculates incorrect loss for `tf.keras` models when using Weights.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (No):
- OS Platform and Distribution (e.g., MacOS + Google Cloud VMs):
- TensorFlow installed from (source):
- TensorFlow version (1.10 and 1.12):
- Python version: 3.5

**Describe the current behavior**
The loss calculation is not correct when working with `tf.keras.`. After building the model, [`tf.keras.fit_generator`](https://www.tensorflow.org/api_docs/python/tf/keras/models/Model) should accept `(inputs, targets, sample_weights)` as inputs. However, if I multiply the `sample_weights` by 10000,  the loss doesn't change.

The bug seems to appear from 1.10 version of Tensorflow onwards e.g. (1.11, 1.12)

**Describe the expected behavior**
If I increase the weighting by a certain factor, the overall loss of the model should increase by the same factor. Given the model is doing random guessing. 

**Code to reproduce the issue**
```
import numpy as np
import tensorflow as tf

WEIGHT_VARIABLE = 1

no_of_features = 10
timesteps = 3
batch_size = 32

def data_gen():

    while True:
        numerical = np.random.randint(5, size=(batch_size, timesteps, no_of_features))
        y = np.random.randint(2, size=batch_size)
        w = np.ones(batch_size) * WEIGHT_VARIABLE # or np.where() for imbalanced datasets

        yield {'numeric_input': numerical}, y, w


def build_model():
    numerical_input = tf.keras.layers.Input(shape=(timesteps, no_of_features), name='numeric_input')
    rnn_out = tf.keras.layers.GRU(32, return_sequences=False)(numerical_input)
    dense = tf.keras.layers.Dense(1, activation='sigmoid', name='main_output')(rnn_out)

    model = tf.keras.models.Model(numerical_input, dense)

    params = {
        'loss': 'binary_crossentropy',
        'optimizer': tf.keras.optimizers.Adam(),
        'metrics': [tf.keras.metrics.binary_crossentropy, tf.keras.metrics.binary_accuracy]
    }
    model.compile(**params)

    return model


def train_model():
    gen1 = data_gen()
    model = build_model()

    model.fit_generator(gen1, epochs=30, steps_per_epoch=10)


if __name__ == '__main__':
    train_model()
```

In the above code, you simply need to change the `WEIGHT_VARIABLE = 1` From 1 to 100000 and rerun the file.


**Other info / logs**
### v1.10

`WEIGHT_VARIABLE = 1`
10/10 [==============================] - 1s 128ms/step - loss: 0.7407 - binary_crossentropy: 0.7407 - binary_accuracy: 0.5031
Epoch 2/5
10/10 [==============================] - 0s 4ms/step - loss: 0.7043 - binary_crossentropy: 0.7043 - binary_accuracy: 0.5125
Epoch 3/5
10/10 [==============================] - 0s 4ms/step - loss: 0.7055 - binary_crossentropy: 0.7055 - binary_accuracy: 0.5219
Epoch 4/5
10/10 [==============================] - 0s 4ms/step - loss: 0.7002 - binary_crossentropy: 0.7002 - binary_accuracy: 0.5250
Epoch 5/5
10/10 [==============================] - 0s 4ms/step - loss: 0.6944 - binary_crossentropy: 0.6944 - binary_accuracy: 0.5375

`WEIGHT_VARIABLE = 10000`

10/10 [==============================] - 1s 131ms/step - loss: 7235.5976 - binary_crossentropy: 0.7236 - binary_accuracy: 0.4562
Epoch 2/5
10/10 [==============================] - 0s 4ms/step - loss: 7271.9184 - binary_crossentropy: 0.7272 - binary_accuracy: 0.4844
Epoch 3/5
10/10 [==============================] - 0s 4ms/step - loss: 7276.9147 - binary_crossentropy: 0.7277 - binary_accuracy: 0.4500
Epoch 4/5
10/10 [==============================] - 0s 4ms/step - loss: 7052.0121 - binary_crossentropy: 0.7052 - binary_accuracy: 0.4625
Epoch 5/5
10/10 [==============================] - 0s 4ms/step - loss: 7187.0285 - binary_crossentropy: 0.7187 - binary_accuracy: 0.4969


### v1.12

`WEIGHT_VARIABLE = 1`
10/10 [==============================] - 1s 68ms/step - loss: 0.7188 - binary_crossentropy: 0.7188 - binary_accuracy: 0.5312
Epoch 2/5
10/10 [==============================] - 0s 4ms/step - loss: 0.7044 - binary_crossentropy: 0.7044 - binary_accuracy: 0.4969
Epoch 3/5
10/10 [==============================] - 0s 4ms/step - loss: 0.7086 - binary_crossentropy: 0.7086 - binary_accuracy: 0.4844
Epoch 4/5
10/10 [==============================] - 0s 4ms/step - loss: 0.7075 - binary_crossentropy: 0.7075 - binary_accuracy: 0.4500
Epoch 5/5
10/10 [==============================] - 0s 4ms/step - loss: 0.6950 - binary_crossentropy: 0.6950 - binary_accuracy: 0.5187

`WEIGHT_VARIABLE = 10000`

10/10 [==============================] - 1s 74ms/step - loss: 0.9084 - binary_crossentropy: 0.9084 - binary_accuracy: 0.4719
Epoch 2/5
10/10 [==============================] - 0s 4ms/step - loss: 0.7120 - binary_crossentropy: 0.7120 - binary_accuracy: 0.5062
Epoch 3/5
10/10 [==============================] - 0s 4ms/step - loss: 0.7024 - binary_crossentropy: 0.7024 - binary_accuracy: 0.5344
Epoch 4/5
10/10 [==============================] - 0s 4ms/step - loss: 0.7257 - binary_crossentropy: 0.7257 - binary_accuracy: 0.4500
Epoch 5/5
10/10 [==============================] - 0s 4ms/step - loss: 0.7013 - binary_crossentropy: 0.7013 - binary_accuracy: 0.4844"
23766,TfLite performance is way worse comapred to Tensorflow mobile,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy S8
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): On PC: current tf-nightly (14.11.2018), on phone current org.tensorflow:tensorflow-lite:0.0.0-nightly
- Python version: 3.6.6
- Bazel version (if compiling from source): - 
- GCC/Compiler version (if compiling from source): - 
- CUDA/cuDNN version: - 
- GPU model and memory: CPU only

**Describe the current behavior**
I build a model with this code:
```
import tensorflow as tf
import numpy as np
from tensorflow.python.tools import freeze_graph


class AlexNet(object):
    """"""Implementation of the AlexNet.""""""

    def __init__(self, x, keep_prob, num_classes, skip_layer,
                 weights_path='DEFAULT'):
        """"""Create the graph of the AlexNet model.
        Args:
            x: Placeholder for the input tensor.
            keep_prob: Dropout probability.
            num_classes: Number of classes in the dataset.
            skip_layer: List of names of the layer, that get trained from
                scratch
            weights_path: Complete path to the pretrained weight file, if it
                isn't in the same folder as this code
        """"""
        # Parse input arguments into class variables
        self.X = x
        self.NUM_CLASSES = num_classes
        self.KEEP_PROB = keep_prob
        self.SKIP_LAYER = skip_layer

        if weights_path == 'DEFAULT':
            self.WEIGHTS_PATH = 'bvlc_alexnet.npy'
        else:
            self.WEIGHTS_PATH = weights_path

        # Call the create function to build the computational graph of AlexNet
        self.create()

    def create(self):
        """"""Create the network graph.""""""
        # 1st Layer: Conv (w ReLu) -> Lrn -> Pool
        conv1 = conv(self.X, 11, 11, 96, 4, 4, padding='VALID', name='conv1')
        norm1 = lrn(conv1, 2, 2e-05, 0.75, name='norm1')
        pool1 = max_pool(norm1, 3, 3, 2, 2, padding='VALID', name='pool1')

        # 2nd Layer: Conv (w ReLu)  -> Lrn -> Pool with 2 groups
        conv2 = conv(pool1, 5, 5, 256, 1, 1, groups=2, name='conv2')
        norm2 = lrn(conv2, 2, 2e-05, 0.75, name='norm2')
        pool2 = max_pool(norm2, 3, 3, 2, 2, padding='VALID', name='pool2')

        # 3rd Layer: Conv (w ReLu)
        self.conv3 = conv(pool2, 3, 3, 384, 1, 1, name='conv3')

        # 4th Layer: Conv (w ReLu) splitted into two groups
        conv4 = conv(self.conv3, 3, 3, 384, 1, 1, groups=2, name='conv4')

        # 5th Layer: Conv (w ReLu) -> Pool splitted into two groups
        conv5 = conv(conv4, 3, 3, 256, 1, 1, groups=2, name='conv5')
        self.pool5 = max_pool(conv5, 3, 3, 2, 2, padding='VALID', name='pool5')

        # 6th Layer: Flatten -> FC (w ReLu) -> Dropout
        self.flattened = tf.reshape(self.pool5, [1, 9216], name='output')

    def load_initial_weights(self, session):
        """"""Load weights from file into network.
        As the weights from http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/
        come as a dict of lists (e.g. weights['conv1'] is a list) and not as
        dict of dicts (e.g. weights['conv1'] is a dict with keys 'weights' &
        'biases') we need a special load function
        """"""
        # Load the weights into memory
        weights_dict = np.load(self.WEIGHTS_PATH, encoding='bytes').item()

        # Loop over all layer names stored in the weights dict
        for op_name in weights_dict:

            # Check if layer should be trained from scratch
            if op_name not in self.SKIP_LAYER:

                with tf.variable_scope(op_name, reuse=True):

                    # Assign weights/biases to their corresponding tf variable
                    for data in weights_dict[op_name]:

                        # Biases
                        if len(data.shape) == 1:
                            var = tf.get_variable('biases', trainable=False)
                            session.run(var.assign(data))

                        # Weights
                        else:
                            var = tf.get_variable('weights', trainable=False)
                            session.run(var.assign(data))


def conv(x, filter_height, filter_width, num_filters, stride_y, stride_x, name,
         padding='SAME', groups=1):
    """"""Create a convolution layer.
    Adapted from: https://github.com/ethereon/caffe-tensorflow
    """"""
    # Get number of input channels
    input_channels = int(x.get_shape()[-1])

    # Create lambda function for the convolution
    convolve = lambda i, k: tf.nn.conv2d(i, k,
                                         strides=[1, stride_y, stride_x, 1],
                                         padding=padding)

    with tf.variable_scope(name) as scope:
        # Create tf variables for the weights and biases of the conv layer
        weights = tf.get_variable('weights', shape=[filter_height,
                                                    filter_width,
                                                    input_channels/groups,
                                                    num_filters])
        biases = tf.get_variable('biases', shape=[num_filters])

    if groups == 1:
        conv = convolve(x, weights)

    # In the cases of multiple groups, split inputs & weights and
    else:
        # Split input and weights and convolve them separately
        input_groups = tf.split(axis=3, num_or_size_splits=groups, value=x)
        weight_groups = tf.split(axis=3, num_or_size_splits=groups,
                                 value=weights)
        output_groups = [convolve(i, k) for i, k in zip(input_groups, weight_groups)]

        # Concat the convolved output together again
        conv = tf.concat(axis=3, values=output_groups)

    # Add biases
    bias = tf.reshape(tf.nn.bias_add(conv, biases), tf.shape(conv))

    # Apply relu function
    relu = tf.nn.relu(bias, name=scope.name)

    return relu


def fc(x, num_in, num_out, name, relu=True):
    """"""Create a fully connected layer.""""""
    with tf.variable_scope(name) as scope:

        # Create tf variables for the weights and biases
        weights = tf.get_variable('weights', shape=[num_in, num_out],
                                  trainable=True)
        biases = tf.get_variable('biases', [num_out], trainable=True)

        # Matrix multiply weights and inputs and add bias
        act = tf.nn.xw_plus_b(x, weights, biases, name=scope.name)

    if relu:
        # Apply ReLu non linearity
        relu = tf.nn.relu(act)
        return relu
    else:
        return act


def max_pool(x, filter_height, filter_width, stride_y, stride_x, name,
             padding='SAME'):
    """"""Create a max pooling layer.""""""
    return tf.nn.max_pool(x, ksize=[1, filter_height, filter_width, 1],
                          strides=[1, stride_y, stride_x, 1],
                          padding=padding, name=name)


def lrn(x, radius, alpha, beta, name, bias=1.0):
    """"""Create a local response normalization layer.""""""
    return tf.nn.local_response_normalization(x, depth_radius=radius,
                                              alpha=alpha, beta=beta,
                                              bias=bias, name=name)


def dropout(x, keep_prob):
    """"""Create a dropout layer.""""""
    return tf.nn.dropout(x, keep_prob)




g = tf.Graph()
with g.as_default():
    # Initialize all variables
    x = tf.placeholder(tf.float32, [1, 227, 227, 3])
    y = tf.placeholder(tf.float32, [1, 1000])
    keep_prob = tf.placeholder(tf.float32)
    alex_net = AlexNet(x, keep_prob, 1000, [""fc6"", ""fc7"", ""fc8""])
```

Then I froze the model. In the first try I converted it to TFlite directly, in a second try I set the `converter.post_training_quantize = True` option in the conversion process to quantize my model and in a second try I optimized my model for inference before converting and quantizing.
The code for the tflite conversion is the following:
```
import tensorflow as tf

graph_def_file = ""alex_frozen_optimized.pb""
input_arrays = [""Placeholder""]
output_arrays = [""output""]

converter = tf.contrib.lite.TFLiteConverter.from_frozen_graph(
    graph_def_file, input_arrays, output_arrays, input_shapes={""Placeholder"" : [1, 227, 227, 3]})
converter.post_training_quantize = True
tflite_model = converter.convert()
open(""save_path/alex_frozen_optimized_quantized.tflite"", ""wb"").write(tflite_model)
```
The code for optimizing for inference was the following terminal call: `python -m tensorflow.python.tools.optimize_for_inference --input alex_frozen.pb --output alex_frozen_optimized.pb --input_names=Placeholder --output_names=output`

All my models, including the plain, unconverted model is uploaded here for testing: [models.zip](https://www.dropbox.com/s/gyu9cfkn3yn07oc/models.zip?dl=0)

**Describe the expected behavior**
I deployed those models to my phone. I get runtime of around 290 ms for the unconverted model, while having runtimes of about 420 ms for the quantized tflite, the optimized and quantized tflite and the plain tflite model. That can't be right, can it?

**Code to reproduce the issue**
The code for running inference on the phone for the tflite models is the following:
```
import android.content.res.AssetFileDescriptor;
import android.content.res.AssetManager;
import android.graphics.Bitmap;
import android.os.Trace;
import java.io.FileInputStream;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.nio.ByteOrder;
import java.nio.MappedByteBuffer;
import java.nio.channels.FileChannel;
import one.realnote.app.PpmItem;
import one.realnote.app.Util;
import org.tensorflow.lite.Interpreter;
import org.tensorflow.lite.Interpreter.Options;

public class AlexFeatureExtractionLite {

  private static final String TAG = ""TFLiteAPIAlex"";

  // Only return this many results.
  private boolean isModelQuantized;
  // Float model
  // Number of threads in the java app
  private static final int NUM_THREADS = 4;
  // Config values.
  private int inputSize;
  // Pre-allocated buffers.
  private float[][] outputVector;
  private int[] intValues;

  private static PpmItem ppm_tfRun = PpmItem.createDebugItem(""TFA"", ""runAlex"", String.class);

  private ByteBuffer imgData;
  private Interpreter tfLite;

  /**
   * Memory-map the model file in Assets.
   */
  private static MappedByteBuffer loadModelFile(AssetManager assets)
      throws IOException {
    AssetFileDescriptor fileDescriptor = assets.openFd(""tf/alex_frozen_optimized_quantized.tflite"");
    FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());
    FileChannel fileChannel = inputStream.getChannel();
    long startOffset = fileDescriptor.getStartOffset();
    long declaredLength = fileDescriptor.getDeclaredLength();
    return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);
  }

  /**
   * Initializes a native TensorFlow session for classifying images.
   *
   * @param assetManager The asset manager to be used to load assets.
   * @param inputSize The size of image input
   * @param isQuantized Boolean representing model is quantized or not
   */
  public static AlexFeatureExtractionLite create(final AssetManager assetManager,
      final int inputSize, final boolean isQuantized) {
    final AlexFeatureExtractionLite d = new AlexFeatureExtractionLite();

    d.inputSize = inputSize;
    Interpreter.Options options = new Options();
    options.setUseNNAPI(false);
    options.setAllowFp16PrecisionForFp32(false);
    options.setNumThreads(4);
    try {
      d.tfLite = new Interpreter(loadModelFile(assetManager), options);
    } catch (Exception e) {
      Util.logError(e);
      throw new RuntimeException(e);
    }

    d.isModelQuantized = isQuantized;
    // Pre-allocate buffers.
    int numBytesPerChannel;
    if (isQuantized) {
      numBytesPerChannel = 1; // Quantized
    } else {
      numBytesPerChannel = 4; // Floating point
    }
    d.imgData = ByteBuffer.allocateDirect(1 * d.inputSize * d.inputSize * 3 * numBytesPerChannel);
    d.imgData.order(ByteOrder.nativeOrder());
    d.intValues = new int[d.inputSize * d.inputSize];
    d.outputVector = new float[1][9216];

    //d.tfLite.setNumThreads(NUM_THREADS);
    return d;
  }

  private AlexFeatureExtractionLite() {
  }


  public float[] calcFeatureVector(final Bitmap bitmap) {
    // Log this method so that it can be analyzed with systrace.
    Trace.beginSection(""recognizeImage"");

    Trace.beginSection(""preprocessBitmap"");
    // Preprocess the image data from 0-255 int to normalized float based
    // on the provided parameters.
    bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());

    imgData.rewind();
    for (int i = 0; i < inputSize; ++i) {
      for (int j = 0; j < inputSize; ++j) {
        int pixelValue = intValues[i * inputSize + j];
        if (isModelQuantized) {
          // Quantized model
          imgData.put((byte) ((pixelValue >> 16) & 0xFF));
          imgData.put((byte) ((pixelValue >> 8) & 0xFF));
          imgData.put((byte) (pixelValue & 0xFF));
        } else { // Float model
          imgData.putFloat((((pixelValue >> 16) & 0xFF) - 104f));
          imgData.putFloat((((pixelValue >> 8) & 0xFF) - 117f));
          imgData.putFloat(((pixelValue & 0xFF) - 124f));
        }
      }
    }
    Trace.endSection(); // preprocessBitmap

    // Copy the input data into TensorFlow.
    Trace.beginSection(""feed"");
    outputVector = new float[1][9216];

    Trace.endSection();

    // Run the inference call.
    Trace.beginSection(""run"");
    ppm_tfRun.start();
    tfLite.run(imgData, outputVector);
    ppm_tfRun.stop();
    Trace.endSection();

    Trace.endSection(); // ""recognizeImage""
    return outputVector[0];
  }
}
```

The code to run inference for the unconverted model is:
```
import android.content.res.AssetManager;
import android.graphics.Bitmap;
import android.os.Trace;
import one.realnote.app.PpmItem;
import org.tensorflow.Graph;
import org.tensorflow.Operation;
import org.tensorflow.contrib.android.TensorFlowInferenceInterface;

public class AlexFeatureExtraction {

    private static final String TAG = ""AlexFeatureExtraction"";

    // Config values.
    private String inputName;
    private int inputSize;

    // Pre-allocated buffers.
    private float[] outputVector;

    private int[] intValues;
    private float[] floatValues;

    private TensorFlowInferenceInterface inferenceInterface;

    public static AlexFeatureExtraction create(
        final AssetManager assetManager) {

        final AlexFeatureExtraction d = new AlexFeatureExtraction();


        final String modelFilename = ""file:///android_asset/tf/alex.pb"";
        final int inputSize = 227;

        d.inferenceInterface = new TensorFlowInferenceInterface(assetManager, modelFilename);

        final Graph g = d.inferenceInterface.graph();

        d.inputName = ""Placeholder""; // ssd: ""image_tensor"";

        // The inputName node has a shape of [N, H, W, C], where
        // N is the batch size
        // H = W are the height and width
        // C is the number of channels (3 for our purposes - RGB)
        final Operation inputOp = g.operation(d.inputName);
        if (inputOp == null) {
            throw new RuntimeException(""Failed to find input Node '"" + d.inputName + ""'"");
        }
        d.inputSize = inputSize;
        // The outputScoresName node has a shape of [N, NumLocations], where N
        // is the batch size.

        // ssd: final Operation outputOp1 = g.operation(""detection_scores"");
        final Operation outputOp1 = g.operation(""output"");
        if (outputOp1 == null) {
            throw new RuntimeException(""Failed to find output Node 'detection_scores'"");
        }

        // Pre-allocate buffers.
        d.outputVector = new float[9216];

        d.intValues = new int[d.inputSize * d.inputSize];
        d.floatValues = new float[d.inputSize * d.inputSize * 3];

        return d;
    }


    private AlexFeatureExtraction() {
    }


    private static PpmItem ppm_tfFeed = PpmItem.createDebugItem(""TFA"", ""feed"", String.class);
    private static PpmItem ppm_tfRun = PpmItem.createDebugItem(""TFA"", ""runAlex"", String.class);


    public float[] calcFeatureVector(final Bitmap bitmap) {
        // Log this method so that it can be analyzed with systrace.
        Trace.beginSection(""recognizeImage"");

        Trace.beginSection(""preprocessBitmap"");
        // Preprocess the image data from 0-255 int to normalized float based
        // on the provided parameters.
        bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());

        for (int i = 0; i < intValues.length; ++i) {
            floatValues[i * 3 + 2] = (float) (intValues[i] & 0xFF) - 124f;
            floatValues[i * 3 + 1] = (float) ((intValues[i] >> 8) & 0xFF) - 117f;
            floatValues[i * 3 + 0] = (float) ((intValues[i] >> 16) & 0xFF) -104f;
        }

        Trace.endSection(); // preprocessBitmap

        // Copy the input data into TensorFlow.
        Trace.beginSection(""feed"");
        ppm_tfFeed.start();
        inferenceInterface.feed(inputName, floatValues, 1, inputSize, inputSize, 3);
        ppm_tfFeed.stop();
        Trace.endSection();

        // Run the inference call.
        Trace.beginSection(""run"");
        ppm_tfRun.start();
        inferenceInterface.run(new String[]{""output""}, false);
        ppm_tfRun.stop();
        Trace.endSection();

        // Copy the output Tensor back into the output array.
        Trace.beginSection(""fetch"");


        inferenceInterface.fetch(""output"", outputVector);

        Trace.endSection();

        Trace.endSection(); // ""recognizeImage""

        return outputVector;
    }

    public void close() {
        inferenceInterface.close();
    }
}
```

**Other info / logs**
Output behavior is as expected for all models. Activating the useNNAPI option doesn't really do anything performance wise aswell as setting the number of threads.
"
23765,Help !  bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package,"mac OS  ，
I'm stuck in this for a long time！ 

ZBMAC-C02VQ1YYH:tensorflow tianchuangxin1$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
WARNING: Processed legacy workspace file /Users/tianchuangxin1/tensorflow/tools/bazel.rc. This file will not be processed in the next release of Bazel. Please read https://github.com/bazelbuild/bazel/issues/6319 for further information, including how to upgrade.
ERROR: /Users/tianchuangxin1/tensorflow/tensorflow/python/BUILD:5810:1: no such package '@cython//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/cython/cython/archive/0.28.4.tar.gz, https://github.com/cython/cython/archive/0.28.4.tar.gz] to /private/var/tmp/_bazel_tianchuangxin1/ef464020c356ceb4fa8b131168a2e389/external/cython/0.28.4.tar.gz: All mirrors are down: [] and referenced by '//tensorflow/python:framework/fast_tensor_util.pyx_cython_translation'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@cython//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/cython/cython/archive/0.28.4.tar.gz, https://github.com/cython/cython/archive/0.28.4.tar.gz] to /private/var/tmp/_bazel_tianchuangxin1/ef464020c356ceb4fa8b131168a2e389/external/cython/0.28.4.tar.gz: All mirrors are down: []
INFO: Elapsed time: 48.999s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)"
23764,How to know the value of the deterministic operation seed of tf.nn.dropout when we have already set the graph level seed and do not specify any seed in the op,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
- Are you willing to contribute it (Yes/No):



**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**
"
23763,Eager Execution doesnt save .meta file,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): example code for cnn network 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows with anaconda
- TensorFlow installed from (source or binary):binary
- TensorFlow version :1.12
- Python version:3.7
- Bazel version (if compiling from source):NA
- GCC/Compiler version (if compiling from source):NA
- CUDA/cuDNN version:9.0/7.5
- GPU model and memory:rtx 2080 


**Current behavior**

training of model in eager execution mode and save checkpoint using tfe.saver.But it only saves the following files:
-checkpoint
-checkpoint.ckpt-00-data
-checkpoint.ckpt-00-index

There is no .meta file being saved

Due to this i am not able to restore my checkpoint file and create a graph from it or use it create a .npy file.


**Describe the expected behavior**

It should have saved three .ckpt files namely:
-checkpoint
-checkpoint.ckpt-00.data
-checkpoint.ckpt-00.index
-checkpoint.ckpt-00.meta


**Code to reproduce the issue**
https://github.com/madalinabuzau/tensorflow-eager-tutorials/blob/master/07_convolutional_neural_networks_for_emotion_recognition.ipynb
This code is similar to my implementation

"
23762,TypeError: visualize_boxes_and_labels_on_image_array() takes at least 7 arguments (8 given),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**My code**

for image_path in TEST_IMAGE_PATHS:
  image = Image.open(image_path)
  image_np = load_image_into_numpy_array(image)
  image_np_expanded = np.expand_dims(image_np, axis=0)
  output_dict = run_inference_for_single_image(image_np, detection_graph)
  vis_util.visualize_boxes_and_labels_on_image_array(
      image_np,
      output_dict['detection_boxes'],
      output_dict['detection_classes'],
      output_dict['detection_scores'],
      category_index,
      instance_masks=output_dict.get('detection_masks'),
      use_normalized_coordinates=True,
      line_thickness=8)
  plt.figure(figsize=IMAGE_SIZE)
  plt.imshow(image_np)

**Describe the current behavior**
TypeError                                 Traceback (most recent call last)
<ipython-input-12-b19082c2666b> in <module>()
     17       instance_masks=output_dict.get('detection_masks'),
     18       use_normalized_coordinates=True,
---> 19       line_thickness=8)
     20   plt.figure(figsize=IMAGE_SIZE)
     21   plt.imshow(image_np)

TypeError: visualize_boxes_and_labels_on_image_array() takes at least 7 arguments (8 given)

**Describe the expected behavior**
detected files(.jpg)

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23761,C++  TensorShape from std::vector or std::array,"**System information**
- TensorFlow version (you are using): **r1.12**
- Are you willing to contribute it (Yes/No): **Yes**



**Describe the feature and the current behavior/state.**

According to [`tensor_shape.h`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/tensor_shape.h), it seems like currently `TensorShape` can only be initialized through `gtl::ArraySlice` or through `std::initializer_list`. It would be nice to initialize it with `std::vector` or `std::array`.

**Will this change the current api? How?**

Explained above.

**Who will benefit with this feature?**

If the shape of the Tensors used are stored in some other place as something like a `std::vector`, it will be easier to just pass the shape variable as the shape argument when creating a Tensor."
23760,tensorflow lite: error when convert from keras model .h5,"When I try convert the keras model to tflite, this error appears:
`
TypeError: ('Keyword argument not understood:', 'negative_slope')`

But only with some models like mobilenet and mobilenetv2. When I convert my inceptionv3 and nasnetmobile is Ok. All the models that Im trying to convert was retrained and fine tuned.

Code:
`import numpy as np`
`import tensorflow as tf`
`converter = tf.contrib.lite.TocoConverter.from_keras_model_file('models/mobilenetv2_tune.h5')`
`tflite_model = converter.convert()`
`open(""mobilenetv2_model.tflite"", ""wb"").write(tflite_model)`

**Keras version**: 2.2.4
**Tensorflow version**: 1.10.1



"
23759,longer latency after post-training quantization,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
I modified the example script a bit to print inference latency
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Google Colaboratory with CPU
- **TensorFlow installed from (source or binary)**:
! pip install -U tf-nightly==1.13.0.dev20181027
- **TensorFlow version (use command below)**:
tf-nightly-1.13.0.dev20181027
- **Python version**:
2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
The post-training quantization documentation claims that it provides 3x lower latency: https://www.tensorflow.org/lite/performance/post_training_quantization

But when I try with the example script: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tutorials/post_training_quant.ipynb
I found that quantized model is apparently slower than original one.
(You can see the logs below. Quantized model provides longer latency).

Why is the experiment result different from the documentation claim?
Should I try some other example/environment?


### Source code / logs
The original one with floating-point:
Accuracy after 500 images: 0.970000 average time: 0.002977
Accuracy after 1000 images: 0.960000 average time: 0.003007
Accuracy after 1500 images: 0.952667 average time: 0.003022
Accuracy after 2000 images: 0.951500 average time: 0.003026
Accuracy after 2500 images: 0.949600 average time: 0.003026
Accuracy after 3000 images: 0.954000 average time: 0.003024
Accuracy after 3500 images: 0.956857 average time: 0.003029
Accuracy after 4000 images: 0.954000 average time: 0.003031
Accuracy after 4500 images: 0.954222 average time: 0.003032
Accuracy after 5000 images: 0.954200 average time: 0.003038
Accuracy after 5500 images: 0.957091 average time: 0.003043
Accuracy after 6000 images: 0.958500 average time: 0.003047
Accuracy after 6500 images: 0.959692 average time: 0.003050
Accuracy after 7000 images: 0.960714 average time: 0.003050
Accuracy after 7500 images: 0.962400 average time: 0.003051
Accuracy after 8000 images: 0.964375 average time: 0.003052
Accuracy after 8500 images: 0.965059 average time: 0.003051
Accuracy after 9000 images: 0.966889 average time: 0.003052
Accuracy after 9500 images: 0.968000 average time: 0.003052
Accuracy after 10000 images: 0.966700 average time: 0.003054
total time: 30.540090
0.9667


With post-training quantization:
Accuracy after 500 images: 0.970000 average time: 0.004113
Accuracy after 1000 images: 0.959000 average time: 0.004096
Accuracy after 1500 images: 0.950667 average time: 0.004093
Accuracy after 2000 images: 0.950000 average time: 0.004084
Accuracy after 2500 images: 0.948000 average time: 0.004077
Accuracy after 3000 images: 0.952667 average time: 0.004081
Accuracy after 3500 images: 0.955714 average time: 0.004103
Accuracy after 4000 images: 0.953000 average time: 0.004125
Accuracy after 4500 images: 0.953111 average time: 0.004133
Accuracy after 5000 images: 0.953200 average time: 0.004130
Accuracy after 5500 images: 0.956182 average time: 0.004126
Accuracy after 6000 images: 0.957667 average time: 0.004132
Accuracy after 6500 images: 0.958923 average time: 0.004137
Accuracy after 7000 images: 0.960000 average time: 0.004142
Accuracy after 7500 images: 0.961600 average time: 0.004146
Accuracy after 8000 images: 0.963625 average time: 0.004149
Accuracy after 8500 images: 0.964353 average time: 0.004148
Accuracy after 9000 images: 0.966222 average time: 0.004145
Accuracy after 9500 images: 0.967368 average time: 0.004140
Accuracy after 10000 images: 0.966100 average time: 0.004136
total time: 41.360908
0.9661

We can see after post-training quantization, the latency is apparently longer......
But in the documentation, it claims to reduce latency....
"
23758,ImportError: cannot import name 'cloud',"The latest build is giving me a weird Import Error on a module, even though I added extra stuff to the necessary __init__.py. Build command and trace below:


**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): Master branch
- Python version: 3.6.5
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source): 8.2.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Code to reproduce the issue**
```bash 
wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel_0.15.0-linux-x86_64.deb && \
    sudo dpkg -i bazel_0.15.0-linux-x86_64.deb && \
    sudo apt-get install -f && \
    rm bazel_0.15.0-linux-x86_64.deb && \
    pip uninstall -y tensorflow-tensorboard tfp-nightly tensorflow_estimator tb-nightly tf-nightly tensorflow && \
    cd /opt && \
    git clone --recursive   https://github.com/tensorflow/tensorflow.git && \
    cd /opt/tensorflow && \
    sed -i 's/2018\.0\.3\.20180406/2019\.0\.1\.20180928/g' tensorflow/contrib/cmake/external/mkl.cmake && \
    sed -i 's/v0\.14/v0\.17\-rc/g' tensorflow/contrib/cmake/external/mkl.cmake && \
    sed -i 's/3063b2e4c943983f6bf5f2fb9a490d4a998cd291/21fb5f2af1dd14e132af4f1b79160977ee487818/g' tensorflow/contrib/cmake/external/mkldnn.cmake && \
    ln -s /usr/lib/libmkldnn.so /usr/lib/libiomp5.so /usr/lib/libmklml_gnu.so  /usr/lib/libmklml_intel.so $MKLROOT/lib/intel64_lin && \
    ln -s /usr/include/mkldnn* $MKLROOT/include && \
    ln -s $MKLROOT/lib/intel64_lin/* $MKLROOT/lib && \
    ln -s /usr/lib /usr/lib/lib && \
    echo """" > /usr/local/lib/license.txt && \
    echo """" > /usr/local/include/license.txt && \
    echo ""from tensorflow.contrib import cloud"" >> tensorflow/contrib/__init__.py && \
    echo ""from tensorflow.contrib import *"" >> tensorflow/contrib/__init__.py && \
    TF_MKL_ROOT=/usr/lib  \
    TF_MKL_DOWNLOAD=0 \
    USE_DEFAULT_PYTHON_LIB_PATH=1 \
    TF_NEED_MKL=1 \
    TF_NEED_JEMALLOC=1 \
    TF_NEED_GCP=0 \
    TF_NEED_HDFS=0 \
    TF_ENABLE_XLA=1 \
    TF_NEED_MPI=0 \
    TF_NEED_GDR=0 \
    TF_NEED_S3=1 \
    TF_NEED_KAFKA=0 \
    TF_SET_ANDROID_WORKSPACE=0 \
    TF_NEED_CUDA=0 \
    TF_MKL_ENABLED=""true"" \
    CI_BUILD_PYTHON=/opt/conda/bin/python \
    PYTHON_BIN_PATH=/opt/conda/bin/python \
    PYTHON_LIB_PATH=/opt/conda/lib/python3.6/site-packages \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    CC_OPT_FLAGS=""-msse4.2 -msse4.1 -mavx -msse2 -msse3 -mfpmath=sse -lmkl_gf_lp64 -Wl,--start-group -lmkldnn -lmklml_intel -lmkl_gnu_thread -lmkl_core -Wl,--end-group -dl -lpthread -lm "" \
    /bin/bash ./configure \
    && \
    bazel build \
    --config=mkl --config=opt \
    --config=noaws --config=nogcp --config=verbs --config=noignite --config=nokafka \
    --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" \
    --copt=-msse4.2 --copt=-msse4.1 --copt=-mavx --copt=-msse2 --copt=-msse3  \
    --copt=-O3 --copt=-mfpmath=both \
    --copt=""-DMKL_LP64"" \
    --copt=""-fPIC"" \
    --linkopt=""-lmkl_gf_lp64"" \
    --linkopt=""-lmkl_gnu_thread"" \
    --linkopt=""-dl"" \
    --linkopt=""-ldl"" \
    --linkopt=""-lpthread"" \
    --linkopt=""-lmkl_core"" \
    --linkopt=""-lm"" \
    --linkopt=""-lmkl_rt"" \
    --linkopt=""-lmkldnn"" \
    tensorflow/tools/pip_package:build_pip_package && \
    bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip && \
    pip install --no-deps /tmp/pip/tensorflow-*.whl && \
    cd /opt && rm -rf /opt/tensorflow /tmp/* && \
    python -c ""import tensorflow as tf; hello = tf.constant('Hello, TensorFlow!'); sess = tf.Session(); print(sess.run(hello))"" && \

### Build TFP after this ###

    python -c ""import tensorflow_probability""

```

** Trace **
```python 
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow_probability/__init__.py"", line 78, in <module>
    from tensorflow_probability.python import *  # pylint: disable=wildcard-import
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow_probability/python/__init__.py"", line 22, in <module>
    from tensorflow_probability.python import distributions
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow_probability/python/distributions/__init__.py"", line 66, in <module>
    from tensorflow_probability.python.distributions.multivariate_student_t import MultivariateStudentTLinearOperator
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow_probability/python/distributions/multivariate_student_t.py"", line 25, in <module>
    from tensorflow_probability.python import math
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow_probability/python/math/__init__.py"", line 22, in <module>
    from tensorflow_probability.python.math.diag_jacobian import diag_jacobian
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow_probability/python/math/diag_jacobian.py"", line 24, in <module>
    tfe = tf.contrib.eager
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/util/lazy_loader.py"", line 53, in __getattr__
    module = self._load()
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/util/lazy_loader.py"", line 42, in _load
    module = importlib.import_module(self.__name__)
  File ""/opt/conda/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/contrib/__init__.py"", line 30, in <module>
    from tensorflow.contrib import cloud
ImportError: cannot import name 'cloud'

```"
23756,Fix technical issues with README.md,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version:
- Doc Link:


**Describe the documentation issue**

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
23755,Windows 10 Failed to load the native TensorFlow runtime,"Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template

System information

OS Platform and Distribution Windows 10 64 bit
TensorFlow installed from (source or binary): pip install tensorflow
TensorFlow version: 1.12.0
Python version:3.6.5
GPU model and memory: Geforce 150mx 4gb
Describe the problem
I'm unable to import keras using ""Tensorflow backend""
when I try to import ""from keras.models import Sequential"" I get the error shown bellow

These are the steps that I did when I installed the libraries
1- conda create -n tensorflow python=3.5 anaconda.

2- activate tensorflow.

3- conda install theano.

4- conda install mingw libpython.

5-pip install tensorflow.

6- pip install keras.

7- conda update --all

8- pip install tensorflow-gpu

Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Error shown:

`from keras.models import Sequential
Using TensorFlow backend.
Traceback (most recent call last):

File """", line 1, in 
from keras.models import Sequential

File ""C:\Users\MHD\Anaconda3\lib\site-packages\keras_init_.py"", line 3, in 
from . import utils

File ""C:\Users\MHD\Anaconda3\lib\site-packages\keras\utils_init_.py"", line 6, in 
from . import conv_utils

File ""C:\Users\MHD\Anaconda3\lib\site-packages\keras\utils\conv_utils.py"", line 9, in 
from .. import backend as K

File ""C:\Users\MHD\Anaconda3\lib\site-packages\keras\backend_init_.py"", line 89, in 
from .tensorflow_backend import *

File ""C:\Users\MHD\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py"", line 5, in 
import tensorflow as tf

File ""C:\Users\MHD\Anaconda3\lib\site-packages\tensorflow_init_.py"", line 24, in 
from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import

File ""C:\Users\MHD\Anaconda3\lib\site-packages\tensorflow\python_init_.py"", line 49, in 
from tensorflow.python import pywrap_tensorflow

File ""C:\Users\MHD\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in 
raise ImportError(msg)

ImportError: Traceback (most recent call last):
File ""C:\Users\MHD\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in 
from tensorflow.python.pywrap_tensorflow_internal import *
File ""C:\Users\MHD\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in 
_pywrap_tensorflow_internal = swig_import_helper()
File ""C:\Users\MHD\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
File ""C:\Users\MHD\Anaconda3\lib\imp.py"", line 243, in load_module
return load_dynamic(name, filename, file)
File ""C:\Users\MHD\Anaconda3\lib\imp.py"", line 343, in load_dynamic
return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions. Include the entire stack trace
above this error message when asking for help.`
"
23754,Sparsification of huge matrix,"**Describe the current behavior**
Currently, the only way in TensorFlow to convert a dense matrix into a sparse one (assuming it has only few non-zero values) is to use `tf.where` (afaik). `tf.where` does however allocate a lot of memory (it seems like 4 times the size of the dense matrix). So for huge dense matrices, this approach fails.

**Describe the expected behavior**
By first counting the number of nonzero elements, then allocating the appropriate amount of memory, and then determining the non-zero values, this can be be done with very little memory (as much as there are non-zero elements).
This is the approach that the `cusparse` library follows and it works great. Unfortunately, there is no way to use e.g. `dense2csr` or `dense2coo` in TensorFlow.

In my opinion this is a bug, because `tf.where` requires far too much memory."
23753,Can't Enable Eager Execution [tensorflow_gpu==1.12.0],"I'm following the [Tensorflow Eager Execution Guide](https://www.tensorflow.org/guide/eager) and can't  enable eager execution.

System info:

> tensorflow_gpu 1.12.0 (std binary)
python 3.6.6
linux Mint 19
kernel 4.15.0-38
nvidia geforce 1050ti

Steps to reproduce:

```python
import tensorflow as tf
tf.enable_eager_execution()
tf.executing_eagerly()
# False
```"
23751,Windows 10: Failed to load native runtime.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS Platform and Distribution Windows 10 64 bit
- TensorFlow installed from (source or binary): pip install tensorflow
- TensorFlow version: 1.12.0
- Python version:3.6.5
- GPU model and memory: Geforce 150mx 4gb

**Describe the problem**
I'm unable to import keras using _""Tensorflow backend""_
when I try to import _""from keras.models import Sequential""_  I get the error shown bellow

**These are the steps that I did when I installed the libraries**
1- conda create -n tensorflow python=3.5 anaconda.

2- activate tensorflow.

3- conda install theano.

4- conda install mingw libpython.

5-pip install tensorflow.

6- pip install keras.

7- conda update --all

8-  pip install tensorflow-gpu

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.



Error shown:

`from keras.models import Sequential
Using TensorFlow backend.
Traceback (most recent call last):

  File ""<ipython-input-5-9c5e0a19b646>"", line 1, in <module>
    from keras.models import Sequential

  File ""C:\Users\MHD\Anaconda3\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils

  File ""C:\Users\MHD\Anaconda3\lib\site-packages\keras\utils\__init__.py"", line 6, in <module>
    from . import conv_utils

  File ""C:\Users\MHD\Anaconda3\lib\site-packages\keras\utils\conv_utils.py"", line 9, in <module>
    from .. import backend as K

  File ""C:\Users\MHD\Anaconda3\lib\site-packages\keras\backend\__init__.py"", line 89, in <module>
    from .tensorflow_backend import *

  File ""C:\Users\MHD\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf

  File ""C:\Users\MHD\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import

  File ""C:\Users\MHD\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow

  File ""C:\Users\MHD\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)

ImportError: Traceback (most recent call last):
  File ""C:\Users\MHD\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\MHD\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\MHD\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\MHD\Anaconda3\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\MHD\Anaconda3\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.`

"
23750,values in the output tensor,"Hi, i have a frozen_graph pretrained model. and i what to know values in the output tensor in this model. How can i print it? maybe who know."
23749,Export train-test split in raster/vector format,"I want to classify an image using support vector machine algorithm. Then, I imported samples in raster format and applied train-test split to assign 70% for training and 30% for testing. I want to export the samples after being split to vector or raster format. Is it possible to perform such things?

Thanks for your help.
"
23748,tensorflow.keras Dense layers complain if the input is a sparse Input layer.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
OSX Mojave 10.14.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
na
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
1.10.0 (v1.10.0-rc1-19-g656e7a2b34)
- Python version:
3.5
- Bazel version (if compiling from source):
NA
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
See below, which I ran on a OSX Mojave Macbook Pro (Early 2015), ipython running python 3.5, tensorflow 1.10.0:

```
In [1]: from tensorflow.keras.models import Model
//anaconda/envs/dssm/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5
  return f(*args, **kwds)

In [2]: from tensorflow.keras.layers import Input, Dense

In [3]: i = Input((4,), sparse=True)

In [4]: d = Dense(4)(i)
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-4-0fb73bda26dc> in <module>
----> 1 d = Dense(4)(i)

//anaconda/envs/dssm/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    718
    719         # Check input assumptions set before layer building, e.g. input rank.
--> 720         self._assert_input_compatibility(inputs)
    721         if input_list and self._dtype is None:
    722           try:

//anaconda/envs/dssm/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py in _assert_input_compatibility(self, inputs)
   1408           spec.min_ndim is not None or
   1409           spec.max_ndim is not None):
-> 1410         if x.shape.ndims is None:
   1411           raise ValueError('Input ' + str(input_index) + ' of layer ' +
   1412                            self.name + ' is incompatible with the layer: '

AttributeError: 'SparseTensor' object has no attribute 'shape'
```

**Describe the expected behavior**

If I were using normal Keras, I'd expect no errors trying to do the above and for the model to compile subsequently without issue.

**Code to reproduce the issue**
See the code in my snippet above.

**Other info / logs**

"
23747,Feature Request - Deeplab TFLITE Android Application,"<em>Feature request, requesting an android application for Deeplab tag:feature_template</em>


**System information**
- TensorFlow version (you are using): Tensorflow 1.12 [tensorflow-lite-1.12 ]
- Model Information: Deeplab V3 MobilenetV2
- Are you willing to contribute it (Yes/No): No
- Android API Version: API 24 (Android Nougat)



**Feature Current Behavior**
There is no mobile application to test out the working of Deeplab tflite model in Android or IOS. This seems as a direct need for developers and it will be helpful for knowing the parsing mechanism for tflite where we get semantic predictions as an output, as there is an unclear way of parsing the specific data type in android and ios(prediction?) as it involves pixel data.

**Current API: Need of change**
We are in need of new API model to help in with parsing the model input. Could be released as a subsequent fix. 

**Beneficiaries**
People who are developing camera applications can directly benefit from this as it involves playing with segmentation on android devices.

**Other information.**
We are trying building the application tweaking with available applications. We are currently facing up some issues. Hereby with, we are attaching the issue links.

[https://stackoverflow.com/questions/53228969/unable-to-test-and-deploy-a-deeplabv3-mobilenetv2-tensorflow-lite-segmentation-m](https://stackoverflow.com/questions/53228969/unable-to-test-and-deploy-a-deeplabv3-mobilenetv2-tensorflow-lite-segmentation-m)

[https://stackoverflow.com/questions/53236290/unable-to-load-tflite-deeplab-segmentation-model-in-android-application-error](https://stackoverflow.com/questions/53236290/unable-to-load-tflite-deeplab-segmentation-model-in-android-application-error)"
23746,"""Invalid loop structure"" for Cleverhans saliency map attack on Keras model with Dropout layers","**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Fedora 29
- TensorFlow installed from: source
- TensorFlow version: v1.11.0-0-gc19e29306c
- Python version: 3.6.6
- Bazel version: 0.19.1- (@non-git)
- GCC/Compiler version: 7.3.0
- CUDA/cuDNN version: 9.2
- GPU model and memory: GeForce GTX 1050 3.95GiB

**Describe the current behavior**
- Prepared & trained a Keras Sequential model based on [this example script](https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py), which contains Dropout layers
- Passed the trained model to a [KerasModelWrapper](https://github.com/tensorflow/cleverhans/blob/master/cleverhans_tutorials/mnist_tutorial_keras_tf.py), a class provided by the [Cleverhans](https://github.com/tensorflow/cleverhans) library
- Instantiated a [SaliencyMapMethod](https://github.com/tensorflow/cleverhans/blob/master/cleverhans_tutorials/mnist_tutorial_jsma.py) attack object on the wrapped model, and attempted to craft an adversarial example with it
- TensorFlow encounters this error:
```
tensorflow.python.framework.errors_impl.InternalError: Invalid loop structure: Mismatched parent frames for ""while_1/while_context"": ""while_1/while_context"" vs """". The node giving this error: {{node while_1/gradients/while_1/model_4/dropout_1/cond/mul_grad/Shape/Enter}}This is an internal bug, please file a bug report with instructions on how to reproduce the error.
```
- Removing the Dropout layers from the Keras model prevents this error from occuring.

**Describe the expected behavior**
Want the `generate_np` method of the SaliencyMapMethod object to return an adversarial example for the wrapped Keras model.

**Code to reproduce the issue**
```
import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.activations import relu, softmax

import cleverhans
from cleverhans.attacks import SaliencyMapMethod
from cleverhans.utils_keras import KerasModelWrapper

import numpy as np

dataset = keras.datasets.cifar10
num_classes = 10

(train_images, train_labels), (test_images, test_labels) = dataset.load_data()
input_shape = train_images.shape[1:]

train_images = train_images.astype('float32') / 255.0
test_images  = test_images.astype('float32') / 255.0

train_labels = keras.utils.to_categorical(train_labels, num_classes)
test_labels  = keras.utils.to_categorical(test_labels, num_classes)

model = Sequential([
    Conv2D(32, (3,3), padding='same', input_shape=input_shape, activation=relu),
    Conv2D(32, (3,3), activation=relu),
    MaxPooling2D(pool_size=(2,2)),
    Dropout(0.25),

    Conv2D(64, (3,3), padding='same', activation=relu),
    Conv2D(64, (3,3), activation=relu),
    MaxPooling2D(pool_size=(2, 2)),
    Dropout(0.25),

    Flatten(),
    Dense(512, activation=relu),
    Dropout(0.5),
    Dense(num_classes, activation=softmax)
])

model.compile(optimizer=keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True),
              loss=keras.losses.categorical_crossentropy,
              metrics=[keras.metrics.categorical_accuracy])

model.fit(train_images, train_labels)

jsma = SaliencyMapMethod(KerasModelWrapper(model), keras.backend.get_session())
jsma_params = {'theta': 1., 'gamma': 0.1,
               'clip_min': 0., 'clip_max': 1.,
               'y_target': None}

# Generate adversarial examples for first test sample
sample_ind = 0
samples = test_images[sample_ind:(sample_ind + 1)]
current_class = int(np.argmax(test_labels[sample_ind]))
for target in cleverhans.utils.other_classes(num_classes, current_class):
    one_hot_target = np.zeros((1, num_classes), dtype=np.float32)
    one_hot_target[0, target] = 1
    jsma_params['y_target'] = one_hot_target
    # Error happens here
    adversarial_images = jsma.generate_np(samples, **jsma_params)
    break
```

**Other info / logs**
- Keras version: 2.2.4 (from pip)
- Cleverhans version: 2.0.0-fe079e17fb307c71a4a413d8bd408d98 (from source, commit [0b22b1c](https://github.com/tensorflow/cleverhans/commit/0b22b1c523cd06efd93d3999c5118986819ac98e))
- Full traceback of the error when running the above code:
```
Traceback (most recent call last):
  File ""~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1292, in _do_call
    return fn(*args)
  File ""~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1277, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1367, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InternalError: Invalid loop structure: Mismatched parent frames for ""while/while_context"": ""while/while_context"" vs """". The node giving this error: {{node while/gradients/while/model_1/dropout_1/cond/mul_grad/Shape/Enter}}This is an internal bug, please file a bug report with instructions on how to reproduce the error.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""crash.py"", line 62, in <module>
    adversarial_images = jsma.generate_np(samples, **jsma_params)
  File ""~/cleverhans/cleverhans/attacks.py"", line 203, in generate_np
    return self.sess.run(x_adv, feed_dict)
  File ""~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 887, in run
    run_metadata_ptr)
  File ""~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1110, in _run
    feed_dict_tensor, options, run_metadata)
  File ""~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run
    run_metadata)
  File ""~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Invalid loop structure: Mismatched parent frames for ""while/while_context"": ""while/while_context"" vs """". The node giving this error: {{node while/gradients/while/model_1/dropout_1/cond/mul_grad/Shape/Enter}}This is an internal bug, please file a bug report with instructions on how to reproduce the error.
```
"
23743,"ckpt file only has .meta file , no .index file or .data file in every step to save","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Liunx centos7.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version:1.11
- Python version:2.7
- Installed using virtualenv? pip? conda?:pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda9 cndnn7
- GPU model and memory:M40 12GB



**Describe the problem**
I use MonitoredTrainingSession to train distribute tf  on 3 machines(1 ps , 2 workers), and it worked ,but the ckpt file  only has .meta file , no .index file or .data file in every step to save. The max train step was 1000,no errors in the training time.
**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
the file like this
-rw-r--r-- 1 root root      693 11月 14 22:10 checkpoint
-rw-r--r-- 1 root root 17226852 11月 14 22:12 events.out.tfevents.1542203421.node58
-rw-r--r-- 1 root root  9268881 11月 14 21:50 graph.pbtxt
-rw-r--r-- 1 root root  4858196 11月 14 22:10 model.ckpt-1000.meta
-rw-r--r-- 1 root root  4858196 11月 14 22:02 model.ckpt-601.meta
-rw-r--r-- 1 root root  4858196 11月 14 22:04 model.ckpt-701.meta
-rw-r--r-- 1 root root  4858196 11月 14 22:06 model.ckpt-801.meta
-rw-r--r-- 1 root root  4858196 11月 14 22:08 model.ckpt-901.meta
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
part of the log 
INFO:tensorflow:global_step/sec: 0.876005
INFO:tensorflow:Saving checkpoints for 501 into /data0/users/haha/tensorflow-test/MonitoredTrainingSession_test/log/model.ckpt.
2018-11-14 22:54:46.889383 : global step = 500, loss = 1.45 (12.6 examples/sec; 1.265sec/batch)
2018-11-14 22:54:59.885239 : global step = 510, loss = 1.13 (12.3 examples/sec; 1.300sec/batch)
2018-11-14 22:55:09.314735 : global step = 520, loss = 0.88 (17.0 examples/sec; 0.943sec/batch)
2018-11-14 22:55:23.053285 : global step = 530, loss = 1.01 (11.6 examples/sec; 1.374sec/batch)

```
the code 
```
def main(_):
tf.logging.set_verbosity(tf.logging.INFO)
    tf.reset_default_graph()
    # 1.cluster spec
    worker_spec = FLAGS.workerSpec.split(',')
    ps_spec = FLAGS.psSpec.split(',')
    print('cluster has %d workers, %d ps' % (len(worker_spec), len(ps_spec)))
    cluster = tf.train.ClusterSpec({'worker': worker_spec,
                                    'ps': ps_spec})
    if FLAGS.job_name == 'ps':
        server.join()
    elif FLAGS.job_name == 'worker':
        if FLAGS.gpu_num > 0:
            worker_device = ""/job:worker/task:{}/gpu:{}"".format(FLAGS.task_index, FLAGS.task_index % FLAGS.gpu_num)
        else:
            worker_device = ""/job:worker/task:{}/cpu:0"".format(FLAGS.task_index)

        # 2. data  https://www.tensorflow.org/programmers_guide/datasets
        with tf.device(tf.train.replica_device_setter(
                                                      worker_device=worker_device,
                                                      cluster=cluster)):
            filenames = get_tfrecord_filenames(FLAGS.dataset_dir)
            tf.logging.info(""dataset %s:"" % filenames)
            dataset = tf.data.TFRecordDataset(filenames)
            dataset = dataset.map(parse_single_image)
            dataset = dataset.shuffle(buffer_size=1000)
            dataset = dataset.batch(FLAGS.batch_size)
            dataset = dataset.repeat(FLAGS.epochs)
            iterator = dataset.make_initializable_iterator()
          
            batch = iterator.get_next()
            image_file_name, img_batch, label_batch = batch
            print(img_batch, label_batch)

            # 3. model
            with slim.arg_scope(inception_v3_arg_scope()):
                logits, _ = inception_v3(img_batch, 9)
            predicts = tf.nn.softmax(logits)
          
            loss = tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=tf.one_hot(label_batch, depth=9, on_value=1, off_value=0))

            global_step = tf.train.get_or_create_global_step()
            opt = tf.train.AdamOptimizer(FLAGS.lr)

            if FLAGS.sync_replicas:
                if FLAGS.replicas_to_aggregate is None:
                    replicas_to_aggregate = len(worker_spec)
                else:
                    replicas_to_aggregate = FLAGS.replicas_to_aggregate
                opt = tf.train.SyncReplicasOptimizer(opt,
                                                     replicas_to_aggregate,
                                                     total_num_replicas=len(worker_spec),
                                                     #replica_id=FLAGS.task_index,
                                                     name=""sync_replicas_optimizer"")

                sync_replicas_hook = opt.make_session_run_hook(is_chief=(FLAGS.task_index == 0), num_tokens=0)

            train_op = opt.minimize(loss, global_step=global_step)

        class _DatasetInitializerHook(tf.train.SessionRunHook):
            def __init__(self, data_iterator):
                super(_DatasetInitializerHook, self).__init__()
                self._iterator = data_iterator

            def begin(self):
                self._initializer = self._iterator.initializer

            def after_create_session(self, session, coord):
                del coord
                session.run(self._initializer)

        class _LogHook(tf.train.SessionRunHook):
            def __init__(self, global_step, loss, log_frequency, batch_size):
                super(_LogHook, self).__init__()
                self.loss = loss
                self.global_step = global_step
                self.log_frequency = log_frequency
                self.batch_size = batch_size

            def begin(self):
                self.start_time = time.time()

            def before_run(self, run_context):
                return tf.train.SessionRunArgs([global_step,loss])

            def after_run(self,
                          run_context,  # pylint: disable=unused-argument
                          run_values):
                global_step_value, loss_value = run_values.results
                if global_step_value % self.log_frequency == 0:
                    current_time = time.time()
                    duration = current_time - self.start_time
                    self.start_time = current_time
                    img_per_sec = self.log_frequency * self.batch_size / duration
                    sec_per_batch = float(duration / self.log_frequency)
                    format_str = ('%s : global step = %d, loss = %.2f (%.1f imgs/sec; %.3fsec/batch)')
                    print(format_str % (datetime.now(), global_step_value, loss_value, img_per_sec, sec_per_batch))

        # 4.train
        saver=tf.train.Saver()
        scaffold = tf.train.Scaffold(saver=saver)

        hooks = [tf.train.StopAtStepHook(last_step=FLAGS.steps),
                 tf.train.NanTensorHook(loss),
                 _LogHook(global_step ,loss, FLAGS.log_frequency, FLAGS.batch_size),
                 _DatasetInitializerHook(iterator),
                 sync_replicas_hook]

        config = tf.ConfigProto(allow_soft_placement=True,
                                log_device_placement=False,
                                device_filters=[""/job:ps"", ""/job:worker/task:%d"" % FLAGS.task_index])
        config.gpu_options.allow_growth = True

        with tf.train.MonitoredTrainingSession(master=server.target,
                                               is_chief=(FLAGS.task_index == 0),
                                               checkpoint_dir=FLAGS.train_dir,
                                               scaffold=scaffold,
                                               hooks=hooks,
                                               save_checkpoint_steps=100,
                                               stop_grace_period_secs=5,
                                               config=config) as mon_sess:

            try:
                while not mon_sess.should_stop():
                    mon_sess.run(train_op)
            except Exception as e:
                print(e)

```
So,anyone to help me ? thank you ! "
23740,No TF import library to link custom operators on Windows,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, 64 bits
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): binary / pip
- TensorFlow version (use command below): 1.12
- Python version: 3.5
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: -
- GPU model and memory: -

**Describe the current behavior**

The import library tensorflow\python\pywrap_tensorflow_internal.lib is missing. I tried to regenerate it myself from _pywrap_tensorflow_internal.pyd to build my custom operators, but
tf.load_op_library throws the exception tensorflow.python.framework.errors_impl.NotFoundError and does not find the library given in argument. The exception appears in TF_LoadLibrary.

**Describe the expected behavior**

In TF 1.8 the import library tensorflow\python\pywrap_tensorflow_internal.lib is present, but not in TF 1.12. This library is required to build custom operators.

tf.load_op_library is working correctly with TensorFlow 1.8 on my configuration. The same custom operators rebuilt for TF 1.12 are not found by tf.load_op_library, although the paths of the dll given to tf.load_op_library are correct.

**Other info / logs**

Either there is a problem with tf.load_op_library in TF 1.12 contrary to TF 1.8, or there is a mistake in my process to generate the import library from the pyd file (the process is normally used for dll, not pyd).

In all cases, the dll of the custom operator generated with the provided import library in TF 1.8 depends on _pywrap_tensorflow_internal.pyd, whereas the dll generated with my own import library for TF 1.12 depends on pywrap_tensorflow_internal.dll which is a non existing file.

The simplest way to tackle this issue first seems to include again pywrap_tensorflow_internal.lib in the pip package to build custom operators on Windows."
23737,tf.load_op_library behaves different in version 1.12 vs 1.11,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): -
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): pip binary
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6.5
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): gcc 6.5 for custom ops
- CUDA/cuDNN version: 9.0/7.3.1
- GPU model and memory: Titan X with 10GB

**Describe the current behavior**
When I use tf.load_op_library to load my library with two ops, only one of them appears in the loaded module.

**Describe the expected behavior**
Both ops should appear in the loaded module. Works in version 1.11."
23736,"Change the dependent @nsync version, resulting in a significant increase in the .so size","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Linux Ubuntu 16.04
- TensorFlow version:1.8.0
- Python version:3.5
- Bazel version (if compiling from source):0.17.2
- GCC/Compiler version (if compiling from source):gcc (Ubuntu 5.4.0-6ubuntu1~16.04.5)



**Describe the problem**
I want compile tensorflow1.8.0 to target cpu arrch64，And got the error as issue #13115.
```
ERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/nsync/BUILD:401:13: Configurable attribute ""copts"" doesn't match this configuration (would a default condition help?).
Conditions checked:
 @nsync//:android_arm
 @nsync//:android_arm64
 @nsync//:android_armeabi
 @nsync//:android_x86_32
 @nsync//:android_x86_64
 @nsync//:clang_macos_x86_64
 @nsync//:gcc_linux_aarch64
 @nsync//:gcc_linux_ppc64
 @nsync//:gcc_linux_x86_64_1
 @nsync//:gcc_linux_x86_64_2
 @nsync//:ios_x86_64
 @nsync//:msvc_windows_x86_64.
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.
```
the issue tells me to add `""//conditions:default"": [],` in the depend nsync. It does work.
But I was asked not to modify any open source binaries，so，I use tensorflow as an external repository and use the following rule to change tensorflow1.8.0's depend nsync version.
```
local_repository(
	name=""org_tensorflow"",
	path=""../tmp"",
    repo_mapping = {""@nsync"" : ""@nsync_1.20.1""},
)
```
```
tf_http_archive(
    name = ""nsync_1.20.1"",
    urls = [
        ""https://mirror.bazel.build/github.com/google/nsync/archive/1.20.1.tar.gz"",
        ""https://github.com/google/nsync/archive/1.20.1.tar.gz"",
    ],
    sha256 = ""692f9b30e219f71a6371b98edd39cef3cbda35ac3abc4cd99ce19db430a5591a"",
    strip_prefix = ""nsync-1.20.1"",
)
```

This works for me，But，this resulting in a significant increase in the .so size. By default nsync it was 50M and when I change change the denpend nsync version，the .so file was 160M.

Is this normal? How should I do now? It's been bothering me for a long time, hoping to give me some advice!
"
23735,Riccati solver suggest,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.7.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
The Discrete/Continuous Algebraic Riccati Equation is very important to linear quadratic regulator system. A solution of solving this need to be implemented in Tensorflow if one wants to combine machine learning with control theory. 

The solver has been implemented in [Matlab](https://se.mathworks.com/help/control/ref/dare.html)/Scipy etc. But there are also possibilities to implement in Tensorflow CPU/GPU.

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
Those who wants to combine control theory with machine learning and deep leanring

**Any Other info.**
"
23734,Object_detection model supports IOS?,"

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:MAC 10.14
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:Phone 8
- **TensorFlow installed from (source or binary)**:bazel 
- **TensorFlow version (use command below)**:1.10.0
- **Python version**:3.6
- **Bazel version (if compiling from source)**:0.15.0
- **GCC/Compiler version (if compiling from source)**:NO
- **CUDA/cuDNN version**:NO
- **GPU model and memory**:NO
- **Exact command to reproduce**:NO


### Source code / logs
How to support IOS for object detection？？？
model:ssd_mobilenet_v1_"
23733,Memory leak in tf.train.Example/tf.train.Features/tf.gfile,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Archlinux
- TensorFlow installed from (source or binary): repository
- TensorFlow version (use command below): 1.12
- Python version: 3.7
- CUDA/cuDNN version: cuda 10, cudnn 7
- GPU model and memory: nvidia 1080ti


**Describe the current behavior**

The system goes OOM and I have 64 GiB of memory.

**Describe the expected behavior**

I should be able to create a dataset converting files I read from Google Cloud to local tfrecords.

I'm debugging the memory usage of python and as you can see below, the usage is around 670K - hence the memory allocation should be outside of python, and the only library I'm using is Tensorflow.

**Code to reproduce the issue**

```python
import subprocess
import multiprocessing
from glob import glob
import tensorflow as tf
from tensorflow import keras as k
import json
import os
import logging
from typing import Pattern, List, Tuple, Callable
import linecache
import tracemalloc


def display_top(snapshot, key_type=""lineno"", limit=10):
    snapshot = snapshot.filter_traces(
        (
            tracemalloc.Filter(False, ""<frozen importlib._bootstrap>""),
            tracemalloc.Filter(False, ""<unknown>""),
        )
    )
    top_stats = snapshot.statistics(key_type)

    print(""Top %s lines"" % limit)
    for index, stat in enumerate(top_stats[:limit], 1):
        frame = stat.traceback[0]
        # replace ""/path/to/module/file.py"" with ""module/file.py""
        filename = os.sep.join(frame.filename.split(os.sep)[-2:])
        print(
            ""#%s: %s:%s: %.1f KiB"" % (index, filename, frame.lineno, stat.size / 1024)
        )
        line = linecache.getline(frame.filename, frame.lineno).strip()
        if line:
            print(""    %s"" % line)

    other = top_stats[limit:]
    if other:
        size = sum(stat.size for stat in other)
        print(""%s other: %.1f KiB"" % (len(other), size / 1024))
    total = sum(stat.size for stat in top_stats)
    print(""Total allocated size: %.1f KiB"" % (total / 1024))


tracemalloc.start()

CACHE = os.path.join(os.getcwd(), "".data"")
ROWS_PER_RECORD = 32
SPLITS = (""train"", ""validation"", ""test"")

LOG = logging.Logger(__name__)
LOG.setLevel(logging.INFO)


def _int64_feature(value):
    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))


def _float_feature(value):
    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))


def _bytes_feature(value):
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))


def rows_in_tfrecord(path):
    """"""Count the number of elements in a tfrecord file.
    Args:
        path: The path of the TFRecord file
    Return:
        The number of examples in the TFRecord
    """"""
    return sum(1 for _ in tf.io.tf_record_iterator(path))


def _is_dir(path: str):
    """"""Determines if path is a directory only looking at the filename.
    It's ugly, but is the only way to get a decent speed.
    Args:
        path: The input path
    Return:
        value: True if the path looks like a directory
    """"""
    return path.endswith(""/"") or not ""."" in path.split(""/"")[-1]


def list_dir(root):
    """"""Recursively list a directory and returns the list of all the files.
    Args:
        root: The path of the root directory
    Return:
        List of all the files found in this directory and its subfolders.
    """"""
    ret = []
    for full_path in tf.gfile.Glob(os.path.join(root, ""*"")):
        if _is_dir(full_path):
            return ret + list_dir(full_path)
        else:
            ret.append(full_path)
    return ret


def _gs2tfrecord(gs_paths: List[str], patterns: List[Pattern], cache: str = CACHE):
    """"""Fetch images from google cloud storage, conver to tf_record and
    properly handle cache.

    Args:
        gs_paths: list of paths on google cloud storage
        patterns: list of compiled regex to fiter the data on each gs_paths
        cache: local cache/dataset folder
    """"""
    gcloud_auth = os.path.join(
        os.path.expanduser(""~""),
        "".config"",
        ""gcloud"",
        ""application_default_credentials.json"",
    )
    if not os.path.exists(gcloud_auth):
        subprocess.check_call([""gcloud"", ""auth"", ""application-default"", ""login""])

    meta_file = os.path.join(cache, ""meta.json"")
    dataset_path = os.path.join(cache, ""dataset"")
    if not os.path.exists(cache):
        os.makedirs(dataset_path)
        for split in SPLITS:
            os.makedirs(os.path.join(dataset_path, split))
        meta = {""local_files"": {path: [] for path in gs_paths}}
    else:
        with open(meta_file, ""r"") as fp:
            meta = json.load(fp)

    for remote_path, pattern in zip(gs_paths, patterns):
        remote_files = set()
        for name in list_dir(remote_path):
            if pattern.search(name.replace(remote_path, """")):
                remote_files.add(name)

        local_files = set(meta[""local_files""][remote_path])

        diff = remote_files - local_files
        LOG.warn(""New remote elements: {}"".format(len(diff)))
        if diff:
            # Check the current tfrecords, find the last one created
            # Fill the empty spaces in this one (rewriting it completely)
            # And create the other ones
            record_id = sum(
                len(glob(os.path.join(dataset_path, split, ""*.tfrecord"")))
                for split in SPLITS
            )

            last_tf_record = os.path.join(dataset_path, ""train"", ""1.tfrecord"")
            for split in SPLITS:
                last_path = os.path.join(dataset_path, split, f""{record_id}.tfrecord"")
                if os.path.exists(last_path):
                    last_tf_record = last_path
                    break

            examples = []
            if os.path.exists(last_tf_record):
                rows_in_last = rows_in_tfrecord(last_tf_record)
                if rows_in_last < ROWS_PER_RECORD:
                    # read all rows in the last tfrecord
                    # we're going to add the last one
                    # and recreate it
                    for row in tf.io.tf_record_iterator(last_tf_record):
                        example = tf.train.Example()
                        examples.append(example.ParseFromString(row))
                else:
                    record_id += 1

            # fetch new files and update the tfrecords
            tot_new_elements = len(diff)
            for idx, new_element in enumerate(diff):
                LOG.warn(f""File: {new_element}"")
                with tf.gfile.GFile(new_element, ""rb"") as fp:
                    img = fp.read()
                remote_meta = os.path.join(os.path.realpath(new_element), ""meta.json"")
                if not tf.gfile.Exists(remote_meta):
                    example_meta = {
                        ""count"": _int64_feature(1),
                        ""feature1"": _float_feature(1076),
                        ""feature2"": _float_feature(100),
                        ""f3"": _float_feature(1076),
                        ""f4"": _float_feature(100),
                        ""f5"": _bytes_feature(""mango"".encode(""UTF-8"")),
                        ""f7"": _bytes_feature(
                            ""banana"".encode(""UTF-8"")
                        ),
                    }
                else:
                    with tf.gfile.GFile(remote_meta, ""r"") as fp:
                        example_meta = json.loads(fp.read())

                # TODO: check if image is an old style image and convert to new format
                example_meta[""img""] = _bytes_feature(img)
                examples.append(
                    tf.train.Example(features=tf.train.Features(feature=example_meta))
                )

                if len(examples) == ROWS_PER_RECORD or idx == tot_new_elements - 1:
                    # write (always in the training dataset) and reset examples buffer
                    filename = os.path.join(
                        dataset_path, ""train"", f""{record_id}.tfrecord""
                    )
                    with tf.io.TFRecordWriter(filename) as writer:
                        for example in examples:
                            writer.write(example.SerializeToString())
                    # Reset example buffer
                    examples.clear()
                    # Increment tfrecord id
                    record_id += 1
                    LOG.warn(f""TFRecord: {filename} written"")
                    snapshot = tracemalloc.take_snapshot()
                    display_top(snapshot)
                    # break

```

Just use the `_gs2tfrecord` function. I'm using it to fetch data from Google Cloud Storage, but since I'm using `tf.gfile` a local path can be used too.

My guess is that leak can be somewhere in `tf.train.Example` or in `tf.train.Feature` since those are the only 2 functions I call in a loop.

**UPDATE**

I've created another function that just download the images from Google Cloud and store them locally. It goes out of memory in the same way.

Maybe the leak is in `tf.gfile`?

Here's the function
```python
def _gs2folder(
    gs_paths: List[str], patterns: List[Pattern], names: List[str], cache: str = CACHE
):
    """"""Fetch images from google cloud storage, conver to tf_record and
    properly handle cache.

    Args:
        gs_paths: list of paths on google cloud storage
        patterns: list of compiled regex to fiter the data on each gs_paths
        names: list of dataset name
        cache: local cache/dataset folder
    """"""
    assert len(gs_paths) == len(patterns) and len(patterns) == len(names)

    gcloud_auth = os.path.join(
        os.path.expanduser(""~""),
        "".config"",
        ""gcloud"",
        ""application_default_credentials.json"",
    )
    if not os.path.exists(gcloud_auth):
        subprocess.check_call([""gcloud"", ""auth"", ""application-default"", ""login""])

    for remote_path, pattern, name in zip(gs_paths, patterns, names):
        meta_file = os.path.join(cache, name, ""meta.json"")
        dataset_path = os.path.join(cache, name, ""original"")
        if not os.path.exists(cache):
            os.makedirs(dataset_path)
            meta = {""local_files"": {path: [] for path in gs_paths}}
        else:
            with open(meta_file, ""r"") as fp:
                meta = json.load(fp)

        remote_files = set()
        for file_name in list_dir(remote_path):
            if pattern.search(file_name.replace(remote_path, """")):
                remote_files.add(file_name)

        local_files = set(meta[""local_files""][remote_path])
        diff = remote_files - local_files
        LOG.warning(""New remote elements: {}"".format(len(diff)))
        last_id = len(local_files) + 1
        if diff:
            # fetch new files and store them
            tot_new_elements = len(diff)

            for idx, new_element in enumerate(diff):
                LOG.warning(f""File: {new_element}"")
                with tf.gfile.GFile(new_element, ""rb"") as fp:
                    img_bytes = fp.read()

                image = Image.open(io.BytesIO(img_bytes))
                image = np.array(image)
                if image.shape[-1] == 4:
                    # remove transparency
                    image = image[..., :3]
                remote_meta = os.path.join(os.path.realpath(new_element), ""meta.json"")
                if not tf.gfile.Exists(remote_meta):
                    example_meta = {
                        ""count"": 1
                    }
                else:
                    with tf.gfile.GFile(remote_meta, ""r"") as fp:
                        example_meta = json.loads(fp.read())

                image = Image.fromarray(image)
                # Just save the image file with its meta info
                image.save(os.path.join(dataset_path, f""{last_id}.png""))
                with open(os.path.join(dataset_path, f""{last_id}.json""), ""w"") as fp:
                    json.dump(example_meta, fp)
                last_id += 1

        meta[""local_files""][remote_path] = remote_files

    with open(meta, ""w"") as fp:
        json.dump(meta, fp)
```


**Other info / logs**

This is the output of a `display_top` invocation:

```
Top 10 lines
#1: python3.7/linecache.py:137: 246.9 KiB
    lines = fp.readlines()
#2: util/compat.py:80: 150.0 KiB
    return bytes_or_text.decode(encoding)
#3: datasets/floorplans.py:143: 64.2 KiB
    diff = remote_files - local_files
#4: datasets/floorplans.py:139: 32.0 KiB
    remote_files.add(name)
#5: internal/python_message.py:475: 21.6 KiB
    self._oneofs = {}
#6: internal/python_message.py:425: 15.5 KiB
    result = message_type._concrete_class()
#7: python3.7/tracemalloc.py:185: 13.4 KiB
    self._frames = tuple(reversed(frames))
#8: internal/python_message.py:472: 12.0 KiB
    self._fields = {}
#9: internal/python_message.py:1402: 10.3 KiB
    self._parent_message_weakref = weakref.proxy(parent_message)
#10: internal/python_message.py:1155: 5.1 KiB
    for field, value in list(self._fields.items()):  # dict can change size!
228 other: 104.0 KiB
Total allocated size: 675.1 KiB
```"
23732,Instance of 'CheckpointState' has no 'model_checkpoint_path' member,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Package based PIP installation
- TensorFlow version (use command below): 1.12.0
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
So I am trying to get checkpoint path, but I am unable to. Error:
Instance of 'CheckpointState' has no 'model_checkpoint_path' member

**Describe the expected behavior**
No error and able to get checkpoint path.

**Code to reproduce the issue**
```
def restore(s, save_instance):
    checkpoint = tensorflow.train.get_checkpoint_state(os.path.dirname(configuration.CHECKPOINT_PATH + ""/checkpoint""))
    if checkpoint and checkpoint.model_checkpoint_path:
        print(""Loading..."")
        save_instance.restore(s, checkpoint.model_checkpoint_path)
    else:
        print(""New Instance being created..."")
```

"
23731,Error when using tf.metrics.mean_iou() in eval_metric_ops of tf.estimator.EstimatorSpec(),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:_no_
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  _Linux Ubuntu 16.04.5 LTS_。
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:_no_
- **TensorFlow installed from (source or binary)**:_binary_
- **TensorFlow version (use command below)**:_1.11.0_
- **Python version**:_Python 3.5.2_
- **Bazel version (if compiling from source)**:_None_
- **GCC/Compiler version (if compiling from source)**:_None_
- **CUDA/cuDNN version**:_CUDA-9.1/cuDNN-7.0.5_
- **GPU model and memory**: _GTX 1080Ti 11GB, 2 same cards_
- **Exact command to reproduce**:_below_

### Supplementary material
I just pulled the tensorflow's offical docker image and RUN some `pip install ...` instructions.
The tag of the iamge is `1.11.0-devel-gpu-py3`.

### Describe the problem
When I use the code below

```
accuracy = tf.metrics.accuracy(valid_labels, valid_preds)
mean_iou = tf.metrics.mean_iou(valid_labels, valid_preds, params['num_classes'])
print(mean_iou)
eval_metrics = {'px_accuracy': accuracy, 'mean_iou': mean_iou}	
......
return tf.estimator.EstimatorSpec(
		mode=mode,
		predictions=None,
		loss=loss,
		train_op=train_op,
		eval_metric_ops=eval_metrics)
```

It will raise `TypeError`

```
TypeError: eval_metric_ops[mean_iou] must be Operation or Tensor, 
given: <tf.Variable 'mean_iou/AssignAddVariableOp' shape=(21, 21) dtype=float64>
```

And the result of  `print` is

```
(<tf.Tensor 'mean_iou/Select_1:0' shape=() dtype=float32>, 
<tf.Variable 'mean_iou/AssignAddVariableOp' shape=(21, 21) dtype=float64>)
```

I google the error and find the similar issue
https://github.com/tensorflow/tensorflow/issues/20418

So I modify the code
```
accuracy = tf.metrics.accuracy(valid_labels, valid_preds)
mean_iou = tf.metrics.mean_iou(valid_labels, valid_preds, params['num_classes'])
mean_iou = (mean_iou[0], tf.to_float(mean_iou[1]))
print(mean_iou)
eval_metrics = {'px_accuracy': accuracy, 'mean_iou': mean_iou}	
```
Then the code works normally.

So I think there may be something wrong in `tf.metrics.mean_iou`.

A strange thing is that the original code(no `tf.to_float()`) used to work normally......
After I add `distribution strategy`  to the config of the estimator and do some other changes , it broke down. 

"
23730,"Multi GPU, GPU to GPU communication stops","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: NVIDIA DOCKER Image
- **TensorFlow version (use command below)**: v1.11.0-0-gc19e29306c 1.11.0
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: NVIDIA DOCKER Image
- **GPU model and memory**: Titan Xp (8 pcs.)
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.



Hi, I'm trying to utilize 8 gpus for my GAN training. I use 8 pieces of Titan Xp. Detail of gpus are
```bash
GPU | Bus-Id
0 | 00000000:1B:00.0
1 | 00000000:1C:00.0
2 | 00000000:1D:00.0
3 | 00000000:1E:00.0
4 | 00000000:3D:00.0
5 | 00000000:3F:00.0
6 | 00000000:40:00.0
7 | 00000000:41:00.0
```

After I build model, I use this code to sync every device. Variables in each device has same prefix such as 'Tower_0', 'Tower_1', ..., 'Tower_7' and the 'Tower_0' is the main one.
```python
# Creating ops
def device_sync_op(prefix, main_idx):
    import re
    all_vars = tf.global_variables() + tf.local_variables()
    var_by_name = dict([(v.name, v) for v in all_vars])
    post_init_ops = []
    match_re = r""{}\d+"".format(prefix.format(""""))
    regex = re.compile(match_re)
    main_tower_name = prefix.format(main_idx)
    for v in all_vars:
        tower_name = regex.search(v.name)
        if tower_name is None:
            continue

        if main_tower_name == tower_name:
            # no need to copy to main tower
            continue

        tower_name = tower_name.group()

        copy_from = var_by_name.get(v.name.replace(tower_name, main_tower_name))
        if v.name == copy_from:
            continue

        if copy_from is not None:
            post_init_ops.append(v.assign(copy_from.read_value()))
        else:
            UserWarning(""Cannot find {} in the graph!"".format(v.name.replace(tower_name, main_tower_name)))

    return tf.group(*post_init_ops, name=""sync_variables_from_main_tower"")

# This is the code I use to sync every devices
sync_op = device_sync_op(""Tower_%d"", 0)
sess.run(sync_op)
```

This code works perfectly, if I utilize only 2 gpus from different PCI BUS group. As you can see above, GPU 0-3, GPU 4-7 have sequent PCI BUS addresses. So I call each group as PCI BUS group for easy explanation. In other words, if I use gpus (0, 4), (0,5), (0, 6), (0, 7), (1, 4), ..., (3, 7), the code works.
However, if I try to use 2 gpus from same group such as (0, 1), (0, 2), (0, 3), ..., (6, 7), it just doesn't response. I have to kill the process using **kill -9** or reboot computer at the worst. I cannot utilize more than 2 gpus for my model because of this problem.

Is there any options I have to enable or drivers to install more to solve this problem? Or is it impossible to make each device communicate each other?

Waiting for your wise solutions."
23729,pybullet.error: createCollisionShape failed.,"It is on Ubuntu 16.04LTS, and it shows this error:

![image](https://user-images.githubusercontent.com/15700681/48466407-c1a1bd00-e820-11e8-80f4-695690415552.png)
"
23728,Unable to import tf.keras.optimizers,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6
- Bazel version (if compiling from source): None
- GCC/Compiler version (if compiling from source): None
- CUDA/cuDNN version: 9.0, 7.0
- GPU model and memory: GTX 1080ti 11gb

**Describe the current behavior**
Cannot import keras.optimizers when done like this
```
from tensorflow.keras.optimizers import *

ImportError                               Traceback (most recent call last)
<ipython-input-17-ea1fa27bfa23> in <module>()
----> 1 from tensorflow.keras.optimizers import *

~/tf/lib/python3.6/site-packages/tensorflow/keras/__init__.py in <module>()
     18 from tensorflow.keras import estimator
     19 from tensorflow.keras import initializers
---> 20 from tensorflow.keras import layers
     21 from tensorflow.keras import losses
     22 from tensorflow.keras import metrics

~/tf/lib/python3.6/site-packages/tensorflow/keras/layers/__init__.py in <module>()
      7 
      8 from tensorflow.python.keras import Input
----> 9 from tensorflow.python.keras.applications.densenet import Activation
     10 from tensorflow.python.keras.applications.densenet import AveragePooling2D
     11 from tensorflow.python.keras.applications.densenet import AveragePooling2D as AvgPool2D

ImportError: cannot import name 'Activation'
```
but this seems to work fine
```
import tensorflow as tf
optim = tf.keras.optimizers.Adam()
```
what weird is that even this works
```
from tensorflow.python.keras.optimizers import *
```"
23727,how to sample tfrecord data with probability like experience replay? does tfrecord support this?,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
23726,Win 10: Failed to load native runtime - DLL load failed,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10
- TensorFlow installed from (source or binary): pip install tensorflow
- TensorFlow version: 1.12.0
- Python version: 3.6.5
- Installed using virtualenv? pip? conda?:
- CUDA/cuDNN version: 10.0 / 7
- GPU model and memory: Geforce 1050 Ti 8gb



**Describe the problem**

I am unable to import tensorflow. It gives the error below.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. Updated my GPU drivers
2. Installed CUDA 10.0
3. Downloaded the cuDNN tool
4. Added both CUDA and cuDNN bin to PATH
5. pip installed tensorflow-gpu
6. Tried to import

7. did pip uninstall tensorflow-gpu
8. tried pip install tensorflow-gpu==1.10.0 (same error)

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Error Shown:

```
C:\Users\Mom2\AppData\Local\Programs\Python\Python36\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Traceback (most recent call last):
  File ""C:\Users\Mom2\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Mom2\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Mom2\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Mom2\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Mom2\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""ml.py"", line 2, in <module>
    from tensorflow import keras
  File ""C:\Users\Mom2\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Mom2\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Mom2\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Mom2\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Mom2\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Mom2\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Mom2\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Mom2\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```
"
23725,Sparse Precision Matrix implemetations (for gaussian processes or GMRFs),"Are there any plans in the near future to implement sparse precision matrix computations for TensorFlow? My main use case when I'll be using this is for models like a Gaussian Process where a sparse precision matrix can be fed in as a Gaussian Markov Random Field to evaluate the GP (the sparse representation of an AR(1) GP is only tri-diagonal whereas the covariance matrix is dense).

Being able to evaluate on sparse matrices would be super helpful in computation speeds I think. Thoughts?



**System information**
- TensorFlow version (you are using): 1.12.0
- Are you willing to contribute it (Yes/No): Yes


**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**
Only additions I hope

**Who will benefit with this feature?**
Statistical modelers with high dimensional sparse data

**Any Other info.**
"
23722,tf.keras - Saving model ignores layers that aren't fed into other layers,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**
- TensorFlow installed from (source or binary): **Binary**
- TensorFlow version (use command below): **b'v1.11.0-rc2-4-gc19e29306c' 1.11.0**
- Python version: **3.6.7**
- Bazel version (if compiling from source): **N/A**
- GCC/Compiler version (if compiling from source): **N/A**
- CUDA/cuDNN version: **N/A**
- GPU model and memory: **N/A**

**Describe the current behavior**
When I save and reload a model that has multiple inputs, but some aren't fed into any layers (they are used in a custom loss function), not all inputs are created in the new model.

Opening the HDF File of the saved model shows that the input layers aren't saved in the model configuration.

**Describe the expected behavior**
All layers should be correctly created

**Code to reproduce the issue**
```python
import tensorflow.keras as keras
import numpy as np


def custom_loss(loss_input):
    def loss(y_true, y_pred):
        return y_true * y_pred * loss_input
    return loss


# Create simple model
input_layer = keras.Input(shape=(1,))
loss_input_layer = keras.Input(shape=(1,))

output = keras.layers.Dense(1)(input_layer)

model = keras.Model(inputs=[input_layer, loss_input_layer], outputs=[output])

model.compile(optimizer=keras.optimizers.Adam(lr=1e4), loss=[custom_loss(loss_input_layer)])

# Train the model with some example data
input_data = np.random.random_sample(512)
loss_data = np.random.random_sample(512)
output_data = input_data * -1

model.train_on_batch(x=[input_data, loss_data], y=[output_data])

# Save and load model
model.save(""test.h5"")
model = keras.models.load_model(""test.h5"", custom_objects={""loss"": custom_loss(loss_input_layer)})

# Test model
model.predict(x=[[np.random.random_sample(1)], [np.random.random_sample(1)]])
```

**Other info / logs**
```
Traceback (most recent call last):
  File ""C:/Users/user/test/testTensorboard.py"", line 31, in <module>
    model.predict(x=[[np.random.random_sample(1)], [np.random.random_sample(1)]])
  File ""C:\ProgramData\Miniconda3\envs\SolitairePPO\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1752, in predict
    x, check_steps=True, steps_name='steps', steps=steps)
  File ""C:\ProgramData\Miniconda3\envs\SolitairePPO\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 993, in _standardize_user_data
    class_weight, batch_size)
  File ""C:\ProgramData\Miniconda3\envs\SolitairePPO\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1112, in _standardize_weights
    exception_prefix='input')
  File ""C:\ProgramData\Miniconda3\envs\SolitairePPO\lib\site-packages\tensorflow\python\keras\engine\training_utils.py"", line 286, in standardize_input_data
    str(len(data)) + ' arrays: ' + str(data)[:200] + '...')
ValueError: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 2 arrays: [array([[0.08552315]]), array([[0.11069975]])]...
```
"
23721,Build error while building contrib/bigtable,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: r1.12 and master (7561099fb65c18bd091751b60fc45550fc5d4805)
- Python version: 3.6.7
- Installed using virtualenv? pip? conda?: pyenv
- Bazel version (if compiling from source): 0.19.1
- GCC/Compiler version (if compiling from source): gcc-6 (Ubuntu 6.4.0-17ubuntu1) 6.4.0 20180424
- CUDA/cuDNN version: 10.0 / 7.4
- GPU model and memory: GTX 1080ti

**Describe the problem**

Error message: ERROR: /home/jxstanford/src/tensorflow/tensorflow/contrib/bigtable/BUILD:53:1: Linking of rule '//tensorflow/contrib/bigtable:p
ython/ops/_bigtable.so' failed (Exit 1) gcc-6 failed: error executing command /usr/bin/gcc-6 -shared -o bazel-out/k8-opt/bin/tensorflow/contri
b/bigtable/python/ops/_bigtable.so ... (remaining 64 argument(s) skipped)

**Provide the exact sequence of commands / steps that you executed before running into the problem**


`git checkout r1.12`
`bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/lite/...`


**Any other info / logs**

Build output attached.
(https://github.com/tensorflow/tensorflow/files/2577363/TF_BUILD_OUTPUT.txt)"
23720,"tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected begin[1] in [0, 8], but got -2","**Error**
```
Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1139, in _do_call
    return fn(*args)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1121, in _run_fn
    status, run_metadata)
  File ""C:\ProgramData\Anaconda3\lib\contextlib.py"", line 88, in __exit__
    next(self.gen)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected begin[1] in [0, 8], but got -2
	 [[Node: Slice = Slice[Index=DT_INT32, T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](up_sampling2d/ResizeNearestNeighbor, Slice/begin, Slice/size)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""E:/VisionLearning/Segmentation/train.py"", line 71, in <module>
    train(epochs,num_steps)
  File ""E:/VisionLearning/Segmentation/train.py"", line 47, in train
    sess.run(optimizer, feed_dict)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 789, in run
    run_metadata_ptr)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 997, in _run
    feed_dict_string, options, run_metadata)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1132, in _do_run
    target_list, options, run_metadata)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1152, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected begin[1] in [0, 8], but got -2
	 [[Node: Slice = Slice[Index=DT_INT32, T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](up_sampling2d/ResizeNearestNeighbor, Slice/begin, Slice/size)]]

Caused by op 'Slice', defined at:
  File ""E:/VisionLearning/Segmentation/train.py"", line 71, in <module>
    train(epochs,num_steps)
  File ""E:/VisionLearning/Segmentation/train.py"", line 28, in train
    logits = build_model(x, 0.5, 128)
  File ""E:\VisionLearning\Segmentation\Model.py"", line 153, in build_model
    up1_0 = crop_and_concat(layers.UpSampling2D(size=(2,2))(conv5), conv4_1)
  File ""E:\VisionLearning\Segmentation\Model.py"", line 22, in crop_and_concat
    x1_crop = tf.slice(x1, offsets, size)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 547, in slice
    return gen_array_ops._slice(input_, begin, size, name=name)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 2896, in _slice
    name=name)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 2506, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1269, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): Expected begin[1] in [0, 8], but got -2
	 [[Node: Slice = Slice[Index=DT_INT32, T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](up_sampling2d/ResizeNearestNeighbor, Slice/begin, Slice/size)]]


Process finished with exit code 1
```


**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes

```
def crop_and_concat(x1,x2):

    x1_shape = tf.shape(x1)
    x2_shape = tf.shape(x2)

    x2_shape1 = x2.get_shape()
    x1_shape1 = x1.get_shape()

    # offsets for the top left corner of the crop
    offsets = [0, (x1_shape[1] - x2_shape[1]) // 2, (x1_shape[2] - x2_shape[2]) // 2, 0]
    print(""offsets"",offsets)
    print(""x1"",x1)

    #offsets = [0, (int(x1_shape1[1]) - int(x2_shape1[1])) // 2, (int(x1_shape1[2]) - int(x2_shape1[2])) // 2, 0]
    size = [-1, int(x2_shape1[1]), int(x2_shape1[2]), -1]
    print(""size"",size)
    #size = [-1, x2_shape[1], x2_shape[2], -1]
    x1_crop = tf.slice(x1, offsets, size)
    print(""x1 crop"",x1_crop)
    return tf.concat([x1_crop, x2], 3)

def build_model(x, keep_prob, batch_size):
    print(x.get_shape())
    conv1 = tf.layers.conv2d(
        inputs= x,
        filters=32,
        kernel_size=(3,3),
        padding='same',
        activation = tf.nn.relu
    )#256
    print(""conv1"", conv1)
    conv1_2 = tf.layers.conv2d(
        inputs=conv1,
        filters=32,
        kernel_size=(3,3),
        padding='same',
        activation=tf.nn.relu
    )#252
    print(""conv1"", conv1_2)
    pool1 = tf.layers.max_pooling2d(
        inputs=conv1_2,
        pool_size=(2,2),
        padding='same',
        strides = 2
    )#126
    print(""pool1"", pool1)
    conv2 = tf.layers.conv2d(
        inputs=pool1,
        filters=64,
        kernel_size=(3, 3),
        padding='same',
        activation=tf.nn.relu
    )#124
    print(""conv2"", conv2.get_shape())
    conv2 = tf.layers.conv2d(
        inputs=conv2,
        filters=64,
        kernel_size=3,
        activation=tf.nn.relu
    )#122
    print(""conv2"", conv2.get_shape())
    pool2 = tf.layers.max_pooling2d(
        inputs=conv2,
        pool_size=2,
        strides=2
    )#61
    print(""pool2"", pool2.get_shape())
    conv3 = tf.layers.conv2d(
        inputs=pool2,
        filters=128,
        kernel_size=(3, 3),
        padding='same',
        activation=tf.nn.relu
    )#59
    print(""conv3"", conv3.get_shape())
    conv3 = tf.layers.conv2d(
        inputs=conv3,
        filters=128,
        kernel_size=3,
        activation=tf.nn.relu
    )#57
    print(""conv3"", conv3.get_shape())
    pool3 = tf.layers.max_pooling2d(
        inputs=conv3,
        pool_size=2,
        strides=2
    )
    print(""pool3"", pool3.get_shape())
    conv4 = tf.layers.conv2d(
        inputs=pool3,
        filters=256,
        kernel_size=(3, 3),
        padding='same',
        activation=tf.nn.relu
    )
    print(""conv4"", conv4.get_shape())
    conv4 = tf.layers.conv2d(
        inputs=conv4,
        filters=256,
        kernel_size=3,
        activation=tf.nn.relu
    )
    print(""conv4"", conv4.get_shape())
    pool4 = tf.layers.max_pooling2d(
        inputs=conv4,
        pool_size=(2,2),
        strides=2
    )
    print(""pool4"", pool4.get_shape())

    conv4_1 = tf.layers.conv2d(
        inputs=pool4,
        filters=512,
        kernel_size=(3, 3),
        padding='same',
        activation=tf.nn.relu
    )
    print(""conv41"", conv4_1.get_shape())
    conv4_1 = tf.layers.conv2d(
        inputs=conv4_1,
        filters=512,
        kernel_size=3,
        activation=tf.nn.relu
    )
    print(""conv41"", conv4_1.get_shape())
    pool4_1 = tf.layers.max_pooling2d(
        inputs=conv4_1,
        pool_size=(2, 2),
        strides=2
    )
    print(""pool41"", pool4_1.get_shape())
    conv5 = tf.layers.conv2d(
        inputs=pool4_1,
        filters=1024,
        kernel_size=(3, 3),
        padding='same',
        activation=tf.nn.relu
    )
    print(""conv5"", conv5.get_shape())
    conv5 = tf.layers.conv2d(
        inputs=conv5,
        filters=1024,
        kernel_size=(3,3),
        activation=tf.nn.relu
    )
    print(""conv5"", conv5.get_shape())

    #up1_0 = layers.concatenate([layers.UpSampling2D(size=(2,2))(conv5),conv4_1])
    up1_0 = crop_and_concat(layers.UpSampling2D(size=(2,2))(conv5), conv4_1)

    print(""up10"", up1_0.get_shape())
    conv6 = tf.layers.conv2d(
        inputs=up1_0,
        filters=512,
        kernel_size=(3, 3),
        padding='same',
        activation=tf.nn.relu
    )
    print(""conv6"", conv6.get_shape())
    conv6 = tf.layers.conv2d(
        inputs=conv6,
        filters=512,
        kernel_size=(3, 3),
        padding='same',
        activation=tf.nn.relu
    )
    print(""conv6"", conv6.get_shape())
    #up1 = layers.concatenate([layers.UpSampling2D(size=(2, 2))(conv6), conv4])
    up1 = crop_and_concat(layers.UpSampling2D(size=(2,2))(conv6), conv4)
    print(""up1"", up1.get_shape())
    conv6_1 = tf.layers.conv2d(
        inputs=up1,
        filters=256,
        kernel_size=(3, 3),
        padding='same',
        activation=tf.nn.relu
    )
    print(""conv61"", conv6_1.get_shape())
    conv6_1 = tf.layers.conv2d(
        inputs=conv6_1,
        filters=256,
        kernel_size=(3, 3),
        padding='same',
        activation=tf.nn.relu
    )
    print(""conv61"", conv6_1.get_shape())
    #up2  = layers.concatenate([layers.UpSampling2D(size=2)(conv6_1),conv3])
    up2 = crop_and_concat(layers.UpSampling2D(size=(2,2))(conv6_1), conv3)
    print(""up2"", up2.get_shape())
    conv7 = tf.layers.conv2d(
        inputs=up2,
        filters=128,
        kernel_size=(3, 3),
        padding='same',
        activation=tf.nn.relu
    )
    print(""conv7"", conv7.get_shape())
    conv7 = tf.layers.conv2d(
        inputs=conv7,
        filters=128,
        kernel_size=(3, 3),
        padding='same',
        activation=tf.nn.relu
    )
    print(""conv7"", conv7.get_shape())
    #up3 =  layers.concatenate([layers.UpSampling2D(size=2)(conv7),conv2])
    up3 = crop_and_concat(layers.UpSampling2D(size=(2,2))(conv7), conv2)
    print(""up3"", up3.get_shape())
    conv8 = tf.layers.conv2d(
        inputs=up3,
        filters=64,
        kernel_size=(3, 3),
        padding='same',
        activation=tf.nn.relu
    )
    print(""conv8"", conv8.get_shape())
    conv8 = tf.layers.conv2d(
        inputs=conv8,
        filters=64,
        kernel_size=(3, 3),
        padding='same',
        activation=tf.nn.relu
    )
    print(""conv8"",conv8.get_shape())
    #up4 = layers.concatenate([layers.UpSampling2D(size=2)(conv8), conv1])
    up4 = crop_and_concat(layers.UpSampling2D(size=(4,4))(conv8), conv1)
    print(""up4"",up4.get_shape())
    conv9 = tf.layers.conv2d(
        inputs=up4,
        filters=32,
        kernel_size=(3, 3),
        padding='same',
        activation=tf.nn.relu
    )
    print(""conv91"",conv9.get_shape())
    conv9 = tf.layers.conv2d(
        inputs=conv9,
        filters=32,
        kernel_size=(3, 3),
        padding='same',
        activation=tf.nn.relu
    )
    print(""conv9"",conv9.get_shape())
    conv10 = tf.layers.conv2d(
        inputs=conv9,
        filters=1,
        kernel_size=1,
        activation= tf.nn.sigmoid
    )
    print(""conv10"",conv10.get_shape())
    return conv10
```

```
def train(epochs, num_steps):
    dir_images = ""Dataset/JPEGImages/480p""
    dir_annotations = ""Dataset/Annotations/480p""

    classes, classidx = find_classes(dir_images)
    datas = make_dataset(dir_images, dir_annotations)
    x = tf.placeholder(tf.float32, shape=[None, 256, 256, 3], name='X') #Need to define the shape of x
    y = tf.placeholder(tf.float32, shape=[None, 256,256, 1], name='Y')
    logits = build_model(x, 0.5, 128)
    model = tf.identity(logits, name='logits')
    Cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y))
    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(Cost)

    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))

    init = tf.global_variables_initializer()
    sess = tf.InteractiveSession()
    # Initialize all variables
    sess.run(init)
    saver = tf.train.Saver()
    with tf.Session() as sess:
        # variables need to be initialized before any sess.run() calls
        tf.global_variables_initializer().run()

        for X_batch, y_batch in generator(datas,32):
            feed_dict = {x: X_batch, y: y_batch}
            sess.run(optimizer, feed_dict)
```

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA
- TensorFlow installed from (source or binary):Source
- TensorFlow version (use command below):1.2.1
- Python version:3.6.4
- Bazel version (if compiling from source):NA
- GCC/Compiler version (if compiling from source):NA
- CUDA/cuDNN version:NA
- GPU model and memory:NA


**Describe the current behavior** Getting this error when I am tring to start the training in images.

**Describe the expected behavior**
It should start training
**Code to reproduce the issue**
Get Davis dataset and put it in respective folders

"
23719,Analysis of target '@local_config_cc//:cc-compiler-x64_windows' failed;,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source, both nightly and 1.12 tried
- TensorFlow version: 1.12 and nightly
- Python version: 3.6
- Installed using virtualenv? pip? conda?: conda environment
- Bazel version (if compiling from source): 0.19.1
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version: 9.0 and 7.4
- GPU model and memory: Geforce 770 4gb



**Describe the problem**
I'm trying to compile from source and I keep running into the same problem regardless of what release i try to build. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I followed the guide https://www.tensorflow.org/install/source_windows promptly. Below is what i get when i try to make the package builder by running 'bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package'

**Any other info / logs**

(venv) B:\tensorflow>python ./configure.py
WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
b:\tensorflow/tools/bazel.rc
nul
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.19.1 installed.
Please specify the location of python. [Default is B:\Program Files\Anaconda\envs\venv\python.exe]:


Found possible Python library paths:
  B:\Program Files\Anaconda\envs\venv\lib\site-packages
Please input the desired Python library path to use.  Default is [B:\Program Files\Anaconda\envs\venv\lib\site-packages]

Do you wish to build TensorFlow with XLA JIT support? [y/N]:
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]:


Please specify the location where CUDA 9.0 toolkit is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0]:


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]:


Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0]:


Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]: 3.0


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:
Eigen strong inline overridden.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=gdr            # Build with GDR support.
        --config=verbs          # Build with libverbs support.
        --config=ngraph         # Build with Intel nGraph support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=noignite       # Disable Apacha Ignite support.
        --config=nokafka        # Disable Apache Kafka support.

(venv) B:\tensorflow>bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
b:\tensorflow/.bazelrc
b:\tensorflow/tools/bazel.rc
Starting local Bazel server and connecting to it...
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
WARNING: Option 'experimental_shortened_obj_file_path' is deprecated
DEBUG: C:/users/nikolaj/_bazel_nikolaj/rm3fg6i6/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest Visual C++ installed.
DEBUG: C:/users/nikolaj/_bazel_nikolaj/rm3fg6i6/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variables, eg. VS140COMNTOOLS
DEBUG: C:/users/nikolaj/_bazel_nikolaj/rm3fg6i6/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: Visual C++ build tools found at C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\
DEBUG: C:/users/nikolaj/_bazel_nikolaj/rm3fg6i6/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest Visual C++ installed.
DEBUG: C:/users/nikolaj/_bazel_nikolaj/rm3fg6i6/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variables, eg. VS140COMNTOOLS
DEBUG: C:/users/nikolaj/_bazel_nikolaj/rm3fg6i6/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: Visual C++ build tools found at C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\
DEBUG: C:/users/nikolaj/_bazel_nikolaj/rm3fg6i6/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest Visual C++ installed.
DEBUG: C:/users/nikolaj/_bazel_nikolaj/rm3fg6i6/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variables, eg. VS140COMNTOOLS
DEBUG: C:/users/nikolaj/_bazel_nikolaj/rm3fg6i6/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: Visual C++ build tools found at C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\
ERROR: C:/users/nikolaj/_bazel_nikolaj/rm3fg6i6/external/local_config_cc/BUILD:106:1: in cc_toolchain rule @local_config_cc//:cc-compiler-x64_windows: Error while selecting cc_toolchain: Toolchain identifier 'msvc_x64' was not found, valid identifiers are [local_linux, local_darwin, local_windows]
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis of target '@local_config_cc//:cc-compiler-x64_windows' failed; build aborted
INFO: Elapsed time: 29.408s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (24 packages loaded, 55 targets configured)
    currently loading: tensorflow/python

"
23718, Failed to load the native TensorFlow runtime.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac os
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): using ""pip install tensorflow""
- TensorFlow version:
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:No GPU



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Installed tensorflow using pip install
When i tried to load import tensorflow as tf, it throws a log

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

--------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>()
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in <module>()
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

~/anaconda3/lib/python3.6/imp.py in load_module(name, file, filename, details)
    242         else:
--> 243             return load_dynamic(name, filename, file)
    244     elif type_ == PKG_DIRECTORY:

~/anaconda3/lib/python3.6/imp.py in load_dynamic(name, path, file)
    342             name=name, loader=loader, origin=path)
--> 343         return _load(spec)
    344 

ImportError: dlopen(/Users/pritee/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _clock_gettime
  Referenced from: /Users/pritee/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so (which was built for Mac OS X 10.12)
  Expected in: /usr/lib/libSystem.B.dylib
 in /Users/pritee/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-3-41389fad42b5> in <module>()
----> 1 import tensorflow as tf

~/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py in <module>()
     22 
     23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     25 
     26 try:

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py in <module>()
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 from tensorflow.python.tools import component_api_helper

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>()
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""/Users/pritee/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/Users/pritee/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/Users/pritee/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/Users/pritee/anaconda3/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/Users/pritee/anaconda3/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/Users/pritee/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _clock_gettime
  Referenced from: /Users/pritee/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so (which was built for Mac OS X 10.12)
  Expected in: /usr/lib/libSystem.B.dylib
 in /Users/pritee/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
23717,Tensorflow Object detection -> How to get class name from trained model (.pb file),"Guys,

Tensorflow Object detection -> How to get class name from trained model (.pb file)
Trained with four class, i want to know how to get class name based on input give.
I checked with existing trained model and can able to get index value , but not able to get index/class name for my own trained mode

Any help appreciated !

"
23716,aarch64 support,"
**System information**
- TensorFlow version (you are using): 1.12.0
- Are you willing to contribute it (Yes/No): Yes
-Target device - NVIDIA DrivePX2 and NVIDIA JetsonTX2



**Describe the feature and the current behavior/state.**
Support aarch64 target architectures.

**Will this change the current api? How?**
NO, the current apis are not changed, this just enables bazel to find aarch64 tool chain during a native compilation on the aarch target.

**Who will benefit with this feature?**
People using NVIDIA Drive/Jetson platforms containing aarch64 devices.

**Any Other info.**

Please add the following lines in the files to enable to support
1. 
Add -
 ```
static int TryToReadNumaNode(const string &pci_bus_id, int device_ordinal) {
-#if defined(__APPLE__)
+#ifdef __aarch64__
+    LOG(INFO) << ""ARM64 does not support NUMA - returning NUMA node zero"";
+    return 0;
+#elif defined(__APPLE__)
   LOG(INFO) << ""OS X does not support NUMA - returning NUMA node zero"";
   return 0;N
 #elif defined(PLATFORM_WINDOWS)
```
in the file -  /tensorflow/stream_executor/cuda/cuda_gpu_executor.cc 

2. https://github.com/tensorflow/tensorflow/issues/21852
add
```
default_toolchain {
  cpu: ""aarch64""
  toolchain_identifier: ""local_linux""
}
```
in the file - ./third_party/gpus/crosstool/CROSSTOOL.tpl
"
23715,Cudnn version of the problem,"Loaded runtime CuDNN library: 7.1.2 but source was compiled with: 7.3.1.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.

 cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2
#define CUDNN_MAJOR 7
#define CUDNN_MINOR 4
#define CUDNN_PATCHLEVEL 1
--
#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)

#include ""driver_types.h""

No 7.1.2 installed.

 tf-nightly-gpu  1.13.0.dev20181110

 tensorflow-gpu  1.12.0rc2



"
23714,How to remove NMS ops in mobilenet+ssd model?,"I use mobilenet+ssd for detection with android NN. But  TFLite is not supported for NMS such customized ops.  So I want to remove NMS ops in the .pb model. Is there any method for that?
Or any case of translating mobilnet+ssd to TFLite with android NN?"
23713,"Audio recognition on 8,000 samples per second audio data, label_wav.py","I followed the Simple Audio Recognition tutorial given on tensorflow website. Everything went good. I downloaded the google speech command data set, trained a model using default setting, got 87% test accuracy, and then tested on some other simple audio clips using  label_wav.py code given as tutorial.

But I need to decode audio files of 8,000 sps. The results of this model is not good on 8000 sps audios. reading from tensorflow audio recognition tutorial I got to know that I need to use 8000 sps data for training.

I downsampled all audio files in data set and trained the model again with this downsampled data using the following command:

python tensorflow/examples/speech_commands/train.py --data_url= --data_dir=C:\tmp\speech_dataset --sample_rate=8000

The accuracy of this model is good i.e. Final test accuracy = 87.2% . But the problem is when I am testing (labeling) any audio file using this model and by using the code label_wav.py I am not getting the right results even for the audio files from the data set.

I am not getting where the problem is? I searched alot but didn't find any solution. I also read the complete tensorflow tutorial on audio recognition.

Kindly Help me!!!! How to decode 8k sps audio clips using the model trained on 8k sps audio data and by using the given label_wav.py Python code?"
23712,Keras with Tensorflow backend stooped working ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `CXXABI_1.3.9' not found,"I am running Keras with Tensorflow backend and encountering a problem like 

```
/home/sbhakat/miniconda2/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
Traceback (most recent call last):
  File ""script.py"", line 1, in <module>
    from keras.callbacks import TensorBoard, ModelCheckpoint
  File ""/home/sbhakat/miniconda2/envs/py36/lib/python3.6/site-packages/keras/__init__.py"", line 3, in <module>
    from . import utils
  File ""/home/sbhakat/miniconda2/envs/py36/lib/python3.6/site-packages/keras/utils/__init__.py"", line 27, in <module>
    from .multi_gpu_utils import multi_gpu_model
  File ""/home/sbhakat/miniconda2/envs/py36/lib/python3.6/site-packages/keras/utils/multi_gpu_utils.py"", line 7, in <module>
    from ..layers.merge import concatenate
  File ""/home/sbhakat/miniconda2/envs/py36/lib/python3.6/site-packages/keras/layers/__init__.py"", line 4, in <module>
    from ..engine.base_layer import Layer
  File ""/home/sbhakat/miniconda2/envs/py36/lib/python3.6/site-packages/keras/engine/__init__.py"", line 8, in <module>
    from .training import Model
  File ""/home/sbhakat/miniconda2/envs/py36/lib/python3.6/site-packages/keras/engine/training.py"", line 21, in <module>
    from . import training_arrays
  File ""/home/sbhakat/miniconda2/envs/py36/lib/python3.6/site-packages/keras/engine/training_arrays.py"", line 8, in <module>
    from scipy.sparse import issparse
  File ""/home/sbhakat/miniconda2/envs/py36/lib/python3.6/site-packages/scipy/sparse/__init__.py"", line 229, in <module>
    from .csr import *
  File ""/home/sbhakat/miniconda2/envs/py36/lib/python3.6/site-packages/scipy/sparse/csr.py"", line 15, in <module>
    from ._sparsetools import csr_tocsc, csr_tobsr, csr_count_blocks, \
ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `CXXABI_1.3.9' not found (required by /home/sbhakat/miniconda2/envs/py36/lib/python3.6/site-packages/scipy/sparse/_sparsetools.cpython-36m-x86_64-linux-gnu.so)
```

I think this problem was quite similar to https://github.com/tensorflow/tensorflow/issues/5017 hence I tried the proposed method which is following

`cp /usr/lib/x86_64-linux-gnu/libstdc++.so.6 /home/sbhakat/miniconda2/envs/py36/lib/`

But now it seems the issue is similar to https://github.com/ContinuumIO/anaconda-issues/issues/5191

Any quick help to fix this. Sorry I am new with this."
23711,please give an introduction of using quantize-aware training with tf.estimator,"How to use  `tf.contrib.quantize.*` with tf.estimator or how to using  post_quantize_training model with inference type of int8.  Demo is perferred, thanks."
23710,the performance of tensorflow distributed,"**Describe the current behavior**
I have trained a speech recognition network using tensorflow both on multi-gpu single machine version(multi-gpu) and multi-gpu multi-machine(distributed) version. the training speed was ok in multi-GPU but the speed on distributed is slow and accuracy is not same as multi-GPU version at same steps. I watched the timeline and found RecvTensor is waste so long time. i‘m wondering how to improve the performance of distributed training.
![c2e456a6 17eda71d 1b93e65b](https://user-images.githubusercontent.com/16797858/48396541-c0f02480-e755-11e8-97fd-8dad532956fa.png)

**log info**
the start script for each worker (3 workers in total and 3 ps) is:
```
CUDA_VISIBLE_DEVICES='' nohup python distributed.py --ps_hosts=gpu47-hca:2220,gpu42-hca:2221,gpu35-hca:2222 --worker_hosts=gpu47-hca:3330,gpu42-hca:3331,gpu35-hca:3332 --job_name=ps --task_index=0 &

nohup python distributed.py --ps_hosts=gpu47-hca:2220,gpu42-hca:2221,gpu35-hca:2222 --worker_hosts=gpu47-hca:3330,gpu42-hca:3331,gpu35-hca:3332 --job_name=worker --task_index=0 --gpu_num=4 --learning_rate=0.00001 --batch_size=128&
```
the training log:
multi-gpu:
```
INFO:tensorflow:Step #312: rate 0.000010, accuracy 0.08653%, cross entropy 7.359044(192.9 examples/sec; 0.166 sec/batch)
INFO:tensorflow:Step #313: rate 0.000010, accuracy 0.09099%, cross entropy 7.384690(218.1 examples/sec; 0.147 sec/batch)
INFO:tensorflow:Step #314: rate 0.000010, accuracy 0.09412%, cross entropy 7.325777(241.5 examples/sec; 0.132 sec/batch)
INFO:tensorflow:Step #315: rate 0.000010, accuracy 0.08608%, cross entropy 7.381987(225.8 examples/sec; 0.142 sec/batch)
```
distributed:
```
INFO:tensorflow:Step #768: learning_rate:0.00001, accuracy 0.49589%, cross entropy 6.968968 (62.8 examples/sec; 0.509 sec/batch)
INFO:tensorflow:Step #769: learning_rate:0.00001, accuracy 0.60303%, cross entropy 7.082638 (87.0 examples/sec; 0.368 sec/batch)
INFO:tensorflow:Step #770: learning_rate:0.00001, accuracy 0.77971%, cross entropy 7.041758 (54.9 examples/sec; 0.583 sec/batch)
INFO:tensorflow:Step #771: learning_rate:0.00001, accuracy 1.00002%, cross entropy 6.996723 (41.4 examples/sec; 0.773 sec/batch)
```


**System information**
- Have I written custom code: yes    (code is [here](https://github.com/dingevin/distributed-training/blob/master/distributed.py))
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS release 6.8
- TensorFlow installed from (source or binary): binary 
- TensorFlow version (use command below): 1.10.1
- Python version: 2.7.13
- GPU model and memory: TITAN X 12G
- Network bandwidt: 125M/s

**What I have tried**
1. the **RecvTensor** is so long time , so i increase the number of ps, the speed more fast than before but accuracy don't improve. i wonder how to confirm the number of ps and worker？if i increate the numer of ps the speed can be faster?  my network is 1CNN+5LSTM+1Fully Connected Layer。
2. the accuracy was 10% in multi-gpu 4000 steps; so i set 2000 steps to each worer(2 ps 2 worker), but the accuracy is 5%， it looks don't sync updata parameter? Is there something wrong with my [code](https://github.com/dingevin/distributed-training/blob/master/distributed.py)?"
23709,image_ocr.py running error on tf.keras,"I try to run image_ocr.py on tf.keras.
image_ocr.py is the example code from keras
https://github.com/keras-team/keras/blob/master/examples/image_ocr.py

>>> import tensorflow as tf
>>> tf.__version__
'1.11.0'
>>> tf.keras.__version__
'2.1.6-tf'
>>>

I only modify some imports to use tf.keras.

import tensorflow as tf
from tensorflow.keras import backend as K
from tensorflow.keras.layers import Conv2D, MaxPooling2D
...

The error code
  File ""D:\Anaconda2\envs\tf-cpu\lib\site-packages\tensorflow\python\keras\utils\data_utils.py"", line 689, in _data_generator_task
    generator_output = next(self._generator)
  File ""image_ocr.py"", line 312, in next_train
    print(self.cur_train_index)
AttributeError: 'TextImageGenerator' object has no attribute 'cur_train_index'

The problem is occurs in model.fit_generator().
I found next_train() and build_word_list() run in different python thread.

the next_train() is called before the build_word_list(). So the initializeation of TextImageGenerator is incorrect.

I also try to set  ""workers=0"" to use single thread. 
    model.fit_generator(
        generator=img_gen.next_train(),
        steps_per_epoch=(words_per_epoch - val_words) // minibatch_size,
        epochs=stop_epoch,
        validation_data=img_gen.next_val(),
        validation_steps=val_words // minibatch_size,
        callbacks=[viz_cb, img_gen],
        initial_epoch=start_epoch,
        workers=0)

Now it can run. But the traing does not converge. 100% error result.
But there is no problem if use Keras intead of tf.keras.







"
23708,"Can't build TensorRT 5.0.10 within Tensorflow 1.12, Tensorflow still search for libnvinfer.so.4 not 5","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source 
- TensorFlow version: r1.12
- Python version: 2.7
- Installed using virtualenv? pip? conda?: No
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source): 4.9.3
- CUDA/cuDNN version: 9.0/7.0.5
- GPU model and memory: TitanXP 12GB



**Describe the problem**
Since the TensorRT 5 is released, but official latest tensorflow is unable to import TRT5, because it still searches TRT4 (libnvinfer.so.4), I manually compiled the tensorflow r1.12 with gpu and with TRT5.0.

After setting the TRT5.0.10 location in ./configure, the compilation is successfully done. Then I generated the pip wheel file and pip installed it.

Then I tried to import tensorflow and from tensorflow.contrib import tensorrt, still got below error:
```
$ python
Python 2.7.13 (default, Jan  6 2017, 21:12:18) 
[GCC 4.8.5 20150623 (Red Hat 4.8.5-4)] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
/home/web_server/dlpy72/dlpy.tensor1.12/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
>>> tf.__version__
'1.12.0'
>>> from tensorflow.contrib import tensorrt
**** Failed to initialize TensorRT. This is either because the TensorRT installation path is not in LD_LIBRARY_PATH, or because you do not have it installed. If not installed, please go to https://developer.nvidia.com/tensorrt to download and install TensorRT ****
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/web_server/dlpy72/dlpy.tensor1.12/lib/python2.7/site-packages/tensorflow/contrib/tensorrt/__init__.py"", line 34, in <module>
    raise e
tensorflow.python.framework.errors_impl.NotFoundError: libnvinfer.so.4: cannot open shared object file: No such file or directory
>>> 

```
Hence the TensorRT 5 is not supported even if compiling it with tensorflow source.

Any idea will be welcome.

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Configuration log:
```
$ ./configure 
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.15.0 installed.
Do you wish to build TensorFlow with Apache Ignite support? [Y/n]: n
No Apache Ignite support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [Y/n]: n
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: 


Please specify the location where CUDA 9.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /home/web_server/xiaolun/cuda-9.0


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 


Do you wish to build TensorFlow with TensorRT support? [y/N]: y
TensorRT support will be enabled for TensorFlow.

Please specify the location where TensorRT is installed. [Default is /usr/lib/x86_64-linux-gnu]:/home/web_server/xiaolun/TensorRT-5.0.0.10


Please specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: 2.3


Assuming NCCL header path is /media/disk1/fordata/web_server/project/xiaolun/nccl_2.3.7/lib/../include/nccl.h
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 6.1]: 


Do you want to use clang as CUDA compiler? [y/N]: n
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /home/web_server/gcc-4.9.3/bin/gcc]: 


Do you wish to build TensorFlow with MPI support? [y/N]: n
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See tools/bazel.rc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=gdr         	# Build with GDR support.
	--config=verbs       	# Build with libverbs support.
	--config=ngraph      	# Build with Intel nGraph support.
Configuration finished

```
Compilation commands:
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
./bazel-bin/tensorflow/tools/pip_package/build_pip_package ./mnt
"
23706,I am getting  the compilation error as 'tensorflow/lite/kernels/register.h' file not found in example cameraxcworkspace,"I am trying to build the example camera from the tensorflow. But it shows the error as 'tensorflow/lite/kernels/register.h' file not found in my Xcode. 
![screen shot 2018-11-13 at 12 34 51 pm](https://user-images.githubusercontent.com/36992338/48396490-9c3d8200-e740-11e8-8cdd-008887ec5880.png)
"
23705,Install for Python 3.7 on Windows 64 bit,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23704,unable to detect multiple faces in an image/video,"Hello,
i am building a face detection system for study purposes.
I installed tensorflow for python and developed a code in python for face detection.

If i have a single person in the image/video i am able to detect it.
But with multiple faces only one face is detected and others are ignored.

If anyone can help me i would be thankful to you.

Regards,
Napster"
23703,import_meta_graph fails: NcclAllReduce,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v1.12.0-rc2-3-ga6d8ffa 1.12.0
- Python version: 3.5.2
- Bazel version (if compiling from source): 0.16.1
- GCC/Compiler version (if compiling from source): gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10)
- CUDA/cuDNN version: CUDA 9.1, cuDNN 7.1.3
- GPU model and memory: dual 1080Ti (11178MiB)

I trained a model with `MirroredStrategy` across two GPUs on a single machine using the `Estimator` API (custom `Estimator`). I'm unable to load the meta graph with the error:

```
$ python test.py
2018-11-12 22:16:59.061730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:03:00.0
totalMemory: 10.92GiB freeMemory: 10.76GiB
2018-11-12 22:16:59.271191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 1 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:04:00.0
totalMemory: 10.91GiB freeMemory: 10.76GiB
2018-11-12 22:16:59.283574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1
2018-11-12 22:17:01.209999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-12 22:17:01.210032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1
2018-11-12 22:17:01.210038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y
2018-11-12 22:17:01.210042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N
2018-11-12 22:17:01.210423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10405 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1)
2018-11-12 22:17:01.210742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10402 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1)
Traceback (most recent call last):
  File ""test.py"", line 13, in <module>
    tf.app.run()
  File ""/home/sharvil/.virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""test.py"", line 6, in main
    saver = tf.train.import_meta_graph('trained_models/student/3/model.ckpt-59112.meta')
  File ""/home/sharvil/.virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/python/training/saver.py"", line 1674, in import_meta_graph
    meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]
  File ""/home/sharvil/.virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/python/training/saver.py"", line 1696, in _import_meta_graph_with_return_elements
    **kwargs))
  File ""/home/sharvil/.virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/meta_graph.py"", line 806, in import_scoped_meta_graph_with_return_elements
    return_elements=return_elements)
  File ""/home/sharvil/.virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/home/sharvil/.virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/importer.py"", line 391, in import_graph_def
    _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)
  File ""/home/sharvil/.virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/importer.py"", line 158, in _RemoveDefaultAttrs
    op_def = op_dict[node.op]
KeyError: 'NcclAllReduce'
```

I'm using the following code to load the meta graph and checkpoint:

```
import tensorflow as tf


def main(args):
  with tf.Session() as sess:
    saver = tf.train.import_meta_graph('trained_models/student/3/model.ckpt-59112.meta')
    saver.restore('trained_models/student/3/model.ckpt-59112')

if __name__ == '__main__':
  tf.app.run()
```"
23702,Weird Bug in Tf.keras.Model.Predict(x=tf.Dataset iterator),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.9.0 and 1.12.0 (I am using 1.9.0 but the bug is present in 1.12.0 also)
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:M60 16 GB (two 8GB GPUs)


**Describe the current behavior**
when using tf.data.Dataset.Iterator in tf.keras.Model.predict(x=tf.data.Dataset.Iterator, steps). I am getting a weird value error:

> Please provide data as a list or tuple of 2 elements  - input and target pair. Received Tensor(""IteratorGetNext:0"", dtype=int64)

The above error is misleading. why does it need (X,Y) for prediction?

My Testing tf.Dataset iterator obviously does not give a (X,Y) tuple. It gives only X in batches. When I give a numpy array of X as input it works as intended. If I use the dataset iterator with **eager_execution** enabled: I get this error (my batch size is 2):

> Please provide data as a list or tuple of 2 elements  - input and target pair. Received tf.Tensor(
> [[     68       5     521 ...       0       0       0]
>  [   6705 1235757    2411 ...    2804     147      13]], shape=(2, 5000), dtype=int64). We do not use the `target` value here. 

Which makes it clear that when eager execution is enabled, Y is not used.

Moreover, why does tf.Dataset iterator need to output a tuple of (X,Y) ? when using tf.keras.Model.predict() ? Is this the expected behaviour?


NOTE: My model is a single input model not a multi input model

EDIT:
I worked around the error by providing (X,Y), But the keras progress bar doesn't seem to work with it."
23701,Tensorflow chatbot rnn model test issue,"I am getting the following error while testing seq2seq rnn model for chatbot

`Key embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/bias not found in checkpoint
 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_HALF, DT_INT32, DT_HALF, DT_HALF, DT_HALF, ..., DT_HALF, DT_HALF, DT_HALF, DT_HALF, DT_HALF], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
 [[Node: save/RestoreV2/_1 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_6_save/RestoreV2"", tensor_type=DT_HALF, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]`"
23700,Builiding tensorflow C++ package on a new CPU platform.,"**System information**
It's a cross compiling for tensorflow. The target platform is on Sunway Taihu Lake super computer. The CPU modle is SW26010 but it's hard for me to describe the instruction sets.  I'm building it through a cross compiler which is based on gcc5.3 with modified compiler backend and it runs on a normal x86 linux (I'm using ubuntu 18.04 through docker).  This compiler supports the C++11.
The bazel version is 0.18.0

**Describe the problem**
During the cross compiling, the boringssl tells me that #error ""Unknown target CPU""
Here is the CROSSTOOL for this cross-compiling:[CROSSTOOL](https://hastebin.com/idezoconuk.http)

**Provide the exact sequence of commands / steps that you executed before running into the problem**
All configure options are ""no"" and the last -opt option is ""-O2"" .
The building commond is bazel build --sandbox_debug --verbose_failures --copt=-O2 --config=monolithic --crosstool_top=//tools/swgcc_compiler:toolchain --cpu=sw26010 //tensorflow:libtensorflow_cc.so

**Any other info / logs**
In file included from external/boringssl/src/include/openssl/asn1.h:61:0,
                 from external/boringssl/src/crypto/asn1/a_print.c:57:
external/boringssl/src/include/openssl/base.h:118:2: error: #error ""Unknown target CPU""
 #error ""Unknown target CPU""
"
23698,"InvalidArgumentError: indices[26,0] = 5001 is not in [0, 5001)  ( at Training Error in Image Captioning with attention Google Colab)","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
Google Colab
image_captioning_with_attention.ipynb


**Describe the problem**
InvalidArgumentError: indices[26,0] = 5001 is not in [0, 5001)  in Training phase.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

This is all error message.
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-30-03bc9960ded7> in <module>()
     19             for i in range(1, target.shape[1]):
     20                 # passing the features through the decoder
---> 21                 predictions, hidden, _ = decoder(dec_input, features, hidden)
     22 
     23                 loss += loss_function(target[:, i], predictions)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    755       if not in_deferred_mode:
    756         self._in_call = True
--> 757         outputs = self.call(inputs, *args, **kwargs)
    758         self._in_call = False
    759         if outputs is None:

<ipython-input-23-b844d20e3fc2> in call(self, x, features, hidden)
     16 
     17     # x shape after passing through embedding == (batch_size, 1, embedding_dim)
---> 18     x = self.embedding(x)
     19 
     20     # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    755       if not in_deferred_mode:
    756         self._in_call = True
--> 757         outputs = self.call(inputs, *args, **kwargs)
    758         self._in_call = False
    759         if outputs is None:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/embeddings.py in call(self, inputs)
    175     if dtype != 'int32' and dtype != 'int64':
    176       inputs = math_ops.cast(inputs, 'int32')
--> 177     out = embedding_ops.embedding_lookup(self.embeddings, inputs)
    178     return out
    179 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py in embedding_lookup(params, ids, partition_strategy, name, validate_indices, max_norm)
    311       name=name,
    312       max_norm=max_norm,
--> 313       transform_fn=None)
    314 
    315 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py in _embedding_lookup_and_transform(params, ids, partition_strategy, name, max_norm, transform_fn)
    131     if np == 1 and (not transform_fn or ids.get_shape().ndims == 1):
    132       with ops.colocate_with(params[0]):
--> 133         result = _clip(array_ops.gather(params[0], ids, name=name),
    134                        ids, max_norm)
    135         if transform_fn:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in gather(***failed resolving arguments***)
   2671     # TODO(apassos) find a less bad way of detecting resource variables without
   2672     # introducing a circular dependency.
-> 2673     return params.sparse_read(indices, name=name)
   2674   except AttributeError:
   2675     return gen_array_ops.gather_v2(params, indices, axis, name=name)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in sparse_read(self, indices, name)
    756         tape.variable_accessed(self)
    757       value = gen_resource_variable_ops.resource_gather(
--> 758           self._handle, indices, dtype=self._dtype, name=name)
    759     return array_ops.identity(value)
    760 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py in resource_gather(resource, indices, dtype, validate_indices, name)
    611       else:
    612         message = e.message
--> 613       _six.raise_from(_core._status_to_exception(e.code, message), None)
    614 
    615 

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: indices[26,0] = 5001 is not in [0, 5001) [Op:ResourceGather] name: rnn__decoder/embedding/embedding_lookup/
"
23697,"Cannot assign a device for operation 'optimizer/dense_1/bias/RMSProp_1': Operation was explicitly assigned to /job:ps/task:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device. 	 [[Node: optimizer/dense_1/bias/RMSProp_1 = VariableV2[_class=[""loc:@dense_1/bias""], container="""", dtype=DT_FLOAT, shape=[5], shared_name="""", _device=""/job:ps/task:0""]()]]","The error is when I am trying to run distributed training with edward. Below is the code : 

```
parameter_servers = [""localhost:2222""]
workers = [""localhost:2223""]
cluster = tf.train.ClusterSpec({""ps"": parameter_servers, ""worker"": workers})

# start a server for a specific task
server = tf.train.Server(
     cluster,
     job_name=FLAGS.job_name,
     task_index=FLAGS.task_index)
     
class VAE(object):
   def __init__():
       # MODEL
       self.n_features = n_features
       self.params = {
                'M': 2048,
                'd': 5,
                'n_epoch': 2,
                'hidden_d': [25, 5],
                'learning_rate': 0.01
            }
       self.saved_dir_path = dir_path
       self.ckpt_path = os.path.join(self.saved_dir_path,
                                     'checkpointFiles/') + 'model.ckpt'
       # distributed training
       if FLAGS.job_name == ""ps"":
           server.join()
       elif FLAGS.job_name == ""worker"":
           # Between-graph replication
           with tf.device(tf.train.replica_device_setter(
                   worker_device=""/job:worker/task:%d"" % FLAGS.task_index,
                   cluster=cluster)):
               self.global_step = tf.get_variable(
                                                    'global_step',
                                                    [],
                                                    dtype=tf.int64,
                                                    initializer=tf.constant_initializer(0),
                                                    trainable=False)
               self.z = Normal(
                   loc=tf.zeros([self.params['M'], self.params['d']]),
                   scale=tf.ones([self.params['M'], self.params['d']]))
               # self.hidden0 = tf.layers.dense(
               self.hidden = tf.layers.dense(
                   self.z, self.params['hidden_d'][0], activation=tf.nn.relu)
               if len(self.params['hidden_d']) > 1:
                   # for i in xrange(1, len(params[hidden_d])):
                   #     self.__dict__['hidden' + str(i)] = \
                   #         tf.layers.dense(
                   #             self.__dict__['hidden' + str(i-1)],
                   #             self.params['hidden_d'][i],
                   #             activation=tf.nn.relu)
                   for i in xrange(1, len(params['hidden_d'])):
                       self.hidden = \
                           tf.layers.dense(
                               self.hidden,
                               self.params['hidden_d'][i],
                               activation=tf.nn.relu)
               self.x = Bernoulli(
                   logits=tf.layers.dense(
                       # self.__dict__['hidden' + str(len(params['hidden_d'] - 1))],
                       self.hidden,
                       self.n_features), dtype=tf.float64)

               # INFERENCE
               self.x_ph = tf.placeholder(dtype=tf.float64,shape=[None, self.n_features])
               self.hidden = tf.layers.dense(
                   tf.cast(self.x_ph, tf.float32),
                   self.params['hidden_d'][-1],
                   activation=tf.nn.relu)
               if len(self.params['hidden_d']) > 1:
                   for i in xrange(1, len(params['hidden_d'])):
                       j = -(1+i)
                       self.hidden = \
                           tf.layers.dense(
                               self.hidden,
                               self.params['hidden_d'][j],
                               activation=tf.nn.relu)
               self.qz = Normal(
                   loc=tf.layers.dense(self.hidden, self.params['d']),
                   scale=tf.layers.dense(
                       self.hidden, self.params['d'], activation=tf.nn.softplus))
               self.x_avg = Bernoulli(
                   logits=tf.reduce_mean(self.x.parameters['logits'], 0),
                   name='x_avg')
               self.log_likli = tf.reduce_mean(self.x_avg.log_prob(self.x_ph), 1)
               self.optimizer = tf.train.RMSPropOptimizer(
                   self.params['learning_rate'], epsilon=1.0)
               # self.
               self.inference = ed.KLqp({self.z: self.qz}, data={self.x: self.x_ph})
               self.inference_init = self.inference.initialize(
                   optimizer=self.optimizer, global_step = self.global_step, logdir='log')
               self.init = tf.global_variables_initializer()
               self.saver = tf.train.Saver()

   def train(self, train_data):
      #Generate x_batch
      start = 0  # pointer to where we are in iteration
      while True:
         stop = start + self.params['M']
         diff = stop - train_data.shape[0]
         if diff <= 0:
             batch = train_data[start:stop]
             start += self.params['M']
         else:
             batch = np.concatenate((train_data[start:], train_data[:diff]))
             start = diff
         yield batch
       train_data_generator = batch

       saver_hook = tf.train.CheckpointSaverHook(
                                                 checkpoint_dir=FLAGS.model_path,
                                                 save_steps=100,
                                                 saver=tf.train.Saver(),
                                                 checkpoint_basename='model.ckpt',
                                                 scaffold=None
                                                 )

       hooks = [saver_hook]

       with tf.train.MonitoredTrainingSession(
                                              master=server.target,
                                              is_chief=(FLAGS.task_index == 0),
                                              checkpoint_dir=FLAGS.model_path,
                                              hooks=hooks,
                                              config= tf.ConfigProto(allow_soft_placement=True,
                                                                     log_device_placement=True),
                                              save_summaries_steps=None,
                                              save_summaries_secs=None
                                              ) as sess:
           sess.run(self.init)
           # sess.run(self.inference_init)
           # self.inference.initialize(optimizer=self.optimizer)
           n_iter_per_epoch = np.ceil(
               train_data.shape[0] / self.params['M']).astype(int)

           for epoch in xrange(1, self.params['n_epoch'] + 1):
               print ""Epoch: {0}"".format(epoch)
               avg_loss = 0.0
               pbar = Progbar(n_iter_per_epoch)
               for t in xrange(1, n_iter_per_epoch + 1):
                   pbar.update(t)
                   x_batch = next(train_data_generator)
                   info_dict = self.inference.update(
                       feed_dict={self.x_ph: x_batch})
                   avg_loss += info_dict['loss']
               avg_loss /= n_iter_per_epoch
               avg_loss /= self.params['M']
               print ""-log p(x) <= {:0.3f}"".format(avg_loss)
           print ""Done training the model.""
           
 if __name__ == ""__main__"":
     vae = VAE()
     vae.train(data)

```

The error stack is as follows : 

```
2018-11-12 15:46:11.525824: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-11-12 15:46:11.527300: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2222}
2018-11-12 15:46:11.527310: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223}
2018-11-12 15:46:11.527775: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2223
51 features are used.
2018-11-12 15:46:16.294466: I tensorflow/core/distributed_runtime/master_session.cc:1024] Start master session e39a9b7d7a1216dc with config:
Epoch: 1
 1/10 [ 10%] ███                            ETA: 0sTraceback (most recent call last):
  File ""dist_vae.py"", line 357, in <module>
    vae.train(data)
  File ""dist_vae.py"", line 220, in train
    feed_dict={self.x_ph: x_batch})
  File ""/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/edward/inferences/variational_inference.py"", line 154, in update
    _, t, loss = sess.run([self.train, self.increment_t, self.loss], feed_dict)
  File ""/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 905, in run
    run_metadata_ptr)
  File ""/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1137, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1355, in _do_run
    options, run_metadata)
  File ""/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1374, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'optimizer/dense_1/bias/RMSProp_1': Operation was explicitly assigned to /job:ps/task:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.
	 [[Node: optimizer/dense_1/bias/RMSProp_1 = VariableV2[_class=[""loc:@dense_1/bias""], container="""", dtype=DT_FLOAT, shape=[5], shared_name="""", _device=""/job:ps/task:0""]()]]

Caused by op u'optimizer/dense_1/bias/RMSProp_1', defined at:
  File ""dist_vae.py"", line 356, in <module>
    vae = VAE(filepath, params, n_features)
  File ""dist_vae.py"", line 173, in __init__
    optimizer=self.optimizer, global_step = self.global_step, logdir='log')
  File ""/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/edward/inferences/klqp.py"", line 110, in initialize
    return super(KLqp, self).initialize(*args, **kwargs)
  File ""/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/edward/inferences/variational_inference.py"", line 121, in initialize
    global_step=global_step)
  File ""/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py"", line 520, in apply_gradients
    self._create_slots([_get_variable_for(v) for v in var_list])
  File ""/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/rmsprop.py"", line 115, in _create_slots
    self._zeros_slot(v, ""momentum"", self._name)
  File ""/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py"", line 910, in _zeros_slot
    named_slots[_var_key(var)] = slot_creator.create_zeros_slot(var, op_name)
  File ""/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py"", line 174, in create_zeros_slot
    colocate_with_primary=colocate_with_primary)
  File ""/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py"", line 148, in create_slot_with_initializer
    dtype)
  File ""/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py"", line 67, in _create_slot_var
    validate_shape=validate_shape)
  File ""/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 1297, in get_variable
    constraint=constraint)
  File ""/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 1093, in get_variable
    constraint=constraint)
  File ""/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 439, in get_variable
    constraint=constraint)
  File ""/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 408, in _true_getter
    use_resource=use_resource, constraint=constraint)
  File ""/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 800, in _get_single_variable
    use_resource=use_resource)
  File ""/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 2157, in variable
    use_resource=use_resource)
  File ""/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 2147, in <lambda>
    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)
  File ""/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 2130, in default_variable_creator
    constraint=constraint)
  File ""/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variables.py"", line 233, in __init__
    constraint=constraint)
  File ""/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variables.py"", line 333, in _init_from_args
    name=name)
  File ""/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/state_ops.py"", line 134, in variable_op_v2
    shared_name=shared_name)
  File ""/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py"", line 1043, in _variable_v2
    shared_name=shared_name, name=name)
  File ""/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3271, in create_op
    op_def=op_def)
  File ""/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1650, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'optimizer/dense_1/bias/RMSProp_1': Operation was explicitly assigned to /job:ps/task:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.
	 [[Node: optimizer/dense_1/bias/RMSProp_1 = VariableV2[_class=[""loc:@dense_1/bias""], container="""", dtype=DT_FLOAT, shape=[5], shared_name="""", _device=""/job:ps/task:0""]()]]

```

When config= tf.ConfigProto(allow_soft_placement=True, log_device_placement=True) : 

```
optimizer/RMSProp/value: (Const): /job:ps/replica:0/task:0/device:CPU:0
2018-11-12 15:50:32.823393: I tensorflow/core/common_runtime/placer.cc:875] optimizer/RMSProp/value: (Const)/job:ps/replica:0/task:0/device:CPU:0
global_step/Initializer/Const: (Const): /job:ps/replica:0/task:0/device:CPU:0

```

Thanks for the help! :) "
23696,"GPU crashes when running Keras/tensorflow-gpu, specifically when clock speed goes to idle at 0 MHz","Below issue was posted by @r8drascal in TF Keras repo.

I'm using Jupyter Notebook to run Keras with a Tensorflow GPU backend. I've done some testing with various dummy models while simultaneously monitoring my GPU usage using MSI Afterburner, GPU-Z, nvidia-smi and Task Manager. My GPU is a GeForce GTX 960M, which has no issues running games. The temperatures are also low when running Keras.

What I've noticed is that the Keras runs fine (e.g. loading or training a model) in the beginning but whenever Keras is not running anything, the GPU naturally wants to idle from 1097 MHz to 0 MHz and as soon as it does that the GPU crashes. I can see that the ""GPU is lost"" on nvidia-nvsmi. I have to then disable and re-enable my GPU in the Device Manager to get it to work.

Here's a sample script, which I believe was from the official documentation:

import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.optimizers import SGD

import numpy as np
x_train = np.random.random((1000, 20))
y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=50)
x_test = np.random.random((100, 20))
y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=50)

model = Sequential()
model.add(Dense(200, activation='relu', input_dim=20))
model.add(Dropout(0.5))
model.add(Dense(200, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(50, activation='softmax'))

sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy',
              optimizer=sgd,
              metrics=['accuracy'])

model.fit(x_train, y_train,
          epochs=20,
          batch_size=128)
score = model.evaluate(x_test, y_test, batch_size=128)


The model runs fine and completes the training but after a few seconds my GPU dies. This even happens if I just load a model. I.e. I import keras modules, then use ""load_model"", and in less than a minute everything crashes as soon as the clock speed drops to 0 MHz.

Does anyone have any idea why this might be happening?"
23695,Failure to parse PYTHONPATH to find keras_preprocessing?,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux variant
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: (master branch, latest commit e84d75dd7429cdcbfa20518cce4a0e886271cfa0)
- Python version: 3.6.5.1
- Installed using virtualenv? pip? conda?: No
- Bazel version (if compiling from source): 0.18.1
- GCC/Compiler version (if compiling from source): gcc 7.3.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**

I am trying to build tensorflow from source using mkl-dnn.  The command I use for compilation is 
```
../bazel-0.18.1/output/bazel build --config=mkl -c opt --copt=-g --strip=never --copt='-Wl,rpath=/opt/gcc/7.3.0/snos' --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mavx512f --copt=-mavx512pf --copt=-mavx512cd --copt=-mavx512er --copt='-mtune=knl' --copt=""-DEIGEN_USE_VML"" //tensorflow/tools/pip_package:build_pip_package
```

I get the error eventually:
```
from tensorflow.python.keras.preprocessing.sequence import _remove_long_seq
  File ""/gpfs/mira-home/cadams/.cache/bazel/_bazel_cadams/33ea5823a114d12c2a1643d7402e760f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/keras/preprocessing/__init__.py"", line 21, in <module>
    import keras_preprocessing
ModuleNotFoundError: No module named 'keras_preprocessing'
```

Following several leads on the internet (such as https://stackoverflow.com/questions/51771039/error-compiling-tensorflow-from-source-no-module-named-keras-applications/51774943#51774943), I installed keras_preprocessing, keras_applications, h5py and put them on PYTHONPATH. This did not work.   

Ultimately, the solution that *did* work was to manually edit the file tensorflow/python/keras/preprocessing/__init__.py to import sys and for the folder containing keras_preprocessing to be added using sys.path.append.

"
23693,Dilated convolution support with nn-api,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.11
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
Try to run a graph that has dilated convolutions in it with nn-api enabled, and it just fails:
NNAPI does not support dilated Conv2D.
This library justifies its existence on the lack of this: https://github.com/daquexian/DNNLibrary

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
Android developers who want to run segmentation models. These models commonly use dilated conv.

**Any Other info.**
"
23692,Tensor RT 5 Windows,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 1.12
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.18.1
- GCC/Compiler version (if compiling from source): Visual Stuido 2015
- CUDA/cuDNN version: CUDA 10/ cudnn 7
- GPU model and memory: Titan V



**Describe the problem**
Unable to build with Tensor RT in windows. Newest version (Tensor RT5) claims windows support but build script (configure.py) assumes linux only.

Has anybody successfully built with TensorRT5 on windows? Will TF 1.13 include support?
"
23691,"[Regresssion] TypeError: In op 'ScatterUpdate', input types are not compatible with expected types","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09
- Python version: 3.6.6
- Bazel version (if compiling from source): 0.18.0
- GCC/Compiler version (if compiling from source):  8.2.1

**Describe the current behavior**
`tf.scatter_update` keeps failing with TypeError

**Describe the expected behavior**
Should work like with TF 1.11.0

**Code to reproduce the issue**
```python
import tensorflow as tf

x = tf.Variable([1,2,3,4,5])
idx = tf.placeholder(dtype=tf.int32, shape=(None,))
update = tf.placeholder(dtype=tf.int32, shape=(None,))
tf.scatter_update(x, idx, update)
```

This example works with Tensorflow 1.11 but fails with 1.12.
It does not matter whether the shapes of `idx` and `update` are set explicitly or `None`.
"
23689,Object detection demo crashing using custom model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): just changed configuration values to match my model 

`  private static final int TF_OD_API_INPUT_SIZE = 640;`
` private static final boolean TF_OD_API_IS_QUANTIZED = false;`
`private static final String TF_OD_API_MODEL_FILE = ""ssd.tflite"";`
`private static final String TF_OD_API_LABELS_FILE = ""file:///android_asset/ssd_labels.txt"";`

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pixel 2 XL
- TensorFlow installed from (source or binary): demo commit 61c6c84
- TensorFlow version (use command below):
- Python version: na
- Bazel version (if compiling from source):  na
- GCC/Compiler version (if compiling from source): na
- CUDA/cuDNN version: na
- GPU model and memory: na

**Describe the current behavior**
Camera view opens for a few seconds, then crashes with output log below

>  --------- beginning of crash
2018-11-12 10:51:23.193 22685-22703/org.tensorflow.lite.demo A/libc: Fatal signal 6 (SIGABRT), code -6 in tid 22703 (inference), pid 22685 (rflow.lite.demo)
2018-11-12 10:51:23.240 22860-22860/? I/crash_dump64: obtaining output fd from tombstoned, type: kDebuggerdTombstone
2018-11-12 10:51:23.241 872-872/? I//system/bin/tombstoned: received crash request for pid 22685
2018-11-12 10:51:23.241 22860-22860/? I/crash_dump64: performing dump of process 22685 (target tid = 22703)
2018-11-12 10:51:23.242 22860-22860/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
2018-11-12 10:51:23.242 22860-22860/? A/DEBUG: Build fingerprint: 'Android/aosp_taimen/taimen:8.1.0/OPM2.171026.006.H1/mcabah10301330:userdebug/test-keys'
2018-11-12 10:51:23.242 22860-22860/? A/DEBUG: Revision: 'rev_10'
2018-11-12 10:51:23.242 22860-22860/? A/DEBUG: ABI: 'arm64'
2018-11-12 10:51:23.242 22860-22860/? A/DEBUG: pid: 22685, tid: 22703, name: inference  >>> org.tensorflow.lite.demo <<<
2018-11-12 10:51:23.242 22860-22860/? A/DEBUG: signal 6 (SIGABRT), code -6 (SI_TKILL), fault addr --------
2018-11-12 10:51:23.242 22860-22860/? A/DEBUG:     x0   0000000000000000  x1   00000000000058af  x2   0000000000000006  x3   0000000000000008
2018-11-12 10:51:23.242 22860-22860/? A/DEBUG:     x4   0000007674fb8000  x5   0000007674fb8000  x6   0000007674fb8000  x7   0000001400000002
2018-11-12 10:51:23.242 22860-22860/? A/DEBUG:     x8   0000000000000083  x9   0000000010000000  x10  0000007688956a90  x11  0000000000000001
2018-11-12 10:51:23.242 22860-22860/? A/DEBUG:     x12  0000001400000002  x13  0000010000000002  x14  00000000000000ff  x15  00000076811417c4
2018-11-12 10:51:23.242 22860-22860/? A/DEBUG:     x16  000000594a538fa8  x17  000000771f8c352c  x18  0000000000000400  x19  000000000000589d
2018-11-12 10:51:23.242 22860-22860/? A/DEBUG:     x20  00000000000058af  x21  0000000000000083  x22  0000007688956d58  x23  0000007688956d70
2018-11-12 10:51:23.242 22860-22860/? A/DEBUG:     x24  000000000000004c  x25  000000769d28aac0  x26  000000767e6328c0  x27  000000769d28aaf8
2018-11-12 10:51:23.242 22860-22860/? A/DEBUG:     x28  000000769d2439a0  x29  0000007688956ad0  x30  000000771f878760
2018-11-12 10:51:23.242 22860-22860/? A/DEBUG:     sp   0000007688956a90  pc   000000771f878788  pstate 0000000060000000
2018-11-12 10:51:23.250 22860-22860/? A/DEBUG: backtrace:
2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #00 pc 000000000001d788  /system/lib64/libc.so (abort+120)
2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #01 pc 00000000000cab70  /data/app/org.tensorflow.lite.demo-_VB0X9UVAsVLe_Ef7nDX7g==/lib/arm64/libtensorflowlite_jni.so
2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #02 pc 00000000000cc5d8  /data/app/org.tensorflow.lite.demo-_VB0X9UVAsVLe_Ef7nDX7g==/lib/arm64/libtensorflowlite_jni.so
2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #03 pc 00000000000c9084  /data/app/org.tensorflow.lite.demo-_VB0X9UVAsVLe_Ef7nDX7g==/lib/arm64/libtensorflowlite_jni.so
2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #04 pc 000000000011cfb8  /data/app/org.tensorflow.lite.demo-_VB0X9UVAsVLe_Ef7nDX7g==/lib/arm64/libtensorflowlite_jni.so
2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #05 pc 0000000000011278  /data/app/org.tensorflow.lite.demo-_VB0X9UVAsVLe_Ef7nDX7g==/lib/arm64/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+32)
2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #06 pc 0000000000553bf0  /system/lib64/libart.so (art_quick_generic_jni_trampoline+144)
2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #07 pc 000000000054ae4c  /system/lib64/libart.so (art_quick_invoke_static_stub+604)
2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #08 pc 00000000000dc5d0  /system/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+264)
2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #09 pc 000000000029b49c  /system/lib64/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+344)
2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #10 pc 0000000000295a90  /system/lib64/libart.so (_ZN3art11interpreter6DoCallILb0ELb0EEEbPNS_9ArtMethodEPNS_6ThreadERNS_11ShadowFrameEPKNS_11InstructionEtPNS_6JValueE+700)
2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #11 pc 0000000000533f50  /system/lib64/libart.so (MterpInvokeStatic+264)
2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #12 pc 000000000053ca94  /system/lib64/libart.so (ExecuteMterpImpl+14612)
2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #13 pc 0000000000275c00  /system/lib64/libart.so (art::interpreter::Execute(art::Thread*, art::DexFile::CodeItem const*, art::ShadowFrame&, art::JValue, bool)+444)
2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #14 pc 000000000027b7cc  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::DexFile::CodeItem const*, art::ShadowFrame*, art::JValue*)+216)
2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #15 pc 0000000000295a70  /system/lib64/libart.so (_ZN3art11interpreter6DoCallILb0ELb0EEEbPNS_9ArtMethodEPNS_6ThreadERNS_11ShadowFrameEPKNS_11InstructionEtPNS_6JValueE+668)
2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #16 pc 0000000000532ad8  /system/lib64/libart.so (MterpInvokeVirtual+652)
2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #17 pc 000000000053c914  /system/lib64/libart.so (ExecuteMterpImpl+14228)
2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #18 pc 0000000000275c00  /system/lib64/libart.so (art::interpreter::Execute(art::Thread*, art::DexFile::CodeItem const*, art::ShadowFrame&, art::JValue, bool)+444)
2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #19 pc 0000000000525450  /system/lib64/libart.so (artQuickToInterpreterBridge+1052)
2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #20 pc 0000000000553d0c  /system/lib64/libart.so (art_quick_to_interpreter_bridge+92)
2018-11-12 10:51:23.250 22860-22860/? A/DEBUG:     #21 pc 0000000000009644  /dev/ashmem/dalvik-jit-code-cache (deleted)


"
23688,"remove reduce_ from established functions, make it closer to numpy","**System information**
- TensorFlow version (you are using): >>> tf.__version__
'1.11.0
- Are you willing to contribute it (Yes/No): I would like to but have no idea how.

**Describe the feature and the current behavior/state.**
Hi everyone. Thanks for your efforts in making tensorflow.
I've been using tensorflow for some time now and no matter my usecase I always find myself in the position of having forgotten how to call the max, sum, ..., etc. functions. I find that the current naming is utterly confusing and doesn't help ppl to easily learn and integrate with the api. It would have been much better if all possible functions were as close as possible to numpy naming conventions. Trying to translate code from some other framework that follows a numpy naming conventions I find myself typing `tf.max` only then to realize that it doesn't exist. Then I have to spend time re-learning that is not `tf.max` but `tf.reduce_max`. Now it could be only me but I suspect that this kind of notation doesn't stick in to ppls long term memory.

**Will this change the current api? How?**
Yes. Most common functions will have the same naming convention to numpy.

**Who will benefit with this feature?**
All existing and new users.

**Any Other info.**
Please keep unnecessary complexity out of tensorflow.  
"
23687,scatter_max doesn't work with MirroredStrategy since v1.11.0,"
**System information**
- Have I written custom code: yes
- OS Platform and Distribution: Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from: binary
- TensorFlow version: tensorflow-gpu v1.12.0-0-ga6d8ffae09, v1.11.0-0-gc19e29306c
- Python version: python 2.7.12
- Bazel version: N/A
- GCC/Compiler version: N/A 
- CUDA/cuDNN version: CUDA 9.0 / cuDNN 7.1.4.18-1+cuda9.0
- GPU model and memory: (GeForce GTX 1080Ti / 11172MiB) x 2

**Describe the current behavior**
scatter_max does not work with MirroredStrategy since v1.11.0.

**Describe the expected behavior**
scatter_max can work with MirroredStrategy in tensorflow-gpu 1.10.0

**Code to reproduce the issue**
```python
from __future__ import absolute_import, division, print_function
import tensorflow as tf
import os, sys

def model_func(features, labels, mode, params):
    tmp_var = tf.Variable(tf.zeros(shape=[2]), trainable=False)
    a=tf.get_variable('a', initializer=tf.ones(shape=[]), trainable=True, aggregation=tf.VariableAggregation.MEAN)
    with tf.control_dependencies([tmp_var.initializer]):
        y = tf.stop_gradient(tf.scatter_max(tmp_var, [0, 1], tf.random_uniform([2])))
        loss = tf.reduce_sum(y * a)
    opt = tf.train.AdamOptimizer().minimize(loss, tf.train.get_global_step())
    return tf.estimator.EstimatorSpec(mode=tf.estimator.ModeKeys.TRAIN, loss=loss, train_op=opt)

def input_fn():
    features =  tf.data.Dataset.from_tensors([[0.]]).repeat()
    labels = tf.data.Dataset.from_tensors(0.).repeat()
    return tf.data.Dataset.zip((features, labels))

def main(argv):
    os.environ['CUDA_VISIBLE_DEVICES'] = ""0,1""
    run_config = tf.estimator.RunConfig(
        train_distribute=tf.contrib.distribute.MirroredStrategy(num_gpus=2),
        session_config=tf.ConfigProto(
            allow_soft_placement=True,
            gpu_options=tf.GPUOptions(force_gpu_compatible=True))
    )
    model = tf.estimator.Estimator(model_fn=model_func, config=run_config)

    model.train(input_fn=input_fn, steps=100)

tf.logging.set_verbosity(tf.logging.INFO)
tf.app.run(argv=[sys.argv[0]])
```

**Other info / logs**

error logs from up code running in v1.12.0
```
INFO:tensorflow:Initializing RunConfig with distribution strategies.
INFO:tensorflow:Not using Distribute Coordinator.
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpiGysT0
INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': gpu_options {
  force_gpu_compatible: true
}
allow_soft_placement: true
, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_train_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7f7bc01ee910>, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f7bc01ee890>, '_model_dir': '/tmp/tmpiGysT0', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': '', '_distribute_coordinator_mode': None}
WARNING:tensorflow:Estimator's model_fn (<function model_func at 0x7f7bc5e48578>) includes params argument, but params are not passed to Estimator.
2018-11-12 06:18:54.074313: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-11-12 06:18:55.364237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:03:00.0
totalMemory: 10.91GiB freeMemory: 10.75GiB
2018-11-12 06:18:55.511370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 1 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:04:00.0
totalMemory: 10.91GiB freeMemory: 10.75GiB
2018-11-12 06:18:55.512283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1
2018-11-12 06:18:55.949305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-12 06:18:55.949341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 
2018-11-12 06:18:55.949347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y 
2018-11-12 06:18:55.949351: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N 
2018-11-12 06:18:55.949765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:0 with 10400 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1)
2018-11-12 06:18:55.950082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:1 with 10400 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1)
INFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0
INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_GPU:0
INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_CPU:0
INFO:tensorflow:Configured nccl all-reduce.
2018-11-12 06:18:55.968070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1
2018-11-12 06:18:55.968135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-12 06:18:55.968145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 
2018-11-12 06:18:55.968150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y 
2018-11-12 06:18:55.968155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N 
2018-11-12 06:18:55.968526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10400 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1)
2018-11-12 06:18:55.968672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10400 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1)
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Error reported to Coordinator: 
Traceback (most recent call last):
  File ""/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 795, in run
    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
  File ""/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1195, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/tmp/model_test.py"", line 9, in model_func
    y = tf.stop_gradient(tf.scatter_max(tmp_var, [0, 1], tf.random_uniform([2])))
  File ""/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py"", line 719, in scatter_max
    use_locking=use_locking, name=name)
  File ""/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 510, in _apply_op_helper
    preferred_dtype=default_dtype)
  File ""/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1146, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/values.py"", line 439, in _tensor_conversion_mirrored
    assert not as_ref
AssertionError
Traceback (most recent call last):
  File ""/tmp/model_test.py"", line 33, in <module>
    tf.app.run(argv=[sys.argv[0]])
  File ""/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/tmp/model_test.py"", line 30, in main
    model.train(input_fn=input_fn, steps=100)
  File ""/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1205, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1316, in _train_model_distributed
    self.config)
  File ""/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/python/training/distribute.py"", line 721, in call_for_each_tower
    return self._call_for_each_tower(fn, *args, **kwargs)
  File ""/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 556, in _call_for_each_tower
    return _call_for_each_tower(self, fn, *args, **kwargs)
  File ""/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 183, in _call_for_each_tower
    coord.join(threads)
  File ""/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 795, in run
    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
  File ""/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1195, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/tmp/model_test.py"", line 9, in model_func
    y = tf.stop_gradient(tf.scatter_max(tmp_var, [0, 1], tf.random_uniform([2])))
  File ""/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py"", line 719, in scatter_max
    use_locking=use_locking, name=name)
  File ""/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 510, in _apply_op_helper
    preferred_dtype=default_dtype)
  File ""/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1146, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/data/tf1.12/local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/values.py"", line 439, in _tensor_conversion_mirrored
    assert not as_ref
AssertionError
```"
23686,ImportError: DLL load failed: The specified procedure could not be found.,"i got this message error
   
       ImportError: DLL load failed: The specified procedure could not be found.

i use this packages with python 3.6.0

$ pip list
Package              Version
-------------------- ------------------
absl-py              0.6.1
APScheduler          3.5.3
astor                0.7.1
attrs                18.2.0
Automat              0.7.0
boto3                1.9.42
botocore             1.12.42
certifi              2018.10.15
cffi                 1.11.5
chardet              3.0.4
Click                7.0
cloudpickle          0.6.1
colorama             0.4.0
colorclass           2.2.0
coloredlogs          10.0
colorhash            1.0.2
ConfigArgParse       0.13.0
constantly           15.1.0
cycler               0.10.0
cymem                2.0.2
cytoolz              0.9.0.1
decorator            4.3.0
dill                 0.2.8.2
docopt               0.6.2
docutils             0.14
en-core-web-md       2.0.0
fakeredis            0.10.3
fbmessenger          5.3.2
Flask                1.0.2
Flask-Cors           3.0.7
Flask-JWT-Simple     0.0.3
future               0.17.1
gast                 0.2.0
gevent               1.3.7
greenlet             0.4.15
grpcio               1.16.0
h5py                 2.8.0
humanfriendly        4.17
hyperlink            18.0.0
idna                 2.7
incremental          17.5.0
itsdangerous         1.1.0
Jinja2               2.10
jmespath             0.9.3
jsonpickle           0.9.6
jsonschema           2.6.0
Keras                2.2.4
Keras-Applications   1.0.6
Keras-Preprocessing  1.0.5
kiwisolver           1.0.1
klein                17.10.0
Markdown             3.0.1
MarkupSafe           1.1.0
matplotlib           2.2.3
mattermostwrapper    2.1
mock                 2.0.0
msgpack              0.5.6
msgpack-numpy        0.4.3.2
murmurhash           1.0.1
networkx             2.2
numpy                1.15.4
packaging            17.1
pathlib              1.0.1
pbr                  5.1.1
pika                 0.11.2
pip                  18.1
plac                 0.9.6
preshed              2.0.1
prompt-toolkit       1.0.14
protobuf             3.6.1
pycparser            2.19
pydot                1.2.4
Pygments             2.2.0
PyHamcrest           1.9.0
PyInquirer           1.0.2
PyJWT                1.6.4
pykwalify            1.6.0
pymongo              3.7.2
pyparsing            2.3.0
pyreadline           2.1
PySocks              1.6.8
python-crfsuite      0.9.6
python-dateutil      2.7.5
python-engineio      2.3.2
python-socketio      2.0.0
python-telegram-bot  10.1.0
pytz                 2018.7
PyYAML               3.13
rasa-core            0.12.0
rasa-core-sdk        0.12.1
rasa-nlu             0.13.7
redis                2.10.6
regex                2018.1.10
requests             2.20.1
requests-toolbelt    0.8.0
rocketchat-API       0.6.22
ruamel.yaml          0.15.77
s3transfer           0.1.13
scikit-learn         0.19.2
scipy                1.1.0
setuptools           39.1.0
simplejson           3.16.0
six                  1.11.0
sklearn-crfsuite     0.3.6
slackclient          1.3.0
spacy                2.0.16
tabulate             0.8.2
tb-nightly           1.13.0a20181112
tensorboard          1.10.0
tensorflow           1.10.0
tensorflow-estimator 1.10.12
termcolor            1.1.0
terminaltables       3.1.0
tf-nightly           1.13.0.dev20181111
thinc                6.12.0
toolz                0.9.0
tqdm                 4.28.1
twilio               6.19.2
Twisted              18.9.0
typing               3.6.6
tzlocal              1.5.1
ujson                1.35
urllib3              1.24.1
wcwidth              0.1.7
webexteamssdk        1.0.3
websocket-client     0.54.0
Werkzeug             0.14.1
wheel                0.32.2
wrapt                1.10.11
zope.interface       4.6.0

complet error stack

      python -m rasa_core.train -d domain.yml -s data/stories.md -o models/current/dialogue --epochs 
      200
     Traceback (most recent call last):
     File ""C:\python36\lib\runpy.py"", line 193, in _run_module_as_main
        ""__main__"", mod_spec)
       File ""C:\python36\lib\runpy.py"", line 85, in _run_code
        exec(code, run_globals)
       File ""C:\python36\lib\site-packages\rasa_core\train.py"", line 14, in <module>
         from rasa_core import config
       File ""C:\python36\lib\site-packages\rasa_core\config.py"", line 9, in <module>
         from rasa_core.policies import PolicyEnsemble
       File ""C:\python36\lib\site-packages\rasa_core\policies\__init__.py"", line 9, in <module>
        from rasa_core.policies.keras_policy import KerasPolicy
       File ""C:\python36\lib\site-packages\rasa_core\policies\keras_policy.py"", line 11, in <module>
        import tensorflow as tf
        File ""C:\python36\lib\site-packages\tensorflow\__init__.py"", line 22, in <module>
       from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
        File ""C:\python36\lib\site-packages\tensorflow\python\__init__.py"", line 52, in <module>
       from tensorflow.core.framework.graph_pb2 import *
       File ""C:\python36\lib\site-packages\tensorflow\core\framework\graph_pb2.py"", line 6, in <module>
       from google.protobuf import descriptor as _descriptor
        File ""C:\python36\lib\site-packages\google\protobuf\descriptor.py"", line 47, in <module>
       from google.protobuf.pyext import _message
       ImportError: DLL load failed: The specified procedure could not be found.
"
23685,Can anyone help me with this import error:,"

### System information
- **custom code from [here](https://www.datacamp.com/community/tutorials/cnn-tensorflow-python)
- **OS Platform and Distribution : winows 10
- **TensorFlow installed from (source or binary)**: coda install tensorflow
- **Python version**: v3.6
- **CUDA/cuDNN version**: v9. 0
- **GPU model and memory**: Quadro M1200 
 TensorFlow version : 'v1.9.0-0-g25c197e023' 1.9.0
......................................................
### Below is my code:


>```
 import numpy as np
> import matplotlib.pyplot as plt
> import tensorflow as tf
> from tensorflow.examples.tutorials.mnist import input_data
> import os
> os.environ[""CUDA_VISIBLE_DEVICES""]=""0"" #for training on gpu
> 
> #import data
> data = input_data.read_data_sets('C:\...\deep learning by python\fashmnist',one_hot=True)
> 

```
............................................................
### Error of above code is: 
( the error is related to this line of source code: 
from tensorflow.examples.tutorials.mnist import input_data)
.........................................................
runfile('C:/.../deep learning by python/fashmnist/fashmnist.py', wdir='C:/.../deep learning by python/fashmnist')
Traceback (most recent call last):

  File ""<ipython-input-19-7a288b4aec63>"", line 1, in <module>
    runfile('C:/.../deep learning by python/fashmnist/fashmnist.py', wdir='C:/.../deep learning by python/fashmnist')

  File ""C:\Users\...\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 668, in runfile
    execfile(filename, namespace)

  File ""C:\Users\...\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 108, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/.../deep learning by python/fashmnist/fashmnist.py"", line 12, in <module>
    from tensorflow.examples.tutorials.mnist import input_data

  File ""C:\Users\...\Anaconda3\lib\site-packages\tensorflow\examples\tutorials\mnist\__init__.py"", line 21, in <module>
    from tensorflow.examples.tutorials.mnist import input_data

  File ""C:\Users\...\Anaconda3\lib\site-packages\tensorflow\examples\tutorials\mnist\input_data.py"", line 30, in <module>
    from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets

  File ""C:\Users\...\Anaconda3\lib\site-packages\tensorflow\contrib\__init__.py"", line 36, in <module>
    from tensorflow.contrib import data

  File ""C:\Users\...\Anaconda3\lib\site-packages\tensorflow\contrib\data\__init__.py"", line 71, in <module>
    from tensorflow.contrib.data.python.ops.error_ops import ignore_errors

  File ""C:\Users\...\Anaconda3\lib\site-packages\tensorflow\contrib\data\python\ops\error_ops.py"", line 20, in <module>
    from tensorflow.contrib.data.python.ops import contrib_op_loader  # pylint: disable=unused-import

  File ""C:\Users\...\Anaconda3\lib\site-packages\tensorflow\contrib\data\python\ops\contrib_op_loader.py"", line 24, in <module>
    resource_loader.get_path_to_datafile(""../../_dataset_ops.so""))

  File ""C:\Users\...\Anaconda3\lib\site-packages\tensorflow\contrib\util\loader.py"", line 56, in load_op_library
    ret = load_library.load_op_library(path)

  File ""C:\Users\...\Anaconda3\lib\site-packages\tensorflow\python\framework\load_library.py"", line 56, in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename)

NotFoundError: C:\Users\...\Anaconda3\lib\site-packages\tensorflow\contrib\data\python\ops\..\..\_dataset_ops.so not found

"
23684,How to freeze layers that located at the last of the whole graph  when fine-tune,"The grpah is A->B->C->loss, and I want to freeze A and C when fine-tune. A is a pretrained network. It's easy to freeze A. B contains LSTM and fc ops. C is variables or tuple tensors which are the results of fc. C stores one tensor  for one time because the tensor is hidden state of LSTM. Actually C will stores 6 tensors for a iteration.
It seems there are several solutions:
1. C is variables and ""trainable=False""           (this results in ""no gradient"" error)
2. tf.stop_gradient()          (not try yet)
3.  add var_list to function optimize_loss()         (not try yet)

Which one should I try? Or are there new solutions? 
"
23683,ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `import tensorflow as tf`
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64-bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): _pip_ package manager
- TensorFlow version (use command below): 1.12.0
- Python version: Python 3.6.7 64-bit
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: No GPU
- GPU model and memory: No GPU 


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
Just to test after installation, I tried importing Tensorflow in Python, but got an unexpected error (see Code section) .

**Describe the expected behavior**
I think it will just keep going like this unless I fix the problem......

**Code to reproduce the issue**
```
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\UNO\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\UNO\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\UNO\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\UNO\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\UNO\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<pyshell#1>"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\UNO\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\UNO\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\UNO\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\UNO\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\UNO\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\UNO\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\UNO\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\UNO\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>> 

```
**Other info / logs**
Yeah...... Maybe no logs here. I've got a little `Not included in PATH` log when installing, though.
"
23682,Can't install on Raspberry pi 3B,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**Raspberry 9** on Pi 3B;Linux version 4.14.79-v7+ (dc4@dc4-XPS13-9333) (gcc version 4.9.3 (crosstool-NG crosstool-ng-1.22.0-88-g8460611)) #1159 SMP Sun Nov 4 17:50:20 GMT 2018



- TensorFlow version:1.12.0
- Python version:**3.5.3**
- Installed using virtualenv? pip? conda?:**pip**

**Describe the problem**

Could not find a version that satisfies the requirement tensorflow (from versions: )
No matching distribution found for tensorflow

**Provide the exact sequence of commands / steps that you executed before running into the problem**
pip3 install tensorflow

**Any other info / logs**
libatlas-base-dev is already the newest version (3.10.3-1+rpi1).

"
23681,Generating of wrapper functions for TensorFlow ops for Go fails with not existing framework_go_proto directory,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **macOS Mojave 10.14.1**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**
- TensorFlow installed from (source or binary): **from source**
- TensorFlow version: **1.12.0**
- Python version: **N/A**
- Installed using virtualenv? pip? conda?: **N/A**
- Bazel version (if compiling from source): **N/A**
- GCC/Compiler version (if compiling from source): **N/A**
- CUDA/cuDNN version: **N/A**
- GPU model and memory: **N/A**



**Describe the problem**

When I try to install Tensorflow from tag v1.12.0 for Go based on instructions: https://www.tensorflow.org/install/lang_go and https://github.com/tensorflow/tensorflow/blob/r0.12/tensorflow/go/README.md.
On step **4. Generate wrapper functions for TensorFlow ops:** I get an error:
```bash
$ go generate github.com/tensorflow/tensorflow/tensorflow/go/op
../genop/internal/api_def_map.go:34:2: cannot find package ""github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/tensorflow/core/framework_go_proto"" in any of:
	/usr/local/Cellar/go/1.11.2/libexec/src/github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/tensorflow/core/framework_go_proto (from $GOROOT)
	/Users/bayandin/go/src/github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/tensorflow/core/framework_go_proto (from $GOPATH)
../../../go/src/github.com/tensorflow/tensorflow/tensorflow/go/op/generate.go:18: running ""go"": exit status 1
```

But `go test -v github.com/tensorflow/tensorflow/tensorflow/go`  passes

**Any other info / logs**

* Go version: go1.11.2 darwin/amd64 (from brew)
* libtensorflow version 1.12.0 (from brew)
* protobuf version 3.6.1 (from brew)
* GOPATH=/Users/bayandin/go

"
23680,lite compile error under tensorflow/lite/tools/make,"when tflite move from tensorflow/contrib/lite to tensorflow/lite, all shell under lite/tools/make has and error, and should be below.

```bash
cd ""$SCRIPT_DIR/../../../..""
```"
23679,Installation of tensorflow r1.10 on windows with visual studio c++ (with gpu) help.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64bit
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.10
- Python version:3.6
- Installed using virtualenv? pip? conda?: pip
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 9.2, cuDNN 7.4.1.5
- GPU model and memory: 940mx 2GB Vram



**Describe the problem**
CUDA and cuDNN were succesfully installed. CMake to configure and generate build files for tensorflow was also done. MSBuild to build the ALL_BUILD.vcxproj project was invoked and an error was encountered during this. The error message shown in the command prompt is given below.


**Any other info / logs**
""C:\Users\Allosimanious\Documents\cud\tensorflow-r1.10\tensorflow\contrib\cmake\build\ALL_BUILD.vcxproj"" (default targe
t) (1) ->
""C:\Users\Allosimanious\Documents\cud\tensorflow-r1.10\tensorflow\contrib\cmake\build\tensorflow.vcxproj"" (default targ
et) (135) ->
""C:\Users\Allosimanious\Documents\cud\tensorflow-r1.10\tensorflow\contrib\cmake\build\tensorflow_static.vcxproj"" (defau
lt target) (136) ->
""C:\Users\Allosimanious\Documents\cud\tensorflow-r1.10\tensorflow\contrib\cmake\build\tf_c.vcxproj"" (default target) (1
37) ->
(ClCompile target) ->
  C:\Users\Allosimanious\Documents\cud\tensorflow-r1.10\tensorflow/core/distributed_runtime/rpc/grpc_server_lib.h(21):
fatal error C1083: Cannot open include file: 'grpcpp/grpcpp.h': No such file or directory [C:\Users\Allosimanious\Docum
ents\cud\tensorflow-r1.10\tensorflow\contrib\cmake\build\tf_c.vcxproj]
"
23678,"Windows does not build tflite in anaconda environment : gives ModuleNotFoundError: No module named ""tensorflow.contrib.lite.python.tflite_convert""","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):WINDOWS 10
- TensorFlow installed from (source or binary):anaconda
- TensorFlow version (use command below):1.9
- Python version:3.6
- Bazel version (if compiling from source):NO
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:no
- GPU model and memory:no
-Exact Command:toco help

Describe the problem
I am trying to run the codelab tutorial of tensorflow lite. After installing Tensorflow for cpu in anaconda3 environment , when I try to run the command ""toco --help"", I get the error ModuleNotFoundError: No module named 'tensorflow.contrib.lite.toco.python.tflite_convert'.


source code/logs

Traceback (most recent call last):
  File ""c:\users\hp\anaconda3\envs\tensorflow\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\users\hp\anaconda3\envs\tensorflow\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\hp\Anaconda3\envs\tensorflow\Scripts\toco.exe\__main__.py"", line 5, in <module>
ModuleNotFoundError: No module named 'tensorflow.contrib.lite.python.tflite_convert'

"
23677,Sparc compatibility,"<em> Tensorflow Lite Sparc Support tag:feature_template</em>


**System information**
- TensorFlow version: r1.10
- Are you willing to contribute it: if able to 


Tensorflow currently supports intel x86 (Windows,IOS) and ARM (raspberry pi), but not sparc architecture. Would be nice to also have support for sparc architecture.
"
23674,bazel build on windows10 failed,"**System information**
- OS Platform and Distribution : windows10
- TensorFlow installed from : source
- TensorFlow version: 1.11.0
- Python version: Python 3.6.3 :: Anaconda custom (64-bit)
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.19.0 not from source

only cpu version.



I followed the instruction from [tensorflow official site.](https://tensorflow.google.cn/install/source_windows#bazel_build)

When I entered `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`
An error occurred
```
WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
d:\tensorflow-1.11.0/.bazelrc
d:\tensorflow-1.11.0/tools/bazel.rc
ERROR: SymlinkDirectories(C:\Users\Administrator/_bazel_Administrator/install/46e308c2aba33f6ef6ad7b7f2c018098, c:\users\administrator\_bazel_administrator\buk4ctdf/install): CreateJunction:
FATAL: failed to create installation symlink 'c:\users\administrator\_bazel_administrator\buk4ctdf/install': success
```
"
23673,Tensorflow build failed to complete,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.10
- Python version: 3
- Installed using virtualenv? pip? conda?: virtualenv/pip
- Bazel version (if compiling from source): 0.19.0
- GCC/Compiler version (if compiling from source): gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10) 
- CUDA/cuDNN version:
- GPU model and memory:N/A



**Describe the problem**
I am building a mobile object detection app and trying to convert the model to tensorflow lite model. Accoding to the steps in [this](https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193) blog post, I am supposed to build tensorflow from source using bazel and I followed steps [here](https://www.tensorflow.org/install/source). But after executing for sometime the build process fails. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
`git clone https://github.com/tensorflow/tensorflow.git
 cd tensorflow`
`bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/lite/...`

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

The error message is : 
```
ERROR: /home/stash/.cache/bazel/_bazel_stash/f5e22f69335a98eb99581e9c850e62a4/external/grpc/BUILD:1332:1: C++ compilation of rule '@grpc//:grpc_resolver_dns_ares' failed (Exit 1) gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 53 argument(s) skipped)

Use --sandbox_debug to see verbose messages from the sandbox
external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_ev_driver_posix.cc:23:18: fatal error: ares.h: No such file or directory
compilation terminated.
INFO: Elapsed time: 26.272s, Critical Path: 18.50s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]
INFO: 12 processes: 12 linux-sandbox.
FAILED: Build did NOT complete successfully
//tensorflow/c:c_api_experimental_test                                NO STATUS
//tensorflow/c:c_api_function_test                                    NO STATUS
//tensorflow/c:c_api_test_gpu                                         NO STATUS
//tensorflow/c:while_loop_test                                        NO STATUS
//tensorflow/c/eager:c_api_test_gpu                                   NO STATUS
//tensorflow/cc:cc_op_gen_test                                        NO STATUS
//tensorflow/cc:client_client_session_test                            NO STATUS
...
...
//tensorflow/tools/proto_text:gen_proto_text_functions_lib_test       NO STATUS

FAILED: Build did NOT complete successfully
```

Elapsed time Critical Path are low in the error message as I had tried the command again after it failed in the first try."
23671,pybullet.error: createCollisionShape failed.,"It is on Ubuntu 16.04LTS, and it shows this error:
![image](https://user-images.githubusercontent.com/15700681/48325102-ff5ee400-e66e-11e8-8569-211441bbe004.png)
"
23670,Run example graph_mobilenet with QASYMM8 data type and CL target,"Hi, 
I try to run example graph_mobilenet with QASYMM8 data type, but it would report error as follows:
$ graph_mobilenet --type=QASYMM8 --target=CL

ERROR in validate_all_nodes src/graph/detail/ExecutionHelpers.cpp:50: in validate_arguments src/core/CL/kernels/CLDirectConvolutionOutputStageKernel.cpp:51: ITensor data type QASYMM8 not supported by this kernel No such process

And running graph_mobilenet with dafault data type goes well:
$graph_mobilenet --target=CL
Could you help to support this issue? Thanks."
23669,Disable warnning logging by default,"One of the thing that I prefer pytorch more is that tensorflow are so annoying. Those infor logging such as tensorflow build doesn't with AVX or which gpu I got information does not needed to print out, cause we just doesn't care... those information we can get from nvidia or something else, but in tensorflow itself, we just need the model and logic....

It would make tensorflow more beautiful and wonderful if there are no such annoying information. simpler is better than complex."
23668,A method to improve accuracy of LazyAdam,"



"
23667,Incorrect cuda9.0 package installation required. Should be cuda-9-0,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 1.11 +
- Doc Link: https://www.tensorflow.org/install/gpu


**Describe the documentation issue**
The documentation states that you will need to install the cuda9.0 package. This does not exist in the repo - http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/. It should be the cuda-9-0 package instead.

If you don't install the package, tensorflow works, but the GPUs are not engaged. This can be verified using the `gpustat -cp` command provided by the python pip gpustat library.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
23665,Possible Error for tf Gradient on mean,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): window 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version: python 3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
b'v1.8.0-0-g93bc2e2072' 1.8.0

**Describe the current behavior**
When taking the gradient respect to a variable and reduce mean the outcome should be 
1 - (1/(# of reduced dimension)) however it does not happen. 


**Describe the expected behavior**
There are a different blog post and resources on how to perform backpropagation for batch norm. 
Centering the data is a key part of any norm, and it seems like auto differentiation is getting the value wrong? Or am I just going crazy. I thought the gradient respect to the input variable should be
1 - 1/(10*94*94) - since 10 is dim - 94 is dim 1 and 2. 

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
import tensorflow as tf
testing_tensor   = tf.placeholder(shape=[10,94,94,32],dtype=tf.float64)
testing_output   = testing_tensor - tf.reduce_mean(testing_tensor,(0,1,2),keep_dims=True) 
what_grad_auto   = tf.gradients(xs=testing_tensor,ys=testing_output)

![image](https://user-images.githubusercontent.com/22832406/48318218-c9682400-e5ca-11e8-8abd-46e660ada701.png)

"
23664,Documents for MirroredStrategy,"**System information**

- TensorFlow version: 1.12
- Doc Link:
  1. https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute
  2. https://www.tensorflow.org/api_docs/python/tf/contrib/distribute/MirroredStrategy


**Describe the documentation issue**

[Document i](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute) says that `tf.contrib.distribute.MirroredStrategy` is for only the case of ""single worker and multi GPUs"".
But [document ii](https://www.tensorflow.org/api_docs/python/tf/contrib/distribute/MirroredStrategy) says that `MirroredStrategy` can work in multi-worker (multi-node) cluster.

Which is right?

If [document i](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute) is out of date, we have to update it.
"
23663,Help with what version to install?,"I have a cuda compute capability 3.0 GPU ,AMD processor and running on windows 10 64bit . So when I checked the official tensorflow website it says requires cuda compute capability 3.5 or higher. 

So which version should I install from the repository to support cuda 3.0 and run on my computer. 

> https://www.tensorflow.org/install/gpu"
23660,__manage.py__ not found,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.11.6
- TensorFlow installed from (source or binary): source, if this means from tensorflow.org
- TensorFlow version:  TensorFlow version 1.12
- Python version: Python 3.6
- Installed using virtualenv? pip? condo?: installed using pip, conda and in a virtual environment
- Bazel version (if compiling from source): not yet
- GCC/Compiler version (if compiling from source): CPU version
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
I installed tensorflow three weeks ago, via pip.  Then tried to start running tutorials or at least verify install.  Kept getting ""file not found"",  checked ""pip list"", it was there, but in downloads.  Then after further checking, noted it to be in site-packages under downloads.  I've done the installation as pip, as conda and in a virtual environment.  Comments like ""virtualenv"" not found frustrated the installation but I was eventually able to get set up a virtual environment.  Last night I tested it using the ""pip --upgrade tensorflow"", command.  Requirements had been met.  Then I tried opening both keras and tensorflow apps.  Okay,  tensorflow has an outer and inner directory.  No prob.  Still now I get ""__manage.py__"" not found in package.  Could not copy entire traceback from terminal but using the conda console I still get errors.  Here's a copy of the last commands I tried today and the traceback from the console,...albeit, the terminal traceback is much larger:

python -c ""import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))""

1. File ""<ipython-input-1-77841da38ed1>"", line 1
    python -c ""import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))""
                                                                                                                         ^
SyntaxError: invalid syntax

2.  File ""<ipython-input-2-92d0143764a7>"", line 1
    python -c ""import tensorflow as tf; tf.enable_eager_execution()
                                                                   ^
SyntaxError: EOL while scanning string literal

3.  File ""<ipython-input-3-7e790e56de0e>"", line 1
    python -c ""import tensorflow as tf; tf.enable_eager_execution();
                                                                    ^
SyntaxError: EOL while scanning string literal

4. File ""<ipython-input-4-77841da38ed1>"", line 1
    python -c ""import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))""
                                                                                                                         ^
SyntaxError: invalid syntax
**Provide the exact sequence of commands / steps that you executed before running into the problem**

5.   File ""<ipython-input-5-50ca0a742afa>"", line 1  [without quotes]
    python -c import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))
                   ^
SyntaxError: invalid syntax
**Any other info / logs**

Trying to copy the terminal log, and traceback.

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

"
23659,TPU: keras support for multiples inputs layers,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colaboratory

**Describe the current behavior**

I am using the functional api of keras models, and TPU does not yet support multiple inputs layers.

**Describe the expected behavior**

It should support multiple inputs models.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py in _validate_shapes(model)
   2077   """"""Validate that all layers in `model` have constant shape.""""""
   2078   for layer in model.layers:
-> 2079     if isinstance(layer.input_shape, tuple):
   2080       input_shapes = [layer.input_shape]
   2081     else:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in input_shape(self)
   1351     else:
   1352       raise AttributeError('The layer ""' + str(self.name) +
-> 1353                            ' has multiple inbound nodes, '
   1354                            'with different input shapes. Hence '
   1355                            'the notion of ""input shape"" is '

AttributeError: The layer ""model_encoder_vgg19 has multiple inbound nodes, with different input shapes. Hence the notion of ""input shape"" is ill-defined for the layer. Use `get_input_shape_at(node_index)` instead.
```
The issue is at line 2079 of `keras_support.py` where `get_input_shape_at(node_index)` should be used instead. 

"
23657,"Multiple tensorflow devices created, then programs ends","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Written
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:-
- TensorFlow installed from (source or binary): source(?) 1.12 from pip.
- TensorFlow version (use command below): 1.12
- Python version: 3.6
- Bazel version (if compiling from source):-
- GCC/Compiler version (if compiling from source):-
- CUDA/cuDNN version: 9.0
- GPU model and memory: GTX 1070Ti 8GB


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
Originally was not an issue. Tensorflow now creates multiple devices, see log. I installed baseline for OpenAI and this started after that. I reinstalled tensorflow and the issue persists.

My scripts no longer execute because of this.

**Code to reproduce the issue**
All code results in the issue.

**Other info / logs**
2018-11-10 14:29:08.296852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-10 14:29:08.297012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2018-11-10 14:29:08.297107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2018-11-10 14:29:08.297353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6393 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)

2018-11-10 14:29:08.577708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-11-10 14:29:08.577917: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-10 14:29:08.578069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2018-11-10 14:29:08.578162: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2018-11-10 14:29:08.578303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:0 with 6393 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)
_________________________________________________________________
2018-11-10 14:29:15.921961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-11-10 14:29:15.922125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-10 14:29:15.922269: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2018-11-10 14:29:15.922362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2018-11-10 14:29:15.922500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:0 with 6393 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)
"
23656,Java - java.lang.UnsatisfiedLinkError: tensorflow_jni.dll: Can't find dependent libraries (only GPU version),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): Tried to compile it from source and tried binary file
- TensorFlow version: 1.12.0
- Python version: 3.7
- Bazel version (if compiling from source): 15.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10.0 & cuDNN 7.3.0
- GPU model and memory: GTX 970 TI


**Describe the problem**
Hello, I'm actually trying to contact you here because I'm stuck on that error for 2 days and it is frustrating.
When I'm trying to use tensorflow_jni.dll for GPU, it says that error but when I use the CPU version, it works. The problem is that by using CPU, it take 15 seconds to recognize one image using Yolo... with my i7 4770k... I installed Visual Studio 2017, 2015, Visual Studio Build Tool 2015 and Visual C++ 2015 Redistributable. I am using java 8

**Source code / logs**
It does that error when tensorflow_jni.dll is being loaded by java.
Exception in thread ""main"" java.lang.UnsatisfiedLinkError: C:\Programmation\test\NeuralNetwork\src\org\tensorflow\native\tensorflow_jni.dll: Can't find dependent libraries


Thank you really much and have a good day !"
23655,r1.12 C++ tutorials_example_trainer failed to build on Windows,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 v1803 Pro 64bit English
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A
- TensorFlow installed from (source or binary):source
- TensorFlow version:1.12
- Python version:3.6
- Installed using virtualenv? pip? conda?:pip
- Bazel version (if compiling from source):0.15
- GCC/Compiler version (if compiling from source):MSVC 2015 update 3
- CUDA/cuDNN version:N/A
- GPU model and memory:N/A

**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

***Commands***
```
bazel build -c opt --verbose_failures //tensorflow/cc:tutorials_example_trainer
```

***Steps***
Setup as described in [official document](https://www.tensorflow.org/install/source_windows)

.bazelrc is customized in the following way.
```
import (YOUR_TENSORFLOW_FOLDER)/tf_configure.bazelrc
```
.tf_configure.bazelrc is customized in the following way.
```
build:ignite --define with_ignite_support=true
build:xla --define with_xla_support=false
build --action_env TF_NEED_OPENCL_SYCL=""0""
build --action_env TF_NEED_ROCM=""0""
build --action_env TF_NEED_CUDA=""0""
build --action_env TF_DOWNLOAD_CLANG=""0""
build:opt --copt=/arch:AVX2
build:opt --define with_default_optimizations=true
build --copt=-w --host_copt=-w
build --verbose_failures
build --distinct_host_configuration=false
build --experimental_shortened_obj_file_path=true
build --define=no_tensorflow_py_deps=true
build --define=override_eigen_strong_inline=true
build:v2 --define=tf_api_version=2
```

**Any other info / logs**
```
unicode_script_op.lo.lib(unicode_script_op.obj) : error LNK2019: unresolved external symbol ""__declspec(dllimport) public: virtual __cdecl icu_62::ErrorCode::~ErrorCode(void)"" (__imp_??1ErrorCode@icu_62@@UEAA@XZ) referenced in function ""public: virtual void __cdecl tensorflow::UnicodeScriptOp::Compute(class tensorflow::OpKernelContext *)"" (?Compute@UnicodeScriptOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
unicode_script_op.lo.lib(unicode_script_op.obj) : error LNK2019: unresolved external symbol ""__declspec(dllimport) public: signed char __cdecl icu_62::ErrorCode::isSuccess(void)const "" (__imp_?isSuccess@ErrorCode@icu_62@@QEBACXZ) referenced in function ""public: virtual void __cdecl tensorflow::UnicodeScriptOp::Compute(class tensorflow::OpKernelContext *)"" (?Compute@UnicodeScriptOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
unicode_script_op.lo.lib(unicode_script_op.obj) : error LNK2019: unresolved external symbol ""__declspec(dllimport) public: enum UErrorCode __cdecl icu_62::ErrorCode::reset(void)"" (__imp_?reset@ErrorCode@icu_62@@QEAA?AW4UErrorCode@@XZ) referenced in function ""public: virtual void __cdecl tensorflow::UnicodeScriptOp::Compute(class tensorflow::OpKernelContext *)"" (?Compute@UnicodeScriptOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
unicode_script_op.lo.lib(unicode_script_op.obj) : error LNK2019: unresolved external symbol ""__declspec(dllimport) const icu_62::ErrorCode::`vftable'"" (__imp_??_7ErrorCode@icu_62@@6B@) referenced in function ""public: virtual void __cdecl tensorflow::UnicodeScriptOp::Compute(class tensorflow::OpKernelContext *)"" (?Compute@UnicodeScriptOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
```
Similar to tutorials_example_trainer, build //tensorflow/examples/label_image:label_image also failed."
23654,tf.contrib.image.transform lead to a ValueError in new releases of tensorflow,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Manjaro 18.0
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): python-tensorflow-opt-cuda from manjaro repositories 
- TensorFlow version (use command below): 1.11
- Python version: 3.6/3.7
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: 10.0.130-2 /   7.3.0-1
- GPU model and memory: 1080Ti 11GB


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

After the release that Allow a different output shape from the input in `tf.contrib.image.transform` code that applied this function stopped working with a value error. For exampleon previous versions, using eager execution this worked:

`image = tf.contrib.image.translate(image, random_translation, 'NEAREST') `

But after this change I get a `ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()`` on `tf.contrib.image.transform` on the condition of the line 273-275 of `tensorflow/tensorflow/contrib/image/python/ops/image_ops.py`, where this condition is triggered (caused by empty output shape call in tf.contrib.image.translate:

```
if output_shape is None:
      output_shape = tensor_util.constant_value(
          array_ops.shape(images)[1:3]) or array_ops.shape(images)[1:3]
```

**Describe the expected behavior**

If instead of `tf.contrib.image.transform` I run it with `output_shape` argument:

`random_transformations = tf.contrib.image.translations_to_projective_transforms(random_shifts)
images = tf.contrib.image.transform(image, random_transformations, 'NEAREST',                                       output_shape=tf.convert_to_tensor(images.numpy().shape[1:3], dtype=np.int32))`

everything goes as expected. So I guess that the issue in on passing output_shape=None in line 122-126 of `tensorflow/tensorflow/contrib/image/python/ops/image_ops.py:

```
def translate(images, translations, interpolation=""NEAREST"", name=None):
  """"""Translate image(s) by the passed vectors(s).
  Args:
    images: A tensor of shape (num_images, num_rows, num_columns, num_channels)
        (NHWC), (num_rows, num_columns, num_channels) (HWC), or
        (num_rows, num_columns) (HW). The rank must be statically known (the
        shape is not `TensorShape(None)`.
    translations: A vector representing [dx, dy] or (if images has rank 4)
        a matrix of length num_images, with a [dx, dy] vector for each image in
        the batch.
    interpolation: Interpolation mode. Supported values: ""NEAREST"", ""BILINEAR"".
    name: The name of the op.
  Returns:
    Image(s) with the same type and shape as `images`, translated by the given
        vector(s). Empty space due to the translation will be filled with zeros.
  Raises:
    TypeError: If `image` is an invalid type.
  """"""
  with ops.name_scope(name, ""translate""):
    return transform(
        images,
        translations_to_projective_transforms(translations),
        interpolation=interpolation)

```"
23653,Tensorflow installation issue: ImportError: DLL load failed with error code -1073741795,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7 Service Pack1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.12.0-cp35-cp35m-win_amd64.whl
- TensorFlow version: 1.12
- Python version: 3.5.4 64-bit
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 
- GPU model and memory:



**Describe the problem**
Unable to install and import Tensorflow on my PC. I have tried the below approach as mentioned on https://www.tensorflow.org/install/pip#package-location. I have also tried installation using Conda but the same problem appears. MS Visual C++ is also installed as mentioned on https://www.tensorflow.org/install/pip#package-location

**Provide the exact sequence of commands / steps that you executed before running into the problem**
python -c ""import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))""


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Traceback (most recent call last):
  File ""C:\Users\Desk\AppData\Local\Programs\Python\Python35\Scripts\venv\lib\si
te-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Desk\AppData\Local\Programs\Python\Python35\Scripts\venv\lib\si
te-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <modul
e>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Desk\AppData\Local\Programs\Python\Python35\Scripts\venv\lib\si
te-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_i
mport_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript
ion)
  File ""C:\Users\Desk\AppData\Local\Programs\Python\Python35\Scripts\venv\lib\im
p.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Desk\AppData\Local\Programs\Python\Python35\Scripts\venv\lib\im
p.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed with error code -1073741795

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\Desk\AppData\Local\Programs\Python\Python35\Scripts\venv\lib\si
te-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-im
port
  File ""C:\Users\Desk\AppData\Local\Programs\Python\Python35\Scripts\venv\lib\si
te-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Desk\AppData\Local\Programs\Python\Python35\Scripts\venv\lib\si
te-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Desk\AppData\Local\Programs\Python\Python35\Scripts\venv\lib\si
te-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Desk\AppData\Local\Programs\Python\Python35\Scripts\venv\lib\si
te-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <modul
e>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Desk\AppData\Local\Programs\Python\Python35\Scripts\venv\lib\si
te-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_i
mport_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript
ion)
  File ""C:\Users\Desk\AppData\Local\Programs\Python\Python35\Scripts\venv\lib\im
p.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Desk\AppData\Local\Programs\Python\Python35\Scripts\venv\lib\im
p.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed with error code -1073741795"
23652, tfjs-node - binary was not compiled to use: SSE4.2 AVX AVX2 FMA and performance on node,"Hi

I have looked for an issue in regards to .js and tfjs node but can't find one so apologies for reposting if i've missed it!

We're trying to deploy an API on EC2 which uses tfjs-node. My main concern is finding a way i can bring down inference time on a server.

I have found instructions on using different binaries for python. Are these binaries compatible with the node tfjs wrapper? 

Apart from anything else i assume theres a way to get rid of these warnings in the terminal?

"
23651,TFLiteConverter.from_session() error,"Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, DIV, FLOOR, FULLY_CONNECTED, MUL. Here is a list of operators for which you will need custom implementations: RandomUniform.
"
23649,cannot link static library libtensorflow.a by go for tensorflow,"
**System information**
GOARCH=""amd64""
GOBIN=""""
GOCACHE=""/search/odin/tensorflow/.cache/go-build""
GOEXE=""""
GOFLAGS=""""
GOHOSTARCH=""amd64""
GOHOSTOS=""linux""
GOOS=""linux""
GOPATH=""/search/odin/tensorflow/gopath/""
GOPROXY=""""
GORACE=""""
GOROOT=""/search/odin/tensorflow/go""
GOTMPDIR=""""
GOTOOLDIR=""/search/odin/tensorflow/go/pkg/tool/linux_amd64""
GCCGO=""gccgo""
CC=""gcc""
CXX=""g++""
CGO_ENABLED=""1""
GOMOD=""""
CGO_CFLAGS=""-I/search/odin/tensorflow/lib64/include""
CGO_CPPFLAGS=""""
CGO_CXXFLAGS=""-g -O2""
CGO_FFLAGS=""-g -O2""
CGO_LDFLAGS=""-L/search/odin/tensorflow/lib64/lib""
PKG_CONFIG=""pkg-config""
GOGCCFLAGS=""-std=gnu99 -fPIC -m64 -pthread -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build821586571=/tmp/go-build -gno-record-gcc-switches""


**Describe the problem**
I'm trying to build an executable binary file by go for tensorflow. I followed the official hello-world instructions and successfully go build everything. 
However, what I need is to link a static library libtensorflow.a instead of the dynamic one so that the executable binary file has no dependencies. **I managed to build the static library libtensorflow.a** through makefile offered with src(https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile) and put it in **/search/odin/tensorflow/lib64/lib**, then I go build with the command below, and I get ld errors which I will post under **Any other info / logs**.
A similar issue is [here](https://github.com/tensorflow/tensorflow/issues/15563). It seems the same problem but It's not. From what I see, it tries to build the libtensorflow.a by Go for Tensorflow then link it, but I have already got a libtensorflow.a in advance. All I need is to link the static library.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
go build -ldflags '-linkmode ""external"" -extldflags ""-static -L /search/odin/tensorflow/lib64/lib""' hello_tf.go

**Any other info / logs**
command-line-arguments
/search/odin/tensorflow/go/pkg/tool/linux_amd64/link: running gcc failed: exit status 1
/tmp/go-link-707261891/000001.o: In function `_cgo_d969e426e2e3_Cfunc_TF_OperationGetAttrBool':
attrs.cgo2.c:(.text+0x6d): undefined reference to `TF_OperationGetAttrBool'
/tmp/go-link-707261891/000001.o: In function `_cgo_d969e426e2e3_Cfunc_TF_OperationGetAttrBoolList':
attrs.cgo2.c:(.text+0xb4): undefined reference to `TF_OperationGetAttrBoolList'
/tmp/go-link-707261891/000001.o: In function `_cgo_d969e426e2e3_Cfunc_TF_OperationGetAttrFloat':
attrs.cgo2.c:(.text+0xf1): undefined reference to `TF_OperationGetAttrFloat'
/tmp/go-link-707261891/000001.o: In function `_cgo_d969e426e2e3_Cfunc_TF_OperationGetAttrFloatList':
attrs.cgo2.c:(.text+0x138): undefined reference to `TF_OperationGetAttrFloatList'
/tmp/go-link-707261891/000001.o: In function `_cgo_d969e426e2e3_Cfunc_TF_OperationGetAttrInt':
attrs.cgo2.c:(.text+0x175): undefined reference to `TF_OperationGetAttrInt'
/tmp/go-link-707261891/000001.o: In function `_cgo_d969e426e2e3_Cfunc_TF_OperationGetAttrIntList':
attrs.cgo2.c:(.text+0x1bc): undefined reference to `TF_OperationGetAttrIntList'
/tmp/go-link-707261891/000001.o: In function `_cgo_d969e426e2e3_Cfunc_TF_OperationGetAttrMetadata':
attrs.cgo2.c:(.text+0x1fe): undefined reference to `TF_OperationGetAttrMetadata'
/tmp/go-link-707261891/000001.o: In function `_cgo_d969e426e2e3_Cfunc_TF_OperationGetAttrShape':
attrs.cgo2.c:(.text+0x27f): undefined reference to `TF_OperationGetAttrShape'
/tmp/go-link-707261891/000001.o: In function `_cgo_d969e426e2e3_Cfunc_TF_OperationGetAttrShapeList':
attrs.cgo2.c:(.text+0x2e6): undefined reference to `TF_OperationGetAttrShapeList'
/tmp/go-link-707261891/000001.o: In function `_cgo_d969e426e2e3_Cfunc_TF_OperationGetAttrString':
attrs.cgo2.c:(.text+0x32e): undefined reference to `TF_OperationGetAttrString'
/tmp/go-link-707261891/000001.o: In function `_cgo_d969e426e2e3_Cfunc_TF_OperationGetAttrStringList':
attrs.cgo2.c:(.text+0x397): undefined reference to `TF_OperationGetAttrStringList'
/tmp/go-link-707261891/000001.o: In function `_cgo_d969e426e2e3_Cfunc_TF_OperationGetAttrTensor':
attrs.cgo2.c:(.text+0x3d4): undefined reference to `TF_OperationGetAttrTensor'
/tmp/go-link-707261891/000001.o: In function `_cgo_d969e426e2e3_Cfunc_TF_OperationGetAttrTensorList':
attrs.cgo2.c:(.text+0x41b): undefined reference to `TF_OperationGetAttrTensorList'
/tmp/go-link-707261891/000001.o: In function `_cgo_d969e426e2e3_Cfunc_TF_OperationGetAttrType':
attrs.cgo2.c:(.text+0x458): undefined reference to `TF_OperationGetAttrType'
/tmp/go-link-707261891/000001.o: In function `_cgo_d969e426e2e3_Cfunc_TF_OperationGetAttrTypeList':
attrs.cgo2.c:(.text+0x49f): undefined reference to `TF_OperationGetAttrTypeList'
/tmp/go-link-707261891/000002.o: In function `TF_SetAttrShapeList_Helper':
graph.cgo2.c:(.text+0xf2): undefined reference to `TF_SetAttrShapeList'
/tmp/go-link-707261891/000002.o: In function `_cgo_d969e426e2e3_Cfunc_TF_AddControlInput':
graph.cgo2.c:(.text+0x12e): undefined reference to `TF_AddControlInput'
/tmp/go-link-707261891/000002.o: In function `_cgo_d969e426e2e3_Cfunc_TF_AddInput':
graph.cgo2.c:(.text+0x163): undefined reference to `TF_AddInput'
/tmp/go-link-707261891/000002.o: In function `_cgo_d969e426e2e3_Cfunc_TF_AddInputList':
graph.cgo2.c:(.text+0x19a): undefined reference to `TF_AddInputList'
/tmp/go-link-707261891/000002.o: In function `_cgo_d969e426e2e3_Cfunc_TF_DeleteBuffer':
graph.cgo2.c:(.text+0x1bf): undefined reference to `TF_DeleteBuffer'
/tmp/go-link-707261891/000002.o: In function `_cgo_d969e426e2e3_Cfunc_TF_DeleteGraph':
graph.cgo2.c:(.text+0x1e4): undefined reference to `TF_DeleteGraph'
/tmp/go-link-707261891/000002.o: In function `_cgo_d969e426e2e3_Cfunc_TF_DeleteImportGraphDefOptions':
graph.cgo2.c:(.text+0x209): undefined reference to `TF_DeleteImportGraphDefOptions'
/tmp/go-link-707261891/000002.o: In function `_cgo_d969e426e2e3_Cfunc_TF_FinishOperation':
graph.cgo2.c:(.text+0x242): undefined reference to `TF_FinishOperation'
/tmp/go-link-707261891/000002.o: In function `_cgo_d969e426e2e3_Cfunc_TF_GraphImportGraphDef':
graph.cgo2.c:(.text+0x2a5): undefined reference to `TF_GraphImportGraphDef'
/tmp/go-link-707261891/000002.o: In function `_cgo_d969e426e2e3_Cfunc_TF_GraphNextOperation':
graph.cgo2.c:(.text+0x2de): undefined reference to `TF_GraphNextOperation'
/tmp/go-link-707261891/000002.o: In function `_cgo_d969e426e2e3_Cfunc_TF_GraphOperationByName':
graph.cgo2.c:(.text+0x33d): undefined reference to `TF_GraphOperationByName'
/tmp/go-link-707261891/000002.o: In function `_cgo_d969e426e2e3_Cfunc_TF_GraphToGraphDef':
graph.cgo2.c:(.text+0x39b): undefined reference to `TF_GraphToGraphDef'
/tmp/go-link-707261891/000002.o: In function `_cgo_d969e426e2e3_Cfunc_TF_ImportGraphDefOptionsSetPrefix':
graph.cgo2.c:(.text+0x3cb): undefined reference to `TF_ImportGraphDefOptionsSetPrefix'
/tmp/go-link-707261891/000002.o: In function `_cgo_d969e426e2e3_Cfunc_TF_NewBuffer':
graph.cgo2.c:(.text+0x3f4): undefined reference to `TF_NewBuffer'
/tmp/go-link-707261891/000002.o: In function `_cgo_d969e426e2e3_Cfunc_TF_NewGraph':
graph.cgo2.c:(.text+0x442): undefined reference to `TF_NewGraph'
/tmp/go-link-707261891/000002.o: In function `_cgo_d969e426e2e3_Cfunc_TF_NewImportGraphDefOptions':
graph.cgo2.c:(.text+0x490): undefined reference to `TF_NewImportGraphDefOptions'
/tmp/go-link-707261891/000002.o: In function `_cgo_d969e426e2e3_Cfunc_TF_NewOperation':
graph.cgo2.c:(.text+0x4f6): undefined reference to `TF_NewOperation'
/tmp/go-link-707261891/000002.o: In function `_cgo_d969e426e2e3_Cfunc_TF_SetAttrBool':
graph.cgo2.c:(.text+0x557): undefined reference to `TF_SetAttrBool'
/tmp/go-link-707261891/000002.o: In function `_cgo_d969e426e2e3_Cfunc_TF_SetAttrBoolList':
graph.cgo2.c:(.text+0x593): undefined reference to `TF_SetAttrBoolList'
/tmp/go-link-707261891/000002.o: In function `_cgo_d969e426e2e3_Cfunc_TF_SetAttrFloat':
graph.cgo2.c:(.text+0x5d2): undefined reference to `TF_SetAttrFloat'
/tmp/go-link-707261891/000002.o: In function `_cgo_d969e426e2e3_Cfunc_TF_SetAttrFloatList':
graph.cgo2.c:(.text+0x60e): undefined reference to `TF_SetAttrFloatList'
/tmp/go-link-707261891/000002.o: In function `_cgo_d969e426e2e3_Cfunc_TF_SetAttrInt':
graph.cgo2.c:(.text+0x646): undefined reference to `TF_SetAttrInt'
/tmp/go-link-707261891/000002.o: In function `_cgo_d969e426e2e3_Cfunc_TF_SetAttrIntList':
graph.cgo2.c:(.text+0x682): undefined reference to `TF_SetAttrIntList'
/tmp/go-link-707261891/000002.o: In function `_cgo_d969e426e2e3_Cfunc_TF_SetAttrShape':
graph.cgo2.c:(.text+0x6be): undefined reference to `TF_SetAttrShape'
/tmp/go-link-707261891/000002.o: In function `_cgo_d969e426e2e3_Cfunc_TF_SetAttrShapeList':
graph.cgo2.c:(.text+0x705): undefined reference to `TF_SetAttrShapeList'
/tmp/go-link-707261891/000002.o: In function `_cgo_d969e426e2e3_Cfunc_TF_SetAttrString':
graph.cgo2.c:(.text+0x789): undefined reference to `TF_SetAttrString'
/tmp/go-link-707261891/000002.o: In function `_cgo_d969e426e2e3_Cfunc_TF_SetAttrStringList':
graph.cgo2.c:(.text+0x7d0): undefined reference to `TF_SetAttrStringList'
/tmp/go-link-707261891/000002.o: In function `_cgo_d969e426e2e3_Cfunc_TF_SetAttrTensor':
graph.cgo2.c:(.text+0x80d): undefined reference to `TF_SetAttrTensor'
/tmp/go-link-707261891/000002.o: In function `_cgo_d969e426e2e3_Cfunc_TF_SetAttrTensorList':
graph.cgo2.c:(.text+0x854): undefined reference to `TF_SetAttrTensorList'
/tmp/go-link-707261891/000002.o: In function `_cgo_d969e426e2e3_Cfunc_TF_SetAttrType':
graph.cgo2.c:(.text+0x88b): undefined reference to `TF_SetAttrType'
/tmp/go-link-707261891/000002.o: In function `_cgo_d969e426e2e3_Cfunc_TF_SetAttrTypeList':
graph.cgo2.c:(.text+0x8c7): undefined reference to `TF_SetAttrTypeList'
/tmp/go-link-707261891/000002.o: In function `_cgo_d969e426e2e3_Cfunc_TF_SetDevice':
graph.cgo2.c:(.text+0x8f7): undefined reference to `TF_SetDevice'
/tmp/go-link-707261891/000004.o: In function `_cgo_d969e426e2e3_Cfunc_TF_GraphGetTensorNumDims':
operation.cgo2.c:(.text+0x73): undefined reference to `TF_GraphGetTensorNumDims'
/tmp/go-link-707261891/000004.o: In function `_cgo_d969e426e2e3_Cfunc_TF_GraphGetTensorShape':
operation.cgo2.c:(.text+0xe5): undefined reference to `TF_GraphGetTensorShape'
/tmp/go-link-707261891/000004.o: In function `_cgo_d969e426e2e3_Cfunc_TF_OperationDevice':
operation.cgo2.c:(.text+0x113): undefined reference to `TF_OperationDevice'
/tmp/go-link-707261891/000004.o: In function `_cgo_d969e426e2e3_Cfunc_TF_OperationInput':
operation.cgo2.c:(.text+0x16c): undefined reference to `TF_OperationInput'
/tmp/go-link-707261891/000004.o: In function `_cgo_d969e426e2e3_Cfunc_TF_OperationInputType':
operation.cgo2.c:(.text+0x1d5): undefined reference to `TF_OperationInputType'
/tmp/go-link-707261891/000004.o: In function `_cgo_d969e426e2e3_Cfunc_TF_OperationName':
operation.cgo2.c:(.text+0x226): undefined reference to `TF_OperationName'
/tmp/go-link-707261891/000004.o: In function `_cgo_d969e426e2e3_Cfunc_TF_OperationNumInputs':
operation.cgo2.c:(.text+0x27a): undefined reference to `TF_OperationNumInputs'
/tmp/go-link-707261891/000004.o: In function `_cgo_d969e426e2e3_Cfunc_TF_OperationNumOutputs':
operation.cgo2.c:(.text+0x2cb): undefined reference to `TF_OperationNumOutputs'
/tmp/go-link-707261891/000004.o: In function `_cgo_d969e426e2e3_Cfunc_TF_OperationOpType':
operation.cgo2.c:(.text+0x31c): undefined reference to `TF_OperationOpType'
/tmp/go-link-707261891/000004.o: In function `_cgo_d969e426e2e3_Cfunc_TF_OperationOutputConsumers':
operation.cgo2.c:(.text+0x384): undefined reference to `TF_OperationOutputConsumers'
/tmp/go-link-707261891/000004.o: In function `_cgo_d969e426e2e3_Cfunc_TF_OperationOutputListLength':
operation.cgo2.c:(.text+0x3e8): undefined reference to `TF_OperationOutputListLength'
/tmp/go-link-707261891/000004.o: In function `_cgo_d969e426e2e3_Cfunc_TF_OperationOutputNumConsumers':
operation.cgo2.c:(.text+0x43e): undefined reference to `TF_OperationOutputNumConsumers'
/tmp/go-link-707261891/000004.o: In function `_cgo_d969e426e2e3_Cfunc_TF_OperationOutputType':
operation.cgo2.c:(.text+0x494): undefined reference to `TF_OperationOutputType'
/tmp/go-link-707261891/000005.o: In function `_cgo_d969e426e2e3_Cfunc_TF_LoadSessionFromSavedModel':
saved_model.cgo2.c:(.text+0xa2): undefined reference to `TF_LoadSessionFromSavedModel'
/tmp/go-link-707261891/000006.o: In function `_cgo_d969e426e2e3_Cfunc_TF_CloseSession':
session.cgo2.c:(.text+0x60): undefined reference to `TF_CloseSession'
/tmp/go-link-707261891/000006.o: In function `_cgo_d969e426e2e3_Cfunc_TF_DeleteDeviceList':
session.cgo2.c:(.text+0x85): undefined reference to `TF_DeleteDeviceList'
/tmp/go-link-707261891/000006.o: In function `_cgo_d969e426e2e3_Cfunc_TF_DeletePRunHandle':
session.cgo2.c:(.text+0xaa): undefined reference to `TF_DeletePRunHandle'
/tmp/go-link-707261891/000006.o: In function `_cgo_d969e426e2e3_Cfunc_TF_DeleteSession':
session.cgo2.c:(.text+0xda): undefined reference to `TF_DeleteSession'
/tmp/go-link-707261891/000006.o: In function `_cgo_d969e426e2e3_Cfunc_TF_DeleteSessionOptions':
session.cgo2.c:(.text+0xff): undefined reference to `TF_DeleteSessionOptions'
/tmp/go-link-707261891/000006.o: In function `_cgo_d969e426e2e3_Cfunc_TF_DeviceListCount':
session.cgo2.c:(.text+0x12d): undefined reference to `TF_DeviceListCount'
/tmp/go-link-707261891/000006.o: In function `_cgo_d969e426e2e3_Cfunc_TF_DeviceListMemoryBytes':
session.cgo2.c:(.text+0x18f): undefined reference to `TF_DeviceListMemoryBytes'
/tmp/go-link-707261891/000006.o: In function `_cgo_d969e426e2e3_Cfunc_TF_DeviceListName':
session.cgo2.c:(.text+0x1f4): undefined reference to `TF_DeviceListName'
/tmp/go-link-707261891/000006.o: In function `_cgo_d969e426e2e3_Cfunc_TF_DeviceListType':
session.cgo2.c:(.text+0x259): undefined reference to `TF_DeviceListType'
/tmp/go-link-707261891/000006.o: In function `_cgo_d969e426e2e3_Cfunc_TF_NewSession':
session.cgo2.c:(.text+0x2c0): undefined reference to `TF_NewSession'
/tmp/go-link-707261891/000006.o: In function `_cgo_d969e426e2e3_Cfunc_TF_NewSessionOptions':
session.cgo2.c:(.text+0x30f): undefined reference to `TF_NewSessionOptions'
/tmp/go-link-707261891/000006.o: In function `_cgo_d969e426e2e3_Cfunc_TF_SessionListDevices':
session.cgo2.c:(.text+0x36d): undefined reference to `TF_SessionListDevices'
/tmp/go-link-707261891/000006.o: In function `_cgo_d969e426e2e3_Cfunc_TF_SessionPRun':
session.cgo2.c:(.text+0x428): undefined reference to `TF_SessionPRun'
/tmp/go-link-707261891/000006.o: In function `_cgo_d969e426e2e3_Cfunc_TF_SessionPRunSetup':
session.cgo2.c:(.text+0x4a5): undefined reference to `TF_SessionPRunSetup'
/tmp/go-link-707261891/000006.o: In function `_cgo_d969e426e2e3_Cfunc_TF_SessionRun':
session.cgo2.c:(.text+0x54a): undefined reference to `TF_SessionRun'
/tmp/go-link-707261891/000006.o: In function `_cgo_d969e426e2e3_Cfunc_TF_SetConfig':
session.cgo2.c:(.text+0x590): undefined reference to `TF_SetConfig'
/tmp/go-link-707261891/000006.o: In function `_cgo_d969e426e2e3_Cfunc_TF_SetTarget':
session.cgo2.c:(.text+0x5c0): undefined reference to `TF_SetTarget'
/tmp/go-link-707261891/000007.o: In function `_cgo_d969e426e2e3_Cfunc_TF_DeleteStatus':
status.cgo2.c:(.text+0x55): undefined reference to `TF_DeleteStatus'
/tmp/go-link-707261891/000007.o: In function `_cgo_d969e426e2e3_Cfunc_TF_GetCode':
status.cgo2.c:(.text+0x83): undefined reference to `TF_GetCode'
/tmp/go-link-707261891/000007.o: In function `_cgo_d969e426e2e3_Cfunc_TF_Message':
status.cgo2.c:(.text+0xd4): undefined reference to `TF_Message'
/tmp/go-link-707261891/000007.o: In function `_cgo_d969e426e2e3_Cfunc_TF_NewStatus':
status.cgo2.c:(.text+0x123): undefined reference to `TF_NewStatus'
/tmp/go-link-707261891/000008.o: In function `_cgo_d969e426e2e3_Cfunc_TF_AllocateTensor':
tensor.cgo2.c:(.text+0x73): undefined reference to `TF_AllocateTensor'
/tmp/go-link-707261891/000008.o: In function `_cgo_d969e426e2e3_Cfunc_TF_DeleteTensor':
tensor.cgo2.c:(.text+0xbe): undefined reference to `TF_DeleteTensor'
/tmp/go-link-707261891/000008.o: In function `_cgo_d969e426e2e3_Cfunc_TF_Dim':
tensor.cgo2.c:(.text+0xf5): undefined reference to `TF_Dim'
/tmp/go-link-707261891/000008.o: In function `_cgo_d969e426e2e3_Cfunc_TF_NumDims':
tensor.cgo2.c:(.text+0x149): undefined reference to `TF_NumDims'
/tmp/go-link-707261891/000008.o: In function `_cgo_d969e426e2e3_Cfunc_TF_StringDecode':
tensor.cgo2.c:(.text+0x1bd): undefined reference to `TF_StringDecode'
/tmp/go-link-707261891/000008.o: In function `_cgo_d969e426e2e3_Cfunc_TF_StringEncode':
tensor.cgo2.c:(.text+0x234): undefined reference to `TF_StringEncode'
/tmp/go-link-707261891/000008.o: In function `_cgo_d969e426e2e3_Cfunc_TF_StringEncodedSize':
tensor.cgo2.c:(.text+0x288): undefined reference to `TF_StringEncodedSize'
/tmp/go-link-707261891/000008.o: In function `_cgo_d969e426e2e3_Cfunc_TF_TensorByteSize':
tensor.cgo2.c:(.text+0x2dc): undefined reference to `TF_TensorByteSize'
/tmp/go-link-707261891/000008.o: In function `_cgo_d969e426e2e3_Cfunc_TF_TensorData':
tensor.cgo2.c:(.text+0x330): undefined reference to `TF_TensorData'
/tmp/go-link-707261891/000008.o: In function `_cgo_d969e426e2e3_Cfunc_TF_TensorType':
tensor.cgo2.c:(.text+0x384): undefined reference to `TF_TensorType'
/tmp/go-link-707261891/000009.o: In function `_cgo_d969e426e2e3_Cfunc_TF_Version':
version.cgo2.c:(.text+0x59): undefined reference to `TF_Version'
collect2: error: ld returned 1 exit status
"
23648,run error when i use xla,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos 7.0
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version:  master branch and commit id is b300c78a889aa75b04f6bcff2a6a5fb5d0976b4d
- Python version: 3.6
- Installed using virtualenv? pip? conda?: no
- Bazel version (if compiling from source): 0.18.0
- GCC/Compiler version (if compiling from source): gcc 4.9.2
- CUDA/cuDNN version: no
- GPU model and memory: 131G



**Describe the problem**
The below is my building step:
1. bazel build --jobs 32  -c opt -c dbg --strip=never  //tensorflow/tools/pip_package:build_pip_package
or bazel build --jobs 32  -c opt -c dbg //tensorflow/tools/pip_package:build_pip_package

2. ./bazel-bin/tensorflow/tools/pip_package/build_pip_package ./tmp/tensorflow_pkg
3. pip3.6 install ./tensorflow-1.12.0rc0-cp36-cp36m-linux_x86_64.whl

But when i run the code with command：
TF_XLA_FLAGS=""--xla_hlo_graph_path=/tmp --xla_generate_hlo_graph=.*"" python3.6 mnist_softmax_xla.py
i got an error:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/local/lib64/python3.6/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/local/lib64/python3.6/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow19XlaCompilationCache28kDefaultCompilationThresholdE

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""mnist_softmax_xla.py"", line 26, in <module>
    import tensorflow as tf
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/local/lib64/python3.6/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/local/lib64/python3.6/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow19XlaCompilationCache28kDefaultCompilationThresholdE

it is just like kDefaultCompilationThreshold dose not existed in tensorflow_framework.so

**Provide the exact sequence of commands / steps that you executed before running into the problem**
check the above 

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23646,tf.scatter_nd_update gradient support ,"It seems that tf.scatter_nd_update does not support gradient back-propagation. Then in essence, it would be really hard to benefit from it in real applications where one needs to connect different components and back-propagate the error. Is there any possibility to provide the gradient support ?  "
23643,Mapping to a shuffled dataset yields an unmatched dataset in order.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution : Linux Ubuntu 16.04.5 LTS (Xenial Xerus)
- Mobile device if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v1.10.1-0-g4dcfddc
- Python version: Python 3.6.5
- Bazel version (if compiling from source): 0.16.1
- GCC/Compiler version (if compiling from source): 5.4.0-6ubuntu1~16.04.10
- CUDA/cuDNN version: V9.0.176/7.1.4.18
- GPU model and memory: GeForce GTX 1080 Ti with 12GB memory

**Describe the current behavior**
datasetB does not correspond to the order of shuffled dataset A.

**Describe the expected behavior**
datasetB is expected to correspond to the order of shuffled dataset A.

**Code to reproduce the issue**
The code below
```
import tensorflow as tf
from tensorflow.data import Dataset
tf.enable_eager_execution()

dataset_A = Dataset.range(10).shuffle(10)
dataset_B = dataset_A.map(lambda x: x * 2)
dataset = Dataset.zip((dataset_A, dataset_B))
for data_A, data_B in dataset:
    print(data_A, data_B)
```
produces a unmatched dataset in order.
```
tf.Tensor(1, shape=(), dtype=int64) tf.Tensor(8, shape=(), dtype=int64)
tf.Tensor(8, shape=(), dtype=int64) tf.Tensor(14, shape=(), dtype=int64)
tf.Tensor(3, shape=(), dtype=int64) tf.Tensor(12, shape=(), dtype=int64)
tf.Tensor(6, shape=(), dtype=int64) tf.Tensor(16, shape=(), dtype=int64)
tf.Tensor(0, shape=(), dtype=int64) tf.Tensor(18, shape=(), dtype=int64)
tf.Tensor(2, shape=(), dtype=int64) tf.Tensor(2, shape=(), dtype=int64)
tf.Tensor(4, shape=(), dtype=int64) tf.Tensor(10, shape=(), dtype=int64)
tf.Tensor(7, shape=(), dtype=int64) tf.Tensor(4, shape=(), dtype=int64)
tf.Tensor(9, shape=(), dtype=int64) tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor(5, shape=(), dtype=int64) tf.Tensor(6, shape=(), dtype=int64)
```"
23641,Calculation precision issue ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows with Tensorflow Docker 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:no
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.12
- Python version:2.7
- Bazel version (if compiling from source):na
- GCC/Compiler version (if compiling from source):na
- CUDA/cuDNN version:9, 7.3.1
- GPU model and memory:2080,8G 


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
import tensorflow as tf

def compute_area(sides):
  a = sides[:,0]  # 5.0, 2.3
  b = sides[:,1]  # 3.0, 4.1
  c = sides[:,2]  # 7.1, 4.8
  
  s = (a + b + c) * 0.5   # (a + b) is a short-cut to tf.add(a, b)
  areasq = s * (s - a) * (s - b) * (s - c) # (a * b) is a short-cut to tf.multiply(a, b), not tf.matmul(a, b)
  return tf.sqrt(areasq)

with tf.Session() as sess:
  area = compute_area(tf.constant([
      [3.000000001, 4.0, 5.0],
    ],dtype='float64'))
  result = sess.run(area)
  print(result)  

[6.]

**Describe the expected behavior**
import numpy as np

s = (3.000000001 + 4 + 5) * 0.5
areasq = s * (s - 3.000000001) * (s - 4) * (s - 5)

print(np.sqrt(areasq))

6.000000001999999

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Attached as above 

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

N/A 
"
23639,`var_list` will cause untrainable variables to be trained,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): below
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Mac 10.13
- TensorFlow installed from (source or binary): `pip`
- TensorFlow version (use command below): `v1.9.0-0-g25c197e023 1.9.0`
- Python version: `3.6.5`


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

```v1.9.0-0-g25c197e023 1.9.0```

**Describe the current behavior**
Both trainable and untrainable variables are updated by the optimizer

**Describe the expected behavior**
Expected: trainable variables are updated by optimizer, but untrainable variables are not.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import tensorflow as tf

x = tf.Variable([1.0], trainable=False)
y = tf.Variable([1.0], trainable=True)
z = x + y

train_op = tf.train.GradientDescentOptimizer(0.1).minimize(z, var_list=[x, y])
sess = tf.Session()
sess.run(tf.global_variables_initializer())
sess.run(train_op)

print(sess.run(x))  # [0.9]  Unexpected
print(sess.run(y))  # [0.9]  Expected
```
"
23638,OSError and ImportError ,"####Solved ########
**System information**
- OS Platform and Distribution: Windows 8.1
- TensorFlow installed from (source or binary): https://pypi.org/project/tensorflow-gpu/1.5.0/#files 
- TensorFlow version: 1.5.0
- Python version:3.6.5.final.0
- Installed using virtualenv? pip? conda?: installed on own computer with pip command 
- Bazel version (if compiling from source): (?)
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: v9.0/v7.4.1 (Nov 8, 2018), for CUDA 9.0
- GPU model and memory: GeForce GTX 960M 6MB 



**Describe the problem**
The problem seems to be that when i download the tensorflow-gpu, I do now have the tensorflow module do I also need to install tensorflow sepratly, tought that will destroy the setup. 
Also the environment does not recognize the cudart64_90.dll file in the folder described provided in the additional information

followed description from https://www.youtube.com/watch?v=uIm3DMprk7M 
- first installing CUDA v9.0 from https://developer.nvidia.com/cuda-90-download-archive
- then installing cuDNN version 7.4.1 for CUDA 9.0  from https://developer.nvidia.com/rdp/cudnn-download 
- then i downloaded tensorflow-gpu from https://pypi.org/project/tensorflow-gpu/1.5.0/?fbclid=IwAR3xogCL0DA4kdhIqLGCcXSYUka9n0CtBCxbpWzAh-rKrubLKQbJPIla8kA#files 
- lastly I opend python in anaconda navigator, then tried to import tensorflow 
output is printed below.

additional information: 
- cudart64_90.dll is located in the folder C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\bin . 
- comands are done on a new environment with following packages 
absl-py                   0.6.1
bleach                    1.5.0
certifi                   2018.10.15
html5lib                  0.9999999
Markdown                  3.0.1
numpy                     1.15.4
pip                       18.1
protobuf                  3.6.1
python                    3.6.7
setuptools                40.5.0
six                       1.11.0
tensorflow-gpu            1.5.0
tensorflow-tensorboard    1.5.1
vc                        14.1
vs2015_runtime            14.15.26706
Werkzeug                  0.14.1
wheel                     0.32.2
wincertstore              0.2

**Any other info / logs**
>>> import tensorflow
Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\envs\aienv1\lib\site-packages\tensorflow\python
\platform\self_check.py"", line 75, in preload_check
    ctypes.WinDLL(build_info.cudart_dll_name)
  File ""C:\ProgramData\Anaconda3\envs\aienv1\lib\ctypes\__init__.py"", line 348,
in __init__
    self._handle = _dlopen(self._name, mode)
OSError: [WinError 126] The specified module could not be found

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\ProgramData\Anaconda3\envs\aienv1\lib\site-packages\tensorflow\__init
__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\ProgramData\Anaconda3\envs\aienv1\lib\site-packages\tensorflow\python
\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\ProgramData\Anaconda3\envs\aienv1\lib\site-packages\tensorflow\python
\pywrap_tensorflow.py"", line 30, in <module>
    self_check.preload_check()
  File ""C:\ProgramData\Anaconda3\envs\aienv1\lib\site-packages\tensorflow\python
\platform\self_check.py"", line 82, in preload_check
    % (build_info.cudart_dll_name, build_info.cuda_version_number))
ImportError: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL
 be installed in a directory that is named in your %PATH% environment variable.
Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-t
oolkit"
23635,TensorFlow Keras Fit Bug Value Error: Dimension -3 must be >= 0,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
I have written custom code, albeit based off of code from TFLearn.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
This issue is untested on mobile devices.
- TensorFlow installed from (source or binary):
TensorFlow installed from binary.
- TensorFlow version (use command below):
TensorFlow Version: 1.11 
- Python version:
Python Version: 3.6.6
- Bazel version (if compiling from source):
Not Compiled from source
- GCC/Compiler version (if compiling from source):
Not Compiled from source
- CUDA/cuDNN version:
CUDA/cuDNN version 9.0
- GPU model and memory:
NVIDIA GeForce MX150, approx total memory 10097 MB


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
When I run the following program, which is supposed to be able to differentiate between images of Cats and Dogs using a convolution model, the program gets stuck at the fit method. It is returning a Value Error: Dimension -3 must be >= 0. I believe all the information in the fit method is correct...

**Describe the expected behavior**
It should complete the training of the model. It has not yet been set up to save it.


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
File is attached.



**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Here is the trace:

2018-11-09 11:45:48.096411: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
Traceback (most recent call last):
  File ""C:/Users/gman7/Documents/BIF/Tensor Flow/Programs/Java Examples/Cats vs Dogs/CvD.py"", line 128, in <module>
    model.fit(X_train, Y_train, epochs=2, steps_per_epoch=500)
  File ""C:\Users\gman7\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1509, in fit
    validation_split=validation_split)
  File ""C:\Users\gman7\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 993, in _standardize_user_data
    class_weight, batch_size)
  File ""C:\Users\gman7\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1029, in _standardize_weights
    self._set_inputs(x)
  File ""C:\Users\gman7\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\training\checkpointable\base.py"", line 426, in _method_wrapper
    method(self, *args, **kwargs)
  File ""C:\Users\gman7\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1222, in _set_inputs
    self.build(input_shape=input_shape)
  File ""C:\Users\gman7\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\engine\sequential.py"", line 224, in build
    shape = layer.compute_output_shape(shape)
  File ""C:\Users\gman7\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\layers\convolutional.py"", line 230, in compute_output_shape
    [self.filters])
  File ""C:\Users\gman7\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\tensor_shape.py"", line 542, in __init__
    self._dims = [as_dimension(d) for d in dims_iter]
  File ""C:\Users\gman7\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\tensor_shape.py"", line 542, in <listcomp>
    self._dims = [as_dimension(d) for d in dims_iter]
  File ""C:\Users\gman7\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\tensor_shape.py"", line 482, in as_dimension
    return Dimension(value)
  File ""C:\Users\gman7\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\tensor_shape.py"", line 42, in __init__
    raise ValueError(""Dimension %d must be >= 0"" % self._value)
ValueError: Dimension -3 must be >= 0

Process finished with exit code 1
"
23634,Incorrect loss calculation after v1.10: fitting tf.Dataset in Keras.,"In 1.12, we can pass `tf.Dataset` into our Keras models. For instance, I'm modelling a binary classification with high class imbalance. 

```
def _parse_func(record):
    keys_to_features = {} # some schema

    parsed = tf.parse_single_example(record, keys_to_features)

    # Build the X output
    x = parsed['x']  
    y = parsed['y']
    w = parsed['w'] * DEBUG_WEIGHT

    return x, y, w
```
where `DEBUG_WEIGHT` is changed from 1 to 10000, with no affect on the loss.
Create `tf.Dataset`

```
def input_fn(f, b=BATCH_SIZE):
    """"""
    :param f: A list of file names e.g. ['data-0.tf-record', 'data-1.tf-record'] to read
    :param b: Batch size, defaults to BATCH_SIZE in hparams.py
    :return: And infinitely iterable data set using tf records of tf.data.Dataset class
    """"""
    d = tf.data.TFRecordDataset(filenames=f)
    d = d.map(map_func=_parse_func)
    d = d.repeat()
    d = d.batch(batch_size=b)
    return d
```

But when I fit the model using Keras, no matter what weight is passed, it seems ignored.

E.g.

```
    train_data = input_fn(f=['path_to_tf_records'])

    params = {
        'loss': 'binary_crossentropy',
        'optimizer': optimizer,
        'metrics': metrics,
    }
    model.compile(**params)

    model.fit(x=train_data.make_one_shot_iterator(),
              epochs=50,
              steps_per_epoch=10000)
```
Interestingly, if I pass `class_weight` to the `fit`, then I get the message that both `sample_weights` and `class_weight` are provided. So why doesn't the loss change?

The loss is incredibly small and it doesn't change with weight.

Epoch 1/30
2018-11-09 16:24:10.605651: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
 461/9375 [>.............................] - ETA: 2:01:50 - loss: 53.4499 - binary_accuracy: 0.8183 - binary_crossentropy: 0.4290 - mean_absolute_error: 0.3171


## Sidenote
When I used numpy arrays with `tf.keras.utils.Sequence` class as a generator, using `fit_generator`, the loss was also not changing properly. 


### Takeaways

Clearly, the weight is not passed to the loss function correctly. Even if I use `fit_generator()` the loss differs in tensor flow > 1.10. Additionally, workers must be set to 0 i.e. `workers=0` for the generator to work which is incredibly slow during training."
23633,Distributed training with TF under BSP does not strictly follow the synchrnous rule,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no experiments on mobile devices
- TensorFlow installed from (source or binary): source 
- TensorFlow version (use command below): r1.10
- Python version: 2.7
- Bazel version (if compiling from source): 0.16.1
- GCC/Compiler version (if compiling from source): GCC
- CUDA/cuDNN version: 9.0
- GPU model and memory: ??


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
The distributed training is conducted with one PS and three workers. The replica_sync is True.  I deliberately slow down one worker (add some sleep in the program of the worker), then I find the other workers can surpass the slow worker by 1~3 iterations 
**Describe the expected behavior**
Since TF follows BSP (it does not support SSP, right?), these workers should be always  under the same iteration.  When slow worker still stays in Iteration 1, the other workers cannot go to Iteration 3 or further. 
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Any Distributed training program should be okay. Just add some sleep in the python code or the rdma.cc file
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23632,Incorrect masking in keras.backend.rnn,"**System information**
```
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
('v1.12.0-rc0-2215-g6735dd4799', '1.13.0-dev20181109')
```

**Describe the current behavior**
For correct masking of `output` it is required that `output == states[0]` for  `keras.backend.rnn(..., unroll=False)`, not so with `unroll=True` or in latest version of [keras-team/keras](https://github.com/keras-team/keras). See [this line](https://github.com/tensorflow/tensorflow/blob/d703f64e452f49603a244978b13239ab7aac0168/tensorflow/python/keras/backend.py#L3496) for the reason.

**Describe the expected behavior**
As per this [PR](https://github.com/keras-team/keras/pull/11499) (and discussions in linked issues) it has been established that output should be independent of states. That is, it should _not_ be assumed that `output == states[0]` for `ouput, states = step_function(inputs, previous_states)` in the `keras.backend.rnn` implementation. For details see test below.

**Code to reproduce the issue**
See test below (basically same as [this test added keras-team/keras](https://github.com/keras-team/keras/pull/11499/files#diff-e942014d73bf67b47a3b55f7f7041797R829))

```python
import numpy as np
from tensorflow import keras

def test_rnn_output_and_state_masking_independent():
    num_samples = 2
    num_timesteps = 4
    state_and_io_size = 5
    mask_last_num_timesteps = 2  # for second sample only

    # a step function that just outputs inputs,
    # but increments states +1 per timestep
    def step_function(inputs, states):
        return inputs, [s + 1 for s in states]

    inputs_vals = np.random.random(
        (num_samples, num_timesteps, state_and_io_size))
    initial_state_vals = np.random.random((num_samples, state_and_io_size))
    # masking of two last timesteps for second sample only
    mask_vals = np.ones((num_samples, num_timesteps))
    mask_vals[1, -mask_last_num_timesteps:] = 0

    # outputs expected to be same as inputs for the first sample
    expected_outputs = inputs_vals.copy()
    # but for the second sample all outputs in masked region should be the same
    # as last output before masked region
    expected_outputs[1, -mask_last_num_timesteps:] = \
        expected_outputs[1, -(mask_last_num_timesteps + 1)]

    expected_state = initial_state_vals.copy()
    # first state should be incremented for every timestep (no masking)
    expected_state[0] += num_timesteps
    # second state should not be incremented for last two timesteps
    expected_state[1] += (num_timesteps - mask_last_num_timesteps)

    # verify same expected output for `unroll=true/false`
    inputs = keras.backend.variable(inputs_vals)
    initial_states = [keras.backend.variable(initial_state_vals)]
    mask = keras.backend.variable(mask_vals)
    for unroll in [True, False]:
        last_output, outputs, last_states = keras.backend.rnn(
            step_function,
            inputs,
            initial_states,
            mask=mask,
            unroll=unroll,
            input_length=num_timesteps if unroll else None)

        np.testing.assert_allclose(
            keras.backend.eval(outputs), expected_outputs,
            err_msg=""Unexpected output for unroll={}"".format(unroll))
        np.testing.assert_allclose(
            keras.backend.eval(last_states[0]), expected_state,
            err_msg=""Unexpected state for unroll={}"".format(unroll))
```
Gives:
```
AssertionError: 
Not equal to tolerance rtol=1e-07, atol=0
Unexpected output for unroll=False
(mismatch 25.0%)
 x: array([0.116687, 0.622734, 0.210443, 0.662715, 0.720813, 0.654062,
       0.936728, 0.451018, 0.471044, 0.560336, 0.133492, 0.228378,
       2.081284, 2.415464, 2.081284, 2.415464], dtype=float32)
 y: array([0.116687, 0.622734, 0.210443, 0.662715, 0.720813, 0.654062,
       0.936728, 0.451018, 0.471044, 0.560336, 0.133492, 0.228378,
       0.133492, 0.228378, 0.133492, 0.228378])
```
**Other info / logs**
There are a few further implications of current implementation addressed by the tests added to keras-team/keras [here](https://github.com/keras-team/keras/pull/11499/files#diff-e942014d73bf67b47a3b55f7f7041797R829))"
23631,Default argument of tf.placeholder,"InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'x' with dtype float
	 [[{{node x}} = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

I think  tf.placeholder(tf.float32, shape=None, name='x') is equivalent to  tf.placeholder(tf.float32,  name='x')? But the results are different for these two lines of code.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution Windows10:
- TensorFlow installed from pip:
- TensorFlow version (use command below): 1.11.0
- Python version: 3.6
- CUDA/cuDNN version: 9.0
- GPU model and memory: 1080Ti



**Code to reproduce the issue**
```python
x = tf.placeholder(tf.float32, shape=None, name='x')
y = tf.placeholder(tf.float32, shape=None, name='y')
with tf.Session() as sess:
    total_loss = 0
    sess.run(tf.global_variables_initializer())
    _, loss_ = sess.run([optimizer, loss], feed_dict={x: 1.0, y: 2.0})
````
The code presented above returned InvalidArgumentError.

```python
x = tf.placeholder(tf.float32, name='x')
y = tf.placeholder(tf.float32, name='y')
with tf.Session() as sess:
    total_loss = 0
    sess.run(tf.global_variables_initializer())
    _, loss_ = sess.run([optimizer, loss], feed_dict={x: 1.0, y: 2.0})
````
These code works well.

"
23630,The output of trained model is constant when using batch normalization,"I have trained a model with tensorflow, with the use of batch normalization.

```
conv2 = slim.conv2d(
inputs=pool1,
num_outputs=64,
kernel_size=[5, 5],
padding=""same"",
normalizer_fn=slim.batch_norm,
activation_fn=tf.nn.relu)
```
And the model has been trained well, I can see the loss curve has been falling in tensorboard, and the validation accuracy reached 99%+

Then I saved my model as ckpt files. But when I tried to load my model from ckpt files and do some prediction, what ever I feed to the model, the output is always an constant array.

Then I removed all bn layer, and train my model and save it as ckpt again.When I load it and do prediction, it works well.

Can anybody tell me why?
"
23629,Cannot compile tensorflow lite minimal.cc with libtensorflowlite.so,"**System information**
Ubuntu 16, armeabi-v7a, bazel 0.18, tried out with NDK 16b and 15c


**Describe the current behavior**

When I run this command to compile `tensorflow/lite/examples/minimal.cc` using a `libtensorflowlite.so` I built:

`<path-to>/android-toolchain-15c/bin/clang++  -std=c++11 -I<path-to>/git/tensorflow-android
-I<path-to>/git/flatbuffers/include -L<path-to>/git/tensorflow-android
-L<path-to>/git/flatbuffers/build minimal.cc -ltensorflowlite -lflatbuffers`

I get an error

`undefined reference to 'tflite::InterpreterBuilder::operator()(std::__ndk1::unique_ptr<tflite::Interpreter, std::__ndk1::default_delete<tflite::Interpreter> >*)'`

**Describe the expected behavior**

To compile.

**Code to reproduce the issue**

1. Download the android ndk. Call `build/tools/make_standalone_toolchain.py --arch arm --api 21 --stl=libc++ --install-dir android-toolchain`

2. Build `libtensorflowlite.so`, I added this to the BUILD file:

```
cc_binary(
    name = ""libtensorflowlite.so"",
    linkopts=[
        ""-shared"",
        ""-Wl,-soname=libtensorflowlite.so"",
    ],  
    linkshared = 1,
    copts = tflite_copts(),
    deps = [ 
        "":framework"",
        ""//tensorflow/lite/kernels:builtin_ops"",
    ],  
)
```
And then called 
```
bazel build //tensorflow/lite:libtensorflowlite.so --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=""-std=c++11""
```

3. Clone and compile the flatbuffer repo.

4. Then call the problematic command 
```
android-toolchain/bin/clang++  -std=c++11 -Igit/tensorflow-android -Igit/flatbuffers/include -Lgit/tensorflow-android -Lgit/flatbuffers/build minimal.cc -ltensorflowlite -lflatbuffers
```
And you should get the error: 
```undefined reference to 'tflite::InterpreterBuilder::operator()(std::__ndk1::unique_ptr<tflite::Interpreter, std::__ndk1::default_delete<tflite::Interpreter> >*)'```

"
23628,Caused by: java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model,"When I put the tflite file trained by the ssdmobilenet model into Android studio, the following error occurred. Who can help me? Below is my configuration information and wrong information.
![image](https://user-images.githubusercontent.com/20412297/48258352-d7456a00-e44f-11e8-8f24-4de8aa60ebf0.png)

![image](https://user-images.githubusercontent.com/20412297/48258389-f3490b80-e44f-11e8-8ea2-4bbf099763e7.png)
 
E/AndroidRuntime: FATAL EXCEPTION: main
    Process: org.tensorflow.lite.demo, PID: 20339
    java.lang.RuntimeException: java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model
        at org.tensorflow.demo.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:124)
        at org.tensorflow.demo.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:110)
        at org.tensorflow.demo.CameraActivity$5.onPreviewSizeChosen(CameraActivity.java:362)
        at org.tensorflow.demo.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:401)
        at org.tensorflow.demo.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:408)
        at org.tensorflow.demo.CameraConnectionFragment.access$000(CameraConnectionFragment.java:64)
        at org.tensorflow.demo.CameraConnectionFragment$1.onSurfaceTextureAvailable(CameraConnectionFragment.java:95)
        at android.view.TextureView.getHardwareLayer(TextureView.java:390)
        at android.view.TextureView.draw(TextureView.java:339)
        at android.view.View.updateDisplayListIfDirty(View.java:18168)
        at android.view.View.draw(View.java:18946)
        at android.view.ViewGroup.drawChild(ViewGroup.java:4238)
        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4024)
        at android.view.View.updateDisplayListIfDirty(View.java:18159)
        at android.view.View.draw(View.java:18946)
        at android.view.ViewGroup.drawChild(ViewGroup.java:4238)
        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4024)
        at android.view.View.draw(View.java:19221)
        at android.view.View.updateDisplayListIfDirty(View.java:18168)
        at android.view.View.draw(View.java:18946)
        at android.view.ViewGroup.drawChild(ViewGroup.java:4238)
        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4024)
        at android.view.View.updateDisplayListIfDirty(View.java:18159)
        at android.view.View.draw(View.java:18946)
        at android.view.ViewGroup.drawChild(ViewGroup.java:4238)
        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4024)
        at android.view.View.updateDisplayListIfDirty(View.java:18159)
        at android.view.View.draw(View.java:18946)
        at android.view.ViewGroup.drawChild(ViewGroup.java:4238)
        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4024)
        at android.view.View.draw(View.java:19221)
        at com.android.internal.policy.DecorView.draw(DecorView.java:791)
        at android.view.View.updateDisplayListIfDirty(View.java:18168)
        at android.view.ThreadedRenderer.updateViewTreeDisplayList(ThreadedRenderer.java:676)
        at android.view.ThreadedRenderer.updateRootDisplayList(ThreadedRenderer.java:682)
        at android.view.ThreadedRenderer.draw(ThreadedRenderer.java:790)
        at android.view.ViewRootImpl.draw(ViewRootImpl.java:3050)
        at android.view.ViewRootImpl.performDraw(ViewRootImpl.java:2845)
        at android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:2398)
        at android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:1431)
        at android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:6861)
        at android.view.Choreographer$CallbackRecord.run(Choreographer.java:1026)
        at android.view.Choreographer.doCallbacks(Choreographer.java:838)
        at android.view.Choreographer.doFrame(Choreographer.java:769)
        at android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:1012)
        at android.os.Handler.handleCallback(Handler.java:790)
        at android.os.Handler.dispatchMessage(Handler.java:99)
        at android.os.Looper.loop(Looper.java:164)
        at android.app.ActivityThread.main(ActivityThread.java:6650)
        at java.lang.reflect.Method.invoke(Native Method)
        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:547)
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:818)
     Caused by: java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model
        at org.tensorflow.lite.NativeInterpreterWrapper.createModelWithBuffer(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:59)
        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:188)
        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:176)
        at org.tensorflow.demo.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:122)
        	... 51 more
 Who can help me? Thanks!


"
23627,TFLiteConverter.from_saved_model - batchNorm is not supported? ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No, i have tried tensorflow example code snippet. 

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab (Linux bedbba52137a 4.14.65+ #1 SMP Sun Sep 9 02:18:33 PDT 2018 x86_64 x86_64 x86_64 GNU/Linux)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): pip on google colab, using following command 
```
pip3 install --upgrade tf-nightly

```
- TensorFlow version (use command below): version: 1.13.0-dev20181109
```
import tensorflow as tf 
print(tf.__version__)

```
- Python version: Python 3.6.6
- Bazel version (if compiling from source): None
- GCC/Compiler version (if compiling from source): None
- CUDA/cuDNN version: using command 'nvcc --version'
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2018 NVIDIA Corporation
Built on Tue_Jun_12_23:07:04_CDT_2018
Cuda compilation tools, release 9.2, V9.2.148

- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I have a [model here](https://drive.google.com/file/d/1mgjhsPpEDKs8Jud1wbtAKfjObAJVO8GL/view?usp=sharing), which is exported SavedModel using following code: 

```
# SavedModel using simple_save()

ins = {""phase_train_placeholder"":phase_train_placeholder}
outs = {""embeddings"":embeddings}
tf.saved_model.simple_save(sess, '/content/generated/', ins, outs)

```        

When i convert this SavedModel to TFLite it give me error, the code snippet is as: 

```
import tensorflow as tf

saved_model_dir = '/content/generated/'

converter = tf.contrib.lite.TFLiteConverter.from_saved_model(saved_model_dir, input_arrays=['phase_train'], input_shapes=(1,160,160,3), 
                                                             output_arrays=['embeddings'])
tflite_model = converter.convert()
open(""converted_model_savedModel.tflite"", ""wb"").write(tflite_model)
```

Following are error logs: 

```
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert_saved_model.py:61: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/queue_runner_impl.py:391: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
INFO:tensorflow:Restoring parameters from /content/generated/variables/variables
INFO:tensorflow:The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'serving_default'}
INFO:tensorflow:input tensors info: 
INFO:tensorflow:Tensor's key in saved_model's tensor_map: phase_train_placeholder
INFO:tensorflow: tensor name: phase_train:0, shape: unknown_rank, type: DT_BOOL
INFO:tensorflow:output tensors info: 
INFO:tensorflow:Tensor's key in saved_model's tensor_map: embeddings
INFO:tensorflow: tensor name: embeddings:0, shape: (-1, 512), type: DT_FLOAT
INFO:tensorflow:Restoring parameters from /content/generated/variables/variables
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-4-63a92824a047> in <module>()
      6 
      7 converter = tf.contrib.lite.TFLiteConverter.from_saved_model(saved_model_dir, input_arrays=['phase_train'], input_shapes=(1,160,160,3), 
----> 8                                                              output_arrays=['embeddings'])
      9 tflite_model = converter.convert()
     10 open(""converted_model_savedModel.tflite"", ""wb"").write(tflite_model)

/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in from_saved_model(cls, saved_model_dir, input_arrays, input_shapes, output_arrays, tag_set, signature_key)
    342 
    343     result = _freeze_saved_model(saved_model_dir, input_arrays, input_shapes,
--> 344                                  output_arrays, tag_set, signature_key)
    345     return cls(
    346         graph_def=result[0], input_tensors=result[1], output_tensors=result[2])

/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert_saved_model.py in freeze_saved_model(saved_model_dir, input_arrays, input_shapes, output_arrays, tag_set, signature_key)
    254     in_tensors = _get_tensors(graph, inputs, input_arrays)
    255     out_tensors = _get_tensors(graph, outputs, output_arrays)
--> 256     set_tensor_shapes(in_tensors, input_shapes)
    257 
    258     output_names = [node.split("":"")[0] for node in outputs]

/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert_saved_model.py in set_tensor_shapes(tensors, shapes)
    201   if shapes:
    202     for tensor in tensors:
--> 203       shape = shapes.get(tensor_name(tensor))
    204       if shape is not None:
    205         tensor.set_shape(shape)

AttributeError: 'tuple' object has no attribute 'get'
```

**Updates on 10-Nov-2018**

I have to give input_shape as dictionary in the following way:

```
import tensorflow as tf

saved_model_dir = '/content/generated/'

converter = tf.contrib.lite.TFLiteConverter.from_saved_model(saved_model_dir, input_arrays=['phase_train'], input_shapes={""phase_train"":[1,160,160,3]}, output_arrays=['embeddings'])

tflite_model = converter.convert()
open(""converted_model_savedModel.tflite"", ""wb"").write(tflite_model) 
```

This fixed the earlier error but now i see a different error and the logs are below: 

```
INFO:tensorflow:Restoring parameters from /content/generated/variables/variables
INFO:tensorflow:The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'serving_default'}
INFO:tensorflow:input tensors info: 
INFO:tensorflow:Tensor's key in saved_model's tensor_map: phase_train_placeholder
INFO:tensorflow: tensor name: phase_train:0, shape: unknown_rank, type: DT_BOOL
INFO:tensorflow:output tensors info: 
INFO:tensorflow:Tensor's key in saved_model's tensor_map: embeddings
INFO:tensorflow: tensor name: embeddings:0, shape: (-1, 512), type: DT_FLOAT
INFO:tensorflow:Restoring parameters from /content/generated/variables/variables
INFO:tensorflow:Froze 490 variables.
INFO:tensorflow:Converted 490 variables to const ops.
---------------------------------------------------------------------------
ConverterError                            Traceback (most recent call last)
<ipython-input-53-91d1899f3204> in <module>()
      8 converter = tf.contrib.lite.TocoConverter.from_saved_model(saved_model_dir, input_arrays=['phase_train'], input_shapes={""phase_train"":[1,160,160,3]}, 
      9                                                    output_arrays=['embeddings'])
---> 10 tflite_model = converter.convert()

/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in convert(self)
    454           input_tensors=self._input_tensors,
    455           output_tensors=self._output_tensors,
--> 456           **converter_kwargs)
    457     else:
    458       result = _toco_convert_graph_def(

/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, *args, **kwargs)
    395   data = toco_convert_protos(model_flags.SerializeToString(),
    396                              toco_flags.SerializeToString(),
--> 397                              input_data.SerializeToString())
    398   return data
    399 

/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str)
    170       stderr = _try_convert_to_unicode(stderr)
    171       raise ConverterError(
--> 172           ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
    173   finally:
    174     # Must manually cleanup files.

ConverterError: TOCO failed. See console for info.
2018-11-11 08:46:00.208147: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: FIFOQueueV2
2018-11-11 08:46:00.216527: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2018-11-11 08:46:00.216572: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: QueueDequeueUpToV2
2018-11-11 08:46:00.216749: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: RefSwitch
2018-11-11 08:46:00.216793: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: AssignSub
2018-11-11 08:46:00.216846: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: RefSwitch

....... logs dropped here 

2018-11-11 08:46:00.291969: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: RefSwitch
2018-11-11 08:46:00.292018: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: AssignSub
2018-11-11 08:46:00.292076: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: RefSwitch
2018-11-11 08:46:00.292113: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: AssignSub
2018-11-11 08:46:00.937387: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 5600 operators, 9398 arrays (0 quantized)
2018-11-11 08:46:01.526448: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 3582 operators, 6259 arrays (0 quantized)
2018-11-11 08:46:01.979950: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 3582 operators, 6259 arrays (0 quantized)
2018-11-11 08:46:01.982607: F tensorflow/lite/toco/graph_transformations/resolve_batch_normalization.cc:45] Check failed: IsConstantParameterArray(*model, bn_op->inputs[1]) && IsConstantParameterArray(*model, bn_op->inputs[2]) && IsConstantParameterArray(*model, bn_op->inputs[3]) Batch normalization resolution requires that mean, multiplier and offset arrays be constant.
Aborted (core dumped)

```

**Describe the expected behavior**
It should create *.lite file instead. 

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

I have given the [SavedModel](https://drive.google.com/file/d/1mgjhsPpEDKs8Jud1wbtAKfjObAJVO8GL/view?usp=sharing) and above code snippet to reproduce it. 

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23626,how to merge different .ckpt files into a .pf file,"
I had trained some  networks and saved them into  .ckpt file separately,  There is a connection between these networks, the output of the previous network is the input of the latter network, so here I want to merge these .ckpt files into a .pb file so I could deploy it very conveniently. But I didn't find the relevant tutorial. Had anyone done a similar job like that?"
23625,question about quantization,"Using quantization, with the same parameters, the difference of the training accuracy is about 5-10% each time？

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:pip
- **TensorFlow version (use command below)**:1.4
- **Python version**:3.4.3
- **Bazel version (if compiling from source)**:1.9
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
23624,question about quantization,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
23623,question about quantization,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
23622,"A bug of tfdbg : Non-OK-status: env->NewWritableFile(file_path, &f) status: Not found: Failed to create a NewWriteableFile:","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:no
- TensorFlow installed from (source or binary):pip install tensorflow
- TensorFlow version (use command below):1.10.0
- Python version: 3.6.5 
- Bazel version (if compiling from source):no
- GCC/Compiler version (if compiling from source):no
- CUDA/cuDNN version:no
- GPU model and memory:no

When I use tfdbg, I input ""run"" in the console, the error info  like this:

2018-11-09 17:07:18.381547: F T:\src\github\tensorflow\tensorflow\core\debug\debug_io_utils.cc:621] Non-OK-status: env->NewWritableFile(file_path, &f) status: Not found: Failed to create a NewWriteableFile: C:\Users\winter\AppData\Local\Temp\tfdbg_312ol3zm/_tfdbg_device_,job_localhost,replica_0,task_0,device_CPU_0/train_step/gradients/bi-lstm/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/Const_0_DebugIdentity_1541754438373045 : ϵͳ�Ҳ���ָ����·����
; No such process

Process finished with exit code -1073740791 (0xC0000409)
"
23621,tf.assign does not support gradient?,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.1 LTS
- TensorFlow version (use command below):1.11
- Python version:3.6
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:CUDA release 9.0, V9.0.176
- GPU model and memory: TITAN Xp / 12Gb


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
It seems like tf.assign does not support gradients. In the forward pass, everything seems to work ok. But when I try to do backpropogate the gradients,it does not work. The error specifies that there are no variables to optimize which is not certainly the case. 
**Describe the expected behavior**
tf.assign should simply allow for gradient to flow, just like its python counterpart Python assign operator.

**Code to reproduce the issue**

    import tensorflow as tf
    import numpy as np

    def func_foo(x):
 
        phi = tf.Variable(tf.zeros([1,10,10,1], dtype='float32'), 
                        dtype='float32',trainable=False)

        phi=tf.assign(phi,x)

        c3=tf.nn.sigmoid(phi)
        c4=tf.reduce_mean(c3)

    return 1-c4

    a = np.random.randint(2, size=(10,10))
    k = np.array([[1,1,1],[1,1,1],[1,1,1]],dtype=np.float32)
    flip = [slice(None, None, -1), slice(None, None, -1)]
    k = k[flip]

    a=a.astype(np.float32)
    a_tensor = tf.reshape(a, [1, 10, 10, 1])
    k_weight = tf.reshape(np.array(k), [3,3,1,1])

    c2=tf.layers.conv2d(a_tensor,filters=1, kernel_size=3, strides=1, padding=""same"",activation=tf.nn.relu) 
    total_loss=func_foo(c2)    
    train_op = tf.train.AdamOptimizer(1e-3).minimize(total_loss,colocate_gradients_with_ops=True)
    init = tf.initialize_all_variables()
    sess=tf.Session()
    with tf.Session() as sess:
        init = tf.initialize_all_variables()
        sess.run(init)
        _,=sess.run([train_op])``

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


Traceback (most recent call last):
  File ""test_tf2.py"", line 31, in <module>
    train_op = tf.train.AdamOptimizer(1e-3).minimize(total_loss,colocate_gradients_with_ops=True)
  File ""/home/ali/anaconda3/envs/tf19/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 406, in minimize
    ([str(v) for _, v in grads_and_vars], loss))
ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [""<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 1, 1) dtype=float32_ref>"", ""<tf.Variable 'conv2d/bias:0' shape=(1,) dtype=float32_ref>""] and loss Tensor(""sub:0"", shape=(), dtype=float32).

"
23619,tf.contrib.distributions.percentile didn't support eager execution,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):tensorflow-py2-cpu API r1.1
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
When I using the eager execution. I try this:

```
import tensorflow as tf

tf.enable_eager_execution()

x = tf.constant([1,2,3,4],dtype='float64')
y = tf.contrib.distributions.percentile(x,q=30)
```

will report an error:

> TypeErrorTraceback
> 
>  (most recent call last)
> <ipython-input-1-b7b170c83880> in <module>()
>       4 
>       5 x = tf.constant([1,2,3,4],dtype='float64')
> ----> 6 y = tf.contrib.distributions.percentile(x,q=30)
> 
> /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distributions/python/ops/sample_stats.pyc in percentile(x, q, axis, interpolation, keep_dims, validate_args, name)
>     301                        (allowed_interpolations, interpolation))
>     302 
> --> 303   with ops.name_scope(name, [x, q]):
>     304     x = ops.convert_to_tensor(x, name=""x"")
>     305     # Double is needed here and below, else we get the wrong index if the array
> 
> /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in __enter__(self)
>    5743       else:
>    5744         cache_key = self._name, self._old_name, self._default_name
> -> 5745         if cache_key in name_scope_cache:
>    5746           self._ctx.scope_name = name_scope_cache[cache_key]
>    5747           return self._ctx.scope_name
> 
> TypeError: unhashable type: 'list'

but when I use tf.Session() to run the same code block:

```
import tensorflow as tf
x = tf.constant([1,2,3,4],dtype='float64')
y = tf.contrib.distributions.percentile(x,q=30)
with tf.Session() as sess:
    print sess.run(y)

```
It worked and report that I wanted:
`2.0`

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**

I'm using tensorflow to make some calculate. Eager is very help but this problem stop me to use it.

Thanks for anyone will fix it."
23618,Build of tensorflow r1.12 fails on ubuntu 18.04,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): r1.12
- Python version: 3.6.5
- Bazel version (if compiling from source): 0.19.0
GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.3.0-16ubuntu3) 7.3.0
CUDA/cuDNN version: 10.0/7.4.1
GPU model and memory: NVIDIA GeForce 940MX
Exact command to reproduce: bazel build -c opt --config=cuda --config=gdr --config=mkl --config=ngraph --config=verbs  //tensorflow/tools/pip_package:build_pip_package --verbose_failures

**Describe the problem**
While building the tensorflow from the branch r1.12 with GPU support, the build fails with the following error:
ERROR: ~/Documents/dev/git/tensorflow/tensorflow/contrib/verbs/BUILD:105:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma_mgr' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command
The issue was reproducable in tensorflow 1.11, It was said that it will be fixed in 1.12 after the merge from master, but apparently this did not happen.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
./configure
bazel build -c opt --config=cuda --config=gdr --config=mkl --config=ngraph --config=verbs  //tensorflow/tools/pip_package:build_pip_package --verbose_failures

**Any other info / logs**
The following command line is available:
ERROR: ~/Documents/dev/git/tensorflow/tensorflow/contrib/verbs/BUILD:105:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma_mgr' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd ~/.cache/bazel/_bazel_username/cf67b2b2e967476eb2b1ee98e33ab5bd/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    NCCL_HDR_PATH=/usr/include \
    NCCL_INSTALL_PATH=/usr/lib/x86_64-linux-gnu \
    PATH=~/bin:/usr/local/sbin:/usr/local/lib:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/linuxbrew/.linuxbrew/bin:/home/linuxbrew/.linuxbrew/sbin:/home/linuxbrew/.linuxbrew/opt/coreutils/libexec/gnubin:/usr/local/cuda/bin:/usr/local/share/apache/hadoop/sbin:/usr/local/share/apache/hadoop/bin:/usr/local/share/apache/spark/sbin:/usr/local/share/apache/spark/bin:/usr/games:/usr/local/games:~/bin:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=5.0 \
    TF_CUDA_VERSION=10.0 \
    TF_CUDNN_VERSION=7 \
    TF_NCCL_VERSION=2 \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL_SYCL=0 \
    TF_NEED_ROCM=0 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/contrib/verbs/_objs/rdma_mgr/rdma_mgr.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/contrib/verbs/_objs/rdma_mgr/rdma_mgr.pic.o' '-DGRPC_ARES=0' '-DPB_FIELD_16BIT=1' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -DTF_USE_SNAPPY -DTENSORFLOW_USE_VERBS -DTENSORFLOW_USE_GDR -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DENABLE_NO_ENCRYPTION -iquote . -iquote bazel-out/k8-opt/genfiles -iquote bazel-out/k8-opt/bin -iquote external/protobuf_archive -iquote bazel-out/k8-opt/genfiles/external/protobuf_archive -iquote bazel-out/k8-opt/bin/external/protobuf_archive -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -iquote bazel-out/k8-opt/bin/external/bazel_tools -iquote external/grpc -iquote bazel-out/k8-opt/genfiles/external/grpc -iquote bazel-out/k8-opt/bin/external/grpc -iquote external/zlib_archive -iquote bazel-out/k8-opt/genfiles/external/zlib_archive -iquote bazel-out/k8-opt/bin/external/zlib_archive -iquote external/boringssl -iquote bazel-out/k8-opt/genfiles/external/boringssl -iquote bazel-out/k8-opt/bin/external/boringssl -iquote external/com_google_absl -iquote bazel-out/k8-opt/genfiles/external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/k8-opt/genfiles/external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/k8-opt/genfiles/external/local_config_sycl -iquote bazel-out/k8-opt/bin/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/k8-opt/genfiles/external/gif_archive -iquote bazel-out/k8-opt/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/k8-opt/genfiles/external/jpeg -iquote bazel-out/k8-opt/bin/external/jpeg -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/genfiles/external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/genfiles/external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/genfiles/external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/genfiles/external/highwayhash -iquote bazel-out/k8-opt/bin/external/highwayhash -iquote external/local_config_cuda -iquote bazel-out/k8-opt/genfiles/external/local_config_cuda -iquote bazel-out/k8-opt/bin/external/local_config_cuda -iquote external/double_conversion -iquote bazel-out/k8-opt/genfiles/external/double_conversion -iquote bazel-out/k8-opt/bin/external/double_conversion -iquote external/mkl_linux -iquote bazel-out/k8-opt/genfiles/external/mkl_linux -iquote bazel-out/k8-opt/bin/external/mkl_linux -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -iquote external/curl -iquote bazel-out/k8-opt/genfiles/external/curl -iquote bazel-out/k8-opt/bin/external/curl -iquote external/jsoncpp_git -iquote bazel-out/k8-opt/genfiles/external/jsoncpp_git -iquote bazel-out/k8-opt/bin/external/jsoncpp_git -iquote external/aws -iquote bazel-out/k8-opt/genfiles/external/aws -iquote bazel-out/k8-opt/bin/external/aws -isystem external/protobuf_archive/src -isystem bazel-out/k8-opt/genfiles/external/protobuf_archive/src -isystem bazel-out/k8-opt/bin/external/protobuf_archive/src -isystem external/grpc/include -isystem bazel-out/k8-opt/genfiles/external/grpc/include -isystem bazel-out/k8-opt/bin/external/grpc/include -isystem external/zlib_archive -isystem bazel-out/k8-opt/genfiles/external/zlib_archive -isystem bazel-out/k8-opt/bin/external/zlib_archive -isystem external/grpc/third_party/address_sorting/include -isystem bazel-out/k8-opt/genfiles/external/grpc/third_party/address_sorting/include -isystem bazel-out/k8-opt/bin/external/grpc/third_party/address_sorting/include -isystem external/boringssl/src/include -isystem bazel-out/k8-opt/genfiles/external/boringssl/src/include -isystem bazel-out/k8-opt/bin/external/boringssl/src/include -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/genfiles/third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/k8-opt/genfiles/external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/k8-opt/genfiles/external/gif_archive/lib -isystem bazel-out/k8-opt/bin/external/gif_archive/lib -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/genfiles/external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include/crt -isystem external/double_conversion -isystem bazel-out/k8-opt/genfiles/external/double_conversion -isystem bazel-out/k8-opt/bin/external/double_conversion -isystem external/mkl_linux/include -isystem bazel-out/k8-opt/genfiles/external/mkl_linux/include -isystem bazel-out/k8-opt/bin/external/mkl_linux/include -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/curl/include -isystem bazel-out/k8-opt/genfiles/external/curl/include -isystem bazel-out/k8-opt/bin/external/curl/include -isystem external/jsoncpp_git/include -isystem bazel-out/k8-opt/genfiles/external/jsoncpp_git/include -isystem bazel-out/k8-opt/bin/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/k8-opt/genfiles/external/aws/aws-cpp-sdk-core/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/k8-opt/genfiles/external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-kinesis/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/k8-opt/genfiles/external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-s3/include '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -mavx -mavx2 -mfma '-mfpmath=both' -msse4.2 -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-DGOOGLE_CUDA=1' '-DINTEL_MKL=1' -DEIGEN_USE_VML -DENABLE_MKL '-DINTEL_NGRAPH=1' -fopenmp -msse3 -pthread '-DGOOGLE_CUDA=1' '-DINTEL_MKL=1' -DENABLE_MKL -c tensorflow/contrib/verbs/rdma_mgr.cc -o bazel-out/k8-opt/bin/tensorflow/contrib/verbs/_objs/rdma_mgr/rdma_mgr.pic.o)
tensorflow/contrib/verbs/rdma_mgr.cc: In static member function 'static void tensorflow::RdmaMgr::RegMemVisitors()':
tensorflow/contrib/verbs/rdma_mgr.cc:282:40: error: invalid use of member 'tensorflow::RdmaMgr::rdma_adapter_' in static member function
     int32_t bus_id = TryToReadNumaNode(rdma_adapter_->context_->device) + 1;
                                        ^~~~~~~~~~~~~
In file included from tensorflow/contrib/verbs/rdma_mgr.cc:18:0:
./tensorflow/contrib/verbs/rdma_mgr.h:50:16: note: declared here
   RdmaAdapter* rdma_adapter_;
                ^~~~~~~~~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 4521.698s, Critical Path: 158.49s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]
INFO: 4797 processes: 4797 local.
FAILED: Build did NOT complete successfully"
23617,Build //tensorflow:libtensorflow.so error.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Linux Ubuntu 16.04:
- TensorFlow installed from (source or binary):source
- TensorFlow version:r1.8.0
- Python version:3.5
- Bazel version (if compiling from source):0.17.2
- GCC/Compiler version (if compiling from source):gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.5)



**Describe the problem**
when I try to build this:
```bash
bazel build -c opt //tensorflow:libtensorflow.so
```
got following errors:
```bash
/usr/bin/ld.gold: error: tf_exported_symbols.lds:2:1: syntax error, unexpected STRING
/usr/bin/ld.gold: error: tf_exported_symbols.lds: not an object or archive

```
here is the tensorflow build rule
```
tf_cc_shared_object(
    name = ""libtensorflow.so"",
    linkopts = select({
        ""//tensorflow:darwin"": [
            ""-Wl,-exported_symbols_list"",  # This line must be directly followed by the exported_symbols.lds file
            ""$(location //tensorflow/c:exported_symbols.lds)"",
            ""-Wl,-install_name,@rpath/libtensorflow.so"",
        ],
        ""//tensorflow:windows"": [],
        ""//tensorflow:windows_msvc"": [],
        ""//conditions:default"": [
            ""-z defs"",
            ""-s"",
            ""-Wl,--version-script"",  #  This line must be directly followed by the version_script.lds file
            ""$(location //tensorflow/c:version_script.lds)"",
        ],
    }),
    deps = [
        ""//tensorflow/c:c_api"",
        ""//tensorflow/c:c_api_experimental"",
        ""//tensorflow/c:exported_symbols.lds"",
        ""//tensorflow/c:version_script.lds"",
        ""//tensorflow/c/eager:c_api"",
        ""//tensorflow/core:tensorflow"",
    ],
)
```
and, the lds file here (I does no change to thie file)
```bash
*tensorflow*
*perftools*gputools*
*tf_*
*TF_*
*TFE_*
*nsync_*
*pywrap_xla*
```
**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23615,XLA does not know associative law,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Arch Linux**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **1.11.0**
- Python version: **3.7.1**

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
output: 
```

== cat /etc/issue ===============================================
Linux ZHANGHAO 4.18.16-arch1-1-ARCH #1 SMP PREEMPT Sat Oct 20 22:06:45 UTC 2018 x86_64 GNU/Linux

== are we in docker =============================================
No

== compiler =====================================================
c++ (GCC) 8.2.1 20180831
Copyright © 2018 Free Software Foundation, Inc.
本程序是自由软件；请参看源代码的版权声明。本软件没有任何担保；
包括没有适销性和某一专用目的下的适用性担保。

== uname -a =====================================================
Linux ZHANGHAO 4.18.16-arch1-1-ARCH #1 SMP PREEMPT Sat Oct 20 22:06:45 UTC 2018 x86_64 GNU/Linux

== check pips ===================================================

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.11.0
tf.GIT_VERSION = b'unknown'
tf.COMPILER_VERSION = b'unknown'
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf_env_collect.sh:行105: nvidia-smi: 未找到命令

== cuda libs  ===================================================
```

You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
output: `b'unknown' 1.11.0`

**Describe the current behavior**
source code:
main.py
```python
import sys
import tensorflow as tf

N = 10000
n = 10

a = tf.random_normal([N,n])
b = tf.random_normal([N,n])
c = tf.random_normal([N,n])
d = tf.random_normal([N,n])

X1 = tf.tensordot(a,b,[[1],[1]],name='X1')
Y1 = tf.tensordot(c,d,[[1],[1]],name='Y1')
Z1 = tf.tensordot(X1,Y1,[[0,1],[0,1]],name='Z1')

X2 = tf.tensordot(a,c,[[0],[0]],name='X2')
Y2 = tf.tensordot(b,d,[[0],[0]],name='Y2')
Z2 = tf.tensordot(X2,Y2,[[0,1],[0,1]],name='Z2')

config = tf.ConfigProto()
config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1

sess = tf.Session(config=config)

if sys.argv[1] == '1':
    for i in range(10):
        print(sess.run(Z1))

if sys.argv[1] == '2':
    for i in range(10):
        print(sess.run(Z2))
```
when run `time python main.py 1` and  `time python main.py 2`, get 4.6s and 1.2s

**Describe the expected behavior**
In fact Z1 and Z2 should be the same, hope there is some method to optimize Z1, since in my program, there are many similar situation. consider order of tensor contract is too complicated"
23614,Why only support tf.train.Int64List? In many case int32 can meet requirements. Int64 will expand the computation.,"message Example {
 Features features = 1;
};

message Features{
 map<string,Feature> featrue = 1;
};

message Feature{
    oneof kind{
        BytesList bytes_list = 1;
        FloatList float_list = 2;
        Int64List int64_list = 3;
    }
};


Only support bytes_list , float_list, int64_list. Why not support int32_list?"
23613,ERROR: Config value opt is not defined in any .rc file,"I wanna change .pb file to tflite file , but I failed.

I use the follow commed.

bazel run --config=opt tensorflow/contrib/lite/toco:toco -- \
--input_file=$OUTPUT_DIR/tflite_graph.pb \
--output_file=$OUTPUT_DIR/detect.tflite \
--input_shapes=1,300,300,3 \
--input_arrays=normalized_input_image_tensor \
--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \
--inference_type=QUANTIZED_UINT8 \
--mean_values=128 \
--std_values=128 \
--change_concat_input_ranges=false \
--allow_custom_ops

> 
**and it come out the error below:**

WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
/home/abb/tensorflow/tools/bazel.rc
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=80
ERROR: Config value opt is not defined in any .rc file

So who can help me to solve this issue?
"
23612,ValueError: No gradients provided for any variable when using custom raw_rnn,"**System: ubuntu 16.04
Docker: nvidia-gpu
Anaconda: 5.1
tensorflow:1.9**

I use custom raw_rnn to implement the graph shown as following:
![rnn](https://user-images.githubusercontent.com/7899459/48241311-494a8e80-e411-11e8-9235-735a8f8f8653.png)

The code of custom raw_rnn is shown as following:

```
def loop_rnn(num_lstm_units, image_embeddings, class_num, batch_size, initializer_scale):
    initializer = tf.random_uniform_initializer(
        minval=-initializer_scale,
        maxval=initializer_scale)

    with tf.variable_scope(""lstm"", initializer=initializer, reuse=tf.AUTO_REUSE) as lstm_scope:
        lstm_cell = tf.contrib.rnn.BasicLSTMCell(
            num_units=num_lstm_units, state_is_tuple=True)

        zero_state = lstm_cell.zero_state(
            batch_size=image_embeddings.get_shape()[0], dtype=tf.float32)

        K = 5
        C = 80
        max_time = K + 1

        temp_variable = (tf.TensorArray(size=K, dtype=tf.float32),
                         tf.TensorArray(size=max_time, dtype=tf.float32))
        temp_variable[1].write(0, [[[1., 0., 0.], [0., 1., 0.]] for i in range(batch_size)])

        lstm_input_size = 14
        zk_size = 4096

        initial_state = zero_state

        output_dim = 4096

        fc_weight_initializer = tf.zeros_initializer()
        fc_bias_initializer = tf.constant_initializer([1., 0., 0., 0., 1., 0.])

        def loop_fn(time, cell_output, cell_state, loop_state):
            if cell_output is None:
                next_cell_state = initial_state
                emit_output = tf.zeros([output_dim])
            else:
                next_cell_state = cell_state
                emit_output = cell_output
                # The graph after LSTM  #
                z_k = tf.contrib.layers.fully_connected(
                    inputs=cell_state,
                    num_outputs=zk_size,
                    activation_fn=tf.nn.relu,
                    weights_initializer=initializer)

                if time != 0:
                    temp_variable[0].write([time - 1],
                              tf.contrib.layers.fully_connected(
                                  inputs=z_k[0],
                                  num_outputs=class_num,
                                  activation_fn=None,
                                  weights_initializer=initializer))

                if time != max_time:
                    M = tf.reshape(
                        tf.contrib.layers.fully_connected(
                            inputs=z_k[0],
                            num_outputs=max_time,
                            activation_fn=None,
                            weights_initializer=fc_weight_initializer,
                            biases_initializer=fc_bias_initializer),
                        [batch_size, 2, 3])
                    tf.assign(M[:, 0, 1], (tf.convert_to_tensor(0.)))
                    tf.assign(M[:, 1, 0], (tf.convert_to_tensor(0.)))
                    temp_variable[1].write(time, M)
               # The graph after LSTM  #

            # The graph before LSTM  #
            if time != max_time:
                f_k_origin = stn.spatial_transformer_network(image_embeddings, temp_variable[1].read(time))
                f_k = tf.layers.max_pooling2d(f_k_origin, pool_size=[2, 2], strides=2, padding='SAME')

                with tf.variable_scope(""fc_1"") as fc_1_scope:
                    f_k = tf.contrib.layers.fully_connected(
                    inputs=tf.reshape(f_k, [batch_size, int(lstm_input_size * lstm_input_size / 4 * 512)]),
                    num_outputs=4096,
                    activation_fn=None,
                    weights_initializer=initializer,
                    scope=fc_1_scope)
            # The graph before LSTM  #

            elements_finished = (time >= max_time)

            if time == max_time:
                next_in = None
            else:
                next_in = f_k

            next_input = next_in

            next_loop_state = temp_variable
            return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)

        outputs_ta, last_state, loop_state_ta = tf.nn.raw_rnn(lstm_cell, loop_fn)
        scores_result = loop_state_ta[0].stack()
        M_result = loop_state_ta[1].stack()

        return scores_result, M_result
```

The error info is shown as following:

```
Traceback (most recent call last):
  File ""train.py"", line 125, in <module>
    tf.app.run()
  File ""/home/trainer/anaconda3/envs/tf19_py36/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""train.py"", line 107, in main
    optimizer=training_config.optimizer)
  File ""/home/trainer/anaconda3/envs/tf19_py36/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/optimizers.py"", line 297, in optimize_loss
    name=""train"")
  File ""/home/trainer/anaconda3/envs/tf19_py36/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 583, in apply_gradients
    ([str(v) for _, _, v in converted_grads_and_vars],))
ValueError: No gradients provided for any variable: [""<_RefVariableProcessor(<tf.Variable 'lstm/rnn/fc_1/weights:0' shape=(25088, 4096) dtype=float32_ref>)>"", ""<_RefVariableProcessor(<tf.Variable 'lstm/rnn/fc_1/biases:0' shape=(4096,) dtype=float32_ref>)>"", ""<_RefVariableProcessor(<tf.Variable 'lstm/rnn/basic_lstm_cell/kernel:0' shape=(8192, 16384) dtype=float32_ref>)>"", ""<_RefVariableProcessor(<tf.Variable 'lstm/rnn/basic_lstm_cell/bias:0' shape=(16384,) dtype=float32_ref>)>"", ""<_RefVariableProcessor(<tf.Variable 'lstm/rnn/fully_connected/weights:0' shape=(4096, 4096) dtype=float32_ref>)>"", ""<_RefVariableProcessor(<tf.Variable 'lstm/rnn/fully_connected/biases:0' shape=(4096,) dtype=float32_ref>)>"", ""<_RefVariableProcessor(<tf.Variable 'lstm/rnn/fully_connected_1/weights:0' shape=(4096, 80) dtype=float32_ref>)>"", ""<_RefVariableProcessor(<tf.Variable 'lstm/rnn/fully_connected_1/biases:0' shape=(80,) dtype=float32_ref>)>"", ""<_RefVariableProcessor(<tf.Variable 'lstm/rnn/fully_connected_2/weights:0' shape=(4096, 6) dtype=float32_ref>)>"", ""<_RefVariableProcessor(<tf.Variable 'lstm/rnn/fully_connected_2/biases:0' shape=(6,) dtype=float32_ref>)>""].
```

**When I look into the graph using tensorboard, it's strange that there are two STN, fc_1, max_pool, basic_lstm_cell, fully_connected, fully_connected_1, fully_connected_2. Why do these ops have double? Does it result in the problem?**

![rdar_graph](https://user-images.githubusercontent.com/7899459/48241504-32f10280-e412-11e8-922e-fb08ef0102c2.png)
 "
23611,How to feed data to the graph after using Graph Transform Tool,"The demo shows that it can strip unused nodes by Graph Transform Tool .
But when setting 'inputs=Mul:0' , the input of the graph becomes an 'Operation:Mul'.
The problem is that the graph needs a tensor as the input not the operation.
How to feed image data to the graph after stripping unused nodes?

bazel build tensorflow/tools/graph_transforms:transform_graph
bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
--in_graph=tensorflow_inception_graph.pb \
--out_graph=optimized_inception_graph.pb \
--inputs='Mul:0' \
--outputs='softmax:0' \
--transforms='
strip_unused_nodes(type=float, shape=""1,299,299,3"")
remove_nodes(op=Identity, op=CheckNumerics)
fold_old_batch_norms
'"
23610,Causal Convolutions in Tensorflow,"**System information**
- TensorFlow version (you are using): 1.10
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
This is feature request to support **causal convolutions** in the Tensorflow layers API. Currently, causal convolutions require `tf.nn.conv2d(...)` with manual masking of weights. To improve speed, the data can be cropped/padded and then ordinary convolutions used.  

**Will this change the current api? How?**
Yes, `tf.layers.conv2d` will have a `causal` boolean flag.

**Who will benefit with this feature?**
Researchers and practitioners working on auto-regressive convolutional models.

**Any Other info.**
An analysis at OpenAI revealed that many open-source implementations of causal convolutions are slow, incorrect, and/or have uninterpretable code. This feature would help the ML community more easily develop architectures containing causal convolutions."
23609,[Cloud TPU] Various issues with uint8 data type,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): GCP
- TensorFlow version (use command below): 1.11+
- Python version: N/A
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: TPUv2

**Describe the current behavior**

There's three issues here:
1. Missing propagation of error messages, crashing the TPU host
2. It not compiling for shapes other than `u8[1024]` or multiples thereof
3. It crashing on infeed even for `u8[1024]`

At the moment, compiling (via XRTCompile) crashes the TPU host for anything that uses UInt8 datatypes, unless it's one dimensional and the size is a multiple of 1024. Using the julia frontend:

```
julia> @async @tpu (()->XLA.HloInfeed(Tuple{XRTArray{UInt8, (256,), 1}})(XLA.HloAfterAll()())[1][1])()
ERROR: Tensorflow error: Status: Socket closed

```

```
julia> @async @tpu (()->XLA.HloInfeed(Tuple{XRTArray{UInt8, (1024,1024), 2}})(XLA.HloAfterAll()())[1][1])()
ERROR: Tensorflow error: Status: Socket closed
```

And though compiling succeeds for one dimensional UInt8 vectors of size 1024, trying to infeed crashes:
```
julia> @async @tpu (()->XLA.HloInfeed(Tuple{XRTArray{UInt8, (1024,), 1}})(XLA.HloAfterAll()())[1][1])()
Task (runnable) @0x00007fae07199120

julia> infeed((zeros(UInt8, 1024),))
ERROR: Tensorflow error: Status: Socket closed
```

**Describe the expected behavior**
Ideally all of these would work. If that takes longer to do, at least exposing the error message to the client, rather than crashing would be desired.

**Code to reproduce the issue**
See above (should be reproducible with any frontend that uses xrt to talk to Cloud TPUs though)."
23608,Tflite for micro kernel does not have conv2d?,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue  happens on mobile device: No.
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): from master
- Python version: 3.5.2
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source): 4.8
- CUDA/cuDNN version: on CPU
- GPU model and memory: on CPU

**Describe the current behavior**
I'm trying to figure out how to use Tflite for microcontroller, I know it's relatively new to the users. I notice that in the kernel folder, there is only the depth wise conv2d kernel. Does this mean models defined with tf.nn.conv2d won't fit into this framework? Is this part currently under developing or I just simply missed something? Thanks in advance.  
"
23607,Segmentation Fault on `help(tf.RunMetadata)`,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave 10.14
- TensorFlow installed from (source or binary): `pip install tensorflow`
- TensorFlow version (use command below): v1.11.0-rc2-4-gc19e29306c 1.11.0
- Python version: 3.6.7

**Describe the current behavior**
`Segmentation fault: 11` and Python crashes.
**Describe the expected behavior**
Print the documentation describing `tf.RunMetadata`.
**Code to reproduce the issue**
```
import tensorflow as tf
help(tf.RunMetadata)
```"
23606,V100 PCIE FP16 PS/worker training data corruption bug occurs when distributed over gRPC,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): [tensorflow/benchmarks:cnn_tf_v1.12_compatible](https://github.com/tensorflow/benchmarks/tree/cnn_tf_v1.12_compatible)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04-hwe
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): ('v1.12.0-0-ga6d8ffae09', '1.12.0')
- Python version: 2.7.12
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 9.0/7.2.1.38-1+cuda9.0
- GPU model and memory: V100-PCIE-32GB, NVIDIA driver version 384.145, with `intel_iommu=on`

```
$ cat /proc/cmdline
BOOT_IMAGE=/boot/vmlinuz-4.15.0-38-generic root=/dev/mapper/vgroot-lvroot ro intel_iommu=on
```
```
$ nvidia-smi topo -m
	GPU0	GPU1	GPU2	GPU3	CPU Affinity
GPU0	 X 	PIX	NODE	NODE	0-9,20-29
GPU1	PIX	 X 	NODE	NODE	0-9,20-29
GPU2	NODE	NODE	 X 	PIX	0-9,20-29
GPU3	NODE	NODE	PIX	 X 	0-9,20-29

Legend:

  X    = Self
  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node
  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
  PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)
  PIX  = Connection traversing a single PCIe switch
  NV#  = Connection traversing a bonded set of # NVLinks
```

**Describe the current behavior**
FP-16 multi-GPU training with CPU as local parameter server is converging in single process mode, but diverging loss value (nan) adding another loopback PS process with grpc. Consistent behaviours for both ResNet-50 and VGG16. 

**Describe the expected behavior**
The same training procedure should yield exactly same result using only 1 worker with/without 1 PS. Not sure why adding SendRecvOps causes data corruption. See other info below.

**Code to reproduce the issue**

ResNet50 local with [output](https://gist.github.com/byronyi/4f0821a4792b479bf884ff81171c4011):
```
$ python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \
  --variable_update=parameter_server \
  --local_parameter_device=cpu \
  --model=resnet50 \
  --num_gpus=4 \
  --use_fp16 \
  --batch_size=256
```

VGG16 local with [output](https://gist.github.com/byronyi/0dc379b8feac762bc8d04cab2b240060):
```
$ python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \
  --variable_update=parameter_server \
  --local_parameter_device=cpu \
  --model=vgg16 \
  --num_gpus=4 \
  --use_fp16 \
  --batch_size=256
```

Distributed using the same PS command with [output](https://gist.github.com/byronyi/ea508b150d1e55b0ac41502c48685b7c):
```
CUDA_VISIBLE_DEVICES= \
python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \
  --ps_hosts=localhost:5000 \
  --worker_hosts=localhost:5001 \
  --job_name=ps \
  --local_parameter_device=cpu \
  --task_index=0 \
  --server_protocol=grpc
```

ResNet50 distributed with [output](https://gist.github.com/byronyi/de05ccb7b481932ce7be91f889f41aee):
```
$ python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \
  --ps_hosts=localhost:5000 \
  --worker_hosts=localhost:5001 \
  --job_name=worker \
  --task_index=0 \
  --server_protocol=grpc \
  --variable_update=parameter_server \
  --local_parameter_device=cpu \
  --model=resnet50 \
  --num_gpus=4 \
  --use_fp16 \
  --batch_size=256
```

VGG16 distributed with [output](https://gist.github.com/byronyi/1113a5cd8048f729a2c73df5bfd97017):
```
$ python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \
  --ps_hosts=localhost:5000 \
  --worker_hosts=localhost:5001 \
  --job_name=worker \
  --task_index=0 \
  --server_protocol=grpc \
  --variable_update=parameter_server \
  --local_parameter_device=cpu \
  --model=vgg16 \
  --num_gpus=4 \
  --use_fp16 \
  --batch_size=256
```

**Other info / logs**
Kernel reports `[DMA Write] PTE Write access is not set` for one of the GPU.
Detailed log could be found [here](https://gist.github.com/dd1e75702116d941d95dfa1fe17b8e27)."
23605,Tensorboard projector visualisation - PCA keeps loading or not working,"Tensorboard projector visualisation - PCA keeps hanging.
I wrote a simple NN to predict the class type of iris dataset. NN model works fine.
```
import pandas as pd 
import numpy as np

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn import preprocessing

import tensorflow as tf
import tensorflow as tf
from tensorflow import keras

iris_data = load_iris()
x = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)
y = pd.DataFrame(iris_data.target, columns=['class'])

encoder = preprocessing.OneHotEncoder(categories='auto')
encoder.fit(y)
#Transform
y_enc = encoder.transform(y).toarray()

x_train, x_test, y_train, y_test = train_test_split(x, y_enc)


model = keras.Sequential()
model.add(keras.layers.Dense(8, name='input_layer', activation=tf.nn.relu, input_shape=(x_train.shape[1],)))
model.add(keras.layers.Dense(4, name='hidden_layer', activation=tf.nn.relu, input_shape=(x_train.shape[1],)))
model.add(keras.layers.Dense(3, name='out_layer', activation=tf.nn.softmax))

model.compile(optimizer=tf.keras.optimizers.Adam(0.005),
              loss=keras.losses.binary_crossentropy,
              metrics=[keras.metrics.categorical_accuracy])
model.fit(x_train, y_train, epochs=50, verbose=0)
result = model.predict(x_test)
```
Now I am trying to visualise the output of the test set. Below is the code for Tensorboard projector. I don't what I am missing but PCA keeps loading even after starting the Tensorboard several minutes ago.

```
import tensorflow as tf
from tensorflow.contrib.tensorboard.plugins import projector
import numpy as np
import os

LOG_DIR = 'logs'  # FULL PATH HERE!!!

metadata_file = os.path.join(LOG_DIR, 'metadata.tsv')
with open(metadata_file, 'w') as f:
f.write('{}\t{}\n'.format('class_name','class_id'))
with open(metadata_file, 'a') as f:
for i in range(len(y_test)):
    c = np.nonzero(y_test[i])[0][0]
    f.write('{}\t{}\n'.format(iris_data.target_names[c],c))


embedding_var = tf.Variable(result,  name='final_layer_embedding')
sess = tf.Session()
sess.run(embedding_var.initializer)
summary_writer = tf.summary.FileWriter(LOG_DIR)
config = projector.ProjectorConfig()
embedding = config.embeddings.add()
embedding.tensor_name = embedding_var.name

embedding.metadata_path = 'metadata.tsv'

projector.visualize_embeddings(summary_writer, config)
saver = tf.train.Saver([embedding_var])
saver.save(sess, os.path.join(LOG_DIR, 'model.ckpt'), 1)
```
I googled to understand what I am doing wrong but I could not fix it. Despite being my model is small very I could not visualise. Any help to resolve this problem would be highly appreciable.
"
23604,c_api.h Why GPU is slower than CPU,"hello i tried the c_api .I have trained a lstm mode and i load the model in my c project to predict my data.I find the GPU is slowly than CPU.
gpu:nvidia TITAN X
CUDA：9.0
cudnn:7.0
cpu: intel E5
tensorflow: 1.11.0
i predicted about 200 data and every data call the function ：
TF_SessionRun(sess,
nullptr, // Run options.
&input_op, &input_tensor, 1, // Input tensors, input tensor values, number of inputs.
&out_op, &output_tensor, 1, // Output tensors, output tensor values, number of outputs.
nullptr, 0, // Target operations, number of targets.
nullptr, // Run metadata.
status // Output status.
);
every time GPU is slowly than CPU
Is my method wrong ?Is there a way to increase the speed? and can I enter data in batches for prediction?how to feed data in batches?
thanks"
23603,Is tensorflow supported in python 2.7?,"

**System information**
- OS Platform and Distribution: **Linux Ubuntu 16.04**
- Python version: **2.7-64bits**
- Installed using pip: **pip version 9.0.3**
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version: 9.1

**Describe the problem**
Hi:
Im trying to install tensorflow with pip with this command:
`pip install tensorflow-gpu`
and the output says:
`Collecting tensorflow-gpu
  Could not find a version that satisfies the requirement tensorflow-gpu (from versions: )
No matching distribution found for tensorflow-gpu
You are using pip version 9.0.3, however version 18.1 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.`

Is my python version compatible with tensorflow? I see that are instructions here [https://www.tensorflow.org/install/pip?lang=python2](url) to install tensorflow for python 2.7 but i have the same result.
I want to use Python 2.7 because the other modules I need for my scripts are installed in python 2.7. I have seen in other posts and issues that tensorflow is not compatible with python 2.7, is that true? How can I solve this? Should I try to install the other modules in python 3.5.2?
Thanks for help.
"
23602,tensorflow-gpu import throw serveral exception,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution:**:win 10 gtx860
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:tensorflow-gpu      1.12.0
- **Python version**:3.5.4
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:cuda 8.0/cuDNN 6.0
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```

### Describe the problem
i have already installed cuda and cuDNN,and the location of cuDNN  below:
C:\Users\wdh>where cudnn64_6.dll
D:\studyTool\cuda\bin\cudnn64_6.dll

and environment is :
 print(""Environmental variable:"", os.environ[""PATH""])
Environmental variable: D:\Anaconda\envs\tensorflow\bin;D:\studyTool\cuda\bin;D:\studyTool\cuda\libnvvp;

how can i solve this problem?

### Source code / logs
>>> import tensorflow
Traceback (most recent call last):
  File ""D:\Anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\Anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\Anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""D:\Anaconda\envs\tensorflow\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""D:\Anaconda\envs\tensorflow\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 找不到指定的模块。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""D:\Anaconda\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""D:\Anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""D:\Anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""D:\Anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\Anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\Anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""D:\Anaconda\envs\tensorflow\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""D:\Anaconda\envs\tensorflow\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 找不到指定的模块。


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
23601,Move autograph into a separate package and into a separate repo,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: any
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a
- **TensorFlow installed from (source or binary)**: any
- **TensorFlow version (use command below)**: any
- **Python version**: any
- **Bazel version (if compiling from source)**: any
- **GCC/Compiler version (if compiling from source)**: any
- **CUDA/cuDNN version**: any
- **GPU model and memory**: any
- **Exact command to reproduce**: n/a

### Describe the problem

I guess autograph should be moved into a separate package and repo. Because I guess it may be possible to add support of other computation graph frameworks there.

"
23600,tensorflow lite AllocateTensors() error after ResizeInputTensor of mobilenet ssd models,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:qualcom 801 platform
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below):1.11
- Python version:
- Bazel version (if compiling from source):1.18
- GCC/Compiler version (if compiling from source):gcc 5.4
- CUDA/cuDNN version:NA
- GPU model and memory:NA


**Describe the current behavior**
I convert ssd_mobilenet_v1_0.75_quantize model to .tflite, I know the model's training input size is 300x300,  I want to use different input tensor size, such as 240x240, so I use the ResizeInputTensor() function,
**But I got an error in AllocateTensors():**
tensorflow/contrib/lite/kernels/reshape.cc:58 num_input_elements != num_output_elements (2700 != 4332)
Node number 36 (RESHAPE) failed to prepare.
Failed to allocate tensors!

How to fix it?

**Describe the expected behavior**
I want to change the input tensor size freely

**Code to reproduce the issue**
`

    std::vector<int> sizes = {1, 240, 240, 3};

    interpreter_->ResizeInputTensor(interpreter_->inputs()[0], sizes);

    if (interpreter_->AllocateTensors() != kTfLiteOk) {
        LOG(INFO) << ""Failed to allocate tensors!"" << ""\n"";
        return false;
}`
"
23599,TFLite constant strided slice wrongly,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.11
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

After convert to tf lite, my model cannot get the correct output, then i realise it is actually the slicing output is wrong.

Code to reproduce the error
```
class TestModel2(object):
    def __init__(self):
        self.sess = tf.Session()

        self.input_placeholder = tf.placeholder(tf.float32, [4, 2])

        self.data = tf.constant([[0, 1, 2, 3],
                                 [4, 5, 6, 7],
                                 [8, 9, 10, 11],
                                 [12, 13, 14, 15]], tf.float32, name='data')

        self.data_slice = self.data[:, :2]

        # self.output_node = self.input_placeholder * self.data_slice

        self.sess.run(tf.global_variables_initializer())

    def test_convert_tflite(self):
        print('loading from session')
        converter = tf.contrib.lite.TocoConverter.from_session(self.sess, [self.input_placeholder],
                                                               [self.data_slice])
        print('converting to tflite')
        tflite_model = converter.convert()
        open('dummy.tflite', 'wb').write(tflite_model)

    def test_run(self, inputs):
        output = self.sess.run([self.data_slice], feed_dict={self.input_placeholder: inputs})
        print(output)
        print('-')

class TestTFLite(object):
    def __init__(self, path):
        # Load TFLite model and allocate tensors.
        interpreter = tf.contrib.lite.Interpreter(model_path=path)
        interpreter.allocate_tensors()

        # Get input and output tensors.
        input_details = interpreter.get_input_details()
        output_details = interpreter.get_output_details()

        print('input tensor:', input_details[0])
        print('output tensor:', output_details[0])

        self.interpreter = interpreter
        self.input_details = input_details
        self.output_details = output_details

    def test_run(self, inputs):
        self.interpreter.set_tensor(self.input_details[0]['index'], inputs)
        self.interpreter.invoke()

        output = self.interpreter.get_tensor(self.output_details[0]['index'])
        print(output)
        # output = self.interpreter.get_tensor(self.output_details[1]['index'])
        # print(output)

if __name__ == '__main__':
    np.random.seed(1000)
    inputs = np.random.randn(4, 2).astype(np.float32)
    tr = TestModel2()
    tr.test_run(inputs)
    tr.test_convert_tflite()

    tl = TestTFLite('dummy.tflite')
    tl.test_run(inputs)

```

tensorflow output
```
array([[ 0.,  1.],
       [ 4.,  5.],
       [ 8.,  9.],
       [12., 13.]], dtype=float32)
```
TFLite output
```
[[ 0.  4.]
 [ 8. 12.]
 [ 1.  5.]
 [ 9. 13.]]
```"
23598,"Wrong output of AveragePool operation, for a quantization TF lite model ","Hello, 

step1: Train a TF model to be a quantization one by adding Fake Node.
step2: Convert the quantization TF model to be a TF Lite model with command: 
` ./bazel-bin/tensorflow/contrib/lite/toco/toco 
        --output_file=~/deeplab_mobilenetv2_quantized.tflite  
        --input_file=~/frozen_inference_graph.pb  
        --inference_type=QUANTIZED_UINT8 
        --input_arrays=Placeholder 
        --output_arrays=ResizeBilinear_2 
        --mean_values=0 
        --std_dev_values=1 
        --input_shapes=1,513,513,3`
         
step3. Run the quantization TF lite model on an Android phone.

I find, for thethe output of model,  there is a big difference between TF and quantization TF lite .

So, I compared the output of maily operations of model, one by one. And I find that:
1> For conv operataion, the output difference are less than 1.
2> But for a AveragePool operation,the output of TF lite seem to be completely wrong. After debug, I find the kernel implementation is running by calling the interface `TF_LITE_AVERAGE_POOL(optimized_ops)`(in file: tensorflow/lite/kernels/pooling.cc).

But when I use `TF_LITE_AVERAGE_POOL(reference_ops)` to replace it by modifying the ""pooling.cc"" in the ""AverageEvalQuantized(...const uint8* input_data...)"" as: 
     ` if (kernel_type == kReference) {`
          `TF_LITE_AVERAGE_POOL(reference_ops);`
     `} else {`
         ` //TF_LITE_AVERAGE_POOL(optimized_ops);`
         `TF_LITE_AVERAGE_POOL(reference_ops) ;`
       `}`
After build and run, find that the difference are also less than 1.

In all, the issue is:
The output of AveragePool operation seem to be wrong when use the interface  `TF_LITE_AVERAGE_POOL(optimized_ops)`.

Would you like to enlighten me whether I make some mistakes when I use AveragePool.

And would you like to do a test for AveragePool operation with:
   input size: 1x33x33x320 (NxHxWxC)
   input data type: int8, range at 100~255
   pool_params->stride_width = 33;
   pool_params->stride_height = 33;
   pool_params->filter_height = 33;
   pool_params->filter_width = 33;
   pool_params->activation = kTfLiteActNone;
   pool_params->padding = kTfLitePaddingValid;

System info for your reference:
   - OS Platform: Linux Ubuntu 16.04
   - Mobile device:XiaoMi Note
   - TensorFlow version: commit d1e9a1ed54cae9b0b10ab89c06d6d7f9b53af3a1
   - Python version:2.7.6
   - Bazel version:0.15.0
"
23597,training translation in multiple gpu with large data,"hello i am using the language translation with attention in tensorflow 
https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb

now i like to train the model into multiple gpu with huge dataset i got some code but i don't know how to combine this with that
NUM_GPUS = 2
strategy = tf.contrib.distribute.MirroredStrategy(num_gpus=NUM_GPUS)
config = tf.estimator.RunConfig(train_distribute=strategy)
estimator = tf.keras.estimator.model_to_estimator(model, config=config)

or any other ways to do that could you please help me

thanks in advance"
23595,Running Keras CNN model give different results on GPU than CPU.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian
- TensorFlow version (use command below): 
Keras, version 2.2.4 is used on Tensorflow CPU 1.5.0 
Keras, version 2.2.4 is used on Tensorflow GPU 1.11.0
- Python version: 3 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 7.2.1
- GPU model and memory: Telsa P4

**Describe the current behaviour**
I trained a CNN model using Keras over Tensorflow. The results showed that the model has nearly 10% better accuracy than running on CPU for the same code.

**Describe the expected behaviour**
I expected the same test accuracy on both GPU and CPU.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.



```
from __future__ import print_function
import keras
from keras.datasets import cifar10,mnist,cifar100
from keras import Sequential,optimizers
from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Activation, Flatten
import LoggerYN as YN
import numpy as np
import scipy.io as sio
import utilsYN as uYN
import datetime
import time



def initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay):
    global Dataset    
    global pbatchSize
    global pnumClasses
    global pEpochs
    global pLearningRate
    global pMomentum
    global pWeightDecay
    Dataset = dataset
    pbatchSize = batchSize
    pnumClasses = numClasses
    pEpochs = epochs
    pLearningRate = learningRate
    pMomentum = momentum
    pWeightDecay = weightDecay
    
def NormalizeData(x_train,x_test):
        x_train /= 255
        x_test /= 255
        return x_train, x_test

def CategorizeData(y_train,y_test,pnumClasses):
    y_train = keras.utils.to_categorical(y_train, pnumClasses)
    y_test = keras.utils.to_categorical(y_test, pnumClasses)
    return y_train, y_test
    
def loadData():

    Dataset = cifar100
    (x_train, y_train), (x_test, y_test) = Dataset.load_data(label_mode='fine') if fineFlag else Dataset.load_data()
    
    global imgRows
    global imgCols
    global inputShape
    
    imgRows = x_train.shape[1]
    imgCols = x_train.shape[2]

    try:
        imgRGB_Dimensions = x_train.shape[3]
    except Exception:
        imgRGB_Dimensions = 1 #For Gray Scale Images

    print(x_train.shape)
    x_train = x_train.reshape(x_train.shape[0], imgRows, imgCols, imgRGB_Dimensions)
    x_test = x_test.reshape(x_test.shape[0], imgRows, imgCols, imgRGB_Dimensions)
    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    x_train, x_test = NormalizeData(x_train, x_test)
    y_train, y_test = CategorizeData(y_train,y_test,pnumClasses)
    inputShape = (imgRows, imgCols, imgRGB_Dimensions)
    return x_train, y_train, x_test, y_test


def model_CIFAR100():
    CIFAR_model = Sequential()
    CIFAR_model.add(Conv2D(128, (3, 3), padding='same',strides=1,input_shape=inputShape))
    CIFAR_model.add(Activation('relu'))
    CIFAR_model.add(Conv2D(128, (3, 3)))
    CIFAR_model.add(Activation('relu'))
    CIFAR_model.add(MaxPooling2D(pool_size=(2, 2),strides=2,padding='valid'))
    CIFAR_model.add(Dropout(0.1))
    
    CIFAR_model.add(Conv2D(256, (3, 3), padding='same',strides=1))
    CIFAR_model.add(Activation('relu'))
    CIFAR_model.add(Conv2D(256, (3, 3)))
    CIFAR_model.add(Activation('relu'))
    CIFAR_model.add(MaxPooling2D(pool_size=(2, 2),strides=2,padding='valid'))
    CIFAR_model.add(Dropout(0.25))
    
    CIFAR_model.add(Conv2D(512, (3, 3), padding='same',strides=1))
    CIFAR_model.add(Activation('relu'))
    CIFAR_model.add(Conv2D(512, (3, 3)))
    CIFAR_model.add(Activation('relu'))
    CIFAR_model.add(MaxPooling2D(pool_size=(2, 2),strides=2,padding='valid'))
    CIFAR_model.add(Dropout(0.5))
    
    
    CIFAR_model.add(Flatten())
    CIFAR_model.add(Dense(1024))
    CIFAR_model.add(Activation('relu'))
    CIFAR_model.add(Dropout(0.5))
    CIFAR_model.add(Dense(pnumClasses))
    CIFAR_model.add(Activation('softmax'))    
    return CIFAR_model


def evaluateModel(model,x_test,y_test,verbose):
    pLoss, pAcc = model.evaluate(x_test, y_test, verbose)
    print(""Test Loss"", pLoss)
    print(""Test Accuracy"", pAcc)
     


def RunCIFAR100(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay):
    initParameters(dataset,batchSize,numClasses,epochs,learningRate,momentum,weightDecay)
    x_train, y_train, x_test, y_test = loadData()
    CIFAR_model = model_CIFAR100()
    CIFAR_sgd = optimizers.SGD(lr=learningRate, decay=weightDecay, momentum=momentum, nesterov=False)
    CIFAR_model.compile(loss='categorical_crossentropy',optimizer=CIFAR_sgd, metrics=['accuracy'])
    CIFAR_model.fit(x_train, y_train,batch_size=batchSize,epochs=epochs,validation_data=(x_test, y_test),shuffle=True)
    evaluateModel(CIFAR_model,x_test, y_test, verbose=1)


def runModel(dataset,batchSize=128,numClasses=10,epochs=12,learningRate=0.01,momentum=0.5,weightDecay=1e-6):
    RunCIFAR100(dataset,batchSize,numClasses=100,epochs=epochs,learningRate=learningRate,momentum=momentum,weightDecay=weightDecay)

def main():
    runModel(""cifar100"",epochs=200)
```
"
23593,Cannot install tensorflow-gpu; pip killed,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 inside a Singularity image
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.12.0
- Python version: 3.6.6
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: 9.0 + 9.1 + 9.2 / 7.3.1.20
- GPU model and memory: GeForce GT 740 / 1999 MiB



**Describe the problem**
When I go to install `tensorflow-gpu`, pip reports a message `Killed`. No errors or other messages. `tensorflow` installs just fine.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
pip install --user tensorflow-gpu
```


**Any other info / logs**
```
Collecting tensorflow-gpu
  Downloading https://files.pythonhosted.org/packages/55/7e/bec4d62e9dc95e828922c6cec38acd9461af8abe749f7c9def25ec4b2fdb/tensorflow_gpu-1.12.0-cp36-cp36m-manylinux1_x86_64.whl (281.7MB)
    100% |████████████████████████████████| 281.7MB 9.4kB/s
Killed
```
"
23591,tf.Variable.load() raises NotImplementedError on TF1.11 but works okay in TF1.10 (RefVariable changes),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.11.0-0-gc19e29306c 1.11.0
- Python version: 3.5
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a 
- GPU model and memory: n/a

**Describe the current behavior**
ccing @alextp as it seemed relevant.

There had been some changes to [variables.py](https://github.com/tensorflow/tensorflow/blob/e1fb7a248bb2d932a0bed6fc1d2d9e3d91b50e89/tensorflow/python/ops/variables.py#L473-L506) which separate RefVariable from default Variable (see [this commit](https://github.com/tensorflow/tensorflow/commit/e1fb7a248bb2d932a0bed6fc1d2d9e3d91b50e89) which was released to TF1.11 build). However, some of these changes seem to break backward compatibility.

For instance, if I create a variable using `tf.get_variable`, and within a session load a value into it using `.load(value, sess)` method, it worked fine in TF1.10. However with TF1.11 this results in `NotImplementedError`:

```
  File ""test.py"", line 41, in <module>
    my_var.load(1.2, sess)
  File ""/scratch/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/variables.py"", line 789, in load
    raise NotImplementedError
NotImplementedError
```

The error points to line 789 in [variables.py](https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/python/ops/variables.py#L789)

**[UPDATE]**
Here's a sample code snippet to reproduce the issue.
```python
# test.py
import tensorflow as tf

class RestoredVariable(tf.Variable):
  """"""
  A variable restored from disk
  """"""
  def __init__(self, name, trainable=True, collections=None, graph=None):
    if graph is None:
      graph = tf.get_default_graph()
    if collections is None:
      collections = [tf.GraphKeys.GLOBAL_VARIABLES]
    if trainable and tf.GraphKeys.TRAINABLE_VARIABLES not in collections:
      collections = collections + [tf.GraphKeys.TRAINABLE_VARIABLES]
    self._variable = graph.as_graph_element(name).outputs[0]
    self._snapshot = graph.as_graph_element(name + '/read').outputs[0]
    self._initializer_op = graph.as_graph_element(name + '/Assign')
    self._constraint = None
    self._trainable = trainable
    i_name = name + '/Initializer/'
    keys = [ k for k in graph._nodes_by_name.keys() if k.startswith(i_name) and '/' not in k[len(i_name):] ]
    if len(keys) != 1:
      raise ValueError('Could not find initializer for variable', keys)
    self._initial_value = graph.as_graph_element(keys[0]).outputs[0]
    for key in collections:
      graph.add_to_collection(key, self)
    self._save_slice_info = None


# Create a variable `my_var`
var_temp = tf.get_variable('my_var', shape=[], dtype=tf.float32)

# Restore `my_var` as a `RestoredVariable` object which derives from `tf.Variable`.
# This is useful when the python handle `var_temp` isn't available in current session
# as may be the case when a loading from a serialized graph.pb or metagraph.
my_var = RestoredVariable('my_var', trainable=True)


with tf.Session() as sess:
  my_var.load(1.2, sess)
  print(sess.run(my_var))
```

**Output on TF1.10:**
```
(tensorflow) $ python test.py
...
1.2
```

**ERROR on TF1.11:**
```
(tensorflow) $ python test.py
...
Traceback (most recent call last):
  File ""test.py"", line 43, in <module>
    print(var)
  File ""/scratch/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1547, in __exit__
    self._default_graph_context_manager.__exit__(exec_type, exec_value, exec_tb)
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/scratch/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 5227, in get_controller
    yield g
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/scratch/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 5035, in get_controller
    yield default
  File ""/scratch/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 5227, in get_controller
    yield g
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/scratch/tensorflow/lib/python3.5/site-packages/tensorflow/python/eager/context.py"", line 357, in _mode
    yield
  File ""/scratch/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 5227, in get_controller
    yield g
  File ""test.py"", line 41, in <module>
    my_var.load(1.2, sess)
  File ""/scratch/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/variables.py"", line 789, in load
    raise NotImplementedError
NotImplementedError
```

Is there a change in the method to load variables within a session? Or am I missing something here. Please help."
23590,tf.manip.roll executes on CPU instead of GPU,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a
- TensorFlow installed from (source or binary):pip
- TensorFlow version (use command below):1.8
- Python version:2.7
- Bazel version (if compiling from source):n/a
- GCC/Compiler version (if compiling from source):n/a
- CUDA/cuDNN version:9/7.1
- GPU model and memory:1080ti/11gb


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
tf.manip.roll executes on CPU and is very slow

**Describe the expected behavior**
![image](https://user-images.githubusercontent.com/4759327/48167964-70a43d00-e2e5-11e8-9c7a-59e6bd50e306.png)


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23587,how to implement fftshift on a tensor?,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
- Are you willing to contribute it (Yes/No):



**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**
"
23586,Allowing GPU memory growth command does not work ,"Hi, i have a memory problem.
I am running a training on a server. I have the following print out.

2018-11-05 21:08:07.907464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-11-05 21:08:07.908090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties:
name: GeForce GTX 980 major: 5 minor: 2 memoryClockRate(GHz): 1.2405
pciBusID: 0000:02:00.0
totalMemory: 3.95GiB freeMemory: 3.87GiB
2018-11-05 21:08:07.908116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:02:00.0, compute capability: 5.2)

As you can see the total memory is higher than the free memory. Actually, running the code i get an ""Out of memory"" message.
So i applied the wrote code at the beginning of my script:
gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3, allow_growth=True)
sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))

Unfortunately the memory usage:
totalMemory: 3.95GiB freeMemory: 3.87GiB

does not change at all. What is the problem?

Thanks and best regards,
Giovanni"
23585,tf.nn.Softmax() crashes if tensor is not two-dimensional ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04 ""nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04"" docker image and nvidia-docker
- TensorFlow installed from (source or binary): binary (conda install tensorflow-gpu)
- TensorFlow version (use command below): 1.10
- Python version: 3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda 9.2, CuDNN 7.2.1
- GPU model and memory: GeForce GTX 1080 Ti, 11264 MB 

**Describe the current behavior**
I'm working on image segmentation, with 5 classes. After computing logits, I want to compute softmax activations : 
`probabilities = tf.nn.softmax(logits)`
with logits of shape [-1,586,586,5], which shouldn't be a problem since softmax internally reshapes it to a 2D tensor. However, I get the following error.
> Check failed: NDIMS == dims() (2 vs. 4)Asking for tensor of 2 dimensions from a tensor of 4 dimensions


I cannot reproduce the issue outside of docker on ubuntu18.04 and tensorflow 1.11, everything works as expected. This happens at inference time, using a frozen model.

**Describe the expected behavior**
tensor should be reshaped internally and crash should't occur. 

**Quick fix**
` flat_logits = tf.reshape(logits,shape=[-1,n_classes])`
` probabilities = tf.nn.softmax(flat_logits)`
Reshaping the tensor myself prevents the crash from happening. "
23582,Is it ok for C++ code to link with _pywrap_tensorflow_internal.so?,"I have a Python module written in C++, in which tensorflow serving API is used to load saved model and inference. Unfortunately, some part of my Python application has to use tensorflow python module directly. Because there are some common symbols between libtensorflow_cc.so and _pywrap_tensorflow_internal.so, there will be CHECK failure if the Python module is linked with libtensorflow_cc.so. 
The only feasible option seems to link the Python module with _pywrap_tensorflow_internal.so. Is that OK?"
23581,Simplest WindowDataset usage not working even for the documented examples,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): tensorflow/tensorflow:1.12.0 docker image, macOS 10.14
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12.0
- Python version: 2.7
- Bazel version (if compiling from source): no
- GCC/Compiler version (if compiling from source): no
- CUDA/cuDNN version: no
- GPU model and memory: no

**Describe the current behavior**

`Dataset.window` documentation has a few examples of which all crash in the process of graph construction (i.e. this example `tf.data.Dataset.range(7).window(2)`).

**Describe the expected behavior**

I would like it not to crash.

**Code to reproduce the issue**

```python

from __future__ import absolute_import, division, print_function

import tensorflow as tf

tf.enable_eager_execution()

dataset = tf.data.Dataset.range(7).window(2)

it = dataset.make_one_shot_iterator()

```

The same error occurs when eager execution is disabled.

**Other info / logs**

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-2-4495120d82f9> in <module>()
      7 dataset = tf.data.Dataset.range(7).window(2)
      8 
----> 9 it = dataset.make_one_shot_iterator()

/usr/local/lib/python2.7/dist-packages/tensorflow/python/data/ops/dataset_ops.pyc in make_one_shot_iterator(self)
    178     """"""
    179     if context.executing_eagerly():
--> 180       return iterator_ops.EagerIterator(self)
    181     # NOTE(mrry): We capture by value here to ensure that `_make_dataset()` is
    182     # a 0-argument function.

/usr/local/lib/python2.7/dist-packages/tensorflow/python/data/ops/iterator_ops.pyc in __init__(self, dataset)
    531         self._resource = gen_dataset_ops.anonymous_iterator(
    532             output_types=self._flat_output_types,
--> 533             output_shapes=self._flat_output_shapes)
    534         gen_dataset_ops.make_iterator(ds_variant, self._resource)
    535         # Delete the resource when this object is deleted

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.pyc in anonymous_iterator(output_types, output_shapes, name)
     71       return anonymous_iterator_eager_fallback(
     72           output_types=output_types, output_shapes=output_shapes, name=name,
---> 73           ctx=_ctx)
     74     except _core._NotOkStatusException as e:
     75       if name is not None:

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.pyc in anonymous_iterator_eager_fallback(output_types, output_shapes, name, ctx)
     89         ""Expected list for 'output_types' argument to ""
     90         ""'anonymous_iterator' Op, not %r."" % output_types)
---> 91   output_types = [_execute.make_type(_t, ""output_types"") for _t in output_types]
     92   if not isinstance(output_shapes, (list, tuple)):
     93     raise TypeError(

/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/execute.pyc in make_type(v, arg_name)
    124   except TypeError:
    125     raise TypeError(""Expected DataType for argument '%s' not %s."" %
--> 126                     (arg_name, repr(v)))
    127   i = v.as_datatype_enum
    128   return i

TypeError: Expected DataType for argument 'output_types' not <tensorflow.python.data.ops.dataset_ops._NestedDatasetComponent object at 0x7feadf71f9d0>.
```"
23578,tensorflow performance bottleneck o IteratorGetNext,
23576,TOCO Runtime error: Check failed: array_copy_size[0] != 0,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.11
- Python version: 3.6
- Bazel version (if compiling from source): 0.15
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9
- GPU model and memory: Titan XP

Convert frozen graph pb file to tflite, but failed due to some low level problem.

** Error Message **

RuntimeError: TOCO failed see console for info.
b'2018-11-07 13:26:38.105273: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 283 operators, 446 arrays (0 quantized)
2018-11-07 13:26:38.111622: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 283 operators, 446 arrays (0 quantized)
2018-11-07 13:26:38.431863: F tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_concatenation.cc:53] Check failed: array_copy_size[0] != 0 (0 vs. 0)
Aborted (core dumped)'

I am not sure how this happens, I am following all tutorials to convert, the pb file can be read and run. However I am failed to convert pb to tflite."
23575, The  compile error is to buid tensorflow1.4 source code in vs15," Cmake tensorflow1.4 source code,but the error is unable to open file  to tensorflow.def  in vs15 windows
"
23574,Ubuntu 18.04 bazel build from source fails,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution:   
Description:	Ubuntu 18.04.1 LTS
Release:	18.04
Codename:	bionic

- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.12.0
- Python version: 3.6
- Installed using virtualenv? pip? conda?: NA
- Bazel version (if compiling from source): bazel release 0.18.1
- GCC/Compiler version (if compiling from source): gcc-6
- CUDA/cuDNN version: CUDA-9.1, cuDNN-7.1
- GPU model and memory: NVIDIA Corporation GP106M [GeForce GTX 1060 Mobile] (rev a1)



**Describe the problem**
Building from source, fails on executing bazel build instruction.
**Provide the exact sequence of commands / steps that you executed before running into the problem**
I'm trying to build tensorflow from source on my system, following https://medium.com/@asmello/how-to-install-tensorflow-cuda-9-1-into-ubuntu-18-04-b645e769f01d as my OS is ubuntu 18.04, following these steps I ran the ./configure and then ""bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package"" to build.
build goes on for a couple hours, everything seems fine until the very end when I get this error

**Any other info / logs**
ERROR: /home/armaan/software/tensorflow/tensorflow/BUILD:561:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1)
Traceback (most recent call last):
  File ""/home/armaan/.cache/bazel/_bazel_armaan/68a0964576af93b9dcbb773eca5ca492/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/armaan/.cache/bazel/_bazel_armaan/68a0964576af93b9dcbb773eca5ca492/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/armaan/.cache/bazel/_bazel_armaan/68a0964576af93b9dcbb773eca5ca492/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: /home/armaan/.cache/bazel/_bazel_armaan/68a0964576af93b9dcbb773eca5ca492/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZTIN6icu_608ByteSinkE

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/armaan/.cache/bazel/_bazel_armaan/68a0964576af93b9dcbb773eca5ca492/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 27, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/home/armaan/.cache/bazel/_bazel_armaan/68a0964576af93b9dcbb773eca5ca492/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/armaan/.cache/bazel/_bazel_armaan/68a0964576af93b9dcbb773eca5ca492/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/armaan/.cache/bazel/_bazel_armaan/68a0964576af93b9dcbb773eca5ca492/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/armaan/.cache/bazel/_bazel_armaan/68a0964576af93b9dcbb773eca5ca492/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/armaan/.cache/bazel/_bazel_armaan/68a0964576af93b9dcbb773eca5ca492/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: /home/armaan/.cache/bazel/_bazel_armaan/68a0964576af93b9dcbb773eca5ca492/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZTIN6icu_608ByteSinkE


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 6680.706s, Critical Path: 297.19s
INFO: 14471 processes: 14471 local.
FAILED: Build did NOT complete successfully
"
23573,"How can I fix ""adding visible gpu devises: 0""","Just fixed one problem and now it is refusing to acknowledge that I have a gpu or smth.
On top of that it is not doing anything. Any fixes?

Using Windows 10
Cuda 9
cuDNN 7
GPU - gtx 1080, 8gb
eGPU - razer core x
Python 3.5.3


Using TensorFlow backend.
2018-11-06 22:16:20.031851: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2018-11-06 22:16:20.718059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:07:00.0
totalMemory: 8.00GiB freeMemory: 6.59GiB
2018-11-06 22:16:20.718927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0


Solutions tried: 
1. uninstall tensorflow-gpu
   install tensorflow-gpu

2.uninstall tensorflow-gpu
   uninstall protobuf
   install tensorflow-gpu
3. Repeat the previous two couple more times (got desperate) "
23572,How to reuse kernel weight with tf.keras.layers.Conv2D,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: win10
- **TensorFlow installed from (source or binary)**: conda
- **TensorFlow version (use command below)**: 1.11.0
- **Python version**: 3.5
- **CUDA/cuDNN version**: 9.0/7.0
- **GPU model and memory**: Titan XP, 12G

I want to reuse kernel weight in keras with tensorflow backend.

My code is as following :

```
from tensorflow.python.keras.layers import  Input
from tensorflow.python.keras import regularizers
def infer3(data_input, Reuse):
    with tf.variable_scope('Network', reuse=Reuse):
        inputs = Input(tensor = data_input)
        network = tf.keras.layers.Conv2D(kernel_size=3, strides=2, filters=64, padding='same',kernel_regularizer=regularizers.l2(1),
                    activation='linear', kernel_initializer=""glorot_normal"", name='conv1', bias_initializer='zeros')(inputs)

    return network

tf.reset_default_graph() 
input_tensor = tf.placeholder(tf.float32,shape=[BATCH_SIZE,img_H,img_W,1])
output_tensor = tf.placeholder(tf.float32,shape=[BATCH_SIZE,img_H,img_W,1]) 
in_training_mode = tf.placeholder(tf.bool)     


network = infer3(input_tensor,False)
network_test = infer3(input_tensor,True)
```

When I type "" tf.trainable_variables() ""

It show message:

```
[<tf.Variable 'Network/conv1/kernel:0' shape=(3, 3, 1, 64) dtype=float32>,
 <tf.Variable 'Network/conv1/bias:0' shape=(64,) dtype=float32>,
 <tf.Variable 'Network_1/conv1/kernel:0' shape=(3, 3, 1, 64) dtype=float32>,
 <tf.Variable 'Network_1/conv1/bias:0' shape=(64,) dtype=float32>]
```
How can I reuse kernel weight?"
23570,Having trouble importing tf,"I am new to tensorflow, and so far been using CPU tensorflow/keras (was working fine with CPU). Recently got a gpu and can't seem to find proper solution. Any suggestions on how to fix it? Thanks in advance.

P.S. could it be because i installed GPU version without uninstalling CPU version?

Using Windows 10
Cuda 9
cuDNN 7
GPU - gtx 1080, 8gb
eGPU - razer core x
Python 3.5.3 


>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\Nozim\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Nozim\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Nozim\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Nozim\AppData\Local\Programs\Python\Python35\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Nozim\AppData\Local\Programs\Python\Python35\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Nozim\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Nozim\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Nozim\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Nozim\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Nozim\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Nozim\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Nozim\AppData\Local\Programs\Python\Python35\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Nozim\AppData\Local\Programs\Python\Python35\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
23569,fatal error when import /tensorflow/.tf_configure.bazelrc during installation,"
**System information**
- OS Platform and Distribution : Linux Ubuntu 18.04.1 LTS
- TensorFlow installed from source
- Bazel version : 18.1

Hello

I want to install tensorflow from source.
After I configured with ./configure I try to launch the .so file, it wants to import a /tf.configure.bazelrc which is not even in the git tensorflow file. I am not sure what to do.
```
~/Documents/molecular dynamics/tensorflow/tensorflow$ bazel build -c opt --verbose_failures //tensorflow:libtensorflow_cc.so
[bazel INFO src/main/cpp/option_processor.cc:362] Looking for the following rc files: /etc/bazel.bazelrc,/home/kotik/Documents/molecular dynamics/tensorflow/.bazelrc,/home/kotik/.bazelrc
[bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /etc/bazel.bazelrc
[bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/kotik/Documents/molecular dynamics/tensorflow/.bazelrc
[bazel FATAL src/main/cpp/blaze.cc:1324] Invalid import declaration in .blazerc file '/home/kotik/Documents/molecular dynamics/tensorflow/.bazelrc': 'import /home/kotik/Documents/molecular dynamics/tensorflow/.tf_configure.bazelrc' (are you in your source checkout/WORKSPACE?)
```

"
23566,tf.nn.embedding_lookup_sparse Converting sparse IndexedSlices Warning,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): r1.11
- Python version: 3.6
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10/7.3

**Describe the current behavior**
Using embedding_lookup_sparse raises the following warning:

> UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory. ""Converting sparse IndexedSlices to a dense Tensor of unknown shape.""

Looking at embedding_lookup_sparse implementation, the issue comes from the gather after embedding lookup.

```
...
ids = sp_ids.values
ids, idx = array_ops.unique(ids)

embeddings = embedding_lookup(params,ids,...)
...
embeddings = array_ops.gather(embeddings, idx)
```

**Describe the expected behavior**
I didn't expect the sparse gradients to be passed to an op that can't back-propagate them inside 
embedding lookup sparse.

Is there a reason this op does not call embedding_lookup on sp_ids.values directly? As opposed to
looking up the unique ids and calling gather on those rows ?
"
23565,//tensorflow/core:util_tensor_slice_set_test fails on 1.12,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 1.12
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.19.0
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: Using CPU
- GPU model and memory: Using CPU



**Describe the problem**

When running tests from source on r1.12, the test //tensorflow/core:util_tensor_slice_set_test fails.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

I'm following the steps at https://www.tensorflow.org/install/sourcepip. This error occurs when I run
`bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/lite/...`

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

[test.log](https://github.com/tensorflow/tensorflow/files/2555024/test.log)

"
23564,//tensorflow/contrib/lookup:lookup_ops_test fails on 1.12,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 1.12
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.19.0
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: Using CPU
- GPU model and memory: Using CPU



**Describe the problem**

When running tests from source on r1.12, the test //tensorflow/contrib/lookup:lookup_ops_test fails.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

I'm following the steps at https://www.tensorflow.org/install/sourcepip. This error occurs when I run
`bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/lite/...`

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


[test.log](https://github.com/tensorflow/tensorflow/files/2555016/test.log)
"
23563,"After Tflite toco, conv2d layer becomes depthwiseconv2d.","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No.
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.10.0
- Python version: 3.6.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: on CPU
- GPU model and memory: on CPU

**Describe the current behavior**
I'm using tensorflow speech_command example's tiny_conv model [from here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/speech_commands). I've trained it and froze it (with quantization set to true) following [the official tutorial](https://www.tensorflow.org/tutorials/sequences/audio_recognition). 
The model itself is simple: one conv2d layer and one fully connected layer.
Now, I send the froze .pb file to tensorflow lite. I can successfully get the .tflite file. However, when I check the model, the original tiny_conv model's conv2d layer becomes a depthwiseconv2d layer.( I have tried several toco converts on other models, they do not have this behavior. ) I'm wondering how could this happen? I have the .pb model shown below, followed by the .tflite model:
![freeze](https://user-images.githubusercontent.com/31113955/48089764-10fd4300-e1ba-11e8-952c-3c25c854103e.JPG)
![tflite](https://user-images.githubusercontent.com/31113955/48089771-165a8d80-e1ba-11e8-9558-2986b76dd9f8.JPG)
As you can see, the conv2d layer becomes depthwiseconv2d.

**Describe the expected behavior**
The expected behavior is after toco, the .tflite still has one conv2d layer and one fully connected layer.

**Code to reproduce the issue**
After training and freezing, in toco, I have following code:
`graph_def_file = ""my_frozen.pb""     //This is the .pb file.
input_arrays = [""Reshape_1""]        //This is the name of the input node
output_arrays = [""labels_softmax""]  //This is the name of the ouput node

//This is the main code to call toco
converter = tf.contrib.lite.TocoConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays)
//Since fake quantization during training, we quantize the graph during converting
converter.inference_type = tf.contrib.lite.constants.QUANTIZED_UINT8
input_arrays = converter.get_input_arrays()
converter.quantized_input_stats = {input_arrays[0] : (0., 2.)}  //mean, std_dev [from this tensorflow tutorial](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/micro_speech)
tflite_model = converter.convert()
open(""tiny_conv.tflite"", ""wb"").write(tflite_model)   //The resulting .tflite file.`

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23562,"[xrt] Add ""Free all memory"" op","**System information**
- TensorFlow version (you are using): nightly
- Are you willing to contribute it (Yes/No): N/A

**Describe the feature and the current behavior/state.**
When a client loses track of xrt-managed state (e.g. because it errors before it could start tracking it) or because it crashes, or because other errors, the allocation stays around on the device side with no way to free it (other than a hard restart of the device). It would be useful to have an xrt op that drops all memory that xrt knows about (perhaps optionally with a preserve list, so the client can say ""I know about these, so don't free them"").

**Will this change the current api? How?**
This will likely be a new xrt op.

**Who will benefit with this feature?**
Users of the xrt API.

**Any Other info.**
@michaelisard requested to be assigned."
23561,Custom op kernels compiled with C++17 are binary incompatible with tensorflow,"- Have I written custom code: yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS 7.5 (in docker)
- TensorFlow installed from (source or binary): pip binary
- TensorFlow version (use command below): 1.11
- Python version: 3.6.4
- GCC/Compiler version (if compiling from source): 7.3.1

**current behavior**
When compiling a custom op kernel with c++17 features, compilation succeeds, but loading the generated library fails at runtime with the following error:
```
tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib64/python3.6/site-packages/package-0.0.1-py3.6-linux-x86_64.egg/_package_ops.cpython-36m-x86_64-linux-gnu.so: undefined symbol: _ZN10tensorflow11GetNodeAttrERKNS_9AttrSliceESt17basic_string_viewIcSt11char_traitsIcEEPSs
```
**expected behavior**
Loading the library should succeed

**Code to reproduce the issue**
Any custom op using `GetAttr` should reproduce this, e.g. the custom op in the custom op tutorial

**Other info / logs**
This symbol that is undefined is `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, std::basic_string_view<char, std::char_traits<char> >, std::basic_string<char, std::char_traits<char>, std::allocator<char> >*)`

the symbol that it should have linked against (which exists in libtensorflow_framework.so) is `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::string_view, std::basic_string<char, std::char_traits<char>, std::allocator<char> >*)`

the difference is `std::basic_string<char, std::char_traits<char>` vs. `absl::string_view` for the second argument.

It appears to be this way because https://github.com/abseil/abseil-cpp/blob/master/absl/strings/string_view.h uses `std::string_view` to provide `absl::string_view` when `std::string_view` is available (i.e. when compiling using c++17 standard)

The tensorflow binary is compiled with c++11, so it has absl::string_view provided by the custom absl implementation, whereas my custom op, compiled with c++17 gets the standard library implementation, so they are binary incompatible.
"
23559,Unenforce StreamHandler in tf_logging,"**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
Currently the tf_logging API will always add its own StreamHandler to the tensorflow logger (see: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/tf_logging.py#L121-L124). According to the [Python logging documentation](https://docs.python.org/3/howto/logging.html#configuring-logging-for-a-library) this is discouraged:

> **Note:** It is strongly advised that you do not add any handlers other than NullHandler to your library’s loggers. This is because the configuration of handlers is the prerogative of the application developer who uses your library. The application developer knows their target audience and what handlers are most appropriate for their application: if you add handlers ‘under the hood’, you might well interfere with their ability to carry out unit tests and deliver logs which suit their requirements. 

**Will this change the current api? How?**
If some optional configuration parameter is added to not use the StreamHandler, this does not need to affect how the tf_logging API is used today.

**Who will benefit with this feature?**
Any developer (like myself) that work on complex projects where tensorflow is not the only component that performs logging. I want to propagate tensorflow LogRecords to a handler at the root logger object, so that I might decide myself how best to print them or save them."
23558,tf.nn.embedding_lookup_sparse Converting sparse IndexedSlices Warning,"Using embedding_lookup_sparse raises the following warning:

`UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory. ""Converting sparse IndexedSlices to a dense Tensor of unknown shape.""`

Looking at embedding_lookup_sparse implementation, I suspect the issue comes from the `gather` after embedding lookup. 

```
ids = sp_ids.values
ids, idx = array_ops.unique(ids)

embeddings = embedding_lookup(...)
...
embeddings = array_ops.gather(embeddings, idx)
```
Is there a reason this op does not call `embedding_lookup` on `sp_ids.values` directly? As opposed to 
looking up the unique ids and calling gather on those rows ? 
"
23557,"Memmory leak Using tensorflow::NewSession , C++","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:N/A
- **TensorFlow installed from (source or binary)**:Source
- **TensorFlow version (use command below)**: 1.12.0-rc0
- **Python version**: Python 2.7.12
- **Bazel version (if compiling from source)**: 0.18.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: N/A. Using Cpu Only. 
- **GPU model and memory**: N/A. Using Cpu Only. 
- **Exact command to reproduce**:

## Code
```
class Temp
{
 public:
  Temp(){
    std::unique_ptr<tensorflow::Session> session(
      tensorflow::NewSession(tensorflow::SessionOptions()));
    };
};

int main(int argc, char **argv)
{
  Temp temp;
  return 0;
}
```

### Describe the problem
When creating a unique_ptr to a session using tensorflow::NewSession, a memory leak occurs according to address sanitizer. There is a similar closed issue posted here: [https://github.com/tensorflow/tensorflow/issues/19163](https://github.com/tensorflow/tensorflow/issues/19163) , however I am still having an issue. 

### Source code / logs

```
==259==ERROR: LeakSanitizer: detected memory leaks

Direct leak of 24 byte(s) in 1 object(s) allocated from:
    #0 0x7f17c81b7532 in operator new(unsigned long) (/usr/lib/x86_64-linux-gnu/libasan.so.2+0x99532)
    #1 0x7f17c1c1aca7 in tensorflow::RegisterXlaDeviceKernels(char const*, char const*) (/tensorflowincludes/bin/libtensorflow_cc.so+0x1cd2ca7)
    #2 0x7f17c1c0e19a in tensorflow::XlaCpuDeviceFactory::CreateDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<tensorflow::Device*, std::allocator<tensorflow::Device*> >*) (/tensorflowincludes/bin/libtensorflow_cc.so+0x1cc619a)
    #3 0x7f17bf851fe8 in tensorflow::DeviceFactory::AddDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<tensorflow::Device*, std::allocator<tensorflow::Device*> >*) (/tensorflowincludes/bin/libtensorflow_framework.so+0x62dfe8)
    #4 0x7f17c61ccea4 in tensorflow::DirectSessionFactory::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) (/tensorflowincludes/bin/libtensorflow_cc.so+0x6284ea4)
    #5 0x7f17bf8abfe8 in tensorflow::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) (/tensorflowincludes/bin/libtensorflow_framework.so+0x687fe8)
    #6 0x7f17bf8ac0ac in tensorflow::NewSession(tensorflow::SessionOptions const&) (/tensorflowincludes/bin/libtensorflow_framework.so+0x6880ac)
    #7 0x402bc2 in Temp::Temp() /var/jenkins_home/workspace/tensorflowleaktest_master-FVC6RQR2GX36IO3JBYVHBT3IJH3EQUQFR2B3OPUAPWVP7LOWN2WQ/tests/src/main.cpp:38
    #8 0x402245 in main /var/jenkins_home/workspace/tensorflowleaktest_master-FVC6RQR2GX36IO3JBYVHBT3IJH3EQUQFR2B3OPUAPWVP7LOWN2WQ/tests/src/main.cpp:44
    #9 0x7f17be6c582f in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x2082f)

Indirect leak of 4096 byte(s) in 1 object(s) allocated from:
    #0 0x7f17c81b7532 in operator new(unsigned long) (/usr/lib/x86_64-linux-gnu/libasan.so.2+0x99532)
    #1 0x7f17c1c1ab50 in void std::vector<std::unique_ptr<tensorflow::kernel_factory::OpKernelRegistrar, std::default_delete<tensorflow::kernel_factory::OpKernelRegistrar> >, std::allocator<std::unique_ptr<tensorflow::kernel_factory::OpKernelRegistrar, std::default_delete<tensorflow::kernel_factory::OpKernelRegistrar> > > >::_M_emplace_back_aux<tensorflow::kernel_factory::OpKernelRegistrar*>(tensorflow::kernel_factory::OpKernelRegistrar*&&) (/tensorflowincludes/bin/libtensorflow_cc.so+0x1cd2b50)
    #2 0x7f17c1c1ae7e in tensorflow::RegisterXlaDeviceKernels(char const*, char const*) (/tensorflowincludes/bin/libtensorflow_cc.so+0x1cd2e7e)
    #3 0x7f17c1c0e19a in tensorflow::XlaCpuDeviceFactory::CreateDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<tensorflow::Device*, std::allocator<tensorflow::Device*> >*) (/tensorflowincludes/bin/libtensorflow_cc.so+0x1cc619a)
    #4 0x7f17bf851fe8 in tensorflow::DeviceFactory::AddDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<tensorflow::Device*, std::allocator<tensorflow::Device*> >*) (/tensorflowincludes/bin/libtensorflow_framework.so+0x62dfe8)
    #5 0x7f17c61ccea4 in tensorflow::DirectSessionFactory::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) (/tensorflowincludes/bin/libtensorflow_cc.so+0x6284ea4)
    #6 0x7f17bf8abfe8 in tensorflow::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) (/tensorflowincludes/bin/libtensorflow_framework.so+0x687fe8)
    #7 0x7f17bf8ac0ac in tensorflow::NewSession(tensorflow::SessionOptions const&) (/tensorflowincludes/bin/libtensorflow_framework.so+0x6880ac)
    #8 0x402bc2 in Temp::Temp() /var/jenkins_home/workspace/tensorflowleaktest_master-FVC6RQR2GX36IO3JBYVHBT3IJH3EQUQFR2B3OPUAPWVP7LOWN2WQ/tests/src/main.cpp:38
    #9 0x402245 in main /var/jenkins_home/workspace/tensorflowleaktest_master-FVC6RQR2GX36IO3JBYVHBT3IJH3EQUQFR2B3OPUAPWVP7LOWN2WQ/tests/src/main.cpp:44
    #10 0x7f17be6c582f in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x2082f)

Indirect leak of 323 byte(s) in 323 object(s) allocated from:
    #0 0x7f17c81b7532 in operator new(unsigned long) (/usr/lib/x86_64-linux-gnu/libasan.so.2+0x99532)
    #1 0x7f17c1c1ad87 in tensorflow::RegisterXlaDeviceKernels(char const*, char const*) (/tensorflowincludes/bin/libtensorflow_cc.so+0x1cd2d87)
    #2 0x7f17c1c0e19a in tensorflow::XlaCpuDeviceFactory::CreateDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<tensorflow::Device*, std::allocator<tensorflow::Device*> >*) (/tensorflowincludes/bin/libtensorflow_cc.so+0x1cc619a)
    #3 0x7f17bf851fe8 in tensorflow::DeviceFactory::AddDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<tensorflow::Device*, std::allocator<tensorflow::Device*> >*) (/tensorflowincludes/bin/libtensorflow_framework.so+0x62dfe8)
    #4 0x7f17c61ccea4 in tensorflow::DirectSessionFactory::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) (/tensorflowincludes/bin/libtensorflow_cc.so+0x6284ea4)
    #5 0x7f17bf8abfe8 in tensorflow::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) (/tensorflowincludes/bin/libtensorflow_framework.so+0x687fe8)
    #6 0x7f17bf8ac0ac in tensorflow::NewSession(tensorflow::SessionOptions const&) (/tensorflowincludes/bin/libtensorflow_framework.so+0x6880ac)
    #7 0x402bc2 in Temp::Temp() /var/jenkins_home/workspace/tensorflowleaktest_master-FVC6RQR2GX36IO3JBYVHBT3IJH3EQUQFR2B3OPUAPWVP7LOWN2WQ/tests/src/main.cpp:38
    #8 0x402245 in main /var/jenkins_home/workspace/tensorflowleaktest_master-FVC6RQR2GX36IO3JBYVHBT3IJH3EQUQFR2B3OPUAPWVP7LOWN2WQ/tests/src/main.cpp:44
    #9 0x7f17be6c582f in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x2082f)

SUMMARY: AddressSanitizer: 4443 byte(s) leaked in 325 allocation(s).

```
"
23556,Persistent Concat op bug with mkl dnn,"This issue appears to be related to #17494 which was closed.  I am opening a new issue to return attention to this.


**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, and I have produced a boiled down script that can reproduce the problem standalone 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux (RH family), running on Intel Xeon Phi (KNL)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary, from  intel's build: https://software.intel.com/en-us/articles/intel-optimization-for-tensorflow-installation-guide
- TensorFlow version (use command below): 1.11
- Python version: Observed in 3.5 and 3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
When using the concat operation with mkl_dnn, the operation fails if run after a convolution with the following error message:

```
tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at mkl_concat_op.cc:814 : Aborted: Operation received an exception:Status: 5, message: could not create a concat primitive descriptor, in file tensorflow/core/kernels/mkl_concat_op.cc:811
INFO:tensorflow:An error was raised. This may be due to a preemption in a connected worker or parameter server. The current session will be closed and a new session will be created. Error: Operation received an exception:Status: 5, message: could not create a concat primitive descriptor, in file tensorflow/core/kernels/mkl_concat_op.cc:811
```

**Describe the expected behavior**

It should not fail to create a concat primitive descriptor

**Code to reproduce the issue**
Please see this gist with the bare minimum python script to reproduce this issue: https://gist.github.com/coreyjadams/35e89feb1fb191c2788792f6268448fc


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23555,slim resnet batchnorm missing,"I just did a MWE of resnet v1 (see code below). However, when I inspect the graph in tensorboard, I could not see the batch norm layers.
```
import tensorflow as tf
import tensorflow.contrib.slim as slim
import tensorflow.contrib.slim.nets 
import numpy as np

input = tf.placeholder(shape=[None, 224, 224, 3], dtype=tf.float32)
resnet = tf.contrib.slim.nets.resnet_v1.resnet_v1_50(input, 1000, is_training=True)
tf.summary.histogram(""resnet"", resnet[0])

init = tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init)
    
    train_writer = tf.summary.FileWriter( './tflogs ', sess.graph)
    
    merge = tf.summary.merge_all()
    
    summary, out = sess.run([merge, resnet], feed_dict={input: np.random.random((2, 224, 224, 3))})
    
    train_writer.add_summary(summary)
```"
23554,Build of tensorflow==1.12.0 on broadwell fails,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.12.0
- Python version: 3.6.5
- Installed using virtualenv? pip? conda?: no
- Bazel version (if compiling from source): 0.19.0
- GCC/Compiler version (if compiling from source): 7.3
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**
I'm unable to build tensorflow for CPU from source.
I'm using command `bazel build --verbose_failures -c opt --copt=-march=native --copt=-mfpmath=both -k //tensorflow/tools/pip_package:build_pip_package`
For tensorflow==1.12.0 I'm getting:
```
INFO: From Compiling tensorflow/contrib/lite/toco/import_tensorflow.cc:
In file included from external/gemmlowp/fixedpoint/fixedpoint.h:874:0,
                 from ./tensorflow/contrib/lite/kernels/internal/common.h:48,
                 from ./tensorflow/contrib/lite/toco/runtime/types.h:18,
                 from ./tensorflow/contrib/lite/toco/model.h:28,
                 from ./tensorflow/contrib/lite/toco/import_tensorflow.h:20,
                 from tensorflow/contrib/lite/toco/import_tensorflow.cc:15:
external/gemmlowp/fixedpoint/./fixedpoint_sse.h:43:39: warning: ignoring attributes on template argument '__m128i {aka __vector(2) long long int}' [-Wignored-attributes]
 struct FixedPointRawTypeTraits<__m128i> {
                                       ^
INFO: From Compiling tensorflow/python/framework/fast_tensor_util.cpp:
In file included from bazel-out/k8-fastbuild/genfiles/external/local_config_python/numpy_include/numpy/ndarraytypes.h:1821:0,
                 from bazel-out/k8-fastbuild/genfiles/external/local_config_python/numpy_include/numpy/ndarrayobject.h:18,
                 from bazel-out/k8-fastbuild/genfiles/external/local_config_python/numpy_include/numpy/arrayobject.h:4,
                 from bazel-out/k8-fastbuild/genfiles/tensorflow/python/framework/fast_tensor_util.cpp:581:
bazel-out/k8-fastbuild/genfiles/external/local_config_python/numpy_include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning ""Using deprecated NumPy API, disable it by "" ""#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION"" [-Wcpp]
 #warning ""Using deprecated NumPy API, disable it by "" \
  ^~~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 4582.121s, Critical Path: 791.83s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]
INFO: 7555 processes: 4 local, 7551 processwrapper-sandbox.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
```


**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
export CC_OPT_FLAGS=""-march=native""
./configure
bazel build -c opt --copt=-march=native --copt=-mfpmath=both -k //tensorflow/tools/pip_package:build_pip_package
```

I'm trying this on the GCE, 32 core broadwell machine in the `us-central1-f` zone.

**Any other info / logs**
I've also tried to build the package by:
```
export CC_OPT_FLAGS=""-march=native""
./configure
bazel build --verbose_failures --config=opt -k //tensorflow/tools/pip_package:build_pip_package
```
Getting exactly the sameproblem.

**EDIT:**
I've tried exactly the same bazel build command with tensorflow==1.11.0 and everything works well. Unfortunately I'd like to use tensorflow==1.12.0 this way. Are there some specific things I've to change for building tensorflow==1.12.0?

**EDIT2:**
After a bit of searching and trying different stuff, such as installing `libc-ares-dev`. I was able to find in the stack trace following error:
```
ERROR: /tensorflow/tensorflow/python/BUILD:3865:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed (Exit 1) gcc failed: error executing command /usr/bin/gcc -shared -o bazel-out/host/bin/tensorflow/python/_pywrap_tensorflow_internal.so -Wl,--version-script bazel-out/host/bin/tensorflow/python/pywrap_tensorflow_internal_versionscript.lds ... (remaining 66 argument(s) skipped)

Use --sandbox_debug to see verbose messages from the sandbox
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_common.pic.o: multiple definition of 'pb_field_iter_begin'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_common.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_common.pic.o: multiple definition of 'pb_field_iter_next'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_common.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_common.pic.o: multiple definition of 'pb_field_iter_find'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_common.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_read'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_istream_from_buffer'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_decode_varint'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_decode_tag'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_skip_field'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_make_string_substream'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_close_string_substream'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_decode_noinit'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_decode'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_decode_delimited'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_decode_svarint'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_decode_fixed32'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_decode_fixed64'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_ostream_from_buffer'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_write'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_encode_varint'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_encode_svarint'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_encode_fixed32'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_encode_fixed64'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_encode_tag'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_encode_tag_for_field'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_encode'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_get_encoded_size'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_encode_string'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_encode_submessage'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_encode_delimited'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```"
23553,"Relationships between Grappler, GraphOptimizer and GraphOptimizationPass","There are so many graph optimization tools in TensorFlow Runtime such as `Grappler`, `GraphOptimizer` and `GraphOptimizationPass`. This makes me confused.  Is there any plan to unify these interfaces？Currently, it seems that these optimizers could not replace with each other.

`Grappler` is very light weight because it does graph optimization on `GraphDef` .The limitation is that  these `optimizers` can only run after `Placer.run()`.  However,  some optimizers in `Grappler` will rewrite placement information. Therefore,  this will possibly cause some placement errors without the protection of `Placer`(Is it reasonable to  run `Placer` after running grappler?). Another, the `FeedBack` function is not being used in framework.

`GraphOptimizationPass` does optimization on `Graph` object. The advantage is that these optimizers can be specified with execution stage(PRE_PLACEMENT, POST_PLACEMENT......) .This is necessary. I think the execution stage of graph optimizers should be treated differently. Some optimizers should be done before placement and the other may be suitable for post partitioning and etc. 

As for `GraphOptimizer`, I heard that it will not be updated in the future. 

It will be nice to unify them and bring their strength together.  @caisq "
23552,"if i set the inputshape with (None, None, featdim),how can I split the input by second dimention(axis=1)?","I want to have a dynamic input in my net, so, I set the input with 2 unknown dim, (1:batch, 2:sequencelength, 3:feature dims),and I defined another variable to specify the real shape of input in forwarding.
i have a test as followings:

    a = tf.placeholder(tf.float32, shape=[None, None, 40], name = ""tensor_a"") 
    b = tf.placeholder(tf.int32, shape=[2], name = ""tensor_b"")  # b[0] indicate the sequence length

    split_tensor_a = split_a(a, b)
    sess = tf.Session()

    array_a = np.logspace(1.0, 2.0, num = 120).reshape(1,3,40)
    array_b = np.array([3, 40])
    feed_dict = {a: array_a, b: array_b}
    split_b_value = sess.run(split_tensor_a, feed_dict = feed_dict)


and the py_func is:

    def split_a(tensor_a, tensor_b):
        sp_tensor =tf.py_func(_split_a, [tensor_a, tensor_b], tf.float32, ""split"")
        return sp_tensor

    def _split_a(a, b):
        tensor_tup = ()
        newsp_a = np.reshape(a, (-1, b[0], a.shape[-1]))
        for n in range(b[0]):
            tensor_tup = tensor_tup + (newsp_a[:, n, :],)
        print(tensor_tup)
        return tensor_tup

In _split_a I can get the value of tensor_tup, but when I run the sess, I got an error:
`InvalidArgumentError (see above for traceback): pyfunc_0 returns 3 values, but expects to see 1 values.
	 [[Node: PyFunc = PyFunc[Tin=[DT_FLOAT, DT_INT32], Tout=[DT_FLOAT], token=""pyfunc_0"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_tensor_a_0_0, _arg_tensor_b_0_1)]]
`
I want to know how to split the unknown dim of tensor?
If the net has 2 unknown dim,how to deal with the input data by one timestep?
The net has cnn layer and lstm layer, I donot want to change the input shape to (-1, featdim) in cnn layer, but it can only do forward by one time step,  the follwing lstm layer must have shape of (batch, numstep, featdim), so, what should I do ?
Can anyone help me?
Thanks~"
23551,[Resolved] Inconsistent output shape for tf.image.resize_images() when preserve_aspect_ratio=True,"TF Version: r1.11

I found a bug in `tf.image.resize_images() ` when specifying `preserve_aspect_ratio=True`.
This is how to reproduce the problem.

```python
def resize_images_issue(img_shape, target):
    img_ph = tf.placeholder(tf.uint8, name=""img"", shape=[None, None, 3])
    img = tf.image.resize_images(img_ph, [target, target], preserve_aspect_ratio=True)

    with tf.Session() as sess:
        img_np  = np.zeros(img_shape, dtype=np.uint8)
        result = sess.run(img, feed_dict={img_ph: img_np})

    if np.max(result.shape) != target:
        print(""Error!! Length of the longer edge is {} though target size is {}"".format(np.max(result.shape), target))
    else:
        print(""ok"")
```

For some sets of input shape and output shape, it returns inconsistent shape.

```python
resize_images_issue((200, 100, 3), 50)
# ok

resize_images_issue((437, 779, 3), 225)
# ok

resize_images_issue((437, 779, 3), 224)
# Error!! Length of the longer edge is 223 though target size is 224
```"
23550,ERROR: Unrecognized option: --input_file=/Users/jyy/model/frozen_inference_graph.pb,"Hello, I trained the "". pb"" file on the cloud server through ssdmobilenet. Now I want to convert it into tflite format, but it failed, and the following error occurred. This is my code. Who can help me? Thank you.

bazel run --config=opt tensorflow/contrib/lite/toco:toco --input_file=/Users/jyy/model/frozen_inference_graph.pb --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=/Users/jyy/wechat_model/wechat.lite --inference_type=FLOAT --input_type=FLOAT --input_arrays=input --output_arrays=predictions --input_shapes=1,224,224,3
ERROR: Unrecognized option: --input_file=/Users/jyy/model/frozen_inference_graph.pb

"
23549,Can the ops in pb files be fully supported by android tensorflow  Mobile ?,"It seems tensorflow mobile(android_tensorflow_lib) contains only part of the  core/kenerns files.
If it is not fully supported ,  can we know how many ops not supported in Tensorflow mobile？"
23547,How to make output sequence length equal to the input sequence length in seq2seq model?,"I am trying to formulate a resource mapping problem using seq2seq model where the input sequence length varies and the output sequence length must be equal to the input sequence length.
From the manual of seq2deq in tensorflow, it seems that the output sequence length is determined by the end token or the maximal time step. So how can I make output length exactly equal to input length when building the seq2seq model?"
23545,Unacceptable framework overhead for huge networks,"I posted the same issue for Keras [here](https://github.com/keras-team/keras/issues/11592), but I guess it's more of a tensorflow problem in general.
so for a large network with thousands or even more layers (the network might not be that large mathematically since each layer could contain just a small number of parameters, but u get the idea), the overhead of basically anything messing with some graph stuff becomes unacceptable, things like model compilation, XLA optimization, gradient checkpoint, and also the keras ""save_weights"" functions are of ABYSMAL performance and could take literally DAYS!! even longer than the actual training process on GPU, I noticed that every time something like the aforementioned stuff was being executed, the python script just got stuck, one CPU core skyrocketed to 100% usage and the rest 39 cores were just almost 0% usage, then it would take hours and hours before that thing was done and then we could finally move on to the actual training.
It's honestly just frustrating and unacceptable, any idea to solve this?
sample script to reproduce the performance problem: https://gist.github.com/IFeelBloated/6e85b251f66941b7eb50f987c95ce69b"
23544,tensorflow_probability 0.4 can't work with tensorflow 1.12,"After I upgrade tensorflow to version 1.12. I get the following error message. This error doesn't happen before upgrade.

>    import tensorflow_probability as tfp;
  File ""C:\Users\yi.xie\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_probability\__init__.py"", line 21, in <module>
    from tensorflow_probability.python import *  # pylint: disable=wildcard-import
  File ""C:\Users\yi.xie\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_probability\python\__init__.py"", line 22, in <module>
    from tensorflow_probability.python import distributions
  File ""C:\Users\yi.xie\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_probability\python\distributions\__init__.py"", line 77, in <module>
    from tensorflow_probability.python.distributions.vector_diffeomixture import quadrature_scheme_softmaxnormal_gauss_hermite
  File ""C:\Users\yi.xie\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_probability\python\distributions\vector_diffeomixture.py"", line 28, in <module>
    from tensorflow.contrib.linalg.python.ops import linear_operator_addition as linop_add_lib
ModuleNotFoundError: No module named 'tensorflow.contrib.linalg'

python version: 3.6
tensorflow version: 1.12.0
tensorflow probability version: 0.4.0
os: windows 10"
23542,No C++ symbols exported after built libtensorflow_cc with bazel on windows,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source
- TensorFlow version:v1.11.0
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):0.18.0 official build
- GCC/Compiler version (if compiling from source):VC++ 2015.3 v14.00 (v140) MSVC
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Following the official documentation, I use `bazel build -c opt //tensorflow:libtensorflow_cc.so` to build tensorflow with c++ api on windows, since cmake is deprecated now.
Everything goes well, and `libtensorflow_cc.so` is generated in bazel out dir, which can be renamed to .dll, but I didn't find `libtensorflow_cc.lib` which should contain the exported symbols. Furthermore, I use `dumpbin /exports libtensorflow_cc.so` to view all the exported symbols (the output is attached in the next section), only `TFE_*` , `TF_*` and some other symbols are exported. Obviously no **C++** symbols are exported.
I dig into the source code and found that there're extra parameters passed to the compiler which specifying a list of symbols to export on darwin and posix, but windows (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/BUILD#L480-L488). I'm not familiar to Microsoft toolchains, but can similar thing be passed to MSVC if necessary so that we can get a proper libtensorflow_cc with necessary symbols exported on windows?

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Output of `dumpbin /exports libtensorflow_cc.so`:

```
Microsoft (R) COFF/PE Dumper Version 14.15.26730.0
Copyright (C) Microsoft Corporation.  All rights reserved.


Dump of file F:\libtensorflow_cc\execroot\org_tensorflow\bazel-out\x64_windows-opt\bin\tensorflow\libtensorflow_cc.so

File Type: DLL

  Section contains the following exports for libtensorflow_cc.so

    00000000 characteristics
    5BDF0174 time date stamp Sun Nov  4 22:25:56 2018
        0.00 version
           1 ordinal base
         233 number of functions
         233 number of names

    ordinal hint RVA      name

          1    0 02C79C50 ?DEVICE_CPU@tensorflow@@3QEBDEB
          2    1 02C79C58 ?DEVICE_GPU@tensorflow@@3QEBDEB
          3    2 02C79C60 ?DEVICE_SYCL@tensorflow@@3QEBDEB
          4    3 02618400 ?ThenBlasGemm@Stream@stream_executor@@QEAAAEAV12@W4Transpose@blas@2@0_K11MAEBV?$DeviceMemory@M@2@H2HMPEAV52@H@Z
          5    4 02618CA0 ?ThenBlasGemm@Stream@stream_executor@@QEAAAEAV12@W4Transpose@blas@2@0_K11MAEBV?$DeviceMemory@Uhalf@Eigen@@@2@H2HMPEAV52@H@Z
          6    5 02619540 ?ThenBlasGemm@Stream@stream_executor@@QEAAAEAV12@W4Transpose@blas@2@0_K11NAEBV?$DeviceMemory@N@2@H2HNPEAV52@H@Z
          7    6 02619DE0 ?ThenBlasGemm@Stream@stream_executor@@QEAAAEAV12@W4Transpose@blas@2@0_K11V?$complex@M@std@@AEBV?$DeviceMemory@V?$complex@M@std@@@2@H3H2PEAV72@H@Z
          8    7 0261A670 ?ThenBlasGemm@Stream@stream_executor@@QEAAAEAV12@W4Transpose@blas@2@0_K11V?$complex@N@std@@AEBV?$DeviceMemory@V?$complex@N@std@@@2@H3H2PEAV72@H@Z
          9    8 02C86690 ?kDatasetGraphKey@DatasetBase@tensorflow@@2QBDB
         10    9 02C866A0 ?kDatasetGraphOutputNodeKey@DatasetBase@tensorflow@@2QBDB
         11    A 001C8370 TFE_ContextAddFunction
         12    B 001C83C0 TFE_ContextAddFunctionDef
         13    C 001C8490 TFE_ContextAsyncClearError
         14    D 001C84A0 TFE_ContextAsyncWait
         15    E 001C84F0 TFE_ContextClearCaches
         16    F 001C8500 TFE_ContextDisableRunMetadata
         17   10 001C8510 TFE_ContextEnableRunMetadata
         18   11 001C8520 TFE_ContextEndStep
         19   12 001C8530 TFE_ContextExportRunMetadata
         20   13 001C8620 TFE_ContextGetDevicePlacementPolicy
         21   14 001C8630 TFE_ContextListDevices
         22   15 001C86A0 TFE_ContextOptionsSetAsync
         23   16 001C86B0 TFE_ContextOptionsSetConfig
         24   17 001C86C0 TFE_ContextOptionsSetDevicePlacementPolicy
         25   18 001C86D0 TFE_ContextSetAsyncForThread
         26   19 001C8720 TFE_ContextSetServerDef
         27   1A 001C87E0 TFE_ContextSetThreadLocalDevicePlacementPolicy
         28   1B 001C87F0 TFE_ContextStartStep
         29   1C 001C8800 TFE_DeleteContext
         30   1D 001C8830 TFE_DeleteContextOptions
         31   1E 001C8870 TFE_DeleteOp
         32   1F 001BFB40 TFE_DeleteTensorDebugInfo
         33   20 001C88A0 TFE_DeleteTensorHandle
         34   21 001C88F0 TFE_Execute
         35   22 001C8A50 TFE_NewContext
         36   23 001C8C40 TFE_NewContextOptions
         37   24 001C8C80 TFE_NewOp
         38   25 001C8E70 TFE_NewTensorHandle
         39   26 001C8F60 TFE_OpAddInput
         40   27 001C8F70 TFE_OpGetAttrType
         41   28 001C90E0 TFE_OpGetDevice
         42   29 001C9120 TFE_OpNameGetAttrType
         43   2A 001C91A0 TFE_OpSetAttrBool
         44   2B 001C9200 TFE_OpSetAttrBoolList
         45   2C 001C93B0 TFE_OpSetAttrFloat
         46   2D 001C9430 TFE_OpSetAttrFloatList
         47   2E 001C94B0 TFE_OpSetAttrFunction
         48   2F 001C9640 TFE_OpSetAttrFunctionList
         49   30 001C9850 TFE_OpSetAttrInt
         50   31 001C98D0 TFE_OpSetAttrIntList
         51   32 001C9950 TFE_OpSetAttrShape
         52   33 001C9C30 TFE_OpSetAttrShapeList
         53   34 001C9FF0 TFE_OpSetAttrString
         54   35 001CA070 TFE_OpSetAttrStringList
         55   36 001CA1E0 TFE_OpSetAttrType
         56   37 001CA230 TFE_OpSetAttrTypeList
         57   38 001CA2B0 TFE_OpSetDevice
         58   39 001CA300 TFE_OpSetXLACompilation
         59   3A 001BFB90 TFE_TensorDebugInfoOnDeviceDim
         60   3B 001BFBA0 TFE_TensorDebugInfoOnDeviceNumDims
         61   3C 001CA370 TFE_TensorHandleCopyToDevice
         62   3D 001CA420 TFE_TensorHandleDataType
         63   3E 001CA430 TFE_TensorHandleDeviceName
         64   3F 001CA510 TFE_TensorHandleDim
         65   40 001CA5C0 TFE_TensorHandleNumDims
         66   41 001CA670 TFE_TensorHandleResolve
         67   42 001BFBB0 TFE_TensorHandleTensorDebugInfo
         68   43 00372290 TF_AbortWhile
         69   44 003722A0 TF_AddControlInput
         70   45 003722B0 TF_AddGradients
         71   46 00372300 TF_AddGradientsWithPrefix
         72   47 00372BB0 TF_AddInput
         73   48 00372BC0 TF_AddInputList
         74   49 00372E50 TF_AllocateTensor
         75   4A 00372EC0 TF_ApiDefMapGet
         76   4B 00373070 TF_ApiDefMapPut
         77   4C 00373200 TF_CloseDeprecatedSession
         78   4D 00373200 TF_CloseSession
         79   4E 00373250 TF_ColocateWith
         80   4F 00373380 TF_DataTypeSize
         81   50 00373390 TF_DeleteApiDefMap
         82   51 003733C0 TF_DeleteBuffer
         83   52 00373400 TF_DeleteDeprecatedSession
         84   53 00373480 TF_DeleteDeviceList
         85   54 00366220 TF_DeleteFunction
         86   55 003734B0 TF_DeleteGraph
         87   56 00373520 TF_DeleteImportGraphDefOptions
         88   57 00373570 TF_DeleteImportGraphDefResults
         89   58 003735A0 TF_DeleteLibraryHandle
         90   59 003735D0 TF_DeletePRunHandle
         91   5A 003735E0 TF_DeleteSession
         92   5B 00373720 TF_DeleteSessionOptions
         93   5C 00373760 TF_DeleteStatus
         94   5D 003737B0 TF_DeleteTensor
         95   5E 00373820 TF_DeprecatedSessionListDevices
         96   5F 003738B0 TF_DeviceListCount
         97   60 003738E0 TF_DeviceListIncarnation
         98   61 00373A30 TF_DeviceListMemoryBytes
         99   62 00373B80 TF_DeviceListName
        100   63 00373CD0 TF_DeviceListType
        101   64 00373E20 TF_Dim
        102   65 00373E30 TF_ExtendGraph
        103   66 00373EF0 TF_FinishOperation
        104   67 00374250 TF_FinishWhile
        105   68 00366250 TF_FunctionGetAttrValueProto
        106   69 00366430 TF_FunctionImportFunctionDef
        107   6A 00366540 TF_FunctionName
        108   6B 00366570 TF_FunctionSetAttrValueProto
        109   6C 00366760 TF_FunctionToFunctionDef
        110   6D 00374890 TF_GetAllOpList
        111   6E 00374B60 TF_GetAllRegisteredKernels
        112   6F 00374C40 TF_GetBuffer
        113   70 001E4360 TF_GetCode
        114   71 00374C60 TF_GetOpList
        115   72 00374C80 TF_GetRegisteredKernelsForOp
        116   73 003667B0 TF_GraphCopyFunction
        117   74 003669D0 TF_GraphGetFunctions
        118   75 00374D90 TF_GraphGetOpDef
        119   76 00374F20 TF_GraphGetTensorNumDims
        120   77 00375050 TF_GraphGetTensorShape
        121   78 00375260 TF_GraphImportGraphDef
        122   79 003752A0 TF_GraphImportGraphDefWithResults
        123   7A 003753F0 TF_GraphImportGraphDefWithReturnOutputs
        124   7B 00375610 TF_GraphNextOperation
        125   7C 00366B80 TF_GraphNumFunctions
        126   7D 003756D0 TF_GraphOperationByName
        127   7E 003757F0 TF_GraphSetTensorShape
        128   7F 00366BF0 TF_GraphToFunction
        129   80 00375950 TF_GraphToGraphDef
        130   81 00375A10 TF_GraphVersions
        131   82 00375AD0 TF_ImportGraphDefOptionsAddControlDependency
        132   83 00375B00 TF_ImportGraphDefOptionsAddInputMapping
        133   84 00375CF0 TF_ImportGraphDefOptionsAddReturnOperation
        134   85 00375E70 TF_ImportGraphDefOptionsAddReturnOutput
        135   86 00375FC0 TF_ImportGraphDefOptionsNumReturnOperations
        136   87 00375FD0 TF_ImportGraphDefOptionsNumReturnOutputs
        137   88 00376000 TF_ImportGraphDefOptionsRemapControlDependency
        138   89 00376110 TF_ImportGraphDefOptionsSetPrefix
        139   8A 00376130 TF_ImportGraphDefOptionsSetUniquifyNames
        140   8B 00376140 TF_ImportGraphDefOptionsSetUniquifyPrefix
        141   8C 00376150 TF_ImportGraphDefResultsMissingUnusedInputMappings
        142   8D 00376170 TF_ImportGraphDefResultsReturnOperations
        143   8E 00376190 TF_ImportGraphDefResultsReturnOutputs
        144   8F 003761B0 TF_LoadLibrary
        145   90 00376260 TF_LoadSessionFromSavedModel
        146   91 00376890 TF_Message
        147   92 003768C0 TF_NewApiDefMap
        148   93 003769B0 TF_NewBuffer
        149   94 003769E0 TF_NewBufferFromString
        150   95 00376A40 TF_NewDeprecatedSession
        151   96 00376AD0 TF_NewGraph
        152   97 00376B00 TF_NewImportGraphDefOptions
        153   98 00376B70 TF_NewOperation
        154   99 00376BE0 TF_NewSession
        155   9A 00376D30 TF_NewSessionOptions
        156   9B 00376D60 TF_NewStatus
        157   9C 00376D90 TF_NewTensor
        158   9D 00377010 TF_NewWhile
        159   9E 00377430 TF_NumDims
        160   9F 00377440 TF_OperationDevice
        161   A0 00377460 TF_OperationGetAttrBool
        162   A1 00377500 TF_OperationGetAttrBoolList
        163   A2 003775F0 TF_OperationGetAttrFloat
        164   A3 003776A0 TF_OperationGetAttrFloatList
        165   A4 00377790 TF_OperationGetAttrInt
        166   A5 00377840 TF_OperationGetAttrIntList
        167   A6 00377930 TF_OperationGetAttrMetadata
        168   A7 00377F90 TF_OperationGetAttrShape
        169   A8 003780F0 TF_OperationGetAttrShapeList
        170   A9 003783B0 TF_OperationGetAttrString
        171   AA 00378490 TF_OperationGetAttrStringList
        172   AB 00378640 TF_OperationGetAttrTensor
        173   AC 00378730 TF_OperationGetAttrTensorList
        174   AD 00378910 TF_OperationGetAttrTensorShapeProto
        175   AE 003789B0 TF_OperationGetAttrTensorShapeProtoList
        176   AF 00378B60 TF_OperationGetAttrType
        177   B0 00378C00 TF_OperationGetAttrTypeList
        178   B1 00378CF0 TF_OperationGetAttrValueProto
        179   B2 00378D60 TF_OperationGetControlInputs
        180   B3 00378ED0 TF_OperationGetControlOutputs
        181   B4 00379040 TF_OperationInput
        182   B5 003790D0 TF_OperationInputListLength
        183   B6 00379290 TF_OperationInputType
        184   B7 003792A0 TF_OperationName
        185   B8 003792C0 TF_OperationNumControlInputs
        186   B9 003793F0 TF_OperationNumControlOutputs
        187   BA 00379520 TF_OperationNumInputs
        188   BB 00379530 TF_OperationNumOutputs
        189   BC 00379540 TF_OperationOpType
        190   BD 00379560 TF_OperationOutputConsumers
        191   BE 00379700 TF_OperationOutputListLength
        192   BF 003798C0 TF_OperationOutputNumConsumers
        193   C0 003799F0 TF_OperationOutputType
        194   C1 00379A00 TF_OperationToNodeDef
        195   C2 00379A60 TF_PRun
        196   C3 00379E00 TF_PRunSetup
        197   C4 0037A2E0 TF_Reset
        198   C5 0037A2F0 TF_Run
        199   C6 00373820 TF_SessionListDevices
        200   C7 0037A6A0 TF_SessionPRun
        201   C8 0037AB50 TF_SessionPRunSetup
        202   C9 0037B130 TF_SessionRun
        203   CA 0037B5F0 TF_SetAttrBool
        204   CB 0037B640 TF_SetAttrBoolList
        205   CC 0037B7E0 TF_SetAttrFloat
        206   CD 0037B820 TF_SetAttrFloatList
        207   CE 0037B880 TF_SetAttrFuncName
        208   CF 0037BA10 TF_SetAttrInt
        209   D0 0037BA50 TF_SetAttrIntList
        210   D1 0037BAB0 TF_SetAttrShape
        211   D2 0037BBC0 TF_SetAttrShapeList
        212   D3 0037BE20 TF_SetAttrString
        213   D4 0037BE80 TF_SetAttrStringList
        214   D5 0037C150 TF_SetAttrTensor
        215   D6 0037C230 TF_SetAttrTensorList
        216   D7 0037C590 TF_SetAttrTensorShapeProto
        217   D8 0037C720 TF_SetAttrTensorShapeProtoList
        218   D9 0037C9A0 TF_SetAttrType
        219   DA 0037C9E0 TF_SetAttrTypeList
        220   DB 0037CA40 TF_SetAttrValueProto
        221   DC 0037CC70 TF_SetConfig
        222   DD 0037CCD0 TF_SetDevice
        223   DE 0037CD10 TF_SetStatus
        224   DF 0037CDB0 TF_SetTarget
        225   E0 0037CDE0 TF_StringDecode
        226   E1 0037CF00 TF_StringEncode
        227   E2 0037CFE0 TF_StringEncodedSize
        228   E3 0037D000 TF_TensorByteSize
        229   E4 0037D010 TF_TensorData
        230   E5 0037D020 TF_TensorMaybeMove
        231   E6 0037D080 TF_TensorType
        232   E7 0037D090 TF_TryEvaluateConstant
        233   E8 0037D1F0 TF_Version

  Summary

      2D4000 .data
        1000 .gfids
      184000 .pdata
      B36000 .rdata
       53000 .reloc
     2949000 .text
        1000 .tls
       11000 _RDATA
```"
23541,compiled error when build android  TFLite Model Benchmark Tool,"tensorflow version: 0.12
describle:
when i build the android  TFLite Model Benchmark Tool use the command: 
bazel build -c opt \
--config=android_arm \
--cxxopt='--std=c++11' \
tensorflow/lite/tools/benchmark:benchmark_model

some errors occurs:
no such package '@com_google_absl//absl/base': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/abseil/abseil-cpp/archive/f95179062eb65ce40895cc76f1398cce25394369.tar.gz, https://github.com/abseil/abseil-cpp/archive/f95179062eb65ce40895cc76f1398cce25394369.tar.gz] to /root/.cache/bazel/_bazel_root/2f52c1a17bc0577f4e941b98cd34b371/external/com_google_absl/f95179062eb65ce40895cc76f1398cce25394369.tar.gz: All mirrors are down: [Could not initialize class sun.security.ssl.SSLContextImpl$DefaultSSLContext] and referenced by '//tensorflow/lite/kernels/internal:tensor_utils'

build aborted: no such package '@com_google_absl//absl/base': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/abseil/abseil-cpp/archive/f95179062eb65ce40895cc76f1398cce25394369.tar.gz, https://github.com/abseil/abseil-cpp/archive/f95179062eb65ce40895cc76f1398cce25394369.tar.gz] to /root/.cache/bazel/_bazel_root/2f52c1a17bc0577f4e941b98cd34b371/external/com_google_absl/f95179062eb65ce40895cc76f1398cce25394369.tar.gz: All mirrors are down: [Could not initialize class sun.security.ssl.SSLContextImpl$DefaultSSLContext]




"
23540,Broken links in tensorflow for poets2 - codelabs,"Hello,
Links are broken in tensorflow for poets 2 [codelabs](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#2) 
Broken link : 
1] [TensorFlow Lite Optimizing Converter](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/README.md) 
2] [quantized graphs](https://www.tensorflow.org/performance/quantization)

"
23537,Failed to obtain tensors of correct batch size,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Red hat 7.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.10
- Python version: 2.7.5
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 9.0
- GPU model and memory: P5000, 16G


The following code is just retrieving specific digits from mnist of specified batch size. But the returned tensor is just of one sample:

```
import numpy as np
import tensorflow as tf
import sonnet as snt

class Input(snt.AbstractModule):
    def __init__(self, batch_size, name = ""input""):
        super(Input, self).__init__(name = name)

        mnist = tf.keras.datasets.mnist

        (X_train, Y_train), (X_test, Y_test) = mnist.load_data()

        train_filter = np.where((Y_train == 0 ) | (Y_train == 1))
        test_filter = np.where((Y_test == 0) | (Y_test == 1))

        X_train, Y_train = X_train[train_filter], Y_train[train_filter]
        X_test, Y_test = X_test[test_filter], Y_test[test_filter]

        print(X_train.shape)
        print(Y_train.shape)

        with self._enter_variable_scope():
            self._db_train = tf.data.Dataset.from_tensor_slices((X_train, Y_train))
            self._db_test = tf.data.Dataset.from_tensor_slices((X_test, Y_test))

            self._db_train.repeat(-1)
            self._db_test.repeat(-1)

            self._db_train.batch(batch_size)
            self._db_test.batch(batch_size)

            # self._db_train.apply(tf.contrib.data.batch_and_drop_remainder(batch_size))
            # self._db_test.apply(tf.contrib.data.batch_and_drop_remainder(batch_size))

            self._it_train = self._db_train.make_one_shot_iterator()
            self._it_test = self._db_test.make_one_shot_iterator()


    def _build(self, is_training = True):

        if is_training:
            inputs, labels = self._it_train.get_next()
        else:
            inputs, labels = self._it_test.get_next()

        return inputs, labels

def test():
    input_ = Input(32)

    inputs, labels = input_()

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())

        inputs_val, labels_val = sess.run([inputs, labels])


        print(inputs_val.shape)
        print(labels_val.shape)


if __name__ == ""__main__"":
    test()

```

The output of the above code snippet is as follows:

```
(12665, 28, 28)
(12665,)
(28, 28)
()

```
Note I deleted something irrelevant."
23536,Build CMAKE C++/Cuda project with tensorflow in debug mode,"**System information**
- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow installed from binary
- TensorFlow version: 1.11.0
- Python version: 3.6.5
- Installed using pip
- CUDA/cuDNN version: 9.0
- GPU model and memory: Tesla K and V series



**Describe the problem**

I am trying to integrate tensorflow in my CMAKE C++/Cuda project. It _works_ when I link tensorflow when I do simply `cmake .`, which is for `RelWithDebInfo` build. However, it fails on `Release` or `Debug` mode. I speculate that the `libtensorflow_framework.so` that I have on my `dist-packages/tensorflow/` is not meant to be for that. Below, I share 2 different ways I try to link tensorflow in debug mode. Both give me the same error as found further below.

**_Can you hint me what goes wrong?_**

## Add Tensorflow N.1
```
SET(TF_ROOT ""/usr/local/lib/python3.6/dist-packages/tensorflow/"")
include_directories(SYSTEM ""${TF_ROOT}/include/"")
link_directories(""${TF_ROOT}"")
target_link_libraries(
	CppLib.Tests
  	debug tensorflow_framework
)
```

## Add Tensorflow N.2
```
SET(TF_ROOT ""/usr/local/lib/python3.6/dist-packages/tensorflow/"")
include_directories(SYSTEM ""${TF_ROOT}/include/"")
#link_directories(""${TF_ROOT}"")

add_executable(CppLib.Tests ${SRC} ${GMOCK_SRC} ${GTEST_SRC} ${GTEST_MAIN_SRC})

add_library(tensorflow_framework_ SHARED IMPORTED)
set_target_properties(tensorflow_framework_ PROPERTIES
                IMPORTED_LOCATION_DEBUG ""${TF_ROOT}/libtensorflow_framework.so"")
set_target_properties(tensorflow_framework_ PROPERTIES
                IMPORTED_LOCATION_RELEASE ""${TF_ROOT}/libtensorflow_framework.so"")

target_link_libraries(
	CppLib.Tests
        tensorflow_framework_
)
```



## Error
```
#ERROR
CMakeFiles/CppLib.Tests.dir/TestTensorflow.cpp.o: In function `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >* tensorflow::internal::MakeCheckOpString<long, int>(long const&, int const&, char const*)':
/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/platform/default/logging.h:187: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString[abi:cxx11]()'
collect2: error: ld returned 1 exit status
tests/CppLib/CMakeFiles/CppLib.Tests.dir/build.make:799: recipe for target 'bin/Debug/CppLib.Tests' failed
make[2]: *** [bin/Debug/CppLib.Tests] Error 1
CMakeFiles/Makefile2:1169: recipe for target 'tests/CppLib/CMakeFiles/CppLib.Tests.dir/all' failed
make[1]: *** [tests/CppLib/CMakeFiles/CppLib.Tests.dir/all] Error 2
Makefile:140: recipe for target 'all' failed
make: *** [all] Error 2
```
"
23535, tf.manip.roll implementation details,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 1.8
- Doc Link: https://www.tensorflow.org/api_docs/python/tf/manip/roll


**Describe the documentation issue**
How is this operation executed? If i ask the tensor of size n to shift by 2, is there a new tensor allocated with size n+2, and the rotation happens by two memcopies of the first/last two values(based on direction of rotation) ?

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
23534,Tensorflow Lite Android - No Operation named [input] in the Graph,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Nexus 5X, Samsung S5
- TensorFlow installed from (source or binary):
- TensorFlow version:  1.12.0-rc2
- Python version: 3
- Installed using virtualenv? pip? conda?:  Google Colab
- Bazel version (if compiling from source): Google Colab
- GCC/Compiler version (if compiling from source): Google Colab
- CUDA/cuDNN version: Google Colab
- GPU model and memory: Google Colab


I am building a simple random model with keras, export it as tensorflow lite model from Google Colab, load it on Android, but i am unnable to run classification

My Colab notebook file is available here:
https://colab.research.google.com/drive/1vf9wVXlNQ77GUF_JyL3bKv0M72aK0xvG

The resulting model can be found here
[model.zip](https://github.com/tensorflow/tensorflow/files/2550726/model.zip)

When i open the model with Netron it looks like following:
![model](https://user-images.githubusercontent.com/2804470/48028365-9695da00-e14b-11e8-8ae1-e262510afead.png)

The Android code:
` 
implementation 'org.tensorflow:tensorflow-android:1.12.0-rc1'

private static final String MODEL_FILE = ""file:///android_asset/model.tflite"";
private static final String INPUT_NODE = ""input"";
private static final long[] INPUT_SIZE = {3};
private static final String[] OUTPUT_NODES = {""output/Softmax""};
private static final String OUTPUT_NODE = ""output/Softmax"";
private static final int OUTPUT_SIZE = 2;

inferenceInterface = new TensorFlowInferenceInterface(inputStream);
float[] data = new float[]{100,200,300};
inferenceInterface.feed(INPUT_NODE, data, INPUT_SIZE);`

But I when i call ""feed"", I always always an exception:
` java.lang.IllegalArgumentException: No Operation named [input] in the Graph`

I tried different input names, and also specify the input from the converter, but still no success and i am really stucked here. 
Does anyone have an idea what i am actually doing wrong?

"
23533,Failed to load the native TensorFlow runtime error,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10 Home
- TensorFlow installed from (source or binary): from cmd using command ""pip install tensorflow""
- TensorFlow version:  tensorflow-1.11.0
- Python version: 3.6.7amd 64
- Installed using virtualenv? pip? conda?: pip install tensorflow
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Problem**
 I thought that was a problem due to my python version Python 3.7.1, so I deleted it and installed a new python 3.6.7.
Then I used cmd to get to script and run ""pip install tensorflow""
Then I created a simple py file ""import tensorflow as tf"" 

I got so many mistakes, such as 
""ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.
Failed to load the native TensorFlow runtime..""

so I removed ""pip uninstall tensorflow"". Then, I tried ""pip install --upgrade tensorflow"" still same errors.
Then ""pip3 install --upgrade tensorflow"" Still cant use tensorflow
Please help me


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Traceback (most recent call last):
  File ""C:\Users\elmia\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\elmia\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\elmia\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\elmia\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\elmia\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\elmia\Desktop\python_test\deep_learning_tensorflow_sucks\1.py"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\elmia\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\elmia\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\elmia\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\elmia\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\elmia\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\elmia\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\elmia\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\elmia\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
23531,Porting tf-coriander to Tensorflow r1.11. Have one general question and one building issue.,"Hello
I am working on to improve tf-coriander and to port it on Tensorflow r1.11 since I could not find someone else's doing it on the web. If there is someone else working on this, please let me know if you want to work together and share the result with others. 
After spending a few weeks working on this, I got the stage to compile properly but fail in the linker. The error message is like:
""..._pywrap_tensorflow_internal.so: undefined symbol: _ZNK10tensorflow7functor4TileIN5Eigen9GpuDeviceEiiEclERKS3_PNS_6TensorERKS7_N4absl4SpanIKiEE"". 

I only have tile_functor commented out 'GOOGLE_CUDA"" as Hugh Perkins suggested. It seems to me that Tensorflow r1.11 separates  the functor API from their implementations, and pack those into different share objects. I am not sure which BUILD file need be changed to make them linked statically. (I have tried 'framework_shared_object' label.)

Could someone share some lights on how to address the issue? or is it the right path to port tf-coriander?


thank you very much,

Steven Wang 

"
23530,Limit Tensorflow running in 1 thread,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.10
- Python version: 2.7
- Bazel version (if compiling from source): 0.16.1
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
- CUDA/cuDNN version:
- GPU model and memory:

I was asked to write a C++ program running inference on Tensorflow without using more than 1 thread. Here is my code: 

```
    tensorflow::SessionOptions options;
    tensorflow::ConfigProto & config = options.config;
    config.set_inter_op_parallelism_threads(1);
    config.set_intra_op_parallelism_threads(1);
    config.set_use_per_session_threads(false);
    
   // also tried set_use_per_session_threads to true and 1 thread pool
    //auto* p = config.add_session_inter_op_thread_pool();
    //p->set_global_name(""large pool"");
    //p->set_num_threads(1);

```

The program indeed uses only 1 core (cpu by htop only 99-100%), however, it always spawns 10 thread. Am I missing anything? 

Thank you in advance. 
"
23529,Oscillating validation loss,"I am training a deep CNN for image dehazing, and during training I plot the training and the validations losses. I plot the validation loss every 1 epoch. I do all the process on Tensorflow framework in Ubuntu 16 . The problem is my training loss decrease really well but my validation loss oscillates and it does not have a decreasing shape. Is it normal?

Here are the images of training and validation loss plots. The orange one is the training loss and the blue one is the validation loss.

The x-axis on the train plot is the iteration number and the x-axis on the validation plot is the epoch number.

![train](https://user-images.githubusercontent.com/32719272/48010891-7daa0c00-e0ec-11e8-95b3-863e66b0f044.png)

![validation](https://user-images.githubusercontent.com/32719272/48010905-84d11a00-e0ec-11e8-9502-bc0eb65fd177.png)


I really appreciate if you help me with this problem to figure out what part of my approach is wrong"
23528,Convert to .tflite from saved checkpoint,"**System information**
- TensorFlow version: 1.10.0
- Are you willing to contribute it  Yes:



**Describe the feature and the current behavior/state.**
tflite_convert --output_file=test.tflite --saved_model_dir=test_checkpoint_saved_data

output:
Traceback (most recent call last):
  File ""/usr/local/bin/tflite_convert"", line 11, in <module>
    sys.exit(main())
  File ""/Library/Python/2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 370, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/Library/Python/2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/Library/Python/2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 366, in run_main
    _convert_model(tflite_flags)
  File ""/Library/Python/2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 94, in _convert_model
    converter = _get_toco_converter(flags)
  File ""/Library/Python/2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 81, in _get_toco_converter
    return converter_fn(**converter_kwargs)
  File ""/Library/Python/2.7/site-packages/tensorflow/contrib/lite/python/lite.py"", line 268, in from_saved_model
    output_arrays, tag_set, signature_key)
  File ""/Library/Python/2.7/site-packages/tensorflow/contrib/lite/python/convert_saved_model.py"", line 239, in freeze_saved_model
    meta_graph = _get_meta_graph_def(saved_model_dir, tag_set)
  File ""/Library/Python/2.7/site-packages/tensorflow/contrib/lite/python/convert_saved_model.py"", line 61, in _get_meta_graph_def
    return loader.load(sess, tag_set, saved_model_dir)
  File ""/Library/Python/2.7/site-packages/tensorflow/python/saved_model/loader_impl.py"", line 211, in load
    loader = SavedModelLoader(export_dir)
  File ""/Library/Python/2.7/site-packages/tensorflow/python/saved_model/loader_impl.py"", line 230, in __init__
    self._saved_model = _parse_saved_model(export_dir)
  File ""/Library/Python/2.7/site-packages/tensorflow/python/saved_model/loader_impl.py"", line 80, in _parse_saved_model
    constants.SAVED_MODEL_FILENAME_PB))
IOError: SavedModel file does not exist at: test_checkpoint_saved_data/{saved_model.pbtxt|saved_model.pb}

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**
"
23527,Acc and loss evaluate to 0.00 and trains for only 1 global steps.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):1.12
- Python version:3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:1xTesla K80  12GB GDDR5 VRAM


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
My model's accuracy and loss are evaluating to 0.

**Describe the expected behavior**
The global steps should be 1625 but it's 1.
The acc and loss shouldn't be equal to 0 as both of them are contradicting each other.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
my input function,keras estimator,train_and_evaluate are as follows:
```
def make_input_fn(addrs,labels,batch_size,mode):
    
    filename_dataset = tf.data.Dataset.from_tensor_slices((addrs,labels))     
    
    dataset = filename_dataset.apply(tf.contrib.data.map_and_batch(lambda addrs, labels: tuple(tf.py_func(
        process, [addrs, labels], [tf.uint8, labels.dtype])),
                                                                   batch_size,
                                                                   num_parallel_batches=2,
                                                                   drop_remainder=False))
    if mode == tf.estimator.ModeKeys.TRAIN:
      num_epochs = None # indefinitely
      dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(buffer_size = 10000))
    else:
      num_epochs = 1
      dataset = dataset.repeat(num_epochs)
   
    dataset = dataset.prefetch(buffer_size=batch_size)
    images,labels = dataset.make_one_shot_iterator().get_next()
    images.set_shape([None,512,512,3])
    labels.set_shape([None,1])
    return images,labels
`

def keras_estimator(model_dir,config):
  base_model = Xception(weights='imagenet', include_top=False,input_shape = (512,512,3),classes = 5)


  x = base_model.output
  x = GlobalAveragePooling2D()(x)
  
  x = Dense(1024, activation='relu')(x)
  x = Dropout(0.2)(x)
  x = Dense(256, activation='relu')(x)
  x = Dropout(0.2)(x)
  
  predictions = Dense(5, activation='softmax')(x)

  
  model = Model(inputs=base_model.input, outputs=predictions)

  
  for layer in base_model.layers:
      layer.trainable = False
  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])
  estimator = tf.keras.estimator.model_to_estimator(keras_model=model,model_dir=model_dir,
    config=config)
  return estimator


def train_and_evaluate(model_dir):
  t_batch_size = 512
  e_batch_size = 64
  num_epochs = 25
  import pandas as pd
  df = pd.read_csv('/content/trainLabels.csv')
  from random import shuffle
  addrs = ['/content/train/train/' + str(df.iloc[i]['image']) + '.jpeg' for i in range(len(df))]
  labels = df['level'].values.tolist()
  c = list(zip(addrs, labels))
  shuffle(c)
  addrs1, labels1 = zip(*c)
  train_addrs = addrs1[0 : int(0.9 * len(addrs))]
  train_labels = labels1[0 : int(0.9 * len(labels))]
  val_addrs = addrs1[ int(0.9 * len(addrs)) : ]
  val_labels = labels1[ int(0.9 * len(addrs)) : ]
  train_addrs = list(train_addrs)
  train_labels = list(train_labels)
  val_addrs = list(val_addrs)
  val_labels = list(val_labels)
  
  run_config = tf.estimator.RunConfig(save_checkpoints_secs=300)
  
  estimator = keras_estimator(model_dir,run_config)
  
  t_max_steps = (len(train_addrs) // t_batch_size) * num_epochs
  
  train_spec = tf.estimator.TrainSpec(input_fn = lambda : make_input_fn(train_addrs,train_labels,t_batch_size,mode=tf.estimator.ModeKeys.TRAIN),max_steps = t_max_steps)
  
  eval_spec = tf.estimator.EvalSpec(input_fn = lambda : make_input_fn(val_addrs,val_labels,e_batch_size,mode=tf.estimator.ModeKeys.EVAL),steps = None,start_delay_secs=10,
        throttle_secs=300)
  
  
  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
```
    




**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
`INFO:tensorflow:Running training and evaluation locally (non-distributed).
INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 300.
WARNING:tensorflow:From <ipython-input-7-80b5bdaf35df>:9: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.experimental.map_and_batch(...)`.
WARNING:tensorflow:From <ipython-input-7-80b5bdaf35df>:12: shuffle_and_repeat (from tensorflow.contrib.data.python.ops.shuffle_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.experimental.shuffle_and_repeat(...)`.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='/content/training/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})
INFO:tensorflow:Warm-starting from: ('/content/training/keras/keras_model.ckpt',)
INFO:tensorflow:Warm-starting variable: dense/kernel; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: dense/bias; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: dense_1/kernel; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: dense_1/bias; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: dense_2/kernel; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: dense_2/bias; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: Adam/iterations; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: Adam/lr; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: Adam/beta_1; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: Adam/beta_2; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: Adam/decay; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_1; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_2; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_3; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_4; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_5; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_6; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_7; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_8; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_9; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_10; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_11; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_12; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_13; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_14; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_15; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_16; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_17; prev_var_name: Unchanged
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Saving checkpoints for 0 into /content/training/model.ckpt.
INFO:tensorflow:Saving checkpoints for 1 into /content/training/model.ckpt.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Starting evaluation at 2018-11-05-13:21:17
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Restoring parameters from /content/training/model.ckpt-1
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Finished evaluation at 2018-11-05-13:22:08
INFO:tensorflow:Saving dict for global step 1: acc = 0.0, global_step = 1, loss = 0.0
INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1: /content/training/model.ckpt-1
INFO:tensorflow:Loss for final step: None.`

"
23526,Tensor flow is breaking with Failed to load the native TensorFlow runtime,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 8.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No Mobile device used
- TensorFlow installed from (source or binary):
   Binary used Pip command
- TensorFlow version:
Name: tensorflow
Version: 1.11.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author: Google Inc.
Author-email: opensource@google.com
License: Apache 2.0
Location: c:\users\avinash.t\anaconda3\lib\site-packages
Requires: tensorboard, numpy, six, keras-applications, setuptools, grpcio, gast,
 protobuf, termcolor, wheel, astor, absl-py, keras-preprocessing


- Python version:
C:\Users\avinash.t>python --version
Python 3.6.7 :: Anaconda, Inc.
- Installed using virtualenv? pip? conda?:
pip
- Bazel version (if compiling from source):
 Not complied from Source
- GCC/Compiler version (if compiling from source):
No use of GCC
- CUDA/cuDNN version:
No use of CUDA
- GPU model and memory:
No use o GPU


**Describe the problem**
Simple import is not working

**Provide the exact sequence of commands / steps that you executed before running into the problem**
C:\Users\avinash.t>python
Python 3.6.7 |Anaconda, Inc.| (default, Oct 28 2018, 19:44:12) [MSC v.1915 64 bi
t (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\avinash.t\Anaconda3\lib\site-packages\tensorflow\python\pywrap_
tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\avinash.t\Anaconda3\lib\site-packages\tensorflow\python\pywrap_
tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\avinash.t\Anaconda3\lib\site-packages\tensorflow\python\pywrap_
tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript
ion)
  File ""C:\Users\avinash.t\Anaconda3\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\avinash.t\Anaconda3\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routin
e failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\avinash.t\Anaconda3\lib\site-packages\tensorflow\__init__.py"",
line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-im
port
  File ""C:\Users\avinash.t\Anaconda3\lib\site-packages\tensorflow\python\__init_
_.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\avinash.t\Anaconda3\lib\site-packages\tensorflow\python\pywrap_
tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\avinash.t\Anaconda3\lib\site-packages\tensorflow\python\pywrap_
tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\avinash.t\Anaconda3\lib\site-packages\tensorflow\python\pywrap_
tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\avinash.t\Anaconda3\lib\site-packages\tensorflow\python\pywrap_
tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript
ion)
  File ""C:\Users\avinash.t\Anaconda3\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\avinash.t\Anaconda3\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routin
e failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_probl
ems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>>

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
C:\Users\avinash.t>python
Python 3.6.7 |Anaconda, Inc.| (default, Oct 28 2018, 19:44:12) [MSC v.1915 64 bi
t (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\avinash.t\Anaconda3\lib\site-packages\tensorflow\python\pywrap_
tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\avinash.t\Anaconda3\lib\site-packages\tensorflow\python\pywrap_
tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\avinash.t\Anaconda3\lib\site-packages\tensorflow\python\pywrap_
tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript
ion)
  File ""C:\Users\avinash.t\Anaconda3\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\avinash.t\Anaconda3\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routin
e failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\avinash.t\Anaconda3\lib\site-packages\tensorflow\__init__.py"",
line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-im
port
  File ""C:\Users\avinash.t\Anaconda3\lib\site-packages\tensorflow\python\__init_
_.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\avinash.t\Anaconda3\lib\site-packages\tensorflow\python\pywrap_
tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\avinash.t\Anaconda3\lib\site-packages\tensorflow\python\pywrap_
tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\avinash.t\Anaconda3\lib\site-packages\tensorflow\python\pywrap_
tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\avinash.t\Anaconda3\lib\site-packages\tensorflow\python\pywrap_
tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript
ion)
  File ""C:\Users\avinash.t\Anaconda3\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\avinash.t\Anaconda3\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routin
e failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_probl
ems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>>


"
23525,tf.contrib.estimator.InMemoryEvaluatorHook does not work with MirroredStrategy,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 3.10.0-862.9.1.el7.x86_64, CentOS 7.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): ('v1.9.0-rc2-2836-g05f8ea8', '1.10.0')
- Python version: 2.7
- Bazel version (if compiling from source): 0.16.1
- GCC/Compiler version (if compiling from source): c++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)
- CUDA/cuDNN version: 9.2.148
- GPU model and memory: 8x GTX 1080 Ti, 11178MiB

**Describe the current behavior**
When using tf.contrib.estimator.InMemoryEvaluatorHook together with tf.contrib.distribute.MirroredStrategy on more than 1 GPU I get an error as soon as evaluation starts:
RuntimeError: Graph is finalized and cannot be modified.

This does not occur when using OneDeviceStrategy. The documentation of InMemoryEvaluatorHook states: ""It doesn't support multi-node distributed mode."" However, I am on a single node and multi-GPU

**Describe the expected behavior**
The error should not occur. If multi-GPU support is not supported yet, will it be in the future? The documentation should also be updated then.

**Code to reproduce the issue**
distribution = tf.contrib.distribute.MirroredStrategy(num_gpus=2)
run_config = tf.estimator.RunConfig(train_distribute=distribution)
estimator = tf.estimator.Estimator(model.model_fn, config=run_config)
evaluator = tf.contrib.estimator.InMemoryEvaluatorHook(estimator, eval_input_fn, steps=20, every_n_iter=20)
estimator.train(train_input_fn, steps=100, hooks=[evaluator])

**Other info / logs**
Traceback (most recent call last):
  File ""/home/xxx/.vscode/extensions/ms-python.python-2018.9.2/pythonFiles/experimental/ptvsd_launcher.py"", line 115, in <module>
    vspd.run(filename, port_num, run_as, *sys.argv[1:])
  File ""/home/xxx/.vscode/extensions/ms-python.python-2018.9.2/pythonFiles/experimental/ptvsd/ptvsd/debugger.py"", line 43, in run
    run_main(address, filename, run_as, *args, **kwargs)
  File ""/home/xxx/.vscode/extensions/ms-python.python-2018.9.2/pythonFiles/experimental/ptvsd/ptvsd/_local.py"", line 52, in run_main
    runner(addr, name, kind == 'module', *extra, **kwargs)
  File ""/home/xxx/.vscode/extensions/ms-python.python-2018.9.2/pythonFiles/experimental/ptvsd/ptvsd/runner.py"", line 32, in run
    set_trace=False)
  File ""/home/xxx/.vscode/extensions/ms-python.python-2018.9.2/pythonFiles/experimental/ptvsd/ptvsd/_vendored/pydevd/pydevd.py"", line 1107, in run
    return self._exec(is_module, entry_point_fn, module_name, file, globals, locals)
  File ""/home/xxx/.vscode/extensions/ms-python.python-2018.9.2/pythonFiles/experimental/ptvsd/ptvsd/_vendored/pydevd/pydevd.py"", line 1114, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""/home/xxx/test/mv_estimator/train.py"", line 91, in <module>
    tf.app.run()
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/home/xxx/test/mv_estimator/train.py"", line 80, in main
    estimator.train(train_input_fn, steps=100, hooks=[evaluator])
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1177, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1320, in _train_model_distributed
    saving_listeners)
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1400, in _train_with_estimator_spec
    log_step_count_steps=self._config.log_step_count_steps) as mon_sess:
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 504, in MonitoredTrainingSession
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 920, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1106, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1111, in _create_session
    return self._sess_creator.create_session()
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 806, in create_session
    hook.after_create_session(self.tf_sess, self.coord)
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/contrib/estimator/python/estimator/hooks.py"", line 178, in after_create_session
    self._evaluate(session)
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/contrib/estimator/python/estimator/hooks.py"", line 181, in _evaluate
    var_name_to_value = train_session.run(self._var_name_to_train_var)
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 887, in run
    run_metadata_ptr)
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1095, in _run
    self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 429, in __init__
    self._fetch_mapper = _FetchMapper.for_fetch(fetches)
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 249, in for_fetch
    return _DictFetchMapper(fetch)
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 387, in __init__
    _FetchMapper.for_fetch(fetch) for fetch in fetches.values()
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 255, in for_fetch
    return _ElementFetchMapper(fetches, contraction_fn)
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 284, in __init__
    fetch, allow_tensor=True, allow_operation=True))
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3473, in as_graph_element
    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3496, in _as_graph_element_locked
    temp_obj = _as_graph_element(obj)
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 148, in _as_graph_element
    return conv_fn()
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/values.py"", line 513, in _as_graph_element
    return self._get_cross_tower()
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/values.py"", line 505, in _get_cross_tower
    total = math_ops.add_n(all_components)
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py"", line 2145, in add_n
    inputs = ops.convert_n_to_tensor_or_indexed_slices(inputs)
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1355, in convert_n_to_tensor_or_indexed_slices
    values=values, dtype=dtype, name=name, as_ref=False)
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1326, in internal_convert_n_to_tensor_or_indexed_slices
    value, dtype=dtype, name=n, as_ref=as_ref))
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1285, in internal_convert_to_tensor_or_indexed_slices
    value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1124, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 1258, in _dense_var_to_tensor
    return var._dense_var_to_tensor(dtype=dtype, name=name, as_ref=as_ref)  # pylint: disable=protected-access
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 1213, in _dense_var_to_tensor
    return self.value()
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 647, in value
    return self._read_variable_op()
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 730, in _read_variable_op
    self._dtype)
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py"", line 508, in read_variable_op
    ""ReadVariableOp"", resource=resource, dtype=dtype, name=name)
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3232, in create_op
    self._check_not_finalized()
  File ""/home/xxx/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2905, in _check_not_finalized
    raise RuntimeError(""Graph is finalized and cannot be modified."")
"
23524,Python - Import Error of Tensorflow GPU in Ubuntu ,"I am using python 3 with ubuntu 18.04 and have installed the following packages:
cuda 9.0
cudnn-9.0-linux-x64-v7.3.1.20 
tensorflow gpu 1.5

But I am still getting an error while importing tensorflow. 
`Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/rafey/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/home/rafey/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 81, in <module>
    from tensorflow.python import keras
  File ""/home/rafey/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/__init__.py"", line 24, in <module>
    from tensorflow.python.keras import activations
  File ""/home/rafey/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/activations/__init__.py"", line 22, in <module>
    from tensorflow.python.keras._impl.keras.activations import elu
  File ""/home/rafey/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/__init__.py"", line 21, in <module>
    from tensorflow.python.keras._impl.keras import activations
  File ""/home/rafey/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/activations.py"", line 23, in <module>
    from tensorflow.python.keras._impl.keras import backend as K
  File ""/home/rafey/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/backend.py"", line 36, in <module>
    from tensorflow.python.layers import base as tf_base_layers
  File ""/home/rafey/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 25, in <module>
    from tensorflow.python.keras.engine import base_layer
  File ""/home/rafey/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/engine/__init__.py"", line 23, in <module>
    from tensorflow.python.keras.engine.base_layer import InputSpec
  File ""/home/rafey/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 33, in <module>
    from tensorflow.python.keras import backend
  File ""/home/rafey/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/backend/__init__.py"", line 22, in <module>
    from tensorflow.python.keras._impl.keras.backend import abs
ImportError: cannot import name 'abs'`
"
23523,Using tf.data.dataset for inference with high efficiency,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: master
- Doc Link:  https://www.tensorflow.org/guide/datasets

tf.data.dataset provide a high level api for data importing and preprocessing. It's convenient to be used in training process, however, it is unclear how to use them in the inference scenario.  We want to reuse the data preprocessing code  for inference which using the interface of tf.data.dataset. By google search and self experiment I found the following ways to use tf.data.dataset for inference:

```python
#using a placeholder
x = tf.placeholder(tf.float32, shape=[None,2])
dataset = tf.data.Dataset.from_tensor_slices(x)
data = np.random.sample((100,2))
iter = dataset.make_initializable_iterator() # create the iterator
el = iter.get_next()
with tf.Session() as sess:
    # feed the placeholder with data
    sess.run(iter.initializer, feed_dict={ x: data }) 
    print(sess.run(el)) # output [ 0.52374458  0.71968478]
```

But the way above will bring some cost, Specifically ,there are two main operations:
1.   we must run the function sess.run  one more time
2.   dataset object need be to initialize each time

 is it efficient to do in this way,  Or we need another way to use tf.data.dataset so that we can keep efficiency?


-------------------------------
Update:
using tf.contrib.data.get_single_element  or tf.data.experimental.get_single_element"
23522,Conversion from pb to tflite fails,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: - 
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.11.0-0-gc19e29306c
- Python version: python 3.5
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: only CPU
- GPU model and memory: - 

**Describe the current behavior**
I built a Neural network with the following code:

```
import tensorflow as tf
import numpy as np


class AlexNet(object):
    """"""Implementation of the AlexNet.""""""

    def __init__(self, x, keep_prob, num_classes, skip_layer,
                 weights_path='DEFAULT'):
        """"""Create the graph of the AlexNet model.
        Args:
            x: Placeholder for the input tensor.
            keep_prob: Dropout probability.
            num_classes: Number of classes in the dataset.
            skip_layer: List of names of the layer, that get trained from
                scratch
            weights_path: Complete path to the pretrained weight file, if it
                isn't in the same folder as this code
        """"""
        # Parse input arguments into class variables
        self.X = x
        self.NUM_CLASSES = num_classes
        self.KEEP_PROB = keep_prob
        self.SKIP_LAYER = skip_layer

        if weights_path == 'DEFAULT':
            self.WEIGHTS_PATH = 'bvlc_alexnet.npy'
        else:
            self.WEIGHTS_PATH = weights_path

        # Call the create function to build the computational graph of AlexNet
        self.create()

    def create(self):
        """"""Create the network graph.""""""
        # 1st Layer: Conv (w ReLu) -> Lrn -> Pool
        conv1 = conv(self.X, 11, 11, 96, 4, 4, padding='VALID', name='conv1')
        norm1 = lrn(conv1, 2, 2e-05, 0.75, name='norm1')
        pool1 = max_pool(norm1, 3, 3, 2, 2, padding='VALID', name='pool1')

        # 2nd Layer: Conv (w ReLu)  -> Lrn -> Pool with 2 groups
        conv2 = conv(pool1, 5, 5, 256, 1, 1, groups=2, name='conv2')
        norm2 = lrn(conv2, 2, 2e-05, 0.75, name='norm2')
        pool2 = max_pool(norm2, 3, 3, 2, 2, padding='VALID', name='pool2')

        # 3rd Layer: Conv (w ReLu)
        self.conv3 = conv(pool2, 3, 3, 384, 1, 1, name='conv3')

        # 4th Layer: Conv (w ReLu) splitted into two groups
        conv4 = conv(self.conv3, 3, 3, 384, 1, 1, groups=2, name='conv4')

        # 5th Layer: Conv (w ReLu) -> Pool splitted into two groups
        conv5 = conv(conv4, 3, 3, 256, 1, 1, groups=2, name='conv5')
        self.pool5 = max_pool(conv5, 3, 3, 2, 2, padding='VALID', name='pool5')

        # 6th Layer: Flatten -> FC (w ReLu) -> Dropout
        flattened = tf.reshape(self.pool5, [-1, 6*6*256])
        fc6 = fc(flattened, 6*6*256, 4096, name='fc6')
        dropout6 = dropout(fc6, self.KEEP_PROB)

        # 7th Layer: FC (w ReLu) -> Dropout
        fc7 = fc(dropout6, 4096, 4096, name='fc7')
        dropout7 = dropout(fc7, self.KEEP_PROB)

        # 8th Layer: FC and return unscaled activations
        self.fc8 = fc(dropout7, 4096, self.NUM_CLASSES, relu=False, name='fc8')

    def load_initial_weights(self, session):
        """"""Load weights from file into network.
        As the weights from http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/
        come as a dict of lists (e.g. weights['conv1'] is a list) and not as
        dict of dicts (e.g. weights['conv1'] is a dict with keys 'weights' &
        'biases') we need a special load function
        """"""
        # Load the weights into memory
        weights_dict = np.load(self.WEIGHTS_PATH, encoding='bytes').item()

        # Loop over all layer names stored in the weights dict
        for op_name in weights_dict:

            # Check if layer should be trained from scratch
            if op_name not in self.SKIP_LAYER:

                with tf.variable_scope(op_name, reuse=True):

                    # Assign weights/biases to their corresponding tf variable
                    for data in weights_dict[op_name]:

                        # Biases
                        if len(data.shape) == 1:
                            var = tf.get_variable('biases', trainable=False)
                            session.run(var.assign(data))

                        # Weights
                        else:
                            var = tf.get_variable('weights', trainable=False)
                            session.run(var.assign(data))


def conv(x, filter_height, filter_width, num_filters, stride_y, stride_x, name,
         padding='SAME', groups=1):
    """"""Create a convolution layer.
    Adapted from: https://github.com/ethereon/caffe-tensorflow
    """"""
    # Get number of input channels
    input_channels = int(x.get_shape()[-1])

    # Create lambda function for the convolution
    convolve = lambda i, k: tf.nn.conv2d(i, k,
                                         strides=[1, stride_y, stride_x, 1],
                                         padding=padding)

    with tf.variable_scope(name) as scope:
        # Create tf variables for the weights and biases of the conv layer
        weights = tf.get_variable('weights', shape=[filter_height,
                                                    filter_width,
                                                    input_channels/groups,
                                                    num_filters])
        biases = tf.get_variable('biases', shape=[num_filters])

    if groups == 1:
        conv = convolve(x, weights)

    # In the cases of multiple groups, split inputs & weights and
    else:
        # Split input and weights and convolve them separately
        input_groups = tf.split(axis=3, num_or_size_splits=groups, value=x)
        weight_groups = tf.split(axis=3, num_or_size_splits=groups,
                                 value=weights)
        output_groups = [convolve(i, k) for i, k in zip(input_groups, weight_groups)]

        # Concat the convolved output together again
        conv = tf.concat(axis=3, values=output_groups)

    # Add biases
    bias = tf.reshape(tf.nn.bias_add(conv, biases), tf.shape(conv))

    # Apply relu function
    relu = tf.nn.relu(bias, name=scope.name)

    return relu


def fc(x, num_in, num_out, name, relu=True):
    """"""Create a fully connected layer.""""""
    with tf.variable_scope(name) as scope:

        # Create tf variables for the weights and biases
        weights = tf.get_variable('weights', shape=[num_in, num_out],
                                  trainable=True)
        biases = tf.get_variable('biases', [num_out], trainable=True)

        # Matrix multiply weights and inputs and add bias
        act = tf.nn.xw_plus_b(x, weights, biases, name=scope.name)

    if relu:
        # Apply ReLu non linearity
        relu = tf.nn.relu(act)
        return relu
    else:
        return act


def max_pool(x, filter_height, filter_width, stride_y, stride_x, name,
             padding='SAME'):
    """"""Create a max pooling layer.""""""
    return tf.nn.max_pool(x, ksize=[1, filter_height, filter_width, 1],
                          strides=[1, stride_y, stride_x, 1],
                          padding=padding, name=name)


def lrn(x, radius, alpha, beta, name, bias=1.0):
    """"""Create a local response normalization layer.""""""
    return tf.nn.local_response_normalization(x, depth_radius=radius,
                                              alpha=alpha, beta=beta,
                                              bias=bias, name=name)


def dropout(x, keep_prob):
    """"""Create a dropout layer.""""""
    return tf.nn.dropout(x, keep_prob)
```
I loaded the weights from a .npy file and saved the resulting pb file. I saved the resulting pb file with this code:
from tensorflow.python.tools import freeze_graph

```
SAVED_MODEL_PATH = ""save_path/""
MODEL_NAME = ""cut_net_pool5""

input_graph = SAVED_MODEL_PATH + MODEL_NAME + '.pb'
# any other saver to use other than default
input_saver = """"
# earlier definition file format text or binary
input_binary = True
# checkpoint file to merge with graph definition
input_checkpoint = SAVED_MODEL_PATH + MODEL_NAME + '.ckpt'
# output nodes inn our model
output_node_names = 'output'
restore_op_name = 'save/restore_all'
filename_tensor_name = 'save/Const:0'
# output path
output_graph = SAVED_MODEL_PATH + '2frozen_' + MODEL_NAME + '.pb'
# default True
clear_devices = True
initializer_nodes = """"
variable_names_blacklist = """"

freeze_graph.freeze_graph(
    input_graph,
    input_saver,
    input_binary,
    input_checkpoint,
    output_node_names,
    restore_op_name,
    filename_tensor_name,
    output_graph,
    clear_devices,
    initializer_nodes,
    variable_names_blacklist
)
```


**Describe the expected behavior**
I tried to export the resulting .pb file to tflite via this command in the terminal:

`tflite_convert --output_file=test.tflite --graph_def_file=frozen.pb --input_arrays=Placeholder --output_arrays=output `

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
The command fails with the following message:

`tensorflow/contrib/lite/toco/tooling_util.cc:981] Check failed: name.substr(colon_pos +1).find_first_not_of(""0123456789"") == string::npos (1 vs. 18446744073709551615)Array name must only have digits after colon\nAborted (core dumped)\n`
"
23521,How to save tensorflow model and the model size is small?,"I met a weird thing. I try to reproduce a project, the author provides network architecture without training codes. So I write the training code to train using Tensorflow. The author also provides the model he trained.
![image](https://user-images.githubusercontent.com/30546375/47996051-35154180-e132-11e8-9129-153c32bed604.png)
Here is my model I saved: 
![image](https://user-images.githubusercontent.com/30546375/47996065-3fcfd680-e132-11e8-8fc9-8c6ae6439315.png)
I don't understand, if we use the same architecture, don't we get nearly the same model size? Why I save the model is so different with author provided..... Maybe I also save other parameters except weights.... Can anyone help me?"
23520,"Install from pip, run test and results in ""illegal instruction""","
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9 AMD 64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Intel Laptop
- TensorFlow installed from (source or binary): pip per [https://www.tensorflow.org/install/pip](https://www.tensorflow.org/install/pip)
- TensorFlow version: 1.11.0
- Python version: 3.5.3
- Installed using virtualenv? pip? conda?: venv per [https://www.tensorflow.org/install/pip](https://www.tensorflow.org/install/pip) 
- Bazel version (if compiling from source):  N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

- set up venv and activate
- pip install tensorflow per [https://www.tensorflow.org/install/pip](https://www.tensorflow.org/install/pip)
- verify install per [https://www.tensorflow.org/install/pip](https://www.tensorflow.org/install/pip)
- import of tensorflow causes ""illegal instruction"" raised."
23519,[SOLVED]Kernel restarting when running hello world example within docker image,"**System information**
- OS Platform and Distribution is debian 9 amd64:
- Intel based laptop:
- TensorFlow installed from (source or binary): docker image per [https://www.tensorflow.org/install/](https://www.tensorflow.org/install/ )
- TensorFlow version: latest per docker based instructions
- Python version: whatever docker image pulls down
- Installed using virtualenv? pip? conda?: docker image
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**
First computation on 1_hello_tensorflow.ipynb causes kernel restart when it is run.  In fact, it appears any code cell that includes imports tensorflow causes kernel restart.  Straight python code cells, including those using other libraries such as numpy (but have not called tensorflow) run.
"
23516,tensorflow conv2d NCHW with mkl slower than NHWC without mkl  on cpu platform,"I tested tensorflow conv2d with NCHW with mkl slower than NHWC wihout mkl on cpu platform.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N
- TensorFlow installed from (source or binary):binary
tf-mkl:
pip install https://storage.googleapis.com/intel-optimized-tensorflow/tensorflow-1.11.0-cp27-cp27mu-linux_x86_64.whl 
tf-no-mkl:
wget https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.11.0-cp27-none-linux_x86_64.whl
pip install tensorflow-1.11.0-cp27-none-linux_x86_64.whl

- TensorFlow version (use command below):1.111.0
- Python version:2.7.5
- Bazel version (if compiling from source):N
- GCC/Compiler version (if compiling from source):N
- CUDA/cuDNN version:N
- GPU model and memory:N
- CPU: Intel(R) Xeon(R) CPU E5-2682 v4 @ 2.50GHz
Thread(s) per core:    2
Core(s) per socket:    16
Socket(s):             2

**Describe the current behavior**
tensorflow conv2d with NCHW with mkl slower than NHWC wihout mkl on cpu platform.

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
import os
import sys
import tensorflow as tf
import numpy as np
import time

""""""
default NWC
input=[batch, in_width, in_channels]  filter=[ filter_width, in_channels, out_channels]
=>tf.nn.conv2d  [batch,1,in_width, in_channels]  [1, filter_width, in_channels, out_channels]

mkl NCW
input=[batch, in_channels, in_width] filter=[filter_width, in_channels, out_channels]
=>tf.nn.conv2d []
""""""

batch = 32
in_width = 102400
in_height = 1
in_channels = 128
filter_width = 3
filter_height = 1
out_channels = 128
data_format_2d = sys.argv[2]
n = 100
start = 99
input_2d = {}

if len(sys.argv) > 1 and sys.argv[1] == ""ow"":
    print(""ow=1"")
    in_width,in_height = in_height, in_width
    filter_width,filter_height = filter_height, filter_width
else:
    print(""oh=1"")
if data_format_2d == ""NCHW"":
    input_ = np.array(np.arange(1, 1 + batch*in_width*in_height*in_channels).reshape([batch, in_channels, in_height,in_width]), dtype=np.float32)
else:
    input_ = np.array(np.arange(1, 1 + batch*in_width*in_height*in_channels).reshape([batch, in_height,in_width,in_channels]), dtype=np.float32)
for i in range(start,n):
    input_2d[i] = input_ + i*n*i*1.0
kernel_2d = np.array(np.arange(1, 1 + filter_width*filter_height*in_channels*out_channels), dtype=np.float32).reshape([filter_height, filter_width, in_channels, out_channels])
a = tf.placeholder(dtype=tf.float32)
conv2d = tf.nn.conv2d(a, kernel_2d, [1,1,1,1], 'VALID', data_format=data_format_2d)

config = tf.ConfigProto()
config.intra_op_parallelism_threads = 32
config.inter_op_parallelism_threads = 2
sess = tf.Session(config=config)

print(""start"")
sess.run(tf.global_variables_initializer())
start_ts = time.time()
for i in range(start,n):
    sess.run(conv2d, feed_dict={a:input_2d[i]})
print(""data_format_2d=%s %d epoch cost %f"" % (data_format_2d,n-start, (time.time()) - start_ts))
print(""shape:"")
print(np.shape(input_2d[start]))
```
cmd to exec
```
MKLDNN_VERBOSE=0 KMP_BLOCKTIME=0 CUDA_VISIBLE_DEVICES=-1  OMP_NUM_THREADS=32 python conv2d.py  oh NHWC
```
or
```
MKLDNN_VERBOSE=0 KMP_BLOCKTIME=0 CUDA_VISIBLE_DEVICES=-1  OMP_NUM_THREADS=32 python conv2d.py  oh NCHW
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
tf-1.11.0 with mkl
```
MKLDNN_VERBOSE=1 KMP_BLOCKTIME=0 CUDA_VISIBLE_DEVICES=-1  OMP_NUM_THREADS=32 python conv2d.py  ow NCHW
```
output:
```
ow=1
2018-11-05 17:17:06.151312: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
start
mkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_nchw out:f32_nChw8c,num:1,32x128x102400x1,83.739
mkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_hwio out:f32_OIhw8i8o,num:1,128x128x3x1,0.157959
mkldnn_verbose,exec,convolution,jit:avx2,forward_training,fsrc:nChw8c fwei:OIhw8i8o fbia:undef fdst:nChw8c,alg:convolution_direct,mb32_g1ic128oc128_ih102400oh102398kh3sh1dh0ph0_iw1ow1kw1sw1dw0pw0,1010.23
mkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_nChw8c out:f32_nchw,num:1,32x128x102398x1,181.471
data_format_2d=NCHW 1 epoch cost 2.135184
shape:
(32, 128, 102400, 1)
```
tf-1.11.0 without mkl
```
MKLDNN_VERBOSE=0 KMP_BLOCKTIME=0 CUDA_VISIBLE_DEVICES=-1  OMP_NUM_THREADS=32 python conv2d.py  oh NHWC
```
output
```
oh=1
2018-11-05 17:16:30.370120: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
start
data_format_2d=NHWC 1 epoch cost 1.402521
shape:
(32, 1, 102400, 128)
```
"
23515,[Eager] Error when training LinearRegressor estimator (Tensor.op),"**System information**
- Have I written custom code: **Yes**.
- OS Platform and Distribution: **Ubuntu 18.04.1 LTS (Bionic Beaver)**.
- TensorFlow installed from: **binary**.
- TensorFlow version: **1.11.0**.
- Python version: **3.6.6**.
- CUDA/cuDNN version: **N/A. CPU only**.
- GPU model and memory: **N/A. CPU only**.
- Other: **Jupyter notebook, 4.4.0**

**Describe the current behavior**

In Eager mode, training a LinearRegressor Estimator calling `model.train(..)` fails with the following error:

```text
Tensor.op is meaningless when eager execution is enabled.
``` 

**Describe the expected behavior**

`model.train(..)` returns without exceptions.

**Code to reproduce the issue**

```python
import tensorflow as tf
# Uncomment the following line after running the script in Graph mode
# tf.enable_eager_execution()

feature_names = ['a', 'b', 'c']
feature_columns = [tf.feature_column.numeric_column(key=name) for name in feature_names]

true_w = [[-2.0], [4.0], [1.0]]
true_b = [0.5]
noise_level = 0.1

batch_size = 32
num_batches = 1000
steps = 100

def synthetic_dataset(w, b, noise_level, batch_size, num_batches):
    return synthetic_dataset_helper(w, b,
                                    tf.shape(w)[0], noise_level, batch_size,
                                    num_batches)


def synthetic_dataset_helper(w, b, num_features, noise_level, batch_size,
                             num_batches):
    def batch(_):
        x = tf.random_normal([batch_size, num_features])
        y = tf.matmul(x, w) + b + noise_level * tf.random_normal([])
        x_dict = {}
        for idx, name in enumerate(feature_names):
            x_dict[name] = x[:, idx]
        return x_dict, y

    return tf.data.Dataset.range(num_batches).map(batch)

dataset = synthetic_dataset(true_w, true_b, noise_level, batch_size, num_batches)

def input_train():
    return (dataset.make_one_shot_iterator().get_next())

model = tf.estimator.LinearRegressor(feature_columns=feature_columns)
model.train(input_fn=input_train, steps=steps)
```

The code is similar to the following official examples: [linear_regression.py](https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/examples/get_started/regression/linear_regression.py) and [linear_regression.py (eager)](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/linear_regression/linear_regression.py).
"
23514,poor performance on conv2d with ow=1,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:no
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):'v1.11.0-0-gc19e293', '1.11.0'
- Python version:2.7.5
- Bazel version (if compiling from source):no
- GCC/Compiler version (if compiling from source):no
- CUDA/cuDNN version:no
- GPU model and memory:no


I try to optimize the voice synthesize model, with a lot of conv1d invoke fllowing the steps of https://www.tensorflow.org/guide/performance/overview#optimizing_for_cpu, I found the results on tensorflow v1.11.0 with mkl version is slower 50% than the same tensorflow without mkl.

First of all, based on https://github.com/intel/mkl-dnn/issues/285,  I test conv2d NCHW with respectively oh=1 or ow=1. I found ow=1 slower slower than oh=1.

So I doubt how to optimize cpu performance on conv1d ???


-----------------------------------------------------------------------------

### Environment
Intel MKL-DNN includes hardware-specific optimizations and may behave
differently on depending on the compiler and build environment. Include
the following information to help reproduce the issue:
* Intel(R) Xeon(R) CPU E5-2682 v4 @ 2.50GHz
* company internal version based on centos
* 4.9.2
* empty value
* no cmake
* tensorflow 1.11

### Steps to reproduce

```
isntall fllowing https://software.intel.com/en-us/articles/intel-optimization-for-tensorflow-installation-guide
# Python 2.7
pip install https://storage.googleapis.com/intel-optimized-tensorflow/tensorflow-1.11.0-cp27-cp27mu-linux_x86_64.whl 

```
ow =1  cmd

```
MKLDNN_VERBOSE=1 KMP_BLOCKTIME=0 CUDA_VISIBLE_DEVICES=-1  OMP_NUM_THREADS=32 python conv2d.py  ow

ow=1
2018-11-05 15:42:35.614687: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
start
mkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_nchw out:f32_nChw8c,num:1,32x128x102400x1,85.6531
mkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_hwio out:f32_OIhw8i8o,num:1,128x128x3x1,0.134766
mkldnn_verbose,exec,convolution,jit:avx2,forward_training,fsrc:nChw8c fwei:OIhw8i8o fbia:undef fdst:nChw8c,alg:convolution_direct,mb32_g1ic128oc128_ih102400oh102398kh3sh1dh0ph0_iw1ow1kw1sw1dw0pw0,991.858
mkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_nChw8c out:f32_nchw,num:1,32x128x102398x1,174.348
data_format_2d=NCHW 100 epoch cost 2.131751
```
oh =1  cmd

```
MKLDNN_VERBOSE=1 KMP_BLOCKTIME=0 CUDA_VISIBLE_DEVICES=-1  OMP_NUM_THREADS=32 python conv2d.py  oh
oh=1
2018-11-05 15:42:05.224622: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
start
mkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_nchw out:f32_nChw8c,num:1,32x128x1x102400,91.1848
mkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_hwio out:f32_OIhw8i8o,num:1,128x128x1x3,0.152832
mkldnn_verbose,exec,convolution,jit:avx2,forward_training,fsrc:nChw8c fwei:OIhw8i8o fbia:undef fdst:nChw8c,alg:convolution_direct,mb32_g1ic128oc128_ih1oh1kh1sh1dh0ph0_iw102400ow102398kw3sw1dw0pw0,835.947
mkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_nChw8c out:f32_nchw,num:1,32x128x1x102398,199.113
data_format_2d=NCHW 100 epoch cost 1.990710

```

### the code shows

```
import os
import sys
import tensorflow as tf
import numpy as np
import time

""""""
default NWC
input=[batch, in_width, in_channels]  filter=[ filter_width, in_channels, out_channels]
=>tf.nn.conv2d  [batch,1,in_width, in_channels]  [1, filter_width, in_channels, out_channels]

mkl NCW
input=[batch, in_channels, in_width] filter=[filter_width, in_channels, out_channels]
=>tf.nn.conv2d []
""""""

batch = 32
in_width = 102400
in_height = 1
in_channels = 128
filter_width = 3
filter_height = 1
out_channels = 128
data_format_2d = 'NCHW'
n = 100
start = 99
input_2d = {}

if len(sys.argv) == 2 and sys.argv[1] == ""ow"":
    print(""ow=1"")
    in_width,in_height = in_height, in_width
    filter_width,filter_height = filter_height, filter_width
else:
    print(""oh=1"")
if data_format_2d == ""NCHW"":
    input_ = np.array(np.arange(1, 1 + batch*in_width*in_height*in_channels).reshape([batch, in_channels, in_height,in_width]), dtype=np.float32)
else:
    input_ = np.array(np.arange(1, 1 + batch*in_width*in_height*in_channels).reshape([batch, in_height,in_width,in_channels]), dtype=np.float32)
for i in range(start,n):
    input_2d[i] = input_ + i*n*i*1.0
kernel_2d = np.array(np.arange(1, 1 + filter_width*filter_height*in_channels*out_channels), dtype=np.float32).reshape([filter_height, filter_width, in_channels, out_channels])
a = tf.placeholder(dtype=tf.float32)
conv2d = tf.nn.conv2d(a, kernel_2d, [1,1,1,1], 'VALID', data_format=data_format_2d)

config = tf.ConfigProto()
config.intra_op_parallelism_threads = 32
config.inter_op_parallelism_threads = 2
sess = tf.Session(config=config)

print(""start"")
sess.run(tf.global_variables_initializer())
start_ts = time.time()
for i in range(start,n):
    sess.run(conv2d, feed_dict={a:input_2d[i]})
print(""data_format_2d=%s %d epoch cost %f"" % (data_format_2d,n, (time.time()) - start_ts))
```

"
23513,"[TF1.7][CUDA9.0] TF compilation from source with GPU failed, something goes wrongly with protobuf_archive","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO
- TensorFlow installed from (source or binary): source 
- TensorFlow version: r1.7
- Python version: Python 2.7
- Installed using virtualenv? pip? conda?: Source 
- Bazel version (if compiling from source): 0.10.1
- GCC/Compiler version (if compiling from source): gcc-5.3
- CUDA/cuDNN version: 9.0 / 7.0.5
- GPU model and memory: TitanXP 12GB



**Describe the problem**
The CPU back-end TF compilation is successfully done, then I tried to compile the Tensorflow from source with GPU back-end support.
After following the tutorial [](https://www.tensorflow.org/install/source), the compilation is proceeded to about: [23xx/6xxx], then an error popped up which is as follow:
```
.
.
.
tensorflow/contrib/lite/arena_planner.cc: In member function 'TfLiteStatus tflite::ArenaPlanner::CalculateAllocationOfInternalTensors(int)':
tensorflow/contrib/lite/arena_planner.cc:232:18: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   if (node_index < graph_info_->num_nodes()) {
                  ^
tensorflow/contrib/lite/arena_planner.cc: In member function 'TfLiteStatus tflite::ArenaPlanner::CalculateDeallocationOfInternalTensors(int)':
tensorflow/contrib/lite/arena_planner.cc:245:18: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   if (node_index < graph_info_->num_nodes()) {
                  ^
INFO: From Compiling external/snappy/snappy-stubs-internal.cc:
cc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++
cc1plus: warning: unrecognized command line option '-Wno-shift-negative-value'
INFO: From Compiling external/snappy/snappy-stubs-internal.cc [for host]:
cc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++
cc1plus: warning: unrecognized command line option '-Wno-shift-negative-value'
INFO: From Compiling external/snappy/snappy.cc:
cc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++
cc1plus: warning: unrecognized command line option '-Wno-shift-negative-value'
INFO: From Compiling external/snappy/snappy.cc [for host]:
cc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++
cc1plus: warning: unrecognized command line option '-Wno-shift-negative-value'
ERROR: /home/web_server/.cache/bazel/_bazel_web_server/9c777c8c1d70f40d5fbbacb4888a5481/external/protobuf_archive/BUILD:259:1: Linking of rule '@protobuf_archive//:js_embed' failed (Exit 1)
/usr/bin/ld: bazel-out/host/bin/external/protobuf_archive/_objs/js_embed/external/protobuf_archive/src/google/protobuf/compiler/js/embed.o: unrecognized relocation (0x2a) in section `.text._ZL7AddFilePKcPSo.constprop.25'
/usr/bin/ld: final link failed: Bad value
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 72.355s, Critical Path: 26.30s
FAILED: Build did NOT complete successfully
```
I guess this shall be an known issue and something relevant to a configuration problem, however, I didn't find anything helpful.

-------
```
Below is my ./configuration:
$ ./configure 
WARNING: Running Bazel server needs to be killed, because the startup options are different.
You have bazel 0.10.1 installed.
Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: n
No jemalloc as malloc support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n
No Google Cloud Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: n
No Hadoop File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n
No Amazon S3 File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Apache Kafka Platform support? [y/N]: n
No Apache Kafka Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [y/N]: n
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with GDR support? [y/N]: n
No GDR support will be enabled for TensorFlow.

Do you wish to build TensorFlow with VERBS support? [y/N]: n
No VERBS support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 9.0]: 


Please specify the location where CUDA 9.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /home/web_server/xiaolun/cuda-9.0


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 


Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /home/web_server/xiaolun/cuda-9.0]:


Do you wish to build TensorFlow with TensorRT support? [y/N]: n
No TensorRT support will be enabled for TensorFlow.

Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 6.1]


Do you want to use clang as CUDA compiler? [y/N]: n
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /bin/gcc]: /home/web_server/gcc-5.3


Do you wish to build TensorFlow with MPI support? [y/N]: n
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See tools/bazel.rc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
Configuration finished

```

------------

I reset the environment variables and recompile it. This time the error is as below:
```
INFO: From Compiling tensorflow/contrib/lite/arena_planner.cc:
tensorflow/contrib/lite/arena_planner.cc: In member function 'virtual TfLiteStatus tflite::ArenaPlanner::PlanAllocations()':
tensorflow/contrib/lite/arena_planner.cc:82:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < graph_info_->num_nodes(); ++i) {
                     ^
tensorflow/contrib/lite/arena_planner.cc:101:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < graph_info_->num_nodes(); ++i) {
                     ^
tensorflow/contrib/lite/arena_planner.cc: In member function 'virtual TfLiteStatus tflite::ArenaPlanner::ExecuteAllocations(int, int)':
tensorflow/contrib/lite/arena_planner.cc:139:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < graph_info_->num_tensors(); ++i) {
                     ^
tensorflow/contrib/lite/arena_planner.cc: In member function 'TfLiteStatus tflite::ArenaPlanner::CalculateAllocationOfInternalTensors(int)':
tensorflow/contrib/lite/arena_planner.cc:232:18: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   if (node_index < graph_info_->num_nodes()) {
                  ^
tensorflow/contrib/lite/arena_planner.cc: In member function 'TfLiteStatus tflite::ArenaPlanner::CalculateDeallocationOfInternalTensors(int)':
tensorflow/contrib/lite/arena_planner.cc:245:18: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   if (node_index < graph_info_->num_nodes()) {
                  ^
ERROR: /home/web_server/.cache/bazel/_bazel_web_server/924cb0e3b475d8bc0bc364784129b908/external/protobuf_archive/BUILD:259:1: Linking of rule '@protobuf_archive//:js_embed' failed (Exit 1)
/usr/bin/ld: bazel-out/host/bin/external/protobuf_archive/_objs/js_embed/external/protobuf_archive/src/google/protobuf/compiler/js/embed.o: unrecognized relocation (0x2a) in section `.text._ZL7AddFilePKcPSo.constprop.25'
/usr/bin/ld: final link failed: Bad value
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 90.569s, Critical Path: 47.78s
FAILED: Build did NOT complete successfully

```

Any idea will be welcome.
"
23512,there is no gen_image_ops.py file,"from tensorflow.python.ops import gen_image_ops
I want to see the code about non_max_suppression,but it in gen_image_ops,but I don't find it ,so where is the file?"
23511,tf.GradientTape() not support to MaxPool3D,"**System information**
- Have I written custom code: **yes**
- OS Platform and Distribution: **Linux Ubuntu 16.04**
- TensorFlow installed from (source or binary): **binary, pip3 install**
- TensorFlow version (use command below): **1.10**
- Python version: **3.5**
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: **CUDA Version 9.1.85**
- GPU model and memory: **TITAN V, 12066MB**

**Describe the current behavior**
I'm trying to create a simple cnn3d network, which contains a convolutional layer in 3d, a max pooling in 3d and a fully connected layer, all this using eager mode.
When trying to calculate the gradients with an optimizer I get this error:
`TypeError: 'NoneType' object has no attribute '__getitem__'`
Then, when trying to remove the MaxPool3D layer or replace with AveragePooling3D, the gradient calculation works without problems.

**Describe the expected behavior**
Calculate the gradients using the eager mode with tf.GradientTape() in a network with a Conv3D layer and a MaxPool3D layer.

**Code to reproduce the issue**
```python3
from __future__ import absolute_import, division, print_function

import tensorflow as tf

# enable eager mode
tf.enable_eager_execution()
tf.set_random_seed(0)
np.random.seed(0)


x = tf.random_uniform((10,5,10,10,3))
y = tf.random_uniform((10, 5))


class MyModel(tf.keras.Model):
  def __init__(self):
    super(MyModel, self).__init__()
    self.conv3d_1 = tf.keras.layers.Conv3D(filters=6,
                                           kernel_size=(3,3,3))
    self.max_pool_1 = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2))
    self.flatten = tf.keras.layers.Flatten()
    self.dense_1 = tf.layers.Dense(5)

  def call(self, input, training=False):
    """"""Run the model.""""""
    model = self.conv3d_1(input)
    model = self.max_pool_1(model)
    model = self.flatten(model)
    model = self.dense_1(model)
    
    return model

model = MyModel()

def loss(model, x, y):
  logits = model(x)
  return tf.losses.softmax_cross_entropy(onehot_labels=y, logits=logits), logits

def grad(model, inputs, targets):
  with tf.GradientTape() as tape:
    loss_value, logits = loss(model, inputs, targets)
  return loss_value, logits, tape.gradient(loss_value, model.trainable_variables)

# Optimize the model
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
global_step = tf.train.get_or_create_global_step()
loss_value, logits, grads = grad(model, x, y)
optimizer.apply_gradients(zip(grads, model.variables), global_step)
```

**Other info / logs**
The traceback error:

```python3
TypeErrorTraceback (most recent call last)
<ipython-input-4-56cdb9bbe4e5> in <module>()
     44 # Optimize the model
     45 optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
---> 46 loss_value, logits, grads = grad(model, x, y)
     47 optimizer.apply_gradients(zip(grads, model.variables), global_step)

<ipython-input-4-56cdb9bbe4e5> in grad(model, inputs, targets)
     40   with tf.GradientTape() as tape:
     41     loss_value, logits = loss(model, inputs, targets)
---> 42   return loss_value, logits, tape.gradient(loss_value, model.trainable_variables)
     43 
     44 # Optimize the model

/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/backprop.pyc in gradient(self, target, sources, output_gradients)
    899         nest.flatten(target),
    900         flat_sources,
--> 901         output_gradients=output_gradients)
    902 
    903     if not self._persistent:

/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/imperative_grad.pyc in imperative_grad(tape, target, sources, output_gradients)
     62       target,
     63       sources,
---> 64       output_gradients)

/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/backprop.pyc in _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)
    115     return [None] * num_inputs
    116 
--> 117   return grad_fn(mock_op, *out_grads)
    118 
    119 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_grad.pyc in _MaxPool3DGrad(op, grad)
    180   return gen_nn_ops.max_pool3d_grad(
    181       op.inputs[0],
--> 182       op.outputs[0],
    183       grad,
    184       ksize=op.get_attr(""ksize""),

TypeError: 'NoneType' object has no attribute '__getitem__'
```

"
23510,'Model' object has no attribute '_is_graph_network',"
I create the model use Model(intput, output) function, but the model cannot use tf.keras.estimator.model_to_estimator(), why? The error is ""'Model' object has no attribute '_is_graph_network'"", I can't understand."
23509,Does keras saved Model can output tensor?,"I found H5 file saved by save API, the output is ONLY layer info supported which is not so convinced when deployed.
Is there exist method that can saved tensor info of Model ??
Thanks in andvance!"
23508,AttributeError: 'Estimator' object has no attribute '_distribution',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.11
- Python version: 3.6
- CUDA/cuDNN version: 9.2/7.2.1
- GPU model and memory: TitanXp 12Gb

When I use an Estimator with a MirroredDistributionStrategy, I encounter the error ""AttributeError: 'Estimator' object has no attribute '_distribution'"". This seems to be tied to this [line](https://github.com/tensorflow/tensorflow/blob/a6d8ffae097d0132989ae4688d224121ec6d8f35/tensorflow/python/estimator/estimator.py#L1330), which has `self._distribution.unwrap(per_device_hook)[0]`. I'm pretty sure this should be `self._train_distribution.unwrap(per_device_hook)[0]`.

The estimator is clearly never assigned a '_distribution' attribute (this is the only line on which its referenced) so I assume this is just a typo. Let me know if I'm missing something."
23506,After Installing Tensorflow when I TRy to verify I getting Follwing error can some body help on this,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
    NO custom code or Module
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
    Windows 8.1 (64 bit)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
   No mobile device
- TensorFlow installed from (source or binary):
   binary 
- TensorFlow version (use command below):
    >pip install tensorflow
    >pip3 install tensorflow
    Currently default comes as tensorflow
C:\Phyton>pip3 show tensorflow
Name: tensorflow
Version: 1.11.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author: Google Inc.
Author-email: opensource@google.com
License: Apache 2.0
Location: c:\users\avinash.t\anaconda3\lib\site-packages
Requires: termcolor, astor, tensorboard, keras-applications, numpy, grpcio, whee
l, keras-preprocessing, setuptools, six, protobuf, gast, absl-py
Required-by:
 
- Python version:
 3.6.7
- Bazel version (if compiling from source):
No compilation used 
- GCC/Compiler version (if compiling from source):
No compilation used 
- CUDA/cuDNN version:
No CUDA
- GPU model and memory:
No GPU

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
Version is :- 1.11
When i execute the above command following error occurs

Failed to load the native TensorFlow runtime.

**Describe the current behavior**
   LOG is Posted
**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
C:\Users\avinash.t\Anaconda3>python
Python 3.6.7 |Anaconda, Inc.| (default, Oct 28 2018, 19:44:12) [MSC v.1915 64 bi
t (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensor
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ModuleNotFoundError: No module named 'tensor'
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\avinash.t\Anaconda3\lib\site-packages\tensorflow\python\pywrap_
tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\avinash.t\Anaconda3\lib\site-packages\tensorflow\python\pywrap_
tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\avinash.t\Anaconda3\lib\site-packages\tensorflow\python\pywrap_
tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript
ion)
  File ""C:\Users\avinash.t\Anaconda3\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\avinash.t\Anaconda3\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routin
e failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\avinash.t\Anaconda3\lib\site-packages\tensorflow\__init__.py"",
line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-im
port
  File ""C:\Users\avinash.t\Anaconda3\lib\site-packages\tensorflow\python\__init_
_.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\avinash.t\Anaconda3\lib\site-packages\tensorflow\python\pywrap_
tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\avinash.t\Anaconda3\lib\site-packages\tensorflow\python\pywrap_
tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\avinash.t\Anaconda3\lib\site-packages\tensorflow\python\pywrap_
tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\avinash.t\Anaconda3\lib\site-packages\tensorflow\python\pywrap_
tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript
ion)
  File ""C:\Users\avinash.t\Anaconda3\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\avinash.t\Anaconda3\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routin
e failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_probl
ems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>>




























"
23505,Facing this error,"2018-11-04 14:31:10.477036: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
GPU mode with 1.0 usage
2018-11-04 14:31:11.087112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: GeForce GTX 745 major: 5 minor: 0 memoryClockRate(GHz): 1.0325
pciBusID: 0000:01:00.0
totalMemory: 4.00GiB freeMemory: 3.36GiB
2018-11-04 14:31:11.087670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-11-04 14:31:12.174849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-04 14:31:12.175138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2018-11-04 14:31:12.175377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2018-11-04 14:31:12.175722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4096 MB memory) -> physical GPU (device: 0, name: GeForce GTX 745, pci bus id: 0000:01:00.0, compute capability: 5.0)
2018-11-04 14:31:12.177136: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2018-11-04 14:31:12.177430: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 3.60G (3865470464 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
Finished in 15.694188356399536s

2018-11-04 14:31:24.869864: E tensorflow/stream_executor/cuda/cuda_dnn.cc:353] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED
[Finished in 30.221s]
<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version: 3.6
- Installed using virtualenv? pip? conda?: conda install -c aaronzs tensorflow-gpu
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda 9.0 and cuDNN7.0.5 
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23483,tensorflow-lite on android: using tflite Interpreter to get an image in the output,"
I am trying to use the workflow of Tensorflow-for-poets-2 TFLite tutorial, https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#6

But, instead of image classification, I am trying to do style transfer. It means that the input and the output of my network are images (compared to the original example, where the input is an image and the output is a list of scores).

One of my many problems is to get the output-processed image from the tflite inference:

After i loaded the tflite model, i have the tflite Interpreter tflite . Using this Interpreter I run the inference:

`tflite.run(imgData, Out_imgData);`
where the

`imgData, Out_imgData`

are ByteBuffers, created in the same way as in Tensorflow-for-poets-2 TFLite tutorial, https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#6.

Now I have my inference output as a ByteBuffer

`Out_imgData`

I want to convert it to a bitmap. How can this be done?
Or do you have any other suggestions on how to apply inference that at the end I i can take the output image (for example I want to save the image and display it on an imageview) "
23482,"No such file or directory  #include ""npy_1_7_deprecated_api.h""","#1243 similar problem re-appears on Centos 7.5 w/ kernel 3.10.0-862.14.4.el7.x86_64 w/ 32 GB of memory after

`git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
git pull
git checkout v1.11.0
`

leads to

`ERROR: /home/user/tensorflow/tensorflow/python/BUILD:5553:1: C++ compilation of rule '//tensorflow/python:framework/fast_tensor_util.so' failed (Exit 1) gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 49 argument(s) skipped)

Use --sandbox_debug to see verbose messages from the sandbox
In file included from bazel-out/host/genfiles/external/local_config_python/numpy_include/numpy/ndarrayobject.h:18:0,
                 from bazel-out/host/genfiles/external/local_config_python/numpy_include/numpy/arrayobject.h:4,
                 from bazel-out/host/genfiles/tensorflow/python/framework/fast_tensor_util.cpp:581:
bazel-out/host/genfiles/external/local_config_python/numpy_include/numpy/ndarraytypes.h:1821:36: fatal error: npy_1_7_deprecated_api.h: No such file or directory
 #include ""npy_1_7_deprecated_api.h""
                                    ^
compilation terminated.
INFO: Elapsed time: 71.255s, Critical Path: 14.15s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]
INFO: 545 processes: 545 processwrapper-sandbox.
FAILED: Build did NOT complete successfully
`

_Originally posted by @ng0177 in https://github.com/tensorflow/tensorflow/issues/1243#issuecomment-435663567_"
23478,Will Python 3.7 support come soon?,"Will Tensorflow support be extended to Python 3.7 anytime in the near future? Given that Miniconda has upgraded their default Python to 3.7,  I imagine there'd be higher demand to install TF on Python 3.7.


**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian Stretch
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: Master branch
- Python version: 3.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source): 8.2.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

"
23477,Tensorflow's Estimator stops training,"I asked this question on Stackoverflow 2 days ago (https://stackoverflow.com/questions/53089945/tensorflows-estimator-stops-training).

I am training a model using Tensorflow's Estimator, but it suddenly stops training after 2600 steps after performing an evaluation. This is the last part of my code:

```
def train():
    train_input_func = lambda: input_fn(mode='train')
    eval_input_func = lambda: input_fn(mode='eval')

    est_conf = tf.estimator.RunConfig(cfg.model_dir, save_checkpoints_secs=120)
    estimator = tf.estimator.Estimator(model_fn, cfg.model_dir, est_conf)


    Path(estimator.eval_dir()).mkdir(parents=True, exist_ok=True)
    train_spec = tf.estimator.TrainSpec(input_fn=train_input_func)
    eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_func, throttle_secs=120)
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)

if __name__ == '__main__':
    train()
```

and this is the `input_fn`:

```
def input_fn(mode=None):
        data_generator = lambda: data_loader.data_generator(mode=mode)

        dataset = tf.data.Dataset.from_generator(data_generator,
                                                 output_types=(tf.int32, tf.int32),
                                                 output_shapes=([None], [None]))

        if mode is 'train':
            dataset.shuffle(cfg.shuffle_buffer).repeat(1000)

        dataset = dataset.padded_batch(cfg.batch_size, padded_shapes=([None],[None])).prefetch(1)

        return dataset
```

The training simply stops like this:

```
loss = 3.530099, step = 2500 (4.443 sec)          
global_step/sec: 25.3421 
loss = 1.3306174, step = 2600 (3.946 sec)         
Saving checkpoints for 2604 into ./model/model.ckpt.                                                
Calling model_fn.        
Done calling model_fn.   
Starting evaluation at 2018-11-03-12:15:23        
Graph was finalized.     
2018-11-03 15:45:23.655651: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-11-03 15:45:23.655681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-03 15:45:23.655685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0          
2018-11-03 15:45:23.655689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N          
2018-11-03 15:45:23.655805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10402 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
Restoring parameters from ./model/model.ckpt-2604 
Running local_init_op.   
Done running local_init_op.                       
Evaluation [10/100]      
Evaluation [20/100]      
Evaluation [30/100]      
Evaluation [40/100]      
Evaluation [50/100]      
Evaluation [60/100]      
Evaluation [70/100]      
Evaluation [80/100]      
Evaluation [90/100]      
Evaluation [100/100]     
Finished evaluation at 2018-11-03-12:15:26        
Saving dict for global step 2604: acc = 0.9593193, global_step = 2604, loss = 2.918349              
Saving 'checkpoint_path' summary for global step 2604: ./model/model.ckpt-2604                      
Loss for final step: 0.83287233.                  

```
Isn't it supposed to continue training until the end of the last epoch?

Also, if I run it on CPU, I recieve this:

```
Caused by op 'cond_1/GatherV2_1', defined at:                                                                                                                                            
  File ""tf_pos_lstm.py"", line 125, in <module>                                                                                                                                           
    train()                                                                                                                                                                              
  File ""tf_pos_lstm.py"", line 122, in train
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
  File ""/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 471, in train_and_evaluate
    return executor.run()
  File ""/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 610, in run
    return self.run_local()
  File ""/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 711, in run_local
    saving_listeners=saving_listeners)
  File ""/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 356, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1181, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1211, in _train_model_default
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1169, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""tf_pos_lstm.py"", line 56, in model_fn
    crf_params)
  File ""/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/contrib/crf/python/ops/crf.py"", line 250, in crf_log_likelihood
    transition_params)
  File ""/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/contrib/crf/python/ops/crf.py"", line 114, in crf_sequence_score
    false_fn=_multi_seq_fn)
  File ""/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/layers/utils.py"", line 202, in smart_cond
    pred, true_fn=true_fn, false_fn=false_fn, name=name)
  File ""/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/framework/smart_cond.py"", line 59, in smart_cond
    name=name)
  File ""/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2087, in cond
    orig_res_f, res_f = context_f.BuildCondBranch(false_fn)
  File ""/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1920, in BuildCondBranch
    original_result = fn()
  File ""/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/contrib/crf/python/ops/crf.py"", line 106, in _multi_seq_fn
    transition_params)
  File ""/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/contrib/crf/python/ops/crf.py"", line 320, in crf_binary_score
    flattened_transition_indices)
  File ""/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 2669, in gather
    return gen_array_ops.gather_v2(params, indices, axis, name=name)
  File ""/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 3232, in gather_v2
    ""GatherV2"", params=params, indices=indices, axis=axis, name=name)
File ""/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op
    op_def=op_def)
  File ""/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): indices[12,23] = 551 is not in [0, 529)
         [[{{node cond_1/GatherV2_1}} = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](cond_1/Reshape_4, cond_1/ad
d_2, gradients/f_count)]]
```


**System information**

```
== cat /etc/issue ===============================================
Linux deep 4.13.0-46-generic #51-Ubuntu SMP Tue Jun 12 12:36:29 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 7.2.0-8ubuntu3.2) 7.2.0
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux deep 4.13.0-46-generic #51-Ubuntu SMP Tue Jun 12 12:36:29 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy               1.14.5
protobuf            3.6.0
tensorflow          1.11.0

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.11.0
tf.GIT_VERSION = b'unknown'
tf.COMPILER_VERSION = b'unknown'
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Sat Nov  3 16:05:47 2018
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 390.67                 Driver Version: 390.67                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 108...  Off  | 00000000:01:00.0 Off |                  N/A |
|  0%   43C    P5    37W / 250W |      0MiB / 11176MiB |      3%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+


```
"
23476,Python Tensorflow crashes on saving the data,"I made a Python script and I almost got it working, I am using this code at the end of my script:

```
filepath = ""RNN_Final-{epoch:02d }-{val_acc:.3f}""

#Saves only the best ones
checkpoint = ModelCheckpoint(""models/{}.model"".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max'))

history = model.fit(
    train_x, train_y,
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    validation_data=(validation_x, validation_y),
    callbacks=[tensorboard, checkpoint])
```
And this is the error I get:

```
File ""/var/www/test.nl/test.py"", line 162, in <module>
    callbacks=[tensorboard, checkpoint])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training.py"", line 1605, in fit
    validation_steps=validation_steps)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_arrays.py"", line 238, in fit_loop
    callbacks.on_epoch_end(epoch, epoch_logs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/callbacks.py"", line 214, in on_epoch_end
    callback.on_epoch_end(epoch, logs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/callbacks.py"", line 568, in on_epoch_end
    filepath = self.filepath.format(epoch=epoch + 1, **logs)
ValueError: Invalid conversion specification
```

The script crashes on this line:
```
callbacks=[tensorboard, checkpoint])
```
What can I do, or what am I doing wrong?"
23475,Using tensor as parameter for tf.range and tf.slice,"**System information**
- TensorFlow version (you are using): 1.11 cpu, Python 3.6.5
- Are you willing to contribute it (Yes/No): Yes? But contribute what?


**Describe the feature and the current behavior/state.**
I am using a model that will generate 2 intermediate values `yp1` and `yp2`( with shape [1, ] ) during training, who are supposed to be the start index and end index of another tensor `context_idxs` who has type [1, *] where the second dimension will change each input.

Now I'd like to extract `context_idxs[yp1 : yp2]` and further feed them into another LSTM cell, but since yp1 and yp2 is tensor, I could not do that directly or with `tf.slice`. I find a function called `tf.gather` that seems to be what I'm looking for, however, I need to generate a range `[yp1, yp1+1, yp1+2, ..., yp2]` to use `tf.gather`.

Then here comes the problem. `tf.range` seems  not to support using tensor as parameter. However, I can't know the value of yp1 and yp2 in the model (I know how to print them in sess.run, but I'm using them inside the model rather than from outside the graph).

I'll use a quick example
```python
self.yp1 = tf.argmax(tf.reduce_max(outer, axis=2), axis=1)
self.yp2 = tf.argmax(tf.reduce_max(outer, axis=1), axis=1)

# ??
# how do I get context_idxs[yp1 : yp2]? If context_idxs has shape [64, *], how do I get context_idxs[ : , yp1 : yp2]?
```

My questions are:
1. How can I get dynamic slice `context_idxs[yp1 : yp2]` of one tensor based on other tensors?
2. If I want to batch some values, for example, the yp1 and yp2 becomes `[64, 1]`, and context_idxs becomes [64, *], is there still any way to do so?
3. Is there any other way to show the value of variable just-in-time without having to `sess.run` each time? Something like normal python will be great.

**Will this change the current api? How?**
Maybe. I know tensorflow is based on graph but I think allowing just-in-time calculation of values will greatly make debugging easier. Normal python will make life much easier...

**Who will benefit with this feature?**
I don't know if anyone else needs it as I'm not an expert.

**Any Other info.**
Ubuntu 18.04.1
Python 3.6.5
"
23474,how can i solve this issue on importing tensorflow in linux?,">

****System information****
- OS Platform and Distribution: (Linux Ubuntu 18.04):
- M device (e.HP-proBook) .
- Tensor-flow installed by running on terminal 
pip3 install --upgrade tensorflow-gpu
- TensorFlow version:
- Python version:3.6.6
- Installed using pip.
 -I dont have a CUDA/cuDNN version but  Intelhaswall.
- GPU model :Intel® Haswell Mobile
-memory:7.7 Gi
**the problem is :** 
I cant install and using tensor flow or numpy for object detection algorithms.
I don't know how can i install virtualenv.
I run on terminal
$sudo apt install python-pip python3-pip      
$pip3 install --upgrade tensorflow 
the tensor flow already installed 
but ,,when i run 
$python3
and import tensorflow the error was 
![screenshot from 2018-11-03 09-05-52](https://user-images.githubusercontent.com/44716923/47949282-f708f980-df48-11e8-9032-98813b6a8767.png)



for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

______________________________________________________________________

**Any other info / logs**
i dont want to use a black-screen !!! to type a command i want to clone a codes and running it from a GitHub,starting coding .. somone advice me to setup 
"
23473,tf.matmul fails with CUBLAS_STATUS_NOT_SUPPORTED for large matrices when using CUDA 9.1 ,"I am hitting this issue on multiple development machines with different GPUs, and all versions of Tensorflow >= 1.9.0. I have a CUDA 9.1 requirement on the host machine, so downgrading to CUDA 9.0 is not an option for me. 

See https://stackoverflow.com/questions/50911052/tensorflow-matmul-blas-xgemmbatched-launch-failed/50918250 for a possibly related issue, though that involved Tensorflow 1.8.0.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): _Yes, a minimum repro is below_
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): _Linux Ubuntu 16.04_
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: _n/a_
- TensorFlow installed from (source or binary): _source_
- TensorFlow version (use command below): _1.9.0 to 1.11.0 (does not reproduce in 1.8.0)_
- Python version: _2.7_
- Bazel version (if compiling from source): _Bazel 0.10.0 for  Tensorflow 1.9.0, Bazel 0.18.0 for Tensorflow 1.10.0 and newer_
- GCC/Compiler version (if compiling from source): _GCC 5.4.0_
- CUDA/cuDNN version: _CUDA 9.1.85 / cuDNN 7.0.5_
- GPU model and memory: _GTX 1080 w/ 7400 MB, GTX 1060 w/ 5600 MB_

**Describe the current behavior**

tf.matmul is failing to multiply matrices above a certain size, with error:
`failed to run cuBLAS routine cublasGemmBatchedEx: CUBLAS_STATUS_NOT_SUPPORTED`

I have confirmed using `nvidia-smi` that the GPU is nowhere close to running out of memory.

**Describe the expected behavior**

The matrix multiplication should complete successfully.

**Code to reproduce the issue**

This is borrowed from the stackoverflow link above:

```python
import tensorflow as tf
import numpy as np

config = tf.ConfigProto()
config.gpu_options.allow_growth=True
tf.Session(config=config).close()

def calc():
    N = 15 # works for N <= 14
    a = 16
    b = 8
    X = np.random.rand(N, 11520, b, 1).astype(np.float32)
    print(X.nbytes*1e-6, ""MB"")
    W = np.random.rand(N, 11520, a, b).astype(np.float32)
    print(W.nbytes*1e-6, ""MB"")
    X_ = tf.constant(X, name=""X-constant"", dtype=tf.float32)
    W_ = tf.constant(W, name=""W-constant"", dtype=tf.float32)

    return tf.matmul(W_, X_, name=""mymatmul"")

tf.reset_default_graph()
a = calc()
sess = tf.Session()
sess.run(tf.global_variables_initializer())
b = sess.run(a)
sess.close()
print(b.shape)
```


**Other info / logs**

I found a workaround for this issue is to patch `CUDABlas::DoBlasGemmBatchedInternal` in `tensorflow/stream_extractor/cuda/cuda_blas.cc` to disable the `#if CUDA_VERSION >= 9010` block which calls `wrap::cublasGemmBatchedEx`. Downgrading to Tensorflow <= 1.8.0 also resolves the issue for me (that codeblock was added in Tensorflow 1.9.0).

Running the above code (N=15) with Tensorflow 1.11.0:
```
2018-11-02 23:13:48.059128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-11-02 23:13:48.059860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:01:00.0
totalMemory: 7.92GiB freeMemory: 7.40GiB
2018-11-02 23:13:48.059884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-11-02 23:13:48.282603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-02 23:13:48.282631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2018-11-02 23:13:48.282636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2018-11-02 23:13:48.282790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7137 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
(5.529599999999999, 'MB')
(88.47359999999999, 'MB')
2018-11-02 23:13:48.963544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-11-02 23:13:48.963579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-02 23:13:48.963585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2018-11-02 23:13:48.963588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2018-11-02 23:13:48.963722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7137 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-11-02 23:13:51.146858: E tensorflow/stream_executor/cuda/cuda_blas.cc:652] failed to run cuBLAS routine cublasGemmBatchedEx: CUBLAS_STATUS_NOT_SUPPORTED
2018-11-02 23:13:51.146900: E tensorflow/stream_executor/cuda/cuda_blas.cc:2574] Internal: failed BLAS call, see log for details
Traceback (most recent call last):
  File ""./tf_matmul.py"", line 25, in <module>
    b = sess.run(a)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 887, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1110, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1286, in _do_run
    run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1308, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Blas xGEMMBatched launch failed : a.shape=[172800,16,8], b.shape=[172800,8,1], m=16, n=1, k=8, batch_size=172800
	 [[{{node mymatmul}} = BatchMatMul[T=DT_FLOAT, adj_x=false, adj_y=false, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](W-constant, X-constant)]]

Caused by op u'mymatmul', defined at:
  File ""./tf_matmul.py"", line 22, in <module>
    a = calc()
  File ""./tf_matmul.py"", line 19, in calc
    return tf.matmul(W_, X_, name=""mymatmul"")
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py"", line 2015, in matmul
    a, b, adj_x=adjoint_a, adj_y=adjoint_b, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 1245, in batch_mat_mul
    ""BatchMatMul"", x=x, y=y, adj_x=adj_x, adj_y=adj_y, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Blas xGEMMBatched launch failed : a.shape=[172800,16,8], b.shape=[172800,8,1], m=16, n=1, k=8, batch_size=172800
	 [[{{node mymatmul}} = BatchMatMul[T=DT_FLOAT, adj_x=false, adj_y=false, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](W-constant, X-constant)]]
```

Reducing the matrix size (N=14):
```
2018-11-02 23:18:29.127555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-11-02 23:18:29.128409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:01:00.0
totalMemory: 7.92GiB freeMemory: 7.39GiB
2018-11-02 23:18:29.128441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-11-02 23:18:29.482292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-02 23:18:29.482346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2018-11-02 23:18:29.482361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2018-11-02 23:18:29.482670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7131 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
(5.160959999999999, 'MB')
(82.57535999999999, 'MB')
2018-11-02 23:18:30.200524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-11-02 23:18:30.200586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-02 23:18:30.200598: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2018-11-02 23:18:30.200605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2018-11-02 23:18:30.200837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7131 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
(14, 11520, 16, 1)

```

After patching `CUDABlas::DoBlasGemmBatchedInternal` (N=15):

```
2018-11-02 23:28:05.667458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-11-02 23:28:05.668033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:01:00.0
totalMemory: 7.92GiB freeMemory: 7.39GiB
2018-11-02 23:28:05.668046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-11-02 23:28:05.866043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-02 23:28:05.866069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2018-11-02 23:28:05.866074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2018-11-02 23:28:05.866218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7129 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
(5.529599999999999, 'MB')
(88.47359999999999, 'MB')
2018-11-02 23:28:06.452688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-11-02 23:28:06.452722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-02 23:28:06.452727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2018-11-02 23:28:06.452730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2018-11-02 23:28:06.452858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7129 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
(15, 11520, 16, 1)

```

Patch for workaround:
```patch
diff --git a/tensorflow/stream_executor/cuda/cuda_blas.cc b/tensorflow/stream_executor/cuda/cuda_blas.cc
index ab7091b..50579b6 100644
--- a/tensorflow/stream_executor/cuda/cuda_blas.cc
+++ b/tensorflow/stream_executor/cuda/cuda_blas.cc
@@ -18,6 +18,8 @@ limitations under the License.
 
 #define SE_CUDA_DATA_HALF CUDA_R_16F
 
+#define USE_CUBLAS_GEMM_BATCHED_EX false
+
 #include ""tensorflow/stream_executor/cuda/cuda_blas.h""
 
 // Both Eigen Half.h and CUDA cuda_fp16.h provide similar typedef for __half. As
@@ -2482,7 +2484,7 @@ port::Status CUDABlas::DoBlasGemmBatchedInternal(
 
   cudaDataType_t data_type = CUDADataType<T>::type;
 
-#if CUDA_VERSION >= 9010
+#if CUDA_VERSION >= 9010 && USE_CUBLAS_GEMM_BATCHED_EX
   int cc_major, cc_minor;
   if (stream->parent()->GetDeviceDescription().cuda_compute_capability(
           &cc_major, &cc_minor) &&
```"
23472,"Fail to configure on Windows(CMake3.11.0-rc2, VS2015, python3.5.3, swigwin-3.0.12)","CMake Warning at CMakeLists.txt:9 (message):
  Your current cmake generator is set to use 32 bit toolset architecture.
  This may cause ""compiler out of heap space"" errors when building.  Consider
  using the flag -Thost=x64 when running cmake.


Selecting Windows SDK version  to target Windows 10.0.17134.
The C compiler identification is MSVC 19.0.23026.0
The CXX compiler identification is MSVC 19.0.23026.0
Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe
Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe -- works
Detecting C compiler ABI info
Detecting C compiler ABI info - done
Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe
Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe -- works
Detecting CXX compiler ABI info
Detecting CXX compiler ABI info - done
Detecting CXX compile features
Detecting CXX compile features - done
Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED
Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED - Failed
Performing Test GCC_OPENMP_SUPPORT
Performing Test GCC_OPENMP_SUPPORT - Failed
Performing Test MSVC_OPENMP_SUPPORT
Performing Test MSVC_OPENMP_SUPPORT - Success
Found PythonInterp: D:/Anaconda3/envs/tensorflow/python.exe (found version ""3.5.4"") 
Found PythonLibs: D:/Anaconda3/envs/tensorflow/libs/python35.lib (found version ""3.5.4"") 
Found SWIG: D:/zbl_back/code/swigwin-3.0.12/swig.exe (found version ""3.0.12"") 
CMake Error at tf_python.cmake:813 (string):
  string sub-command REPLACE requires at least four arguments.
Call Stack (most recent call first):
  CMakeLists.txt:541 (include)


CMake Error at tf_python.cmake:814 (string):
  string sub-command REPLACE requires at least four arguments.
Call Stack (most recent call first):
  CMakeLists.txt:541 (include)


CMake Error at tf_python.cmake:815 (string):
  string sub-command REPLACE requires at least four arguments.
Call Stack (most recent call first):
  CMakeLists.txt:541 (include)

if I use tensorflow_BUILD_PYTHON_BINDINGS=OFF, this error will not happen."
23471,Won't build tf r1.12rc2 because of MPI support,"**System information**
- OS Platform and Distribution: CentOS Linux release 7.5.1804 (Core)
- TensorFlow installed from (source or binary): source, official release r1.12rc2
- TensorFlow version: r1.12rc2
- Python version: 3.6.6 (anaconda)
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.18.1
- GCC/Compiler version (if compiling from source): 8.2.0 with experimental clang support (of configure),
but also not working on gcc4 (centos7-native), gcc6 (rh-scl-devtoolset-6), gcc7 (rh-scl-devtoolset-7)
- CUDA/cuDNN version: None, integrated nGraph
- GPU model and memory: None, CPU: AMD Ryzen 7 1800X

**Describe the problem**
A released source won't build if I enable MPI support flags while configuring.
Tried multiple gcc versions, however there was no luck.
OpenMPI was also installed from official OMPI src release v3.1.2.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1> Usual ./configure after tarball extract
2> config: Apache Ignite (Y) / XLA JIT (Y) / SYCL (N) / ROCm (N) / CUDA (N) / clang (Y) / MPI (Y) / COPT (-march=native) / Android (N)
3>  bazel --server_javabase=$JAVA_HOME build --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --config=opt --config=ngraph //tensorflow/tools/pip_package:build_pip_package
4> (optional) if I disable MPI support while ./configure -ing, then tf r1.12rc2 builds successfully.

**Any other info / logs**
_Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached._

[Manual Patch]
I applied this manual fix: [[Pull req #20147](https://github.com/tensorflow/tensorflow/pull/20147/files)] of tensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc only.

[Logs]
```
INFO: From Compiling tensorflow/stream_executor/device_description.cc [for host]:
tensorflow/stream_executor/device_description.cc:148:18: warning: 'DivideCeil' is deprecated: Use MathUtil::CeilOfRatio directly instead. [-Wdeprecated-declarations]
  *block_count = DivideCeil(element_count, *threads_per_block);
                 ^
./tensorflow/stream_executor/device_description.h:362:1: note: 'DivideCeil' has been explicitly marked deprecated here
ABSL_DEPRECATED(""Use MathUtil::CeilOfRatio directly instead."")
^
external/com_google_absl/absl/base/macros.h:149:49: note: expanded from macro 'ABSL_DEPRECATED'
#define ABSL_DEPRECATED(message) __attribute__((deprecated(message)))
                                                ^
1 warning generated.
ERROR: /home/xo/pyenv-ngraph/mpicol/tensorflow-1.12.0-rc2/tensorflow/contrib/mpi_collectives/BUILD:40:1: C++ compilation of rule '//tensorflow/contrib/mpi_collectives:python/ops/_mpi_ops.so' failed (Exit 1)
**tensorflow/contrib/mpi_collectives/kernels/ring.cc:58:6: error: explicit specialization of 'CopyTensorData<Eigen::ThreadPoolDevice>' after instantiation**
void CopyTensorData<CPUDevice>(void* dst, void* src, size_t size) {
     ^
**./tensorflow/contrib/mpi_collectives/kernels/ring.h:165:3:** note: implicit instantiation first required here
  CopyTensorData<Device>((void*)buffer, (void*)input->tensor_data().data(),
  ^
**tensorflow/contrib/mpi_collectives/kernels/ring.cc:71:1: error: explicit specialization of 'AccumulateTensorData<Eigen::ThreadPoolDevice, int>' after instantiation**
GENERATE_ACCUMULATE(int);
^
**tensorflow/contrib/mpi_collectives/kernels/ring.cc:65:8:** note: expanded from macro 'GENERATE_ACCUMULATE'
  void AccumulateTensorData<CPUDevice, type>(type * dst, type * src, \
       ^
**./tensorflow/contrib/mpi_collectives/kernels/ring.h:221:5:** note: implicit instantiation first required here
    AccumulateTensorData<Device, T>(segment_update, segment_recv,
    ^
**tensorflow/contrib/mpi_collectives/kernels/ring.cc:72:1: error: explicit specialization of 'AccumulateTensorData<Eigen::ThreadPoolDevice, long long>' after instantiation**
GENERATE_ACCUMULATE(long long);
^
**tensorflow/contrib/mpi_collectives/kernels/ring.cc:65:8:** note: expanded from macro 'GENERATE_ACCUMULATE'
  void AccumulateTensorData<CPUDevice, type>(type * dst, type * src, \
       ^
**./tensorflow/contrib/mpi_collectives/kernels/ring.h:221:5:** note: implicit instantiation first required here
    AccumulateTensorData<Device, T>(segment_update, segment_recv,
    ^
**tensorflow/contrib/mpi_collectives/kernels/ring.cc:73:1: error: explicit specialization of 'AccumulateTensorData<Eigen::ThreadPoolDevice, float>' after instantiation**
GENERATE_ACCUMULATE(float);
^
**tensorflow/contrib/mpi_collectives/kernels/ring.cc:65:8:** note: expanded from macro 'GENERATE_ACCUMULATE'
  void AccumulateTensorData<CPUDevice, type>(type * dst, type * src, \
       ^
**./tensorflow/contrib/mpi_collectives/kernels/ring.h:221:5:** note: implicit instantiation first required here
    AccumulateTensorData<Device, T>(segment_update, segment_recv,
    ^
4 errors generated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
**INFO:** Elapsed time: 1532.667s, Critical Path: 295.58s
**INFO:** 6912 processes: 6912 local.
**FAILED:** Build did NOT complete successfully
```"
23470,Error in tensorflow __init__: TypeError: unsupported operand type(s) for +: 'NoneType' and 'str',"- TensorFlow version: v1.11.0-0-gc19e29306c 1.11.0
- Python version: 
    3.6.4, via Anaconda (Anaconda3-5.3.0-Linux-x86_64), and also 3.6.7, and 3.7 via Anaconda
    also 3.6.4 as a direct install
- Centos7, in a Docker container, with all dependencies pip installed

During \_\_init\_\_ on import of tensorflow,  seeing ```TypeError: unsupported operand type(s) for +: 'NoneType' and 'str'`` - exception trace below

```
try:
    print(""Importing tensorflow"", flush=True)
    import tensorflow as tf
    print(""Imported tensorflow"", tf, flush=True)
   
except:
    pass
```
The circumstances are unusual and difficult to reproduce. The code runs fine except in situ installed to support a Streams python map operator. What's needed here is some guidance as to what could cause this specific failure at this specific location in the `__init__` method on import (specifically, at  `import tensorflow.python.ops import array_ops`), and what measures should be taken to address it.

**Other info / logs**
```
Importing tensorflow
02 Nov 2018 22:01:40.634-0400 [62] ERROR #splapplog,J[0],P[0],CandidateEvents.MinimalExtractedObjectsSRLNN,python M[splpy_general.h:importModule:352]  - CDIST0305E: Fatal error: missing module: CandidateEventsExtractor.
Traceback (most recent call last):
  File ""/tmp/StorylineAggregator1.distributed/toolkits/com.ibm.btn.util.candidate_events_extractor/opt/python/streams/CandidateEventsExtractor.py"", line 5, in <module>
    from SRLNN_Pipeline import SRLNN_Pipeline
  File ""/tmp/StorylineAggregator1.distributed/toolkits/com.ibm.btn.util.candidate_events_extractor/opt/python/modules/SRLNN_Pipeline.py"", line 9, in <module>
    from sentence_embeddings import read_corpus, do_faiss
  File ""/tmp/StorylineAggregator1.distributed/toolkits/com.ibm.btn.util.candidate_events_extractor/opt/python/modules/sentence_embeddings.py"", line 19, in <module>
    import tensorflow_hub as hub
  File ""/opt/conda/envs/srl-nn/lib/python3.6/site-packages/tensorflow_hub/__init__.py"", line 21, in <module>
    import tensorflow as tf
  File ""/opt/conda/envs/srl-nn/lib/python3.6/site-packages/tensorflow/__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/opt/conda/envs/srl-nn/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 63, in <module>
    from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin
  File ""/opt/conda/envs/srl-nn/lib/python3.6/site-packages/tensorflow/python/framework/framework_lib.py"", line 52, in <module>
    from tensorflow.python.framework.importer import import_graph_def
  File ""/opt/conda/envs/srl-nn/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 27, in <module>
    from tensorflow.python.framework import function
  File ""/opt/conda/envs/srl-nn/lib/python3.6/site-packages/tensorflow/python/framework/function.py"", line 35, in <module>
    from tensorflow.python.ops import array_ops
  File ""/opt/conda/envs/srl-nn/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 153, in <module>
    listdiff.__doc__ = gen_array_ops.list_diff.__doc__ + ""\n"" + listdiff.__doc__
TypeError: unsupported operand type(s) for +: 'NoneType' and 'str'
02 Nov 2018 22:01:40.639-0400 [62] ERROR #splapplog,J[0],P[0],CandidateEvents.MinimalExtractedObjectsSRLNN,spl_pe M[PEImpl.cpp:logTerminatingException:2183]  - CDISR5033E: An exception occurred during the execution of the CandidateEvents.MinimalExtractedObjectsSRLNN operator. Processing element number 0 is terminating.
02 Nov 2018 22:01:40.639-0400 [62] ERROR #splapptrc,J[0],P[0],CandidateEvents.MinimalExtractedObjectsSRLNN,spl_operator M[PEImpl.cpp:handleOperatorFailure:661]  - CDISR5030E: An exception occurred during the execution of the CandidateEvents.MinimalExtractedObjectsSRLNN operator. The exception is: Unknown Python error"
23469,ModelCheckpoint Callback Doesn't Log Validation Accuracy on ML Engine,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ML Engine Instance (Debian)
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.10.0
- Python version: 2.7.14
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A

**Describe the current behavior**

I've set up a CNN and have created callbacks for model checkpointing, TensorBoard visualization, and early stopping.  I have ~408K samples in my `tf.data.Dataset`, am using a batch size of 256, and train my model with 1,000 steps per epoch with 1000 epochs.  When I do this, everything trains correctly on ML Engine but I don't get logging like I normally do.

**Describe the expected behavior**

I should see logging on ML Engine that indicates validation accuracy for each epoch, like this:

Epoch 2/2000
1/200 [..............................] - ETA: 2:00 - loss: 5.7345 - acc: 0.1055

2/200...

When I set `steps_per_epoch` to be only 200, I *do* see validation accuracy logging as expected.  For what it's worth, the training still works and produces .h5 files.  But it's crucial that I be able to view logging as the model's training.

I spoke with folks on the ML Engine team and it's not an issue on their end.  They suspect it's a bug with the `tf.keras.callbacks.ModelCheckpoint` callback or me misusing `steps_per_epoch` somehow.  Is there a certain value I shouldn't exceed with `steps_per_epoch`?  I wanted to do one full pass over the data, so that should be: # samples / (steps_per_epoch * batch_size), which would be ~1,593 steps.  Even 1,000 steps_per_epoch causes this issue though.

**Code to reproduce the issue**

Unfortunately, I can't share the data I'm using.  I'm using a CNN built with `tf.keras.layers` layers (called `model`) and my compile step, callbacks definitions, and fit step look like this:

        model.compile(
            loss='sparse_categorical_crossentropy',
            optimizer='adam',
            metrics=['accuracy'],
        )
        callbacks = [
            ModelCheckpoint(""model_{epoch:04d}_{val_acc:.4f}.h5"",
                            monitor='val_acc',
                            verbose=1,
                            save_best_only=True,
                            mode='max'),
            TensorBoard(),
            EarlyStopping(monitor='val_acc', patience=5, min_delta=0, mode='max')
        ]

        model.fit(dataset, steps_per_epoch=1000, epochs=1000, validation_data=val_dataset,
                  validation_steps=3, callbacks=callbacks)

**Other info / logs**

Here's what I see in the ML Engine logs:

`14:24 Epoch 1/4`

`15:00 Starting evaluation`

With no logging in between 👎 
"
23467,TensorRT crashes on converting a simple convolutional graph,"TensorRT crashes during conversion of a trivial graph with the following error:

```
python: customWinogradConvActLayer.cpp:48:
nvinfer1::cudnn::WinogradConvActLayer::WinogradConvActLayer(const string&, const EngineTensors&, const EngineTensors&, const nvinfer1::ConvolutionParameters&, bool, const std::vector<float>&): 
Assertion `matchNbDims(inputs[0], outputs[0]) && (inputs.size() == 1 || inputs[1].extent == outputs[0].extent)' failed.
```

The following minimal example reproduces this issue:

```
import tensorflow as tf
from tensorflow.contrib import tensorrt

input = tf.placeholder(tf.float32, shape=(1, 32, 32, 3))
weights = tf.ones([3, 3, 3, 8], tf.float32)
output = tf.nn.conv2d(input, weights, strides=[1, 1, 1, 1], padding='SAME')
output = output + tf.ones((8,), dtype=tf.float32)
output = tf.nn.relu(output, name='output')

tensorrt.create_inference_graph(
    input_graph_def=tf.get_default_graph().as_graph_def(), outputs=[output.op.name])
```

I've been able to reproduce this issue on multiple machines with similar configurations.

A few things to note:

- The addition and the terminal operation are critical for the issue to manifest
- The `relu` can be replaced with most other ops (eg: `tf.identity`) and the issue still manifests

A variant of this bug was previously reported here: https://github.com/tensorflow/tensorflow/issues/22577 (and was closed without resolution).

**System information**
- OS Platform and Distribution: Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.11.0
- Python version: 2.7.12
- CUDA/cuDNN version: 9.0/7
- GPU model and memory: GTX 1080 Ti"
23466,Segmentation Fault When Running Multiple Sessios In a Row,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 1.4.1
- **Bazel version (if compiling from source)**: 0.6.0
- **GCC/Compiler version (if compiling from source)**: 5
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: GTX 1080 TI
- **Exact command to reproduce**: session_one->Run(
          {{INPUT_LAYER, input_tensor}},
          {OUTPUT_LAYER}, {}, &outputs_tensor);
Run the above method twice in a row

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

### Describe the problem
It appears that multiple sessions cannot be ran right after the other, otherwise I get segmentation fault. There is gotta be a delay in between Run() methods (such as extra computations)

### Source code / logs
For instance, the following code makes my program have a segmentation fault:
```
Status run_session_one_status = session_ONE->Run(
          {{THIS_GRAPH_input_layer, input_tensor_one}},
          {THIS_GRAPH_output_layer}, {}, &output_tensor_one);
  if (!run_session_one_status.ok()) {
      LOG(ERROR) <<
                 ""run_session_one_status failed."" << run_session_one_status;
  }
  Status run_session_two_status = session_TWO->Run(
          {{DIFFERENT_GRAPH_input_layer, input_tensor_two}},
          {DIFFERENT_GRAPH_output_layer}, {}, &output_tensor_two);
  if (!run_session_two_status.ok()) {
      LOG(ERROR) <<
                 ""run_session_two_status failed."" << run_session_two_status;
  }
```

Whereas the following code stops the segmentation fault from happening:

```
Status run_session_one_status = session_ONE->Run(
          {{THIS_GRAPH_input_layer, input_tensor_one}},
          {THIS_GRAPH_output_layer}, {}, &output_tensor_one);
  if (!run_session_one_status.ok()) {
      LOG(ERROR) <<
                 ""run_session_one_status failed."" << run_session_one_status;
  }

  /// adding extra operations just to give GPU a break
  feature_vector.clear();
  for (uint32_t i = 0; i < output_tensor_one[0].NumElements(); ++i) {
      feature_vector.push_back(output_tensor_one[0].flat<float>()(i));
  }

  Status run_session_two_status = session_TWO->Run(
          {{DIFFERENT_GRAPH_input_layer, input_tensor_two}},
          {DIFFERENT_GRAPH_output_layer}, {}, &output_tensor_two);
  if (!run_session_two_status.ok()) {
      LOG(ERROR) <<
                 ""run_session_two_status failed."" << run_session_two_status;
  }
```

I have plenty of GPU memory, and each session only needs 0.5% of my GPU's total memory. And, yes, I tried giving each session more memory (e.g., 50% per session, etc.).

Even though I found a work around, I still wanted to throw the bug out there for the TensorFlow team to be aware of it.

"
23465,Bug in image_captioning_with_attention.ipynb ,"**System information:** ***Running the notebook on Colab.***

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Colab
- TensorFlow version (use command below): Colab ('1.12.0-rc2')
- Python version: Colab (3.6)
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: Colab
- GPU model and memory: Colab

**Describe the current behavior**

Error in Cell 27 (Training): 

`InvalidArgumentError: indices[37,0] = 5001 is not in [0, 5001) [Op:ResourceGather] name: rnn__decoder_1/embedding_1/embedding_lookup/`

**Describe the expected behavior**

If `tokenizer.word_index[tokenizer.oov_token] = top_k + 1` in Cell 10 is commented out, the notebook runs end-to-end on Colab. 

**Code to reproduce the issue**

The reason this happens could be because in Cells 9 and 10, Tokenizer is defined with `oov_token=""<unk>""` in Cell 9, which assigns ""\<unk\> : 1"" in the token dictionary, and then `tokenizer.word_index[tokenizer.oov_token] = top_k + 1` in Cell 10 assigns ""\<unk\>: top_k + 1"" which makes the number \<unk\> was previously assigned to non-existent in the dictionary and causes trouble during lookup. 

The following demonstration explains why we can get an error when we refer to the dictionary later on. 

```
import tensorflow as tf
tf.enable_eager_execution()
import re
import numpy as np

sent = ""<start> Alice in wonderland. <end>""
sent2 = ""<start> When suddenly Alice saw a White Rabbit. <end>""
x_train = [sent, sent2]

top_k = 5
tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k, 
                                                  oov_token=""<unk>"", 
                                                  filters='!""#$%&()*+.,-/:;=?@[\]^_`{|}~ ')
tokenizer.fit_on_texts(x_train)
train_seqs = tokenizer.texts_to_sequences(x_train)
tokenizer.word_index = {key:value for key, value in tokenizer.word_index.items() if value <= top_k}
print(tokenizer.word_index)
idx = tokenizer.word_index['<unk>'] #saving the original index of '<unk>'
tokenizer.word_index[tokenizer.oov_token] = top_k + 1 # the problematic line
print(tokenizer.word_index)
index_word = {value:key for key, value in tokenizer.word_index.items()}
index_word[idx]
```

`Output:`
```
{'<unk>': 1, '<start>': 2, 'alice': 3, '<end>': 4, 'in': 5}
{'<unk>': 6, '<start>': 2, 'alice': 3, '<end>': 4, 'in': 5}
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-11-55bbbd305835> in <module>()
     11 print(tokenizer.word_index)
     12 index_word = {value:key for key, value in tokenizer.word_index.items()}
---> 13 index_word[idx]

KeyError: 1
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-58-03bc9960ded7> in <module>()
     19             for i in range(1, target.shape[1]):
     20                 # passing the features through the decoder
---> 21                 predictions, hidden, _ = decoder(dec_input, features, hidden)
     22 
     23                 loss += loss_function(target[:, i], predictions)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    755       if not in_deferred_mode:
    756         self._in_call = True
--> 757         outputs = self.call(inputs, *args, **kwargs)
    758         self._in_call = False
    759         if outputs is None:

<ipython-input-54-b844d20e3fc2> in call(self, x, features, hidden)
     16 
     17     # x shape after passing through embedding == (batch_size, 1, embedding_dim)
---> 18     x = self.embedding(x)
     19 
     20     # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    755       if not in_deferred_mode:
    756         self._in_call = True
--> 757         outputs = self.call(inputs, *args, **kwargs)
    758         self._in_call = False
    759         if outputs is None:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/embeddings.py in call(self, inputs)
    175     if dtype != 'int32' and dtype != 'int64':
    176       inputs = math_ops.cast(inputs, 'int32')
--> 177     out = embedding_ops.embedding_lookup(self.embeddings, inputs)
    178     return out
    179 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py in embedding_lookup(params, ids, partition_strategy, name, validate_indices, max_norm)
    311       name=name,
    312       max_norm=max_norm,
--> 313       transform_fn=None)
    314 
    315 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py in _embedding_lookup_and_transform(params, ids, partition_strategy, name, max_norm, transform_fn)
    131     if np == 1 and (not transform_fn or ids.get_shape().ndims == 1):
    132       with ops.colocate_with(params[0]):
--> 133         result = _clip(array_ops.gather(params[0], ids, name=name),
    134                        ids, max_norm)
    135         if transform_fn:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in gather(***failed resolving arguments***)
   2671     # TODO(apassos) find a less bad way of detecting resource variables without
   2672     # introducing a circular dependency.
-> 2673     return params.sparse_read(indices, name=name)
   2674   except AttributeError:
   2675     return gen_array_ops.gather_v2(params, indices, axis, name=name)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in sparse_read(self, indices, name)
    756         tape.variable_accessed(self)
    757       value = gen_resource_variable_ops.resource_gather(
--> 758           self._handle, indices, dtype=self._dtype, name=name)
    759     return array_ops.identity(value)
    760 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py in resource_gather(resource, indices, dtype, validate_indices, name)
    611       else:
    612         message = e.message
--> 613       _six.raise_from(_core._status_to_exception(e.code, message), None)
    614 
    615 

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: indices[37,0] = 5001 is not in [0, 5001) [Op:ResourceGather] name: rnn__decoder_1/embedding_1/embedding_lookup/
```"
23464,tf.keras with tf.dataset takes longer per epoch eventually and sometimes errors out,"<em>Performance Issue</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA
- TensorFlow installed from (source or binary):No
- TensorFlow version (use command below):1.10.1
- Python version:2.7
- Bazel version (if compiling from source):NA
- GCC/Compiler version (if compiling from source):NA
- CUDA/cuDNN version:9.1.85
- GPU model and memory: Tesla P4, 7GB

**Describe the current behavior**
The time taken per epoch hugely varies from 10 min to 2 hours from 1st epoch to 2nd. Below are the accurate arguments used in `tf.dataset` creation and its consumption by `tf.keras.model`.

**Describe the expected behavior**
Every epoch should roughly take the same time

**Code to reproduce the issue**
Its a little complicated to share ready to run code due to privacy. But here is crux of it,

```python
dataset = tf.data.TFRecordDataset(data_files, num_parallel_reads=79)

if is_training:
    dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(1743, -1, 42))

dataset = dataset.apply(tf.contrib.data.map_and_batch(decode, 128,
                                                        num_parallel_batches=58,
                                                        drop_remainder=is_training)
                        )
dataset = dataset.prefetch(1064)
```
and fit a `tf.keras.model` like so,
```python
model.fit(
    dataset.make_one_shot_iterator(),
    steps_per_epoch=model_meta.train_records // 128,
    epochs=2,
    validation_data=test_dataset.make_one_shot_iterator(),
    validation_steps=test_records // 128
)
```

**Question 2**: It also errors out almost at the end of the 2nd epoch with `tensorflow.python.framework.errors_impl.OutOfRangeError: End of sequence`. Not sure why it did not fail in the first epoch."
23463,[Bug] TensorRT converted graph gives wrong output,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.11.0
- Python version: 2.7.12
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 9.0/7
- GPU model and memory: GTX 1080, 8GB

env.txt:
```

== cat /etc/issue ===============================================
Linux yiwei-desktop 4.15.0-36-generic #39~16.04.1-Ubuntu SMP Tue Sep 25 08:59:23 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.5 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux yiwei-desktop 4.15.0-36-generic #39~16.04.1-Ubuntu SMP Tue Sep 25 08:59:23 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy                              1.15.2                
protobuf                           3.6.0                 
tensorflow-gpu                     1.11.0                
tensorflow-tensorboard             0.1.8                 

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.11.0
tf.GIT_VERSION = v1.11.0-0-gc19e29306c
tf.COMPILER_VERSION = v1.11.0-0-gc19e29306c
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Thu Nov  1 18:42:17 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.130                Driver Version: 384.130                   |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 1080    Off  | 00000000:01:00.0  On |                  N/A |
| 29%   46C    P2    42W / 180W |   1391MiB /  8112MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      2190      G   /usr/lib/xorg/Xorg                           582MiB |
|    0      5166      G   compiz                                       493MiB |
|    0      6354      G   ...-token=6EB0A193120E130143DC89992F94A977   196MiB |
|    0      7163      C   /usr/lib/libreoffice/program/soffice.bin     107MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.252
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.7
```



**Describe the current behavior**
When using TensorRT to convert a graph that contains a specific structure, the output of the graph would be wrong compare to the output of from the original graph with same input. Sometimes the output can be as large as infinity or some very big numbers.

**Describe the expected behavior**
We expect the two graph to output similiarly (Normally, we get absolute error < 1e-5)

**Code to reproduce the issue**
```
import numpy as np
import tensorflow as tf
from tensorflow.contrib import tensorrt as trt


# Set to False would not trigger the bug.
BUGGED_VERSION = True


def build_graph_from_def(graph_def, input_nodes, output_nodes):
    """"""
    build the actual graph from definition
    """"""
    tf.reset_default_graph()
    graph = tf.Graph()
    with graph.as_default():
        return_tensors = [operation_name +
                          "":0"" for operation_name in input_nodes + output_nodes]
        tensors = tf.import_graph_def(graph_def=graph_def, name="""",
                                      return_elements=return_tensors)
        input_tensor_list = tensors[:len(input_nodes)]
        output_tensor_list = tensors[len(input_nodes):]

    return graph, input_tensor_list, output_tensor_list


def conv(inp, name):
    # Seems to be data-format irrelevant. Tested with both NCHW and NHWC.
    # However, when bias is zero (as initialized by tf.layers.conv2d by default)
    # The bug would trigger.
    if BUGGED_VERSION:
        return tf.layers.conv2d(inp, filters=16, kernel_size=(3, 3), name=name)
    else:
        return tf.layers.conv2d(inp, filters=16, kernel_size=(3, 3),
                                bias_initializer=tf.variance_scaling_initializer(), name=name)


def main():
    with tf.variable_scope(""Net""):
        inp = tf.placeholder(tf.float32, shape=(1, 28, 28, 3), name=""input"")
        conv_shared = conv(inp, ""conv_shared"")
        conv1_1 = conv(conv_shared, ""conv1_1"")
        conv1_2 = conv(conv1_1, ""conv1_2"")
        conv2_1 = conv(conv_shared, ""conv2_1"")
        conv2_2 = conv(conv2_1, ""conv2_2"")
    input_nodes = [""Net/input""]

    # We need to output both to trigger the error.
    # Also note that if we includes ""Net/conv_shared/BiasAdd"" to output_nodes, the bug
    # disappears.
    output_nodes = [""Net/conv1_2/BiasAdd"", ""Net/conv2_2/BiasAdd""]
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        const_graph_def = tf.graph_util.convert_variables_to_constants(
            sess, sess.graph.as_graph_def(), output_nodes)

    optimized_graph_def = trt.create_inference_graph(
        input_graph_def=const_graph_def,
        outputs=output_nodes,
        max_batch_size=1,
        max_workspace_size_bytes=1 << 25)

    graph, input_tensors, output_tensors = build_graph_from_def(
        optimized_graph_def, input_nodes, output_nodes)

    # Seems to be input-value irrelevant, also tested with np.ones and random
    input_value = np.zeros((1, 28, 28, 3))
    with tf.Session(graph=graph) as sess:
        converted_output = sess.run(output_tensors[0], feed_dict={
                                    input_tensors[0]: input_value})

    graph, input_tensors, output_tensors = build_graph_from_def(
        const_graph_def, input_nodes, output_nodes)
    with tf.Session(graph=graph) as sess:
        original_output = sess.run(output_tensors[0], feed_dict={
            input_tensors[0]: input_value})

    deltas = np.abs(converted_output - original_output)

    # Output would be random and varies each time (maybe correct by accident),
    # indicating being memory or graph-weight specific issue.
    print(""min={}"".format(np.min(deltas)))
    print(""max={}"".format(np.max(deltas)))
    print(""median={}"".format(np.median(deltas)))
    print(""mean={}"".format(np.mean(deltas)))

if __name__ == ""__main__"":
    main()
```


**Other info / logs**
This may be Nvidia's bug as well, yet not able to test that directly. Thoughts about the problem has been included in the sample code. A typical output looks like the following:
```
2018-11-02 11:51:59.336625: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-11-02 11:51:59.401658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-11-02 11:51:59.402192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:01:00.0
totalMemory: 7.92GiB freeMemory: 6.11GiB
2018-11-02 11:51:59.402205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-11-02 11:51:59.676511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-02 11:51:59.676541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2018-11-02 11:51:59.676547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2018-11-02 11:51:59.676703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5857 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-11-02 11:51:59.778756: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 1
2018-11-02 11:51:59.778802: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session
2018-11-02 11:51:59.778973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-11-02 11:51:59.778987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-02 11:51:59.778992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2018-11-02 11:51:59.778996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2018-11-02 11:51:59.779084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5857 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-11-02 11:51:59.787791: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'Net/', converted to graph
2018-11-02 11:51:59.787804: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:418] Can't find a device placement for the op!
2018-11-02 11:52:01.472421: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine Net/my_trt_op_0 creation for segment 0, composed of 13 nodes succeeded.
2018-11-02 11:52:01.475092: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-11-02 11:52:01.475574: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2018-11-02 11:52:01.476125: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: tf_graph
2018-11-02 11:52:01.476138: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 21 nodes (-10), 20 edges (-10), time = 1.141ms.
2018-11-02 11:52:01.476143: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 26 nodes (5), 26 edges (6), time = 0.54ms.
2018-11-02 11:52:01.476147: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 14 nodes (-12), 14 edges (-12), time = 1685.31897ms.
2018-11-02 11:52:01.476151: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 14 nodes (0), 14 edges (0), time = 0.442ms.
2018-11-02 11:52:01.476155: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 14 nodes (0), 14 edges (0), time = 0.576ms.
2018-11-02 11:52:01.476159: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: Net/my_trt_op_0_native_segment
2018-11-02 11:52:01.476163: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 14 nodes (0), 13 edges (0), time = 0.537ms.
2018-11-02 11:52:01.476167: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 14 nodes (0), 13 edges (0), time = 0.339ms.
2018-11-02 11:52:01.476171: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 14 nodes (0), 13 edges (0), time = 0.071ms.
2018-11-02 11:52:01.476175: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 14 nodes (0), 13 edges (0), time = 0.405ms.
2018-11-02 11:52:01.476180: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 14 nodes (0), 13 edges (0), time = 0.066ms.
2018-11-02 11:52:01.505052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-11-02 11:52:01.505082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-02 11:52:01.505088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2018-11-02 11:52:01.505092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2018-11-02 11:52:01.505183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5857 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-11-02 11:52:01.527376: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-11-02 11:52:01.527397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-02 11:52:01.527403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2018-11-02 11:52:01.527407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2018-11-02 11:52:01.527486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5857 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
min=4.95342129678e+29
max=2.45457421171e+31
median=6.9235024529e+30
mean=8.45157562195e+30

```
"
23459,AOT compilation of an Estimator,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:1.11
- **Python version**:2.7
- **Bazel version (if compiling from source)**:NA
- **GCC/Compiler version (if compiling from source)**:NA
- **CUDA/cuDNN version**:NA
- **GPU model and memory**:NA
- **Exact command to reproduce**:NA

I find it very challenging to find in the documentation any help on this important issue.. Indeed, after creating an estimator (canned or custom), one wants to `tf.compile` the resulting predictor, produce the `.so` and link it to one's project..

So I have my calib class in which I define a simple linear estimator

    self.model = tf.estimator.LinearRegressor(
            feature_columns=self.feature_columns,
            model_dir = self.model_dir)

After training, I want to 
1- get the trained model with optimal parameters (load it in my variable self.model)
2- extract the graph and freeze it
3- tf.compile that graph

I could not find any way to do parts 1- and 2-. 
Can you please point me to a good way to it?"
23458,XLA creates CUDA contexts on GPUs not in gpu_options.visible_device_list,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 1.12.0-rc2
- Python version: 3.5
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version: CUDA 10.0, cuDNN 7.4
- GPU model and memory: 8xV100 32GB

**Describe the current behavior**
When XLA is enabled, CUDA contexts are created on every device visible to the CUDA driver (excluding devices with CUDA_VISIBLE_DEVICES works, but gpu_options.visible_device_list in ConfigProto does not).

**Describe the expected behavior**
When gpu_options.visible_device_list is specified, CUDA contexts should only be created on the devices listed.

**Code to reproduce the issue**
To reproduce, run the following on a multi-gpu system and check nvidia-smi during the 20 second sleep.

```
import tensorflow as tf
import numpy as np
import time

config = tf.ConfigProto()
config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1
config.gpu_options.visible_device_list=""0""

input = tf.placeholder(tf.float32, [1000])
data = np.random.rand(1000).astype('float32')
output = tf.nn.softmax(tf.nn.relu(input))

with tf.Session(config=config) as sess:
  res = sess.run(output, feed_dict={input: data})
  print(""Session executed, check devices with nvidia-smi"")
  time.sleep(20)
  print(""Exiting"")
```

**Other info / logs**

"
23457,Previous Issue Persists in Android Demo (https://github.com/tensorflow/tensorflow/issues/21431),"https://github.com/tensorflow/tensorflow/issues/21431 is  both listed as awaiting response and closed. With latest version of TensorFlow I have same compilation issue in Android Demo project.

Namely the Java Compiler Error:
""error: cannot find symbol class Fill where T is a type-variable:T extends Object declared in class Zeros""
when attempting to run to either device or simulator.  The Gradle build is successful but project cannot be run.

TFMobile is still important to me still because TFLite doesn't support Switch and as a result many example graphs do not work in TFLite yet.
"
23456,"Tensorflow softmax is not updating, docu doesn't help?","Please be so kind to delete this issue for me, thank you in advance."
23455,Excessive MKL memory consumption with variable sized tensors,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Both source and binary
- TensorFlow version (use command below): 1.11.0
- Python version: 3.5.2
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source): gcc 5.4.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
During inference of variable sized examples with Tensorflow MKL system memory consumption gradually increases.
If a server application is running long enough to go through a lot of examples of same rank but different shapes it would eventually run out of system memory.
After further investigation setting the [TF_MKL_OPTIMIZE_PRIMITVE_MEMUSE](https://github.com/tensorflow/tensorflow/blob/c19e29306ce1777456b2dbb3a14f511edf7883a8/tensorflow/core/util/mkl_util.h#L2041) to either 0 or 1 does not help.
Most MKL primitive factories ignore TF_MKL_OPTIMIZE_PRIMITVE_MEMUSE and even some that know about it may choose to ignore it based on other heuristics like [batches smaller than 32](https://github.com/tensorflow/tensorflow/commit/86d9ce130c5691cdba16024f7cc7987082acd294#diff-192dfeafbdd684934bdb0dfa8983674a) for example (I am not sure this is intended behavior).
These [""do_not_cache"" heuristics](https://github.com/tensorflow/tensorflow/blob/c19e29306ce1777456b2dbb3a14f511edf7883a8/tensorflow/core/kernels/mkl_conv_ops.cc#L902
) on the other hand don't seem to follow the described logic in the comments above replacing ""or"" for ""and"".

**Describe the expected behavior**
There should be a way to disable MKL primitive memory reuse globally with an environment variable be it TF_MKL_OPTIMIZE_PRIMITVE_MEMUSE or better yet something else.
Primitive memory reuse does not play nice with unknown tensor sizes as most of the time there are no cache hits and allocated memory is simply piled up in the cache.

**Code to reproduce the issue**
I've attached a small example.
[min-leak-example.txt](https://github.com/tensorflow/tensorflow/files/2542973/min-leak-example.txt)

**Other info / logs**
I've attached Valgrind results of the example running on TF version 1.11.0 with and without MKL.
[min-leak-example-mkl.txt](https://github.com/tensorflow/tensorflow/files/2542977/min-leak-example-mkl.txt)
[min-leak-example-no-mkl.txt](https://github.com/tensorflow/tensorflow/files/2542978/min-leak-example-no-mkl.txt)

I've been working on a solution I can propose.
Should you find it reasonable I've extended the solution to full refactoring of the MKL primitive factories."
23454,checkpoint version,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
23452,Eager: dtype not checked,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution: Ubuntu 16.04.4 LTS
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): `v1.11.0-0-gc19e29306c 1.11.0`
- Python version: 3.6.5 Anaconda
- CUDA/cuDNN version: Cuda 9.0
- GPU model and memory: does not apply - I used CPU

**Describe the current behavior**

The following code
```
import tensorflow as tf
tf.enable_eager_execution()
print(1.2*tf.constant(2))
```
does not crash and outputs `Tensor(2, dtype=int32)`. 

**Describe the expected behavior**

Either there should be an error indicating dtype not matched, or there should be an implicit cast.

**Code to reproduce the issue**

See above. "
23451,Current implementation does not yet support strides in the batch and depth dimensions.,"Hello,

I try to design autoencoder in tensorflow that was previously made with tensorflow contrib tf.contrib.layer.
Here is the code : 
```
# An undercomplete autoencoder on MNIST dataset
from __future__ import division, print_function, absolute_import
import tensorflow.contrib.layers as lays

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from skimage import transform
from tensorflow.examples.tutorials.mnist import input_data

batch_size = 500  # Number of samples in each batch
epoch_num = 5     # Number of epochs to train the network
lr = 0.001        # Learning rate

def resize_batch(imgs):
    # A function to resize a batch of MNIST images to (32, 32)
    # Args:
    #   imgs: a numpy array of size [batch_size, 28 X 28].
    # Returns:
    #   a numpy array of size [batch_size, 32, 32].
    imgs = imgs.reshape((-1, 28, 28, 1))
    resized_imgs = np.zeros((imgs.shape[0], 32, 32, 1))
    for i in range(imgs.shape[0]):
        resized_imgs[i, ..., 0] = transform.resize(imgs[i, ..., 0], (32, 32))
    return resized_imgs


def autoencoder(inputs):
    # encoder
    # 32 x 32 x 1   ->  16 x 16 x 32
    # 16 x 16 x 32  ->  8 x 8 x 16
    # 8 x 8 x 16    ->  2 x 2 x 8
    # print('inputs {0}'.format(inputs))

    # net = lays.conv2d(inputs, 32, [5, 5], stride=2, padding='SAME')
    # print('net {0}'.format(net))

    # net = lays.conv2d(net, 16, [5, 5], stride=2, padding='SAME')
    # print('net {0}'.format(net))

    # net = lays.conv2d(net, 8, [5, 5], stride=4, padding='SAME')
    # print('net {0}'.format(net))

    # # decoder
    # # 2 x 2 x 8    ->  8 x 8 x 16
    # # 8 x 8 x 16   ->  16 x 16 x 32
    # # 16 x 16 x 32  ->  32 x 32 x 1
    # net = lays.conv2d_transpose(net, 16, [5, 5], stride=4, padding='SAME')
    # print('net {0}'.format(net))

    # net = lays.conv2d_transpose(net, 32, [5, 5], stride=2, padding='SAME')
    # print('net {0}'.format(net))

    # net = lays.conv2d_transpose(net, 1, [5, 5], stride=2, padding='SAME', activation_fn=tf.nn.tanh)

    # print('net {0}'.format(net))
    # return net

    xi = tf.nn.conv2d(ae_inputs,
                 filter=tf.Variable(tf.random_normal([5,5,1,32])),
                 strides=[1,2,2,1],
                 padding='SAME')
    xi = tf.nn.relu(xi)
    print(""xi {0}"".format(xi))

    xi = tf.nn.conv2d(xi,
                     filter=tf.Variable(tf.random_normal([5,5,32,16])),
                     strides=[1,2,2,32],
                     padding='SAME')
    xi = tf.nn.relu(xi)
    print(""xi {0}"".format(xi))

    xi = tf.nn.conv2d(xi,
                     filter=tf.Variable(tf.random_normal([5,5,16,8])),
                     strides=[1,4,4,16],
                     padding='SAME')
    xi = tf.nn.relu(xi)

    print(""xi {0}"".format(xi))






    xo = tf.nn.conv2d_transpose(xi,
                     filter=tf.Variable(tf.random_normal([5,5,16,8])),
                     output_shape=[tf.shape(xi)[0], 8, 8, 16],
                     strides=[1,4,4,1],
                     padding='SAME')
    xo = tf.nn.relu(xo)

    print(""xo {0}"".format(xo))

    xo = tf.nn.conv2d_transpose(xo,
                     filter=tf.Variable(tf.random_normal([5,5,32,16])),
                     output_shape=[tf.shape(xo)[0], 16, 16, 32],
                     strides=[1,2,2,1],
                     padding='SAME')
    xo = tf.nn.relu(xo)

    print(""xo {0}"".format(xo))

    xo = tf.nn.conv2d_transpose(xo,
                     filter=tf.Variable(tf.random_normal([5,5,1,32])),
                     output_shape=[tf.shape(xo)[0], 32, 32, 1],
                     strides=[1,2,2,1],
                     padding='SAME')
    xo = tf.nn.tanh(xo)


    print(""xo {0}"".format(xo))
    return xo
# read MNIST dataset
mnist = input_data.read_data_sets(""MNIST_data"", one_hot=True)

# calculate the number of batches per epoch
batch_per_ep = mnist.train.num_examples // batch_size

ae_inputs = tf.placeholder(tf.float32, (None, 32, 32, 1))  # input to the network (MNIST images)
ae_outputs = autoencoder(ae_inputs)  # create the Autoencoder network

# calculate the loss and optimize the network
loss = tf.reduce_mean(tf.square(ae_outputs - ae_inputs))  # claculate the mean square error loss
with tf.name_scope('train'):
    train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)



# initialize the network
init = tf.global_variables_initializer()
saver = tf.train.Saver()
stddev = tf.sqrt(tf.reduce_mean(tf.square(ae_inputs - ae_outputs)))

with tf.name_scope('stddev'):
        tf.summary.scalar('accuracy', stddev)
with tf.Session() as sess:
    sess.run(init)
    train_writer = tf.summary.FileWriter('/tmp/autoencoder/', sess.graph)
    index = 0
    for ep in range(epoch_num):  # epochs loop
        for batch_n in range(batch_per_ep):  # batches loop
            batch_img, batch_label = mnist.train.next_batch(batch_size)  # read a batch
            batch_img = batch_img.reshape((-1, 28, 28, 1))               # reshape each sample to an (28, 28) image
            batch_img = resize_batch(batch_img)                          # reshape the images to (32, 32)
            run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
            run_metadata = tf.RunMetadata()
            accuracy, c = sess.run([train_op, loss], feed_dict={ae_inputs: batch_img},
                              options=run_options,
                              run_metadata=run_metadata)
            print('Epoch: {} - cost= {:.5f}'.format((ep + 1), c))
            if batch_n % 10 == 0:

              summ=tf.Summary()
              summ.value.add(tag='Cost', simple_value = c)
              summ.value.add(tag='Accuracy', simple_value = accuracy)
              #train_writer.add_run_metadata(run_metadata, 'step%03d' % batch_n)
              train_writer.add_summary(summ,index)
              index+=1
              train_writer.flush()
              save_path = saver.save(sess, ""/tmp/autoencoder/model.ckpt"")

              #file_writer = tf.summary.FileWriter(logdir='/tmp/autoencoder/')
              print(""Model saved in path: %s"" % save_path)

    train_writer.close()

    # test the trained network
    batch_img, batch_label = mnist.test.next_batch(50)
    batch_img = resize_batch(batch_img)
    recon_img = sess.run([ae_outputs], feed_dict={ae_inputs: batch_img})[0]

    # plot the reconstructed images and their ground truths (inputs)
    plt.figure(1)
    plt.title('Reconstructed Images')
    for i in range(50):
        plt.subplot(5, 10, i+1)
        plt.imshow(recon_img[i, ..., 0], cmap='gray')
    plt.figure(2)
    plt.title('Input Images')
    for i in range(50):
        plt.subplot(5, 10, i+1)
        plt.imshow(batch_img[i, ..., 0], cmap='gray')
    plt.show()


```

Where I run the code I get that error:

```
`WARNING:tensorflow:From ./simple_autoencoder.py:115: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Please write your own downloading logic.
WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data to implement this functionality.
Extracting MNIST_data/train-images-idx3-ubyte.gz
WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data to implement this functionality.
Extracting MNIST_data/train-labels-idx1-ubyte.gz
WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.one_hot on tensors.
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
xi Tensor(""Relu:0"", shape=(?, 16, 16, 32), dtype=float32)
xi Tensor(""Relu_1:0"", shape=(?, 8, 8, 16), dtype=float32)
xi Tensor(""Relu_2:0"", shape=(?, 2, 2, 8), dtype=float32)
xo Tensor(""Relu_3:0"", shape=(?, 8, 8, 16), dtype=float32)
xo Tensor(""Relu_4:0"", shape=(?, 16, 16, 32), dtype=float32)
xo Tensor(""Tanh:0"", shape=(?, 32, 32, 1), dtype=float32)
2018-11-02 09:21:05.882917: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.
  warn(""The default mode, 'constant', will be changed to 'reflect' in ""
2018-11-02 09:21:06.345232: E tensorflow/core/common_runtime/executor.cc:630] Executor failed to create kernel. Invalid argument: Current implementation does not yet support strides in the batch and depth dimensions.
	 [[{{node Conv2D_1}} = Conv2D[T=DT_FLOAT, data_format=""NHWC"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 2, 2, 32], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Relu, Variable_1/read)]]
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1292, in _do_call
    return fn(*args)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1277, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1367, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Current implementation does not yet support strides in the batch and depth dimensions.
	 [[{{node Conv2D_1}} = Conv2D[T=DT_FLOAT, data_format=""NHWC"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 2, 2, 32], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Relu, Variable_1/read)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""./simple_autoencoder.py"", line 150, in <module>
    run_metadata=run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 887, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1110, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1286, in _do_run
    run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1308, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Current implementation does not yet support strides in the batch and depth dimensions.
	 [[{{node Conv2D_1}} = Conv2D[T=DT_FLOAT, data_format=""NHWC"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 2, 2, 32], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Relu, Variable_1/read)]]

Caused by op 'Conv2D_1', defined at:
  File ""./simple_autoencoder.py"", line 121, in <module>
    ae_outputs = autoencoder(ae_inputs)  # create the Autoencoder network
  File ""./simple_autoencoder.py"", line 69, in autoencoder
    padding='SAME')
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py"", line 957, in conv2d
    data_format=data_format, dilations=dilations, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): Current implementation does not yet support strides in the batch and depth dimensions.
	 [[{{node Conv2D_1}} = Conv2D[T=DT_FLOAT, data_format=""NHWC"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 2, 2, 32], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Relu, Variable_1/read)]]
`

```
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): 1.10
- Python version: 3.5
- Bazel version (if compiling from source): None
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609


"
23450,"[tf.nn.ctc_loss] Why we need input *sequence_length*, when we already inputted *inputs*?","In tf.nn.ctc_loss:

tf.nn.ctc_loss(
    labels,
    inputs,
    sequence_length,
    preprocess_collapse_repeated=False,
    ctc_merge_repeated=True,
    ignore_longer_outputs_than_inputs=False,
    time_major=True
)

_**inputs**_ are sequence,
_**sequence_length**_ are length of sequence.

Q1:
Why we need to setting **_sequence_length_** when sequence are given?
Q2:
Can we setting **_sequence_length_** shorter then len(inputs)?"
23449,Bug in tensorflow lite java wrapper,"In file 
""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/src/main/java/org/tensorflow/lite/NativeInterpreterWrapper.java"" line 286:
  ****/** Gets the number of output tensors. */
  int getOutputTensorCount() {
    return inputTensors.length;
  }****

In this function, output tensors.length should be used instead of inputTensors.length"
23448,Not getting position of detected objects,"Hi 
Not able to get the position of detected objects.
Thanks "
23447,CONV2 occurrence mismatch in timeline,"**System information**
- Linux Ubuntu 16.04
- TensorFlow installed binary
- TensorFlow 1.10 and 1.12 both
- Python 2.7:
- CUDA 9.0
- cuDNN 7.3
- GPU model and memory: NVIDIA V100 (16GB) x 8

**Describe the current behavior**
dump timeline of benchmark with following command:
python tf_cnn_benchmarks.py --local_parameter_device=gpu --model=resnet50 --data_dir=/home/nvme/dataset/imagenet/tf_records/train/ --variable_update=replicated --all_reduce_spec=nccl --use_fp16=True --batch_size=128 --num_gpus=1 --trace_file=/home/timeline/benchmark_resnet50_imagenet_gpu1.json

then the number of CONV2D op send to GPU is 53
![image](https://user-images.githubusercontent.com/15809856/47891157-fc087300-de8c-11e8-81d6-40446b20ec3d.png)

but GPU run so many CONV2D:
![image](https://user-images.githubusercontent.com/15809856/47891199-31ad5c00-de8d-11e8-8c89-3823de6d5ecc.png)

I think they should be 53 both.
"
23443,Python client link is broken,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: unrelated, I'm using github ui viewing the code
- Doc Link: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/client_lib.py#L18


**Describe the documentation issue**
This link https://tensorflow.org/api_guides/python/client gives 404

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
23442,Can't install tensorflow with Python 3.7,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacoS Mojave
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): PIP
- TensorFlow version: 1.10.0
- Python version: 3.7
- Installed using virtualenv? pip? conda?: pip



**Describe the problem**

There's no wheel file provided for Python 3.7 hence I cannot install tensorflow while using Python 3.7

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23440,Tensorflow micro build fail: fatal error: 'NEON_2_SSE.h' file not found,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac
- TensorFlow installed from (source or binary): source
- TensorFlow version: latest git

**Describe the problem**

Following the instructions here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro#getting-started

```
make -f tensorflow/lite/experimental/micro/tools/make/Makefile test
tensorflow/lite/experimental/micro/tools/make/gen/osx_x86_64/bin/tensorflow/lite/experimental/micro/micro_error_reporter.test_target tensorflow/lite/experimental/micro/tools/make/gen/osx_x86_64/bin/tensorflow/lite/experimental/micro/micro_interpreter.test_target tensorflow/lite/experimental/micro/tools/make/gen/osx_x86_64/bin/tensorflow/lite/experimental/micro/micro_mutable_op_resolver.test_target tensorflow/lite/experimental/micro/tools/make/gen/osx_x86_64/bin/tensorflow/lite/experimental/micro/simple_tensor_allocator.test_target tensorflow/lite/experimental/micro/tools/make/gen/osx_x86_64/bin/tensorflow/lite/experimental/micro/kernels/depthwise_conv.test_target tensorflow/lite/experimental/micro/tools/make/gen/osx_x86_64/bin/tensorflow/lite/experimental/micro/kernels/fully_connected.test_target tensorflow/lite/experimental/micro/tools/make/gen/osx_x86_64/bin/tensorflow/lite/experimental/micro/kernels/softmax.test_target
g++ -O3 -DNDEBUG --std=c++11 -g -DTF_LITE_STATIC_MEMORY -I. -Itensorflow/lite/experimental/micro/tools/make/../../../../../ -Itensorflow/lite/experimental/micro/tools/make/../../../../../../ -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/experimental/micro/kernels/depthwise_conv.cc -o tensorflow/lite/experimental/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/experimental/micro/kernels/depthwise_conv.o
In file included from tensorflow/lite/experimental/micro/kernels/depthwise_conv.cc:16:
./tensorflow/lite/c/builtin_op_data.h:144:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]
typedef struct {
        ^
./tensorflow/lite/c/builtin_op_data.h:147:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]
typedef struct {
        ^
./tensorflow/lite/c/builtin_op_data.h:217:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]
typedef struct {
        ^
./tensorflow/lite/c/builtin_op_data.h:220:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]
typedef struct {
        ^
./tensorflow/lite/c/builtin_op_data.h:259:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]
typedef struct {
        ^
In file included from tensorflow/lite/experimental/micro/kernels/depthwise_conv.cc:18:
./tensorflow/lite/kernels/internal/common.h:42:10: fatal error: 'NEON_2_SSE.h' file not found
#include ""NEON_2_SSE.h""
         ^~~~~~~~~~~~~~
5 warnings and 1 error generated.
make: *** [tensorflow/lite/experimental/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/experimental/micro/kernels/depthwise_conv.o] Error 1
```

Before that there is another error about BZL_FILE_PATH but I've used the fix in https://github.com/tensorflow/tensorflow/pull/23424 to get past that.

Could this NEON_2_SSE.h problem also be due the the move out of contrib?"
23438,LSTM Cell Training Error,"<em>I have a problem with multi layer lstm training. The accuracy and output model slowly changing when calling ""sess.run(logits, feed_dict={X:...})"" during train process. The problem doesn't happens when I decreased number of layers or neurons or dataset size.

**System information**
- Google colab selecting gpu
- Python version 3




```
n_epochs = 6
batch_size = 150

n_layers = 3
n_neurons = 300
n_outputs = 2
learning_rate = 0.001


reset_graph()

X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])
y = tf.placeholder(tf.int32, [None])
devices = [""/gpu:0""] # replace with [""/gpu:0"", ""/gpu:1"", ""/gpu:2""]
lstm_cells = [DeviceCellWrapper(dev,tf.contrib.rnn.LSTMCell(num_units=n_neurons))
       for layer in range(n_layers)
       for dev in devices]
multi_cell = tf.contrib.rnn.MultiRNNCell(lstm_cells)      
outputs, states = tf.nn.dynamic_rnn(multi_cell, X, dtype=tf.float32)
top_layer_h_state = states[-1][1]
logits = tf.layers.dense(top_layer_h_state, n_outputs, name=""softmax"")
xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)
loss = tf.reduce_mean(xentropy, name=""loss"")
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
training_op = optimizer.minimize(loss)
correct = tf.nn.in_top_k(logits, y, 1)
accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))

init = tf.global_variables_initializer()


saver = tf.train.Saver()


with tf.Session() as sess:
    init.run()
    maxValidationPoint = float('-inf')

    for epoch in range(n_epochs):
        trainDataIndexInPtr = 0
        while(True):
            X_batch, y_batch = next_batch(batch_size, X_train, y_train)
            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})
            if(trainDataIndexInPtr == 0):
                break

        acc_train = accuracy.eval(feed_dict={X: X_train.reshape((len(X_train), n_steps, n_inputs)), y: y_train.reshape((len(y_train)))})
        acc_validation = accuracy.eval(feed_dict={X: X_validation, y: y_validation})

        y_pred = sess.run(logits, feed_dict={X: X_validation})
        y_pred2 = sess.run(logits, feed_dict={X: X_test})

        print(""Train accuracy ="", ""%.7f"" % acc_train, ""Validation accuracy ="", ""%.7f"" % acc_validation)

        if(acc_validation > maxAcc_validation ):
            maxAcc_validation = acc_validation
            saver.save(sess, ""./Model"")
```

### Output:

Train accuracy = 0.6400015 Validation accuracy = 0.6296040
Train accuracy = 0.6409340 Validation accuracy = 0.6253070
Train accuracy = 0.6552114 Validation accuracy = 0.6393685
Train accuracy = 0.6643262 Validation accuracy = 0.6265539
Train accuracy = 0.6804933 Validation accuracy = 0.6232543
Train accuracy = 0.7023602 Validation accuracy = 0.6231584

### if I throw away ""y_pred = sess.run(logits, feed_dict={X: X_validation})"" and ""y_pred2 = sess.run(logits, feed_dict={X: X_test})"", Output:

Train accuracy = 0.6400015 Validation accuracy = 0.6296040
Train accuracy = 0.6409340 Validation accuracy = 0.6253070
Train accuracy = 0.6552114 Validation accuracy = 0.6393685
Train accuracy = 0.6643139 Validation accuracy = 0.6265347
**Train accuracy = 0.6794249 Validation accuracy = 0.6231584
Train accuracy = 0.7062445 Validation accuracy = 0.6132405**
"
23437,Deploying TF-TRT model on DLA on NVIDIA Xavier platform,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.8
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
Currently the TF-TRT integration only allows to deploy on dGPU or iGPU by changing DEFAULT_CUDA_VISIBLE_DEVICE. However, to deploy on DLA you need to go through native TRT support, which nullifies the USP of TF-TRT(falling back on TF for unsupported ops instead of writing custom ops). Please see https://github.com/nvdla/sw for details.

**Will this change the current api? How?** no

**Who will benefit with this feature?** Everyone deploying TF models on NVIDIA based embedded H/W including Jetson, DRIVE AGX

**Any Other info.**
"
23436,Tensorflow crashes on source building!!!,"**System information**
- Linux Ubuntu 18.04.1):
- No mobile problem
- https://github.com/tensorflow/tensorflow.git
- TensorFlow version: 1.11.0
- Python version: 2.7.15rc1
- Haven't done any pip installation
- Bazel version: Build label: 0.19.0
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version:
- GPU model and memory: Not relevant



**I am trying to install Tensorflow, but I keep getting the same problem when I use this command to build it:
`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package //tensorflow:libtensorflow_framework.so`**

**Command sequence:**
````
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
./configure
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package //tensorflow:libtensorflow_framework.so
````

**The output I get:**
ERROR: /root/tensorflow/tensorflow/python/BUILD:371:1: C++ compilation of rule '//tensorflow/python:ndarray_tensor' failed (Exit 1) gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 338 argument(s) skipped)

Use --sandbox_debug to see verbose messages from the sandbox
In file included from ./tensorflow/python/lib/core/ndarray_tensor.h:20:0,
                 from tensorflow/python/lib/core/ndarray_tensor.cc:16:
./tensorflow/python/lib/core/numpy.h:35:10: fatal error: Python.h: No such file or directory
 #include <Python.h>
          ^~~~~~~~~~
compilation terminated.
"
23435,Tensorflow crashes on source building!!!,"I am trying to install Tensorflow, but I keep getting the same problem when I use this command to build it:
`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package //tensorflow:libtensorflow_framework.so
`
The error I get is:

/root/tensorflow/tensorflow/python/BUILD:371:1: C++ compilation of rule '//tensorflow/python:ndarray_tensor' failed (Exit 1) gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 338 argument(s) skipped)

Use --sandbox_debug to see verbose messages from the sandbox
In file included from ./tensorflow/python/lib/core/ndarray_tensor.h:20:0,
                 from tensorflow/python/lib/core/ndarray_tensor.cc:16:
./tensorflow/python/lib/core/numpy.h:35:10: fatal error: Python.h: No such file or directory
 #include <Python.h>
          ^~~~~~~~~~
compilation terminated.

Server: DELL9020 Linux Ubuntu server
Python version: 2.7.15rc1

Here are the commands I used:
`git clone https://github.com/tensorflow/tensorflow.git`
`cd tensorflow`
`./configure`

And this is the command where it crashes:
`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`

Does someone know what to do?

"
23434,Windows cluster: could not start gRPC server,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.8.0
- Python version: 3.6.5
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: 9
- GPU model and memory:  Quadro K22000 with 3.34GB


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I have a cluster running on Windows 10 and wanted to test tensorflow on a distributed environment. As a test, I want to detect the video card installed on 3 machines. When I ran the code, I get the following error messages:

E1101 16:04:32.821000000 17460 server_chttp2.cc:40] {""created"":""@1541106272.820000000"",""description"":""No address added out of total 1 resolved"",""file"":""T:\src\github\tensorflow\cmake_build\grpc\src\grpc\src\core\ext\transport\chttp2\server\chttp2_server.cc"",""file_line"":307,""referenced_errors"":[{""created"":""@1541106272.820000000"",""description"":""Failed to add port to server"",""file"":""T:\src\github\tensorflow\cmake_build\grpc\src\grpc\src\core\lib\iomgr\tcp_server_windows.cc"",""file_line"":508,""referenced_errors"":[{""created"":""@1541106272.820000000"",""description"":""OS Error"",""file"":""T:\src\github\tensorflow\cmake_build\grpc\src\grpc\src\core\lib\iomgr\tcp_server_windows.cc"",""file_line"":200,""os_error"":""Only one usage of each socket address (protocol/network address/port) is normally permitted.\r\n"",""syscall"":""bind"",""wsa_error"":10048}]}]}
Process Process-4:
Traceback (most recent call last):
  File ""C:\Temp\Python\Python-3.6.5\lib\multiprocessing\process.py"", line 258, in _bootstrap
    self.run()
  File ""C:\Temp\Python\Python-3.6.5\lib\multiprocessing\process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""C:\Users\IMarroquin\Documents\My_Python_Scripts\MLP\Test_main_distributed.py"", line 34, in find_amount_gpus
    server= tf.train.Server(spec, job_name= params.job_name, task_index= params.task_index)
  File ""C:\Temp\Python\Python-3.6.5\lib\site-packages\tensorflow\python\training\server_lib.py"", line 147, in __init__
    self._server_def.SerializeToString(), status)
  File ""C:\Temp\Python\Python-3.6.5\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 519, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.UnknownError: Could not start gRPC server
EE11101 16:04:321.0976000000 17868 server_chttp2.1 16:04:32c.c976000000 16224 server_chttp2.cc:40] {""created"":""@1541106272.976000000"",""description"":""No address added out of total 1 resolved"",""file"":""T:\src\github\tensorflow\cmake_build\grpc\src\grpc\src\core\ext\transport\chttp2\server\chttp2_server.cc"",""file_line"":307:,40] {""created"":""@1541106272.976000000"",""description"":""No address added out of total 1 resolv""referenced_errors"":[{""created"":""@1541106272.976000000"",""description"":""Failed to add port to server"",""filed"",""file"":""T:\src\github\tensorflow\cmake_beu""ild\grpc\src\grpc\src\core\ext\transpo:rt\chttp2\server\chttp2_server.cc"",""file_line"":307,""referenced_errors"":[{""created"":""@1541106272.976000000"",""descript""Ti:\src\github\tensorflow\cmake_build\grpc\src\grpc\src\core\lib\iomgr\tcp_server_windows.cc"",""file_line"":508,""referenced_errors"":[{""created"":""@1541106272.9760000on"":""Failed to add p0ort to0 server"",""file"":""T:\src\github\tensorflow\cmake_bui""l,d\grpc\src\grpc\src\core\lib\iomgr\tcp_server_windows.cc"",""file_line"":508,""referenced_errors"":[{""crea""ted"":""@1541106272.976000000"",""descriptdion"":""OS Error"",""file"":""T:\src\github\tensorflow\cmaeke_builds\grpc\src\grpc\src\core\lib\iomgr\tcp_secrirver_windows.cc"",""file_line"":200,""os_error"":""Only one usage option"":""OS Error"",""file"":""T:\src\github\tensorflow\cmake_build\grpc\src\grpc\src\core\lib\iomgr\tcp_server_windows.cc"",""file_line"":200,""os_error"":""Only one usage of eachf  seoach socket address (protocol/network address/port) is normally permitted.\r\n"",""syscall""c:""bind"",""wsa_error"":10048}]}]}
ket addressProcess Process-2:
 (protocol/network addreTraceback (most recent call last):
ss  File ""C:\Temp\Python\Python-3.6.5\lib\multiprocessing\process.py"", line 258, in _bootstrap
    self.run()
/port) is normally permitted.\r\n"",""syscall"":""bind"",""wsa_error""  File ""C:\Temp\Python\Python-3.6.5\lib\multiprocessing\process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
:10048}]}]}
  File ""C:\Users\IMarroquin\Documents\My_Python_Scripts\MLP\Test_main_distributed.py"", line 34, in find_amount_gpus
    server= tf.train.Server(spec, job_name= params.job_name, task_index= params.task_index)
  File ""C:\Temp\Python\Python-3.6.5\lib\site-packages\tensorflow\python\training\server_lib.py"", line 147, in __init__
    self._server_def.SerializeToString(), status)
Process Process-3:
  File ""C:\Temp\Python\Python-3.6.5\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 519, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.UnknownError: Could not start gRPC server
Traceback (most recent call last):
  File ""C:\Temp\Python\Python-3.6.5\lib\multiprocessing\process.py"", line 258, in _bootstrap
    self.run()
  File ""C:\Temp\Python\Python-3.6.5\lib\multiprocessing\process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""C:\Users\IMarroquin\Documents\My_Python_Scripts\MLP\Test_main_distributed.py"", line 34, in find_amount_gpus
    server= tf.train.Server(spec, job_name= params.job_name, task_index= params.task_index)
  File ""C:\Temp\Python\Python-3.6.5\lib\site-packages\tensorflow\python\training\server_lib.py"", line 147, in __init__
    self._server_def.SerializeToString(), status)
  File ""C:\Temp\Python\Python-3.6.5\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 519, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.UnknownError: Could not start gRPC server


**Describe the expected behavior**
I would like to be able to detect the video card and report it

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Main.py file:

from multiprocessing import Process, Pipe
import tensorflow as tf
from tensorflow.contrib.training import HParams
from GPUs import gpus_configuration

N_workers= 3
SPEC_Second= {""ps"": [""192.168.3.18:2222""], ""worker"": [""192.168.3.18:2222"", ""192.168.3.14:2222"", ""192.168.3.16:2222""]}

def find_amount_gpus(task, connection):
    spec= tf.train.ClusterSpec(SPEC_Second)
    
    params= HParams(cluster= spec, job_name= task[0], task_index= task[1])
    server= tf.train.Server(spec, job_name= params.job_name, task_index= params.task_index)
    
    if (params.job_name == ""ps""):
        server.join()
    elif (params.job_name == ""worker""):
        with tf.device(tf.train.replica_device_setter(worker_device= ""/job:worker/task:%d"" % params.task_index, cluster= spec)):
            gpus_configuration(connection)

if __name__ == '__main__':
    devices= [['ps', 0], ['worker', 0], ['worker', 1], ['worker', 2]]

jobs= []
    pipe_list= []
    
    for i in devices: #for i in range(0, 1):#N_workers):
        print(""In loop to find GPUs {}\n"".format(i))
        parent_conn, child_conn= Pipe()
        p= Process(target= find_amount_gpus, args=(i, child_conn))
        jobs.append(p)
        pipe_list.append(parent_conn)
        p.start()
    
    total_results= [out.recv() for out in pipe_list]
    
    Worker_1_gpus= total_results[0]['GPUs_info']
    Worker_1_ip= total_results[0]['IPaddr']
    print(""Worker 1 {}\n"".format(Worker_1_gpus))
    print(""Worker 1 IP {}\n"".format(Worker_1_ip))
    Worker_2_gpus= total_results[1]['GPUs_info']
    Worker_2_ip= total_results[1]['IPaddr']
    print(""Worker 2 {}\n"".format(Worker_2_gpus))
    print(""Worker 2 IP {}\n"".format(Worker_2_ip))
    Worker_3_gpus= total_results[2]['GPUs_info']
    Worker_3_ip= total_results[2]['IPaddr']
    print(""Worker 3 {}\n"".format(Worker_3_gpus))
    print(""Worker 3 IP {}\n"".format(Worker_3_ip))

GPUs.py file

import os
import tensorflow as tf
from tensorflow.python.client import device_lib
    
os.environ['TF_CPP_MIN_LOG_LEVEL'] = ""3"" # Turn off Tensorflow messages
os.environ[""TF_MIN_GPU_MULTIPROCESSOR_COUNT""] = ""4""

"""""" Determine presence of GPUs or CPUs
""""""

def gpus_configuration(child_conn):
    import socket
    
    hostname= socket.gethostname()
    IPaddr= socket.gethostbyname(hostname)

    config= tf.ConfigProto()
    config.gpu_options.allow_growth= True            
    
    with tf.Session(config= config) as sess:
        local_device_protos= device_lib.list_local_devices()
        sess.close()

    gpus= len([x.name for x in local_device_protos if x.device_type == 'GPU'])
    
    total_results= {""GPUs_info"": gpus,
                    ""IPaddr"": IPaddr}
    
    child_conn.send(total_results)

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23432,Incorrect image resizing output in pix2pix_eager.ipynb,"The image resizing in the pix2pix sample code provided in this repository outputs images with out-of-bound values for RGB channels. [Link to pix2pix_eager.ipynb](https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/contrib/eager/python/examples/pix2pix/pix2pix_eager.ipynb)

In `load_image()` function, while loading test dataset, in the line

```
input_image = tf.image.resize_images(input_image, size=[IMG_HEIGHT, IMG_WIDTH], align_corners=True, method=2)
```

values greater than 255 and less than 0 are generated.

This issue does not occur while loading train dataset because `method=tf.image.ResizeMethod.NEAREST_NEIGHBOR` is used for resizing. Using the same for test dataset resolves the issue."
23431,tf keras model.save_weights not working with MirroredStrategy in 1.12,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes, duplicated the minimal code from documentation snippet from [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute#example-with-keras-api)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):""18.04.1 LTS (Bionic Beaver)""
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:None
- TensorFlow installed from (source or binary): source and binary
- TensorFlow version (use command below): v1.12.0-rc2-0-g748435b8ef
- Python version: 3.6.6
- Bazel version (if compiling from source): 0.15.2
- GCC/Compiler version (if compiling from source): 6.4.0
- CUDA/cuDNN version: 9.0 / 7.3
- GPU model and memory: 2 x 1080-ti


**Describe the current behavior**
Cannot save a tf.keras model if trained with MirroredStrategy either by calling save_weight or by a tf.keras.callbacks.ModelCheckpoint, but does work if MirroredStrategy is not used.


**Code to reproduce the issue**
```
inputs = tf.keras.layers.Input(shape=(1,))
predictions = tf.keras.layers.Dense(1)(inputs)
model = tf.keras.models.Model(inputs=inputs, outputs=predictions)

features = tf.data.Dataset.from_tensors([1.]).repeat(10000).batch(10)
labels = tf.data.Dataset.from_tensors([1.]).repeat(10000).batch(10)
train_dataset = tf.data.Dataset.zip((features, labels))

distribution = tf.contrib.distribute.MirroredStrategy()

model.compile(loss='categorical_crossentropy',
              optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.2),
              distribute=distribution)
model.fit(train_dataset, epochs=5, steps_per_epoch=10)
```
And adding:
```
model.save_weights('my_weight')
```
Error looks like:

```
Traceback (most recent call last):
  File ""/home/BP/anaconda3/envs/tf12_gpu/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/values.py"", line 72, in get
    return self._index[device]
KeyError: '/replica:0/task:0/device:CPU:0'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""minimal_example.py"", line 17, in <module>
    model.save_weights('my_weight')
  File ""/home/BP/anaconda3/envs/tf12_gpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 1449, in save_weights
    session = backend.get_session()
  File ""/home/BP/anaconda3/envs/tf12_gpu/lib/python3.6/site-packages/tensorflow/python/keras/backend.py"", line 469, in get_session
    _initialize_variables(session)
  File ""/home/BP/anaconda3/envs/tf12_gpu/lib/python3.6/site-packages/tensorflow/python/keras/backend.py"", line 722, in _initialize_variables
    variables = _get_variables(ops.get_default_graph())
  File ""/home/BP/anaconda3/envs/tf12_gpu/lib/python3.6/site-packages/tensorflow/python/keras/backend.py"", line 716, in _get_variables
    variables.update(opt.optimizer.variables())
  File ""/home/BP/anaconda3/envs/tf12_gpu/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 787, in variables
    optimizer_variables = [v for v in self._non_slot_variables()
  File ""/home/BP/anaconda3/envs/tf12_gpu/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 788, in <listcomp>
    if _from_current_graph(v)]
  File ""/home/BP/anaconda3/envs/tf12_gpu/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 781, in _from_current_graph
    return variable.op.graph is current_graph
  File ""/home/BP/anaconda3/envs/tf12_gpu/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/values.py"", line 308, in op
    return self.get().op
  File ""/home/BP/anaconda3/envs/tf12_gpu/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/values.py"", line 76, in get
    (device, self._index.keys(), device_util.current())), e)
  File ""<string>"", line 3, in raise_from
ValueError: Device /replica:0/task:0/device:CPU:0 not found in dict_keys(['/replica:0/task:0/device:GPU:0', '/replica:0/task:0/device:GPU:1']) (current device )
```
Changing to:
```
checkpoint_path = ""my_weight""
cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,
                                                 save_weights_only=True,
                                                 verbose=1,
                                                 period=1)
model.fit(train_dataset,
          epochs=5,
          steps_per_epoch=10,
          callbacks=[cp_callback])
```
Also does not work an end up with:
```
Epoch 1/5
 1/10 [==>...........................] - ETA: 4s - loss: 1.1921e-07
Epoch 00001: saving model to model_dir/my_weight
WARNING:tensorflow:You are accessing attribute _replicated_modelof the DistributedCallbackModel that may not have been set correctly.
Traceback (most recent call last):
  File ""minimal_example.py"", line 35, in <module>
    callbacks=[cp_callback])
  File ""/home/BP/anaconda3/envs/tf12_gpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1624, in fit
    validation_steps=validation_steps)
  File ""/home/BP/anaconda3/envs/tf12_gpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_distributed.py"", line 198, in fit_loop
    callbacks.on_epoch_end(epoch, epoch_logs)
  File ""/home/BP/anaconda3/envs/tf12_gpu/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py"", line 214, in on_epoch_end
    callback.on_epoch_end(epoch, logs)
  File ""/home/BP/anaconda3/envs/tf12_gpu/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py"", line 599, in on_epoch_end
    self.model.save_weights(filepath, overwrite=True)
  File ""/home/BP/anaconda3/envs/tf12_gpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 2336, in save_weights
    self._replicated_model.save_weights(filepath, overwrite=overwrite,
AttributeError: 'NoneType' object has no attribute 'save_weights'
```
"
23430,Copy just variable's value to another variable (both are trainable variables),"Hi!

I have investigated all resources and documentation for finding the way how to copy values of trainable variable to another and couldn't find how to do it. 

All existing solutions do deep copy (tf.assign(), tf.identity() etc.), so another variable contains the reference to the original variable: when I change one variable, another one is changing too. I don't need this, there shouldn't be any connection between variables.

How I can do it?

Thanks.
"
23429,how to get the timestamp of the specified operation?,"Hi,

  I want to get the timestamp of some operations in each iterations when training DNN. I know timeline can do this, but after some experiments I found timeline caused performance loss. And I only need to get information about a few operations, so I wonder whether it possible to get the timestamp of the specified operation?

Thanks
"
23427,Build failing with ` ERROR: Config value cuda is not defined in any .rc file`,"I build TF on a biweekly basis, the last successful build was done 3 days ago (29th Oct), but the build job is failing today, while there has been no changes in the installation script.

The build happens on an amazon linux machine with cuda 9.0 and cudnn 7.3, using the following command, 

```
bazel build --config=opt --config=mk1 --config=cuda --copt=-mavx --copt=-msse4.2 --copt=-msse4.1 //tensorflow/tools/pip_package:build_pip_package
```
Some of the ralevent flags are set as follows,
```
export TF_NEED_CUDA=1
export TF_CUDA_VERSION=""$(nvcc --version | sed -n 's/^.*release \(.*\),.*/\1/p')""
    export TF_CUDNN_VERSION=""$(sed -n 's/^#define CUDNN_MAJOR\s*\(.*\).*/\1/p' $CUDNN_INSTALL_PATH/include/cudnn.h)""
```
The job failes with the following error message,
```
+ bazel build --config=opt --config=cuda --copt=-mavx --copt=-msse4.2 --copt=-msse4.1 //tensorflow/tools/pip_package:build_pip_package
WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
/home/ec2-user/workspace/build-dl-package/src/deep-learning/tensorflow/tensorflow/tools/bazel.rc
Extracting Bazel installation...
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=0 --terminal_columns=80
INFO: Reading rc options for 'build' from /home/ec2-user/workspace/build-dl-package/src/deep-learning/tensorflow/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/lib/a-4.2.9-py-2.7.13-dl-gpu//bin/python --action_env PYTHON_LIB_PATH=/usr/lib/a-4.2.9-py-2.7.13-dl-gpu/lib/python2.7/site-packages --python_path=/usr/lib/a-4.2.9-py-2.7.13-dl-gpu//bin/python --define with_jemalloc=true --define with_hdfs_support=true --define with_aws_support=true --define with_kafka_support=true --action_env TF_NEED_OPENCL_SYCL=0 --action_env TF_NEED_CUDA=1 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda --action_env TF_CUDA_VERSION=9.0 --action_env CUDNN_INSTALL_PATH=/usr/local/cuda-9.0 --action_env TF_CUDNN_VERSION=7 --action_env NCCL_INSTALL_PATH=/usr/local/nccl --action_env TF_NCCL_VERSION=2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.0,3.5,5.2 --action_env LD_LIBRARY_PATH=/usr/java/jdk1.7.0_67/jre/lib/amd64/server:/usr/java/jdk1.7.0_67/jre/lib/amd64:/usr/java/jdk1.7.0_67/jre/../lib/amd64:/usr/lib/hadoop2.6/lib/native/:/usr/local/cuda-8.0/lib64:/usr/java/jdk1.8.0_121//jre/lib/amd64/server:/usr/lib/torch/install/lib:/usr/local/mpi/lib: --action_env TF_CUDA_CLANG=0 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/gcc --config=cuda --define with_mpi_support=true --define grpc_no_ares=true
ERROR: Config value cuda is not defined in any .rc file
````
The build is run against `r1.11` along with commit `89979f42e827d9eb5c349259a5aa2ec32d38c86a` cherry-picked.

Should I change something?
Thank you."
23426,problems about tf.nn.softmax,"I need a softmax function like this:

def softmax(a):
    c = np.max(a)
    exp_a = np.exp(a - c)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y 

please help
Thanks!"
23425,NameError name 'uses_learning_phase' is not defined when using Keras TimeDistributed with batch_shape on Input,"I have an LSTM layer where I am using `stateful=True`. As a result I need to specify `batch_shape` on my `Input` layers:

    input_xs = tf.keras.layers.Input(batch_shape=(batch_size,seq_length))

I have a layer where I add a context vector to each input *at each timestep*. I currently achieve this with `TimeDistributed` and a `Lambda` layer as follows:

    concatenator = tf.keras.layers.TimeDistributed(tf.keras.layers.Lambda(lambda x: tf.keras.layers.Concatenate()([x,concat_context])))

But the moment I added the `batch_shape` to my inputs I started getting the following error:

```
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 769, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\keras\layers\wrappers.py"", line 250, in call
    unroll=False)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\keras\backend.py"", line 3193, in rnn
    outputs, _ = step_function(inputs[0], initial_states + constants)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\keras\layers\wrappers.py"", line 242, in step
    uses_learning_phase)
NameError: name 'uses_learning_phase' is not defined
```

This seems related to: https://github.com/keras-team/keras/issues/4178

Would appreciate any thoughts on what is happening here."
23422,a  confused problem about placeholder,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- TensorFlow installed from (source or binary):from pycharm(+anaconda)
- TensorFlow version (use command below):1.9
- Python version:3.6
- CUDA/cuDNN version:8.0/5.1
- GPU model and memory:1060/6g


I want to  give values of lr and beta1 to adam with placeholder but there is an error'You must feed a value for placeholder tensor 'm' with dtype float'. it's confused because when i fix the beta1=0.5, the lr can deliver to the adam, once i want to deliver lr and beta1 to adam there will be error. 

`def build_training_pi_graph(x, y, ul_x, lr, unsup_weight, m):
    global_step = tf.get_variable(
        name=""global_step"",
        shape=[],
        dtype=tf.float32,
        initializer=tf.constant_initializer(0.0),
        trainable=False,
    )

    z_label1 = PiModels.call(x)
    z_label2 = PiModels.call(x)

    z_unlabeled1 = PiModels.call(ul_x)
    z_unlabeled2 = PiModels.call(ul_x)
    with tf.variable_scope(tf.get_variable_scope(), reuse=True):
        ce = tf.losses.softmax_cross_entropy(z_label1, y)
        ul_loss = tf.losses.mean_squared_error(z_label1, z_label2)
        l_loss = tf.losses.mean_squared_error(z_unlabeled1, z_unlabeled2)
        loss = ce + unsup_weight * (ul_loss + l_loss)

    opt = tf.train.AdamOptimizer(learning_rate=lr, beta1=m, beta2=0.999)
    tvars = tf.trainable_variables()
    grads_and_vars = opt.compute_gradients(loss, tvars)
    train_op = opt.apply_gradients(grads_and_vars, global_step=global_step)

    return loss, train_op, global_step`
`def main():
    with tf.Graph().as_default() as g:

        with tf.device(""/cpu:0""):
            train_labeled_iterator = inputs(batch_size=batch_size,
                                    train=True,
                                    validation=validation,
                                    shuffle=True, num_epochs=num_epochs+1000)
            train_x, train_y, label_index = train_labeled_iterator.get_next()

            train_unlabeled_iterator = unlabeled_inputs(batch_size=ul_batch_size,
                                        validation=validation,
                                         shuffle=True,num_epochs=num_epochs)
            ul_x, _, unlabel_index = train_unlabeled_iterator.get_next()

            validation_iterator = inputs(batch_size=eval_batch_size,
                                                          train=True,
                                                          validation=True,
                                                          shuffle=True)
            val_x, val_y, _ = validation_iterator.get_next()

            itest_iterator = inputs(batch_size=eval_batch_size,
                                                        train=False,
                                                        validation=validation,
                                                        shuffle=True)
            test_x, test_y, _= itest_iterator.get_next()

        with tf.device(device):
            lr = tf.placeholder(tf.float32, shape=[], name='learning_rate')
            unsupervised_weight = tf.placeholder(tf.float32, shape=[], name='unsupervised_weight')
            m = tf.placeholder(tf.float32, shape=[], name='m')
            with tf.variable_scope('CNN', reuse=tf.AUTO_REUSE):
                loss, train_op, global_step = build_training_pi_graph(train_x, train_y, ul_x, lr, unsupervised_weight, m)

            init_op = tf.global_variables_initializer()

        with tf.Session().as_default() as sess:
            sess.run(init_op)
            for ep in range(num_epochs):

                sum_loss = 0

                feed_dict = {lr: 0.001, unsupervised_weight: 0.5, m: 0.5}

                start = time.time()
                for i in range(num_iter_per_epoch):
                    batch_loss, _, _ = sess.run([loss, train_op, global_step], feed_dict=feed_dict)
                    sum_loss += batch_loss
                end = time.time()
                print('epoch:', ep, 'epoch train loss:', sum_loss / num_iter_per_epoch, 'epoch train time:',
                      end - start)

main()`

**Other info / logs**
Traceback (most recent call last):
  File ""/home/zw/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1292, in _do_call
    return fn(*args)
  File ""/home/zw/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1277, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/zw/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1367, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'm' with dtype float
	 [[{{node m}} = Placeholder[dtype=DT_FLOAT, shape=[], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/zw/PycharmProjects/self/tempens-self/piModel.py"", line 172, in <module>
    main()
  File ""/home/zw/PycharmProjects/self/tempens-self/piModel.py"", line 157, in main
    sess.run(init_op)
  File ""/home/zw/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 887, in run
    run_metadata_ptr)
  File ""/home/zw/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1110, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/zw/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run
    run_metadata)
  File ""/home/zw/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'm' with dtype float
	 [[{{node m}} = Placeholder[dtype=DT_FLOAT, shape=[], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

Caused by op 'm', defined at:
  File ""/home/zw/PycharmProjects/self/tempens-self/piModel.py"", line 172, in <module>
    main()
  File ""/home/zw/PycharmProjects/self/tempens-self/piModel.py"", line 146, in main
    m = tf.placeholder(tf.float32, shape=[], name='m')
  File ""/home/zw/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 1745, in placeholder
    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)
  File ""/home/zw/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 5020, in placeholder
    ""Placeholder"", dtype=dtype, shape=shape, name=name)
  File ""/home/zw/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/zw/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/home/zw/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op
    op_def=op_def)
  File ""/home/zw/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'm' with dtype float
	 [[{{node m}} = Placeholder[dtype=DT_FLOAT, shape=[], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

any help, thanks"
23420,bazel build on Windows with mkl failed,"I'm trying to compile tensorflow with MKL support on Windows 10 64bit via Bazel with command:
`bazel build --config=mkl --config=opt //tensorflow/tools/pip_package:build_pip_package`
on the following environments:
- Windows 10 64 bit
- Tensorflow 1.12
- Python 3.6.6
- Bazel 0.18

On the same setup, I can successfully build it without --config=mkl term,  but when building with mkl it failed as following:
who can tell me if it has anything related to mkl?
```
ERROR: F:/tools/tf1.12/tensorflow/tensorflow/python/BUILD:3766:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed (Exit 1000): link.exe failed: error executing command
  cd C:/users/10267/_bazel_10267/udaytyio/execroot/org_tensorflow
  SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\LIB\amd64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.10240.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\lib\um\x64;C:\Program Files (x86)\Windows Kits\8.1\lib\winv6.3\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\WINDOWS\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\Tools;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Team Tools\Performance Tools;C:\Program Files (x86)\Windows Kits\8.1\bin\x64;C:\Program Files (x86)\Windows Kits\8.1\bin\x86;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;;C:\WINDOWS\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Program Files/Python36/python.exe
    SET PYTHON_LIB_PATH=C:/Program Files/Python36/lib/site-packages
    SET TEMP=C:\Users\10267\AppData\Local\Temp
    SET TF_DOWNLOAD_CLANG=0
    SET TF_NEED_CUDA=0
    SET TF_NEED_OPENCL_SYCL=0
    SET TMP=C:\Users\10267\AppData\Local\Temp
  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/link.exe /nologo /DLL /SUBSYSTEM:CONSOLE -defaultlib:advapi32.lib -DEFAULTLIB:advapi32.lib /MACHINE:X64 @bazel-out/x64_windows-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so-2.params /OPT:ICF /OPT:REF /DEF:bazel-out/x64_windows-opt/genfiles/tensorflow/python/pywrap_tensorflow_filtered_def_file.def /ignore:4070
LINK : warning LNK4044: unrecognized option '/lpthread'; ignored

bazel-out/x64_windows-opt/bin/external/nsync/nsync_cpp.lib : fatal error LNK1000: Internal error during CImplib::EmitThunk

  Version 14.00.24215.1

  ExceptionCode            = C0000005
  ExceptionFlags           = 00000000
  ExceptionAddress         = 00007FF7E0686896 (00007FF7E0670000) ""C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\bin\amd64\link.exe""
  NumberParameters         = 00000002
  ExceptionInformation[ 0] = 0000000000000000
  ExceptionInformation[ 1] = 0000000000000008

CONTEXT:
  Rax    = 0000000000000000  R8     = 00007FF7E076FBE0
  Rbx    = 0000000000000000  R9     = 00007FF7E076E9F0
  Rcx    = 0000000000000000  R10    = 0000000000000000
  Rdx    = 00007FF7E076FBD8  R11    = 0000000000000000
  Rsp    = 000000082B71DE68  R12    = 00007FF7E073D950
  Rbp    = 000001A60F8F8360  E13    = 0000000000000000
  Rsi    = 0000000000008000  R14    = 0000000000000000
  Rdi    = 000001A6102AA300  R15    = 0000000000000000
  Rip    = 00007FF7E0686896  EFlags = 0000000000010246
  SegCs  = 0000000000000033  SegDs  = 000000000000002B
  SegSs  = 000000000000002B  SegEs  = 000000000000002B
  SegFs  = 0000000000000053  SegGs  = 000000000000002B
  Dr0    = 0000000000000000  Dr3    = 0000000000000000
  Dr1    = 0000000000000000  Dr6    = 0000000000000000
  Dr2    = 0000000000000000  Dr7    = 0000000000000000
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 3172.917s, Critical Path: 253.76s
INFO: 3875 processes: 3875 local.
FAILED: Build did NOT complete successfully
```





"
23419,install adanet model got wrong ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23418,im wrong,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23417,"WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.","**System information**
- Linux Ubuntu 16.04
- TensorFlow installed from  binary:
- TensorFlow version :1.11
- Python version :3.5
- CUDA/cuDNN version:9.0/7.1
- GPU model and memory:GeForce GTX TITAN X 

**Describe the current behavior**
(tfzkl) ddc@:~/zklcode/leapmotion$ cd /home/zklcode/leapmotion ; env ""PYTHONIOENCODING=UTF-8"" ""PYTHONUNBUFFERED=1"" /home/anaconda3/envs/tfzkl/bin/python3.5 /home/ddc/.vscode/extensions/ms-python.python-2018.9.1/pythonFiles/experimental/ptvsd_launcher.py 41383 /home/zklcode/leapmotion/leap/leaptf/tfleap.train.py
dataset shape:(100, 2)
Backend TkAgg is interactive backend. Turning interactive mode on.
WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.
WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.

my code don`t call tf.shape() function, why show this WARNING? anyone have this issue?
"
23416,Error in compile  tensorflowr1.8 on windows by vs2015 (Error: LNK1189 ：More than 65535 object restrictions),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Windows 10):
- TensorFlow installed from (source or binary):source
- TensorFlow version:1.8
- Python version:3.6
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:9.0/7
- GPU model and memory:2



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23413,Models cannot be loaded,"### System information
- **What is the top-level directory of the model you are using**:  ~/
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, written a simple MWE script
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04.1
- **TensorFlow installed from (source or binary)**: Compiled from source
- **TensorFlow version (use command below)**: 1.11.0
- **Python version**: 3.6.6
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version**: 7.3.0
- **CUDA/cuDNN version**: Cuda: 9.2.148, cuDNN: 9.2-v7.3.1.20 
- **GPU model and memory**: GeForce GTX1080Ti, 11GB
- **Exact command to reproduce**:

Minimal working example code

```
from tensorflow.keras.layers import Conv2D, MaxPooling2D, ReLU, Flatten, Dense, Input
from tensorflow.keras.models import Model
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model


def cnn_model(rows, cols, channels):
    model = Model()
    inputs = Input(shape=(rows, cols, channels), dtype='float32')

    conv1 = Conv2D(64, (3, 3),activation='linear',kernel_initializer='he_uniform')(inputs)
    relu1 = ReLU()(conv1)
    pooling1 = MaxPooling2D(pool_size=(5, 5))(relu1)

    flatten = Flatten()(pooling1)

    dense1 = Dense(512)(flatten)

    relu2 = ReLU()(dense1)
    predictions = Dense(4, activation='softmax')(relu2)

    model = Model(inputs=inputs, outputs=predictions)

    return model


batch_size = 16
rows = 128
cols = 128
channels = 1
model_input = np.random.randint(0, 255, (batch_size, rows, cols, channels))
model_labels = np.random.randint(0, 1, (batch_size, 4))

model = cnn_model(rows, cols,channels)

adam = tf.train.AdamOptimizer(learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8)

model.compile(loss='categorical_crossentropy',
              optimizer=adam,
              metrics=['categorical_accuracy'])
model.fit(x=model_input,
          y=model_labels,
          epochs=2)

model.save(""/home/svdvoort/test_model.hdf5"")
load_model(""/home/svdvoort/test_model.hdf5"")
```

### Describe the problem

Returns the following output (error included):

```
Epoch 1/2
2018-10-31 19:09:12.162738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683
pciBusID: 0000:01:00.0
totalMemory: 10.91GiB freeMemory: 10.76GiB
2018-10-31 19:09:12.290346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 1 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683
pciBusID: 0000:02:00.0
totalMemory: 10.92GiB freeMemory: 10.76GiB
2018-10-31 19:09:12.291317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0, 1
2018-10-31 19:09:12.750668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-10-31 19:09:12.750702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 1 
2018-10-31 19:09:12.750707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N Y 
2018-10-31 19:09:12.750710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 1:   Y N 
2018-10-31 19:09:12.751100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10404 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-10-31 19:09:12.855072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10407 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)
16/16 [==============================] - 3s 201ms/step - loss: 0.0000e+00 - categorical_accuracy: 0.0000e+00
Epoch 2/2
16/16 [==============================] - 0s 1ms/step - loss: 0.0000e+00 - categorical_accuracy: 0.0000e+00
WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).
Traceback (most recent call last):
  File ""Loading_error_example.py"", line 47, in <module>
    load_model(""/home/svdvoort/test_model.hdf5"")
  File ""/packages/tensorflow/1.11.0/Python-3.6.6/tensorflow/python/keras/engine/saving.py"", line 230, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File ""/packages/tensorflow/1.11.0/Python-3.6.6/tensorflow/python/keras/engine/saving.py"", line 310, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File ""/packages/tensorflow/1.11.0/Python-3.6.6/tensorflow/python/keras/layers/serialization.py"", line 64, in deserialize
    printable_module_name='layer')
  File ""/packages/tensorflow/1.11.0/Python-3.6.6/tensorflow/python/keras/utils/generic_utils.py"", line 173, in deserialize_keras_object
    list(custom_objects.items())))
  File ""/packages/tensorflow/1.11.0/Python-3.6.6/tensorflow/python/keras/engine/network.py"", line 1292, in from_config
    process_layer(layer_data)
  File ""/packages/tensorflow/1.11.0/Python-3.6.6/tensorflow/python/keras/engine/network.py"", line 1278, in process_layer
    layer = deserialize_layer(layer_data, custom_objects=custom_objects)
  File ""/packages/tensorflow/1.11.0/Python-3.6.6/tensorflow/python/keras/layers/serialization.py"", line 64, in deserialize
    printable_module_name='layer')
  File ""/packages/tensorflow/1.11.0/Python-3.6.6/tensorflow/python/keras/utils/generic_utils.py"", line 175, in deserialize_keras_object
    return cls.from_config(config['config'])
  File ""/packages/tensorflow/1.11.0/Python-3.6.6/tensorflow/python/keras/engine/base_layer.py"", line 1617, in from_config
    return cls(**config)
  File ""/packages/tensorflow/1.11.0/Python-3.6.6/tensorflow/python/keras/layers/advanced_activations.py"", line 310, in __init__
    if max_value is not None and max_value < 0.:
TypeError: '<' not supported between instances of 'dict' and 'float'
```
Same is true for all other models that are being trained



### Source code / logs
Output of tf_env attached
[tf_env.txt](https://github.com/tensorflow/models/files/2535750/tf_env.txt)

"
23412,Unable to import tensorflow  after successful installation ,">>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\cmwas\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\cmwas\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\cmwas\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\cmwas\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\cmwas\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\cmwas\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\cmwas\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\cmwas\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\cmwas\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\cmwas\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\cmwas\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\cmwas\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\cmwas\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
23411,"Couldn't find field google.protobuf.FileOptions.javanano_use_deprecated_package""","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubunto 16.04

- TensorFlow version: 1.4.0
- Python version:2.7.1
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):



whenever i run a command such as : python pythonFile.py
i got this error : 
Traceback (most recent call last):
  File ""111.py"", line 1, in <module>
    import tensorflow as tf
  File ""/home/lenovo/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/lenovo/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 54, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""/home/lenovo/.local/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 10, in <module>
    from google.protobuf import descriptor_pb2
  File ""/home/lenovo/.local/lib/python2.7/site-packages/google/protobuf/descriptor_pb2.py"", line 964, in <module>
    options=None),
  File ""/home/lenovo/.local/lib/python2.7/site-packages/google/protobuf/descriptor.py"", line 505, in __new__
    return _message.default_pool.FindFieldByName(full_name)
KeyError: ""Couldn't find field google.protobuf.FileOptions.javanano_use_deprecated_package""


i tried all solutions such as uninstalling and installing protobuf==3.5.0.post1  , but nothing helped , any ideas ?
"
23410,UnboundLocalError: local variable 'a' referenced before assignment,"hi there, i have install phyton 3.7 with the following build : v1.11.0-rc2-4-gc19e29306c 1.11.0

here the phyton sample code: 

keras_file = ""keras_model.h5""
tf.keras.models.save_model(model, keras_file)

# Convert to TensorFlow Lite model.
converter = tf.contrib.lite.TocoConverter.from_keras_model_file(keras_file)
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)

when it try to execute : converter = tf.contrib.lite.TocoConverter.from_keras_model_file(keras_file)

and throw the following error:

converter = tf.contrib.lite.TocoConverter.from_keras_model_file(keras_file)
  File ""/Users/htan/venv/lib/python3.7/site-packages/tensorflow/contrib/lite/python/lite.py"", line 354, in from_keras_model_file
    _keras.backend.clear_session()
  File ""/Users/htan/venv/lib/python3.7/site-packages/tensorflow/python/keras/backend.py"", line 335, in clear_session
    False, shape=(), name='keras_learning_phase')
  File ""/Users/htan/venv/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 5148, in placeholder_with_default
    ""PlaceholderWithDefault"", input=input, shape=shape, name=name)
  File ""/Users/htan/venv/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 510, in _apply_op_helper
    preferred_dtype=default_dtype)
  File ""/Users/htan/venv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 1144, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/Users/htan/venv/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 228, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/Users/htan/venv/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 207, in constant
    value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/Users/htan/venv/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py"", line 542, in make_tensor_proto
    append_fn(tensor_proto, proto_values)
  File ""tensorflow/python/framework/fast_tensor_util.pyx"", line 134, in tensorflow.python.framework.fast_tensor_util.AppendBoolArrayToTensorProto
  File ""/Users/htan/venv/lib/python3.7/site-packages/numpy/lib/type_check.py"", line 489, in asscalar
    return a.item()
UnboundLocalError: local variable 'a' referenced before assignment"
23407,"Tensorflow eager version fails, while Tensorflow static graph works","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Archlinux
- TensorFlow installed from (source or binary): repository
- TensorFlow version (use command below): 1.11
- Python version: 3.7
- CUDA/cuDNN version: cuda 10, cudnn 7
- GPU model and memory: nvidia 1080ti

**Describe the current behavior**

I'm porting a ML model ( https://github.com/samet-akcay/ganomaly ) implemented in pytorch to tensorflow (using the keras layers, knowing that tf 2.0 will come soon).

The first implementation was using the eager version to do the train, but the model collapses, nothing works.

The same model definition has been reused but it has been used to first define a static graph and then train the model: it works perfectly.

**Describe the expected behavior**

The static graph version and the eager version should have the same behavior.

**Code to reproduce the issue**

### Model description (same for both eager and static)

```python
from typing import Dict
import tensorflow as tf
import tensorflow.keras as k
import numpy as np

conv_initializer = k.initializers.random_normal(0.0, 0.02)
batchnorm_inizializer = k.initializers.random_normal(1.0, 0.02)

eps = 1e-5
momentum = 0.99


class Decoder(k.models.Model):
    """"""
    Decoder (Generator) Network
    """"""

    def __init__(self, output_depth: int = 1):
        super(Decoder, self).__init__()

        self.conv1 = k.layers.Conv2DTranspose(
            filters=256,
            kernel_size=(4, 4),
            strides=(1, 1),
            kernel_initializer=conv_initializer,
            input_shape=(-1, 1, 1, 100),
            use_bias=False,
        )
        self.batchnorm1 = k.layers.BatchNormalization(
            epsilon=eps,
            momentum=momentum,
            beta_initializer=batchnorm_inizializer,
            gamma_initializer=batchnorm_inizializer,
        )

        self.conv2 = k.layers.Conv2DTranspose(
            filters=128,
            kernel_size=(4, 4),
            strides=(2, 2),
            padding=""same"",
            kernel_initializer=conv_initializer,
            use_bias=False,
        )
        self.batchnorm2 = k.layers.BatchNormalization(
            epsilon=eps,
            momentum=momentum,
            beta_initializer=batchnorm_inizializer,
            gamma_initializer=batchnorm_inizializer,
        )

        self.conv3 = k.layers.Conv2DTranspose(
            filters=64,
            kernel_size=(4, 4),
            strides=(2, 2),
            padding=""same"",
            kernel_initializer=conv_initializer,
            use_bias=False,
        )
        self.batchnorm3 = k.layers.BatchNormalization(
            epsilon=eps,
            momentum=momentum,
            beta_initializer=batchnorm_inizializer,
            gamma_initializer=batchnorm_inizializer,
        )

        self.conv4 = k.layers.Conv2DTranspose(
            filters=output_depth,
            kernel_size=(4, 4),
            strides=(2, 2),
            padding=""same"",
            kernel_initializer=conv_initializer,
            use_bias=False,
        )

    def call(self, x, training=True):
        # print(""X.SHAPE: "", x.shape)

        x = self.conv1(x)
        x = self.batchnorm1(x, training=training)
        x = tf.nn.relu(x)

        x = self.conv2(x)
        x = self.batchnorm2(x, training=training)
        x = tf.nn.relu(x)

        x = self.conv3(x)
        x = self.batchnorm3(x, training=training)
        x = tf.nn.relu(x)

        x = self.conv4(x)
        x = tf.nn.tanh(x)  # image

        # print(""Decoder call output size: "", x.shape)

        return x


class Encoder(k.models.Model):

    def __init__(self, latent_dimensions: int = 100):
        super(Encoder, self).__init__()

        self.conv0 = k.layers.Conv2D(
            filters=64,
            kernel_size=(4, 4),
            strides=(2, 2),
            padding=""same"",
            kernel_initializer=conv_initializer,
            input_shape=(-1, 32, 32, 1),
            use_bias=False,
        )

        self.conv1 = k.layers.Conv2D(
            filters=128,
            kernel_size=(4, 4),
            strides=(2, 2),
            padding=""same"",
            kernel_initializer=conv_initializer,
            use_bias=False,
        )
        self.batchnorm1 = k.layers.BatchNormalization(
            epsilon=eps,
            momentum=momentum,
            beta_initializer=batchnorm_inizializer,
            gamma_initializer=batchnorm_inizializer,
        )

        self.conv2 = k.layers.Conv2D(
            filters=256,
            kernel_size=(4, 4),
            strides=(2, 2),
            padding=""same"",
            kernel_initializer=conv_initializer,
            use_bias=False,
        )
        self.batchnorm2 = k.layers.BatchNormalization(
            epsilon=eps,
            momentum=momentum,
            beta_initializer=batchnorm_inizializer,
            gamma_initializer=batchnorm_inizializer,
        )

        self.conv3 = k.layers.Conv2D(
            filters=latent_dimensions,
            kernel_size=(4, 4),
            strides=(1, 1),
            padding=""valid"",
            kernel_initializer=conv_initializer,
            use_bias=False,
        )

    def call(self, x, training=True):
        # x = self.conv0(x, input_shape=x.shape[1:])

        x = self.conv0(x)
        x = tf.nn.leaky_relu(x)

        x = self.conv1(x)
        x = self.batchnorm1(x, training=training)
        x = tf.nn.leaky_relu(x)

        x = self.conv2(x)
        x = self.batchnorm2(x, training=training)
        x = tf.nn.leaky_relu(x)

        x = self.conv3(x)
        # x = tf.nn.tanh(x)       # latent space unitary sphere [-1,1] TODO: temporary?

        # print(""Encoder call output size: "", x.shape)

        return x


class Discriminator(k.models.Model):

    def __init__(self):
        super(Discriminator, self).__init__()

        self.conv0 = k.layers.Conv2D(
            filters=64,
            kernel_size=(4, 4),
            strides=(2, 2),
            padding=""same"",
            kernel_initializer=conv_initializer,
            use_bias=False,
        )

        self.conv1 = k.layers.Conv2D(
            filters=128,
            kernel_size=(4, 4),
            strides=(2, 2),
            padding=""same"",
            kernel_initializer=conv_initializer,
            use_bias=False,
        )
        self.batchnorm1 = k.layers.BatchNormalization(
            epsilon=eps,
            momentum=momentum,
            beta_initializer=batchnorm_inizializer,
            gamma_initializer=batchnorm_inizializer,
        )

        self.conv2 = k.layers.Conv2D(
            filters=256,
            kernel_size=(4, 4),
            strides=(2, 2),
            padding=""same"",
            kernel_initializer=conv_initializer,
            use_bias=False,
        )
        self.batchnorm2 = k.layers.BatchNormalization(
            epsilon=eps,
            momentum=momentum,
            beta_initializer=batchnorm_inizializer,
            gamma_initializer=batchnorm_inizializer,
        )

        self.conv3 = k.layers.Conv2D(
            filters=1,
            kernel_size=(4, 4),
            strides=(1, 1),
            padding=""valid"",
            kernel_initializer=conv_initializer,
            use_bias=False,
        )

    def call(self, x, training=True):
        x = self.conv0(x)
        x = tf.nn.leaky_relu(x)

        x = self.conv1(x)
        x = self.batchnorm1(x, training=training)
        x = tf.nn.leaky_relu(x)

        x = self.conv2(x)
        x = self.batchnorm2(x, training=training)
        x = tf.nn.leaky_relu(x)

        x = self.conv3(x)

        return x
```

Also, the following variable definitions are shared in both the eager and static implementations

```python
            global_step = tf.train.get_or_create_global_step()
            generator_optimizer = tf.train.AdamOptimizer(2e-4, 0.5)
            discriminator_optimizer = tf.train.AdamOptimizer(2e-4, 0.5)
            discirminator = Discriminator()
            g_encoder = Encoder()
            g_decoder = Decoder()
            encoder = Encoder()
```

### Eager training

I show just a single training update, that's run in a training loop
```python
            # Discriminator training
            with tf.GradientTape() as tape:

                discriminator.trainable = True
                disc_x = tf.squeeze(discriminator(x, training=False), axis=[1, 2])

                disc_real_loss = tf.losses.sigmoid_cross_entropy(  # discriminator loss on result disc_x
                    multi_class_labels=tf.ones_like(disc_x), logits=disc_x
                )
                g_encoder.trainable = False
                g_decoder.trainable = False
                # recreate the data (=> x_hat), starting from real data x
                z = g_encoder(x, training=True)  # Not training
                x_hat = g_decoder(z, training=True)  # Not training

                disc_x_hat = tf.squeeze(discriminator(x_hat, training=False), axis=[1, 2])
                disc_gen_loss = tf.losses.sigmoid_cross_entropy(  # discriminator loss on result disc_x_hat
                    multi_class_labels=tf.zeros_like(disc_x_hat), logits=disc_x_hat
                )
                disc_loss = disc_real_loss + disc_gen_loss

            discriminator_gradients = tape.gradient(
                disc_loss, discriminator.trainable_variables
            )

            discriminator_optimizer.apply_gradients(
                zip(discriminator_gradients, discriminator.trainable_variables)
            )

            # Generator Training
            with tf.GradientTape() as tape:

                # err_g_bce
                g_encoder.trainable = True
                g_decoder.trainable = True
                encoder.trainable = True
                z = g_encoder(x, training=True)
                x_hat = g_decoder(z, training=True)
                disc_x_hat = tf.squeeze(
                    discriminator(x_hat, training=False), axis=[1, 2]
                ) 
                bce_loss = tf.losses.sigmoid_cross_entropy(
                    multi_class_labels=tf.ones_like(disc_x_hat),
                    logits=disc_x_hat,  # G wants to generate reals so ones_like
                )

                l1_loss = tf.losses.absolute_difference(x, x_hat)
                # err_g_enc
                z_hat = encoder(x_hat, training=True)
                l2_loss = tf.losses.mean_squared_error(z, z_hat)

                gen_loss = 1* bce_loss + 50 * l1_loss + 1 * l2_loss
                
            trainable_variable_list = (
                g_encoder.trainable_variables
                + g_decoder.trainable_variables
                + encoder.trainable_variables
            )

            generator_gradients = tape.gradient(gen_loss, trainable_variable_list)

            generator_optimizer.apply_gradients(
                zip(generator_gradients, trainable_variable_list),
                global_step=global_step,
            )
```

### Static graph training
First I show the graph definition,then how is used in the training loop

#### Graph def
```python

    # Discriminator on real
    D_x = discriminator(x)
    D_x = tf.squeeze(D_x, axis=[1, 2])

    # Generate fake
    z = g_encoder(x)
    x_hat = g_decoder(z)

    D_x_hat = discriminator(x_hat)
    D_x_hat = tf.squeeze(D_x_hat, axis=[1, 2])
    ## Discriminator
    d_loss = tf.losses.sigmoid_cross_entropy( 
                    multi_class_labels=tf.ones_like(D_x), logits=D_x) + tf.losses.sigmoid_cross_entropy(
                    multi_class_labels=tf.zeros_like(D_x_hat), logits=D_x_hat)

    # encode x_hat to z_hat
    z_hat = encoder(x_hat)

    ## Generator
    bce_loss = tf.losses.sigmoid_cross_entropy(tf.ones_like(D_x_hat), D_x_hat)
    l1_loss = tf.losses.absolute_difference(x, x_hat)
    l2_loss = tf.losses.mean_squared_error(z, z_hat)

    g_loss = 1 * bce_loss + 1 * l1_loss + 1 * l2_loss
    global_step = tf.train.get_or_create_global_step()

    # Define the D train op
    train_d = tf.train.AdamOptimizer(
        lr, beta1=0.5).minimize(
            d_loss, var_list=D.trainable_variables)

    # train G_e G_d E
    train_g = tf.train.AdamOptimizer(
        lr, beta1=0.5).minimize(
            g_loss,
            global_step=global_step,
            var_list=g_encoder.trainable_variables + g_decoder.trainable_variables +
            encoder.trainable_variables)
```

And this is what's inside the training loop (executed in a MonitoredSession):

```python
            # extract from tf.data.Dataset, x is a placeholder and x_ is the iterator.get_next
            real = sess.run(x_) 
            feed_dict = {x: real}

            # train D
            _, d_loss_value = sess.run([train_d, d_loss], feed_dict)

            # train G+E
            _, g_loss_value, step = sess.run([train_g, g_loss, global_step],
                                             feed_dict)
````

The model definition is the same, the loss definition is the same, the training steps and the same, the only difference is the eager mode enabled or disabled. The results with eager on are:

D loss: collapses to zero, that's wrong since its aim is to stay around 0.5
![bad d](https://user-images.githubusercontent.com/8427788/47802503-b2a31100-dd30-11e8-8d53-ef024f30879d.png)

Generated images: wrong, bad reconstructions since D collapsed
![bad_gen](https://user-images.githubusercontent.com/8427788/47802519-bafb4c00-dd30-11e8-9a70-16b2c27d69b7.png)

While when eager is off, the discriminator loss looks correct and the generated output are the one expected:

D loss:
![good d](https://user-images.githubusercontent.com/8427788/47802443-8e473480-dd30-11e8-8525-38ec50040a16.png)

Generated output:
![good_gen](https://user-images.githubusercontent.com/8427788/47802455-930be880-dd30-11e8-85c3-efdd87311530.png)

"
23406,TPUEstimator cant save checkpoint at google colab,"Error recorded from training_loop: File system scheme '[local]' not implemented (file: '/tmp/tmpu0amtet8/model.ckpt-0_temp_b2eedcafbb1e42d1949c6e0ec2f85501')
	 [[node save/SaveV2 (defined at /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py:2403)  = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:worker/replica:0/task:0/device:CPU:0""](save/ShardedFilename, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, ReadVariables_2242190548333892376/_1:2, ReadVariables_2242190548333892376/_1, ReadVariables_2242190548333892376/_1:1, ReadVariables_2242190548333892376/_1:5, ReadVariables_2242190548333892376/_1:3, ReadVariables_2242190548333892376/_1:4, ReadVariables_2242190548333892376/_1:8, ReadVariables_2242190548333892376/_1:6, ReadVariables_2242190548333892376/_1:7, ReadVariables_2242190548333892376/_1:11, ReadVariables_2242190548333892376/_1:9, ReadVariables_2242190548333892376/_1:10, ReadVariables_2242190548333892376/_1:14, ReadVariables_2242190548333892376/_1:12, ReadVariables_2242190548333892376/_1:13, ReadVariables_2242190548333892376/_1:17, ReadVariables_2242190548333892376/_1:15, ReadVariables_2242190548333892376/_1:16, ReadVariables_2242190548333892376/_1:20, ReadVariables_2242190548333892376/_1:18, ReadVariables_2242190548333892376/_1:19, ReadVariables_2242190548333892376/_1:23, ReadVariables_2242190548333892376/_1:21, ReadVariables_2242190548333892376/_1:22, ReadVariables_2242190548333892376/_1:26, ReadVariables_2242190548333892376/_1:24, ReadVariables_2242190548333892376/_1:25, ReadVariables_2242190548333892376/_1:29, ReadVariables_2242190548333892376/_1:27, ReadVariables_2242190548333892376/_1:28, ReadVariables_2242190548333892376/_1:32, ReadVariables_2242190548333892376/_1:30, ReadVariables_2242190548333892376/_1:31, ReadVariables_2242190548333892376/_1:35, ReadVariables_2242190548333892376/_1:33, ReadVariables_2242190548333892376/_1:34, ReadVariables_2242190548333892376/_1:38, ReadVariables_2242190548333892376/_1:36, ReadVariables_2242190548333892376/_1:37, ReadVariables_2242190548333892376/_1:41, ReadVariables_2242190548333892376/_1:39, ReadVariables_2242190548333892376/_1:40, ReadVariables_2242190548333892376/_1:44, ReadVariables_2242190548333892376/_1:42, ReadVariables_2242190548333892376/_1:43, ReadVariables_2242190548333892376/_1:47, ReadVariables_2242190548333892376/_1:45, ReadVariables_2242190548333892376/_1:46, ReadVariables_2242190548333892376/_1:50, ReadVariables_2242190548333892376/_1:48, ReadVariables_2242190548333892376/_1:49, ReadVariables_2242190548333892376/_1:53, ReadVariables_2242190548333892376/_1:51, ReadVariables_2242190548333892376/_1:52, ReadVariables_2242190548333892376/_1:56, ReadVariables_2242190548333892376/_1:54, ReadVariables_2242190548333892376/_1:55, ReadVariables_2242190548333892376/_1:59, ReadVariables_2242190548333892376/_1:57, ReadVariables_2242190548333892376/_1:58, ReadVariables_2242190548333892376/_1:62, ReadVariables_2242190548333892376/_1:60, ReadVariables_2242190548333892376/_1:61, ReadVariables_2242190548333892376/_1:65, ReadVariables_2242190548333892376/_1:63, ReadVariables_2242190548333892376/_1:64, ReadVariables_2242190548333892376/_1:68, ReadVariables_2242190548333892376/_1:66, ReadVariables_2242190548333892376/_1:67, ReadVariables_2242190548333892376/_1:71, ReadVariables_2242190548333892376/_1:69, ReadVariables_2242190548333892376/_1:70, ReadVariables_2242190548333892376/_1:74, ReadVariables_2242190548333892376/_1:72, ReadVariables_2242190548333892376/_1:73, ReadVariables_2242190548333892376/_1:77, ReadVariables_2242190548333892376/_1:75, ReadVariables_2242190548333892376/_1:76, ReadVariables_2242190548333892376/_1:80, ReadVariables_2242190548333892376/_1:78, ReadVariables_2242190548333892376/_1:79, ReadVariables_2242190548333892376/_1:83, ReadVariables_2242190548333892376/_1:81, ReadVariables_2242190548333892376/_1:82, ReadVariables_2242190548333892376/_1:86, ReadVariables_2242190548333892376/_1:84, ReadVariables_2242190548333892376/_1:85, ReadVariables_2242190548333892376/_1:89, ReadVariables_2242190548333892376/_1:87, ReadVariables_2242190548333892376/_1:88, ReadVariables_2242190548333892376/_1:92, ReadVariables_2242190548333892376/_1:90, ReadVariables_2242190548333892376/_1:91, ReadVariables_2242190548333892376/_1:95, ReadVariables_2242190548333892376/_1:93, ReadVariables_2242190548333892376/_1:94, ReadVariables_2242190548333892376/_1:98, ReadVariables_2242190548333892376/_1:96, ReadVariables_2242190548333892376/_1:97, ReadVariables_2242190548333892376/_1:101, ReadVariables_2242190548333892376/_1:99, ReadVariables_2242190548333892376/_1:100, ReadVariables_2242190548333892376/_1:104, ReadVariables_2242190548333892376/_1:102, ReadVariables_2242190548333892376/_1:103, ReadVariables_2242190548333892376/_1:107, ReadVariables_2242190548333892376/_1:105, ReadVariables_2242190548333892376/_1:106, ReadVariables_2242190548333892376/_1:110, ReadVariables_2242190548333892376/_1:108, ReadVariables_2242190548333892376/_1:109, ReadVariables_2242190548333892376/_1:113, ReadVariables_2242190548333892376/_1:111, ReadVariables_2242190548333892376/_1:112, ReadVariables_2242190548333892376/_1:116, ReadVariables_2242190548333892376/_1:114, ReadVariables_2242190548333892376/_1:115, ReadVariables_2242190548333892376/_1:119, ReadVariables_2242190548333892376/_1:117, ReadVariables_2242190548333892376/_1:118, ReadVariables_2242190548333892376/_1:122, ReadVariables_2242190548333892376/_1:120, ReadVariables_2242190548333892376/_1:121, ReadVariables_2242190548333892376/_1:125, ReadVariables_2242190548333892376/_1:123, ReadVariables_2242190548333892376/_1:124, ReadVariables_2242190548333892376/_1:126, ReadVariables_2242190548333892376/_1:127, ReadVariables_2242190548333892376/_1:128, ReadVariables_2242190548333892376/_1:129, ReadVariables_2242190548333892376/_1:130)]]

Caused by op 'save/SaveV2', defined at:

Whole code:
https://colab.research.google.com/drive/101FjBAIMVuXyNyeUvq_Vfx-Z6CR3g4df?authuser=1#scrollTo=Sh3TpyAI8Oln

May be i do wrong something"
23405,"build with default cuda options fails with ""/bin/bash: CUDA_TOOLKIT_PATH: unbound variable""","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04.5  x86_64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): master branch from 10/31/18 - 
- Python version:
- Bazel version (if compiling from source): 0.18.0
- GCC/Compiler version (if compiling from source): 5.4.0 20160609
- CUDA/cuDNN version: 9.0/7
- GPU model and memory: 2x Tesla P100 16 GB

**Describe the current behavior**
Configuring TF to build with CUDA support and choosing all the default option for all the other questions in ./configure including the default for NCCL ""Please specify the locally installed NCCL version you want to use. [Default is to use https://github.com/nvidia/nccl]:"", the build fails with ""/bin/bash: CUDA_TOOLKIT_PATH: unbound variable""

This has been broken since this commit:
https://github.com/tensorflow/tensorflow/commit/fc6cd33c334f88759ce637e29e1586733076e094

Running in the docker container: nvidia/cuda:9.0-cudnn7-devel (for ease of setup)
export TF_NEED_CUDA=1
"""" | ./configure
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

failed with:
ERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/nccl_archive/BUILD.bazel:139:1: Executing genrule @nccl_archive//:device_code_fatbin_h failed (Exit 1)
/bin/bash: CUDA_TOOLKIT_PATH: unbound variable
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 267.057s, Critical Path: 93.66s
INFO: 3606 processes: 3606 local.
FAILED: Build did NOT complete successfully


**Describe the expected behavior**
Build should succeed. 

**Code to reproduce the issue**
export TF_NEED_CUDA=1
"""" | ./configure
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

**Other info / logs**
Fails on both x86 and ppc64le, tried with both bazel 0.15.0 and 0.18.0
"
23404,TFLite Android: My lite Model file does not loaded,"I'm having a problem with loading a lite model to the using the android tensorflow lite Interpreter.
I am trying to use the workflow of  Tensorflow-for-poets-2 TFLite tutorial, [https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#6](url)

instead of using the function:
```
private MappedByteBuffer loadModelFile(Activity activity,String MODEL_FILE) throws IOException {
    AssetFileDescriptor fileDescriptor = activity.getAssets().openFd(MODEL_FILE);
    FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());
    FileChannel fileChannel = inputStream.getChannel();
    long startOffset = fileDescriptor.getStartOffset();
    long declaredLength = fileDescriptor.getDeclaredLength();
    return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);
```
in order to load my model, I use: 

```
AssetFileDescriptor fileDescriptor = null;
try {
fileDescriptor = getAssets().openFd(MODEL_PATH);
} catch (IOException e) {
e.printStackTrace();
 }
FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());
FileChannel fileChannel = inputStream.getChannel();
long startOffset = fileDescriptor.getStartOffset();
long declaredLength = fileDescriptor.getDeclaredLength();
try {
MByteBuffer = fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);
} catch (IOException e) {
e.printStackTrace();
}

```
then, I use the tflite Interpreter by calling:
`tflite = new Interpreter(MByteBuffer);`

Using this snippet of an android code, I am able to laod the .lite graph that i got following the Tensorflow-for-poets-2 TFLite tutorial, [https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#6](url)

But when i try to load my .lite model I got the following error:

`A/libc: Fatal signal 6 (SIGABRT), code -6 (SI_TKILL) in tid 9320 (flitecamerademo), pid 9320 (flitecamerademo)`

I want to notify that my model works (preforms a style transfer). I test it using python API, in the following way:

```
tflite_graph_filename = 'debug_graph.lite' # this is my model
# Load TFLite model and allocate tensors.
interpreter = tf.contrib.lite.Interpreter(model_path=tflite_graph_filename)
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
print('input_details: \n{}\n'.format(input_details))
output_details = interpreter.get_output_details()
print('output_details: \n{}\n'.format(output_details))

# Test model on random input data.
input_shape = input_details[0]['shape']
print('input_shape: \n{}\n'.format(input_shape))

X = np.zeros(input_shape,np.float32)
X[0] = content_image

input_data = X # np.array(np.random.random_sample(input_shape), dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)

interpreter.invoke()
output_data = interpreter.get_tensor(output_details[0]['index'])

```
The above test is working so I assume my model is successfully converted to .lite format.

use the following command to toco convert:

`toco --graph_def_file=/fullpathto/debug_graph.pb  --output_file=/fullpathto/debug_graph.lite --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE --input_shape=1,474,712,3 --input_array=img_placeholder --output_array=transform/up-sample/mul --inference_type=FLOAT   --input_data_type=FLOAT`

Can you please help me with ideas on how to solve such problem or how can I debug this?

Thanks,
Vadim

"
23403,tf.MonitoredSession accessing raw session raw_session like in SingularMonitoredSession,"**System information**
- TensorFlow version (you are using): 1.11.0
- Are you willing to contribute it (Yes/No): yes

**Will this change the current api? How?**
not substantially

**Who will benefit with this feature?**
Potentially everybody using a MonitoredTrainingSession

**Any Other info.**
None

**Describe the feature and the current behavior/state.**

I am using tf.MonitoredSession, it happens sometimes that you want to evaluate some nodes but you don't want to proceed in the training step, so you do not want to use the Hooks to log stuffs. It seems that the session is wrapped several times and the raw session is stored in a nested way: https://github.com/tensorflow/tensorflow/issues/8425 but it can also be retrieved with method _tf_sess() https://github.com/tensorflow/tensorflow/issues/11971

To recap I think there are two possible ways to run without hooks (to the best of my knowledge):

1. use step_fn https://www.tensorflow.org/api_docs/python/tf/train/MonitoredSession#run_step_fn
2. access the raw session with raw_session = sess._tf_sess()

I think way 2. is the more intuitive for me if I want to be free from the hooks, but is it proper practice are there some problems in doing this? I wonder why is it private and it is not contemplated to have a method to access the raw session like in SingularMonitoredSession.raw_session()? https://www.tensorflow.org/api_docs/python/tf/train/SingularMonitoredSession 

At line 1010 in https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/python/training/monitored_session.py I can see that raw_session is defined exactly returning self._tf_sess(), so I wonder why the explicit choice to not have the user the possibility to access the raw session in MonitoredSession, is it possible to incur in some problems using sess._tf_sess()? I wonder if it has something to do with some problems in the distributed setting..

Summarizing: Would it be possible to have a method to access the raw session in MonitoredSession, are there some counter-indications with this kind of procedure?
(accessing the raw session with sess._tf_sess() )

"
23402,Failure to build TF 1.12 from source - multiple definitions in grpc,"**System information**
- Linux Ubuntu 16.04
- TensorFlow installed from source
- TensorFlow version: 1.12
- Python version: 2.7
- Bazel version (if compiling from source): 0.19
- GCC/Compiler version (if compiling from source): 5.4.0

**Describe the problem**
I've cloned tensorflow and checked out 1.12 branch
I've run configure and then build as instructed in the ""build from source"" page at TF website.
I've configured using all the default options and disabled XLA.
At the end of the build I get the following error:

src/main/tools/linux-sandbox.cc:204: child exited normally with exitcode 0
ERROR: /home/ogabbay/trees/tensorflow/tensorflow/python/BUILD:3865:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed (Exit 1) linux-sandbox failed: error executing command 
  (cd /home/ogabbay/.cache/bazel/_bazel_ogabbay/336c59c3c10f4f0f593d30a996172c97/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/home/ogabbay/builds/latest \
    PATH=/home/ogabbay/bin:/home/ogabbay/.local/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/snap/bin:/usr/local/installs/SW/eclipse/oxygen-M2/:/usr/lib/jvm/java-8-oracle/bin:/usr/lib/jvm/java-8-oracle/db/bin:/usr/lib/jvm/java-8-oracle/jre/bin \
    PWD=/proc/self/cwd \
    TMPDIR=/tmp \
  /home/ogabbay/.cache/bazel/_bazel_ogabbay/336c59c3c10f4f0f593d30a996172c97/execroot/org_tensorflow/_bin/linux-sandbox -t 15 -w /home/ogabbay/.cache/bazel/_bazel_ogabbay/336c59c3c10f4f0f593d30a996172c97/sandbox/linux-sandbox/1889/execroot/org_tensorflow -w /tmp -w /dev/shm -D -- /usr/bin/gcc -shared -o bazel-out/host/bin/tensorflow/python/_pywrap_tensorflow_internal.so -Wl,--version-script bazel-out/host/bin/tensorflow/python/pywrap_tensorflow_internal_versionscript.lds '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..' -Wl,-soname,_pywrap_tensorflow_internal.so -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread '-fuse-ld=gold' -Wl,-no-as-needed -Wl,-z,relro,-z,now -B/usr/bin -B/usr/bin -pass-exit-codes -Wl,--gc-sections -Wl,-S -Wl,@bazel-out/host/bin/tensorflow/python/_pywrap_tensorflow_internal.so-2.params): linux-sandbox failed: error executing command 
  (cd /home/ogabbay/.cache/bazel/_bazel_ogabbay/336c59c3c10f4f0f593d30a996172c97/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/home/ogabbay/builds/latest \
    PATH=/home/ogabbay/bin:/home/ogabbay/.local/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/snap/bin:/usr/local/installs/SW/eclipse/oxygen-M2/:/usr/lib/jvm/java-8-oracle/bin:/usr/lib/jvm/java-8-oracle/db/bin:/usr/lib/jvm/java-8-oracle/jre/bin \
    PWD=/proc/self/cwd \
    TMPDIR=/tmp \
  /home/ogabbay/.cache/bazel/_bazel_ogabbay/336c59c3c10f4f0f593d30a996172c97/execroot/org_tensorflow/_bin/linux-sandbox -t 15 -w /home/ogabbay/.cache/bazel/_bazel_ogabbay/336c59c3c10f4f0f593d30a996172c97/sandbox/linux-sandbox/1889/execroot/org_tensorflow -w /tmp -w /dev/shm -D -- /usr/bin/gcc -shared -o bazel-out/host/bin/tensorflow/python/_pywrap_tensorflow_internal.so -Wl,--version-script bazel-out/host/bin/tensorflow/python/pywrap_tensorflow_internal_versionscript.lds '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..' -Wl,-soname,_pywrap_tensorflow_internal.so -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread '-fuse-ld=gold' -Wl,-no-as-needed -Wl,-z,relro,-z,now -B/usr/bin -B/usr/bin -pass-exit-codes -Wl,--gc-sections -Wl,-S -Wl,@bazel-out/host/bin/tensorflow/python/_pywrap_tensorflow_internal.so-2.params)
src/main/tools/linux-sandbox.cc:154: linux-sandbox-pid1 has PID 26733
src/main/tools/linux-sandbox-pid1.cc:175: working dir: /home/ogabbay/.cache/bazel/_bazel_ogabbay/336c59c3c10f4f0f593d30a996172c97/sandbox/linux-sandbox/1889/execroot/org_tensorflow
src/main/tools/linux-sandbox-pid1.cc:194: writable: /home/ogabbay/.cache/bazel/_bazel_ogabbay/336c59c3c10f4f0f593d30a996172c97/sandbox/linux-sandbox/1889/execroot/org_tensorflow
src/main/tools/linux-sandbox-pid1.cc:194: writable: /tmp
src/main/tools/linux-sandbox-pid1.cc:194: writable: /dev/shm
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /dev
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /dev/pts
src/main/tools/linux-sandbox-pid1.cc:265: remount rw: /dev/shm
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /dev/mqueue
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /dev/hugepages
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /run
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /run/lock
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /run/rpc_pipefs
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /run/user/0
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /run/user/108
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /run/user/108/gvfs
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/kernel/security
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/fs/cgroup
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/fs/cgroup/systemd
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/fs/cgroup/perf_event
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/fs/cgroup/pids
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/fs/cgroup/freezer
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/fs/cgroup/devices
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/fs/cgroup/blkio
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/fs/cgroup/net_cls,net_prio
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/fs/cgroup/cpu,cpuacct
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/fs/cgroup/rdma
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/fs/cgroup/cpuset
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/fs/cgroup/memory
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/fs/cgroup/hugetlb
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/fs/pstore
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/kernel/config
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/fs/fuse/connections
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/kernel/debug
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /proc
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /proc/sys/fs/binfmt_misc
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /boot
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /software
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /vlsi/h3
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /vlsi/h2
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /tools/hl_shared_dir
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /usr/local/installs
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /usr/local/installs/SW
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /vovlogs
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /syntmp
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /utils
src/main/tools/linux-sandbox-pid1.cc:265: remount ro: /home/ogabbay/.gvfs
src/main/tools/linux-sandbox-pid1.cc:265: remount rw: /home/ogabbay/.cache/bazel/_bazel_ogabbay/336c59c3c10f4f0f593d30a996172c97/sandbox/linux-sandbox/1889/execroot/org_tensorflow
src/main/tools/linux-sandbox-pid1.cc:265: remount rw: /home/ogabbay/.cache/bazel/_bazel_ogabbay/336c59c3c10f4f0f593d30a996172c97/sandbox/linux-sandbox/1889/execroot/org_tensorflow
src/main/tools/linux-sandbox-pid1.cc:265: remount rw: /tmp
src/main/tools/linux-sandbox-pid1.cc:265: remount rw: /dev/shm
src/main/tools/process-tools.cc:118: sigaction(32, &sa, nullptr) failed
src/main/tools/process-tools.cc:118: sigaction(33, &sa, nullptr) failed
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_common.pic.o: multiple definition of 'pb_field_iter_begin'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_common.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_common.pic.o: multiple definition of 'pb_field_iter_next'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_common.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_common.pic.o: multiple definition of 'pb_field_iter_find'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_common.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_read'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_istream_from_buffer'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_decode_varint'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_decode_tag'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_skip_field'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_make_string_substream'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_close_string_substream'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_decode_noinit'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_decode'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_decode_delimited'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_decode_svarint'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_decode_fixed32'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_decode.pic.o: multiple definition of 'pb_decode_fixed64'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_decode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_ostream_from_buffer'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_write'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_encode_varint'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_encode_svarint'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_encode_fixed32'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_encode_fixed64'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_encode_tag'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_encode_tag_for_field'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_encode'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_get_encoded_size'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_encode_string'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_encode_submessage'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/host/bin/external/grpc/_objs/grpc_nanopb/pb_encode.pic.o: multiple definition of 'pb_encode_delimited'
/usr/bin/ld.gold: bazel-out/host/bin/external/grpc/third_party/nanopb/_objs/nanopb/pb_encode.pic.o: previous definition here
collect2: error: ld returned 1 exit status
src/main/tools/linux-sandbox-pid1.cc:437: waitpid returned 2
src/main/tools/linux-sandbox-pid1.cc:457: child exited with code 1
src/main/tools/linux-sandbox.cc:204: child exited normally with exitcode 1
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 1603.699s, Critical Path: 114.71s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]
INFO: 1907 processes: 1905 linux-sandbox, 2 local.
FAILED: Build did NOT complete successfully
"
23401,Build from source -> build the pip package -> GPU support -> bazel build -> ERROR: Config value cuda is not defined in any .rc file,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Source 
- TensorFlow version: v1.12.0-rc2
- Python version: Python 3.7.0
- Installed using virtualenv? pip? conda?: conda 4.5.11 
- Bazel version (if compiling from source): 0.19.0
- GCC/Compiler version (if compiling from source): GCC 7.2.0
- CUDA/cuDNN version: 10.0 / 7.3.1
- GPU model and memory: nvidia GTX 1070 / 8GB



**Describe the problem**
Got error while building pip package from source
ERROR: Config value cuda is not defined in any .rc file

**Provide the exact sequence of commands / steps that you executed before running into the problem**
gopi@gp:~/tensorflow$ ./configure 
WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
/home/gopi/tensorflow/tools/bazel.rc
WARNING: Running Bazel server needs to be killed, because the startup options are different.
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.19.0 installed.
Please specify the location of python. [Default is /home/gopi/anaconda3/bin/python]: 


Found possible Python library paths:
  /home/gopi/anaconda3/lib/python3.7/site-packages
Please input the desired Python library path to use.  Default is [/home/gopi/anaconda3/lib/python3.7/site-packages]

Do you wish to build TensorFlow with Apache Ignite support? [Y/n]: n
No Apache Ignite support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [Y/n]: n
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: N
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: N
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: 10.0


Please specify the location where CUDA 10.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 


Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 


Do you wish to build TensorFlow with TensorRT support? [y/N]: N
No TensorRT support will be enabled for TensorFlow.

Please specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: 1.3


Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 6.1]: 


Do you want to use clang as CUDA compiler? [y/N]: N
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 


Do you wish to build TensorFlow with MPI support? [y/N]: N
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See tools/bazel.rc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=gdr         	# Build with GDR support.
	--config=verbs       	# Build with libverbs support.
	--config=ngraph      	# Build with Intel nGraph support.
Configuration finished
gopi@gp:~/tensorflow$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
/home/gopi/tensorflow/tools/bazel.rc
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=204
INFO: Reading rc options for 'build' from /home/gopi/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/gopi/anaconda3/bin/python --action_env PYTHON_LIB_PATH=/home/gopi/anaconda3/lib/python3.7/site-packages --python_path=/home/gopi/anaconda3/bin/python --action_env TF_NEED_OPENCL_SYCL=0 --action_env TF_NEED_ROCM=0 --action_env TF_NEED_CUDA=1 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda --action_env TF_CUDA_VERSION=10.0 --action_env CUDNN_INSTALL_PATH=/usr/local/cuda-10.0 --action_env TF_CUDNN_VERSION=7 --action_env TF_NCCL_VERSION=1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1 --action_env LD_LIBRARY_PATH=:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64 --action_env TF_CUDA_CLANG=0 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/gcc --config=cuda
ERROR: Config value cuda is not defined in any .rc file


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23399,TFLite failed to allocate tensors when changing the input size,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone 7 simulator
- TensorFlow installed from (source or binary): source 
- TensorFlow version (use command below): master
- Python version: 2.7
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source): None
- CUDA/cuDNN version: None
- GPU model and memory: None

**Describe the current behavior**
I convert the TF model to TFLite model by tflite_convert command with float inference_type, and input_shapes set to 1,W,H,3; when I run the model with input shape W*H, **it works well**; However, when I **change** the input shapes to different values, it would give error like ""tensorflow/contrib/lite/kernels/kernel_util.cc:125 d1 == d2 || d1 == 1 || d2 == 1 was not true; Node number xx (ADD) failed to prepare"" for tf.add op. Similar errors appear for other ops like CONCAT and RESHAPE. When I set breakpoints in tflite codes, it seems to always check/use the tensor shapes when converting the model, even the input shape has changed. 

Part of the codes are like this:

```
int input = interpreter->inputs()[0];
    interpreter->ResizeInputTensor(input, new_input_sizes);
    if (interpreter->AllocateTensors() != kTfLiteOk) {
        NSLog(@""Failed to allocate tensors for model."");
        exit(-1);
    }
```
"
23398,Cannot build libtensorflow.so with bazel 0.19,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Archlinux
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.12
- Python version: 3.7
- Bazel version (if compiling from source): Build label: 0.19.0- (@non-git)
- GCC/Compiler version (if compiling from source): gcc 8.2.1
- CUDA/cuDNN version: cuda 10, cudnn 7
- GPU model and memory: nvidia 1080 ti

**Describe the problem**

When I try to compile tensorflow from sources running:

```
go get -d github.com/tensorflow/tensorflow/tensorflow/go
cd ${GOPATH}/src/github.com/tensorflow/tensorflow
git checkout origin/r1.12
./configure
bazel build -c opt //tensorflow:libtensorflow.so
```
I got this warning from bazel:

```
WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
/data/pgaleone/go/src/github.com/tensorflow/tensorflow/tools/bazel.rc
```

and this error next:

```
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=271
INFO: Reading rc options for 'build' from /data/pgaleone/go/src/github.com/tensorflow/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python --action_env PYTHON_LIB_PATH=/usr/lib/python3.7/site-packages --python_path=/usr/bin/python --define with_ignite_support=true --define with_xla_support=true --action_env TF_NEED_OPENCL_SYCL=0 --action_env TF_NEED_ROCM=0 --action_env TF_NEED_CUDA=1 --action_env CUDA_TOOLKIT_PATH=/opt/cuda --action_env TF_CUDA_VERSION=10.0 --action_env CUDNN_INSTALL_PATH=/opt/cuda --action_env TF_CUDNN_VERSION=7 --action_env NCCL_INSTALL_PATH=/usr/lib --action_env NCCL_HDR_PATH=/usr/lib/../include --action_env TF_NCCL_VERSION=2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1 --action_env LD_LIBRARY_PATH=/opt/cuda/lib64:/opt/cuda/extras/CUPTI/lib64:/home/pgaleone/sources/ml/tensorflow/bazel-bin/tensorflow/ --action_env TF_CUDA_CLANG=0 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/gcc-7 --config=cuda
ERROR: Config value cuda is not defined in any .rc file
```
"
23397,parallel_for: No converter defined for MaxPoolGradGrad,"I have written a `fwd_gradients`-based implementation of `batch_jacobian` (comparable to the `tf.gradients`-based `tensorflow.python.ops.parallel_for.batch_jacobian`). To efficiently implement `batch_jacobian`, TensorFlow added support `parallel_for`: unfortunately, `pfor` does not support the `MaxPoolGradGrad` op, requiring a fallback to a slow while loop. For performance reasons, it would be great to add a converter for `MaxPoolGradGrad` to `parallel_for`.

**Code to reproduce the issue**
The following code is a minimal example to reproduce the issue and can be run as is.

```python
#!/usr/bin/env python3
import tensorflow as tf
from tensorflow.python.ops.parallel_for import control_flow_ops
from tensorflow.contrib.nn.python.ops.fwd_gradients import fwd_gradients
from tensorflow.python.platform import flags
import sys
import numpy as np


def main():
    sess = tf.InteractiveSession()

    print(tf.__version__)

    flags.FLAGS(sys.argv)

    x = tf.placeholder(tf.float32, (None, 2, 2, 1))
    y = tf.nn.max_pool(x, (1, 2, 2, 1), strides=(1, 1, 1, 1), padding='SAME')

    input_shape = tf.shape(x)
    batch_size = input_shape[0]

    input_size = 4
    one_hot_vectors = tf.eye(input_size)

    def loop_fn(i):
        one_hot = tf.gather(one_hot_vectors, i)
        one_hot = one_hot[tf.newaxis]
        grad_inputs = tf.tile(one_hot, (batch_size, 1))
        grad_inputs = tf.reshape(grad_inputs, input_shape)
        g = fwd_gradients([y], [x], [grad_inputs])
        assert len(g) == 1
        print(g[0])
        return g[0]

    J = control_flow_ops.pfor(loop_fn, 4)

    x_ = np.array([4, 1, 2, 3]).reshape(1, 2, 2, 1)
    y_, J_ = sess.run([y, J], feed_dict={x: x_})
    print(x_.squeeze())
    print(y_.squeeze())
    print(J_.squeeze())


if __name__ == '__main__':
    main()
```

**Describe the current behavior**
```
1.11.0
Tensor(""loop_body/gradients_1/loop_body/gradients/MaxPool_grad/MaxPoolGrad_grad/MaxPoolGradGrad:0"", shape=(?, 2, 2, 1), dtype=float32)
Traceback (most recent call last):
  File ""./minimal_example.py"", line 46, in <module>
    main()
  File ""./minimal_example.py"", line 36, in main
    J = control_flow_ops.pfor(loop_fn, 4)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py"", line 129, in pfor
    outputs.append(converter.convert(loop_fn_output))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/parallel_for/pfor.py"", line 1077, in convert
    output = self._convert_helper(y)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/parallel_for/pfor.py"", line 1223, in _convert_helper
    ""which may run slower"" % (y_op.type, y_op, converted_inputs))
ValueError: No converter defined for MaxPoolGradGrad
name: ""loop_body/gradients_1/loop_body/gradients/MaxPool_grad/MaxPoolGrad_grad/MaxPoolGradGrad""
op: ""MaxPoolGradGrad""
input: ""Placeholder""
input: ""MaxPool""
input: ""loop_body/gradients_1/grad_ys_0""
attr {
  key: ""T""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""data_format""
  value {
    s: ""NHWC""
  }
}
attr {
  key: ""ksize""
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
attr {
  key: ""padding""
  value {
    s: ""SAME""
  }
}
attr {
  key: ""strides""
  value {
    list {
      i: 1
      i: 1
      i: 1
      i: 1
    }
  }
}

inputs: [WrappedTensor(t=<tf.Tensor 'Placeholder:0' shape=(?, 2, 2, 1) dtype=float32>, is_stacked=False, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'MaxPool:0' shape=(?, 2, 2, 1) dtype=float32>, is_stacked=False, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'loop_body/gradients_1/grad_ys_0/pfor/Identity:0' shape=(4, ?, 2, 2, 1) dtype=float32>, is_stacked=True, is_sparse_stacked=False)]. 
Either add a converter or set --op_conversion_fallback_to_while_loop=True, which may run slower
```

**Describe the expected behavior**
(produced by running the above script with `--op_conversion_fallback_to_while_loop`)

```
1.11.0
Tensor(""loop_body/gradients_1/loop_body/gradients/MaxPool_grad/MaxPoolGrad_grad/MaxPoolGradGrad:0"", shape=(?, 2, 2, 1), dtype=float32)
WARNING:tensorflow:Using a while_loop for converting MaxPoolGradGrad
[[4 1]
 [2 3]]
[[4. 3.]
 [3. 3.]]
[[[1. 0.]
  [0. 0.]]

 [[0. 0.]
  [0. 0.]]

 [[0. 0.]
  [0. 0.]]

 [[0. 1.]
  [1. 1.]]]
```

P.S. If you are interested, I'd be happy to contribute my `fwd_gradients`-based `batch_jacobian` to TensorFlow once this performance issue with MaxPool ops is fixed and I've cleaned up my code."
23395,deploying the Tensorflow model in Python," While I am training everything is working fine but when I move on for a realtime forecast or prediction, the output what I received flunked. I do not know why is this happening. I used the reference of teh code from here: https://www.kaggle.com/raoulma/ny-stock-price-prediction-rnn-lstm-gru/notebook And tried to implement or deploy using the same code with few changes.

See the following code:
```
import numpy as np
import pandas as pd
import sklearn
import sklearn.preprocessing
import datetime
import os
import tensorflow as tf

df = pd.read_csv(""Realtime_Values.csv"", index_col = 0)
df.info()
def load_data(stock,seq_len):

    data_raw = stock.as_matrix() # convert to numpy array
    data = []

    for index in range(len(data_raw) - seq_len): 
        data.append(data_raw[index: index + seq_len])
    #print(len(data))
    data = np.array(data);

    x_forecast = data[:,:-1,:]
    return x_forecast

def normalize_data(df):
    cols = list(df.columns.values)
    min_max_scaler = sklearn.preprocessing.MinMaxScaler()
    df = pd.DataFrame(min_max_scaler.fit_transform(df.values))
    df.columns = cols
    return df
model_path =""modelsOHLC""
seq_len = 9
# parameters
n_steps = seq_len-1 
n_inputs = 4
n_neurons = 100 
n_outputs = 4
n_layers = 4
learning_rate = 0.01
batch_size = 10
n_epochs = 1000
tf.reset_default_graph()

X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])
y = tf.placeholder(tf.float32, [None, n_outputs])
layers = [tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.elu)
          for layer in range(n_layers)]
multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)
rnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)

stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons]) 
stacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)
outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])
outputs = outputs[:,n_steps-1,:] # keep only last output of sequence

loss = tf.reduce_mean(tf.square(outputs - y)) # loss function = mean squared error 
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) 
training_op = optimizer.minimize(loss)
saver = tf.train.Saver()
sess  =tf.Session()
sess.run(tf.global_variables_initializer())    
if(tf.train.checkpoint_exists(tf.train.latest_checkpoint(model_path))):
        saver.restore(sess, tf.train.latest_checkpoint(model_path))
df = normalize_data(df)
x_forecast = load_data(df,seq_len)
y_forecast_pred = sess.run(outputs, feed_dict={X: x_forecast})
print(y_forecast_pred)
```
I am getting weird output values and are not expected. The training and testing worked well. But when it came to real time scenario, everything was gone and I thought that need to train again. Why my saved models are not working as expected? This is truely a tensorflow bug.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: I dont know that. I am using desktop PC.
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): b'v1.10.0-rc1-19-g656e7a2b34' 1.10.0

- Python version: Python 3.5
- Bazel version (if compiling from source): ?
- GCC/Compiler version (if compiling from source): ?
- CUDA/cuDNN version: 9.0
- GPU model and memory: Not available right now."
23393,"request implementation of conv1DLSTM, conv3DLSTM in keras","
**System information**
- 1.12
- No


**Describe the feature and the current behavior/state.**
there is no conv1DLSTM, conv3DLSTM in keras.

**Will this change the current api? How?**
no
"
23392,Build  model_pruning for bug,"Hi,
bazel build -c opt examples/cifar10:cifar10_{train,val}
EERROR: Skipping 'examples/cifar10:cifar10_train': no such package 'tensorflow/contrib/examples/cifar10': BUILD file not found on package path
ERROR: no such package 'tensorflow/contrib/examples/cifar10': BUILD file not found on package path
Thank you for your help!"
23391,Is the tensorflow framework wrong?,"When I use the tensorflow framework to train my model through the CPU, I get an tensorflow unexpected error.Error is as follows:
```
Traceback (most recent call last):
  File ""/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1361, in _do_call
    return fn(*args)
  File ""/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1340, in _run_fn
    target_list, status, run_metadata)
  File ""/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.AbortedError: Operation received an exception:Status: 5, message: could not create a eltwise forward primitive descriptor, in file tensorflow/core/kernels/mkl_relu_op.cc:457
	 [[Node: DAGLayer_4/Relu = _MklRelu[T=DT_FLOAT, _kernel=""MklOp"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](DAGLayer_4/xw_plus_b, DMT/_8)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/worker/bin/run.py"", line 375, in task_main
    await module_run(api) if iscoroutinefunction(module_run) else module_run(api)
  File ""/worker/package/smiles_package_DAG_qlaVw/train_model/__init__.py"", line 135, in run
    callbacks=[processcallback])
  File ""/worker/package/smiles_package_DAG_qlaVw/DAG_tensorgraph.py"", line 91, in fit_generator
    fetched_values = self.session.run(fetches, feed_dict=train_generator)
  File ""/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 905, in run
    run_metadata_ptr)
  File ""/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1137, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1355, in _do_run
    options, run_metadata)
  File ""/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1374, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.AbortedError: Operation received an exception:Status: 5, message: could not create a eltwise forward primitive descriptor, in file tensorflow/core/kernels/mkl_relu_op.cc:457
	 [[Node: DAGLayer_4/Relu = _MklRelu[T=DT_FLOAT, _kernel=""MklOp"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](DAGLayer_4/xw_plus_b, DMT/_8)]]

Caused by op 'DAGLayer_4/Relu', defined at:
  File ""main.py"", line 32, in <module>
    main()
  File ""main.py"", line 28, in main
    router.task_run()
  File ""/worker/bin/router.py"", line 129, in task_run
    p.start()
  File ""/usr/local/anaconda3/lib/python3.6/multiprocessing/process.py"", line 105, in start
    self._popen = self._Popen(self)
  File ""/usr/local/anaconda3/lib/python3.6/multiprocessing/context.py"", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File ""/usr/local/anaconda3/lib/python3.6/multiprocessing/context.py"", line 277, in _Popen
    return Popen(process_obj)
  File ""/usr/local/anaconda3/lib/python3.6/multiprocessing/popen_fork.py"", line 26, in __init__
    self._launch(process_obj)
  File ""/usr/local/anaconda3/lib/python3.6/multiprocessing/popen_fork.py"", line 80, in _launch
    code = process_obj._bootstrap()
  File ""/usr/local/anaconda3/lib/python3.6/multiprocessing/process.py"", line 258, in _bootstrap
    self.run()
  File ""/usr/local/anaconda3/lib/python3.6/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/worker/bin/router.py"", line 111, in run_worker
    run([worker])
  File ""/atlas/home/test/.local/lib/python3.6/site-packages/autobahn/asyncio/component.py"", line 373, in run
    loop.run_forever()
  File ""/usr/local/anaconda3/lib/python3.6/asyncio/base_events.py"", line 421, in run_forever
    self._run_once()
  File ""/usr/local/anaconda3/lib/python3.6/asyncio/base_events.py"", line 1431, in _run_once
    handle._run()
  File ""/usr/local/anaconda3/lib/python3.6/asyncio/events.py"", line 145, in _run
    self._callback(*self._args)
  File ""/worker/bin/router.py"", line 38, in onJoin
    api = await task_main(task_id)
  File ""/worker/bin/run.py"", line 375, in task_main
    await module_run(api) if iscoroutinefunction(module_run) else module_run(api)
  File ""/worker/package/smiles_package_DAG_qlaVw/train_model/__init__.py"", line 115, in run
    model = DAG_fit.load_from_dir(model_dir)
  File ""/usr/local/anaconda3/lib/python3.6/site-packages/deepchem/models/tensorgraph/tensor_graph.py"", line 818, in load_from_dir
    tensorgraph.restore()
  File ""/usr/local/anaconda3/lib/python3.6/site-packages/deepchem/models/tensorgraph/tensor_graph.py"", line 785, in restore
    self.build()
  File ""/usr/local/anaconda3/lib/python3.6/site-packages/deepchem/models/tensorgraph/tensor_graph.py"", line 476, in build
    layer.create_tensor(training=self._training_placeholder)
  File ""/usr/local/anaconda3/lib/python3.6/site-packages/deepchem/models/tensorgraph/graph_layers.py"", line 686, in create_tensor
    batch_outputs = self.DAGgraph_step(batch_inputs, self.W_list, self.b_list)
  File ""/usr/local/anaconda3/lib/python3.6/site-packages/deepchem/models/tensorgraph/graph_layers.py"", line 705, in DAGgraph_step
    outputs = self.activation(outputs)
  File ""/usr/local/anaconda3/lib/python3.6/site-packages/deepchem/models/tensorgraph/activations.py"", line 96, in relu
    return model_ops.relu(x, alpha=alpha, max_value=max_value)
  File ""/usr/local/anaconda3/lib/python3.6/site-packages/deepchem/models/tensorgraph/model_ops.py"", line 516, in relu
    x = tf.nn.relu(x)
  File ""/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 4331, in relu
    ""Relu"", features=features, name=name)
  File ""/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3271, in create_op
    op_def=op_def)
  File ""/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1650, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

AbortedError (see above for traceback): Operation received an exception:Status: 5, message: could not create a eltwise forward primitive descriptor, in file tensorflow/core/kernels/mkl_relu_op.cc:457
	 [[Node: DAGLayer_4/Relu = _MklRelu[T=DT_FLOAT, _kernel=""MklOp"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](DAGLayer_4/xw_plus_b, DMT/_8)]]
```

I managed to train this model on my GPU by running a model built by tensorflow, but I got this unexpected error when I used CPU training.

Why can't I train on the CPU? My version of tensorflow is 1.6, please give me some guidance, thank you!"
23390,FLOP calculation wrong for matmul with batch dimension,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v1.11.0-rc2-4-gc19e29306c 1.11.0
- Python version: 3.6.5
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
FLOPS is always 0 when specifying batch dimension

**Describe the expected behavior**
FLOPS should be linear in batch dimension

**Code to reproduce the issue**
```
import tensorflow as tf
g = tf.Graph()
with g.as_default():
    b, m, p, q = 2, 25, 16, 9
    A = tf.Variable(tf.zeros([b, m, p]))
    B = tf.Variable(tf.zeros([b, p, q]))
    C = tf.matmul(A,B)

    flops = tf.profiler.profile(g, options = tf.profiler.ProfileOptionBuilder.float_operation())
    print('FLOP should be', b * m * q * 2 * p)
    print('Calculated FLOP', flops.total_float_ops)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23389,"projector.tensorflow.org - ""Load bookmarks"" broken in Chrome 70","The ""Load bookmarks"" button (below) in projector.tensorflow.org doesn't seem to be working: 

![screen shot 2018-10-30 at 2 38 16 pm](https://user-images.githubusercontent.com/8292856/47752506-dd6a6800-dc51-11e8-9ac9-3fb93a6bf599.png)




Below is the error as reported in Chrome JS console:

```
(index):10152 Uncaught TypeError: Cannot read property 'select' of undefined
    at HTMLElement.b._uploadFile ((index):10152)
    at HTMLElement.d ((index):4668)
    at HTMLElement.fire ((index):4690)
    at Object.fire ((index):4678)
    at Object.forward ((index):4685)
    at Object.click ((index):4684)
    at HTMLElement.handleNative ((index):4674)
```

**System information**
Chrome Version 70.0.3538.77 (Official Build) (64-bit)
macOS High Sierra

"
23386,InvalidArgumentError: Invalid name: An op that loads optimization parameters into HBM for embedding. (ConfigureTPUEmbeddingHost),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 1.12.0-rc0
- Python version: 3.6.7rc1
- Bazel version (if compiling from source): 0.19.0
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: 10.0.130 / 7.3.1.20
- GPU model and memory: Nvidia M1000M / ~4GB

**Describe the current behavior**
PyPI packages for tensorflow-gpu didn't work for me on Ubuntu 18.10, but I (somehow) managed to compile the latest Tensorflow. I then tried to run some code that I was working on before (and which used to run fine) and got the following error:
```

Using TensorFlow backend.
Tensorflow: 1.12.0-rc0
Traceback (most recent call last):
  File ""/home/neopostmodern/.PyCharm2018.1/config/scratches/scratch_3.py"", line 10, in <module>
    model.add(CuDNNLSTM(64, input_shape=(10, 39), return_sequences=True))
  File ""/home/neopostmodern/.local/lib/python3.6/site-packages/keras/engine/sequential.py"", line 165, in add
    layer(x)
  File ""/home/neopostmodern/.local/lib/python3.6/site-packages/keras/layers/recurrent.py"", line 532, in __call__
    return super(RNN, self).__call__(inputs, **kwargs)
  File ""/home/neopostmodern/.local/lib/python3.6/site-packages/keras/engine/base_layer.py"", line 431, in __call__
    self.build(unpack_singleton(input_shapes))
  File ""/home/neopostmodern/.local/lib/python3.6/site-packages/keras/layers/cudnn_recurrent.py"", line 425, in build
    from tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops
  File ""/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/contrib/__init__.py"", line 40, in <module>
    from tensorflow.contrib import distribute
  File ""/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/contrib/distribute/__init__.py"", line 34, in <module>
    from tensorflow.contrib.distribute.python.tpu_strategy import TPUStrategy
  File ""/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/tpu_strategy.py"", line 29, in <module>
    from tensorflow.contrib.tpu.python.ops import tpu_ops
  File ""/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/contrib/tpu/__init__.py"", line 69, in <module>
    from tensorflow.contrib.tpu.python.ops.tpu_ops import *
  File ""/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/ops/tpu_ops.py"", line 39, in <module>
    resource_loader.get_path_to_datafile(""_tpu_ops.so""))
  File ""/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/contrib/util/loader.py"", line 56, in load_op_library
    ret = load_library.load_op_library(path)
  File ""/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py"", line 60, in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Invalid name: 
An op that loads optimization parameters into HBM for embedding. Must be
preceded by a ConfigureTPUEmbeddingHost op that sets up the correct
embedding table configuration. For example, this op is used to install
parameters that are loaded from a checkpoint before a training loop is
executed.

parameters: A tensor containing the initial embedding table parameters to use in embedding
lookups using the Adagrad optimization algorithm.
accumulators: A tensor containing the initial embedding table accumulators to use in embedding
lookups using the Adagrad optimization algorithm.
table_name: Name of this table; must match a name in the
  TPUEmbeddingConfiguration proto (overrides table_id).
num_shards: Number of shards into which the embedding tables are divided.
shard_id: Identifier of shard for this operation.
table_id: Index of this table in the EmbeddingLayerConfiguration proto
  (deprecated).
 (Did you use CamelCase?); in OpDef: name: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" input_arg { name: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" description: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" type: DT_FLOAT type_attr: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" number_attr: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" type_list_attr: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" } input_arg { name: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" description: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" type: DT_FLOAT type_attr: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" number_attr: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" type_list_attr: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" } attr { name: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" type: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" default_value { i: -1 } description: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" has_minimum: true minimum: -1 } attr { name: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" type: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" default_value { s: """" } description: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" } attr { name: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" type: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" description: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" } attr { name: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" type: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" description: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" } summary: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" description: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" is_stateful: true
```

**Describe the expected behavior**
It should run, as it did before.

**Code to reproduce the issue**
Reduced to the minimum:
```
import tensorflow as tf
from keras.models import Sequential
from keras.layers import CuDNNLSTM

tf.set_random_seed(42)

print(f'Tensorflow: {tf.__version__}')

model = Sequential()
model.add(CuDNNLSTM(64, input_shape=(10, 39), return_sequences=True))
```
"
23385,"Ignoring visible gpu device (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1) with Cuda compute capability 6.1. The minimum required Cuda capability is 7.0","I downloaded the docker container at (floydhub/tensorflow:1.5.0-gpu.cuda9cudnn7-py3_aws.22). But when I tried to run a sample gpu script I got the ""ignoring visible GPU"" error mentioned in the title. 

**System information**
- OS Platform and Distribution : Linux Ubuntu 16.04:
- TensorFlow pre installed in docker image:
- TensorFlow version 1.5.0:
- Python version: 3.6.2
- CUDA/cuDNN version: 9.0.176
- GPU model and memory: GeForce GTX 1070 (8GB memory)
- Driver version: 390.87

**Code to reproduce the issue using python**
       import tensorflow as tf
       print(tf.contrib.eager.num_gpus())
  
  Output:
   2018-10-30 18:48:01.232945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-10-30 18:48:01.233214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.695
pciBusID: 0000:01:00.0
totalMemory: 7.92GiB freeMemory: 6.59GiB
**2018-10-30 18:48:01.233235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1168] Ignoring visible gpu device (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1) with Cuda compute capability 6.1. The minimum required Cuda capability is 7.0**.
0

TF 1.5 clearly doesn't require CUDA CC of 7.0. The 1070 has a CC of 6.1 according to NVIDIA which should be enough to support TF1.5. I also have the latest drivers. Someone suggested recompiling TF from source, but that kind of defeats the purpose of using a docker container. Is there any other way? Any pointers will be much appreciated. Thank you. "
23384,TensorFlow Lite Interpreter get_tensor() ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS Linux release 7.5.1804 (Core)
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: Nightly 1.13.0-dev20181029
- **Python version**: 3.6.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: Run the _label_image_tflite.py_ script attached with _gray128.jpg_, _inception_v3-l1a-zero-bias-zero-weights.tflite_, _labels.txt_, and _layer_outputs_gray128_f32_b0w0/_ in the same directory

### Describe the problem
When trying to write out tensors to file using the TensorFlow Lite Interpreter::get_tensor() function, mostly incorrect data is being returned. For the attached input, _gray128.jpg_, an image with all pixels set to RGB(128, 128, 128), I expect the layer outputs to be fairly repetitive, but it is not.

To further narrow down the issue, I modified the bias and weight tensors for the first layer to 0.0. After convolution and activation, I would expect the output tensor to be completely 0's, but it is not. The output is consistent across runs (deterministic). The input, output, bias, and weight tensors all seem to be written out correctly, but most of the intermediate output tensors do not seem to be.

I am doing this to try and verify the intermediate outputs with my own calculations. I was hoping to get inception verified with floating point, then with the uint8 quantized model, then with my own model.

### Source code / logs
Attached is:

- _label_image_tflite.py_
- _inception_v3-l1a-zero-bias-zero-weights.tflite_
- _labels.txt_
- _gray128.jpg_

Just run the script (`python3 label_image_tflite.py`) in the same directory as the other files and a subdirectory, _layer_outputs_gray128_f32_b0w0/_ .

You can see in _layer_outputs_gray128_f32_b0w0/6_tensor.txt_ that the output is not zero and doesn't appear to be repetitive despite the gray uniform input.

The label_image script was modified from the [TensorFlow Lite example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/label_image.py). The modified portion:
```python
.
.
.
  interpreter.set_tensor(input_details[0]['index'], input_data)
  interpreter.invoke()

  # BEGIN Custom code to write out all tensors to files in a subdirectory
  ind = 0
  try:
    while True:
      dets = interpreter._get_tensor_details(ind)
      tens = interpreter.get_tensor(ind)
      # Expects subdirectory to exist
      f_dets = open(""layer_outputs_gray128_f32_b0w0/{}_details_{}.txt"".format(ind, dets['name'].replace('/', '-')), ""w"")
      # Write details
      f_dets.write(""tensor[{}]: {}\n"".format(ind, dets))
      # Write tensor data
      # Output tensors (Relu's) are giving incorrect values
      tens.tofile(""layer_outputs_gray128_f32_b0w0/{}_tensor.txt"".format(ind), "","")
      ind += 1
      f_dets.close()
  except:
    print(""Finished writing tensor data"")
  # END custom code

  output_data = interpreter.get_tensor(output_details[0]['index'])
  results = np.squeeze(output_data)
.
.
.
```

https://drive.google.com/open?id=1OhFLGVm9SVb9RnGxqw6gdePc12i5y1Nw (84MB)

If there is an alternate way I can inspect the intermediate outputs, that would be appreciated as well.

Thanks!

Edit: I suspect this issue is related: #22891"
23383,Infinite loop in graph optimization,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.11.0
- Python version: 3.6.6
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**

When calling `sess.run` for certain graphs (e.g., see below), the session call gets stuck in an infinite loop.

**Describe the expected behavior**

The session call should complete normally.

**Code to reproduce the issue**

```
import numpy as np
import tensorflow as tf

const1 = tf.zeros((1,))
const2 = tf.zeros((1,))
const3 = tf.zeros((1,))
const4 = tf.zeros((1,))
ph = tf.placeholder(tf.float32, (1,))

c1 = tf.concat((const1, const2, ph), axis=0)
c2 = tf.concat((const3, const4, c1), axis=0)

with tf.Session() as sess:
    sess.run(c2, feed_dict={ph: np.zeros((1,))})  # gets stuck in infinite loop here
```

**Other info / logs**
Specifically, the build process is getting stuck in the ``ConstantFolding`` optimization pass.  This is due to the new ``MergeConcat`` optimization introduced in https://github.com/tensorflow/tensorflow/commit/e96d65246835b3a33a55c70d1f1057517ef0aa8e.  It can create a graph structure that causes the ``PartialConcatConstFolding`` optimization to enter an infinite loop.
"
23380,MKL option not building successfully,"Hello,

TF isn't building for me with the MKL option (this problem started a couple days ago).
Build call and error trace are below. I'm trying to build in a Docker.

For what its worth, I actually have base MKL and MKL-DNN built into my system already, but I've never been able to make TF actually pick up those libraries (always errorred out).


**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian:Stretch
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: Master branch
- Python version: 3.6.5
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 0.19.0
- GCC/Compiler version (if compiling from source): 8.2.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Build call**

```bash 
echo ""deb [arch=amd64] http://storage.googleapis.com/bazel-apt stable jdk1.8"" | sudo tee /etc/apt/sources.list.d/bazel.list && \
    curl https://bazel.build/bazel-release.pub.gpg | sudo apt-key add - && \
    apt-get update && apt-get install -y bazel  && rm -rf /var/lib/apt/lists/* && \
    ldconfig && \
    pip uninstall -y tensorflow-tensorboard tfp-nightly tensorflow_estimator tb-nightly tf-nightly tensorflow && \
    cd /opt && \
    git clone https://github.com/tensorflow/tensorflow.git && \
    cd /opt/tensorflow && \
    /bin/bash ./configure \
    && \
    bazel build \
    --config=mkl --config=opt \
    --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" \
    --copt=-msse4.2 --copt=-msse4.1 --copt=-mavx --copt=-msse2 --copt=-msse3  \
    --copt=-O3 --copt=-mfpmath=both \
    --copt=""-DMKL_LP64"" \
    --copt=""-fPIC"" \
    --linkopt=""-lmkl_gf_lp64"" \
    --linkopt=""-lmkl_gnu_thread"" \
    --linkopt=""-dl"" \
    --linkopt=""-lpthread"" \
    --linkopt=""-lmkl_core"" \
    --linkopt=""-lm"" \
    --linkopt=""-lmkl_rt"" \
    --linkopt=""-lmkldnn"" \
    tensorflow/tools/pip_package:build_pip_package && \
    bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip && \
    pip install --no-deps /tmp/pip/tensorflow-*.whl && \
    cd /opt && rm -rf /opt/tensorflow /tmp/* && \
    python -c ""import tensorflow as tf; hello = tf.constant('Hello, TensorFlow!'); sess = tf.Session(); print(sess.run(hello))"" && \
    python -c ""import tensorboard""
```


** Error trace **
```bash 

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=gdr         	# Build with GDR support.
	--config=verbs       	# Build with libverbs support.
	--config=ngraph      	# Build with Intel nGraph support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=noaws       	# Disable AWS S3 filesystem support.
	--config=nogcp       	# Disable GCP support.
	--config=nohdfs      	# Disable HDFS support.
	--config=noignite    	# Disable Apacha Ignite support.
	--config=nokafka     	# Disable Apache Kafka support.
Configuration finished
[91mWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
/opt/tensorflow/tools/bazel.rc
[0m[91mStarting local Bazel server and connecting to it...
[0m[91mINFO: Options provided by the client:
  Inherited 'common' options: --isatty=0 --terminal_columns=80
[0m[91mINFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/bin/python --action_env PYTHON_LIB_PATH=/opt/conda/lib/python3.6/site-packages --python_path=/opt/conda/bin/python --action_env TF_NEED_OPENCL_SYCL=0 --action_env TF_NEED_ROCM=0 --action_env TF_NEED_CUDA=0 --action_env TF_DOWNLOAD_CLANG=0
ERROR: Config value mkl is not defined in any .rc file
[0mThe command '/bin/sh -c echo ""deb [arch=amd64] http://storage.googleapis.com/bazel-apt stable jdk1.8"" | sudo tee /etc/apt/sources.list.d/bazel.list &&     curl https://bazel.build/bazel-release.pub.gpg | sudo apt-key add - &&     apt-get update && apt-get install -y bazel  && rm -rf /var/lib/apt/lists/* &&     ldconfig &&     pip uninstall -y tensorflow-tensorboard tfp-nightly tensorflow_estimator tb-nightly tf-nightly tensorflow &&     cd /opt &&     git clone https://github.com/tensorflow/tensorflow.git &&     cd /opt/tensorflow &&     /bin/bash ./configure     &&     bazel build     --config=mkl --config=opt     --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0""     --copt=-msse4.2 --copt=-msse4.1 --copt=-mavx --copt=-msse2 --copt=-msse3      --copt=-O3 --copt=-mfpmath=both     --copt=""-DMKL_LP64""     --copt=""-fPIC""     --linkopt=""-lmkl_gf_lp64""     --linkopt=""-lmkl_gnu_thread""     --linkopt=""-dl""     --linkopt=""-lpthread""     --linkopt=""-lmkl_core""     --linkopt=""-lm""     --linkopt=""-lmkl_rt""     --linkopt=""-lmkldnn""     tensorflow/tools/pip_package:build_pip_package &&     bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip &&     pip install --no-deps /tmp/pip/tensorflow-*.whl &&     cd /opt && rm -rf /opt/tensorflow /tmp/* &&     python -c ""import tensorflow as tf; hello = tf.constant('Hello, TensorFlow!'); sess = tf.Session(); print(sess.run(hello))"" &&     python -c ""import tensorboard""' returned a non-zero code: 2
```

 "
23378,Can we load a pretrained tensorflow model which don't have metafile?,"Hi Team,

I have a pre-trained model which contains ""model_gs_19860_0.19.data-00000-of-00001"" and ""model_gs_19860_0.19.index"" but don't have any metafile, is it possible to load this model?"
23377,official support AMD's GPU plan?,"**System information**
- OS Platform and Distribution : Ubuntu 18.04
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):tensorflow-gpu-1.8.0
- Python version:python3.6
- CUDA/cuDNN version:cuda9.2 cudnn7.2
- GPU model and memory: AMD Radeon Radeon RX Vega 64


**Describe the feature and the current behavior/state.**
Does the official support AMD's [ROCm](https://rocm.github.io/index.html) plan?


**Who will benefit with this feature?**
The expectations of the majority of AMD users，After all, cost-effective."
23376,ParameterServerStrategy throws an exception when training locally with multiple GPUS,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos-release-7-4.1708.el7.centos.x86_64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): compiled form source with flags 
```
export TF_NEED_HDFS=1
export TF_NEED_KAFKA=1
export TF_ENABLE_XLA=1
export TF_NEED_CUDA=1
export TF_CUDA_COMPUTE_CAPABILITIES=5.2,7.0
export TF_CUDA_VERSION=9.2
export TF_CUDNN_VERSION=7
export TF_NCCL_VERSION=1.3

bazel build \
            --config=opt \
            --config=cuda \
            --copt=-msse4.2 \
            --copt=-mavx \
            --copt=-mavx2 \
            --copt=-mfma \
            --copt=-O3 \
            //tensorflow/tools/pip_package:build_pip_package
```
- TensorFlow version (use command below): b'v1.11.0-0-gc19e293' 1.11.0
- Python version: Python 3.6.6
- Bazel version (if compiling from source): 0.18.0
- GCC/Compiler version (if compiling from source): 4.8.5 20150623
- CUDA/cuDNN version: 9.2, 7
- GPU model and memory: Tesla V100-PCIE 16GB and Tesla V100-PCIE 16GB

**Describe the current behavior**
I'm trying to run an official resnet model on cifar 10 dataset from https://github.com/tensorflow/models/tree/master/official/resnet 
where I replaced MirroredStrategy by ParameterServerStrategy in https://github.com/tensorflow/models/blob/master/official/utils/misc/distribution_utils.py#L48
as following
```
# return tf.contrib.distribute.MirroredStrategy(num_gpus=num_gpus)
     return tf.contrib.distribute.ParameterServerStrategy(num_gpus)
```

It throws an exception
```
I1030 10:21:32.401904 139688013772544 tf_logging.py:115] Done calling model_fn.
Traceback (most recent call last):
  File ""./models/official/resnet/cifar10_main.py"", line 285, in <module>
    absl_app.run(main)
  File ""/home/a.eryshev/miniconda3/envs/py36/lib/python3.6/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/home/a.eryshev/miniconda3/envs/py36/lib/python3.6/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""./models/official/resnet/cifar10_main.py"", line 278, in main
    run_cifar(flags.FLAGS)
  File ""./models/official/resnet/cifar10_main.py"", line 273, in run_cifar
    shape=[_HEIGHT, _WIDTH, _NUM_CHANNELS])
  File ""/home/a.eryshev/dev/tf-experiments/models/official/resnet/resnet_run_loop.py"", line 565, in resnet_main
    hooks=train_hooks, max_steps=flags_obj.max_train_steps)
  File ""/home/a.eryshev/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 356, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/a.eryshev/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1179, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/home/a.eryshev/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1295, in _train_model_distributed
    destinations='/device:CPU:0'))[0]
  File ""/home/a.eryshev/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/distribute.py"", line 748, in reduce
    return self._reduce(aggregation, value, destinations)
  File ""/home/a.eryshev/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/parameter_server_strategy.py"", line 305, in _reduce
    self._verify_destinations_not_different_worker(destinations)
  File ""/home/a.eryshev/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/parameter_server_strategy.py"", line 302, in _verify_destinations_not_different_worker
    (d, self._worker_device))
AttributeError: 'ParameterServerStrategy' object has no attribute '_worker_device'
```

**Describe the expected behavior**
Replacing MirroredStrategy by ParameterServerStrategy launches a training with CPU acting as PS and 2 GPU workers.

**Code to reproduce the issue**
Run command:
```
python ./models/official/resnet/cifar10_main.py --data_dir=/data --model_dir=/data --benchmark_logger_type=BaseBenchmarkLogger --resnet_version=2 --clean --log_step_count_steps=10 --save_summary_steps=10 --epochs_between_evals=1820 --train_epochs=1820 --batch_size=512 --num_gpus=2
```"
23375,"Protobuf error, mismatch with system installed version","- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1 LTS
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 1.11.0
- Python version: 3.6.6 (virtualenv)
- Bazel version (if compiling from source): 0.17.2
- GCC/Compiler version (if compiling from source): gcc-6 (Ubuntu 6.4.0-17ubuntu1) 6.4.0 20180424
- CUDA/cuDNN/NCCL version: 9.1/7.1/2.1.15
- GPU model and memory: Quadro M1200

I know this a typical problem and i could compile and install from source using a previous version but this time it's a bit different.

Compile log is : 

```
ERROR: /home/god_kane/Downloads/tensorflow-1.11.0/tensorflow/contrib/nccl/BUILD:24:1: error while parsing .d file: /home/god_kane/.cache/bazel/_bazel_god_kane/380ea18d2d4a7f2f6db5af2f6d4bc750/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/nccl/_objs/python/ops/_nccl_ops_gpu/nccl_manager.pic.d (No such file or directory)
In file included from ./tensorflow/core/framework/variant.h:26:0,
                 from ./tensorflow/core/framework/allocator.h:26,
                 from ./tensorflow/core/framework/tensor.h:20,
                 from ./tensorflow/core/framework/log_memory.h:19,
                 from ./tensorflow/core/common_runtime/gpu/gpu_event_mgr.h:21,
                 from ./tensorflow/contrib/nccl/kernels/nccl_manager.h:31,
                 from tensorflow/contrib/nccl/kernels/nccl_manager.cc:15:
bazel-out/k8-opt/genfiles/tensorflow/core/framework/tensor.pb.h:12:2: error: #error This file was generated by a newer version of protoc which is
 #error This file was generated by a newer version of protoc which is
  ^~~~~
bazel-out/k8-opt/genfiles/tensorflow/core/framework/tensor.pb.h:13:2: error: #error incompatible with your Protocol Buffer headers. Please update
 #error incompatible with your Protocol Buffer headers.  Please update
  ^~~~~
bazel-out/k8-opt/genfiles/tensorflow/core/framework/tensor.pb.h:14:2: error: #error your headers.
 #error your headers.
  ^~~~~
```

Those file are correctly generated with protobuf 3.6.0 which i believe is compiled as third_party.  
What i don't understand is that it seems what is included later are my system protobuf headers and not the one from tensorflow folder resulting in an error.  
I tried to do something dirty to check, installed protobuf from source and created symbolink link to it replacing my system installed version.  
Doing that worked but it's not a valid solution. I also should not need to install my own protobuf version just for using it in python.
The fault seems to be with nccl but i don't know where to look for a more clean fix."
23374,build libtensorflow_cc.so with debug symbols using bazel,"**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.8.0
- Python version: Python 2.7.12
- Installed using virtualenv? pip? conda?: No
- Bazel version (if compiling from source): 0.16.1
- GCC/Compiler version (if compiling from source): gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10)
- CUDA/cuDNN version: 9.2/7.2.1
- GPU model and memory: Tesla K40c 12 GB VRAM

**Describe the problem**
I need to debug `libtensorflow_cc.so` to trace some other issue. I learned from `bazel` that I need to pass the following flag `-c dbg` -probably this one too `--strip=never`- while building the library. I noticed a significant increase in the binary size which makes sense! yet I'm still unable to step into the library code. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
`bazel build --config=opt --config=cuda --config=monolithic tensorflow:libtensorflow_cc.so`

**Any other info / logs**
- I passed `--config=monolithic` to resolve OpenCV's `imread` issue always returning a 0x0 mat.
- Furthermore, the binary built with  `-c dbg` and `--strip=never`, failed to load my model that was loaded correctly before passing those flags. but we can discuss it in a different issue if we resolve this one first.
"
23373,car Detection ,"Hi 
I only want to detect car objects. Can anybody tell me which tflite class used? So that I am able to detect only car object in Android.
Thanks!"
23372,TensorFlow installation for Python 3.7,"I have Python 3.7 running on Anaconda on my new Windows PC. MSVCP140.dll is present

I still have no clue why is it not working. If Python 3.7 is official, can you please fix Tensorflow module to support the same?

Screenshots : 

![image](https://user-images.githubusercontent.com/36528857/47712661-846ce680-dbf5-11e8-987e-fa1554230a86.png)
"
23371,Tensorflow GPU setup issue on Ryzen 5 + 2080 RTX GPU + Ubuntu 18.04,"Hi Team,

I am trying to setup tensorflow-gpu on my new machine with configuration AMD Ryzen 5 2600 + 2080 RTX GPU + Ubuntu 18.04. I have tried tensorflow-gpu docker, tensorflow build from source.

In docker image, it was exiting after first epoch without any error. 

And in other approach we are getting following errors:

ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory.

InvalidArgumentError (see above for traceback): Cannot assign a device for operation MatMul: Operation was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:0 ]. Make sure the device specification refers to a valid device.
	 [[node MatMul (defined at test.py:6)  = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=""/device:GPU:0""](a, b)]]


Please assist with right steps to setup tensorflow gpu on Ryzen 5 + 2080 RTX GPU + Ubuntu 18.04 machine. Quick response will be really appreciated.

Regards,
Ankit Aggarwal
"
23369,KeyError: 'BlockLSTM' when using tf.train.import_meta_graph(),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
CentOS 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
pip install tensorflow-gpu
- TensorFlow version (use command below):
1.11
- Python version:
python 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
CUDA 9.0, cuDNN 7.31
- GPU model and memory:
TITAN X (Pascal), 12G

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

v1.11.0-0-gc19e29306c 1.11.0

**Describe the current behavior**

I'm using two kinds of LSTM operations like below.
```python
    def __bi_lstm(self, inputs, lengths, rnn_size, keep_prob=0.5, scope='bi-lstm'):
        """"""Apply bi-directional LSTM
        """"""
        with tf.variable_scope(scope):
            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size)
            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size)
            (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(cell_fw,
                                                                        cell_bw,
                                                                        inputs,
                                                                        sequence_length=lengths,
                                                                        dtype=tf.float32)
            outputs = tf.concat([output_fw, output_bw], axis=-1)
            return tf.nn.dropout(outputs, keep_prob)

    def __bi_lstm_fused(self, inputs, lengths, rnn_size, keep_prob=0.5, scope='bi-lstm-fused'):
        """"""Apply bi-directional LSTM block fused
        """"""
        with tf.variable_scope(scope):
            t = tf.transpose(inputs, perm=[1, 0, 2])  # Need time-major
            lstm_cell_fw = tf.contrib.rnn.LSTMBlockFusedCell(rnn_size)
            lstm_cell_bw = tf.contrib.rnn.LSTMBlockFusedCell(rnn_size)
            lstm_cell_bw = tf.contrib.rnn.TimeReversedFusedRNN(lstm_cell_bw)
            output_fw, _ = lstm_cell_fw(t, dtype=tf.float32, sequence_length=lengths)
            output_bw, _ = lstm_cell_bw(t, dtype=tf.float32, sequence_length=lengths)
            outputs = tf.concat([output_fw, output_bw], axis=-1)
            outputs = tf.transpose(outputs, perm=[1, 0, 2])
            return tf.nn.dropout(outputs, keep_prob)
```
i trained a model and save it via saver.save().
and then, tried to import_meta_data

```python
loader = tf.train.import_meta_graph(meta_file, clear_devices=True)
....
```

in case tf.contrib.rnn.LSTMCell(), everything goes fine. 
but, when it comes to use tf.contrib.rnn.LSTMBlockFusedCell(), 
there is an key error.

```bash
 Traceback (most recent call last):
  File ""export.py"", line 55, in <module>
    export(args)
  File ""export.py"", line 13, in export
    loader = tf.train.import_meta_graph(meta_file, clear_devices=True)
  File ""~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1666, in import_meta_graph
    meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]
  File ""~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1688, in _import_meta_graph_with_return_elements
    **kwargs))
  File ""~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py"", line 806, in import_scoped_meta_graph_with_return_elements
    return_elements=return_elements)
  File ""~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 391, in import_graph_def
    _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)
  File ""~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 158, in _RemoveDefaultAttrs
    op_def = op_dict[node.op]
KeyError: 'BlockLSTM'
```

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23368,tensorflow\contrib\coder\python\ops\_coder_ops.so not found,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
sample from matterport/maskrcnn
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows 10 x64
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):
conda list
tensorflow-gpu            1.10.0                   py36_0    aaronzs
tensorflow-gpu            1.11.0                    <pip>
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
b'v1.10.0-rc1-19-g656e7a2b34' 1.10.0
- Python version:3.6.7
- CUDA/cuDNN version:9.0/7.1.4
- GPU model and memory:GeForce GTX 1060 3GB


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I tired to train a sample in the project from [https://github.com/matterport/Mask_RCNN](url)
then there is an error showed up.
I tried the basic functions of tensorflow like session they works fine.
but when I used train something about DLL showed up
it said that it can't find something like program input  ""? MakeShape@TensorShapeUtils@tensorflow@@SA?AVStatus@2@V?$Span@$$CBH@absl@@PEAVTensorShape@2@@Z(in DLL C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv\lib\site-packages\tensorflow\contrib\coder\python\ops_coder_ops.so)""
**Describe the expected behavior**
there should be no error 
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
there is no error until this line
`train(model)`
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Loading weights  C:\HSIR\Mask_RCNN\mask_rcnn_coco.h5
2018-10-30 15:35:55.918873: I T:\src\github\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2018-10-30 15:35:56.255762: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1405] Found device 0 with properties:
name: GeForce GTX 1060 3GB major: 6 minor: 1 memoryClockRate(GHz): 1.759
pciBusID: 0000:01:00.0
totalMemory: 3.00GiB freeMemory: 2.42GiB
2018-10-30 15:35:56.255976: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1484] Adding visible gpu devices: 0
2018-10-30 15:35:56.948830: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-10-30 15:35:56.948953: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:971]      0
2018-10-30 15:35:56.953167: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:984] 0:   N
2018-10-30 15:35:56.954126: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2125 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
Training network heads
Traceback (most recent call last):
  File ""balloon.py"", line 364, in <module>
    train(model)
  File ""balloon.py"", line 199, in train
    layers='heads')
  File ""C:\HSIR\Mask_RCNN\mrcnn\model.py"", line 2341, in train
    histogram_freq=0, write_graph=True, write_images=False),
  File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv\lib\site-packages\keras\callbacks.py"", line 745, in __init__
    from tensorflow.contrib.tensorboard.plugins import projector
  File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv\lib\site-packages\tensorflow\contrib\__init__.py"", line 31, in <module>
    from tensorflow.contrib import coder
  File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv\lib\site-packages\tensorflow\contrib\coder\__init__.py"", line 22, in <module>
    from tensorflow.contrib.coder.python.layers.entropybottleneck import *
  File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv\lib\site-packages\tensorflow\contrib\coder\python\layers\entropybottleneck.py"", line 24, in <module>
    from tensorflow.contrib.coder.python.ops import coder_ops
  File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv\lib\site-packages\tensorflow\contrib\coder\python\ops\coder_ops.py"", line 30, in <module>
    resource_loader.get_path_to_datafile(""_coder_ops.so""))
  File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv\lib\site-packages\tensorflow\contrib\util\loader.py"", line 56, in load_op_library
    ret = load_library.load_op_library(path)
  File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv\lib\site-packages\tensorflow\python\framework\load_library.py"", line 56, in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename)
tensorflow.python.framework.errors_impl.NotFoundError: C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv\lib\site-packages\tensorflow\contrib\coder\python\ops\_coder_ops.so not found"
23367,How to visualizing a model structure in tensorboard when eager execution is open? ,"tensorflow-gpu == 1.10.0
python == 3.5

Hello, i want to visualize a model structure in tensorboard when eager execution is open, but i can not find one available solution to do this. In graph mode, we can use tf.summary.writer('', sess) to save model structure, but how to do as same as this under eager mode? "
23366,Tensorflow Lite Andriod bug,"hey guys, i'm very new for Tensorflow and Tensorflow lite.
I followed the the guide from TensorFlow For Poets, which i see from official site of Tensorflow Lite https://www.tensorflow.org/lite/

I am finished very quickly the first guide TensorFlow For Poets, and then i tried to set up Tensorflow on my andriod device(I have a Arm-RK3399, which i have been installed andorid). The first 3 Steps were no problem, but after I opend andriod studio, it became very slow, as in the guide, i choosed Tensorflow-for-poets-2/android/tflite from the working directory and then i didn't get a ""gradle Sync"" popup, but in android studio i saw the Gradle was Syncing, and also configure building Cpp. but I waited almost 1 hour, it was still building. I also tried changing to offline work, but it didn't help. 

Is that because I used Linux in Virtual Machine caused this ?  or I should used android Phone? 

Thx
Jason


"
23365,Integrating embeddings with language translation,"i am using the tensorflow nmt with attention program for language translation 
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb

In there i like to add the pretrained word embeddings is there is any way that i can add them and used for predictions"
23364,How to read particular text from scanned or image file using python with DL library,"How to read particular text (like empno,name,salary,etx) from scanned or image file and store in db using python with any one deep learning library (like tensorflow, pytorch,etc)

pls. provide some sample code and which one we can use for this concepts"
23361,Installation problem with Tensorflow hub,"Device: Raspberry pi 3B
System information
TensorFlow version: 1.11.0
Python version: 3.5.3

Anyone knows how to install tensorflow hub?"
23360,Any detailed explain about timeline?,"I can not find a official and detailed timeline document telling me how to explain the timeline graph.
For example, what are the meaning and difference  of /device:GPU:0/stream:all and /job:localhost/replica:0/task:0/device:GPU:0, /job:localhost/replica:0/task:0/device:GPU:0? In a multi-gpu environment, why there is only /device:GPU:0/stream:all, where is the /device:GPU:1/stream:all?"
23359,build models_pruning  for bug,"Hi,
bazel-bin/$examples_dir/cifar10/cifar10_train --pruning_hparams=name=cifar10_pruning,begin_pruning_step=10000,end_pruning_step=100000,target_sparsity=0.9,sparsity_function_begin_step=10000,sparsity_function_end_step=100000
bash: bazel-bin/contrib/model_pruning/examples/cifar10/cifar10_train: No such file or directory

Thank you for your help!"
23357,FP16 Sparse Matrix multiply returns incorrect results on ARM,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **no**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux for Tegra (Ubuntu 18.04)**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **Jetson AGX Xavier (ARM-aarch64)**
- TensorFlow installed from (source or binary): **source**
- TensorFlow version (use command below): **1.11**
- Python version: **2.7**
- Bazel version (if compiling from source): **0.15.0**
- GCC/Compiler version (if compiling from source): **Linaro 7.3.0**
- CUDA/cuDNN version: **CUDA 10 cuDNN 7.3**
- GPU model and memory: **Jetson AGX Xavier, 16GB (shared with host)**


**Describe the current behavior**

When a matrix multiply is called such that one of the input tensors is of data type fp16 and declared sparse, the function returns incorrect results.  This error causes the failure of the sparse_matmul_op_test (found at tensorflow/python/kernel_tests/sparse_matmul_op_test.py)

**Describe the expected behavior**

The matmul function should return correct results, regardless of sparsity or data type.

**Code to reproduce the issue**

Running the sparse_matmul_op_test.py script on an ARM architecture device can reproduce the failure.  This python script is a reduced reproduction:

```
import numpy as np
import tensorflow as tf
from tensorflow.python.framework import dtypes
from tensorflow.python.ops import math_ops


with tf.Session() as sess:
  x=(np.clip(
      np.random.uniform(
          low=-256.0, high=256.0, size=3 * 3), -64,
      64) / 128.0).reshape([3, 3]).astype(np.float32)
  y=(np.clip(
      np.random.uniform(
          low=-256.0, high=256.0, size=3 * 3), -64,
      64) / 128.0).reshape([3, 3]).astype(np.float32)



  #Test matmul where a is sparse fp16, and b is not sparse fp32
  tf_x = math_ops.cast(x, dtypes.bfloat16)
  tf_y = math_ops.cast(y, dtypes.float32)
  tf_ans = math_ops.matmul(
      tf_x,
      tf_y,
      a_is_sparse=True,
      b_is_sparse=False)
  out = sess.run(tf_ans)
  #Test the same matmul case as above, without sparsity
  tf_ans_nosparse = math_ops.matmul(
      tf_x,
      tf_y,
      a_is_sparse=False,
      b_is_sparse=False)
  out_nosparse = sess.run(tf_ans_nosparse)

  #Numpy result where both types are np32, since the dot operation is not supported with fp16
  np_x = sess.run(math_ops.cast(tf_x, dtypes.float32))
  np_y = sess.run(math_ops.cast(tf_y, dtypes.float32))
  np_ans = np.matrix(np_x) * np.matrix(np_y)

  print(""\tNumpy answer: "")
  print(np_ans)
  print(""\tNon-Sparse TF: "")
  print(out_nosparse)
  print(""\tSparse fp16 a, non-sparse fp32 b"")
  print(out)
```


**Other info / logs**

Here is a sample output testing different configurations for the matmul inputs’ sparsity and data types.  (This output was generating by adding additional test configurations to the provided reproduce script.)

	

> Numpy answer: 
> [[-0.24025428 -0.13065982 -0.25      ]
>  [-0.08487757 -0.36836362  0.74902344]
>  [ 0.16072105  0.00858951  0.3720703 ]]
> 	Non-Sparse TF: 
> [[-0.24025428 -0.13065982 -0.25      ]
>  [-0.08487757 -0.36836362  0.74902344]
>  [ 0.16072105  0.00858951  0.3720703 ]]
> 	Sparse TF: 
> Sparse fp16 a, non-sparse fp32 b
> [[ 0.58551383  0.6306598  -0.25      ]
>  [ 0.58551383  0.6306598  -0.25      ]
>  [-0.33927894 -0.25273013 -0.12792969]]
> Sparse fp32 a, non-sparse fp16 b
> [[-0.24023438 -0.13085938 -0.25      ]
>  [-0.08523875 -0.36806947  0.74892884]
>  [ 0.16089517  0.0092376   0.3716218 ]]
> Non-sparse fp16 a, sparse fp32 b
> [[-0.24025428 -0.13065982 -0.25      ]
>  [-0.08487757 -0.36836362  0.74902344]
>  [ 0.16072105  0.00858951  0.3720703 ]]
> Non-sparse fp32 a, sparse fp16 b
> [[-0.17285156 -0.08886719  0.25      ]
>  [-0.17211097 -0.17229089  0.24892886]
>  [ 0.08876151  0.02520579 -0.12837823]]
> Sparse fp16 a, sparse fp32 b
> [[ 0.58551383  0.6306598  -0.25      ]
>  [ 0.58551383  0.6306598  -0.25      ]
>  [-0.33927894 -0.25273013 -0.12792969]]
> Sparse fp16 a, sparse fp16 b
> [[bfloat16(0.5859375) bfloat16(0.6328125) bfloat16(-0.25)]
>  [bfloat16(0.5859375) bfloat16(0.6328125) bfloat16(-0.25)]
>  [bfloat16(-0.33984375) bfloat16(-0.25390625) bfloat16(-0.127929688)]]
> Sparse fp32 a, sparse fp32 b
> [[-0.24025428 -0.13065982 -0.25      ]
>  [-0.08481594 -0.36826903  0.74892884]
>  [ 0.16101329  0.00903805  0.3716218 ]]
> "
23356,Betainc_op_test fails on ARM because one of the gradients is NaN,"

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **no**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux for Tegra (Ubuntu 18.04)**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **Jetson AGX Xavier (ARM-aarch64)**
- TensorFlow installed from (source or binary): **source**
- TensorFlow version (use command below): **1.11**
- Python version: **2.7**
- Bazel version (if compiling from source): **0.15.0**
- GCC/Compiler version (if compiling from source): **Linaro 7.3.0**
- CUDA/cuDNN version: **CUDA 10 cuDNN 7.3**
- GPU model and memory: **Jetson AGX Xavier, 16GB (shared with host)**


**Describe the current behavior**

The betainc_op test testBetaIncFpropAndBpropAreNeverNAN (within tensorflow/python/kernel_tests/betainc_op_test.py) fails because one of the gradient values is NaN.

**Describe the expected behavior**

The betainc_op test testBetaIncFpropAndBpropAreNeverNAN should pass, where none of the values are NaN

**Code to reproduce the issue**

Running tensorflow/python/kernel_tests/betainc_op_test.py on an ARM architecture machine should reproduce the issue.
In addition, this small python script shows the same results:

```
import tensorflow as tf
import numpy as np
from tensorflow.python.framework import constant_op
from tensorflow.python.framework import dtypes
from tensorflow.python.ops import gradients_impl
from tensorflow.python.ops import math_ops

with tf.Session() as sess:
  ga_s = [.4, .1, .2, .3]
  gb_s = [.4, .1, .2, .3]
  gx_s = [.7, .8, .9, 1.]

  ga_s_t = constant_op.constant(ga_s, dtype=dtypes.float32)
  gb_s_t = constant_op.constant(gb_s, dtype=dtypes.float32)
  gx_s_t = constant_op.constant(gx_s, dtype=dtypes.float32)
  tf_gout_t = math_ops.betainc(ga_s_t, gb_s_t, gx_s_t)
  tf_gout, grads_x = sess.run(
      [tf_gout_t,
       gradients_impl.gradients(tf_gout_t, [ga_s_t, gb_s_t, gx_s_t])[2]])

  print(np.isnan(tf_gout))
  print(np.isnan(grads_x))
  print(grads_x)

```

**Other info / logs**

This appears to be caused by one of the inputs to the betainc function being set to 1, as changing the  testBetaIncFpropAndBpropAreNeverNAN definition of space_x from
	

> space_x = np.linspace(1e-16, 1 – 1e-16).tolist()


to
	

> space_x = np.linspace(1e-16, 1 – 1e-8).tolist()

removes the NaN.

Interestingly, if the number of elements in the tensor is less than four, the gradient is returned correctly (inf instead of nan)


Here is the output of the failed test:

> FAIL: testBetaIncFpropAndBpropAreNeverNAN (__main__.BetaincTest)
> 
> Traceback (most recent call last):
>   File ""tensorflow/python/kernel_tests/betainc_op_test.py"", line 160, in testBetaIncFpropAndBpropAreNeverNAN
>     np.isnan(grads_x))
>   File ""/home/nvidia/Tensorflow/tensorflow/tf_env2.7/local/lib/python2.7/site-packages/tensorflow/python/framework/test_util.py"", line 1556, in assertAllEqual
>     np.testing.assert_array_equal(a, b, err_msg=msg)
>   File ""/home/nvidia/Tensorflow/tensorflow/tf_env2.7/local/lib/python2.7/site-packages/numpy/testing/nose_tools/utils.py"", line 855, in assert_array_equal
>     verbose=verbose, header='Arrays are not equal')
>   File ""/home/nvidia/Tensorflow/tensorflow/tf_env2.7/local/lib/python2.7/site-packages/numpy/testing/nose_tools/utils.py"", line 779, in assert_array_compare
>     raise AssertionError(msg)
> AssertionError:
> Arrays are not equal
> 
> (mismatch 2.0%)
>  x: array([False, False, False, ..., False, False, False])
>  y: array([False, False, False, ..., False, False,  True])
"
23355,Failed to load,"**System information**
- OS Platform and Distribution: Windows 10 x64
- TensorFlow installed from (source or binary):
- TensorFlow version: tensorflow_gpu-1.11.0
- Python version: Python 3.6.7
- Installed using: pip
- CUDA/cuDNN version: cuda 9.0 and cuDNN v7.3.1
- GPU model and memory: 1080 ti 11 GB

Running import tensorflow as tf returns: 

Traceback (most recent call last):
  File ""C:\Users\Sebastian\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Sebastian\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Sebastian\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Sebastian\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Sebastian\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Could not find the module.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""test.py"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\Sebastian\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Sebastian\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Sebastian\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Sebastian\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Sebastian\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Sebastian\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Sebastian\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Sebastian\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Could not find the module.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

I guess my problem is something with the paths. I have added the following: 
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\bin
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\lib\x64

"
23353,OS : Windows 10,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS : Windows 10
- TensorFlow installed from (source or binary): from pip
- TensorFlow version: 1.11.0
- Python version: 3.6
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): No 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10 
- GPU model and memory: Nvidia 1050Ti


Package             Version
------------------- ----------
tensorflow-gpu      1.11.0


able to see tensorflow gpu installed but when run 

# Creates a graph.
a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
c = tf.matmul(a, b)
# Creates a session with log_device_placement set to True.
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
# Runs the op.
print(sess.run(c))


I'm getting a error GPU is not avalable, I did go through lot threads but nothing helped me to fix this. I'm a beginner so couldn't understand much where exactly the issue. Can some one help me with this ?"
23352,Wildly different quantization performance on different DenseNet169 models,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14 Mojave
- TensorFlow installed from (source or binary): binary (pip)
- TensorFlow version (use command below): 1.11.0
- Python version: 3.6.4

**Describe the current behavior**
I have two models that I have trained using Keras. The two models use the same architecture (Keras' DenseNet169 implementation), however have a different number of target classes (80 in one case, 200 in the other case).

* Converting both models to .pb format works just fine (identical performance in inference).

* Converting both models to .tflite format using TOCO works just fine (again, identical performance in inference).

* Converting the 80-class model to .tflite using quantization in TOCO works reasonably well (<1% drop in top 3 accuracy).

* Converting the 200-class model to .tflite using quantization in TOCO goes off the rails (~30% drop in top 3 accuracy).

I'm using an identical conversion process for both models.

`toco --graph_def_file frozen_graph.pb --output_file quantized_graph.tflite --inference_type FLOAT --inference_input_type FLOAT --output_format TFLITE --input_arrays input_1 --output_arrays output_node0 --quantize True`


**Describe the expected behavior**

I would expect that two models trained to similar accuracy on the same architecture should both quantize reasonably well, despite the number of prediction target classes in the fully-connected layers.

"
23351,fake_quant_min_max_args/vars works unexpected,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.10
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: -
- GPU model and memory: NVidia 1080 Ti


**Describe the current behavior**
import tensorflow as tf
s = tf.Session()
s.run([tf.fake_quant_with_min_max_args(123, 12, 123, narrow_range=False)])
OUTPUT: [111.0]

**Describe the expected behavior**
OUTPUT: [123.0]

**Code to reproduce the issue**
import tensorflow as tf
s = tf.Session()
s.run([tf.fake_quant_with_min_max_args(123, 12, 123, narrow_range=False)])

"
23350,"""EncapsulateTPUComputationsPass failed"" on a model with Conv3D and Dropout layers","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.11.0
- Python version: 3.6.6
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a
- TPU version: v3.8

The model fails to run on a TPU. Here is a code snippet to reproduce the issue:

```
from tensorflow.keras.layers import Conv3D, Dropout, Input
import tensorflow as tf
import tensorflow.keras.backend as K


K.set_image_data_format(""channels_last"")


BATCH_SIZE = 8
INPUT_SIZE = (80, 40, 20, 3)
OUTPUT_SIZE = (40, 20, 10, 32)


TPU = ""<tpu-name>""
TPU_ZONE = ""europe-west4-a""
GCP_PROJECT = ""<gcp-project>""
MODEL_DIR = ""gs://<gcs-bucket>""


def input_fn(params):
    return (
        tf.random_uniform((BATCH_SIZE, *INPUT_SIZE)),
        tf.random_uniform((BATCH_SIZE, *OUTPUT_SIZE)))


def model_fn(features, labels, mode, params):
    x = Input(tensor=features)
    x = Conv3D(filters=32, kernel_size=(5, 5, 5), padding='same', strides=2)(x)
    x = Dropout(0.5)(x)
    loss = tf.losses.mean_squared_error(labels, x)
    optimizer = tf.train.AdamOptimizer()
    optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)
    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())
    return tf.contrib.tpu.TPUEstimatorSpec(mode=mode, loss=loss, train_op=train_op)


tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU,
                                                                      zone=TPU_ZONE,
                                                                      project=GCP_PROJECT)

session_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)
tpu_config = tf.contrib.tpu.TPUConfig(iterations_per_loop=1, num_shards=8)
run_config = tf.contrib.tpu.RunConfig(cluster=tpu_cluster_resolver,
                                      model_dir=MODEL_DIR,
                                      save_checkpoints_secs=3600,
                                      session_config=session_config,
                                      tpu_config=tpu_config)

estimator = tf.contrib.tpu.TPUEstimator(
    model_fn=model_fn,
    use_tpu=True,
    config=run_config,
    train_batch_size=8)
estimator.train(input_fn=input_fn, max_steps=2)
```

Full stack trace:

```
WARNING:tensorflow:Estimator's model_fn (<function model_fn at 0x7fdd54a11158>) includes params argument, but params are not passed to Estimator.
2018-10-29 13:02:51.199228: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:349] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.
WARNING:tensorflow:Reraising captured error
Traceback (most recent call last):
  File ""/opt/miniconda3/envs/tpu-tf-1.11/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1292, in _do_call
    return fn(*args)
  File ""/opt/miniconda3/envs/tpu-tf-1.11/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1275, in _run_fn
    self._extend_graph()
  File ""/opt/miniconda3/envs/tpu-tf-1.11/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1312, in _extend_graph
    tf_session.ExtendSession(self._session)
tensorflow.python.framework.errors_impl.InvalidArgumentError: {{node gradients/dropout/cond/dropout/div_grad/Neg/Enter}} has inputs from different frames. The input {{node dropout/cond/dropout/div/Switch}} is in frame 'while_context'. The input {{node TPUReplicateMetadata}} is in frame ''.
        EncapsulateTPUComputationsPass failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""dropout_issue.py"", line 54, in <module>
    estimator.train(input_fn=input_fn, max_steps=2)
  File ""/opt/miniconda3/envs/tpu-tf-1.11/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py"", line 2400, in train
    rendezvous.raise_errors()
  File ""/opt/miniconda3/envs/tpu-tf-1.11/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/error_handling.py"", line 128, in raise_errors
    six.reraise(typ, value, traceback)
  File ""/opt/miniconda3/envs/tpu-tf-1.11/lib/python3.6/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/opt/miniconda3/envs/tpu-tf-1.11/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py"", line 2394, in train
    saving_listeners=saving_listeners
  File ""/opt/miniconda3/envs/tpu-tf-1.11/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 356, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/opt/miniconda3/envs/tpu-tf-1.11/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1181, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/opt/miniconda3/envs/tpu-tf-1.11/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1215, in _train_model_default
    saving_listeners)
  File ""/opt/miniconda3/envs/tpu-tf-1.11/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1406, in _train_with_estimator_spec
    log_step_count_steps=self._config.log_step_count_steps) as mon_sess:
  File ""/opt/miniconda3/envs/tpu-tf-1.11/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 504, in MonitoredTrainingSession
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/opt/miniconda3/envs/tpu-tf-1.11/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/opt/miniconda3/envs/tpu-tf-1.11/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File ""/opt/miniconda3/envs/tpu-tf-1.11/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File ""/opt/miniconda3/envs/tpu-tf-1.11/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session
    return self._sess_creator.create_session()
  File ""/opt/miniconda3/envs/tpu-tf-1.11/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session
    self.tf_sess = self._session_creator.create_session()
  File ""/opt/miniconda3/envs/tpu-tf-1.11/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 566, in create_session
    init_fn=self._scaffold.init_fn)
  File ""/opt/miniconda3/envs/tpu-tf-1.11/lib/python3.6/site-packages/tensorflow/python/training/session_manager.py"", line 287, in prepare_session
    sess.run(init_op, feed_dict=init_feed_dict)
  File ""/opt/miniconda3/envs/tpu-tf-1.11/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 887, in run
    run_metadata_ptr)
  File ""/opt/miniconda3/envs/tpu-tf-1.11/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1110, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/opt/miniconda3/envs/tpu-tf-1.11/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run
    run_metadata)
  File ""/opt/miniconda3/envs/tpu-tf-1.11/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: {{node gradients/dropout/cond/dropout/div_grad/Neg/Enter}} has inputs from different frames. The input {{node dropout/cond/dropout/div/Switch}} is in frame 'while_context'. The input {{node TPUReplicateMetadata}} is in frame ''.
        EncapsulateTPUComputationsPass failed
```

"
23349,my CNN models print NAN. what is reason??? i don't know what is it.,"I made a CNN model.
All results are output as ""nan"".
What's wrong? Please help me.

here is my repo
https://github.com/pervin0527/pervin0527 

def get_filename_set(data_set):
    labels = []
    filename_set = []

    with open(FLAGS.data_dir + '/labels.txt') as f:
        for line in f:
            inner_list = [elt.strip() for elt in line.split(',')]
            labels += inner_list

    for i, lable in enumerate(labels):
        list = os.listdir(FLAGS.data_dir  + '/' + data_set + '/' + lable)
        for filename in list:
            filename_set.append([i, FLAGS.data_dir  + '/' + data_set + '/' + lable + '/' + filename])

    random.shuffle(filename_set)
    return filename_set

def read_jpeg(filename):
    value = tf.read_file(filename)
    decoded_image = tf.image.decode_jpeg(value, channels=FLAGS.depth)
    resized_image = tf.image.resize_images(decoded_image, FLAGS.raw_height, FLAGS.raw_width)
    resized_image = tf.cast(resized_image, tf.uint8)

    return resized_image

def convert_images(sess, data_set):
    filename_set = get_filename_set(data_set)

    with open('./data/' + data_set + '_data.bin', 'wb') as f:
        for i in range(0, len(filename_set)):
            resized_image = read_jpeg(filename_set[i][1])

            try:
                image = sess.run(resized_image)
            except Exception as e:
                print e.message
                continue

            #plt.imshow(np.reshape(image.data, [FLAGS.raw_height, FLAGS.raw_width, FLAGS.depth]))
            #plt.show()

            print i, filename_set[i][0], image.shape
            f.write(chr(filename_set[i][0]))
            f.write(image.data)

def read_raw_images(sess, data_set):
    filename = ['./data/' + data_set + '_data.bin']
    filename_queue = tf.train.string_input_producer(filename)

    record_bytes = (FLAGS.raw_height) * (FLAGS.raw_width) * FLAGS.depth + 1
    reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)
    key, value = reader.read(filename_queue)
    record_bytes = tf.decode_raw(value, tf.uint8)

    tf.train.start_queue_runners(sess=sess)

    for i in range(0, 100):
        result = sess.run(record_bytes)
        print i, result[0]
        image = result[1:len(result)]

        #plt.imshow(np.reshape(image, [FLAGS.raw_height, FLAGS.raw_width, FLAGS.depth]))
        #plt.show()



FLAGS = tf.app.flags.FLAGS

def weight_variable(shape, name):
    with tf.device('/cpu:0'):
        initial = tf.truncated_normal(shape, stddev=0.1)
        var = tf.Variable(initial, name=name)
    return var

def bias_variable(shape, name):
    with tf.device('/cpu:0'):
        initial = tf.constant(0.1, shape=shape)
        var = tf.Variable(initial, name=name)
    return var

def conv2d(x, W):
    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')

def max_pool(x, height, width):
    return tf.nn.max_pool(x, ksize=[1, height, width, 1], strides=[1, height, width, 1], padding='SAME')

def make_network(images, labels, keep_prob):
    num_conv = FLAGS.num_conv
    kernel_size = FLAGS.kernel_size
    pool_size = FLAGS.pool_size
    num_map = FLAGS.num_map
    num_fc_layer = FLAGS.num_fc_layer
    num_fc_input = FLAGS.num_fc_input

    height = FLAGS.height
    width = FLAGS.width
    prev_num_map = FLAGS.depth
    h_pool = images

    for i in range(num_conv):
        W_conv = weight_variable([kernel_size, kernel_size, prev_num_map, num_map], 'W_conv' + str(i+1))
        b_conv = bias_variable([num_map], 'b_conv' + str(i+1))
        h_conv = tf.nn.relu(conv2d(h_pool, W_conv) + b_conv)
        h_pool = max_pool(h_conv, pool_size, pool_size)
        prev_num_map = num_map
        num_map *= 2
        height /= 2
        width /= 2

    num_map /= 2
    h_fc_input = tf.reshape(h_pool, [-1, height * width * num_map])
    prev_num_fc_input = height * width * num_map

    for i in range(num_fc_layer):
        W_fc = weight_variable([prev_num_fc_input, num_fc_input], 'W_fc' + str(i+1))
        b_fc = bias_variable([num_fc_input], 'b_fc' + str(i+1))
        h_fc = tf.nn.relu(tf.matmul(h_fc_input, W_fc) + b_fc)
        h_fc_input = tf.nn.dropout(h_fc, keep_prob)
        prev_num_fc_input = num_fc_input
        num_fc_input /= 2

    num_fc_input *= 2
    W_fc = weight_variable([num_fc_input, FLAGS.num_class], 'W_fc' + str(i+2))
    b_fc = bias_variable([FLAGS.num_class], 'b_fc' + str(i+2))

    hypothesis = tf.matmul(h_fc_input, W_fc) + b_fc
    cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(hypothesis, labels))
    train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(cross_entropy)

    return hypothesis, cross_entropy, train_step"
23348,"CPU memory slowly filling, during inference, cannot seem to find the reason","**System information**
- We have made a specific adaptation of a inference script, that reads in tensorflow records, pushes the data through the network and then stores the result.
- Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): default Ubuntu repositories
- TensorFlow version (use command below): 1.3
- Python version: 2.7
- CUDA/cuDNN version: Cuda 8.0 CuDNN 6.0
- GPU model and memory: NVIDIA P5000 - 16GB

**Describe the current behavior**

The script runs slow but fine. However, CPU memory of the system is slowly clogging up and system will fail.

**Describe the expected behavior**

We expected the code to run through all data parts without issues.

**Code to reproduce the issue**

```
""""""Applies model to all examples in a train record.

""""""

import os
import numpy as np
import tensorflow as tf

import constants
import model
import util

_RESOLUTIONS = ['5cm', '9cm', '19cm']
_INPUT_FEATURE = 'input_sdf'
_TARGET_FEATURE = 'target_df'
_TARGET_SEM_FEATURE = 'target_sem'
_TRAIN_FEATURE = 'samples'
_TRAIN_SEM_FEATURE = 'samples_sem'

flags = tf.flags
FLAGS = flags.FLAGS
flags.DEFINE_string('base_dir', '',
                    'Root directory. Expects a directory containing the model.')
flags.DEFINE_string('model_checkpoint', '',
                    'Model checkpoint to use (empty for latest).')
flags.DEFINE_string('data_filepattern', '/tmp/data/train_*.tfrecords',
                    'Training data file pattern.')
flags.DEFINE_string('output_folder', FLAGS.base_dir + '/data/vox5-9-19-dim3D_edited/',
                    'Folder in which to put the new tfrecords.')

# Parameters for applying model.
flags.DEFINE_integer('height_input', 64, 'Input block y dim.')
flags.DEFINE_integer('hierarchy_level', 1, 'Hierachy level (1: finest level).')
flags.DEFINE_bool('is_base_level', True, 'If base level of hierarchy.')
flags.DEFINE_integer('num_quant_levels', 256, 'Number of quantization bins.')
flags.DEFINE_integer('num_total_hierarchy_levels', 1,
                     'Number of total hierarchy levels.')
flags.DEFINE_integer('stored_dim_block', 64,
                     'Stored data block x/z dim, high-resolution.')
flags.DEFINE_integer('stored_height_block', 64,
                     'Stored data block y dim, high-resolution.')
flags.DEFINE_integer('p_norm', 1, 'P-norm loss (0 to disable).')
flags.DEFINE_bool('predict_semantics', True,
                  'Also predict semantic labels per-voxel.')
flags.DEFINE_float('temperature', 100.0, 'Softmax temperature for sampling.')


def read_record(filename):
    for serialized_example in tf.python_io.tf_record_iterator(filename):
	samples_lo = np.zeros([FLAGS.stored_dim_block // 2, FLAGS.stored_height_block // 2,
                               FLAGS.stored_dim_block // 2, 2])
        samples_sem_lo = np.zeros([FLAGS.stored_dim_block // 2, FLAGS.stored_height_block // 2,
                                   FLAGS.stored_dim_block // 2], dtype=np.uint8)
        if not FLAGS.is_base_level:
            key_samples = 'samples_' + _RESOLUTIONS[FLAGS.hierarchy_level]
            key_samples_sem = 'sem_samples_' + _RESOLUTIONS[FLAGS.hierarchy_level]
            spec = {
                'data':
                    tf.FixedLenFeature((), tf.string),
                key_samples:
                    tf.FixedLenFeature((FLAGS.stored_dim_block // 2, FLAGS.stored_height_block // 2,
                                        FLAGS.stored_dim_block // 2, 1), tf.float32),
                key_samples_sem:
                    tf.FixedLenFeature((), tf.string)
            }
            example = tf.parse_single_example(serialized_example, spec)
            samples_lo = example[key_samples]
            samples_sem_lo = example[key_samples_sem]
            samples_sem_lo = tf.decode_raw(samples_sem_lo, tf.uint8)
            samples_sem_lo = tf.reshape(samples_sem_lo, [
                FLAGS.stored_dim_block // 2, FLAGS.stored_height_block // 2, FLAGS.stored_dim_block // 2])

            serialized_example = example['data']
	
        # Parse sequence example.
        key_input = _RESOLUTIONS[FLAGS.hierarchy_level - 1] + '_' + _INPUT_FEATURE
        key_target = _RESOLUTIONS[FLAGS.hierarchy_level - 1] + '_' + _TARGET_FEATURE
        key_target_sem = _RESOLUTIONS[FLAGS.hierarchy_level - 1] + '_' + _TARGET_SEM_FEATURE
	
        sequence_features_spec = {
            key_input:
                tf.FixedLenFeature(
                    (FLAGS.stored_dim_block, FLAGS.stored_height_block, FLAGS.stored_dim_block, 1),
                    tf.float32),
            key_target:
                tf.FixedLenFeature(
                    (FLAGS.stored_dim_block, FLAGS.stored_height_block, FLAGS.stored_dim_block, 1),
                    tf.float32),
            key_target_sem:
                tf.FixedLenFeature((), tf.string)
        }
	print ""did it""
        example = tf.parse_single_example(serialized_example, sequence_features_spec)
	print ""did it""
        input_sdf_tensor = example[key_input]
	
        input_sdf = input_sdf_tensor.eval()
        input_sdf = np.squeeze(input_sdf, axis = 3)
        input_sdf = util.preprocess_sdf(input_sdf, constants.TRUNCATION)

        yield input_sdf, samples_lo, samples_sem_lo, serialized_example


def predict_from_model(logit_groups_geometry, logit_groups_semantics,
                       temperature):
    """"""Reconstruct predicted geometry and semantics from model output.""""""
    predictions_geometry_list = []
    for logit_group in logit_groups_geometry:
        if FLAGS.p_norm > 0:
            predictions_geometry_list.append(logit_group[:, :, :, :, 0])
        else:
            logit_group_shape = logit_group.shape_as_list()
            logit_group = tf.reshape(logit_group, [-1, logit_group_shape[-1]])
            samples = tf.multinomial(temperature * logit_group, 1)
            predictions_geometry_list.append(
                tf.reshape(samples, logit_group_shape[:-1]))
    predictions_semantics_list = []
    if FLAGS.predict_semantics:
        for logit_group in logit_groups_semantics:
            predictions_semantics_list.append(tf.argmax(logit_group, 4))
    else:
        predictions_semantics_list = [
                                         tf.zeros(shape=predictions_geometry_list[0].shape, dtype=tf.uint8)
                                     ] * len(predictions_geometry_list)
    return predictions_geometry_list, predictions_semantics_list


def create_dfs_from_output(output_df):
    """"""Rescales model output to distance fields (in voxel units).""""""
    if FLAGS.p_norm > 0:
        output_df = constants.TRUNCATION * (output_df[0, :, :, :, 0] + 1)
    else:
        output_df = (output_df[0, :, :, :, 0] + 1) * 0.5 * (
            FLAGS.num_quant_levels - 1)
        output_df = util.dequantize(output_df, FLAGS.num_quant_levels,
                                    constants.TRUNCATION)
    return output_df


def create_model(scene_dim_x, scene_dim_y, scene_dim_z):
    """"""Init model graph for scene.""""""
    input_placeholder = tf.placeholder(
        tf.float32,
        shape=[1, scene_dim_z, scene_dim_y, scene_dim_x, 2],
        name='pl_scan')
    target_placeholder = tf.placeholder(
        tf.float32,
        shape=[1, scene_dim_z, scene_dim_y, scene_dim_x, 2],
        name='pl_target')
    target_lo_placeholder = tf.placeholder(
        tf.float32,
        shape=[1, scene_dim_z // 2, scene_dim_y // 2, scene_dim_x // 2, 2],
        name='pl_target_lo')
    target_sem_placeholder = tf.placeholder(
        tf.uint8,
        shape=[1, scene_dim_z, scene_dim_y, scene_dim_x],
        name='pl_target_sem')
    target_sem_lo_placeholder = tf.placeholder(
        tf.uint8,
        shape=[1, scene_dim_z // 2, scene_dim_y // 2, scene_dim_x // 2],
        name='pl_target_sem_lo')
    # No previous level input if at base level.
    if FLAGS.is_base_level:
        target_scan_low_resolution = None
        target_semantics_low_resolution = None
    else:
        target_scan_low_resolution = target_lo_placeholder
        target_semantics_low_resolution = target_sem_lo_placeholder
    logits = model.model(
        input_scan=input_placeholder,
        target_scan_low_resolution=target_scan_low_resolution,
        target_scan=target_placeholder,
        target_semantics_low_resolution=target_semantics_low_resolution,
        target_semantics=target_sem_placeholder,
        num_quant_levels=FLAGS.num_quant_levels,
        predict_semantics=FLAGS.predict_semantics,
        use_p_norm=FLAGS.p_norm > 0)
    return (input_placeholder, target_placeholder, target_lo_placeholder,
            target_sem_placeholder, target_sem_lo_placeholder, logits)


def main(_):
    model_path = FLAGS.base_dir
    output_folder = FLAGS.output_folder
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    if FLAGS.model_checkpoint:
        checkpoint_path = os.path.join(model_path, FLAGS.model_checkpoint)
        print ""Used model: "" + str(checkpoint_path)
    else:
        checkpoint_path = tf.train.latest_checkpoint(model_path)
        print ""Used model: "" + str(checkpoint_path)	

    # Loop over all tfrecords in training data
    data_filepattern = FLAGS.data_filepattern
    if not isinstance(data_filepattern, list):
        data_filepattern = [data_filepattern]
    # Get filenames matching filespec.
    tf.logging.info('data_filepattern: %s', data_filepattern)
    filenames = []
    for p in data_filepattern:
        filenames.extend(tf.gfile.Glob(p))
    tf.logging.info('filenames: %s', filenames)

    for filename in filenames:
        tf.reset_default_graph()

        # Init model.
        (input_placeholder, target_placeholder, target_lo_placeholder,
         target_sem_placeholder, target_sem_lo_placeholder, logits) = create_model(
            FLAGS.stored_dim_block, FLAGS.stored_height_block, FLAGS.stored_dim_block)
        logit_groups_geometry = logits['logits_geometry']
        logit_groups_semantics = logits['logits_semantics']
        feature_groups = logits['features']

        predictions_geometry_list, predictions_semantics_list = predict_from_model(
            logit_groups_geometry, logit_groups_semantics, FLAGS.temperature)


        config = tf.ConfigProto(device_count={'GPU': 0})
        init_op = tf.group(tf.global_variables_initializer(),
                           tf.local_variables_initializer())

        with tf.Session(config=config) as session:
            session.run(init_op)
	    
            assign_fn = tf.contrib.framework.assign_from_checkpoint_fn(
                checkpoint_path, tf.contrib.framework.get_variables_to_restore())
            assign_fn(session)
   
            filename_out = os.path.join(output_folder, os.path.basename(filename))
            print(filename_out)
            writer = tf.python_io.TFRecordWriter(filename_out)

            for input_sdf, samples_lo, samples_sem_lo, serialized_example in read_record(filename):             

		# Make batch size 1 to input to model.
                input_sdf = input_sdf[np.newaxis, :, :, :, :]
                samples_lo = samples_lo[np.newaxis, :, :, :, :]
                samples_sem_lo = samples_sem_lo[np.newaxis, :, :, :]
                output_df = np.ones(shape=input_sdf.shape)
                output_df[:, :, :, :, 0] *= constants.TRUNCATION  # Fill with truncation, known values.
                output_sem = np.zeros(
                    shape=[1, FLAGS.stored_dim_block, FLAGS.stored_height_block, FLAGS.stored_dim_block],
                    dtype=np.uint8)

                # First get features.
                feed_dict = {
                    input_placeholder: input_sdf,
                    target_lo_placeholder: samples_lo,
                    target_placeholder: output_df,
                    target_sem_lo_placeholder: samples_sem_lo,
                    target_sem_placeholder: output_sem
                }
		
                # Cache these features.
                feature_groups_ = session.run(feature_groups, feed_dict)

                for n in range(8):
                    tf.logging.info('Predicting group [%d/%d]', n + 1, 8)
                    # Predict
                    feed_dict[feature_groups[n]] = feature_groups_[n]
                    predictions = session.run(
                        {
                            'prediction_geometry': predictions_geometry_list[n],
                            'prediction_semantics': predictions_semantics_list[n]
                        },
                        feed_dict=feed_dict)
                    prediction_geometry = predictions['prediction_geometry']
                    prediction_semantics = predictions['prediction_semantics']
                    # Put into [-1,1] for next group.
                    if FLAGS.p_norm == 0:
                        prediction_geometry = prediction_geometry.astype(np.float32) / (
                            (FLAGS.num_quant_levels - 1) / 2.0) - 1.0

                    util.assign_voxel_group(output_df, prediction_geometry,
                                            n + 1)
                    if FLAGS.predict_semantics:
                        util.assign_voxel_group(output_sem,
                                                prediction_semantics, n + 1)

                # Final outputs.
                output_sem = output_sem[0]
                output_df = create_dfs_from_output(output_df)  # Fill with truncation, known values.

                # Export predictions
                key_samples = 'samples_' + _RESOLUTIONS[FLAGS.hierarchy_level - 1]
                key_samples_sem = 'sem_samples_' + _RESOLUTIONS[FLAGS.hierarchy_level - 1]
                out_feature = {
                    'data': util.bytes_feature(serialized_example),
                    key_samples: util.float_feature(output_df.flatten().tolist()),
                    key_samples_sem: util.bytes_feature(output_sem.flatten().tobytes())
                }
                example = tf.train.Example(features=tf.train.Features(feature=out_feature))
                writer.write(example.SerializeToString())
        session.close()

if __name__ == '__main__':
    tf.app.run(main)
```

There are no errors reported. Any help welcome."
23347,version ？,"ubuntu16.04  cuda9.2 cudnn7.1   tensorflow1.11  ，，
import tensorflow   error  "
23346,"Installation error in MacOs,Throw error about numpy","I ran ""import tensorflow as tf"" in pyCharm. and Throw the errors.

> Using TensorFlow backend.
RuntimeError: module compiled against API version 0xc but this version of numpy is 0xb
ImportError: numpy.core.multiarray failed to import
ImportError: numpy.core.umath failed to import
ImportError: numpy.core.umath failed to import
2018-10-29 17:42:25.858222: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr 

I using anaconda, my python version is 3.6, numpy version is 1.13.3. My System is MacOS"
23345,"how to use DATASET to replace function feed_dict,because it is so slow","when I learn a tensorflow project,find one line code:

`cls_prob, box_pred = sess.run([output_cls_prob, output_box_pred], feed_dict={input_img: blob})`

But, this line code It took a lot of time. (use CPU need 15 seconds...┭┮﹏┭┮)

By consulting information, I find use function 'dataset' could solve this problem which took a lot of time, How should I use it?

source of 'blob':

`img = cv2.imread('./imgs/001.jpg')
img_scale = float(600) / min(img_data.shape[0], img_data.shape[1])
if np.round(img_scale * max(img_data.shape[0], img_data.shape[1])) > 1200:
    img_scale = float(1200) / max(img_data.shape[0], img_data.shape[1])
img_data = cv2.resize(img_data, None, None, fx=img_scale, fy=img_scale, interpolation=cv2.INTER_LINEAR)
img_orig = img_data.astype(np.float32, copy=True)
blob = np.zeros((1, img_data.shape[0], img_data.shape[1], 3),dtype=np.float32)
blob[0, 0:img_data.shape[0], 0:img_data.shape[1], :] = img_orig`

source of 'output_cls_prob'&'output_box_pred'&'input_img':

`# Actually,read PB model...
input_img = sess.graph.get_tensor_by_name('Placeholder:0')
output_cls_prob = sess.graph.get_tensor_by_name('Reshape_2:0')
output_box_pred = sess.graph.get_tensor_by_name('rpn_bbox_pred/Reshape_1:0')`

Parameter type：

> blob：type 'numpy.ndarray'
> 
> output_cls_prob：class 'tensorflow.python.framework.ops.Tensor'
> 
> output_box_pred：class 'tensorflow.python.framework.ops.Tensor'
> 
> input_img：class 'tensorflow.python.framework.ops.Tensor'

These codes are used for model computation.The model has been trained.

If you see the same problem on Stack Overflow，Yes, yes, that's what I asked myself.\(^o^)/~
"
23344,LayoutOptimizer optimizes to unsupported data_format for max_pool on CPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):  v1.11.0-rc2-4-gc19e29306c 1.11.0
- Python version: 3.6.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.0.176/7.1.3 
- GPU model and memory: NVIDIA Quadro M2200

**Describe the current behavior**

TensorFlow automatically replaces my MaxPoolingOp by one using another data format which is not supported, subsequently. 
This throws the `InvalidArgumentError` as seen below.
The problem arose when upgrading from TensorFlow 1.8 to 1.11 and from the error it seems to be caused by a `TransposeNHWCToNCHW-LayoutOptimizer`. When isolating the issue to reproduce it, it seems that `max_pool`, `dataset` and `squeeze` are involved to raise the error.  The only related (closed) issue I could find: #19497 ""NHWC convolution sometimes incorrectly considered NCHW"" 

`
InvalidArgumentError (see above for traceback): Default MaxPoolingOp only supports NHWC on device type CPU
	 [[{{node label_image_dilated}} = MaxPool[T=DT_INT32, data_format=""NCHW"", ksize=[1, 1, 3, 3], padding=""SAME"", strides=[1, 1, 1, 1]](label_image_dilated-0-TransposeNHWCToNCHW-LayoutOptimizer)]]
	 [[{{node OneShotIterator_2}} = OneShotIterator[container="""", dataset_factory=_make_dataset_UaZD9hBkHvg[], output_shapes=[[?,?]], output_types=[DT_INT32], shared_name="""", _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]`

**Describe the expected behavior**

To not throw the error as was the case for TensorFlow 1.8.

**Code to reproduce the issue**
```python 
import tensorflow as tf
import numpy as np

dataset_np = {'height_or_width': 512, 
              'indices': np.random.randint(low=0, high=512, size=(1000,2),dtype=np.int64),
              'values' : np.random.randint(low=0, high=1000, size=(1000,),dtype=np.int32)}
dataset = tf.data.Dataset.from_tensors(dataset_np)

def densify(element):
    label_image_sparse = tf.SparseTensor(indices = element['indices'], 
                                         values = element['values'], 
                                         dense_shape = tf.cast(tf.stack([element['height_or_width'],
                                                                         element['height_or_width']]),tf.int64))
    label_image = tf.sparse_tensor_to_dense(label_image_sparse, 
                                            default_value=-1, 
                                            validate_indices=False, 
                                            name='label_image')
    label_image_dilated = tf.squeeze(tf.nn.max_pool([tf.expand_dims(label_image, axis=-1)], 
                                                     data_format=""NHWC"", 
                                                     ksize= [1,3,3,1], 
                                                     strides = [1,1,1,1], 
                                                     padding='SAME', 
                                                     name='label_image_dilated'),[0,-1])
    return {'label_image_dilated':label_image_dilated}

dataset = dataset.map(densify)
element = dataset.make_one_shot_iterator().get_next()

with tf.Session() as sess:
    result = sess.run(element)
    print(result)
```

**Other info / logs**

"
23343,I have an one line text image.I want to split word by word. Can anyone help me,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
23342,Issue in building Tensorflow library on Windows machine,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 (64-bit)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Source
- TensorFlow version:1.11
- Python version:3.6
- Installed using virtualenv? pip? conda?: Git
- Bazel version (if compiling from source):No
- GCC/Compiler version (if compiling from source): CMake : 3.10.1
- CUDA/cuDNN version: No
- GPU model and memory: No



**Describe the problem**
I am trying to build tensorflow in windows. After downloading 1.11 from the git branch, I tried to build it using one of the link avilable online. CMake step was executed successfully and visual studion solution file was also generated.
When i tried to build it in visual studion 2017 it started giving me error of file not found.
Link followed to build the library:
https://joe-antognini.github.io/machine-learning/build-windows-tf

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. Created all the required folder as described in the link.
2. Run the build command:
cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=C:\swigwin-3.0.12\swig.exe -DPYTHON_EXECUTABLE=C:\Python365\python.exe  -DPYTHON_LIBRARIES=C:\Python365\libs\python36.lib
3.Open the solution in Visualstudion 2017 and build in release mode.
**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
********************************************
Visual Studio 2017 Logs:
********************************************
1>------ Build started: Project: zlib, Configuration: Release x64 ------
2>------ Build started: Project: farmhash, Configuration: Release x64 ------
3>------ Build started: Project: gif, Configuration: Release x64 ------
4>------ Build started: Project: sqlite, Configuration: Release x64 ------
5>------ Build started: Project: highwayhash, Configuration: Release x64 ------
6>------ Build started: Project: jpeg, Configuration: Release x64 ------
7>------ Build started: Project: lmdb, Configuration: Release x64 ------
8>------ Build started: Project: nsync, Configuration: Release x64 ------
1>Performing update step for 'zlib'
9>------ Build started: Project: farmhash_create_destination_dir, Configuration: Release x64 ------
10>------ Build started: Project: gif_create_destination_dir, Configuration: Release x64 ------
11>------ Build started: Project: sqlite_create_destination_dir, Configuration: Release x64 ------
5>Performing update step for 'highwayhash'
12>------ Build started: Project: png, Configuration: Release x64 ------
13>------ Build started: Project: protobuf, Configuration: Release x64 ------
14>------ Build started: Project: zlib_create_destination_dir, Configuration: Release x64 ------
15>------ Build started: Project: eigen, Configuration: Release x64 ------
16>------ Build started: Project: jpeg_create_destination_dir, Configuration: Release x64 ------
17>------ Build started: Project: highwayhash_create_destination_dir, Configuration: Release x64 ------
18>------ Build started: Project: png_create_destination_dir, Configuration: Release x64 ------
19>------ Build started: Project: lmdb_create_destination_dir, Configuration: Release x64 ------
13>Performing update step for 'protobuf'
20>------ Build started: Project: re2, Configuration: Release x64 ------
21>------ Build started: Project: double_conversion, Configuration: Release x64 ------
8>Performing update step for 'nsync'
22>------ Build started: Project: snappy, Configuration: Release x64 ------
23>------ Build started: Project: jpeg_copy_headers_to_destination, Configuration: Release x64 ------
24>------ Build started: Project: cub, Configuration: Release x64 ------
20>Performing update step for 're2'
21>Performing update step for 'double_conversion'
25>------ Build started: Project: nsync_create_destination_dir, Configuration: Release x64 ------
22>Performing update step for 'snappy'
26>------ Build started: Project: highwayhash_copy_headers_to_destination, Configuration: Release x64 ------
27>------ Build started: Project: grpc, Configuration: Release x64 ------
28>------ Build started: Project: jsoncpp, Configuration: Release x64 ------
29>------ Build started: Project: nsync_copy_headers_to_destination, Configuration: Release x64 ------
30>------ Build started: Project: lmdb_copy_headers_to_destination, Configuration: Release x64 ------
31>------ Build started: Project: png_copy_headers_to_destination, Configuration: Release x64 ------
32>------ Build started: Project: gif_copy_headers_to_destination, Configuration: Release x64 ------
27>Performing update step for 'grpc'
28>Performing update step for 'jsoncpp'
33>------ Build started: Project: sqlite_copy_headers_to_destination, Configuration: Release x64 ------
34>------ Build started: Project: gemmlowp, Configuration: Release x64 ------
35>------ Build started: Project: tf_protos_cc, Configuration: Release x64 ------
36>------ Build started: Project: fft2d, Configuration: Release x64 ------
37>------ Build started: Project: farmhash_copy_headers_to_destination, Configuration: Release x64 ------
38>------ Build started: Project: zlib_copy_headers_to_destination, Configuration: Release x64 ------
39>------ Build started: Project: create_cc_ops_header_dir, Configuration: Release x64 ------
40>------ Build started: Project: force_rebuild_target, Configuration: Release x64 ------
41>------ Build started: Project: tf_python_copy_scripts_to_destination, Configuration: Release x64 ------
40>Generating __force_rebuild
40>
40>Generating C:/Users/john/tensorflow/tensorflow/core/util/version_info.cc
35>debug_service.pb.cc
35>debugger_event_metadata.pb.cc
35>example.pb.cc
35>example_parser_configuration.pb.cc
35>feature.pb.cc
35>allocation_description.pb.cc
35>api_def.pb.cc
35>attr_value.pb.cc
40>fatal: Not a git repository: 'C:/Users/john/tensorflow/.git'
35>cost_graph.pb.cc
35>device_attributes.pb.cc
35>function.pb.cc
35>graph.pb.cc
35>graph_transfer_info.pb.cc
35>iterator.pb.cc
35>kernel_def.pb.cc
35>log_memory.pb.cc
35>node_def.pb.cc
35>op_def.pb.cc
35>reader_base.pb.cc
35>remote_fused_graph_execute_info.pb.cc
35>resource_handle.pb.cc
35>step_stats.pb.cc
35>summary.pb.cc
35>tensor.pb.cc
35>tensor_description.pb.cc
35>tensor_shape.pb.cc
35>tensor_slice.pb.cc
35>types.pb.cc
35>variable.pb.cc
35>versions.pb.cc
35>op_performance_data.pb.cc
35>boosted_trees.pb.cc
35>error_codes.pb.cc
35>profile.pb.cc
35>tfprof_log.pb.cc
35>tfprof_options.pb.cc
35>tfprof_output.pb.cc
35>checkpointable_object_graph.pb.cc
35>cluster.pb.cc
35>config.pb.cc
35>control_flow.pb.cc
35>critical_section.pb.cc
35>debug.pb.cc
35>device_properties.pb.cc
35>eager_service.pb.cc
35>master.pb.cc
35>master_service.pb.cc
35>meta_graph.pb.cc
35>named_tensor.pb.cc
35>queue_runner.pb.cc
35>rewriter_config.pb.cc
35>saved_model.pb.cc
35>saver.pb.cc
35>tensor_bundle.pb.cc
35>tensorflow_server.pb.cc
35>transport_options.pb.cc
35>worker.pb.cc
35>worker_service.pb.cc
35>event.pb.cc
35>example_proto_fast_parsing_test.pb.cc
35>memmapped_file_system.pb.cc
35>saved_tensor_slice.pb.cc
35>test_log.pb.cc
35>xla_service.pb.cc
35>backend_configs.pb.cc
35>hlo.pb.cc
35>hlo_profile_printer_data.pb.cc
35>xla.pb.cc
35>xla_data.pb.cc
35>learner.pb.cc
35>quantiles.pb.cc
35>split_info.pb.cc
35>tree_config.pb.cc
35>compilation_result.pb.cc
35>optimization_parameters.pb.cc
35>topology.pb.cc
35>tpu_embedding_config.pb.cc
35>tf_protos_cc.vcxproj -> C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\Release\tf_protos_cc.lib
42>------ Build started: Project: proto_text, Configuration: Release x64 ------
42>gen_proto_text_functions.cc
42>gen_proto_text_functions_lib.cc
42>path.obj : error LNK2019: unresolved external symbol ""void __cdecl absl::base_internal::ThrowStdOutOfRange(char const *)"" (?ThrowStdOutOfRange@base_internal@absl@@YAXPEBD@Z) referenced in function ""class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > __cdecl tensorflow::io::internal::JoinPathImpl(class std::initializer_list<class absl::string_view>)"" (?JoinPathImpl@internal@io@tensorflow@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$initializer_list@Vstring_view@absl@@@5@@Z)
42>path.obj : error LNK2019: unresolved external symbol ""public: unsigned __int64 __cdecl absl::string_view::rfind(char,unsigned __int64)const "" (?rfind@string_view@absl@@QEBA_KD_K@Z) referenced in function ""class absl::string_view __cdecl tensorflow::io::Extension(class absl::string_view)"" (?Extension@io@tensorflow@@YA?AVstring_view@absl@@V34@@Z)
42>collection_registry.obj : error LNK2019: unresolved external symbol ""class std::basic_ostream<char,struct std::char_traits<char> > & __cdecl absl::operator<<(class std::basic_ostream<char,struct std::char_traits<char> > &,class absl::string_view)"" (??6absl@@YAAEAV?$basic_ostream@DU?$char_traits@D@std@@@std@@AEAV12@Vstring_view@0@@Z) referenced in function ""public: class std::unique_ptr<class tensorflow::monitoring::CollectionRegistry::RegistrationHandle,struct std::default_delete<class tensorflow::monitoring::CollectionRegistry::RegistrationHandle> > __cdecl tensorflow::monitoring::CollectionRegistry::Register(class tensorflow::monitoring::AbstractMetricDef const *,class std::function<void __cdecl(class tensorflow::monitoring::MetricCollectorGetter)> const &)"" (?Register@CollectionRegistry@monitoring@tensorflow@@QEAA?AV?$unique_ptr@VRegistrationHandle@CollectionRegistry@monitoring@tensorflow@@U?$default_delete@VRegistrationHandle@CollectionRegistry@monitoring@tensorflow@@@std@@@std@@PEBVAbstractMetricDef@23@AEBV?$function@$$A6AXVMetricCollectorGetter@monitoring@tensorflow@@@Z@5@@Z)
42>str_util.obj : error LNK2019: unresolved external symbol ""public: unsigned __int64 __cdecl absl::string_view::find(char,unsigned __int64)const "" (?find@string_view@absl@@QEBA_KD_K@Z) referenced in function ""class std::vector<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::allocator<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > > __cdecl tensorflow::str_util::Split<struct tensorflow::str_util::AllowEmpty>(class absl::string_view,class absl::string_view,struct tensorflow::str_util::AllowEmpty)"" (??$Split@UAllowEmpty@str_util@tensorflow@@@str_util@tensorflow@@YA?AV?$vector@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@std@@Vstring_view@absl@@0UAllowEmpty@01@@Z)
42>C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\Release\proto_text.exe : fatal error LNK1120: 4 unresolved externals
42>Done building project ""proto_text.vcxproj"" -- FAILED.
43>------ Build started: Project: tf_core_framework, Configuration: Release x64 ------
43>Generating __force_rebuild
43>
43>Running C++ protocol buffer text compiler (proto_text) on tensorflow/core/example/example.proto
43>'Release\proto_text.exe' is not recognized as an internal or external command,
43>operable program or batch file.
43>C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Common7\IDE\VC\VCTargets\Microsoft.CppCommon.targets(171,5): error MSB6006: ""cmd.exe"" exited with code 9009.
43>Done building project ""tf_core_framework.vcxproj"" -- FAILED.
44>------ Build started: Project: tf_cc_op_gen_main, Configuration: Release x64 ------
45>------ Build started: Project: tf_core_cpu, Configuration: Release x64 ------
46>------ Build started: Project: tf_cc_framework, Configuration: Release x64 ------
47>------ Build started: Project: tf_python_op_gen_main, Configuration: Release x64 ------
44>cc_op_gen.cc
44>cc_op_gen_main.cc
46>ops.cc
46>scope.cc
47>python_op_gen.cc
47>python_op_gen_internal.cc
47>python_op_gen_main.cc
45>loader.cc
45>reader.cc
45>accumulate_n_optimizer.cc
45>allocator_retry.cc
45>base_collective_executor.cc
45>bfc_allocator.cc
45>buf_rendezvous.cc
45>build_graph_options.cc
47>c:\users\john\tensorflow\tensorflow\python\framework\python_op_gen_internal.cc(24): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/op_def.pb_text.h': No such file or directory
47>c:\users\john\tensorflow\tensorflow\python\framework\python_op_gen.cc(23): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/op_def.pb_text.h': No such file or directory
45>collective_executor_mgr.cc
44>c:\users\john\tensorflow\tensorflow\cc\framework\cc_op_gen.cc(28): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/types.pb_text.h': No such file or directory
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\base_collective_executor.cc)
45>collective_param_resolver_local.cc
44>Done building project ""tf_cc_op_gen_main.vcxproj"" -- FAILED.
48>------ Build started: Project: control_flow_ops_gen_cc, Configuration: Release x64 ------
49>------ Build started: Project: ctc_ops_gen_cc, Configuration: Release x64 ------
50>------ Build started: Project: cudnn_rnn_ops_gen_cc, Configuration: Release x64 ------
51>------ Build started: Project: data_flow_ops_gen_cc, Configuration: Release x64 ------
52>------ Build started: Project: image_ops_gen_cc, Configuration: Release x64 ------
45>collective_rma_local.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\accumulate_n_optimizer.cc)
45>collective_util.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\buf_rendezvous.cc)
45>copy_tensor.cc
46>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\cc\framework\scope.cc)
45>costmodel_manager.cc
47>Done building project ""tf_python_op_gen_main.vcxproj"" -- FAILED.
53>------ Build started: Project: random_ops_gen_cc, Configuration: Release x64 ------
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\collective_executor_mgr.cc)
45>debugger_state_interface.cc
45>device.cc
46>Done building project ""tf_cc_framework.vcxproj"" -- FAILED.
54>------ Build started: Project: io_ops_gen_cc, Configuration: Release x64 ------
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\collective_param_resolver_local.cc)
45>device_factory.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\collective_rma_local.cc)
45>device_mgr.cc
48>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
51>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
48>Done building project ""control_flow_ops_gen_cc.vcxproj"" -- FAILED.
49>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
55>------ Build started: Project: summary_ops_gen_cc, Configuration: Release x64 ------
52>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
51>Done building project ""data_flow_ops_gen_cc.vcxproj"" -- FAILED.
56>------ Build started: Project: sendrecv_ops_gen_cc, Configuration: Release x64 ------
52>Done building project ""image_ops_gen_cc.vcxproj"" -- FAILED.
49>Done building project ""ctc_ops_gen_cc.vcxproj"" -- FAILED.
57>------ Build started: Project: decode_proto_ops_gen_cc, Configuration: Release x64 ------
50>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
58>------ Build started: Project: remote_fused_graph_ops_gen_cc, Configuration: Release x64 ------
50>Done building project ""cudnn_rnn_ops_gen_cc.vcxproj"" -- FAILED.
59>------ Build started: Project: sdca_ops_gen_cc, Configuration: Release x64 ------
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\collective_util.cc)
45>device_resolver_local.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\copy_tensor.cc)
45>device_set.cc
53>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
58>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
57>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
53>Done building project ""random_ops_gen_cc.vcxproj"" -- FAILED.
60>------ Build started: Project: linalg_ops_gen_cc, Configuration: Release x64 ------
58>Done building project ""remote_fused_graph_ops_gen_cc.vcxproj"" -- FAILED.
61>------ Build started: Project: string_ops_gen_cc, Configuration: Release x64 ------
57>Done building project ""decode_proto_ops_gen_cc.vcxproj"" -- FAILED.
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\debugger_state_interface.cc)
62>------ Build started: Project: list_ops_gen_cc, Configuration: Release x64 ------
45>attr_builder.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\device.cc)
45>context.cc
45>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\eager\attr_builder.cc)
59>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
45>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\eager\attr_builder.cc)
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\device_mgr.cc)
45>eager_executor.cc
45>eager_operation.cc
59>Done building project ""sdca_ops_gen_cc.vcxproj"" -- FAILED.
63>------ Build started: Project: stateless_random_ops_gen_cc, Configuration: Release x64 ------
54>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
54>Done building project ""io_ops_gen_cc.vcxproj"" -- FAILED.
45>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\eager\eager_operation.cc)
64>------ Build started: Project: resource_variable_ops_gen_cc, Configuration: Release x64 ------
45>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\eager\eager_operation.cc)
55>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
62>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
45>execute.cc
60>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
55>Done building project ""summary_ops_gen_cc.vcxproj"" -- FAILED.
62>Done building project ""list_ops_gen_cc.vcxproj"" -- FAILED.
65>------ Build started: Project: encode_proto_ops_gen_cc, Configuration: Release x64 ------
66>------ Build started: Project: script_ops_gen_cc, Configuration: Release x64 ------
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\device_set.cc)
45>kernel_and_device.cc
60>Done building project ""linalg_ops_gen_cc.vcxproj"" -- FAILED.
67>------ Build started: Project: logging_ops_gen_cc, Configuration: Release x64 ------
61>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
61>Done building project ""string_ops_gen_cc.vcxproj"" -- FAILED.
68>------ Build started: Project: state_ops_gen_cc, Configuration: Release x64 ------
45>tensor_handle.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\device_resolver_local.cc)
45>eval_const_tensor.cc
66>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
56>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\eager\attr_builder.cc)
66>Done building project ""script_ops_gen_cc.vcxproj"" -- FAILED.
69>------ Build started: Project: lookup_ops_gen_cc, Configuration: Release x64 ------
64>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
45>executor.cc
56>Done building project ""sendrecv_ops_gen_cc.vcxproj"" -- FAILED.
70>------ Build started: Project: array_ops_gen_cc, Configuration: Release x64 ------
64>Done building project ""resource_variable_ops_gen_cc.vcxproj"" -- FAILED.
71>------ Build started: Project: manip_ops_gen_cc, Configuration: Release x64 ------
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\eager\context.cc)
45>executor_factory.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\eager\eager_executor.cc)
45>function.cc
67>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
63>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
67>Done building project ""logging_ops_gen_cc.vcxproj"" -- FAILED.
72>------ Build started: Project: rpc_ops_gen_cc, Configuration: Release x64 ------
63>Done building project ""stateless_random_ops_gen_cc.vcxproj"" -- FAILED.
73>------ Build started: Project: math_ops_gen_cc, Configuration: Release x64 ------
70>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\eager\eager_operation.cc)
70>Done building project ""array_ops_gen_cc.vcxproj"" -- FAILED.
74>------ Build started: Project: audio_ops_gen_cc, Configuration: Release x64 ------
45>gpu_id_manager.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\eager\execute.cc)
45>graph_execution_state.cc
68>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\eager\kernel_and_device.cc)
45>graph_optimizer.cc
68>Done building project ""state_ops_gen_cc.vcxproj"" -- FAILED.
75>------ Build started: Project: nn_ops_gen_cc, Configuration: Release x64 ------
65>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
71>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
65>Done building project ""encode_proto_ops_gen_cc.vcxproj"" -- FAILED.
76>------ Build started: Project: spectral_ops_gen_cc, Configuration: Release x64 ------
71>Done building project ""manip_ops_gen_cc.vcxproj"" -- FAILED.
77>------ Build started: Project: no_op_gen_cc, Configuration: Release x64 ------
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\eager\tensor_handle.cc)
45>graph_runner.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\executor.cc)
45>hierarchical_tree_broadcaster.cc
72>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
72>Done building project ""rpc_ops_gen_cc.vcxproj"" -- FAILED.
78>------ Build started: Project: batch_ops_gen_cc, Configuration: Release x64 ------
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\function.cc)
73>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
45>local_device.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\eval_const_tensor.cc)
45>lower_if_op.cc
73>Done building project ""math_ops_gen_cc.vcxproj"" -- FAILED.
79>------ Build started: Project: user_ops_gen_cc, Configuration: Release x64 ------
45>lower_while_op.cc
74>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
74>Done building project ""audio_ops_gen_cc.vcxproj"" -- FAILED.
80>------ Build started: Project: bitwise_ops_gen_cc, Configuration: Release x64 ------
77>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
77>Done building project ""no_op_gen_cc.vcxproj"" -- FAILED.
81>------ Build started: Project: parsing_ops_gen_cc, Configuration: Release x64 ------
75>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
75>Done building project ""nn_ops_gen_cc.vcxproj"" -- FAILED.
82>------ Build started: Project: sparse_ops_gen_cc, Configuration: Release x64 ------
69>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
69>Done building project ""lookup_ops_gen_cc.vcxproj"" -- FAILED.
83>------ Build started: Project: functional_ops_gen_cc, Configuration: Release x64 ------
76>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
78>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\graph_execution_state.cc)
76>Done building project ""spectral_ops_gen_cc.vcxproj"" -- FAILED.
45>memory_types.cc
84>------ Build started: Project: dataset_ops_gen_cc, Configuration: Release x64 ------
78>Done building project ""batch_ops_gen_cc.vcxproj"" -- FAILED.
85>------ Build started: Project: training_ops_gen_cc, Configuration: Release x64 ------
45>mkl_cpu_allocator.cc
45>optimization_registry.cc
79>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
79>Done building project ""user_ops_gen_cc.vcxproj"" -- FAILED.
86>------ Build started: Project: candidate_sampling_ops_gen_cc, Configuration: Release x64 ------
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\graph_runner.cc)
45>parallel_concat_optimizer.cc
85>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
81>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
82>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
85>Done building project ""training_ops_gen_cc.vcxproj"" -- FAILED.
81>Done building project ""parsing_ops_gen_cc.vcxproj"" -- FAILED.
87>------ Build started: Project: checkpoint_ops_gen_cc, Configuration: Release x64 ------
88>------ Build started: Project: boosted_trees_ops_gen_cc, Configuration: Release x64 ------
82>Done building project ""sparse_ops_gen_cc.vcxproj"" -- FAILED.
80>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
89>------ Build started: Project: set_ops_gen_cc, Configuration: Release x64 ------
80>Done building project ""bitwise_ops_gen_cc.vcxproj"" -- FAILED.
90>------ Build started: Project: remote_fused_graph_ops_gen_python, Configuration: Release x64 ------
83>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\graph_optimizer.cc)
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\local_device.cc)
45>placer.cc
45>pool_allocator.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\hierarchical_tree_broadcaster.cc)
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\lower_if_op.cc)
83>Done building project ""functional_ops_gen_cc.vcxproj"" -- FAILED.
45>process_function_library_runtime.cc
45>process_state.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\lower_while_op.cc)
45>process_util.cc
91>------ Build started: Project: sdca_ops_gen_python, Configuration: Release x64 ------
84>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
84>Done building project ""dataset_ops_gen_cc.vcxproj"" -- FAILED.
92>------ Build started: Project: random_ops_gen_python, Configuration: Release x64 ------
87>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
88>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
86>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
87>Done building project ""checkpoint_ops_gen_cc.vcxproj"" -- FAILED.
93>------ Build started: Project: set_ops_gen_python, Configuration: Release x64 ------
88>Done building project ""boosted_trees_ops_gen_cc.vcxproj"" -- FAILED.
86>Done building project ""candidate_sampling_ops_gen_cc.vcxproj"" -- FAILED.
94>------ Build started: Project: parsing_ops_gen_python, Configuration: Release x64 ------
95>------ Build started: Project: sparse_ops_gen_python, Configuration: Release x64 ------
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\optimization_registry.cc)
45>renamed_device.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\process_function_library_runtime.cc)
45>rendezvous_mgr.cc
90>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
92>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
90>Done building project ""remote_fused_graph_ops_gen_python.vcxproj"" -- FAILED.
95>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
91>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
96>------ Build started: Project: nn_ops_gen_python, Configuration: Release x64 ------
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\placer.cc)
91>Done building project ""sdca_ops_gen_python.vcxproj"" -- FAILED.
97>------ Build started: Project: math_ops_gen_python, Configuration: Release x64 ------
45>rendezvous_util.cc
95>Done building project ""sparse_ops_gen_python.vcxproj"" -- FAILED.
92>Done building project ""random_ops_gen_python.vcxproj"" -- FAILED.
98>------ Build started: Project: manip_ops_gen_python, Configuration: Release x64 ------
99>------ Build started: Project: lookup_ops_gen_python, Configuration: Release x64 ------
94>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
94>Done building project ""parsing_ops_gen_python.vcxproj"" -- FAILED.
100>------ Build started: Project: logging_ops_gen_python, Configuration: Release x64 ------
89>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
89>Done building project ""set_ops_gen_cc.vcxproj"" -- FAILED.
101>------ Build started: Project: tf_cc_ops, Configuration: Release x64 ------
100>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
100>Done building project ""logging_ops_gen_python.vcxproj"" -- FAILED.
102>------ Build started: Project: state_ops_gen_python, Configuration: Release x64 ------
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\parallel_concat_optimizer.cc)
45>ring_reducer.cc
45>scoped_allocator.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\renamed_device.cc)
45>scoped_allocator_mgr.cc
98>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
45>session_ref.cc
45>session_state.cc
102>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
98>Done building project ""manip_ops_gen_python.vcxproj"" -- FAILED.
103>------ Build started: Project: list_ops_gen_python, Configuration: Release x64 ------
102>Done building project ""state_ops_gen_python.vcxproj"" -- FAILED.
104>------ Build started: Project: stateless_random_ops_gen_python, Configuration: Release x64 ------
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\rendezvous_mgr.cc)
45>shape_refiner.cc
93>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
93>Done building project ""set_ops_gen_python.vcxproj"" -- FAILED.
105>------ Build started: Project: linalg_ops_gen_python, Configuration: Release x64 ------
103>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
103>Done building project ""list_ops_gen_python.vcxproj"" -- FAILED.
106>------ Build started: Project: string_ops_gen_python, Configuration: Release x64 ------
99>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
97>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
99>Done building project ""lookup_ops_gen_python.vcxproj"" -- FAILED.
107>------ Build started: Project: io_ops_gen_python, Configuration: Release x64 ------
96>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
97>Done building project ""math_ops_gen_python.vcxproj"" -- FAILED.
104>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
108>------ Build started: Project: summary_ops_gen_python, Configuration: Release x64 ------
45>stats_publisher_interface.cc
96>Done building project ""nn_ops_gen_python.vcxproj"" -- FAILED.
109>------ Build started: Project: image_ops_gen_python, Configuration: Release x64 ------
104>Done building project ""stateless_random_ops_gen_python.vcxproj"" -- FAILED.
110>------ Build started: Project: functional_ops_gen_python, Configuration: Release x64 ------
105>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
105>Done building project ""linalg_ops_gen_python.vcxproj"" -- FAILED.
111>------ Build started: Project: encode_proto_ops_gen_python, Configuration: Release x64 ------
106>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
107>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
106>Done building project ""string_ops_gen_python.vcxproj"" -- FAILED.
112>------ Build started: Project: rpc_ops_gen_python, Configuration: Release x64 ------
107>Done building project ""io_ops_gen_python.vcxproj"" -- FAILED.
113>------ Build started: Project: debug_ops_gen_python, Configuration: Release x64 ------
109>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
109>Done building project ""image_ops_gen_python.vcxproj"" -- FAILED.
114>------ Build started: Project: resource_variable_ops_gen_python, Configuration: Release x64 ------
108>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
112>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
108>Done building project ""summary_ops_gen_python.vcxproj"" -- FAILED.
115>------ Build started: Project: dataset_ops_gen_python, Configuration: Release x64 ------
112>Done building project ""rpc_ops_gen_python.vcxproj"" -- FAILED.
116>------ Build started: Project: spectral_ops_gen_python, Configuration: Release x64 ------
114>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
113>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
114>Done building project ""resource_variable_ops_gen_python.vcxproj"" -- FAILED.
117>------ Build started: Project: data_flow_ops_gen_python, Configuration: Release x64 ------
45>step_stats_collector.cc
113>Done building project ""debug_ops_gen_python.vcxproj"" -- FAILED.
118>------ Build started: Project: cudnn_rnn_ops_gen_python, Configuration: Release x64 ------
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\ring_reducer.cc)
45>sycl_allocator.cc
45>sycl_device.cc
45>sycl_device_context.cc
45>sycl_device_factory.cc
45>threadpool_device.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\shape_refiner.cc)
45>threadpool_device_factory.cc
45>debug.cc
115>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
115>Done building project ""dataset_ops_gen_python.vcxproj"" -- FAILED.
119>------ Build started: Project: script_ops_gen_python, Configuration: Release x64 ------
45>debug_callback_registry.cc
116>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
116>Done building project ""spectral_ops_gen_python.vcxproj"" -- FAILED.
111>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
120>------ Build started: Project: ctc_ops_gen_python, Configuration: Release x64 ------
110>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
111>Done building project ""encode_proto_ops_gen_python.vcxproj"" -- FAILED.
121>------ Build started: Project: control_flow_ops_gen_python, Configuration: Release x64 ------
110>Done building project ""functional_ops_gen_python.vcxproj"" -- FAILED.
122>------ Build started: Project: contrib_text_skip_gram_ops_gen_python, Configuration: Release x64 ------
117>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
119>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
117>Done building project ""data_flow_ops_gen_python.vcxproj"" -- FAILED.
119>Done building project ""script_ops_gen_python.vcxproj"" -- FAILED.
123>------ Build started: Project: contrib_tensor_forest_stats_ops_gen_python, Configuration: Release x64 ------
124>------ Build started: Project: contrib_tensor_forest_ops_gen_python, Configuration: Release x64 ------
45>debug_graph_utils.cc
125>------ Build started: Project: contrib_tensor_forest_model_ops_gen_python, Configuration: Release x64 ------
120>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
122>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
120>Done building project ""ctc_ops_gen_python.vcxproj"" -- FAILED.
126>------ Build started: Project: contrib_tensor_forest_hybrid_ops_gen_python, Configuration: Release x64 ------
122>Done building project ""contrib_text_skip_gram_ops_gen_python.vcxproj"" -- FAILED.
127>------ Build started: Project: contrib_seq2seq_beam_search_ops_gen_python, Configuration: Release x64 ------
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\threadpool_device_factory.cc)
45>debug_io_utils.cc
123>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
45>debug_node_key.cc
45>debugger_state_impl.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\common_runtime\threadpool_device.cc)
45>server_lib.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\debug\debug.cc)
45>algorithm.cc
45>colors.cc
123>Done building project ""contrib_tensor_forest_stats_ops_gen_python.vcxproj"" -- FAILED.
128>------ Build started: Project: contrib_rnn_lstm_ops_gen_python, Configuration: Release x64 ------
121>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
126>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
127>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
121>Done building project ""control_flow_ops_gen_python.vcxproj"" -- FAILED.
129>------ Build started: Project: contrib_rnn_gru_ops_gen_python, Configuration: Release x64 ------
45>control_flow.cc
126>Done building project ""contrib_tensor_forest_hybrid_ops_gen_python.vcxproj"" -- FAILED.
124>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
130>------ Build started: Project: contrib_resampler_ops_gen_python, Configuration: Release x64 ------
127>Done building project ""contrib_seq2seq_beam_search_ops_gen_python.vcxproj"" -- FAILED.
131>------ Build started: Project: contrib_periodic_resample_ops_gen_python, Configuration: Release x64 ------
124>Done building project ""contrib_tensor_forest_ops_gen_python.vcxproj"" -- FAILED.
132>------ Build started: Project: contrib_nearest_neighbor_ops_gen_python, Configuration: Release x64 ------
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\debug\debug_graph_utils.cc)
45>costmodel.cc
45>gradients.cc
125>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
125>Done building project ""contrib_tensor_forest_model_ops_gen_python.vcxproj"" -- FAILED.
133>------ Build started: Project: contrib_nccl_ops_gen_python, Configuration: Release x64 ------
131>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
131>Done building project ""contrib_periodic_resample_ops_gen_python.vcxproj"" -- FAILED.
134>------ Build started: Project: contrib_memory_stats_ops_gen_python, Configuration: Release x64 ------
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\debug\debugger_state_impl.cc)
130>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
130>Done building project ""contrib_resampler_ops_gen_python.vcxproj"" -- FAILED.
45>graph_constructor.cc
135>------ Build started: Project: contrib_layers_sparse_feature_cross_ops_gen_python, Configuration: Release x64 ------
128>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
128>Done building project ""contrib_rnn_lstm_ops_gen_python.vcxproj"" -- FAILED.
136>------ Build started: Project: contrib_input_pipeline_ops_gen_python, Configuration: Release x64 ------
101>Generating tensorflow/cc/ops/audio_ops.h, tensorflow/cc/ops/audio_ops.cc, tensorflow/cc/ops/audio_ops_internal.h, tensorflow/cc/ops/audio_ops_internal.cc
45>graph_def_builder_util.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\graph\gradients.cc)
45>graph_partition.cc
45>mkl_layout_pass.cc
101>'Release\audio_ops_gen_cc.exe' is not recognized as an internal or external command,
45>mkl_tfconversion_pass.cc
101>operable program or batch file.
133>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
45>optimizer_cse.cc
133>Done building project ""contrib_nccl_ops_gen_python.vcxproj"" -- FAILED.
137>------ Build started: Project: contrib_image_sirds_ops_gen_python, Configuration: Release x64 ------
45>quantize_training.cc
135>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
101>C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Common7\IDE\VC\VCTargets\Microsoft.CppCommon.targets(171,5): error MSB6006: ""cmd.exe"" exited with code 9009.
101>Done building project ""tf_cc_ops.vcxproj"" -- FAILED.
138>------ Build started: Project: tf_cc_while_loop, Configuration: Release x64 ------
135>Done building project ""contrib_layers_sparse_feature_cross_ops_gen_python.vcxproj"" -- FAILED.
139>------ Build started: Project: tf_cc, Configuration: Release x64 ------
132>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
136>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
132>Done building project ""contrib_nearest_neighbor_ops_gen_python.vcxproj"" -- FAILED.
140>------ Build started: Project: contrib_image_ops_gen_python, Configuration: Release x64 ------
136>Done building project ""contrib_input_pipeline_ops_gen_python.vcxproj"" -- FAILED.
134>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
141>------ Build started: Project: contrib_image_distort_image_ops_gen_python, Configuration: Release x64 ------
134>Done building project ""contrib_memory_stats_ops_gen_python.vcxproj"" -- FAILED.
142>------ Build started: Project: contrib_framework_variable_ops_gen_python, Configuration: Release x64 ------
129>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
129>Done building project ""contrib_rnn_gru_ops_gen_python.vcxproj"" -- FAILED.
143>------ Build started: Project: contrib_gcs_config_ops_gen_python, Configuration: Release x64 ------
137>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
137>Done building project ""contrib_image_sirds_ops_gen_python.vcxproj"" -- FAILED.
144>------ Build started: Project: contrib_factorization_factorization_ops_gen_python, Configuration: Release x64 ------
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\graph\graph_constructor.cc)
45>validate.cc
45>cluster.cc
138>while_loop.cc
45>virtual_cluster.cc
45>analytical_cost_estimator.cc
139>client_session.cc
139>array_grad.cc
139>data_flow_grad.cc
139>c:\users\john\tensorflow\tensorflow\cc\gradients\data_flow_grad.cc(16): fatal error C1083: Cannot open include file: 'tensorflow/cc/ops/data_flow_ops.h': No such file or directory
139>image_grad.cc
139>math_grad.cc
139>nn_grad.cc
139>c:\users\john\tensorflow\tensorflow\cc\gradients\nn_grad.cc(16): fatal error C1083: Cannot open include file: 'tensorflow/cc/ops/nn_ops.h': No such file or directory
139>coordinator.cc
139>c:\users\john\tensorflow\tensorflow\cc\gradients\math_grad.cc(19): fatal error C1083: Cannot open include file: 'tensorflow/cc/ops/array_ops_internal.h': No such file or directory
139>queue_runner.cc
139>grad_op_registry.cc
139>gradient_checker.cc
139>gradients.cc
139>c:\users\john\tensorflow\tensorflow\cc\gradients\array_grad.cc(18): fatal error C1083: Cannot open include file: 'tensorflow/cc/ops/array_ops_internal.h': No such file or directory
139>while_gradients.cc
142>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\graph\quantize_training.cc)
45>graph_memory.cc
142>Done building project ""contrib_framework_variable_ops_gen_python.vcxproj"" -- FAILED.
145>------ Build started: Project: contrib_factorization_clustering_ops_gen_python, Configuration: Release x64 ------
45>graph_properties.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\grappler\clusters\cluster.cc)
45>measuring_cost_estimator.cc
141>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
144>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
141>Done building project ""contrib_image_distort_image_ops_gen_python.vcxproj"" -- FAILED.
144>Done building project ""contrib_factorization_factorization_ops_gen_python.vcxproj"" -- FAILED.
146>------ Build started: Project: contrib_data_dataset_ops_gen_python, Configuration: Release x64 ------
147>------ Build started: Project: contrib_coder_ops_gen_python, Configuration: Release x64 ------
140>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
143>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
138>c:\users\john\tensorflow\tensorflow\cc\ops\while_loop.cc(19): fatal error C1083: Cannot open include file: 'tensorflow/cc/ops/control_flow_ops_internal.h': No such file or directory
140>Done building project ""contrib_image_ops_gen_python.vcxproj"" -- FAILED.
143>Done building project ""contrib_gcs_config_ops_gen_python.vcxproj"" -- FAILED.
148>------ Build started: Project: contrib_boosted_trees_training_ops_gen_python, Configuration: Release x64 ------
149>------ Build started: Project: contrib_boosted_trees_stats_accumulator_ops_gen_python, Configuration: Release x64 ------
139>c:\users\john\tensorflow\tensorflow\cc\gradients\image_grad.cc(19): fatal error C1083: Cannot open include file: 'tensorflow/cc/ops/image_ops_internal.h': No such file or directory
139>c:\users\john\tensorflow\tensorflow\cc\ops\standard_ops.h(19): fatal error C1083: Cannot open include file: 'tensorflow/cc/ops/array_ops.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\cc\framework\gradients.cc)
139>c:\users\john\tensorflow\tensorflow\cc\framework\while_gradients.cc(20): fatal error C1083: Cannot open include file: 'tensorflow/cc/ops/control_flow_ops_internal.h': No such file or directory
139>c:\users\john\tensorflow\tensorflow\cc\ops\standard_ops.h(19): fatal error C1083: Cannot open include file: 'tensorflow/cc/ops/array_ops.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\cc\framework\gradient_checker.cc)
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\grappler\clusters\virtual_cluster.cc)
45>op_level_cost_estimator.cc
138>Done building project ""tf_cc_while_loop.vcxproj"" -- FAILED.
150>------ Build started: Project: tf_c, Configuration: Release x64 ------
45>robust_stats.cc
45>virtual_placer.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\grappler\costs\analytical_cost_estimator.cc)
45>virtual_scheduler.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\grappler\costs\graph_memory.cc)
45>devices.cc
45>gen_node.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\grappler\costs\graph_properties.cc)
45>graph_analyzer.cc
45>c:\users\john\tensorflow\tensorflow\core\grappler\graph_analyzer\graph_analyzer.cc(20): fatal error C1083: Cannot open include file: 'absl/strings/str_format.h': No such file or directory
45>graph_analyzer_tool.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\grappler\costs\measuring_cost_estimator.cc)
45>sig_node.cc
45>c:\users\john\tensorflow\tensorflow\core\grappler\graph_analyzer\gen_node.cc(18): fatal error C1083: Cannot open include file: 'absl/strings/str_format.h': No such file or directory
45>graph_view.cc
150>c_api.cc
147>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
150>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C
150>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams'
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\grappler\costs\virtual_placer.cc)
147>Done building project ""contrib_coder_ops_gen_python.vcxproj"" -- FAILED.
45>grappler_item.cc
151>------ Build started: Project: contrib_boosted_trees_split_handler_ops_gen_python, Configuration: Release x64 ------
45>grappler_item_builder.cc
139>Done building project ""tf_cc.vcxproj"" -- FAILED.
152>------ Build started: Project: decode_proto_ops_gen_python, Configuration: Release x64 ------
151>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
151>Done building project ""contrib_boosted_trees_split_handler_ops_gen_python.vcxproj"" -- FAILED.
146>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
153>------ Build started: Project: contrib_boosted_trees_prediction_ops_gen_python, Configuration: Release x64 ------
146>Done building project ""contrib_data_dataset_ops_gen_python.vcxproj"" -- FAILED.
154>------ Build started: Project: contrib_boosted_trees_model_ops_gen_python, Configuration: Release x64 ------
149>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
149>Done building project ""contrib_boosted_trees_stats_accumulator_ops_gen_python.vcxproj"" -- FAILED.
155>------ Build started: Project: contrib_bigquery_reader_ops_gen_python, Configuration: Release x64 ------
148>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
152>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
148>Done building project ""contrib_boosted_trees_training_ops_gen_python.vcxproj"" -- FAILED.
156>------ Build started: Project: checkpoint_ops_gen_python, Configuration: Release x64 ------
152>Done building project ""decode_proto_ops_gen_python.vcxproj"" -- FAILED.
157>------ Build started: Project: array_ops_gen_python, Configuration: Release x64 ------
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\grappler\costs\virtual_scheduler.cc)
145>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
154>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
150>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory
145>Done building project ""contrib_factorization_clustering_ops_gen_python.vcxproj"" -- FAILED.
45>file_input_yielder.cc
45>c:\users\john\tensorflow\tensorflow\core\grappler\graph_analyzer\sig_node.cc(20): fatal error C1083: Cannot open include file: 'absl/strings/str_format.h': No such file or directory
45>mutable_graph_view.cc
45>op_types.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\grappler\grappler_item_builder.cc)
45>arithmetic_optimizer.cc
158>------ Build started: Project: candidate_sampling_ops_gen_python, Configuration: Release x64 ------
154>Done building project ""contrib_boosted_trees_model_ops_gen_python.vcxproj"" -- FAILED.
45>auto_parallel.cc
159>------ Build started: Project: boosted_trees_ops_gen_python, Configuration: Release x64 ------
150>Done building project ""tf_c.vcxproj"" -- FAILED.
153>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
160>------ Build started: Project: tf_c_python_api, Configuration: Release x64 ------
153>Done building project ""contrib_boosted_trees_prediction_ops_gen_python.vcxproj"" -- FAILED.
161>------ Build started: Project: bitwise_ops_gen_python, Configuration: Release x64 ------
45>custom_graph_optimizer_registry.cc
158>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
158>Done building project ""candidate_sampling_ops_gen_python.vcxproj"" -- FAILED.
162>------ Build started: Project: training_ops_gen_python, Configuration: Release x64 ------
155>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
159>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
155>Done building project ""contrib_bigquery_reader_ops_gen_python.vcxproj"" -- FAILED.
159>Done building project ""boosted_trees_ops_gen_python.vcxproj"" -- FAILED.
163>------ Build started: Project: user_ops_gen_python, Configuration: Release x64 ------
164>------ Build started: Project: batch_ops_gen_python, Configuration: Release x64 ------
156>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
156>Done building project ""checkpoint_ops_gen_python.vcxproj"" -- FAILED.
165>------ Build started: Project: audio_ops_gen_python, Configuration: Release x64 ------
157>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
161>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
157>Done building project ""array_ops_gen_python.vcxproj"" -- FAILED.
166>------ Build started: Project: contrib_boosted_trees_quantiles_ops_gen_python, Configuration: Release x64 ------
161>Done building project ""bitwise_ops_gen_python.vcxproj"" -- FAILED.
163>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
163>Done building project ""user_ops_gen_python.vcxproj"" -- FAILED.
45>filter_fusion.cc
45>fusion_utils.cc
45>graph_utils.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\grappler\optimizers\arithmetic_optimizer.cc)
45>latency_all_edges.cc
162>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
45>map_and_batch_fusion.cc
162>Done building project ""training_ops_gen_python.vcxproj"" -- FAILED.
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\grappler\optimizers\auto_parallel.cc)
45>map_and_filter_fusion.cc
45>map_fusion.cc
45>map_vectorization.cc
164>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
165>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
164>Done building project ""batch_ops_gen_python.vcxproj"" -- FAILED.
165>Done building project ""audio_ops_gen_python.vcxproj"" -- FAILED.
166>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj'
166>Done building project ""contrib_boosted_trees_quantiles_ops_gen_python.vcxproj"" -- FAILED.
167>------ Build started: Project: tf_python_ops, Configuration: Release x64 ------
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\grappler\optimizers\data\filter_fusion.cc)
45>noop_elimination.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\grappler\optimizers\data\latency_all_edges.cc)
45>shuffle_and_repeat_fusion.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\grappler\optimizers\data\map_and_batch_fusion.cc)
45>debug_stripper.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\grappler\optimizers\data\map_fusion.cc)
45>dependency_optimizer.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\grappler\optimizers\data\map_and_filter_fusion.cc)
45>evaluation_utils.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\grappler\optimizers\data\map_vectorization.cc)
45>function_optimizer.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\grappler\optimizers\data\noop_elimination.cc)
45>gpu_swapping_kernels.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\grappler\optimizers\data\shuffle_and_repeat_fusion.cc)
45>gpu_swapping_ops.cc
45>graph_optimizer_stage.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\grappler\optimizers\debug_stripper.cc)
45>graph_rewriter.cc
45>layout_optimizer.cc
167>Generating tf_python/tensorflow/python/ops/gen_audio_ops.py
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\grappler\optimizers\dependency_optimizer.cc)
45>loop_optimizer.cc
167>'Release\audio_ops_gen_python.exe' is not recognized as an internal or external command,
167>operable program or batch file.
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\grappler\optimizers\function_optimizer.cc)
45>memory_optimizer.cc
167>C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Common7\IDE\VC\VCTargets\Microsoft.CppCommon.targets(171,5): error MSB6006: ""cmd.exe"" exited with code 9009.
167>Done building project ""tf_python_ops.vcxproj"" -- FAILED.
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\grappler\optimizers\gpu_swapping_kernels.cc)
45>meta_optimizer.cc
45>model_pruner.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\grappler\optimizers\graph_optimizer_stage.cc)
45>remapper.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\grappler\optimizers\loop_optimizer.cc)
45>scoped_allocator_optimizer.cc
45>shape_optimizer.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\grappler\optimizers\layout_optimizer.cc)
45>static_schedule.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\grappler\optimizers\memory_optimizer.cc)
45>symbolic_shapes.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\grappler\optimizers\meta_optimizer.cc)
45>colocation.cc
45>frame.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\grappler\optimizers\shape_optimizer.cc)
45>functions.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\grappler\optimizers\remapper.cc)
45>scc.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\grappler\optimizers\scoped_allocator_optimizer.cc)
45>topological_sort.cc
45>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\grappler\optimizers\static_schedule.cc)
45>traversal.cc
45>Done building project ""tf_core_cpu.vcxproj"" -- FAILED.
168>------ Build started: Project: tf_core_kernels, Configuration: Release x64 ------
169>------ Build started: Project: tf_grappler, Configuration: Release x64 ------
170>------ Build started: Project: tf_core_distributed_runtime, Configuration: Release x64 ------
171>------ Build started: Project: tf_core_direct_session, Configuration: Release x64 ------
171>direct_session.cc
169>single_machine.cc
169>cost_analyzer.cc
169>model_analyzer.cc
170>base_rendezvous_mgr.cc
170>call_options.cc
170>cluster_function_library_runtime.cc
170>collective_param_resolver_distributed.cc
170>collective_rma_distributed.cc
170>device_resolver_distributed.cc
170>eager_service_impl.cc
170>graph_mgr.cc
170>local_master.cc
168>adjust_contrast_op.cc
168>adjust_hue_op.cc
168>adjust_saturation_op.cc
168>aggregate_ops.cc
168>argmax_op.cc
168>as_string_op.cc
168>attention_ops.cc
168>avgpooling_op.cc
170>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\distributed_runtime\collective_rma_distributed.cc)
170>master.cc
170>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\distributed_runtime\base_rendezvous_mgr.cc)
170>master_session.cc
169>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\python\grappler\cost_analyzer.cc)
170>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\distributed_runtime\eager\eager_service_impl.cc)
170>message_wrappers.cc
170>c:\users\john\tensorflow\tensorflow\core\distributed_runtime\message_wrappers.h(24): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/tensor.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\distributed_runtime\cluster_function_library_runtime.cc)
170>partial_run_mgr.cc
169>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\grappler\clusters\single_machine.cc)
169>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\python\grappler\model_analyzer.cc)
170>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\distributed_runtime\device_resolver_distributed.cc)
170>recent_request_ids.cc
171>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory
170>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\distributed_runtime\graph_mgr.cc)
170>remote_device.cc
170>c:\users\john\tensorflow\tensorflow\core\distributed_runtime\message_wrappers.h(24): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/tensor.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\distributed_runtime\collective_param_resolver_distributed.cc)
170>request_id.cc
169>Done building project ""tf_grappler.vcxproj"" -- FAILED.
171>Done building project ""tf_core_direct_session.vcxproj"" -- FAILED.
170>c:\users\john\tensorflow\tensorflow\core\distributed_runtime\message_wrappers.h(24): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/tensor.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\distributed_runtime\local_master.cc)
170>grpc_eager_client.cc
170>grpc_eager_service.cc
170>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\distributed_runtime\master_session.cc)
170>grpc_eager_service_impl.cc
170>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\distributed_runtime\master.cc)
170>grpc_channel.cc
170>c:\users\john\tensorflow\tensorflow\core\distributed_runtime\message_wrappers.h(24): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/tensor.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\distributed_runtime\message_wrappers.cc)
170>grpc_master_service.cc
170>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\distributed_runtime\remote_device.cc)
170>grpc_master_service_impl.cc
168>barrier_ops.cc
168>base64_ops.cc
170>c:\users\john\tensorflow\tensorflow\core\distributed_runtime\message_wrappers.h(24): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/tensor.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\distributed_runtime\partial_run_mgr.cc)
170>grpc_remote_master.cc
168>batch_kernels.cc
170>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\distributed_runtime\rpc\eager\grpc_eager_service_impl.cc)
170>grpc_remote_worker.cc
170>grpc_rpc_factory.cc
170>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\distributed_runtime\rpc\grpc_master_service.cc)
170>grpc_rpc_factory_registration.cc
170>grpc_server_lib.cc
170>c:\users\john\tensorflow\tensorflow\core\distributed_runtime\message_wrappers.h(24): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/tensor.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\distributed_runtime\rpc\grpc_remote_master.cc)
168>batch_matmul_op_complex.cc
170>grpc_session.cc
168>batch_matmul_op_real.cc
170>grpc_tensor_coding.cc
170>grpc_util.cc
170>c:\users\john\tensorflow\tensorflow\core\distributed_runtime\message_wrappers.h(24): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/tensor.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\distributed_runtime\rpc\grpc_remote_worker.cc)
170>grpc_worker_cache.cc
170>c:\users\john\tensorflow\tensorflow\core\distributed_runtime\message_wrappers.h(24): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/tensor.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\distributed_runtime\rpc\grpc_session.cc)
170>grpc_worker_service.cc
170>c:\users\john\tensorflow\tensorflow\core\distributed_runtime\message_wrappers.h(24): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/tensor.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\distributed_runtime\rpc\grpc_server_lib.cc)
170>grpc_worker_service_impl.cc
168>batch_norm_op.cc
170>rpc_rendezvous_mgr.cc
170>rpc_collective_executor_mgr.cc
170>c:\users\john\tensorflow\tensorflow\core\distributed_runtime\message_wrappers.h(24): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/tensor.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\distributed_runtime\rpc\grpc_worker_cache.cc)
170>scheduler.cc
170>session_mgr.cc
170>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\distributed_runtime\rpc\rpc_rendezvous_mgr.cc)
170>tensor_coding.cc
170>worker.cc
170>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\distributed_runtime\rpc\grpc_worker_service.cc)
170>worker_cache_logger.cc
170>worker_cache_partial.cc
168>fake_clock_env.cc
168>periodic_function.cc
168>batchtospace_op.cc
170>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\distributed_runtime\session_mgr.cc)
170>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\distributed_runtime\scheduler.cc)
170>worker_session.cc
170>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\distributed_runtime\rpc_collective_executor_mgr.cc)
168>bcast_ops.cc
170>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\distributed_runtime\tensor_coding.cc)
170>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\distributed_runtime\worker.cc)
170>c:\users\john\tensorflow\tensorflow\core\distributed_runtime\message_wrappers.h(24): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/tensor.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\distributed_runtime\worker_cache_partial.cc)
168>betainc_op.cc
168>bias_op.cc
170>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\distributed_runtime\worker_session.cc)
168>c:\users\john\tensorflow\tensorflow\core\kernels\batchtospace_op.cc(198): warning C4002: too many actual parameters for macro 'TF_BATCHTOSPACE_BLOCK_DIMS_CASE'
170>Done building project ""tf_core_distributed_runtime.vcxproj"" -- FAILED.
168>bincount_op.cc
168>bitcast_op.cc
168>resource_ops.cc
168>resources.cc
168>stats_ops.cc
168>broadcast_to_op.cc
168>bucketize_op.cc
168>candidate_sampler_ops.cc
168>cast_op.cc
168>cast_op_impl_bfloat.cc
168>cast_op_impl_bool.cc
168>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\kernels\cast_op.cc)
168>cast_op_impl_complex128.cc
168>cast_op_impl_complex64.cc
168>cast_op_impl_double.cc
168>cast_op_impl_float.cc
168>cast_op_impl_half.cc
168>cast_op_impl_int16.cc
168>cast_op_impl_int32.cc
168>cast_op_impl_int64.cc
168>cast_op_impl_int8.cc
168>cast_op_impl_uint16.cc
168>cast_op_impl_uint32.cc
168>cast_op_impl_uint64.cc
168>cast_op_impl_uint8.cc
168>check_numerics_op.cc
168>cholesky_grad.cc
168>cholesky_op.cc
168>collective_ops.cc
168>colorspace_op.cc
168>compare_and_bitpack_op.cc
168>concat_lib_cpu.cc
168>concat_lib_gpu.cc
168>c:\users\john\tensorflow\tensorflow\core\kernels\compare_and_bitpack_op.cc(99): warning C4805: '|': unsafe mix of type 'int' and type 'bool' in operation
168>c:\users\john\tensorflow\tensorflow\core\kernels\compare_and_bitpack_op.cc(92): note: while compiling class template member function 'void tensorflow::functor::ComputeShard<T,void,void>::Compute(Eigen::TensorMap<Eigen::Tensor<const T,2,1,IndexType>,16,Eigen::MakePointer>,Eigen::TensorMap<Eigen::Tensor<unsigned char,2,1,IndexType>,16,Eigen::MakePointer>,const T &,tensorflow::int64,tensorflow::int64)'
168>        with
168>        [
168>            T=tensorflow::bfloat16,
168>            IndexType=Eigen::DenseIndex
168>        ]
168>c:\users\john\tensorflow\tensorflow\core\kernels\compare_and_bitpack_op.cc(149): note: see reference to function template instantiation 'void tensorflow::functor::ComputeShard<T,void,void>::Compute(Eigen::TensorMap<Eigen::Tensor<const T,2,1,IndexType>,16,Eigen::MakePointer>,Eigen::TensorMap<Eigen::Tensor<unsigned char,2,1,IndexType>,16,Eigen::MakePointer>,const T &,tensorflow::int64,tensorflow::int64)' being compiled
168>        with
168>        [
168>            T=tensorflow::bfloat16,
168>            IndexType=Eigen::DenseIndex
168>        ]
168>c:\users\john\tensorflow\tensorflow\core\kernels\compare_and_bitpack_op.cc(149): note: see reference to class template instantiation 'tensorflow::functor::ComputeShard<T,void,void>' being compiled
168>        with
168>        [
168>            T=tensorflow::bfloat16
168>        ]
168>c:\users\john\tensorflow\tensorflow\core\kernels\compare_and_bitpack_op.cc(146): note: while compiling class template member function 'void tensorflow::functor::CompareAndBitpack<Device,T>::operator ()(tensorflow::OpKernelContext *,Eigen::TensorMap<Eigen::Tensor<const T,2,1,IndexType>,16,Eigen::MakePointer>,Eigen::TensorMap<Eigen::TensorFixedSize<const T,Eigen::Sizes<>,1,IndexType>,16,Eigen::MakePointer>,Eigen::TensorMap<Eigen::Tensor<unsigned char,2,1,IndexType>,16,Eigen::MakePointer>)'
168>        with
168>        [
168>            Device=tensorflow::CPUDevice,
168>            T=tensorflow::bfloat16,
168>            IndexType=Eigen::DenseIndex
168>        ]
168>c:\users\john\tensorflow\tensorflow\core\kernels\compare_and_bitpack_op.cc(71): note: see reference to function template instantiation 'void tensorflow::functor::CompareAndBitpack<Device,T>::operator ()(tensorflow::OpKernelContext *,Eigen::TensorMap<Eigen::Tensor<const T,2,1,IndexType>,16,Eigen::MakePointer>,Eigen::TensorMap<Eigen::TensorFixedSize<const T,Eigen::Sizes<>,1,IndexType>,16,Eigen::MakePointer>,Eigen::TensorMap<Eigen::Tensor<unsigned char,2,1,IndexType>,16,Eigen::MakePointer>)' being compiled
168>        with
168>        [
168>            Device=tensorflow::CPUDevice,
168>            T=tensorflow::bfloat16,
168>            IndexType=Eigen::DenseIndex
168>        ]
168>c:\users\john\tensorflow\tensorflow\core\kernels\compare_and_bitpack_op.cc(70): note: see reference to class template instantiation 'tensorflow::functor::CompareAndBitpack<Device,T>' being compiled
168>        with
168>        [
168>            Device=tensorflow::CPUDevice,
168>            T=tensorflow::bfloat16
168>        ]
168>c:\users\john\tensorflow\tensorflow\core\kernels\compare_and_bitpack_op.cc(42): note: while compiling class template member function 'void tensorflow::CompareAndBitpackOp<tensorflow::CPUDevice,tensorflow::bfloat16>::Compute(tensorflow::OpKernelContext *)'
168>c:\users\john\tensorflow\tensorflow\core\kernels\compare_and_bitpack_op.cc(80): note: see reference to class template instantiation 'tensorflow::CompareAndBitpackOp<tensorflow::CPUDevice,tensorflow::bfloat16>' being compiled
168>concat_op.cc
168>conditional_accumulator_base.cc
168>conditional_accumulator_base_op.cc
168>conditional_accumulator_op.cc
168>constant_op.cc
168>control_flow_ops.cc
168>conv_grad_filter_ops.cc
168>conv_grad_input_ops.cc
168>conv_grad_ops.cc
168>conv_grad_ops_3d.cc
168>conv_ops.cc
168>conv_ops_3d.cc
168>conv_ops_fused.cc
168>conv_ops_using_gemm.cc
168>count_up_to_op.cc
168>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\kernels\conv_ops_fused.cc)
168>crop_and_resize_op.cc
168>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\core\kernels\conv_ops_using_gemm.cc)
168>cross_op.cc
168>ctc_decoder_ops.cc
168>ctc_loss_op.cc
168>cuda_solvers.cc
168>cudnn_pooling_gpu.cc
168>cudnn_rnn_ops.cc
168>cwise_op_abs.cc
168>cwise_op_acos.cc
168>cwise_op_acosh.cc
168>cwise_op_add_1.cc
168>cwise_op_add_2.cc
168>cwise_op_arg.cc
168>cwise_op_asin.cc
168>cwise_op_asinh.cc
168>cwise_op_atan.cc
168>cwise_op_atan2.cc
168>cwise_op_atanh.cc
168>cwise_op_bessel.cc
168>cwise_op_bitwise_and.cc
168>cwise_op_bitwise_or.cc
168>cwise_op_bitwise_xor.cc
168>cwise_op_ceil.cc
168>cwise_op_clip.cc
168>cwise_op_complex.cc
168>cwise_op_conj.cc
168>cwise_op_cos.cc
168>cwise_op_cosh.cc
168>cwise_op_digamma.cc
168>cwise_op_div.cc
168>cwise_op_equal_to_1.cc
168>cwise_op_equal_to_2.cc
168>cwise_op_erf.cc
168>cwise_op_erfc.cc
168>cwise_op_exp.cc
168>cwise_op_expm1.cc
168>cwise_op_floor.cc
168>cwise_op_floor_div.cc
168>cwise_op_floor_mod.cc
168>cwise_op_greater.cc
168>cwise_op_greater_equal.cc
168>cwise_op_igammas.cc
168>cwise_op_imag.cc
168>cwise_op_invert.cc
168>cwise_op_isfinite.cc
168>cwise_op_isinf.cc
168>cwise_op_isnan.cc
168>cwise_op_left_shift.cc
168>c:\users\john\tensorflow\tensorflow\contrib\cmake\build\external\eigen_archive\eigen\src\core\products\generalblockpanelkernel.h(1902): fatal error C1002: compiler is out of heap space in pass 2
168>cwise_op_less.cc
168>cl : Command line error D8040: error creating or communicating with child process
168>Done building project ""tf_core_kernels.vcxproj"" -- FAILED.
172>------ Build started: Project: tf_tools_transform_graph_lib, Configuration: Release x64 ------
173>------ Build started: Project: tf_label_image_example, Configuration: Release x64 ------
174>------ Build started: Project: grpc_tensorflow_server, Configuration: Release x64 ------
175>------ Build started: Project: tf_tutorials_example_trainer, Configuration: Release x64 ------
176>------ Build started: Project: benchmark_model, Configuration: Release x64 ------
173>main.cc
174>grpc_tensorflow_server.cc
175>example_trainer.cc
176>benchmark_model.cc
172>add_default_attributes.cc
172>backports.cc
172>file_utils.cc
172>flatten_atrous.cc
172>fold_batch_norms.cc
176>benchmark_model_main.cc
172>fold_constants_lib.cc
172>fold_old_batch_norms.cc
172>freeze_requantization_ranges.cc
175>c:\users\john\tensorflow\tensorflow\cc\ops\standard_ops.h(19): fatal error C1083: Cannot open include file: 'tensorflow/cc/ops/array_ops.h': No such file or directory
175>Done building project ""tf_tutorials_example_trainer.vcxproj"" -- FAILED.
172>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\tools\graph_transforms\fold_constants_lib.cc)
172>fuse_convolutions.cc
172>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\tools\graph_transforms\backports.cc)
172>insert_logging.cc
172>obfuscate_names.cc
172>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\tools\graph_transforms\fold_old_batch_norms.cc)
172>quantize_nodes.cc
172>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\tools\graph_transforms\fold_batch_norms.cc)
172>quantize_weights.cc
173>c:\users\john\tensorflow\tensorflow\examples\label_image\main.cc(42): fatal error C1083: Cannot open include file: 'tensorflow/cc/ops/image_ops.h': No such file or directory
173>Done building project ""tf_label_image_example.vcxproj"" -- FAILED.
172>remove_attribute.cc
172>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\tools\graph_transforms\fuse_convolutions.cc)
172>remove_control_dependencies.cc
172>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\tools\graph_transforms\insert_logging.cc)
172>remove_device.cc
172>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\tools\graph_transforms\obfuscate_names.cc)
172>remove_nodes.cc
172>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\tools\graph_transforms\quantize_weights.cc)
172>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\tools\graph_transforms\quantize_nodes.cc)
172>rename_attribute.cc
172>rename_op.cc
172>round_weights.cc
172>set_device.cc
172>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\tools\graph_transforms\remove_attribute.cc)
172>sort_by_execution_order.cc
174>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_core_cpu.dir\Release\accumulate_n_optimizer.obj'
174>Done building project ""grpc_tensorflow_server.vcxproj"" -- FAILED.
172>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\tools\graph_transforms\rename_attribute.cc)
172>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\tools\graph_transforms\rename_op.cc)
172>sparsify_gather.cc
172>strip_unused_nodes.cc
172>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\tools\graph_transforms\sparsify_gather.cc)
172>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\tools\graph_transforms\sparsify_gather.cc)
172>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\tools\graph_transforms\remove_device.cc)
172>transform_graph.cc
172>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\tools\graph_transforms\remove_nodes.cc)
172>transform_utils.cc
172>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\tools\graph_transforms\round_weights.cc)
176>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_core_cpu.dir\Release\accumulate_n_optimizer.obj'
176>Done building project ""benchmark_model.vcxproj"" -- FAILED.
172>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\tools\graph_transforms\sort_by_execution_order.cc)
172>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\tools\graph_transforms\strip_unused_nodes.cc)
172>Done building project ""tf_tools_transform_graph_lib.vcxproj"" -- FAILED.
177>------ Build started: Project: pywrap_tensorflow_internal_static, Configuration: Release x64 ------
178>------ Build started: Project: summarize_graph, Configuration: Release x64 ------
179>------ Build started: Project: compare_graphs, Configuration: Release x64 ------
180>------ Build started: Project: transform_graph, Configuration: Release x64 ------
179>compare_graphs.cc
178>summarize_graph_main.cc
180>transform_graph_main.cc
177>Generating __force_rebuild
177>
177>Running SWIG to generate Python wrappers
177>print_model_analysis.cc
177>pywrap_tensor.cc
177>pywrap_tfe_src.cc
177>tf_session_helper.cc
177>cpp_shape_inference.cc
177>python_op_gen.cc
177>python_op_gen_internal.cc
177>bfloat16.cc
177>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\python\eager\pywrap_tfe_src.cc)
177>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\python\eager\pywrap_tfe_src.cc)
177>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\python\client\tf_session_helper.cc)
177>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\python\client\tf_session_helper.cc)
177>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\python\eager\pywrap_tensor.cc)
177>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\python\eager\pywrap_tensor.cc)
177>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\python\framework\cpp_shape_inference.cc)
177>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\python\framework\cpp_shape_inference.cc)
177>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\core\bfloat16.cc)
177>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\core\bfloat16.cc)
177>c:\users\john\tensorflow\tensorflow\python\framework\python_op_gen.cc(23): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/op_def.pb_text.h': No such file or directory
177>c:\users\john\tensorflow\tensorflow\python\framework\python_op_gen_internal.cc(24): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/op_def.pb_text.h': No such file or directory
177>numpy.cc
177>ndarray_tensor.cc
177>ndarray_tensor_bridge.cc
177>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\core\ndarray_tensor.cc)
177>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\core\ndarray_tensor.cc)
177>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\python\eager\pywrap_tfe_src.cc)
177>py_func.cc
177>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\core\ndarray_tensor_bridge.cc)
177>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\core\ndarray_tensor_bridge.cc)
177>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\python\client\tf_session_helper.cc)
177>py_exception_registry.cc
177>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\core\profiler\internal\print_model_analysis.cc)
177>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\core\profiler\internal\print_model_analysis.cc)
177>py_seq_tensor.cc
177>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\core\py_exception_registry.cc)
177>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\core\py_exception_registry.cc)
180>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_tools_transform_graph_lib.dir\Release\backports.obj'
179>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_tools_transform_graph_lib.dir\Release\backports.obj'
180>Done building project ""transform_graph.vcxproj"" -- FAILED.
177>py_util.cc
179>Done building project ""compare_graphs.vcxproj"" -- FAILED.
177>safe_ptr.cc
177>py_record_reader.cc
177>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\io\py_record_reader.cc)
177>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\io\py_record_reader.cc)
177>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\core\safe_ptr.cc)
177>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\core\safe_ptr.cc)
177>py_record_writer.cc
177>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\io\py_record_writer.cc)
177>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\io\py_record_writer.cc)
178>LINK : fatal error LNK1181: cannot open input file 'C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\tf_tools_transform_graph_lib.dir\Release\backports.obj'
178>Done building project ""summarize_graph.vcxproj"" -- FAILED.
177>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\core\py_func.cc)
177>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\core\py_func.cc)
177>kernel_registry.cc
177>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\core\py_func.cc)
177>util.cc
177>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\core\py_seq_tensor.cc)
177>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\core\py_seq_tensor.cc)
177>ops.cc
177>scope.cc
177>pywrap_tensorflow_internal.cc
177>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\python\util\util.cc)
177>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\python\util\util.cc)
177>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.cc)
177>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.cc)
177>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\cc\framework\scope.cc)
177>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.cc)
177>Done building project ""pywrap_tensorflow_internal_static.vcxproj"" -- FAILED.
181>------ Build started: Project: pywrap_tensorflow_internal, Configuration: Release x64 ------
181>Generating __force_rebuild
181>
181>Running SWIG to generate Python wrappers
181>print_model_analysis.cc
181>pywrap_tensor.cc
181>pywrap_tfe_src.cc
181>tf_session_helper.cc
181>cpp_shape_inference.cc
181>python_op_gen.cc
181>python_op_gen_internal.cc
181>bfloat16.cc
181>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\python\eager\pywrap_tfe_src.cc)
181>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\python\eager\pywrap_tfe_src.cc)
181>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\python\client\tf_session_helper.cc)
181>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\python\client\tf_session_helper.cc)
181>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\python\eager\pywrap_tensor.cc)
181>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\python\eager\pywrap_tensor.cc)
181>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\python\framework\cpp_shape_inference.cc)
181>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\python\framework\cpp_shape_inference.cc)
181>c:\users\john\tensorflow\tensorflow\python\framework\python_op_gen.cc(23): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/op_def.pb_text.h': No such file or directory
181>numpy.cc
181>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\core\bfloat16.cc)
181>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\core\bfloat16.cc)
181>c:\users\john\tensorflow\tensorflow\python\framework\python_op_gen_internal.cc(24): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/op_def.pb_text.h': No such file or directory
181>ndarray_tensor.cc
181>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\core\profiler\internal\print_model_analysis.cc)
181>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\core\profiler\internal\print_model_analysis.cc)
181>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\core\ndarray_tensor.cc)
181>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\core\ndarray_tensor.cc)
181>ndarray_tensor_bridge.cc
181>py_func.cc
181>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\python\client\tf_session_helper.cc)
181>py_exception_registry.cc
181>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\core\ndarray_tensor_bridge.cc)
181>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\core\ndarray_tensor_bridge.cc)
181>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\core\py_exception_registry.cc)
181>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\core\py_exception_registry.cc)
181>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\python\eager\pywrap_tfe_src.cc)
181>py_seq_tensor.cc
181>py_util.cc
181>safe_ptr.cc
181>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\core\safe_ptr.cc)
181>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\core\safe_ptr.cc)
181>py_record_reader.cc
181>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\io\py_record_reader.cc)
181>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\io\py_record_reader.cc)
181>py_record_writer.cc
181>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\io\py_record_writer.cc)
181>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\io\py_record_writer.cc)
181>kernel_registry.cc
181>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\core\py_func.cc)
181>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\core\py_func.cc)
181>util.cc
181>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\core\py_func.cc)
181>ops.cc
181>scope.cc
181>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\core\py_seq_tensor.cc)
181>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\python\lib\core\py_seq_tensor.cc)
181>pywrap_tensorflow_internal.cc
181>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\python\util\util.cc)
181>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\python\util\util.cc)
181>c:\users\john\tensorflow\tensorflow\c\c_api.h(1113): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.cc)
181>c:\users\john\tensorflow\tensorflow\c\c_api.h(1069): note: see declaration of 'TF_WhileParams' (compiling source file C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.cc)
181>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\cc\framework\scope.cc)
181>c:\users\john\tensorflow\tensorflow\core\common_runtime\device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\Users\john\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.cc)
181>Done building project ""pywrap_tensorflow_internal.vcxproj"" -- FAILED.
182>------ Build started: Project: _nearest_neighbor_ops, Configuration: Release x64 ------
183>------ Build started: Project: _gru_ops, Configuration: Release x64 ------
184>------ Build started: Project: _beam_search_ops, Configuration: Release x64 ------
185>------ Build started: Project: _lstm_ops, Configuration: Release x64 ------
186>------ Build started: Project: _periodic_resample_op, Configuration: Release x64 ------
182>cl : Command line warning D9025: overriding '/DTF_COMPILE_LIBRARY' with '/UTF_COMPILE_LIBRARY'
184>cl : Command line warning D9025: overriding '/DTF_COMPILE_LIBRARY' with '/UTF_COMPILE_LIBRARY'
183>cl : Command line warning D9025: overriding '/DTF_COMPILE_LIBRARY' with '/UTF_COMPILE_LIBRARY'
183>blas_gemm.cc
185>cl : Command line warning D9025: overriding '/DTF_COMPILE_LIBRARY' with '/UTF_COMPILE_LIBRARY'
186>cl : Command line warning D9025: overriding '/DTF_COMPILE_LIBRARY' with '/UTF_COMPILE_LIBRARY'
184>beam_search_ops.cc
182>cl : Command line warning D9025: overriding '/DTF_COMPILE_LIBRARY' with '/UTF_COMPILE_LIBRARY'
182>cl : Command line warning D9025: overriding '/DTF_COMPILE_LIBRARY' with '/UTF_COMPILE_LIBRARY'
182>hyperplane_lsh_probes.cc
182>nearest_neighbor_ops.cc
186>cl : Command line warning D9025: overriding '/DTF_COMPILE_LIBRARY' with '/UTF_COMPILE_LIBRARY'
186>periodic_resample_op.cc
186>cl : Command line warning D9025: overriding '/DTF_COMPILE_LIBRARY' with '/UTF_COMPILE_LIBRARY'
185>blas_gemm.cc
186>array_ops.cc
183>cl : Command line warning D9025: overriding '/DTF_COMPILE_LIBRARY' with '/UTF_COMPILE_LIBRARY'
183>gru_ops.cc
185>cl : Command line warning D9025: overriding '/DTF_COMPILE_LIBRARY' with '/UTF_COMPILE_LIBRARY'
185>lstm_ops.cc
184>cl : Command line warning D9025: overriding '/DTF_COMPILE_LIBRARY' with '/UTF_COMPILE_LIBRARY'
184>beam_search_ops.cc
186>LINK : fatal error LNK1181: cannot open input file 'Release\pywrap_tensorflow_internal.lib'
186>Done building project ""_periodic_resample_op.vcxproj"" -- FAILED.
182>LINK : fatal error LNK1181: cannot open input file 'Release\pywrap_tensorflow_internal.lib'
182>Done building project ""_nearest_neighbor_ops.vcxproj"" -- FAILED.
184>LINK : fatal error LNK1181: cannot open input file 'Release\pywrap_tensorflow_internal.lib'
184>Done building project ""_beam_search_ops.vcxproj"" -- FAILED.
183>cl : Command line warning D9025: overriding '/DTF_COMPILE_LIBRARY' with '/UTF_COMPILE_LIBRARY'
183>gru_ops.cc
185>cl : Command line warning D9025: overriding '/DTF_COMPILE_LIBRARY' with '/UTF_COMPILE_LIBRARY'
185>lstm_ops.cc
183>LINK : fatal error LNK1181: cannot open input file 'Release\pywrap_tensorflow_internal.lib'
183>Done building project ""_gru_ops.vcxproj"" -- FAILED.
185>LINK : fatal error LNK1181: cannot open input file 'Release\pywrap_tensorflow_internal.lib'
185>Done building project ""_lstm_ops.vcxproj"" -- FAILED.
187>------ Build started: Project: tf_python_api, Configuration: Release x64 ------
188>------ Skipped Build: Project: INSTALL, Configuration: Release x64 ------
188>Project not selected to build for this solution configuration 
187>Generating __init__.py files for Python API.
187>The parameter is incorrect
187>C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Common7\IDE\VC\VCTargets\Microsoft.CppCommon.targets(171,5): error MSB6006: ""cmd.exe"" exited with code 1.
187>Done building project ""tf_python_api.vcxproj"" -- FAILED.
189>------ Skipped Build: Project: tf_python_build_pip_package, Configuration: Release x64 ------
189>Project not selected to build for this solution configuration 
========== Build: 41 succeeded, 146 failed, 84 up-to-date, 2 skipped ==========

Please Let me know of any other details required
"
23341,I can not use my RTX2080ti to train my model ,"**System information**
- OS Platform and Distribution :windows10
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):tensorflow-gpu-1.8.0
- Python version:python3.6
- CUDA/cuDNN version:cuda9.2 cudnn7.2
- GPU model and memory:RTX 2080ti 11G

I can not use my RTX2080ti to train my model ,and it showed this error message:Process finished with exit code -1073741819 (0xC0000005)
however, I can train my model without error by using tensorflow-1.8.0
and both tensorflow and tensorflow all can run tensorflow code
error happened only when I train  my model withtensorflow-gpu
or will you have any ideas to use my rtx2080ti to train my model with tensorflow-gpu

pciBusID: 0000:01:00.0
totalMemory: 11.00GiB freeMemory: 8.99GiB
2018-10-29 14:18:40.275850: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1435] Adding visible gpu devices: 0
2018-10-29 14:18:40.822494: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-10-29 14:18:40.822696: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:929]      0 
2018-10-29 14:18:40.822833: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:942] 0:   N 
2018-10-29 14:18:40.823047: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8695 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
Epoch 1/100

### Process finished with exit code -1073741819 (0xC0000005)



"
23340,TFLite object detection toco error(the size of detect.tflite is 0kb),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): None
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy S9
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.12.0rc0
- Python version: 3.5.4
- Bazel version (if compiling from source): 0.17.2
- GCC/Compiler version (if compiling from source): gcc 5.4.0
- CUDA/cuDNN version: cuda 9.0, cudnn 7.1
- GPU model and memory: gtx titan x, 12gib



I've follow this instructions. 
https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193

I just want to make tensorflow lite demo app with pre-trained weight file from tesnorflow model zoo.
(ssd_mobilenet_v1_0.75_depth_coco weight file)

And successfully generated `tfilite_graph.pb` file. After that, I tried to make `detect.tflite` file.

```bazel run -c opt tensorflow/contrib/lite/toco:toco -- \
--input_file=$OUTPUT_DIR/tflite_graph.pb \
--output_file=$OUTPUT_DIR/detect.tflite \
--input_shapes=1,300,300,3 \
--input_arrays=normalized_input_image_tensor \
--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'  \
--inference_type=QUANTIZED_UINT8 \
--mean_values=128 \
--std_values=128 \
--change_concat_input_ranges=false \
--allow_custom_ops
```

bazel build is done successfully, but got an ERROR.

```
Starting local Bazel server and connecting to it...
INFO: Analysed target //tensorflow/contrib/lite/toco:toco (69 packages loaded).
INFO: Found 1 target...
INFO: From Compiling external/snappy/snappy-stubs-internal.cc:
cc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++
cc1plus: warning: unrecognized command line option '-Wno-shift-negative-value'
INFO: From Compiling external/snappy/snappy-sinksource.cc:
cc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++
cc1plus: warning: unrecognized command line option '-Wno-shift-negative-value'
INFO: From Compiling external/snappy/snappy.cc:
cc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++
cc1plus: warning: unrecognized command line option '-Wno-shift-negative-value'
INFO: From Compiling tensorflow/contrib/lite/c/c_api_internal.c:
In file included from tensorflow/contrib/lite/c/c_api_internal.c:16:0:
./tensorflow/contrib/lite/c/c_api_internal.h:60:34: warning: 'struct TfLiteContext' declared inside parameter list
   TfLiteStatus (*Refresh)(struct TfLiteContext* context);
                                  ^
./tensorflow/contrib/lite/c/c_api_internal.h:60:34: warning: its scope is only this definition or declaration, which is probably not what you want
INFO: From Compiling tensorflow/contrib/lite/util.cc:
tensorflow/contrib/lite/util.cc: In function 'TfLiteIntArray* tflite::ConvertArrayToTfLiteIntArray(int, const int*)':
tensorflow/contrib/lite/util.cc:32:24: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (size_t i = 0; i < rank; i++) {
                        ^
INFO: From Compiling tensorflow/contrib/lite/arena_planner.cc:
tensorflow/contrib/lite/arena_planner.cc: In member function 'virtual TfLiteStatus tflite::ArenaPlanner::PlanAllocations()':
tensorflow/contrib/lite/arena_planner.cc:133:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < graph_info_->num_nodes(); ++i) {
                     ^
tensorflow/contrib/lite/arena_planner.cc:151:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < graph_info_->num_nodes(); ++i) {
                     ^
tensorflow/contrib/lite/arena_planner.cc: In member function 'virtual TfLiteStatus tflite::ArenaPlanner::ExecuteAllocations(int, int)':
tensorflow/contrib/lite/arena_planner.cc:191:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < graph_info_->num_tensors(); ++i) {
                     ^
tensorflow/contrib/lite/arena_planner.cc: In member function 'TfLiteStatus tflite::ArenaPlanner::CalculateAllocationOfInternalTensors(int)':
tensorflow/contrib/lite/arena_planner.cc:282:18: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   if (node_index < graph_info_->num_nodes()) {
                  ^
tensorflow/contrib/lite/arena_planner.cc: In member function 'TfLiteStatus tflite::ArenaPlanner::CalculateDeallocationOfInternalTensors(int)':
tensorflow/contrib/lite/arena_planner.cc:295:18: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   if (node_index < graph_info_->num_nodes()) {
                  ^
INFO: From Compiling tensorflow/contrib/lite/nnapi_delegate_disabled.cc:
In file included from ./tensorflow/contrib/lite/nnapi_delegate.h:21:0,
                 from tensorflow/contrib/lite/nnapi_delegate_disabled.cc:15:
./tensorflow/contrib/lite/interpreter.h: In member function 'TfLiteTensor* tflite::Interpreter::tensor(int)':
./tensorflow/contrib/lite/interpreter.h:237:22: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (tensor_index >= context_.tensors_size || tensor_index < 0)
                      ^
./tensorflow/contrib/lite/interpreter.h: In member function 'const TfLiteTensor* tflite::Interpreter::tensor(int) const':
./tensorflow/contrib/lite/interpreter.h:244:22: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (tensor_index >= context_.tensors_size || tensor_index < 0)
                      ^
./tensorflow/contrib/lite/interpreter.h: In member function 'const std::pair<TfLiteNode, _TfLiteRegistration>* tflite::Interpreter::node_and_registration(int) const':
./tensorflow/contrib/lite/interpreter.h:252:20: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (node_index >= nodes_and_registration_.size() || node_index < 0)
                    ^
In file included from ./tensorflow/contrib/lite/allocation.h:23:0,
                 from ./tensorflow/contrib/lite/nnapi_delegate.h:18,
                 from tensorflow/contrib/lite/nnapi_delegate_disabled.cc:15:
./tensorflow/contrib/lite/interpreter.h: In member function 'TfLiteStatus tflite::Interpreter::EnsureTensorDataIsReadable(int)':
./tensorflow/contrib/lite/interpreter.h:363:44: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     TF_LITE_ENSURE(&context_, tensor_index < tensors_size());
                                            ^
./tensorflow/contrib/lite/c/c_api_internal.h:119:11: note: in definition of macro 'TF_LITE_ENSURE'
     if (!(a)) {                                                             \
           ^
INFO: From Compiling tensorflow/contrib/lite/core/api/flatbuffer_conversions.cc:
tensorflow/contrib/lite/core/api/flatbuffer_conversions.cc: In function 'void tflite::{anonymous}::FlatBufferIntVectorToArray(int, const flatbuffers::Vector<int>*, int*, tflite::ErrorReporter*)':
tensorflow/contrib/lite/core/api/flatbuffer_conversions.cc:36:24: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (num_dimensions > max_size_of_buffer / sizeof(int)) {
                        ^
INFO: From Compiling tensorflow/contrib/lite/model.cc:
In file included from ./tensorflow/contrib/lite/model.h:40:0,
                 from tensorflow/contrib/lite/model.cc:26:
./tensorflow/contrib/lite/interpreter.h: In member function 'TfLiteTensor* tflite::Interpreter::tensor(int)':
./tensorflow/contrib/lite/interpreter.h:237:22: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (tensor_index >= context_.tensors_size || tensor_index < 0)
                      ^
./tensorflow/contrib/lite/interpreter.h: In member function 'const TfLiteTensor* tflite::Interpreter::tensor(int) const':
./tensorflow/contrib/lite/interpreter.h:244:22: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (tensor_index >= context_.tensors_size || tensor_index < 0)
                      ^
./tensorflow/contrib/lite/interpreter.h: In member function 'const std::pair<TfLiteNode, _TfLiteRegistration>* tflite::Interpreter::node_and_registration(int) const':
./tensorflow/contrib/lite/interpreter.h:252:20: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (node_index >= nodes_and_registration_.size() || node_index < 0)
                    ^
In file included from ./tensorflow/contrib/lite/allocation.h:23:0,
                 from tensorflow/contrib/lite/model.cc:22:
./tensorflow/contrib/lite/interpreter.h: In member function 'TfLiteStatus tflite::Interpreter::EnsureTensorDataIsReadable(int)':
./tensorflow/contrib/lite/interpreter.h:363:44: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     TF_LITE_ENSURE(&context_, tensor_index < tensors_size());
                                            ^
./tensorflow/contrib/lite/c/c_api_internal.h:119:11: note: in definition of macro 'TF_LITE_ENSURE'
     if (!(a)) {                                                             \
           ^
tensorflow/contrib/lite/model.cc: In member function 'TfLiteStatus tflite::InterpreterBuilder::ParseNodes(const flatbuffers::Vector<flatbuffers::Offset<tflite::Operator> >*, tflite::Interpreter*)':
tensorflow/contrib/lite/model.cc:209:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < operators->Length(); ++i) {
                     ^
tensorflow/contrib/lite/model.cc:212:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (index < 0 || index >= flatbuffer_op_index_to_registration_.size()) {
                            ^
tensorflow/contrib/lite/model.cc: In member function 'TfLiteStatus tflite::InterpreterBuilder::ParseTensors(const flatbuffers::Vector<flatbuffers::Offset<tflite::Buffer> >*, const flatbuffers::Vector<flatbuffers::Offset<tflite::Tensor> >*, tflite::Interpreter*)':
tensorflow/contrib/lite/model.cc:271:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < tensors->Length(); ++i) {
                     ^
tensorflow/contrib/lite/model.cc: In member function 'TfLiteStatus tflite::InterpreterBuilder::operator()(std::unique_ptr<tflite::Interpreter>*, int)':
tensorflow/contrib/lite/model.cc:445:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < (*interpreter)->tensors_size(); ++i) {
                     ^
tensorflow/contrib/lite/model.cc: In instantiation of 'std::vector<int> tflite::{anonymous}::FlatBufferIntArrayToVector(T*) [with T = const flatbuffers::Vector<int>]':
tensorflow/contrib/lite/model.cc:238:50:   required from here
tensorflow/contrib/lite/model.cc:186:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < flat_array->Length(); i++) {
                     ^
INFO: From Compiling tensorflow/contrib/lite/graph_info.cc:
tensorflow/contrib/lite/graph_info.cc: In member function 'void tflite::{anonymous}::PartitionGraphIntoIndependentSubgraphsImpl::Partition()':
tensorflow/contrib/lite/graph_info.cc:75:41: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     for (int node_index = 0; node_index < info_->num_nodes(); node_index++) {
                                         ^
tensorflow/contrib/lite/graph_info.cc: In member function 'void tflite::{anonymous}::PartitionGraphIntoIndependentSubgraphsImpl::BuildSubgraph()':
tensorflow/contrib/lite/graph_info.cc:190:43: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
       for (int node_index = 0; node_index < info_->num_nodes(); node_index++) {
                                           ^
INFO: From Compiling tensorflow/contrib/lite/string_util.cc:
In file included from tensorflow/contrib/lite/string_util.cc:21:0:
./tensorflow/contrib/lite/interpreter.h: In member function 'TfLiteTensor* tflite::Interpreter::tensor(int)':
./tensorflow/contrib/lite/interpreter.h:237:22: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (tensor_index >= context_.tensors_size || tensor_index < 0)
                      ^
./tensorflow/contrib/lite/interpreter.h: In member function 'const TfLiteTensor* tflite::Interpreter::tensor(int) const':
./tensorflow/contrib/lite/interpreter.h:244:22: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (tensor_index >= context_.tensors_size || tensor_index < 0)
                      ^
./tensorflow/contrib/lite/interpreter.h: In member function 'const std::pair<TfLiteNode, _TfLiteRegistration>* tflite::Interpreter::node_and_registration(int) const':
./tensorflow/contrib/lite/interpreter.h:252:20: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (node_index >= nodes_and_registration_.size() || node_index < 0)
                    ^
In file included from ./tensorflow/contrib/lite/string_util.h:45:0,
                 from tensorflow/contrib/lite/string_util.cc:16:
./tensorflow/contrib/lite/interpreter.h: In member function 'TfLiteStatus tflite::Interpreter::EnsureTensorDataIsReadable(int)':
./tensorflow/contrib/lite/interpreter.h:363:44: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     TF_LITE_ENSURE(&context_, tensor_index < tensors_size());
                                            ^
./tensorflow/contrib/lite/c/c_api_internal.h:119:11: note: in definition of macro 'TF_LITE_ENSURE'
     if (!(a)) {                                                             \
           ^
tensorflow/contrib/lite/string_util.cc: In member function 'int tflite::DynamicBuffer::WriteToBuffer(char**)':
tensorflow/contrib/lite/string_util.cc:89:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < offset_.size(); i++) {
                     ^
INFO: From Compiling tensorflow/contrib/lite/interpreter.cc:
tensorflow/contrib/lite/interpreter.cc:195:1: warning: multi-line comment [-Wcomment]
 // | TfLiteIntArray* nodes_to_replace; |--\
 ^
In file included from tensorflow/contrib/lite/interpreter.cc:16:0:
./tensorflow/contrib/lite/interpreter.h: In member function 'TfLiteTensor* tflite::Interpreter::tensor(int)':
./tensorflow/contrib/lite/interpreter.h:237:22: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (tensor_index >= context_.tensors_size || tensor_index < 0)
                      ^
./tensorflow/contrib/lite/interpreter.h: In member function 'const TfLiteTensor* tflite::Interpreter::tensor(int) const':
./tensorflow/contrib/lite/interpreter.h:244:22: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (tensor_index >= context_.tensors_size || tensor_index < 0)
                      ^
./tensorflow/contrib/lite/interpreter.h: In member function 'const std::pair<TfLiteNode, _TfLiteRegistration>* tflite::Interpreter::node_and_registration(int) const':
./tensorflow/contrib/lite/interpreter.h:252:20: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (node_index >= nodes_and_registration_.size() || node_index < 0)
                    ^
In file included from ./tensorflow/contrib/lite/allocation.h:23:0,
                 from ./tensorflow/contrib/lite/interpreter.h:25,
                 from tensorflow/contrib/lite/interpreter.cc:16:
./tensorflow/contrib/lite/interpreter.h: In member function 'TfLiteStatus tflite::Interpreter::EnsureTensorDataIsReadable(int)':
./tensorflow/contrib/lite/interpreter.h:363:44: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     TF_LITE_ENSURE(&context_, tensor_index < tensors_size());
                                            ^
./tensorflow/contrib/lite/c/c_api_internal.h:119:11: note: in definition of macro 'TF_LITE_ENSURE'
     if (!(a)) {                                                             \
           ^
tensorflow/contrib/lite/interpreter.cc: In destructor 'tflite::Interpreter::~Interpreter()':
tensorflow/contrib/lite/interpreter.cc:157:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < context_.tensors_size; i++) {
                     ^
tensorflow/contrib/lite/interpreter.cc: In member function 'TfLiteStatus tflite::Interpreter::BytesRequired(TfLiteType, const int*, size_t, size_t*)':
tensorflow/contrib/lite/interpreter.cc:398:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int k = 0; k < dims_size; k++) count *= dims[k];
                     ^
In file included from ./tensorflow/contrib/lite/allocation.h:23:0,
                 from ./tensorflow/contrib/lite/interpreter.h:25,
                 from tensorflow/contrib/lite/interpreter.cc:16:
tensorflow/contrib/lite/interpreter.cc: In member function 'TfLiteStatus tflite::Interpreter::ResizeInputTensor(int, const std::vector<int>&)':
tensorflow/contrib/lite/interpreter.cc:557:31: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
                  tensor_index < context_.tensors_size && tensor_index >= 0);
                               ^
./tensorflow/contrib/lite/c/c_api_internal.h:119:11: note: in definition of macro 'TF_LITE_ENSURE'
     if (!(a)) {                                                             \
           ^
tensorflow/contrib/lite/interpreter.cc: In member function 'TfLiteStatus tflite::Interpreter::PrepareOpsStartingAt(int, int*)':
tensorflow/contrib/lite/interpreter.cc:578:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
        execution_plan_index < execution_plan_.size(); execution_plan_index++) {
                             ^
tensorflow/contrib/lite/interpreter.cc: In member function 'TfLiteStatus tflite::Interpreter::Invoke()':
tensorflow/contrib/lite/interpreter.cc:632:47: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (next_execution_plan_index_to_prepare_ == execution_plan_.size()) {
                                               ^
tensorflow/contrib/lite/interpreter.cc:653:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
        execution_plan_index < execution_plan_.size(); execution_plan_index++) {
                             ^
tensorflow/contrib/lite/interpreter.cc: In member function 'TfLiteStatus tflite::Interpreter::AddTensors(int, int*)':
tensorflow/contrib/lite/interpreter.cc:735:30: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = base_index; i < tensors_.size(); i++) {
                              ^
In file included from ./tensorflow/contrib/lite/allocation.h:23:0,
                 from ./tensorflow/contrib/lite/interpreter.h:25,
                 from tensorflow/contrib/lite/interpreter.cc:16:
tensorflow/contrib/lite/interpreter.cc: In member function 'TfLiteStatus tflite::Interpreter::GetNodeAndRegistration(int, TfLiteNode**, TfLiteRegistration**)':
tensorflow/contrib/lite/interpreter.cc:755:40: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   TF_LITE_ENSURE(&context_, node_index < nodes_size() && node_index >= 0);
                                        ^
./tensorflow/contrib/lite/c/c_api_internal.h:119:11: note: in definition of macro 'TF_LITE_ENSURE'
     if (!(a)) {                                                             \
           ^
tensorflow/contrib/lite/interpreter.cc: In member function 'TfLiteStatus tflite::Interpreter::SetTensorParametersReadOnly(int, TfLiteType, const char*, size_t, const int*, TfLiteQuantizationParams, const char*, size_t, const tflite::Allocation*)':
tensorflow/contrib/lite/interpreter.cc:781:31: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
                  tensor_index < context_.tensors_size && tensor_index >= 0);
                               ^
./tensorflow/contrib/lite/c/c_api_internal.h:119:11: note: in definition of macro 'TF_LITE_ENSURE'
     if (!(a)) {                                                             \
           ^
tensorflow/contrib/lite/interpreter.cc: In member function 'TfLiteStatus tflite::Interpreter::SetTensorParametersReadWrite(int, TfLiteType, const char*, size_t, const int*, TfLiteQuantizationParams, bool)':
tensorflow/contrib/lite/interpreter.cc:825:31: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
                  tensor_index < context_.tensors_size && tensor_index >= 0);
                               ^
./tensorflow/contrib/lite/c/c_api_internal.h:119:11: note: in definition of macro 'TF_LITE_ENSURE'
     if (!(a)) {                                                             \
           ^
tensorflow/contrib/lite/interpreter.cc: In member function 'TfLiteStatus tflite::Interpreter::SetExecutionPlan(const std::vector<int>&)':
tensorflow/contrib/lite/interpreter.cc:857:61: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     TF_LITE_ENSURE(&context_, node_index >= 0 && node_index < nodes_size());
                                                             ^
./tensorflow/contrib/lite/c/c_api_internal.h:119:11: note: in definition of macro 'TF_LITE_ENSURE'
     if (!(a)) {                                                             \
           ^
tensorflow/contrib/lite/interpreter.cc: In member function 'TfLiteStatus tflite::Interpreter::ModifyGraphWithDelegate(TfLiteDelegate*, bool)':
tensorflow/contrib/lite/interpreter.cc:945:48: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (last_execution_plan_index_prepared + 1 == execution_plan_.size()) {
                                                ^
In file included from ./tensorflow/contrib/lite/allocation.h:23:0,
                 from ./tensorflow/contrib/lite/interpreter.h:25,
                 from tensorflow/contrib/lite/interpreter.cc:16:
tensorflow/contrib/lite/interpreter.cc: In member function 'TfLiteStatus tflite::Interpreter::SetBufferHandle(int, TfLiteBufferHandle, TfLiteDelegate*)':
tensorflow/contrib/lite/interpreter.cc:987:42: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   TF_LITE_ENSURE(&context_, tensor_index < tensors_size());
                                          ^
./tensorflow/contrib/lite/c/c_api_internal.h:119:11: note: in definition of macro 'TF_LITE_ENSURE'
     if (!(a)) {                                                             \
           ^
tensorflow/contrib/lite/interpreter.cc: In member function 'TfLiteStatus tflite::Interpreter::GetBufferHandle(int, TfLiteBufferHandle*, TfLiteDelegate**)':
tensorflow/contrib/lite/interpreter.cc:1006:42: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   TF_LITE_ENSURE(&context_, tensor_index < tensors_size());
                                          ^
./tensorflow/contrib/lite/c/c_api_internal.h:119:11: note: in definition of macro 'TF_LITE_ENSURE'
     if (!(a)) {                                                             \
           ^
INFO: From Compiling tensorflow/contrib/lite/optional_debug_tools.cc:
In file included from ./tensorflow/contrib/lite/optional_debug_tools.h:20:0,
                 from tensorflow/contrib/lite/optional_debug_tools.cc:15:
./tensorflow/contrib/lite/interpreter.h: In member function 'TfLiteTensor* tflite::Interpreter::tensor(int)':
./tensorflow/contrib/lite/interpreter.h:237:22: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (tensor_index >= context_.tensors_size || tensor_index < 0)
                      ^
./tensorflow/contrib/lite/interpreter.h: In member function 'const TfLiteTensor* tflite::Interpreter::tensor(int) const':
./tensorflow/contrib/lite/interpreter.h:244:22: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (tensor_index >= context_.tensors_size || tensor_index < 0)
                      ^
./tensorflow/contrib/lite/interpreter.h: In member function 'const std::pair<TfLiteNode, _TfLiteRegistration>* tflite::Interpreter::node_and_registration(int) const':
./tensorflow/contrib/lite/interpreter.h:252:20: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (node_index >= nodes_and_registration_.size() || node_index < 0)
                    ^
In file included from ./tensorflow/contrib/lite/allocation.h:23:0,
                 from ./tensorflow/contrib/lite/interpreter.h:25,
                 from ./tensorflow/contrib/lite/optional_debug_tools.h:20,
                 from tensorflow/contrib/lite/optional_debug_tools.cc:15:
./tensorflow/contrib/lite/interpreter.h: In member function 'TfLiteStatus tflite::Interpreter::EnsureTensorDataIsReadable(int)':
./tensorflow/contrib/lite/interpreter.h:363:44: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     TF_LITE_ENSURE(&context_, tensor_index < tensors_size());
                                            ^
./tensorflow/contrib/lite/c/c_api_internal.h:119:11: note: in definition of macro 'TF_LITE_ENSURE'
     if (!(a)) {                                                             \
           ^
tensorflow/contrib/lite/optional_debug_tools.cc: In function 'void tflite::PrintInterpreterState(tflite::Interpreter*)':
tensorflow/contrib/lite/optional_debug_tools.cc:86:43: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int tensor_index = 0; tensor_index < interpreter->tensors_size();
                                           ^
tensorflow/contrib/lite/optional_debug_tools.cc:96:39: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int node_index = 0; node_index < interpreter->nodes_size();
                                       ^
INFO: From Compiling tensorflow/contrib/lite/tools/verifier.cc:
In file included from ./tensorflow/contrib/lite/model.h:40:0,
                 from ./tensorflow/contrib/lite/tools/verifier.h:22,
                 from tensorflow/contrib/lite/tools/verifier.cc:16:
./tensorflow/contrib/lite/interpreter.h: In member function 'TfLiteTensor* tflite::Interpreter::tensor(int)':
./tensorflow/contrib/lite/interpreter.h:237:22: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (tensor_index >= context_.tensors_size || tensor_index < 0)
                      ^
./tensorflow/contrib/lite/interpreter.h: In member function 'const TfLiteTensor* tflite::Interpreter::tensor(int) const':
./tensorflow/contrib/lite/interpreter.h:244:22: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (tensor_index >= context_.tensors_size || tensor_index < 0)
                      ^
./tensorflow/contrib/lite/interpreter.h: In member function 'const std::pair<TfLiteNode, _TfLiteRegistration>* tflite::Interpreter::node_and_registration(int) const':
./tensorflow/contrib/lite/interpreter.h:252:20: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (node_index >= nodes_and_registration_.size() || node_index < 0)
                    ^
In file included from ./tensorflow/contrib/lite/stderr_reporter.h:19:0,
                 from ./tensorflow/contrib/lite/error_reporter.h:20,
                 from ./tensorflow/contrib/lite/tools/verifier.h:21,
                 from tensorflow/contrib/lite/tools/verifier.cc:16:
./tensorflow/contrib/lite/interpreter.h: In member function 'TfLiteStatus tflite::Interpreter::EnsureTensorDataIsReadable(int)':
./tensorflow/contrib/lite/interpreter.h:363:44: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     TF_LITE_ENSURE(&context_, tensor_index < tensors_size());
                                            ^
./tensorflow/contrib/lite/c/c_api_internal.h:119:11: note: in definition of macro 'TF_LITE_ENSURE'
     if (!(a)) {                                                             \
           ^
tensorflow/contrib/lite/tools/verifier.cc: In function 'bool tflite::{anonymous}::VerifyStringTensorBuffer(const tflite::Buffer&, tflite::ErrorReporter*)':
tensorflow/contrib/lite/tools/verifier.cc:88:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 1; i <= num_strings; i++, offset += sizeof(int32_t)) {
                     ^
tensorflow/contrib/lite/tools/verifier.cc:90:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (string_offset < prev_ptr || string_offset > buffer_size) {
                       ^
tensorflow/contrib/lite/tools/verifier.cc:90:51: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (string_offset < prev_ptr || string_offset > buffer_size) {
                                                   ^
INFO: From Compiling tensorflow/contrib/lite/tools/optimize/quantize_weights.cc:
In file included from ./tensorflow/contrib/lite/model.h:40:0,
                 from ./tensorflow/contrib/lite/tools/optimize/quantize_weights.h:21,
                 from tensorflow/contrib/lite/tools/optimize/quantize_weights.cc:15:
./tensorflow/contrib/lite/interpreter.h: In member function 'TfLiteTensor* tflite::Interpreter::tensor(int)':
./tensorflow/contrib/lite/interpreter.h:237:22: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (tensor_index >= context_.tensors_size || tensor_index < 0)
                      ^
./tensorflow/contrib/lite/interpreter.h: In member function 'const TfLiteTensor* tflite::Interpreter::tensor(int) const':
./tensorflow/contrib/lite/interpreter.h:244:22: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (tensor_index >= context_.tensors_size || tensor_index < 0)
                      ^
./tensorflow/contrib/lite/interpreter.h: In member function 'const std::pair<TfLiteNode, _TfLiteRegistration>* tflite::Interpreter::node_and_registration(int) const':
./tensorflow/contrib/lite/interpreter.h:252:20: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (node_index >= nodes_and_registration_.size() || node_index < 0)
                    ^
In file included from ./tensorflow/contrib/lite/context.h:19:0,
                 from ./tensorflow/contrib/lite/tools/optimize/quantize_weights.h:20,
                 from tensorflow/contrib/lite/tools/optimize/quantize_weights.cc:15:
./tensorflow/contrib/lite/interpreter.h: In member function 'TfLiteStatus tflite::Interpreter::EnsureTensorDataIsReadable(int)':
./tensorflow/contrib/lite/interpreter.h:363:44: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     TF_LITE_ENSURE(&context_, tensor_index < tensors_size());
                                            ^
./tensorflow/contrib/lite/c/c_api_internal.h:119:11: note: in definition of macro 'TF_LITE_ENSURE'
     if (!(a)) {                                                             \
           ^
tensorflow/contrib/lite/tools/optimize/quantize_weights.cc: In function 'uint64_t tflite::optimize::{anonymous}::CountTensorConsumers(const tflite::ModelT*, const tflite::SubGraphT*, int32_t)':
tensorflow/contrib/lite/tools/optimize/quantize_weights.cc:90:31: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int op_idx = 0; op_idx < subgraph->operators.size(); ++op_idx) {
                               ^
tensorflow/contrib/lite/tools/optimize/quantize_weights.cc:95:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     for (int i = 0; i < op->inputs.size(); ++i) {
                       ^
tensorflow/contrib/lite/tools/optimize/quantize_weights.cc: In function 'int32_t tflite::optimize::{anonymous}::GetOrInsertDequantizeOpCodeIndex(tflite::ModelT*)':
tensorflow/contrib/lite/tools/optimize/quantize_weights.cc:309:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < model->operator_codes.size(); ++i) {
                     ^
tensorflow/contrib/lite/tools/optimize/quantize_weights.cc: In function 'TfLiteStatus tflite::optimize::{anonymous}::QuantizeWeightsInternal(flatbuffers::FlatBufferBuilder*, const tflite::Model*, bool, uint64_t)':
tensorflow/contrib/lite/tools/optimize/quantize_weights.cc:361:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < subgraph->operators.size(); ++i) {
                     ^
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/reorder_elementwise_unary.cc:
tensorflow/contrib/lite/toco/graph_transformations/reorder_elementwise_unary.cc: In member function 'virtual bool toco::ReorderElementwiseUnary::Run(toco::Model*, std::size_t)':
tensorflow/contrib/lite/toco/graph_transformations/reorder_elementwise_unary.cc:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     for (int i = 0; i < model->operators.size(); i++) {
                       ^
tensorflow/contrib/lite/toco/graph_transformations/reorder_elementwise_unary.cc:127:25: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
       for (int j = 0; j < consumer->inputs.size(); j++) {
                         ^
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_unary.cc:
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_unary.cc: In member function 'virtual bool toco::ResolveConstantUnaryOperator::Run(toco::Model*, std::size_t)':
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_unary.cc:256:26: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     for (size_t i = 0; i < output_buffer_size; ++i) {
                          ^
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_unary.cc:115:55: warning: 'input_float_data' may be used uninitialized in this function [-Wmaybe-uninitialized]
   std::vector<DataType<ArrayDataType::kFloat>> const* input_float_data;
                                                       ^
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/merge_reshape_into_preceding_transpose.cc:
tensorflow/contrib/lite/toco/graph_transformations/merge_reshape_into_preceding_transpose.cc: In function 'std::vector<int> toco::{anonymous}::ReshapeToTranspose(const toco::Model&, const toco::TensorFlowReshapeOperator*)':
tensorflow/contrib/lite/toco/graph_transformations/merge_reshape_into_preceding_transpose.cc:73:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < in_shape.size(); i++) {
                     ^
tensorflow/contrib/lite/toco/graph_transformations/merge_reshape_into_preceding_transpose.cc: In member function 'virtual bool toco::MergeReshapeIntoPrecedingTranspose::Run(toco::Model*, std::size_t)':
tensorflow/contrib/lite/toco/graph_transformations/merge_reshape_into_preceding_transpose.cc:169:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < merged_perm.size(); i++) {
                     ^
INFO: From Compiling tensorflow/core/grappler/utils.cc:
In file included from tensorflow/core/grappler/utils.cc:16:0:
./tensorflow/core/grappler/utils.h: In function 'int tensorflow::grappler::NodePositionIfSameNode(const string&, const string&)':
./tensorflow/core/grappler/utils.h:117:49: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
       std::distance(input_it, input_name.end()) < node_name.size()) {
                                                 ^
tensorflow/core/grappler/utils.cc: In function 'void tensorflow::grappler::PermuteNodesInPlace(tensorflow::GraphDef*, std::vector<int>*, bool)':
tensorflow/core/grappler/utils.cc:327:14: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     while (n != (*permutation)[n]) {
              ^
tensorflow/core/grappler/utils.cc: In member function 'std::__cxx11::string tensorflow::grappler::SimpleGraphView::PrintToString() const':
tensorflow/core/grappler/utils.cc:500:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     for (int j = 0; j < outputs(i).size(); ++j) {
                       ^
INFO: From Compiling tensorflow/core/protobuf/config.pb_text.cc:
bazel-out/k8-opt/genfiles/tensorflow/core/protobuf/config.pb_text.cc: In function 'bool tensorflow::internal::ProtoParseFromScanner(tensorflow::strings::Scanner*, bool, bool, tensorflow::ConfigProto*)':
bazel-out/k8-opt/genfiles/tensorflow/core/protobuf/config.pb_text.cc:865:34: warning: 'map_value' may be used uninitialized in this function [-Wmaybe-uninitialized]
       (*map)[map_key] = map_value;
                                  ^
bazel-out/k8-opt/genfiles/tensorflow/core/protobuf/config.pb_text.cc:856:9: note: 'map_value' was declared here
   int32 map_value;
         ^
INFO: From Compiling tensorflow/core/framework/model.cc:
In file included from tensorflow/core/framework/model.cc:16:0:
./tensorflow/core/framework/model.h: In constructor 'tensorflow::data::model::SharedState::SharedState(tensorflow::int64, std::shared_ptr<tensorflow::mutex>, std::shared_ptr<tensorflow::condition_variable>)':
./tensorflow/core/framework/model.h:46:9: warning: 'tensorflow::data::model::SharedState::value' will be initialized after [-Wreorder]
   int64 value;
         ^
./tensorflow/core/framework/model.h:44:26: warning:   'std::shared_ptr<tensorflow::mutex> tensorflow::data::model::SharedState::mu' [-Wreorder]
   std::shared_ptr<mutex> mu;
                          ^
./tensorflow/core/framework/model.h:40:12: warning:   when initialized here [-Wreorder]
   explicit SharedState(int64 value, std::shared_ptr<mutex> mu,
            ^
INFO: From Compiling tensorflow/core/framework/common_shape_fns.cc:
In file included from ./tensorflow/core/framework/common_shape_fns.h:22:0,
                 from tensorflow/core/framework/common_shape_fns.cc:15:
./tensorflow/core/util/tensor_format.h: In function 'int tensorflow::GetTensorDimsFromSpatialDims(int, tensorflow::TensorFormat)':
./tensorflow/core/util/tensor_format.h:148:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
./tensorflow/core/util/tensor_format.h: In function 'int tensorflow::GetTensorSpatialDims(int, tensorflow::TensorFormat)':
./tensorflow/core/util/tensor_format.h:124:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/dequantize.cc:
tensorflow/contrib/lite/toco/graph_transformations/dequantize.cc: In instantiation of 'void toco::{anonymous}::DequantizeBuffer(toco::Array*) [with toco::ArrayDataType A = (toco::ArrayDataType)4u]':
tensorflow/contrib/lite/toco/graph_transformations/dequantize.cc:89:52:   required from here
tensorflow/contrib/lite/toco/graph_transformations/dequantize.cc:38:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < old_data.size(); i++) {
                     ^
tensorflow/contrib/lite/toco/graph_transformations/dequantize.cc: In instantiation of 'void toco::{anonymous}::DequantizeBuffer(toco::Array*) [with toco::ArrayDataType A = (toco::ArrayDataType)7u]':
tensorflow/contrib/lite/toco/graph_transformations/dequantize.cc:91:52:   required from here
tensorflow/contrib/lite/toco/graph_transformations/dequantize.cc:38:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_concatenation.cc:
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_concatenation.cc: In instantiation of 'void toco::{anonymous}::CopyTensorSegments(const std::vector<toco::Array*>&, const std::vector<int>&, int, toco::Array*) [with toco::ArrayDataType A = (toco::ArrayDataType)2u; T = float]':
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_concatenation.cc:100:37:   required from 'void toco::{anonymous}::ConcatenateTensorBuffers(const std::vector<toco::Array*>&, int, toco::Array*) [with toco::ArrayDataType A = (toco::ArrayDataType)2u]'
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_concatenation.cc:175:64:   required from here
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_concatenation.cc:67:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     for (int i = 0; i < input_arrays.size(); i++) {
                       ^
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_concatenation.cc: In instantiation of 'void toco::{anonymous}::CopyTensorSegments(const std::vector<toco::Array*>&, const std::vector<int>&, int, toco::Array*) [with toco::ArrayDataType A = (toco::ArrayDataType)4u; T = unsigned char]':
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_concatenation.cc:100:37:   required from 'void toco::{anonymous}::ConcatenateTensorBuffers(const std::vector<toco::Array*>&, int, toco::Array*) [with toco::ArrayDataType A = (toco::ArrayDataType)4u]'
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_concatenation.cc:180:64:   required from here
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_concatenation.cc:67:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_concatenation.cc: In instantiation of 'void toco::{anonymous}::CopyTensorSegments(const std::vector<toco::Array*>&, const std::vector<int>&, int, toco::Array*) [with toco::ArrayDataType A = (toco::ArrayDataType)7u; T = int]':
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_concatenation.cc:100:37:   required from 'void toco::{anonymous}::ConcatenateTensorBuffers(const std::vector<toco::Array*>&, int, toco::Array*) [with toco::ArrayDataType A = (toco::ArrayDataType)7u]'
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_concatenation.cc:184:64:   required from here
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_concatenation.cc:67:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_concatenation.cc: In instantiation of 'void toco::{anonymous}::CopyTensorSegments(const std::vector<toco::Array*>&, const std::vector<int>&, int, toco::Array*) [with toco::ArrayDataType A = (toco::ArrayDataType)9u; T = long long int]':
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_concatenation.cc:100:37:   required from 'void toco::{anonymous}::ConcatenateTensorBuffers(const std::vector<toco::Array*>&, int, toco::Array*) [with toco::ArrayDataType A = (toco::ArrayDataType)9u]'
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_concatenation.cc:188:64:   required from here
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_concatenation.cc:67:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_concatenation.cc: In instantiation of 'void toco::{anonymous}::CopyTensorSegments(const std::vector<toco::Array*>&, const std::vector<int>&, int, toco::Array*) [with toco::ArrayDataType A = (toco::ArrayDataType)11u; T = std::__cxx11::basic_string<char>]':
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_concatenation.cc:100:37:   required from 'void toco::{anonymous}::ConcatenateTensorBuffers(const std::vector<toco::Array*>&, int, toco::Array*) [with toco::ArrayDataType A = (toco::ArrayDataType)11u]'
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_concatenation.cc:192:64:   required from here
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_concatenation.cc:67:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
INFO: From Compiling tensorflow/core/framework/dataset.cc:
In file included from ./tensorflow/core/framework/dataset.h:26:0,
                 from tensorflow/core/framework/dataset.cc:15:
./tensorflow/core/framework/model.h: In constructor 'tensorflow::data::model::SharedState::SharedState(tensorflow::int64, std::shared_ptr<tensorflow::mutex>, std::shared_ptr<tensorflow::condition_variable>)':
./tensorflow/core/framework/model.h:46:9: warning: 'tensorflow::data::model::SharedState::value' will be initialized after [-Wreorder]
   int64 value;
         ^
./tensorflow/core/framework/model.h:44:26: warning:   'std::shared_ptr<tensorflow::mutex> tensorflow::data::model::SharedState::mu' [-Wreorder]
   std::shared_ptr<mutex> mu;
                          ^
./tensorflow/core/framework/model.h:40:12: warning:   when initialized here [-Wreorder]
   explicit SharedState(int64 value, std::shared_ptr<mutex> mu,
            ^
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/convert_trivial_tile_to_concat.cc:
tensorflow/contrib/lite/toco/graph_transformations/convert_trivial_tile_to_concat.cc: In member function 'virtual bool toco::ConvertTrivialTileToConcat::Run(toco::Model*, std::size_t)':
tensorflow/contrib/lite/toco/graph_transformations/convert_trivial_tile_to_concat.cc:52:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < multiples.size(); ++i) {
                     ^
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:
tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc: In function 'void toco::{anonymous}::ProcessTensorFlowReshapeOperator(toco::Model*, toco::TensorFlowReshapeOperator*)':
tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:432:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < shape_data.size(); i++) {
                     ^
tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc: In function 'void toco::{anonymous}::ProcessTensorFlowReductionOperator(toco::Model*, toco::Operator*)':
tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:575:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     for (int i = 0; i < reduction_indices.size(); ++i) {
                       ^
tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc: In function 'void toco::{anonymous}::ProcessSliceOperator(toco::Model*, toco::SliceOperator*)':
tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:628:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < op->begin.size(); ++i) {
                     ^
tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc: In function 'void toco::{anonymous}::ProcessPadOperator(toco::Model*, toco::PadOperator*)':
tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1144:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < op->left_padding.size(); ++i) {
                     ^
tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc: In function 'void toco::{anonymous}::ProcessPadV2Operator(toco::Model*, toco::PadV2Operator*)':
tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1170:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < op->left_padding.size(); ++i) {
                     ^
tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc: In function 'void toco::{anonymous}::ProcessStridedSliceOperator(toco::Model*, toco::StridedSliceOperator*)':
tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1313:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < op->strides.size(); i++) {
                     ^
tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc: In function 'void toco::{anonymous}::ProcessSqueezeOperator(toco::Model*, toco::SqueezeOperator*)':
tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1367:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < input_dims.size(); ++i) {
                     ^
tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc: In function 'void toco::{anonymous}::ProcessTransposeOperator(toco::Model*, toco::TransposeOperator*)':
tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1439:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < perm.size(); i++) {
                     ^
tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc: In function 'void toco::{anonymous}::ProcessTileOperator(toco::Model*, toco::TensorFlowTileOperator*)':
tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1537:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < mutable_dims->size(); ++i) {
                     ^
tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc: In function 'void toco::{anonymous}::ProcessUnpackOperator(toco::Model*, toco::UnpackOperator*)':
tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1609:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < input_dims.size(); ++i) {
                     ^
tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc: In member function 'virtual bool toco::PropagateFixedSizes::Run(toco::Model*, std::size_t)':
tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1841:25: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
       for (int i = 0; i < op->outputs.size(); ++i) {
                         ^
tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc: In instantiation of 'void toco::{anonymous}::ProcessArgMinMaxOperator(toco::Model*, Op*) [with Op = toco::ArgMaxOperator]':
tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1828:50:   required from here
tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1460:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < input_dims.size() - 1; ++i) {
                     ^
tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc: In instantiation of 'void toco::{anonymous}::ProcessArgMinMaxOperator(toco::Model*, Op*) [with Op = toco::ArgMinOperator]':
tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1832:50:   required from here
tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1460:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
INFO: From Compiling tensorflow/contrib/lite/toco/tflite/import.cc:
In file included from ./tensorflow/contrib/lite/model.h:40:0,
                 from tensorflow/contrib/lite/toco/tflite/import.cc:18:
./tensorflow/contrib/lite/interpreter.h: In member function 'TfLiteTensor* tflite::Interpreter::tensor(int)':
./tensorflow/contrib/lite/interpreter.h:237:22: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (tensor_index >= context_.tensors_size || tensor_index < 0)
                      ^
./tensorflow/contrib/lite/interpreter.h: In member function 'const TfLiteTensor* tflite::Interpreter::tensor(int) const':
./tensorflow/contrib/lite/interpreter.h:244:22: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (tensor_index >= context_.tensors_size || tensor_index < 0)
                      ^
./tensorflow/contrib/lite/interpreter.h: In member function 'const std::pair<TfLiteNode, _TfLiteRegistration>* tflite::Interpreter::node_and_registration(int) const':
./tensorflow/contrib/lite/interpreter.h:252:20: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (node_index >= nodes_and_registration_.size() || node_index < 0)
                    ^
In file included from ./tensorflow/contrib/lite/core/api/op_resolver.h:18:0,
                 from ./tensorflow/contrib/lite/model.h:39,
                 from tensorflow/contrib/lite/toco/tflite/import.cc:18:
./tensorflow/contrib/lite/interpreter.h: In member function 'TfLiteStatus tflite::Interpreter::EnsureTensorDataIsReadable(int)':
./tensorflow/contrib/lite/interpreter.h:363:44: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     TF_LITE_ENSURE(&context_, tensor_index < tensors_size());
                                            ^
./tensorflow/contrib/lite/c/c_api_internal.h:119:11: note: in definition of macro 'TF_LITE_ENSURE'
     if (!(a)) {                                                             \
           ^
tensorflow/contrib/lite/toco/tflite/import.cc: In function 'void toco::tflite::ImportTensors(const tflite::Model&, toco::Model*)':
tensorflow/contrib/lite/toco/tflite/import.cc:72:25: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
       for (int i = 0; i < shape->Length(); ++i) {
                         ^
tensorflow/contrib/lite/toco/tflite/import.cc: In function 'void toco::tflite::ImportOperators(const tflite::Model&, const std::map<std::__cxx11::basic_string<char>, std::unique_ptr<toco::tflite::BaseOperator> >&, const TensorsTable&, const OperatorsTable&, toco::Model*)':
tensorflow/contrib/lite/toco/tflite/import.cc:111:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (index < 0 || index > operators_table.size()) {
                            ^
tensorflow/contrib/lite/toco/tflite/import.cc:146:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     for (int i = 0; i < inputs->Length(); i++) {
                       ^
tensorflow/contrib/lite/toco/tflite/import.cc:160:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     for (int i = 0; i < outputs->Length(); i++) {
                       ^
In file included from ./tensorflow/core/lib/strings/stringprintf.h:29:0,
                 from ./tensorflow/contrib/lite/toco/format_port.h:23,
                 from ./tensorflow/contrib/lite/toco/toco_port.h:23,
                 from ./tensorflow/contrib/lite/toco/model.h:29,
                 from ./tensorflow/contrib/lite/toco/tflite/import.h:19,
                 from tensorflow/contrib/lite/toco/tflite/import.cc:15:
./tensorflow/core/platform/default/logging.h: In instantiation of 'std::__cxx11::string* tensorflow::internal::Check_EQImpl(const T1&, const T2&, const char*) [with T1 = int; T2 = unsigned int; std::__cxx11::string = std::__cxx11::basic_string<char>]':
tensorflow/contrib/lite/toco/tflite/import.cc:83:9:   required from here
./tensorflow/core/platform/default/logging.h:230:25: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
                         ==)  // Compilation error with CHECK_EQ(NULL, x)?
                         ^
./tensorflow/core/platform/macros.h:88:49: note: in definition of macro 'TF_PREDICT_TRUE'
 #define TF_PREDICT_TRUE(x) (__builtin_expect(!!(x), 1))
                                                 ^
./tensorflow/core/platform/default/logging.h:229:1: note: in expansion of macro 'TF_DEFINE_CHECK_OP_IMPL'
 TF_DEFINE_CHECK_OP_IMPL(Check_EQ,
 ^
tensorflow/contrib/lite/toco/tflite/import.cc: At global scope:
tensorflow/contrib/lite/toco/tflite/import.cc:188:6: warning: 'bool toco::tflite::{anonymous}::Verify(const void*, size_t)' defined but not used [-Wunused-function]
 bool Verify(const void* buf, size_t len) {
      ^
INFO: From Compiling tensorflow/contrib/lite/toco/tflite/export.cc:
In file included from ./tensorflow/contrib/lite/model.h:40:0,
                 from ./tensorflow/contrib/lite/tools/optimize/quantize_weights.h:21,
                 from tensorflow/contrib/lite/toco/tflite/export.cc:24:
./tensorflow/contrib/lite/interpreter.h: In member function 'TfLiteTensor* tflite::Interpreter::tensor(int)':
./tensorflow/contrib/lite/interpreter.h:237:22: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (tensor_index >= context_.tensors_size || tensor_index < 0)
                      ^
./tensorflow/contrib/lite/interpreter.h: In member function 'const TfLiteTensor* tflite::Interpreter::tensor(int) const':
./tensorflow/contrib/lite/interpreter.h:244:22: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (tensor_index >= context_.tensors_size || tensor_index < 0)
                      ^
./tensorflow/contrib/lite/interpreter.h: In member function 'const std::pair<TfLiteNode, _TfLiteRegistration>* tflite::Interpreter::node_and_registration(int) const':
./tensorflow/contrib/lite/interpreter.h:252:20: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (node_index >= nodes_and_registration_.size() || node_index < 0)
                    ^
In file included from ./tensorflow/contrib/lite/util.h:25:0,
                 from ./tensorflow/contrib/lite/toco/tflite/export.h:20,
                 from tensorflow/contrib/lite/toco/tflite/export.cc:15:
./tensorflow/contrib/lite/interpreter.h: In member function 'TfLiteStatus tflite::Interpreter::EnsureTensorDataIsReadable(int)':
./tensorflow/contrib/lite/interpreter.h:363:44: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     TF_LITE_ENSURE(&context_, tensor_index < tensors_size());
                                            ^
./tensorflow/contrib/lite/c/c_api_internal.h:119:11: note: in definition of macro 'TF_LITE_ENSURE'
     if (!(a)) {                                                             \
           ^
tensorflow/contrib/lite/toco/tflite/export.cc: In function 'flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<tflite::Operator> > > toco::tflite::ExportOperators(const toco::Model&, const std::map<toco::OperatorType, std::unique_ptr<toco::tflite::BaseOperator> >&, const OperatorsMap&, const TensorsMap&, flatbuffers::FlatBufferBuilder*, std::set<int>*, const toco::tflite::ExportParams&)':
tensorflow/contrib/lite/toco/tflite/export.cc:301:27: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
         for (int i = 0; i < op->inputs.size(); ++i) {
                           ^
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/remove_trivial_passthrough.cc:
tensorflow/contrib/lite/toco/graph_transformations/remove_trivial_passthrough.cc: In function 'bool toco::RemoveTrivialPassthroughOp(toco::GraphTransformation*, toco::Model*, std::size_t, int)':
tensorflow/contrib/lite/toco/graph_transformations/remove_trivial_passthrough.cc:65:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     for (int i = 0; i < passthru_op->inputs.size(); i++) {
                       ^
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_strided_slice.cc:
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_strided_slice.cc: In instantiation of 'void toco::{anonymous}::StridedSlice(const toco::StridedSliceOperator&, const toco::Array&, toco::Array*) [with toco::ArrayDataType Type = (toco::ArrayDataType)2u]':
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_strided_slice.cc:147:74:   required from here
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_strided_slice.cc:101:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   } while (dst_offset < output_data.size());
                       ^
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_strided_slice.cc: In instantiation of 'void toco::{anonymous}::StridedSlice(const toco::StridedSliceOperator&, const toco::Array&, toco::Array*) [with toco::ArrayDataType Type = (toco::ArrayDataType)4u]':
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_strided_slice.cc:150:74:   required from here
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_strided_slice.cc:101:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_strided_slice.cc: In instantiation of 'void toco::{anonymous}::StridedSlice(const toco::StridedSliceOperator&, const toco::Array&, toco::Array*) [with toco::ArrayDataType Type = (toco::ArrayDataType)7u]':
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_strided_slice.cc:153:74:   required from here
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_strided_slice.cc:101:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_strided_slice.cc: In instantiation of 'void toco::{anonymous}::StridedSlice(const toco::StridedSliceOperator&, const toco::Array&, toco::Array*) [with toco::ArrayDataType Type = (toco::ArrayDataType)9u]':
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_strided_slice.cc:156:74:   required from here
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_strided_slice.cc:101:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
INFO: From Compiling tensorflow/contrib/lite/toco/tflite/operator.cc:
tensorflow/contrib/lite/toco/tflite/operator.cc: In member function 'virtual int toco::tflite::Lstm::GetVersion(const toco::Operator&) const':
tensorflow/contrib/lite/toco/tflite/operator.cc:721:3: warning: control reaches end of non-void function [-Wreturn-type]
   }
   ^
tensorflow/contrib/lite/toco/tflite/operator.cc: In member function 'virtual flatbuffers::Offset<tflite::LSTMOptions> toco::tflite::Lstm::WriteOptions(const TocoOperator&, flatbuffers::FlatBufferBuilder*) const':
tensorflow/contrib/lite/toco/tflite/operator.cc:680:30: warning: 'kernel_type' may be used uninitialized in this function [-Wmaybe-uninitialized]
     ::tflite::LSTMKernelType kernel_type;
                              ^
tensorflow/contrib/lite/toco/tflite/operator.cc: In member function 'toco::tflite::Options toco::tflite::BuiltinOperator<T1, T2, TfLiteEnum>::Serialize(const toco::Operator&, flatbuffers::FlatBufferBuilder*) const [with T1 = toco::LstmCellOperator; T2 = tflite::LSTMOptions; tflite::BuiltinOptions TfLiteEnum = (tflite::BuiltinOptions)14u]':
tensorflow/contrib/lite/toco/tflite/operator.cc:680:30: warning: 'kernel_type' may be used uninitialized in this function [-Wmaybe-uninitialized]
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_transpose.cc:
In file included from ./tensorflow/core/lib/strings/stringprintf.h:29:0,
                 from ./tensorflow/contrib/lite/toco/format_port.h:23,
                 from ./tensorflow/contrib/lite/toco/toco_port.h:23,
                 from ./tensorflow/contrib/lite/toco/model.h:29,
                 from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23,
                 from tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_transpose.cc:17:
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_transpose.cc: In instantiation of 'void toco::{anonymous}::Transpose(toco::Model*, const toco::Array&, const std::vector<int>&, toco::Array*) [with toco::ArrayDataType Type = (toco::ArrayDataType)2u]':
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_transpose.cc:145:53:   required from here
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_transpose.cc:43:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(perm.size() >= dim);
                     ^
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_transpose.cc:43:3: note: in expansion of macro 'CHECK'
   CHECK(perm.size() >= dim);
   ^
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_transpose.cc: In instantiation of 'void toco::{anonymous}::Transpose(toco::Model*, const toco::Array&, const std::vector<int>&, toco::Array*) [with toco::ArrayDataType Type = (toco::ArrayDataType)4u]':
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_transpose.cc:149:53:   required from here
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_transpose.cc:43:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(perm.size() >= dim);
                     ^
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_transpose.cc:43:3: note: in expansion of macro 'CHECK'
   CHECK(perm.size() >= dim);
   ^
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_transpose.cc: In instantiation of 'void toco::{anonymous}::Transpose(toco::Model*, const toco::Array&, const std::vector<int>&, toco::Array*) [with toco::ArrayDataType Type = (toco::ArrayDataType)7u]':
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_transpose.cc:153:53:   required from here
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_transpose.cc:43:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(perm.size() >= dim);
                     ^
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_transpose.cc:43:3: note: in expansion of macro 'CHECK'
   CHECK(perm.size() >= dim);
   ^
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_transpose.cc: In instantiation of 'void toco::{anonymous}::Transpose(toco::Model*, const toco::Array&, const std::vector<int>&, toco::Array*) [with toco::ArrayDataType Type = (toco::ArrayDataType)9u]':
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_transpose.cc:157:53:   required from here
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_transpose.cc:43:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(perm.size() >= dim);
                     ^
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_transpose.cc:43:3: note: in expansion of macro 'CHECK'
   CHECK(perm.size() >= dim);
   ^
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/propagate_fake_quant_num_bits.cc:
tensorflow/contrib/lite/toco/graph_transformations/propagate_fake_quant_num_bits.cc: In function 'bool toco::{anonymous}::RecursivelyBackwardPropagateDataType(toco::GraphTransformation*, toco::Model*, toco::Operator*, toco::ArrayDataType, const toco::MinMax&)':
tensorflow/contrib/lite/toco/graph_transformations/propagate_fake_quant_num_bits.cc:149:41: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int input_index = 0; input_index < op->inputs.size(); ++input_index) {
                                         ^
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/fuse_broadcast_into_following_binary.cc:
tensorflow/contrib/lite/toco/graph_transformations/fuse_broadcast_into_following_binary.cc: In function 'bool toco::{anonymous}::IsBroadcastingOp(const toco::Model&, toco::Operator*)':
tensorflow/contrib/lite/toco/graph_transformations/fuse_broadcast_into_following_binary.cc:37:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     for (int i = 1; i < op->inputs.size(); ++i) {
                       ^
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/ensure_uint8_weights_safe_for_fast_int8_kernels.cc:
tensorflow/contrib/lite/toco/graph_transformations/ensure_uint8_weights_safe_for_fast_int8_kernels.cc: In member function 'virtual bool toco::EnsureUint8WeightsSafeForFastInt8Kernels::Run(toco::Model*, std::size_t)':
tensorflow/contrib/lite/toco/graph_transformations/ensure_uint8_weights_safe_for_fast_int8_kernels.cc:168:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < buffer_data.size(); i++) {
                     ^
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_slice.cc:
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_slice.cc: In instantiation of 'bool toco::{anonymous}::Slice(const toco::SliceOperator&, const toco::Array&, toco::Array*) [with toco::ArrayDataType Type = (toco::ArrayDataType)2u]':
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_slice.cc:128:72:   required from here
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_slice.cc:53:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < begin.size(); ++i) {
                     ^
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_slice.cc: In instantiation of 'bool toco::{anonymous}::Slice(const toco::SliceOperator&, const toco::Array&, toco::Array*) [with toco::ArrayDataType Type = (toco::ArrayDataType)4u]':
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_slice.cc:133:72:   required from here
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_slice.cc:53:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_slice.cc: In instantiation of 'bool toco::{anonymous}::Slice(const toco::SliceOperator&, const toco::Array&, toco::Array*) [with toco::ArrayDataType Type = (toco::ArrayDataType)7u]':
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_slice.cc:138:72:   required from here
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_slice.cc:53:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_slice.cc: In instantiation of 'bool toco::{anonymous}::Slice(const toco::SliceOperator&, const toco::Array&, toco::Array*) [with toco::ArrayDataType Type = (toco::ArrayDataType)9u]':
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_slice.cc:143:72:   required from here
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_slice.cc:53:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_batch_normalization.cc:
In file included from ./tensorflow/core/lib/strings/stringprintf.h:29:0,
                 from ./tensorflow/contrib/lite/toco/format_port.h:23,
                 from ./tensorflow/contrib/lite/toco/toco_port.h:23,
                 from ./tensorflow/contrib/lite/toco/model.h:29,
                 from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23,
                 from tensorflow/contrib/lite/toco/graph_transformations/resolve_batch_normalization.cc:20:
tensorflow/contrib/lite/toco/graph_transformations/resolve_batch_normalization.cc: In member function 'virtual bool toco::ResolveBatchNormalization::Run(toco::Model*, std::size_t)':
tensorflow/contrib/lite/toco/graph_transformations/resolve_batch_normalization.cc:124:31: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(mul_float_data.size() == buffer_size);
                               ^
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
tensorflow/contrib/lite/toco/graph_transformations/resolve_batch_normalization.cc:124:3: note: in expansion of macro 'CHECK'
   CHECK(mul_float_data.size() == buffer_size);
   ^
tensorflow/contrib/lite/toco/graph_transformations/resolve_batch_normalization.cc:125:31: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(add_float_data.size() == buffer_size);
                               ^
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
tensorflow/contrib/lite/toco/graph_transformations/resolve_batch_normalization.cc:125:3: note: in expansion of macro 'CHECK'
   CHECK(add_float_data.size() == buffer_size);
   ^
tensorflow/contrib/lite/toco/graph_transformations/resolve_batch_normalization.cc:126:32: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(mean_float_data.size() == buffer_size);
                                ^
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
tensorflow/contrib/lite/toco/graph_transformations/resolve_batch_normalization.cc:126:3: note: in expansion of macro 'CHECK'
   CHECK(mean_float_data.size() == buffer_size);
   ^
tensorflow/contrib/lite/toco/graph_transformations/resolve_batch_normalization.cc:127:38: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(multiplier_float_data.size() == buffer_size);
                                      ^
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
tensorflow/contrib/lite/toco/graph_transformations/resolve_batch_normalization.cc:127:3: note: in expansion of macro 'CHECK'
   CHECK(multiplier_float_data.size() == buffer_size);
   ^
tensorflow/contrib/lite/toco/graph_transformations/resolve_batch_normalization.cc:128:34: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(offset_float_data.size() == buffer_size);
                                  ^
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
tensorflow/contrib/lite/toco/graph_transformations/resolve_batch_normalization.cc:128:3: note: in expansion of macro 'CHECK'
   CHECK(offset_float_data.size() == buffer_size);
   ^
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/reorder_reshape_transpose.cc:
tensorflow/contrib/lite/toco/graph_transformations/reorder_reshape_transpose.cc: In function 'std::vector<int> toco::{anonymous}::ComputeNewPerm(std::vector<int>, std::vector<int>, std::vector<int>)':
tensorflow/contrib/lite/toco/graph_transformations/reorder_reshape_transpose.cc:63:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < input_dims.size(); i++) {
                     ^
tensorflow/contrib/lite/toco/graph_transformations/reorder_reshape_transpose.cc:72:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < intermediate_dims.size(); i++) {
                     ^
tensorflow/contrib/lite/toco/graph_transformations/reorder_reshape_transpose.cc:83:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < perm.size(); i++) {
                     ^
tensorflow/contrib/lite/toco/graph_transformations/reorder_reshape_transpose.cc:90:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int index = 0; index < input_dims.size(); index++) {
                             ^
tensorflow/contrib/lite/toco/graph_transformations/reorder_reshape_transpose.cc: In member function 'virtual bool toco::ReorderReshapeTranspose::Run(toco::Model*, std::size_t)':
tensorflow/contrib/lite/toco/graph_transformations/reorder_reshape_transpose.cc:193:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     for (int i = 0; i < model->operators.size(); i++) {
                       ^
tensorflow/contrib/lite/toco/graph_transformations/reorder_reshape_transpose.cc:195:25: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
       for (int j = 0; j < consumer->inputs.size(); j++) {
                         ^
tensorflow/contrib/lite/toco/graph_transformations/reorder_reshape_transpose.cc: At global scope:
tensorflow/contrib/lite/toco/graph_transformations/reorder_reshape_transpose.cc:51:6: warning: 'void toco::{anonymous}::Filter(std::vector<int>*, int)' defined but not used [-Wunused-function]
 void Filter(std::vector<int>* vec, int value) {
      ^
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/unpartition_embedding_lookup.cc:
tensorflow/contrib/lite/toco/graph_transformations/unpartition_embedding_lookup.cc:40:3: warning: multi-line comment [-Wcomment]
   //   (ids)--+-->FloorDiv--+-->DynamicPartition-->[[Gather]]--\
   ^
tensorflow/contrib/lite/toco/graph_transformations/unpartition_embedding_lookup.cc: In member function 'virtual bool toco::UnpartitionEmbeddingLookup::Run(toco::Model*, std::size_t)':
tensorflow/contrib/lite/toco/graph_transformations/unpartition_embedding_lookup.cc:57:24: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (size_t i = 0; i < stitch_op->num_partitions; ++i) {
                        ^
tensorflow/contrib/lite/toco/graph_transformations/unpartition_embedding_lookup.cc:60:48: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (size_t i = stitch_op->num_partitions; i < stitch_op->num_partitions * 2;
                                                ^
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/convert_trivial_transpose_to_reshape.cc:
tensorflow/contrib/lite/toco/graph_transformations/convert_trivial_transpose_to_reshape.cc: In function 'bool toco::{anonymous}::TransposeAffectsMemoryOrder(std::vector<int>, std::vector<int>)':
tensorflow/contrib/lite/toco/graph_transformations/convert_trivial_transpose_to_reshape.cc:34:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < in_shape.size(); i++) {
                     ^
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_pack.cc:
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_pack.cc: In instantiation of 'void toco::{anonymous}::Pack(toco::Model*, const toco::PackOperator&) [with toco::ArrayDataType Type = (toco::ArrayDataType)2u]':
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_pack.cc:90:45:   required from here
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_pack.cc:39:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < op.inputs.size(); i++) {
                     ^
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_pack.cc: In instantiation of 'void toco::{anonymous}::Pack(toco::Model*, const toco::PackOperator&) [with toco::ArrayDataType Type = (toco::ArrayDataType)4u]':
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_pack.cc:93:45:   required from here
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_pack.cc:39:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_pack.cc: In instantiation of 'void toco::{anonymous}::Pack(toco::Model*, const toco::PackOperator&) [with toco::ArrayDataType Type = (toco::ArrayDataType)7u]':
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_pack.cc:96:45:   required from here
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_pack.cc:39:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_pack.cc: In instantiation of 'void toco::{anonymous}::Pack(toco::Model*, const toco::PackOperator&) [with toco::ArrayDataType Type = (toco::ArrayDataType)9u]':
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_pack.cc:99:45:   required from here
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_pack.cc:39:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/propagate_array_data_types.cc:
tensorflow/contrib/lite/toco/graph_transformations/propagate_array_data_types.cc: In member function 'virtual bool toco::PropagateArrayDataTypes::Run(toco::Model*, std::size_t)':
tensorflow/contrib/lite/toco/graph_transformations/propagate_array_data_types.cc:158:25: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
       for (int i = 0; i < op->outputs.size(); ++i) {
                         ^
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/drop_fake_quant.cc:
tensorflow/contrib/lite/toco/graph_transformations/drop_fake_quant.cc: In member function 'virtual bool toco::DropFakeQuant::Run(toco::Model*, std::size_t)':
tensorflow/contrib/lite/toco/graph_transformations/drop_fake_quant.cc:46:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 1; i < fakequant_op->inputs.size(); i++) {
                     ^
INFO: From Compiling tensorflow/contrib/lite/toco/tensorflow_graph_matching/resolve_svdf.cc:
tensorflow/contrib/lite/toco/tensorflow_graph_matching/resolve_svdf.cc: In member function 'void toco::SvdfCluster::MaybeMergeConstNodes(const std::vector<const tensorflow::NodeDef*>&, bool, const std::unique_ptr<tensorflow::NodeDef>&)':
tensorflow/contrib/lite/toco/tensorflow_graph_matching/resolve_svdf.cc:189:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < const_node_parts.size(); i++) {
                     ^
tensorflow/contrib/lite/toco/tensorflow_graph_matching/resolve_svdf.cc:217:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < const_node_parts.size(); i++) {
                     ^
INFO: From Compiling tensorflow/contrib/lite/toco/toco_cmdline_flags.cc:
tensorflow/contrib/lite/toco/toco_cmdline_flags.cc: In function 'void toco::ReadTocoFlagsFromCommandLineFlags(const toco::ParsedTocoFlags&, toco::TocoFlags*)':
tensorflow/contrib/lite/toco/toco_cmdline_flags.cc:305:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     for (int i = 1; i < input_types.size(); i++) {
                       ^
INFO: From Compiling tensorflow/contrib/lite/toco/model_cmdline_flags.cc:
In file included from ./tensorflow/core/lib/strings/stringprintf.h:29:0,
                 from ./tensorflow/contrib/lite/toco/format_port.h:23,
                 from ./tensorflow/contrib/lite/toco/toco_port.h:23,
                 from ./tensorflow/contrib/lite/toco/args.h:24,
                 from ./tensorflow/contrib/lite/toco/model_cmdline_flags.h:22,
                 from tensorflow/contrib/lite/toco/model_cmdline_flags.cc:15:
tensorflow/contrib/lite/toco/model_cmdline_flags.cc: In function 'void toco::ReadModelFlagsFromCommandLineFlags(const toco::ParsedModelFlags&, toco::ModelFlags*)':
tensorflow/contrib/lite/toco/model_cmdline_flags.cc:263:31: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     QCHECK(mean_values.size() == model_flags->input_arrays_size());
                               ^
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/core/platform/default/logging.h:294:27: note: in expansion of macro 'CHECK'
 #define QCHECK(condition) CHECK(condition)
                           ^
tensorflow/contrib/lite/toco/model_cmdline_flags.cc:263:5: note: in expansion of macro 'QCHECK'
     QCHECK(mean_values.size() == model_flags->input_arrays_size());
     ^
tensorflow/contrib/lite/toco/model_cmdline_flags.cc:264:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     for (int i = 0; i < mean_values.size(); ++i) {
                       ^
In file included from ./tensorflow/core/lib/strings/stringprintf.h:29:0,
                 from ./tensorflow/contrib/lite/toco/format_port.h:23,
                 from ./tensorflow/contrib/lite/toco/toco_port.h:23,
                 from ./tensorflow/contrib/lite/toco/args.h:24,
                 from ./tensorflow/contrib/lite/toco/model_cmdline_flags.h:22,
                 from tensorflow/contrib/lite/toco/model_cmdline_flags.cc:15:
tensorflow/contrib/lite/toco/model_cmdline_flags.cc:280:30: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     QCHECK(std_values.size() == model_flags->input_arrays_size());
                              ^
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/core/platform/default/logging.h:294:27: note: in expansion of macro 'CHECK'
 #define QCHECK(condition) CHECK(condition)
                           ^
tensorflow/contrib/lite/toco/model_cmdline_flags.cc:280:5: note: in expansion of macro 'QCHECK'
     QCHECK(std_values.size() == model_flags->input_arrays_size());
     ^
tensorflow/contrib/lite/toco/model_cmdline_flags.cc:281:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     for (int i = 0; i < std_values.size(); ++i) {
                       ^
In file included from ./tensorflow/core/lib/strings/stringprintf.h:29:0,
                 from ./tensorflow/contrib/lite/toco/format_port.h:23,
                 from ./tensorflow/contrib/lite/toco/toco_port.h:23,
                 from ./tensorflow/contrib/lite/toco/args.h:24,
                 from ./tensorflow/contrib/lite/toco/model_cmdline_flags.h:22,
                 from tensorflow/contrib/lite/toco/model_cmdline_flags.cc:15:
tensorflow/contrib/lite/toco/model_cmdline_flags.cc:298:36: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     QCHECK(input_data_types.size() == model_flags->input_arrays_size());
                                    ^
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/core/platform/default/logging.h:294:27: note: in expansion of macro 'CHECK'
 #define QCHECK(condition) CHECK(condition)
                           ^
tensorflow/contrib/lite/toco/model_cmdline_flags.cc:298:5: note: in expansion of macro 'QCHECK'
     QCHECK(input_data_types.size() == model_flags->input_arrays_size());
     ^
tensorflow/contrib/lite/toco/model_cmdline_flags.cc:299:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     for (int i = 0; i < input_data_types.size(); ++i) {
                       ^
In file included from ./tensorflow/core/lib/strings/stringprintf.h:29:0,
                 from ./tensorflow/contrib/lite/toco/format_port.h:23,
                 from ./tensorflow/contrib/lite/toco/toco_port.h:23,
                 from ./tensorflow/contrib/lite/toco/args.h:24,
                 from ./tensorflow/contrib/lite/toco/model_cmdline_flags.h:22,
                 from tensorflow/contrib/lite/toco/model_cmdline_flags.cc:15:
tensorflow/contrib/lite/toco/model_cmdline_flags.cc:321:32: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     QCHECK(input_shapes.size() == model_flags->input_arrays_size());
                                ^
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/core/platform/default/logging.h:294:27: note: in expansion of macro 'CHECK'
 #define QCHECK(condition) CHECK(condition)
                           ^
tensorflow/contrib/lite/toco/model_cmdline_flags.cc:321:5: note: in expansion of macro 'QCHECK'
     QCHECK(input_shapes.size() == model_flags->input_arrays_size());
     ^
tensorflow/contrib/lite/toco/model_cmdline_flags.cc:322:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     for (int i = 0; i < input_shapes.size(); ++i) {
                       ^
INFO: From Compiling tensorflow/stream_executor/cuda/cuda_driver.cc:
tensorflow/stream_executor/cuda/cuda_driver.cc: In static member function 'static stream_executor::port::Status stream_executor::cuda::CUDADriver::CreateContext(CUdevice, const stream_executor::DeviceOptions&, stream_executor::cuda::CudaContext**)':
tensorflow/stream_executor/cuda/cuda_driver.cc:394:36: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   if (former_primary_context_flags != flags) {
                                    ^
INFO: From Compiling tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:180:15: warning: 'std::__cxx11::string stream_executor::cuda::GetBinaryDir(bool)' defined but not used [-Wunused-function]
 static string GetBinaryDir(bool strip_exe) {
               ^
INFO: From Compiling tensorflow/stream_executor/cuda/cuda_dnn.cc:
tensorflow/stream_executor/cuda/cuda_dnn.cc: In function 'cudnnDataType_t stream_executor::cuda::{anonymous}::GetCudnnDataType(stream_executor::dnn::DataLayout) [with T = signed char]':
tensorflow/stream_executor/cuda/cuda_dnn.cc:165:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
INFO: From Compiling tensorflow/stream_executor/cuda/cuda_blas.cc:
tensorflow/stream_executor/cuda/cuda_blas.cc: In function 'cudaDataType_t stream_executor::cuda::{anonymous}::CUDAComputationType(stream_executor::blas::ComputationType)':
tensorflow/stream_executor/cuda/cuda_blas.cc:623:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
INFO: From Compiling tensorflow/stream_executor/stream_executor_pimpl.cc:
tensorflow/stream_executor/stream_executor_pimpl.cc: In member function 'void* stream_executor::StreamExecutor::Allocate(tensorflow::uint64)':
tensorflow/stream_executor/stream_executor_pimpl.cc:476:31: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
       mem_alloc_bytes_ + size > memory_limit_bytes_) {
                               ^
In file included from ./tensorflow/core/platform/default/logging.h:24:0,
                 from ./tensorflow/core/platform/logging.h:25,
                 from ./tensorflow/core/lib/core/status.h:25,
                 from ./tensorflow/stream_executor/lib/status.h:21,
                 from ./tensorflow/stream_executor/stream_executor_pimpl.h:26,
                 from tensorflow/stream_executor/stream_executor_pimpl.cc:20:
./tensorflow/core/platform/default/logging.h: In instantiation of 'std::__cxx11::string* tensorflow::internal::Check_EQImpl(const T1&, const T2&, const char*) [with T1 = int; T2 = long long unsigned int; std::__cxx11::string = std::__cxx11::basic_string<char>]':
tensorflow/stream_executor/stream_executor_pimpl.cc:712:3:   required from here
./tensorflow/core/platform/default/logging.h:230:25: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
                         ==)  // Compilation error with CHECK_EQ(NULL, x)?
                         ^
./tensorflow/core/platform/macros.h:88:49: note: in definition of macro 'TF_PREDICT_TRUE'
 #define TF_PREDICT_TRUE(x) (__builtin_expect(!!(x), 1))
                                                 ^
./tensorflow/core/platform/default/logging.h:229:1: note: in expansion of macro 'TF_DEFINE_CHECK_OP_IMPL'
 TF_DEFINE_CHECK_OP_IMPL(Check_EQ,
 ^
INFO: From Compiling tensorflow/stream_executor/stream.cc:
tensorflow/stream_executor/stream.cc: In member function 'stream_executor::Stream* stream_executor::Stream::GetOrCreateSubStream()':
tensorflow/stream_executor/stream.cc:1967:31: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int64 index = 0; index < sub_streams_.size();) {
                               ^
tensorflow/stream_executor/stream.cc: In member function 'void stream_executor::Stream::ReturnSubStream(stream_executor::Stream*)':
tensorflow/stream_executor/stream.cc:2013:31: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int64 index = 0; index < sub_streams_.size(); ++index) {
                               ^
INFO: From Compiling tensorflow/core/platform/stacktrace_handler.cc:
tensorflow/core/platform/stacktrace_handler.cc: In function 'void tensorflow::testing::InstallStacktraceHandler()':
tensorflow/core/platform/stacktrace_handler.cc:118:51: warning: ignoring return value of 'ssize_t write(int, const void*, size_t)', declared with attribute warn_unused_result [-Wunused-result]
       (void)write(STDERR_FILENO, buf, strlen(buf));
                                                   ^
tensorflow/core/platform/stacktrace_handler.cc:125:51: warning: ignoring return value of 'ssize_t write(int, const void*, size_t)', declared with attribute warn_unused_result [-Wunused-result]
       (void)write(STDERR_FILENO, buf, strlen(buf));
                                                   ^
tensorflow/core/platform/stacktrace_handler.cc: In function 'void tensorflow::testing::StacktraceHandler(int, siginfo_t*, void*)':
tensorflow/core/platform/stacktrace_handler.cc:80:47: warning: ignoring return value of 'ssize_t write(int, const void*, size_t)', declared with attribute warn_unused_result [-Wunused-result]
   (void)write(STDERR_FILENO, buf, strlen(buf));
                                               ^
tensorflow/core/platform/stacktrace_handler.cc:90:70: warning: ignoring return value of 'ssize_t write(int, const void*, size_t)', declared with attribute warn_unused_result [-Wunused-result]
   (void)write(STDERR_FILENO, stacktrace.c_str(), stacktrace.length());
                                                                      ^
tensorflow/core/platform/stacktrace_handler.cc: In function 'void tensorflow::testing::SafePrintStackTrace()':
tensorflow/core/platform/stacktrace_handler.cc:47:59: warning: ignoring return value of 'ssize_t write(int, const void*, size_t)', declared with attribute warn_unused_result [-Wunused-result]
   (void)write(STDERR_FILENO, begin_msg, strlen(begin_msg));
                                                           ^
tensorflow/core/platform/stacktrace_handler.cc:58:55: warning: ignoring return value of 'ssize_t write(int, const void*, size_t)', declared with attribute warn_unused_result [-Wunused-result]
   (void)write(STDERR_FILENO, end_msg, strlen(end_msg));
                                                       ^
INFO: From Compiling tensorflow/core/ops/word2vec_ops.cc:
In file included from ./tensorflow/core/framework/common_shape_fns.h:22:0,
                 from tensorflow/core/ops/word2vec_ops.cc:16:
./tensorflow/core/util/tensor_format.h: In function 'tensorflow::TensorShape tensorflow::ShapeFromFormat(tensorflow::TensorFormat, tensorflow::int64, tensorflow::gtl::ArraySlice<long long int>, tensorflow::int64)':
./tensorflow/core/util/tensor_format.h:501:45: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (format == FORMAT_NHWC_VECT_W && dim == spatial.size() - 1) {
                                             ^
In file included from ./tensorflow/core/platform/default/logging.h:24:0,
                 from ./tensorflow/core/platform/logging.h:25,
                 from ./tensorflow/core/lib/core/status.h:25,
                 from ./tensorflow/core/lib/core/errors.h:21,
                 from ./tensorflow/core/framework/tensor_shape.h:23,
                 from ./tensorflow/core/framework/partial_tensor_shape.h:20,
                 from ./tensorflow/core/framework/attr_value_util.h:23,
                 from ./tensorflow/core/framework/node_def_util.h:22,
                 from ./tensorflow/core/framework/shape_inference.h:20,
                 from ./tensorflow/core/framework/common_shape_fns.h:20,
                 from tensorflow/core/ops/word2vec_ops.cc:16:
./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetTensorDim(tensorflow::gtl::ArraySlice<T>, tensorflow::TensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':
./tensorflow/core/util/tensor_format.h:452:47:   required from here
./tensorflow/core/util/tensor_format.h:420:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(index >= 0 && index < dimension_attributes.size())
                             ^
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/core/util/tensor_format.h:420:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attributes.size())
   ^
./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':
./tensorflow/core/util/tensor_format.h:461:54:   required from here
./tensorflow/core/util/tensor_format.h:435:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(index >= 0 && index < dimension_attribute.size())
                             ^
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/core/util/tensor_format.h:435:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attribute.size())
   ^
INFO: From Compiling tensorflow/core/kernels/lookup_util.cc:
In file included from ./tensorflow/core/framework/common_shape_fns.h:22:0,
                 from ./tensorflow/core/framework/resource_mgr.h:25,
                 from ./tensorflow/core/framework/lookup_interface.h:19,
                 from ./tensorflow/core/kernels/lookup_util.h:19,
                 from tensorflow/core/kernels/lookup_util.cc:16:
./tensorflow/core/util/tensor_format.h: In function 'tensorflow::TensorShape tensorflow::ShapeFromFormat(tensorflow::TensorFormat, tensorflow::int64, tensorflow::gtl::ArraySlice<long long int>, tensorflow::int64)':
./tensorflow/core/util/tensor_format.h:501:45: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (format == FORMAT_NHWC_VECT_W && dim == spatial.size() - 1) {
                                             ^
tensorflow/core/kernels/lookup_util.cc: In member function 'virtual void tensorflow::lookup::{anonymous}::TextFileLineIterator::Next()':
tensorflow/core/kernels/lookup_util.cc:133:46: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
       if (std::max(key_index_, value_index_) >= tokens.size()) {
                                              ^
In file included from ./tensorflow/core/platform/default/logging.h:24:0,
                 from ./tensorflow/core/platform/logging.h:25,
                 from ./tensorflow/core/lib/core/status.h:25,
                 from ./tensorflow/core/lib/core/errors.h:21,
                 from ./tensorflow/core/framework/tensor_shape.h:23,
                 from ./tensorflow/core/framework/partial_tensor_shape.h:20,
                 from ./tensorflow/core/framework/attr_value_util.h:23,
                 from ./tensorflow/core/framework/node_def_util.h:22,
                 from ./tensorflow/core/framework/shape_inference.h:20,
                 from ./tensorflow/core/framework/common_shape_fns.h:20,
                 from ./tensorflow/core/framework/resource_mgr.h:25,
                 from ./tensorflow/core/framework/lookup_interface.h:19,
                 from ./tensorflow/core/kernels/lookup_util.h:19,
                 from tensorflow/core/kernels/lookup_util.cc:16:
./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetTensorDim(tensorflow::gtl::ArraySlice<T>, tensorflow::TensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':
./tensorflow/core/util/tensor_format.h:452:47:   required from here
./tensorflow/core/util/tensor_format.h:420:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(index >= 0 && index < dimension_attributes.size())
                             ^
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/core/util/tensor_format.h:420:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attributes.size())
   ^
./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':
./tensorflow/core/util/tensor_format.h:461:54:   required from here
./tensorflow/core/util/tensor_format.h:435:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(index >= 0 && index < dimension_attribute.size())
                             ^
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/core/util/tensor_format.h:435:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attribute.size())
   ^
tensorflow/core/kernels/lookup_util.cc: In member function 'virtual tensorflow::int64 tensorflow::lookup::{anonymous}::TextFileLineIterator::total_size() const':
tensorflow/core/kernels/lookup_util.cc:172:51: warning: 'new_size' may be used uninitialized in this function [-Wmaybe-uninitialized]
       *const_cast<int64*>(&vocab_size_) = new_size;
                                                   ^
INFO: From Compiling tensorflow/core/kernels/initializable_lookup_table.cc:
In file included from ./tensorflow/core/framework/common_shape_fns.h:22:0,
                 from ./tensorflow/core/framework/resource_mgr.h:25,
                 from ./tensorflow/core/framework/lookup_interface.h:19,
                 from ./tensorflow/core/kernels/initializable_lookup_table.h:19,
                 from tensorflow/core/kernels/initializable_lookup_table.cc:16:
./tensorflow/core/util/tensor_format.h: In function 'tensorflow::TensorShape tensorflow::ShapeFromFormat(tensorflow::TensorFormat, tensorflow::int64, tensorflow::gtl::ArraySlice<long long int>, tensorflow::int64)':
./tensorflow/core/util/tensor_format.h:501:45: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (format == FORMAT_NHWC_VECT_W && dim == spatial.size() - 1) {
                                             ^
In file included from ./tensorflow/core/platform/default/logging.h:24:0,
                 from ./tensorflow/core/platform/logging.h:25,
                 from ./tensorflow/core/lib/core/status.h:25,
                 from ./tensorflow/core/lib/core/errors.h:21,
                 from ./tensorflow/core/framework/tensor_shape.h:23,
                 from ./tensorflow/core/framework/partial_tensor_shape.h:20,
                 from ./tensorflow/core/framework/attr_value_util.h:23,
                 from ./tensorflow/core/framework/node_def_util.h:22,
                 from ./tensorflow/core/framework/shape_inference.h:20,
                 from ./tensorflow/core/framework/common_shape_fns.h:20,
                 from ./tensorflow/core/framework/resource_mgr.h:25,
                 from ./tensorflow/core/framework/lookup_interface.h:19,
                 from ./tensorflow/core/kernels/initializable_lookup_table.h:19,
                 from tensorflow/core/kernels/initializable_lookup_table.cc:16:
./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetTensorDim(tensorflow::gtl::ArraySlice<T>, tensorflow::TensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':
./tensorflow/core/util/tensor_format.h:452:47:   required from here
./tensorflow/core/util/tensor_format.h:420:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(index >= 0 && index < dimension_attributes.size())
                             ^
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/core/util/tensor_format.h:420:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attributes.size())
   ^
./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':
./tensorflow/core/util/tensor_format.h:461:54:   required from here
./tensorflow/core/util/tensor_format.h:435:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(index >= 0 && index < dimension_attribute.size())
                             ^
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/core/util/tensor_format.h:435:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attribute.size())
   ^
INFO: From Compiling tensorflow/core/grappler/grappler_item.cc:
In file included from tensorflow/core/grappler/grappler_item.cc:25:0:
./tensorflow/core/grappler/utils.h: In function 'int tensorflow::grappler::NodePositionIfSameNode(const string&, const string&)':
./tensorflow/core/grappler/utils.h:117:49: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
       std::distance(input_it, input_name.end()) < node_name.size()) {
                                                 ^
INFO: From Compiling tensorflow/core/ops/array_ops.cc:
In file included from ./tensorflow/core/framework/common_shape_fns.h:22:0,
                 from tensorflow/core/ops/array_ops.cc:16:
./tensorflow/core/util/tensor_format.h: In function 'int tensorflow::GetTensorDimsFromSpatialDims(int, tensorflow::TensorFormat)':
./tensorflow/core/util/tensor_format.h:148:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
INFO: From Compiling tensorflow/core/grappler/grappler_item.cc:
In file included from tensorflow/core/grappler/grappler_item.cc:25:0:
./tensorflow/core/grappler/utils.h: In function 'int tensorflow::grappler::NodePositionIfSameNode(const string&, const string&)':
./tensorflow/core/grappler/utils.h:117:49: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
       std::distance(input_it, input_name.end()) < node_name.size()) {
                                                 ^
INFO: From Compiling tensorflow/core/grappler/op_types.cc:
In file included from tensorflow/core/grappler/op_types.cc:22:0:
./tensorflow/core/grappler/utils.h: In function 'int tensorflow::grappler::NodePositionIfSameNode(const string&, const string&)':
./tensorflow/core/grappler/utils.h:117:49: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
       std::distance(input_it, input_name.end()) < node_name.size()) {
                                                 ^
INFO: From Compiling tensorflow/core/grappler/op_types.cc:
In file included from tensorflow/core/grappler/op_types.cc:22:0:
./tensorflow/core/grappler/utils.h: In function 'int tensorflow::grappler::NodePositionIfSameNode(const string&, const string&)':
./tensorflow/core/grappler/utils.h:117:49: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
       std::distance(input_it, input_name.end()) < node_name.size()) {
                                                 ^
INFO: From Compiling tensorflow/core/grappler/utils.cc:
In file included from tensorflow/core/grappler/utils.cc:16:0:
./tensorflow/core/grappler/utils.h: In function 'int tensorflow::grappler::NodePositionIfSameNode(const string&, const string&)':
./tensorflow/core/grappler/utils.h:117:49: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
       std::distance(input_it, input_name.end()) < node_name.size()) {
                                                 ^
tensorflow/core/grappler/utils.cc: In function 'void tensorflow::grappler::PermuteNodesInPlace(tensorflow::GraphDef*, std::vector<int>*, bool)':
tensorflow/core/grappler/utils.cc:327:14: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     while (n != (*permutation)[n]) {
              ^
tensorflow/core/grappler/utils.cc: In member function 'std::__cxx11::string tensorflow::grappler::SimpleGraphView::PrintToString() const':
tensorflow/core/grappler/utils.cc:500:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     for (int j = 0; j < outputs(i).size(); ++j) {
                       ^
INFO: From Compiling tensorflow/contrib/lite/toco/export_tensorflow.cc:
tensorflow/contrib/lite/toco/export_tensorflow.cc:1881:6: warning: 'void toco::{anonymous}::ConvertSparseToDenseOperator(const toco::Model&, const toco::SparseToDenseOperator&, const char*, tensorflow::GraphDef*)' defined but not used [-Wunused-function]
 void ConvertSparseToDenseOperator(const Model& model,
      ^
INFO: From Compiling tensorflow/core/common_runtime/hierarchical_tree_broadcaster.cc:
tensorflow/core/common_runtime/hierarchical_tree_broadcaster.cc: In member function 'int tensorflow::HierarchicalTreeBroadcaster::GetDeviceTask(int, const std::vector<int>&)':
tensorflow/core/common_runtime/hierarchical_tree_broadcaster.cc:71:72: warning: 'task_hi' may be used uninitialized in this function [-Wmaybe-uninitialized]
   LOG(FATAL) << ""Unexpected device rank "" << device_rank << "" for "" << task_hi
                                                                        ^
Target //tensorflow/contrib/lite/toco:toco up-to-date:
  bazel-bin/tensorflow/contrib/lite/toco/toco
INFO: Elapsed time: 491.360s, Critical Path: 35.66s
INFO: 1474 processes: 1474 local.
INFO: Build completed successfully, 1482 total actions
INFO: Running command line: bazel-bin/tensorflow/contrib/lite/toco/toco '--input_file=/tmp/tflite/tflite_graph.pb' '--output_file=/tmp/tflite/detect.tflite' '--input_shapes=1,300,300,3' '--input_arrays=normalized_input_image_tensor' '--output_arrays=TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3' '--inference_type=QUANTIZED_UINT8' '--mean_values=128' '--std_values=128' '--change_concat_input_ranges=falINFO: Build completed successfully, 1482 total actions
2018-10-29 14:13:43.319112: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TFLite_Detection_PostProcess
2018-10-29 14:13:43.337239: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 500 operators, 754 arrays (0 quantized)
2018-10-29 14:13:43.347042: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 500 operators, 754 arrays (0 quantized)
2018-10-29 14:13:43.366275: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 65 operators, 177 arrays (1 quantized)
2018-10-29 14:13:43.367151: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 65 operators, 177 arrays (1 quantized)
2018-10-29 14:13:43.367654: F tensorflow/contrib/lite/toco/tooling_util.cc:1634] Array FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Relu6, which is an input to the DepthwiseConv operator producing the output array FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/Relu6, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.
Aborted (core dumped)

```
You can see the last output, Aborted (core dumped).
And generated `detect.tflite`file has 0 kb file size mening empty.

What is the problem? please help me guys.
"
23339,Output 'tensorflow/core/kernels/_objs/broadcast_to_op_gpu/broadcast_to_op_gpu.cu.pic.o' was not created,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.5
- TensorFlow installed from (source or binary): source
- TensorFlow version: r1.11
- Python version: 3.6.6
- Installed using virtualenv? pip? conda?: None
- Bazel version (if compiling from source): 0.18.0
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version: 8.0 / 7.0.1
- GPU model and memory: GeForce GT 740M; 2004MiB



**Describe the problem**

Every time I  use the `bazel` command to build tensorflow, this error appears:

`ERROR: /home/rosgori/Python/tengpu/tensorflow/tensorflow/core/kernels/BUILD:673:1: output 'tensorflow/core/kernels/_objs/broadcast_to_op_gpu/broadcast_to_op_gpu.cu.pic.o' was not created`

I have used `gcc` version 5.4 and 4.9 and every time a new error appears, some examples:

```
ERROR: /home/rosgori/Python/tengpu/tensorflow/tensorflow/core/kernels/BUILD:2250:1: output 'tensorflow/core/kernels/_objs/resize_nearest_neighbor_op_gpu/resize_nearest_neighbor_op_gpu.cu.pic.o' was not created

ERROR: /home/rosgori/Python/tengpu/tensorflow/tensorflow/contrib/image/BUILD:115:1: output 'tensorflow/contrib/image/_objs/python/ops/_distort_image_ops_gpu/adjust_hsv_in_yiq_op_gpu.cu.pic.o' was not created

ERROR: /home/rosgori/Python/tengpu/tensorflow/tensorflow/core/kernels/BUILD:3360:1: output 'tensorflow/core/kernels/_objs/conv_ops_gpu/conv_ops_gpu_3.cu.pic.o' was not created

ERROR: /home/rosgori/Python/tengpu/tensorflow/tensorflow/core/kernels/BUILD:2178:1: output 'tensorflow/core/kernels/_objs/crop_and_resize_op_gpu/crop_and_resize_op_gpu.cu.pic.o' was not created

ERROR: /home/rosgori/Python/tengpu/tensorflow/tensorflow/core/kernels/BUILD:4355:1: output 'tensorflow/core/kernels/_objs/scatter_op_gpu/scatter_op_gpu.cu.pic.o' was not created

ERROR: /home/rosgori/Python/tengpu/tensorflow/tensorflow/core/kernels/BUILD:3782:1: output 'tensorflow/core/kernels/_objs/batch_space_ops_gpu/spacetobatch_functor_gpu.cu.pic.o' was not created

ERROR: /home/rosgori/Python/tengpu/tensorflow/tensorflow/core/kernels/BUILD:673:1: output 'tensorflow/core/kernels/_objs/broadcast_to_op_gpu/broadcast_to_op_gpu.cu.pic.o' was not created
```
They have something in common, the extension `.cu.pic.o` is in every error. So my questions are: What is the meaning of this error? Can it be fixed?

I even tried `r1.12` today (2018-10-28) with no success. I think it is strange that the build was successful when I tried `r1.10`, so... I don't know what happened there.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

`bazel build --verbose_failures -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2 --config=opt --config=cuda --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/tools/pip_package:build_pip_package`

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23338,AttributeError: 'module' object has no attribute 'Model',"This is my TensorFlow configuration.
tensorflow-gpu          1.5.0
Keras                  2.2.4    
cuda           8.0
cudnn       6.0
The following error occurred when I tried to run the TensorFlow SSD-mobilenet model.
root@VM-0-15-ubuntu:/mine/models/research# python object_detection/model_main.py --train_dir object_detection/train --pipeline_config_path object_detection/ssd_model/ssd_mobilenet_v1_pets.config
/usr/lib/python2.7/dist-packages/matplotlib/__init__.py:1352: UserWarning:  This call to matplotlib.use() has no effect
because the backend has already been chosen;
matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

  warnings.warn(_use_error_msg)
Traceback (most recent call last):
  File ""object_detection/model_main.py"", line 26, in <module>
    from object_detection import model_lib
  File ""/usr/local/lib/python2.7/dist-packages/object_detection-0.1-py2.7.egg/object_detection/model_lib.py"", line 28, in <module>
    from object_detection import inputs
  File ""/usr/local/lib/python2.7/dist-packages/object_detection-0.1-py2.7.egg/object_detection/inputs.py"", line 26, in <module>
    from object_detection.builders import model_builder
  File ""/usr/local/lib/python2.7/dist-packages/object_detection-0.1-py2.7.egg/object_detection/builders/model_builder.py"", line 22, in <module>
    from object_detection.builders import box_predictor_builder
  File ""/usr/local/lib/python2.7/dist-packages/object_detection-0.1-py2.7.egg/object_detection/builders/box_predictor_builder.py"", line 21, in <module>
    from object_detection.predictors import convolutional_box_predictor
  File ""/usr/local/lib/python2.7/dist-packages/object_detection-0.1-py2.7.egg/object_detection/predictors/convolutional_box_predictor.py"", line 19, in <module>
    from object_detection.core import box_predictor
  File ""/usr/local/lib/python2.7/dist-packages/object_detection-0.1-py2.7.egg/object_detection/core/box_predictor.py"", line 137, in <module>
    class KerasBoxPredictor(tf.keras.Model):
AttributeError: 'module' object has no attribute 'Model'

Try the same mistakes with tensorflow-gpu 1.4 and 1.5, and try changing the Keras version but still have the same problem.

Who can help me? Thank you very much.

"
23337,[Feature Request][TPU] tf.image.crop_and_resize in TPU,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.11.0, TPU
- Are you willing to contribute it (Yes/No):

**Describe the feature and the current behavior/state.**
tf.image.crop_and_resize is a very important function of the object detection tasks.  Most the object detection networks are based on region proposal network.  However, it's not supported by TPU yet.  I fail to implement it with existing tpu APIs

I find that now only tf.image.central_crop and tf.image.resize_bilinear are supported. Now they both have some limits, eg, Crop factor must be a compile-time constant.

**Will this change the current api? How?**
No.

**Who will benefit with this feature?**
TPU users. It is very important for the google cloud.

**Any Other info.**
"
23334,Bug: issue initializing iterator,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
 Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
N/A
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
Version: v1.11.0-0-gc19e29306c 1.11.0
 **Python version**:
3.5.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
See source code below

### Describe the problem
There seems to be an issue initializing an iterator created with `tf.data.Iterator.form_structure` a second time in the _same_ session where the dataset contains `Function` instances.

### Source code / logs
To produce problem
```python
import tensorflow as tf

def g():
  for i in range(10):
    yield i

iterator = tf.data.Iterator.from_structure(tf.int64, tf.TensorShape([]))
x = iterator.get_next()
sess = tf.Session()

dataset_1 = tf.data.Dataset.from_generator(g, output_types=tf.int64)
sess.run(iterator.make_initializer(dataset_1))
sess.run(x)

dataset_2 = tf.data.Dataset.from_generator(g, output_types=tf.int64)
sess.run(iterator.make_initializer(dataset_2))
sess.run(x)
```
Executing the above leads to the following error
```
NotFoundError (see above for traceback): Function tf_data_structured_function_wrapper_uv0gphzJzQY is not defined.
```"
23333,tf.scatter_update does not update variable with float type on GPU ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 18.04.1 LTS
- TensorFlow installed from (source or binary): 1.11
- TensorFlow version (use command below): pip 
- Python version: 3.6
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA release 9.0, V9.0.176
- GPU model and memory: TITAN Xp / 12Gb

**Describe the current behavior**

On GPU, tf.scatter_update does not update the variable if the type of the variable is defined as float. However, the same code works without a problem if the code is executed on CPU. If you run the code I provided here for you, you will get random values like the one that comes in the following every time you run the code:  

[array([-0.27238935, -0.11273658, -0.18543988, -0.34574947, -0.4174637 ,
       -0.35478222], dtype=float32), 6]

This problem does not happen on a CPU, or if the variable type is set to integer. 

**Describe the expected behavior**
It should simply update the variable and should output the following in a deterministic manner: 


[array([19., 19., 19., 19., 19., 19.], dtype=float32), 6]

I suppose this is a bug in the caching process involved if the code is run on a GPU.  

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

    import tensorflow as tf
    import os

    os.environ[""CUDA_DEVICE_ORDER""]=""PCI_BUS_ID""
    os.environ[""CUDA_VISIBLE_DEVICES""]='0'
    def cond(size, i):
	    return tf.less(i,size)

    def body(size, i):
	    b=2*7.5+c

	    with tf.variable_scope(""a"", reuse=tf.AUTO_REUSE):
		    a = tf.get_variable(""a"",[6],dtype=tf.float32)

	        a = tf.scatter_update(a,i,b)

	        with tf.control_dependencies([a]):
		        return (size, i+1)

    with tf.Session() as sess:
        c=tf.constant(4.0)
        i = tf.constant(0)
        size = tf.constant(6)
        _,i = tf.while_loop(cond,
                body,
                [size, i])
    
        a = tf.get_variable(""a"",[6],dtype=tf.float32)
    
        init = tf.initialize_all_variables()
        sess.run(init)
        print(sess.run([a,i]))

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23332,ctc_loss gives  No valid path found incorrectly,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I used stock functions as is but through keras
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win 10 
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.8
- Python version: 3.6.3
- CUDA/cuDNN version: 7.1 to the best of my memory
- GPU model and memory: ti 1070 6bg 

I am using ctc_loss() and I get the error ""T:\src\github\tensorflow\tensorflow\core\util\ctc\ctc_loss_calculator.cc:144] No valid path found."" 
I am pretty sure that all the data I fed in included some valid path - that is the length of the labels is ~.5x the length of the inputs and the labels have very few repetitions. 

Its a bit hard to reproduce since I am not sure which sample is causing the issue.
I can copy paste the batch that I feed into my network if that helps? 
 
"
23331,SpatialSoftmax,"It seams to me that `width_lin` should be used when we want to look up the corresponding value to x_loc: [this line](https://github.com/tensorflow/tensorflow/blob/6bfb36b241dadfecb345edb0589a8d0ae72dc968/tensorflow/contrib/layers/python/layers/layers_test.py#L3566)
```sh
 x_lin = np.expand_dims(np.array([height_lin[i] for i in x_loc]), 1)
```
should be

```sh
 x_lin = np.expand_dims(np.array([width_lin[i] for i in x_loc]), 1)
``` 
"
23330,download tensorflow to run on  python 3.7.1 : version not satisfied,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): WIN 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): GITHUB
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
Not able to download tensorflow

**Provide the exact sequence of commands / steps that you executed before running into the problem**
C:\WINDOWS\system32> pip install tensorflow
Collecting tensorflow
  Could not find a version that satisfies the requirement tensorflow (from versions: )
No matching distribution found for tensorflow

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23329,ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro 1809 x64 OS Build : 17763.55
- TensorFlow installed from (source or binary): Binary
- TensorFlow version: 1.10.0
- Python version: 3.6.6 x64
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: I'm using CPU version
- GPU model and memory: I'm using CPU version



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Here this the command that i use : import tensorflow as tf

**Any other info / logs**

Using TensorFlow backend.
Traceback (most recent call last):
File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
return importlib.import_module(mname)
File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\importlib_init_.py"", line 126, in import_module
return _bootstrap._gcd_import(name[level:], package, level)
File """", line 994, in _gcd_import
File """", line 971, in _find_and_load
File """", line 955, in _find_and_load_unlocked
File """", line 658, in _load_unlocked
File """", line 571, in module_from_spec
File """", line 922, in create_module
File """", line 219, in _call_with_frames_removed
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in 
from tensorflow.python.pywrap_tensorflow_internal import *
File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in 
_pywrap_tensorflow_internal = swig_import_helper()
File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
return importlib.import_module('pywrap_tensorflow_internal')
File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\importlib_init.py"", line 126, in import_module
return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
File ""decensor.py"", line 8, in 
from libs.pconv_hybrid_model import PConvUnet
File ""C:\Users\minat\Desktop\DeepCreamPy\libs\pconv_hybrid_model.py"", line 4, in 
from keras.models import Model
File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\keras_init_.py"", line 3, in 
from . import utils
File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\utils_init_.py"", line 6, in 
from . import conv_utils
File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\utils\conv_utils.py"", line 9, in 
from .. import backend as K
File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\backend_init_.py"", line 89, in 
from .tensorflow_backend import *
File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\backend\tensorflow_backend.py"", line 5, in 
import tensorflow as tf
File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_init_.py"", line 22, in 
from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import
File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python_init_.py"", line 49, in 
from tensorflow.python import pywrap_tensorflow
File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in 
raise ImportError(msg)
ImportError: Traceback (most recent call last):
File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
return importlib.import_module(mname)
File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\importlib_init_.py"", line 126, in import_module
return _bootstrap._gcd_import(name[level:], package, level)
File """", line 994, in _gcd_import
File """", line 971, in _find_and_load
File """", line 955, in _find_and_load_unlocked
File """", line 658, in _load_unlocked
File """", line 571, in module_from_spec
File """", line 922, in create_module
File """", line 219, in _call_with_frames_removed
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in 
from tensorflow.python.pywrap_tensorflow_internal import *
File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in 
_pywrap_tensorflow_internal = swig_import_helper()
File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
return importlib.import_module('pywrap_tensorflow_internal')
File ""C:\Users\minat\AppData\Local\Programs\Python\Python36\lib\importlib_init.py"", line 126, in import_module
return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions. Include the entire stack trace
above this error message when asking for help.
"
23328,TensorRT error:*** stack smashing detected ***: python3 terminated,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
not sure
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
1.7.0  and 1.10.0
- Python version:
3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
 5.4.0
- CUDA/cuDNN version:
CUDA9.0 / CUDNN 7.0.5
- GPU model and memory:
1080TI / 12g

I am using tensorflow version 1.7, cuda9.0, python 3.5 ，and following the readme.md. and I am getting following error.


ray@ray-MS-7B48:~/data/models/research/tensorrt$ python3 tensorrt.py --frozen_graph=resnetv2_imagenet_frozen_graph.pb --image_file=image.jpg --native --fp32 --fp16 --int8 --output_dir=./output
WARNING:tensorflow:From /home/ray/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Use the retry module or similar alternatives.
2018-10-28 16:42:07.777004: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-10-28 16:42:07.889329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-10-28 16:42:07.889593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:01:00.0
totalMemory: 10.91GiB freeMemory: 10.31GiB
2018-10-28 16:42:07.889607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2018-10-28 16:42:08.417071: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-10-28 16:42:08.417094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 
2018-10-28 16:42:08.417100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N 
2018-10-28 16:42:08.417464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8935 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
Running native graph
INFO:tensorflow:Starting execution
2018-10-28 16:42:08.973439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2018-10-28 16:42:08.973468: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-10-28 16:42:08.973474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 
2018-10-28 16:42:08.973477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N 
2018-10-28 16:42:08.973566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8935 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
INFO:tensorflow:Starting Warmup cycle
INFO:tensorflow:Starting timing.
INFO:tensorflow:Timing loop done!
Running FP32 graph
2018-10-28 16:42:30.856792: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 1
2018-10-28 16:42:31.229082: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:383] MULTIPLE tensorrt candidate conversion: 2
*** stack smashing detected ***: python3 terminated
Aborted (core dumped)

"
23327,tfp.distributions.TransformedDistribution can't handle input and output events with different ndims,"TransformedDistribution can infer event_ndims from base distribution's event_ndims. When the output event (usually an image) has different ndims from input event (usually a 1-D vector), the training would crash at TransformedDistribution::_log_prob(self,y). It calls underlying bijector's Bijector::inverse_log_det_jacobian(self,y,event_ndims) and pass inferred event_ndims which equals 1 according to base distribution's ndims (MultivariateNormalDiag in my case). but y is an image which has ndims of 3 (height x width x channel). The ndims sanity check would fail in Bijector::_call_inverse_log_det_jacobian(self,y,event_ndims).

I recommend that TransformedDistribution infer the forward_event_ndims and inverse_event_ndims separately from base distribution and underlying bijector respectively to get proper behavior.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a
- TensorFlow installed from (source or binary):pip repo
- TensorFlow version (use command below):1.11
- Python version:3.6
- Bazel version (if compiling from source):n/a
- GCC/Compiler version (if compiling from source):n/a
- CUDA/cuDNN version:n/a
- GPU model and memory:n/a"
23326,DataLossError (see above for traceback): not an sstable (bad magic number),"hi, @tensorflow.org.I got a problem in this case. I can run this code in windows, but I can't run this in my Ubuntu 16.04.I got a DataLossError. But the model was found in this case. And this model can run in windows.

I got this Error. So sad.
```
DataLossError (see above for traceback): not an sstable (bad magic number)
	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

```
And the code is like this.
```
ckpt_filename = '/home/yangziyuan/SSD-Tensorflow/checkpoints/ssd_300_vgg.ckpt'
isess.run(tf.global_variables_initializer())
saver = tf.train.Saver()
saver.restore(isess.ckpt_filename)
```
But saver.restore() stopped everything.
So anyone has any clue? Thanks for your guys time and patience.

"
23325,tf.contrib.image.dense_image_warp to support dynamic shape tensor input,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.11.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
I used optical flow to warp images with TF function tf.contrib.image.dense_image_warp . However, the current version  does not support dynamic shapes. 

**Will this change the current api? How?**
No.

**Who will benefit with this feature?**
People use this function to warp images.

**Any Other info.**
The source code of dense_image_warp:
***********
```
  with ops.name_scope(name):
    batch_size, height, width, channels = image.get_shape().as_list()
    # The flow is defined on the image grid. Turn the flow into a list of query
    # points in the grid space.
    grid_x, grid_y = array_ops.meshgrid(
        math_ops.range(width), math_ops.range(height))
    stacked_grid = math_ops.cast(
        array_ops.stack([grid_y, grid_x], axis=2), flow.dtype)
    batched_grid = array_ops.expand_dims(stacked_grid, axis=0)
    query_points_on_grid = batched_grid - flow
    query_points_flattened = array_ops.reshape(query_points_on_grid,
                                               [batch_size, height * width, 2])
    # Compute values at the query points, then reshape the result back to the
    # image grid.
    interpolated = _interpolate_bilinear(image, query_points_flattened)
    interpolated = array_ops.reshape(interpolated,
                                     [batch_size, height, width, channels])
    return interpolated
```

*************
I think changing 
`batch_size, height, width, channels = image.get_shape().as_list()`
to
`batch_size, height, width, channels = tf.shape(image).as_list()`
might be sufficient."
23324,spatialsoftmax,"It seams to me that `width_lin` should be used when we want to look up the corresponding value to x_loc: 
```sh
 x_lin = np.expand_dims(np.array([height_lin[i] for i in x_loc]), 1)
```
should be

```sh
 x_lin = np.expand_dims(np.array([width_lin[i] for i in x_loc]), 1)
``` 
"
23323,i encounter very strange and serious Bug with dataset feeding （Emergency）,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>

**System information** MAC OS 10.14 And Ubuntu 18 either
- TensorFlow version (you are using): 1.11.0

My model is LSTM + DNN, dataset structure is:

[
    [ [1,2,3],4,5,6 ],
    [ [11,22,33],44,55,66 ],
    [ [4,5,6],7,8,9 ],
]

convert to Dict is

{
    a: [ [1,2,3], [11,22,33], [4,5,6] ]
    b: [4,44,7]
    c: [5,55,8]
    d: [6,66,9]
}

it's work well but when LSTM exceed 31 step, i will reshape feature ""a"" to (time_step, columns) for LSTM input, when time_step more than 31, like 32, 33..., software will crash, and show bug info below:

`Retval[0] does not have value`

```
Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1295, in _do_call
    return fn(*args)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1277, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1375, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""estimator.py"", line 340, in <module>
    tf.app.run(main)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""estimator.py"", line 275, in main
    steps=args.train_steps
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 356, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1181, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1215, in _train_model_default
    saving_listeners)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1409, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 671, in run
    run_metadata=run_metadata)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1148, in run
    run_metadata=run_metadata)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1239, in run
    raise six.reraise(*original_exc_info)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1224, in run
    return self._sess.run(*args, **kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1296, in run
    run_metadata=run_metadata)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1076, in run
    return self._sess.run(*args, **kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 887, in run
    run_metadata_ptr)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1110, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run
    run_metadata)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1314, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value
``` 

i'm first think reason is data too long，so i change and reduce columns (32, 50) to (32, 30), (32, 10), (32, 1), it's can't work, but i change and reduce shape to (31, 50), (31, 30), (31, 60), (32, 1), (16, 60), (2, 16) it's work well, just say, when LSTM time step exceed 31, Tensorflow can't work,

**Who will benefit with this fix?**

every one

**Any Other info.**

i use tech beblow:

1. tensorflow estimator + dataset,
2. RNN ( LSTM + MUILTI + Dynamic),
3. DNN( fully connected layer )

it's strange Bug and important Bug, even cause i can't running completely now，hope official can fix Bug as soon as possible，very thanks



"
23322,How can parse_fn be put into tf.data.map without arguments?,"**System information**
- TensorFlow version: 1.10.1

**Describe the documentation issue**
I am familiar with parsing tfrecord back to tensor without using tf.data API. And now I'm trying to use this API to construct a more robust pipeline. The code goes like this:
`def parse_fn(serialized):
    features = {
        'image': tf.FixedLenFeature([], tf.string),
        'label': tf.FixedLenFeature([], tf.int64)
    }
    parse_exp = tf.parse_single_example(serialized=serialized, features=features)
    data = parse_exp['image']
    data = tf.decode_raw(data, tf.uint8)
    data = tf.cast(data, tf.float32)
    return data, parse_exp['label']


def input_fn(data_list, batch_size=32, shuffle_size=1024, prefetch_size=2):
    files = tf.data.Dataset.list_files(data_list)
    dataset = files.interleave(tf.data.TFRecordDataset)
    dataset = dataset.shuffle(buffer_size=shuffle_size)
    dataset = dataset.repeat()
    dataset = dataset.map(parse_fn, num_parallel_calls=4)
    dataset = dataset.batch(batch_size)
    dataset = dataset.prefetch(buffer_size=prefetch_size)
    iterator = dataset.make_one_shot_iterator()
    images_batch, labels_batch = iterator.get_next()
    return {'image': images_batch}, labels_batch`

But based on my knowledge, tfrecord file is a protocol buffer file containing binary data. So here are my 2 consecutive questions...
1. How can tf know which part and which part should be shuffled before parse the file into tf datatype? 
2. How can the parse_fn function be substituted into map func without argument? where will the 'serialized' argument come from? What if I want to add more arguments to parse_fn, what can I do?

Lastly, in this case I use 'label' as the int64List dict name to store data into tfrecord so that I can extract the corresponding data using key 'label' here again. But why I didn't get error when I use the other name as the key in order to get data? This makes me feel weird because when I was using TFRecordReader.read() to extract binary data, it is pretty sensitive that I should use exactly the same key name so that there would be no errors."
23321,"Win10, AMD: Failed to load the native TensorFlow runtime.","
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Pip
- TensorFlow version: tf nightly gpu [the last one until this date i suppose]
- Python version: 3.6.4 Anaconda
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: APU AMD A10 - AMD GPU R6 M340DX 2GB



**Describe the problem**

I had installed tf cpu in my computer, and then i wanted to use tf in my gpu, so i installed the nightly 
gpu version. After i installed it, i couldn't use more tensorflow

**Provide the exact sequence of commands / steps that you executed before running into the problem**

import tensorflow as tf

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
~\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

~\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>()
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

~\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

C:\ProgramData\Anaconda3\lib\imp.py in load_module(name, file, filename, details)
    242         else:
--> 243             return load_dynamic(name, filename, file)
    244     elif type_ == PKG_DIRECTORY:

C:\ProgramData\Anaconda3\lib\imp.py in load_dynamic(name, path, file)
    342             name=name, loader=loader, origin=path)
--> 343         return _load(spec)
    344 

ImportError: DLL load failed: No se puede encontrar el módulo especificado.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-4-1c24dce58f6f> in <module>()
      3 import numpy as np
      4 import os
----> 5 import tensorflow as tf
      6 import time

~\AppData\Roaming\Python\Python36\site-packages\tensorflow\__init__.py in <module>()
     22 
     23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     25 
     26 from tensorflow.python.tools import component_api_helper as _component_api_helper

~\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\__init__.py in <module>()
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 # Protocol buffers

~\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\Users\dcifuen3\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\dcifuen3\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\dcifuen3\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\ProgramData\Anaconda3\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\ProgramData\Anaconda3\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: No se puede encontrar el módulo especificado.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
23320,"[Feature Request] Addition of new operation to Tensorflow Lite for ""ENet""","<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>

**System information**
- TensorFlow version (you are using): **[Tensorflow Lite v1.11.0 (Self-Build)](https://github.com/PINTO0309/Tensorflow-bin.git)**
- Hardware: RaspberryPi3
- OS: Raspbian Stretch
- Are you willing to contribute it (Yes/No): No (Because, There are few Custom Operation tutorials, and I can not write C++ programs.)

**Describe the feature and the current behavior/state.**
If I implement ""semantic segmentation"" model ""ENet"", I can not do it unless you support various behaviors of the unpooling layer.

- **Layer that I would like to support with Tensorflow Lite**
    - ~FloorMod~
    - ~Range~
    - ~Rank~
    - Abs
    - MaxPoolWithArgmax
    - ~ScatterNd~
    - SparseTensor
    - sparse_add
    - ~gather_nd~

- **Sample message of Unsupport Error**
```Python
Traceback (most recent call last):
  File ""main.py"", line 5, in <module>
    interpreter = tf.contrib.lite.Interpreter(model_path=""semanticsegmentation_enet_non_quantized.tflite"")
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lite/python/interpreter.py"", line 53, in __init__
    model_path))
ValueError: Didn't find custom op for name 'FloorMod' with version 1
Didn't find custom op for name 'Range' with version 1
Didn't find custom op for name 'Rank' with version 1
Didn't find custom op for name 'Abs' with version 1
Didn't find custom op for name 'MaxPoolWithArgmax' with version 1
Didn't find custom op for name 'ScatterNd' with version 1
Registration failed.
```

**Will this change the current api? How?**
Yes.
Custom Operation must be officially implemented.

**Who will benefit with this feature?**
- Engineers who study automatic driving technology in general
- Engineer who studies fast inference using lightweight mobile
- Engineers to study lightweight models

**Any Other info.**
My repository and sample program are below.  
For Python2.x / Python3.x.
  
**https://github.com/PINTO0309/TensorFlow-ENet.git**  
**https://github.com/PINTO0309/TensorflowLite-UNet.git**  
  
<details><summary>【Reference】 Model Logic</summary><div>

```Python
import tensorflow as tf
from tensorflow.contrib.layers.python.layers import initializers
slim = tf.contrib.slim

@slim.add_arg_scope
def prelu(x, scope, decoder=False):

    #If decoder, then perform relu and just return the output
    if decoder:
        return tf.nn.relu(x, name=scope)

    alpha= tf.get_variable(scope + 'alpha', x.get_shape()[-1],
                       initializer=tf.constant_initializer(0.0),
                        dtype=tf.float32)
    pos = tf.nn.relu(x)
    neg = alpha * (x - abs(x)) * 0.5
    return pos + neg

def spatial_dropout(x, p, seed, scope, is_training=True):

    if is_training:
        keep_prob = 1.0 - p
        input_shape = x.get_shape().as_list()
        noise_shape = tf.constant(value=[input_shape[0], 1, 1, input_shape[3]])
        output = tf.nn.dropout(x, keep_prob, noise_shape, seed=seed, name=scope)

        return output

    return x

def unpool(updates, mask, k_size=[1, 2, 2, 1], output_shape=None, scope=''):

    with tf.variable_scope(scope):
        mask = tf.cast(mask, tf.int32)
        input_shape = tf.shape(updates, out_type=tf.int32)
        #  calculation new shape
        if output_shape is None:
            output_shape = (input_shape[0], input_shape[1] * ksize[1], input_shape[2] * ksize[2], input_shape[3])

        # calculation indices for batch, height, width and feature maps
        one_like_mask = tf.ones_like(mask, dtype=tf.int32)
        batch_shape = tf.concat([[input_shape[0]], [1], [1], [1]], 0)
        batch_range = tf.reshape(tf.range(output_shape[0], dtype=tf.int32), shape=batch_shape)
        b = one_like_mask * batch_range
        y = mask // (output_shape[2] * output_shape[3])
        x = (mask // output_shape[3]) % output_shape[2] #mask % (output_shape[2] * output_shape[3]) // output_shape[3]
        feature_range = tf.range(output_shape[3], dtype=tf.int32)
        f = one_like_mask * feature_range

        # transpose indices & reshape update values to one dimension
        updates_size = tf.size(updates)
        indices = tf.transpose(tf.reshape(tf.stack([b, y, x, f]), [4, updates_size]))
        values = tf.reshape(updates, [updates_size])
        ret = tf.scatter_nd(indices, values, output_shape)
        return ret

@slim.add_arg_scope
def initial_block(inputs, is_training=True, scope='initial_block'):

    #Convolutional branch
    net_conv = slim.conv2d(inputs, 13, [3,3], stride=2, activation_fn=None, scope=scope+'_conv')
    net_conv = slim.batch_norm(net_conv, is_training=is_training, fused=True, scope=scope+'_batchnorm')
    net_conv = prelu(net_conv, scope=scope+'_prelu')

    #Max pool branch
    net_pool = slim.max_pool2d(inputs, [2,2], stride=2, scope=scope+'_max_pool')

    #Concatenated output - does it matter max pool comes first or conv comes first? probably not.
    net_concatenated = tf.concat([net_conv, net_pool], axis=3, name=scope+'_concat')
    return net_concatenated

@slim.add_arg_scope
def bottleneck(inputs,
               output_depth,
               filter_size,
               regularizer_prob,
               projection_ratio=4,
               seed=0,
               is_training=True,
               downsampling=False,
               upsampling=False,
               pooling_indices=None,
               output_shape=None,
               dilated=False,
               dilation_rate=None,
               asymmetric=False,
               decoder=False,
               scope='bottleneck'):

    #Calculate the depth reduction based on the projection ratio used in 1x1 convolution.
    reduced_depth = int(inputs.get_shape().as_list()[3] / projection_ratio)

    with slim.arg_scope([prelu], decoder=decoder):

        #=============DOWNSAMPLING BOTTLENECK====================
        if downsampling:
            #=============MAIN BRANCH=============
            #Just perform a max pooling
            net_main, pooling_indices = tf.nn.max_pool_with_argmax(inputs,
                                                                   ksize=[1,2,2,1],
                                                                   strides=[1,2,2,1],
                                                                   padding='SAME',
                                                                   name=scope+'_main_max_pool')

            #First get the difference in depth to pad, then pad with zeros only on the last dimension.
            inputs_shape = inputs.get_shape().as_list()
            depth_to_pad = abs(inputs_shape[3] - output_depth)
            paddings = tf.convert_to_tensor([[0,0], [0,0], [0,0], [0, depth_to_pad]])
            net_main = tf.pad(net_main, paddings=paddings, name=scope+'_main_padding')

            #=============SUB BRANCH==============
            #First projection that has a 2x2 kernel and stride 2
            net = slim.conv2d(inputs, reduced_depth, [2,2], stride=2, scope=scope+'_conv1')
            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm1')
            net = prelu(net, scope=scope+'_prelu1')

            #Second conv block
            net = slim.conv2d(net, reduced_depth, [filter_size, filter_size], scope=scope+'_conv2')
            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm2')
            net = prelu(net, scope=scope+'_prelu2')

            #Final projection with 1x1 kernel
            net = slim.conv2d(net, output_depth, [1,1], scope=scope+'_conv3')
            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm3')
            net = prelu(net, scope=scope+'_prelu3')

            #Regularizer
            net = spatial_dropout(net, p=regularizer_prob, seed=seed, scope=scope+'_spatial_dropout')

            #Finally, combine the two branches together via an element-wise addition
            net = tf.add(net, net_main, name=scope+'_add')
            net = prelu(net, scope=scope+'_last_prelu')

            #also return inputs shape for convenience later
            return net, pooling_indices, inputs_shape

        #============DILATION CONVOLUTION BOTTLENECK====================
        #Everything is the same as a regular bottleneck except for the dilation rate argument
        elif dilated:
            #Check if dilation rate is given
            if not dilation_rate:
                raise ValueError('Dilation rate is not given.')

            #Save the main branch for addition later
            net_main = inputs

            #First projection with 1x1 kernel (dimensionality reduction)
            net = slim.conv2d(inputs, reduced_depth, [1,1], scope=scope+'_conv1')
            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm1')
            net = prelu(net, scope=scope+'_prelu1')

            #Second conv block --- apply dilated convolution here
            net = slim.conv2d(net, reduced_depth, [filter_size, filter_size], rate=dilation_rate, scope=scope+'_dilated_conv2')
            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm2')
            net = prelu(net, scope=scope+'_prelu2')

            #Final projection with 1x1 kernel (Expansion)
            net = slim.conv2d(net, output_depth, [1,1], scope=scope+'_conv3')
            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm3')
            net = prelu(net, scope=scope+'_prelu3')

            #Regularizer
            net = spatial_dropout(net, p=regularizer_prob, seed=seed, scope=scope+'_spatial_dropout')
            net = prelu(net, scope=scope+'_prelu4')

            #Add the main branch
            net = tf.add(net_main, net, name=scope+'_add_dilated')
            net = prelu(net, scope=scope+'_last_prelu')

            return net

        #===========ASYMMETRIC CONVOLUTION BOTTLENECK==============
        #Everything is the same as a regular bottleneck except for a [5,5] kernel decomposed into two [5,1] then [1,5]
        elif asymmetric:
            #Save the main branch for addition later
            net_main = inputs

            #First projection with 1x1 kernel (dimensionality reduction)
            net = slim.conv2d(inputs, reduced_depth, [1,1], scope=scope+'_conv1')
            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm1')
            net = prelu(net, scope=scope+'_prelu1')

            #Second conv block --- apply asymmetric conv here
            net = slim.conv2d(net, reduced_depth, [filter_size, 1], scope=scope+'_asymmetric_conv2a')
            net = slim.conv2d(net, reduced_depth, [1, filter_size], scope=scope+'_asymmetric_conv2b')
            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm2')
            net = prelu(net, scope=scope+'_prelu2')

            #Final projection with 1x1 kernel
            net = slim.conv2d(net, output_depth, [1,1], scope=scope+'_conv3')
            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm3')
            net = prelu(net, scope=scope+'_prelu3')

            #Regularizer
            net = spatial_dropout(net, p=regularizer_prob, seed=seed, scope=scope+'_spatial_dropout')
            net = prelu(net, scope=scope+'_prelu4')

            #Add the main branch
            net = tf.add(net_main, net, name=scope+'_add_asymmetric')
            net = prelu(net, scope=scope+'_last_prelu')

            return net

        #============UPSAMPLING BOTTLENECK================
        #Everything is the same as a regular one, except convolution becomes transposed.
        elif upsampling:
            #Check if pooling indices is given
            if pooling_indices == None:
                raise ValueError('Pooling indices are not given.')

            #Check output_shape given or not
            if output_shape == None:
                raise ValueError('Output depth is not given')

            #=======MAIN BRANCH=======
            #Main branch to upsample. output shape must match with the shape of the layer that was pooled initially, in order
            #for the pooling indices to work correctly. However, the initial pooled layer was padded, so need to reduce dimension
            #before unpooling. In the paper, padding is replaced with convolution for this purpose of reducing the depth!
            net_unpool = slim.conv2d(inputs, output_depth, [1,1], scope=scope+'_main_conv1')
            net_unpool = slim.batch_norm(net_unpool, is_training=is_training, scope=scope+'batch_norm1')
            net_unpool = unpool(net_unpool, pooling_indices, output_shape=output_shape, scope='unpool')

            #======SUB BRANCH=======
            #First 1x1 projection to reduce depth
            net = slim.conv2d(inputs, reduced_depth, [1,1], scope=scope+'_conv1')
            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm2')
            net = prelu(net, scope=scope+'_prelu1')

            #Second conv block -----------------------------> NOTE: using tf.nn.conv2d_transpose for variable input shape.
            net_unpool_shape = net_unpool.get_shape().as_list()
            output_shape = [net_unpool_shape[0], net_unpool_shape[1], net_unpool_shape[2], reduced_depth]
            output_shape = tf.convert_to_tensor(output_shape)
            filter_size = [filter_size, filter_size, reduced_depth, reduced_depth]
            filters = tf.get_variable(shape=filter_size, initializer=initializers.xavier_initializer(), dtype=tf.float32, name=scope+'_transposed_conv2_filters')

            # net = slim.conv2d_transpose(net, reduced_depth, [filter_size, filter_size], stride=2, scope=scope+'_transposed_conv2')
            net = tf.nn.conv2d_transpose(net, filter=filters, strides=[1,2,2,1], output_shape=output_shape, name=scope+'_transposed_conv2')
            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm3')
            net = prelu(net, scope=scope+'_prelu2')

            #Final projection with 1x1 kernel
            net = slim.conv2d(net, output_depth, [1,1], scope=scope+'_conv3')
            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm4')
            net = prelu(net, scope=scope+'_prelu3')

            #Regularizer
            net = spatial_dropout(net, p=regularizer_prob, seed=seed, scope=scope+'_spatial_dropout')
            net = prelu(net, scope=scope+'_prelu4')

            #Finally, add the unpooling layer and the sub branch together
            net = tf.add(net, net_unpool, name=scope+'_add_upsample')
            net = prelu(net, scope=scope+'_last_prelu')

            return net

        #OTHERWISE, just perform a regular bottleneck!
        #==============REGULAR BOTTLENECK==================
        #Save the main branch for addition later
        net_main = inputs

        #First projection with 1x1 kernel
        net = slim.conv2d(inputs, reduced_depth, [1,1], scope=scope+'_conv1')
        net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm1')
        net = prelu(net, scope=scope+'_prelu1')

        #Second conv block
        net = slim.conv2d(net, reduced_depth, [filter_size, filter_size], scope=scope+'_conv2')
        net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm2')
        net = prelu(net, scope=scope+'_prelu2')

        #Final projection with 1x1 kernel
        net = slim.conv2d(net, output_depth, [1,1], scope=scope+'_conv3')
        net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm3')
        net = prelu(net, scope=scope+'_prelu3')

        #Regularizer
        net = spatial_dropout(net, p=regularizer_prob, seed=seed, scope=scope+'_spatial_dropout')
        net = prelu(net, scope=scope+'_prelu4')

        #Add the main branch
        net = tf.add(net_main, net, name=scope+'_add_regular')
        net = prelu(net, scope=scope+'_last_prelu')

        return net

#Now actually start building the network
def ENet(inputs,
         num_classes,
         batch_size,
         num_initial_blocks=1,
         stage_two_repeat=2,
         skip_connections=True,
         reuse=None,
         is_training=True,
         scope='ENet'):

    #Set the shape of the inputs first to get the batch_size information
    inputs_shape = inputs.get_shape().as_list()
    inputs.set_shape(shape=(batch_size, inputs_shape[1], inputs_shape[2], inputs_shape[3]))

    with tf.variable_scope(scope, reuse=reuse):
        #Set the primary arg scopes. Fused batch_norm is faster than normal batch norm.
        with slim.arg_scope([initial_block, bottleneck], is_training=is_training),\
             slim.arg_scope([slim.batch_norm], fused=True), \
             slim.arg_scope([slim.conv2d, slim.conv2d_transpose], activation_fn=None): 
            #=================INITIAL BLOCK=================
            net = initial_block(inputs, scope='initial_block_1')
            for i in xrange(2, max(num_initial_blocks, 1) + 1):
                net = initial_block(net, scope='initial_block_' + str(i))

            #Save for skip connection later
            if skip_connections:
                net_one = net

            #===================STAGE ONE=======================
            net, pooling_indices_1, inputs_shape_1 = bottleneck(net, output_depth=64, filter_size=3, regularizer_prob=0.01, downsampling=True, scope='bottleneck1_0')
            net = bottleneck(net, output_depth=64, filter_size=3, regularizer_prob=0.01, scope='bottleneck1_1')
            net = bottleneck(net, output_depth=64, filter_size=3, regularizer_prob=0.01, scope='bottleneck1_2')
            net = bottleneck(net, output_depth=64, filter_size=3, regularizer_prob=0.01, scope='bottleneck1_3')
            net = bottleneck(net, output_depth=64, filter_size=3, regularizer_prob=0.01, scope='bottleneck1_4')

            #Save for skip connection later
            if skip_connections:
                net_two = net

            #regularization prob is 0.1 from bottleneck 2.0 onwards
            with slim.arg_scope([bottleneck], regularizer_prob=0.1):
                net, pooling_indices_2, inputs_shape_2 = bottleneck(net, output_depth=128, filter_size=3, downsampling=True, scope='bottleneck2_0')
                
                #Repeat the stage two at least twice to get stage 2 and 3:
                for i in xrange(2, max(stage_two_repeat, 2) + 2):
                    net = bottleneck(net, output_depth=128, filter_size=3, scope='bottleneck'+str(i)+'_1')
                    net = bottleneck(net, output_depth=128, filter_size=3, dilated=True, dilation_rate=2, scope='bottleneck'+str(i)+'_2')
                    net = bottleneck(net, output_depth=128, filter_size=5, asymmetric=True, scope='bottleneck'+str(i)+'_3')
                    net = bottleneck(net, output_depth=128, filter_size=3, dilated=True, dilation_rate=4, scope='bottleneck'+str(i)+'_4')
                    net = bottleneck(net, output_depth=128, filter_size=3, scope='bottleneck'+str(i)+'_5')
                    net = bottleneck(net, output_depth=128, filter_size=3, dilated=True, dilation_rate=8, scope='bottleneck'+str(i)+'_6')
                    net = bottleneck(net, output_depth=128, filter_size=5, asymmetric=True, scope='bottleneck'+str(i)+'_7')
                    net = bottleneck(net, output_depth=128, filter_size=3, dilated=True, dilation_rate=16, scope='bottleneck'+str(i)+'_8')

            with slim.arg_scope([bottleneck], regularizer_prob=0.1, decoder=True):
                #===================STAGE FOUR========================
                bottleneck_scope_name = ""bottleneck"" + str(i + 1)

                #The decoder section, so start to upsample.
                net = bottleneck(net, output_depth=64, filter_size=3, upsampling=True,
                                 pooling_indices=pooling_indices_2, output_shape=inputs_shape_2, scope=bottleneck_scope_name+'_0')

                #Perform skip connections here
                if skip_connections:
                    net = tf.add(net, net_two, name=bottleneck_scope_name+'_skip_connection')

                net = bottleneck(net, output_depth=64, filter_size=3, scope=bottleneck_scope_name+'_1')
                net = bottleneck(net, output_depth=64, filter_size=3, scope=bottleneck_scope_name+'_2')

                #===================STAGE FIVE========================
                bottleneck_scope_name = ""bottleneck"" + str(i + 2)

                net = bottleneck(net, output_depth=16, filter_size=3, upsampling=True,
                                 pooling_indices=pooling_indices_1, output_shape=inputs_shape_1, scope=bottleneck_scope_name+'_0')

                #perform skip connections here
                if skip_connections:
                    net = tf.add(net, net_one, name=bottleneck_scope_name+'_skip_connection')

                net = bottleneck(net, output_depth=16, filter_size=3, scope=bottleneck_scope_name+'_1')

            #=============FINAL CONVOLUTION=============
            logits = slim.conv2d_transpose(net, num_classes, [2,2], stride=2, scope='fullconv')
            probabilities = tf.nn.softmax(logits, name='logits_to_softmax')

        return logits, probabilities


def ENet_arg_scope(weight_decay=2e-4,
                   batch_norm_decay=0.1,
                   batch_norm_epsilon=0.001):

  # Set weight_decay for weights in conv2d and separable_conv2d layers.
  with slim.arg_scope([slim.conv2d],
                      weights_regularizer=slim.l2_regularizer(weight_decay),
                      biases_regularizer=slim.l2_regularizer(weight_decay)):

    # Set parameters for batch_norm.
    with slim.arg_scope([slim.batch_norm],
                        decay=batch_norm_decay,
                        epsilon=batch_norm_epsilon) as scope:
      return scope
```
</div></details><br>"
23319,tflite_convert - ValueError: Invalid tensors 'input' were found.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 18.04
- TensorFlow installed from (source or binary):
Installed from binary.
- TensorFlow version: Apparently this problem exists in all versions.  The versions I have tested: 1.5 | 1.11 | 1.12.0-rc2 | tf-nightly
- Python version: 3.6
- CUDA/cuDNN version: I have the CPU version.
- GPU model and memory: I have the CPU version.

**Describe the current behavior**
I want to convert Mobilenet_0.50_224 frozen graph using tflite_convert or toco to a TensorFlow Lite model, but I get the following error:
`2018-10-27 21:31:49.213953: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
Traceback (most recent call last):
  File ""/home/ramtin/Desktop/Model/venv/bin/tflite_convert"", line 11, in <module>
    sys.exit(main())
  File ""/home/ramtin/Desktop/Model/venv/lib/python3.6/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 412, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/home/ramtin/Desktop/Model/venv/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/home/ramtin/Desktop/Model/venv/lib/python3.6/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 408, in run_main
    _convert_model(tflite_flags)
  File ""/home/ramtin/Desktop/Model/venv/lib/python3.6/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 100, in _convert_model
    converter = _get_toco_converter(flags)
  File ""/home/ramtin/Desktop/Model/venv/lib/python3.6/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 87, in _get_toco_converter
    return converter_fn(**converter_kwargs)
  File ""/home/ramtin/Desktop/Model/venv/lib/python3.6/site-packages/tensorflow/contrib/lite/python/lite.py"", line 283, in from_frozen_graph
    sess.graph, input_arrays)
  File ""/home/ramtin/Desktop/Model/venv/lib/python3.6/site-packages/tensorflow/contrib/lite/python/convert_saved_model.py"", line 189, in get_tensors_from_tensor_names
    "","".join(invalid_tensors)))
ValueError: Invalid tensors 'input' were found.
`
**Describe the expected behavior**
 tflite_convert could be able to convert the frozen graph to a tflite model successfully.
**Code to reproduce the issue**
I run the following command:
```
tflite_convert \
  --output_file=foo.tflite \
  --graph_def_file=frozen_graph.pb \
  --input_arrays=input \
  --output_arrays=MobilenetV1/Predictions/Reshape_1
```

**Other info / logs**
I have tried this on both the default Mobilenet_0.50_224 frozen graph and the retrained graph which is the output of retrain.py script from [Tensorflow for poets 2](https://github.com/googlecodelabs/tensorflow-for-poets-2/blob/master/scripts/retrain.py).
I also tried to use toco, but the result was the same.
However, I can optimize the graph for Tensorflow Mobile without any  problem.
"
23318,tf.nn.conv2d_transpose with the same inputs produces different outputs on different calls on GPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.5 LTS (Xenial Xerus)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v1.10.1-0-g4dcfddc
- Python version: Python 3.6.5
- Bazel version (if compiling from source): 0.16.1
- GCC/Compiler version (if compiling from source): 5.4.0-6ubuntu1~16.04.10
- CUDA/cuDNN version: V9.0.176/7.1.4.18
- GPU model and memory: GeForce GTX 1080 Ti with 12GB memory

**Describe the current behavior**
`tf.nn.conv2d_transpose` with the same inputs produces different outputs on different calls on GPU.

**Describe the expected behavior**
`tf.nn.conv2d_transpose` with the same inputs is expected to produce the same outputs on different calls on GPU.

**Code to reproduce the issue**
The code below

```
import tensorflow as tf
tf.enable_eager_execution()


x = tf.random_uniform([1, 1024, 32, 32])
filter = tf.random_uniform([4, 4, 96, 1024])

y1 = tf.nn.conv2d_transpose(x, filter, [1, 96, 64, 64], strides=[1, 1, 2, 2], padding='SAME', data_format='NCHW')
y2 = tf.nn.conv2d_transpose(x, filter, [1, 96, 64, 64], strides=[1, 1, 2, 2], padding='SAME', data_format='NCHW')

# show difference between y1 and y2
print(tf.reduce_mean(tf.abs(y1 - y2)))
```

produces a non-zero result like

```
tf.Tensor(8.213489e-06, shape=(), dtype=float32)
```

**Other info / logs**
`tf.nn.conv2d_transpose` with the same inputs produces the same outputs on different calls on **CPU**."
23316,Wrong gradients in combination with placeholders.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Ubuntu 14.04.5 LTS`, `Debian 4.18.10-2`, `Archlinux`
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): `v1.4.0-19-ga52c8d9 1.4.1`, `v1.10.1-0-g4dcfddc5d1 1.10.1`, `1.10.0`, 
- Python version: `3.6.0`, `3.4.3`, `3.7.0`
- CUDA/cuDNN version: `9.0/7.1.2`, `8.0.61/6.0.21`, `CPU only`
- GPU model and memory: Titan 12GB

**Describe the current behavior**
Incorrect gradient computation when placeholder weights are involved. In a situation where two variables are involved, optimization of one of the variables leads to wrong gradients for the other variable when placeholders weights are used.

**Describe the expected behavior**
Gradients should be correct and there should be no difference in gradients whether a placeholder or another variable is used.

**Code to reproduce the issue**
Here is a minimal example to reproduce the issue (the case of `indirect = True` can be used as a workaround and shows the expected output in contrast of `indirect = False`):

```
import tensorflow as tf                                                                                   
                                                                                                          
def test(indirect):                                                                                       
    print(""indirect:"", indirect)                                                                          
    a = tf.Variable(1.0)                                                                                  
    b = tf.Variable(2.0)                                                                                  
    weight_var = tf.Variable(3.0, trainable = False)                                                      
                                                                                                          
    weight_placeholder = tf.placeholder(tf.float32)                                                       
    if indirect:                                                                                          
        weight = tf.assign(weight_var, weight_placeholder)                                                
    else:                                                                                                 
        weight = weight_placeholder                                                                       
                                                                                                          
    common_loss = tf.square(a*b)                                                                          
    b_only_loss = tf.square(b)                                                                            
                                                                                                          
    common_grads = tf.gradients(                                                                          
            common_loss,                                                                                  
            [b])                                                                                          
    b_only_grads = tf.gradients(                                                                          
            weight*b_only_loss,                                                                           
            [b])                                                                                          
    summed_grads = tf.gradients(                                                                          
            common_loss + weight*b_only_loss,                                                             
            [b])                                                                                          
    grad_diff = tf.abs(common_grads[0] + b_only_grads[0] - summed_grads[0])                               
                                                                                                          
    optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)                                      
    opt_a = optimizer.minimize(common_loss, var_list=[a])                                                 
                                                                                                          
    feed={weight_placeholder: 0.0}                                                                        
    fetch={""common_grads"": common_grads, ""b_only_grads"": b_only_grads,                                    
           ""summed_grads"": summed_grads, ""grad_diff"": grad_diff,                                          
           ""opt_a"": opt_a}                                                                                
    s = tf.Session()                                                                                      
    s.run(tf.global_variables_initializer())                                                              
    r = s.run(fetch, feed)                                                                                
    for k in sorted(r):                                                                                   
        print(""{:16}"".format(k), r[k])                                                                    
    assert r[""grad_diff""] < 1e-11, r[""grad_diff""]
                                                                                                          
                                                                                                          
if __name__ == ""__main__"":                                                                                
    print(""tensorflow"", tf.GIT_VERSION, tf.VERSION)                                                       
    test(True)                                                                                            
    test(False)
```

Example output:

```
tensorflow b'unknown' 1.10.0
indirect: True
b_only_grads     [0.0]
common_grads     [4.0]
grad_diff        0.0
opt_a            None
summed_grads     [4.0]
indirect: False
b_only_grads     [0.0]
common_grads     [4.0]
grad_diff        32.0
opt_a            None
summed_grads     [-28.0]
Traceback (most recent call last):
  File ""minimal_example.py"", line 47, in <module>
    test(False)
  File ""minimal_example.py"", line 41, in test
    assert r[""grad_diff""] < 1e-11, r[""grad_diff""]
AssertionError: 32.0
```

**Other info / logs**
The output is not deterministic and sometimes produces correct results so the code might have to be run multiple times. The code also works fine without the `opt_a` call. Constant values for the weight also work fine, just in combination with a placeholder it seems to be problematic. The problem remains the same for other values of the weight and thus in general violates commutativity of the gradient but the case of a weight of zero illustrates the problem best because the zero weighted summand really should not have any influence on the gradients. Finally, the gradients are not just reported falsely but optimization of `b`, e.g. `optimizer.minimize(common_loss + weight*b_only_loss, var_list=[b])`, produces wrong results (i.e. gradient descent uses wrong gradients).
"
23315,[surface book1] What version of CUDA should be installed on the Microsoft GeForce 940MX?,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information** windows 10
- TensorFlow version: 1.11
- Doc Link: https://www.tensorflow.org/install/install_windows?hl=zh-cn


**Describe the documentation issue**

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
23314,Changing learning rate of the optimizer in eager mode,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.11
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: K80 



**Describe the current behavior**
When using `tf.keras` with `eager` execution enabled, I don't have the control over learning rate of the optimizer i.e. I am unable to change the learning rate on the fly. If you are using pure `keras` with no `eager` mode, then you can change the learning rate of the keras optimizer in the following way 

`K.set_value(self.mode.optimizer.lr, value)`

But as the `keras` optimizers aren't supported in `eager` mode, I don't have any control over the learning rate of my optimizer.

**Describe the expected behavior**
The expected behavior is that I should be able to change the learning rate of my optimizer after a batch ends or after an epoch ends


"
23313,(roadmap request)python 3.7 and windows build roadmap,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.12.0rc
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
build tensorflow with python 3.7 gets error ( related to google protobuf c++ code  ), windows bazel build can't recognize path
**Will this change the current api? How?** 
no
**Who will benefit with this feature?**
almost everyone
**Any Other info.**

protobuf for python 3.7 is stable and published on pip, and tensorflow is not compatible with that.
windows bazel-build has path-related error( which is an exact path ), maybe this is caused by different path-mechanism and/or regional(korean, in my case) encoding-related

could you tensorflowers give us specific roadmap for python 3.7 compatibility and windows 10(for all countries) build support roadmap?"
23312,Bug: `tf.cumsum` is not numerically stable when sequence is long.,"
**System information**
- Have I written custom code: yes, but very little
- OS Platform and Distribution: CentOS 5
- TensorFlow version (use command below): both 1.18 and 1.12
- Python version: both python 2 and 3
- GPU model and memory: TitanX and 1080Ti

**Describe the current behavior**
`cumsum` has small (possibly rounding or something similar) errors, and these errors accumulates quickly as the sequence length increase, e.g. a time sequence of length 1000. 

**Describe the expected behavior**
`cumsum` should be as accurate as the `float32` precision.

**Code to reproduce the issue**
Here is the colab gist to reproduce the result

https://colab.research.google.com/gist/rex-yue-wu/cb19dc2af6f8709002b6d412611e972a/unstable-tf-cumsum.ipynb
"
23311,Hey. Just saw that the vocabulary size is a parameter you have to define manually? Seems not very intuitive for me as it can be computed automatically.,_Originally posted by @b-3-n in https://github.com/tensorflow/tensorflow/issues/2734#issuecomment-224732629_
23310,Win10 + Anaconda3  cannot install tensorflow-gpu,"Hi All,
I'm new to setup GPU environment. Has been stuck for a few hours, hope to get some solutions.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10
- TensorFlow installed from (source or binary): pip install --ignore-installed --upgrade tensorflow-gpu
- Python version: Python 3.5.3
- CUDA/cuDNN version: CUDA 9.0.176,  cudnn-9.0-windows10-x64-v7.3.1.20
- GPU model and memory: NVIDIA GeForce GTX 1050, 4GB


**Describe the current behavior**
Cannot install. Error message as below. 

(PY35) C:\>pip install --ignore-installed --upgrade tensorflow-gpu
Collecting tensorflow-gpu
  Using cached https://files.pythonhosted.org/packages/43/93/07f5cae2c8e02b37c4d64fa9c9eb1f8d3b39128247d0c1acd9110da45f67/tensorflow_gpu-1.11.0-cp35-cp35m-win_amd64.whl
Collecting six>=1.10.0 (from tensorflow-gpu)
  Using cached https://files.pythonhosted.org/packages/67/4b/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a/six-1.11.0-py2.py3-none-any.whl
Collecting tensorboard<1.12.0,>=1.11.0 (from tensorflow-gpu)
  Using cached https://files.pythonhosted.org/packages/9b/2f/4d788919b1feef04624d63ed6ea45a49d1d1c834199ec50716edb5d310f4/tensorboard-1.11.0-py3-none-any.whl
Collecting grpcio>=1.8.6 (from tensorflow-gpu)
  Using cached https://files.pythonhosted.org/packages/5e/8c/da9316699398607a22c91e39e16e4c0f3e8233e0faa88ed52df736f2b1d6/grpcio-1.16.0-cp35-cp35m-win_amd64.whl
Collecting keras-preprocessing>=1.0.3 (from tensorflow-gpu)
  Using cached https://files.pythonhosted.org/packages/fc/94/74e0fa783d3fc07e41715973435dd051ca89c550881b3454233c39c73e69/Keras_Preprocessing-1.0.5-py2.py3-none-any.whl
Collecting termcolor>=1.1.0 (from tensorflow-gpu)
  Using cached https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):
      File ""<string>"", line 1, in <module>
      File ""c:\users\ericx\appdata\local\conda\conda\envs\py35\lib\site-packages\setuptools\__init__.py"", line 191, in <module>
        monkey.patch_all()
      File ""c:\users\ericx\appdata\local\conda\conda\envs\py35\lib\site-packages\setuptools\monkey.py"", line 101, in patch_all
        patch_for_msvc_specialized_compiler()
      File ""c:\users\ericx\appdata\local\conda\conda\envs\py35\lib\site-packages\setuptools\monkey.py"", line 138, in patch_for_msvc_specialized_compiler
        msvc = import_module('setuptools.msvc')
      File ""c:\users\ericx\appdata\local\conda\conda\envs\py35\lib\importlib\__init__.py"", line 126, in import_module
        return _bootstrap._gcd_import(name[level:], package, level)
      File ""c:\users\ericx\appdata\local\conda\conda\envs\py35\lib\site-packages\setuptools\msvc.py"", line 58, in <module>
        from distutils.msvc9compiler import Reg
      File ""c:\users\ericx\appdata\local\conda\conda\envs\py35\lib\distutils\msvc9compiler.py"", line 254
        log.debug(""Unable to find vcvarsall.bat Eric edit C:\Users\ericx\AppData\Local\conda\conda\envs\r-tensorflow\Lib\site-packages\numpy\distutils"")
                 ^
    SyntaxError: (unicode error) 'unicodeescape' codec can't decode bytes in position 41-42: truncated \UXXXXXXXX escape

    ----------------------------------------
Command ""python setup.py egg_info"" failed with error code 1 in C:\Users\ericx\AppData\Local\Temp\pip-install-mxk6dgpl\termcolor\

**Describe the expected behavior**

**Code to reproduce the issue**
Cannot install, every time same error. "
23309,Cannot build model on windows 8 - valueerror,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 8.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA
- TensorFlow installed from (source or binary):https://www.tensorflow.org/install/
- TensorFlow version:1.11
- Python version:3.6
- Installed using virtualenv? pip? conda?:pip
- Bazel version (if compiling from source):NA
- GCC/Compiler version (if compiling from source):No idea
- CUDA/cuDNN version:No
- GPU model and memory:No

**Describe the problem**
  File ""Text_generation01.py"", line 109, in <module>
    model.build(tf.TensorShape([BATCH_SIZE, seq_length]))
  File ""C:\Users\acer\AppData\Local\Programs\Python\Python36\lib\site-packages\t
ensorflow\python\keras\engine\network.py"", line 788, in build
    raise ValueError('Currently, you cannot build your model if it has '
ValueError: Currently, you cannot build your model if it has positional or keywo
rd arguments that are not inputs to the model, but are required for its `call` m
ethod. Instead, in order to instantiate and build your model, `call` your model
on real tensor data with all expected call arguments.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
model.build(tf.TensorShape([BATCH_SIZE, seq_length]))

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23308,Cannot import tensorflow into my file. Error message suggests that this is a build issue I think. ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): ""pip install tensorflow"" from cmd
- TensorFlow version: 1.11.0
- Python version: 3.6.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a



**Describe the problem**
I am getting an error message when trying to import tensorflow in my python file. I will post the error message below. I have uninstalled and reinstalled tensorflow using pip multiple times (like 3) using the same command every time : ""pip install tensorflow""

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I installed tensorflow using pip in cmd using the command ""pip install tensorflow."" I am running this in my x64 laptop and this device does not have a GPU. Tensorflows says that it installs correctly. However, when I open my python file in my text editor and I type ""import tensorflow"" and then I try to run the code, it gives me a long error message. 

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

This is the error message I recieve in the terminal when trying to run my python file. Note that even though I do not have a gpu and did not install it using pip with the gpu configuration, it gives me an error that other people get trying to install the that uses their GPU. I don't know if that's important or not. 

Traceback (most recent call last):
  File ""C:\Users\18ngc\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\18ngc\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\18ngc\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\18ngc\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\18ngc\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\18ngc\Documents\python-workspace\workshop\__main__.py"", line 1, in <module>
    import tensorflow
  File ""C:\Users\18ngc\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\18ngc\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\18ngc\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\18ngc\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\18ngc\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\18ngc\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\18ngc\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\18ngc\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
23307,Missing Documentation: Multiple links return 404 in Python API Guide,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: Version 2.0
- Doc Link: The following docs all have links that return a 404 error:

  - [tensorflow/contrib/util/\_\_init\_\_.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/util/__init__.py)
    - [https://www.tensorflow.org/api_guides/python/contrib.util](https://www.tensorflow.org/api_guides/python/contrib.util)
  - [tensorflow/python/lib/io/python_io.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/lib/io/python_io.py)
    - [https://www.tensorflow.org/api_guides/python/python_io](https://www.tensorflow.org/api_guides/python/python_io)
  - [tensorflow/python/client/client_lib.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/client_lib.py)
    - [https://www.tensorflow.org/api_guides/python/client](https://www.tensorflow.org/api_guides/python/client)
  - [tensorflow/contrib/crf/\_\_init\_\_.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/crf/\_\_init\_\_.py)
    - [https://www.tensorflow.org/api_guides/python/contrib.crf](https://www.tensorflow.org/api_guides/python/contrib.crf)
  - [tensorflow/python/debug/\_\_init\_\_.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/debug/__init__.py)
    - [https://tensorflow.org/api_guides/python/tfdbg](https://tensorflow.org/api_guides/python/tfdbg)
  - [tensorflow/contrib/rnn/\_\_init\_\_.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/__init__.py)
    - [https://tensorflow.org/api_guides/python/contrib.rnn](https://tensorflow.org/api_guides/python/contrib.rnn)
  - [tensorflow/core/api_def/base_api/api_def_SegmentMax.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_SegmentMax.pbtxt)
  - [tensorflow/core/api_def/base_api/api_def_SegmentMean.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_SegmentMean.pbtxt)
  - [tensorflow/core/api_def/base_api/api_def_SegmentMin.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_SegmentMin.pbtxt)
  - [tensorflow/core/api_def/base_api/api_def_SegmentProd.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_SegmentProd.pbtxt)
  - [tensorflow/core/api_def/base_api/api_def_SegmentSum.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_SegmentSum.pbtxt)
  - [tensorflow/core/api_def/base_api/api_def_UnsortedSegmentMax.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_UnsortedSegmentMax.pbtxt)
  - [tensorflow/core/api_def/base_api/api_def_UnsortedSegmentSum.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_UnsortedSegmentSum.pbtxt)
  - [tensorflow/core/api_def/base_api/api_def_SparseSegmentSqrtN.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_SparseSegmentSqrtN.pbtxt)
  - [tensorflow/core/api_def/base_api/api_def_SparseSegmentMean.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_SparseSegmentMean.pbtxt)
  - [tensorflow/core/api_def/base_api/api_def_SparseSegmentMeanWithNumSegments.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_SparseSegmentMeanWithNumSegments.pbtxt)
  - [tensorflow/core/api_def/base_api/api_def_SparseSegmentSqrtNWithNumSegments.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_SparseSegmentSqrtNWithNumSegments.pbtxt)
  - [tensorflow/core/api_def/base_api/api_def_SparseSegmentSum.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_SparseSegmentSum.pbtxt)
  - [tensorflow/core/api_def/base_api/api_def_SparseSegmentSumWithNumSegments.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_SparseSegmentSumWithNumSegments.pbtxt)
  - [tensorflow/core/api_def/base_api/api_def_UnsortedSegmentMin.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_UnsortedSegmentMin.pbtxt)
  - [tensorflow/core/api_def/base_api/api_def_UnsortedSegmentProd.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_UnsortedSegmentProd.pbtxt)
    - [https://tensorflow.org/api_guides/python/math_ops#Segmentation](https://tensorflow.org/api_guides/python/math_ops#Segmentation)
  - [tensorflow/contrib/framework/\_\_init\_\_.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/framework/__init__.py)
    - [https://tensorflow.org/api_guides/python/contrib.framework](https://tensorflow.org/api_guides/python/contrib.framework)
  - [tensorflow/contrib/layers/\_\_init\_\_.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/__init__.py)
    - [https://tensorflow.org/api_guides/python/contrib.layers](https://tensorflow.org/api_guides/python/contrib.layers)
  - [tensorflow/python/training/training.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/training.py)
    - [https://www.tensorflow.org/api_guides/python/train](https://www.tensorflow.org/api_guides/python/train)
  - [tensorflow/python/summary/summary.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/summary/summary.py)
    - [https://tensorflow.org/api_guides/python/summary](https://tensorflow.org/api_guides/python/summary)
  - [tensorflow/python/ops/session_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/session_ops.py)
    - [https://tensorflow.org/api_guides/python/session_ops](https://tensorflow.org/api_guides/python/session_ops)
  - [tensorflow/python/ops/string_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/string_ops.py)
    - [https://tensorflow.org/api_guides/python/string_ops](https://tensorflow.org/api_guides/python/string_ops)
  - [tensorflow/python/ops/state_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/state_ops.py)
    - [https://tensorflow.org/api_guides/python/state_ops](https://tensorflow.org/api_guides/python/state_ops)
  - [tensorflow/python/ops/math_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_ops.py)
    - [https://tensorflow.org/api_guides/python/math_ops#segmentation](https://tensorflow.org/api_guides/python/math_ops#segmentation)
  - [tensorflow/python/ops/functional_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/functional_ops.py)
    - [https://tensorflow.org/api_guides/python/functional_ops](https://tensorflow.org/api_guides/python/functional_ops)
  - [tensorflow/python/ops/check_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/check_ops.py)
    - [https://tensorflow.org/api_guides/python/check_ops](https://tensorflow.org/api_guides/python/check_ops)
  - [tensorflow/python/ops/array_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py)
    - [https://tensorflow.org/api_guides/python/array_ops](https://tensorflow.org/api_guides/python/array_ops)
  - [tensorflow/python/ops/control_flow_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/control_flow_ops.py)
    - [https://tensorflow.org/api_guides/python/control_flow_ops](https://tensorflow.org/api_guides/python/control_flow_ops)
  - [tensorflow/python/ops/sparse_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/sparse_ops.py)
    - [https://tensorflow.org/api_guides/python/sparse_ops](https://tensorflow.org/api_guides/python/sparse_ops)

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?** No
"
