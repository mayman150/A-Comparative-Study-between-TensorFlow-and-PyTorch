Issue Number,Issue Title,Issue Body
60525,Iterating over dataset produces different label distribution statistics.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.10.1

### Custom Code

No

### OS Platform and Distribution

Windows 10 GPU

### Mobile device

Windows 10 GPU

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

cuda 11.8, cudaNN 8.6.0

### GPU model and memory

_No response_

### Current Behaviour?

Iterating over tf.data.Dataset object and counting the number of occurrences of each label produces different results each time.

I initially thought that this may be caused by batching and shuffling (With dropping remainders), however, there is no batching of the dataset and it is not shuffling after each iteration as well.


```python
datasets = [val_dataset, val_dataset, val_dataset] # Using same dataset three times. expect to see same results each three times.
table_data = [[],[],[]]
amounts = np.zeros(len(labels)) # Allocating array outside to be more memory efficient.
counter = 0

for i, ds in enumerate(datasets):
    ds_size = int(ds.__len__())
    amounts = amounts*0
    total = 0
    for _, label in ds:
        label = int(label)
        amounts[label] += 1
        counter += 1

    for amount in amounts:
        table_data[i].append(f""{amount} ({(100*amount/ds_size):.1f}%)"")

table = pd.DataFrame(table_data, columns=labels, index=['ds 1', ""ds 2"", ""ds 3""])
display(table)
```


Table displayed shows: 
![image](https://user-images.githubusercontent.com/62092803/236663799-260d48b1-6f4c-41dc-92b9-d8410066f251.png)

The name of columns are the classes, we expect to see each row to have the same value as other rows of the same column, however, we don't. The counter reports back the correct amount of images. For reference, the dataset used has 39209 images. split 0.8. 0.1 and 0.1 for train, val, test. It is a supervised dataset.




### Standalone code to reproduce the issue

```shell
dataset = keras.utils.image_dataset_from_directory(""to_some_dir"",
                                                interpolation='bilinear',
                                                shuffle=True,
                                                image_size=(64, 64),
                                                batch_size=None)

dataset_size = int(dataset.__len__())

train_size = int(0.8 * dataset_size)
val_size = int(0.1 * dataset_size)
test_size = int(0.1 * dataset_size)

train_dataset = dataset.take(train_size)
val_dataset = dataset.skip(train_size).take(val_size)
test_dataset = dataset.skip(train_size).skip(val_size)

datasets = [val_dataset, val_dataset, val_dataset] # Using the same dataset three times for test only.
table_data = [[],[],[]]
amounts = np.zeros(len(labels)) # Allocating array outside to be more memory efficient.
counter = 0

for i, ds in enumerate(datasets):
    ds_size = int(ds.__len__())
    amounts = amounts*0
    total = 0
    for _, label in ds:
        label = int(label)
        amounts[label] += 1
        counter += 1

    for amount in amounts:
        table_data[i].append(f""{amount} ({(100*amount/ds_size):.1f}%)"")

table = pd.DataFrame(table_data, columns=labels, index=['Train', ""Validation"", ""Test""])
display(table)
```


### Relevant log output

_No response_</details>"
60524,Cannot find the source code -> 'gen_stateless_random_ops',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tensorflow 2.12.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04.6 LTS

### Mobile device

_No response_

### Python version

3.9.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

12.0

### GPU model and memory

_No response_

### Current Behaviour?

from tensorflow.python.ops import gen_stateless_random_ops (in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/ops/stateless_random_ops.py)

I would like to check the source code for 'gen_stateless_random_ops', but I cannot find this. Could you please help me with this? Thank you for your help in advance!


### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

_No response_</details>"
60523,Empty Data Batches Passed to Custom Layer,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Linux 5.10.147

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When using a Data Generator with a Custom layer, the data passed to the layer call method is empty. 

However, the same code works as intended in tensorflow 2.10.0, and only seems to pass empty batches to the call method in a custom layer in 2.12.0.

I have a attached a notebook showing the issue. It first runs the code using tensorflow 2.12.0, then downgrades to tensorflow 2.10.0 then reruns the same notebook, and the error is gone.


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1E1lDWQWKbo6t0VQmzGEFwSEOCxTJ3t2S?usp=sharing
```


### Relevant log output

```shell
ValueError: in user code:

    File ""/usr/local/lib/python3.10/dist-packages/keras/engine/training.py"", line 1284, in train_function  *
        return step_function(self, iterator)
    File ""/usr/local/lib/python3.10/dist-packages/keras/engine/training.py"", line 1268, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/usr/local/lib/python3.10/dist-packages/keras/engine/training.py"", line 1249, in run_step  **
        outputs = model.train_step(data)
    File ""/usr/local/lib/python3.10/dist-packages/keras/engine/training.py"", line 1050, in train_step
        y_pred = self(x, training=True)
    File ""/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler
        raise e.with_traceback(filtered_tb) from None
    File ""/tmp/__autograph_generated_fileny5tmqhi.py"", line 13, in tf__call
        patches = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(patches), [ag__.ld(batch_size), -1, ag__.ld(patch_dims)]), None, fscope)

    ValueError: Exception encountered when calling layer 'patches' (type Patches).
    
    in user code:
    
        File ""<ipython-input-4-1c4669f3e34b>"", line 16, in call  *
            patches = tf.reshape(patches, [batch_size, -1, patch_dims])
    
        ValueError: Tried to convert 'shape' to a tensor and failed. Error: None values not supported.
    
    
    Call arguments received by layer 'patches' (type Patches):
      • images=tf.Tensor(shape=(None, 224, 224, None), dtype=float32)
```
</details>"
60521,error message is inconsistent with documentation in tf.nn.leaky_relu,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.12.0

### Custom Code

Yes

### OS Platform and Distribution

win11

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

According to [doc](https://tensorflow.google.cn/api_docs/python/tf/nn/leaky_relu), the argument `features` can be `float16, float32, float64, int32, int64`. But the error message in following snippet code indicates that the type of `features` can only be `float16, float32, float64` without `int`.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
results={}
try:
  features = [True]
  results[""res""] = tf.nn.leaky_relu(features=features,)
except Exception as e:
  results[""err""] = ""Error:""+str(e)
print(results)
# results={'err': ""Error:Value for attr 'T' of bool is not in the list of allowed values: half, bfloat16, float, double\n\t; NodeDef: {{node LeakyRelu}}; Op<name=LeakyRelu; signature=features:T -> activations:T; attr=alpha:float,default=0.2; attr=T:type,default=DT_FLOAT,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]> [Op:LeakyRelu]""}
```


### Relevant log output

_No response_</details>"
60519,type of argument x in tf.math.rsqrt should be float,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.12.0

### Custom Code

Yes

### OS Platform and Distribution

win11

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

According to [doc](https://tensorflow.google.cn/api_docs/python/tf/math/rsqrt), the argument of `x` in `tf.math.rsqrt` should be `bfloat16, half, float32, float64`. But when the values of `x` includes both float and bool, it wont throw an exception as following snippet code 1 shows, which is unexpected. What's more, snippet code 2 indicates that the type of `x` can be complex, which is inconsistent with the documentation.

### Standalone code to reproduce the issue

Snippet code 1: 
```
import tensorflow as tf
results={}
try:
  x_0 = 0.0
  x_1 = 1024
  x_2 = -1.0
  x_3 = -1.0
  x_4 = 0.0
  x_5 = False
  x_6 = True
  x = [x_0,x_1,x_2,x_3,x_4,x_5,x_6,]
  results[""res""] = tf.math.rsqrt(x=x,)
except Exception as e:
  results[""err""] = ""Error:""+str(e)
print(results)
# results={'res': <tf.Tensor: shape=(7,), dtype=float32, numpy=
array([    inf, 0.03125,     nan,     nan,     inf,     inf, 1.     ],
      dtype=float32)>}
```

Snippet code 2:
```
import tensorflow as tf
results={}
try:
  x = 3
  results[""res""] = tf.math.rsqrt(x=x,)
except Exception as e:
  results[""err""] = ""Error:""+str(e)
print(results)
# results={'err': ""Error:Value for attr 'T' of int32 is not in the list of allowed values: bfloat16, half, float, double, complex64, complex128\n\t; NodeDef: {{node Rsqrt}}; Op<name=Rsqrt; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]> [Op:Rsqrt]""}
```

### Relevant log output

_No response_</details>"
60518,"Network-Level Control of Precision (fp32 in training, fp16 in inference)","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf 2.12

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

A very common use case is that we train and export the model in fp32, then use fp16 mode in inference. But it seems XLA doesn't support this?

It would be good if XLA can support [Network-Level Control of Precision](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#network-level-control) like TensorRT

### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

_No response_</details>"
60515,Speech recognition : No code is present,"Please add source code for android for speech recognition currently only resources file is present

https://github.com/tensorflow/examples/tree/master/lite/examples/speech_recognition/android/app/src/main/res
@synandi  @tensorflow-copybara can u please help"
60514,How to convert KerasTensor to numpy array or TensorFlow EagerTensor?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.9.2

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I have a KerasTensor object with shape (None, 128, 128, 1) that I need to pass to an OpenCV function. However, I'm having trouble converting the KerasTensor to either a numpy array or a TensorFlow EagerTensor that can be accepted by the function. Specifically, I want to convert the KerasTensor to a format that is compatible with OpenCV's input requirements. Any suggestions on how to achieve this conversion?
I tried using tensor.numpy() to convert the KerasTensor to a numpy array, but I got an AttributeError: 'KerasTensor' object has no attribute 'numpy'.


### Standalone code to reproduce the issue

```python
import tensorflow as tf
from tensorflow import keras

# Define the shape of the input tensor
input_shape = (128, 128, 1)

# Create an input tensor with the specified shape
input_tensor = keras.layers.Input(shape=input_shape)

# Create a model that uses the input tensor
x = keras.layers.Conv2D(32, 3, activation='relu', padding='same')(input_tensor)
x = keras.layers.Conv2D(64, 3, activation='relu', padding='same')(x)
output_tensor = keras.layers.Conv2D(1, 3, activation='sigmoid', padding='same')(x)

print(type(output_tensor)) # <class 'keras.engine.keras_tensor.KerasTensor'>

# Convert the output tensor to a numpy array
print(type(output_tensor.numpy())) # AttributeError: 'KerasTensor' object has no attribute 'numpy'
print(type(tf.keras.backend.eval(output_tensor))) #AttributeError: 'KerasTensor' object has no attribute 'numpy'`
```


### Relevant log output

_No response_</details>"
60513,tf.math.is_non_decreasing should throw an exception when argument x isn't numeric,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.12.0

### Custom Code

Yes

### OS Platform and Distribution

win11

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

According to [doc](https://tensorflow.google.cn/api_docs/python/tf/math/is_non_decreasing), the argument `x` should be numeric tensor. When the values of the argument `x` only contain bool, it throws an exception as following snippet code 1 shows. However, when the values of the argument `x` are contained with int and bool value, it wont throw an exception as snippet code 2 shows. I suppose that it may cast bool type to int type when there're both int and bool value in argument `x`, but I think this conversion is unexpected.

### Standalone code to reproduce the issue

```shell
snippet code 1:

import tensorflow as tf
results={}
try:
  x_0 = True
  x_1 = True
  x_2 = False
  x = (x_0,x_1,x_2,)
  results[""res""] = tf.math.is_non_decreasing(x=x,)
except Exception as e:
  results[""err""] = ""Error:""+str(e)
print(results)
# results={'err': 'Error:Expected x to be numeric, instead found: tf.Tensor([ True  True False], shape=(3,), dtype=bool)'}
```

snippet code 2:
```
import tensorflow as tf
results={}
try:
  x_0 = -16
  x_1 = 16
  x_2 = False
  x = (x_0,x_1,x_2,)
  results[""res""] = tf.math.is_non_decreasing(x=x,)
except Exception as e:
  results[""err""] = ""Error:""+str(e)
print(results)
# results={'res': <tf.Tensor: shape=(), dtype=bool, numpy=False>}
```
```


### Relevant log output

_No response_</details>"
60512,"""No matching distribution found for tensorflow"" when installing with pip on macOS ","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.3.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

`ERROR: No matching distribution found for tensorflow==2.3.0`

### Standalone code to reproduce the issue

```shell
pip install -U --index-url https://pypi.org/ tensorflow==2.3.0
```


### Relevant log output

```shell
Same issue as here: ""No matching distribution found for tensorflow"" when installing with pip on macOS 11 #47205, https://github.com/tensorflow/tensorflow/issues/47205
The issue did not resolve but was closed.
The solution doesn't work it errored with ""ERROR: No matching distribution found for tensorflow==2.3.0""
```
</details>"
60511,arguments in tf.math.greater should be the same type,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.12.0

### Custom Code

Yes

### OS Platform and Distribution

win11

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

According to [doc](https://tensorflow.google.cn/api_docs/python/tf/math/greater), the argument `x` and `y` in `tf.math.greater` should be the same type. However, in following snippet code, the type of argument `x` is int and the type of argument `y` is float32, it doesnt throw an exception, which is inconsitent with the doc.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
results={}
try:
  x = 16
  y_tensor = tf.random.uniform([4, 1, 1], dtype=tf.float32)
  y = tf.identity(y_tensor)
  results[""res""] = tf.math.greater(x=x,y=y,)
except Exception as e:
  results[""err""] = ""Error:""+str(e)
print(results)

results = 'res': <tf.Tensor: shape=(4, 1, 1), dtype=bool, numpy=
array([[[ True]],
      [[ True]],
      [[ True]],
      [[ True]]])>
```
```


### Relevant log output

_No response_</details>"
60510,Accuracy metric appears to behave incorrectly with multiple outputs,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.7.0

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.9.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

In the example below, the training set is 800 records, validation set is 200 records and batch size is 10. My understanding of the accuracy metric is that it should go up in steps of 1/[number of records] between 0 and 1. It's result should be the fraction of all individual predictions that were correct. As the description of the accuracy metric on tensorflow's website: ""Calculates how often predictions equal labels.""

However, in the log you can see that the accuracies on the training set (with 80 batches of size 10) go up in steps of 0.0125 (1/80). The validation set (containing 20 batches of size 10) go up in steps of 0.05 (1/20). It must be somehow calculating the fraction of the batches that are ""correct"" instead of the number of correct predictions out of all the batches.

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

tf.keras.utils.set_random_seed(1)

# define the data
x = np.random.random(1000)
y1 = np.random.randint(0, 2, 1000)
y2 = np.random.randint(0, 2, 1000)

# define the model
input = tf.keras.layers.Input(shape=(1,))
model = tf.keras.layers.Dense(64, activation='relu')(input)
model = tf.keras.layers.Dense(2, activation='sigmoid')(model)
out1 = tf.keras.layers.Lambda(lambda x: x[..., 0], name='out1')(model)
out2 = tf.keras.layers.Lambda(lambda x: x[..., 1], name='out2')(model)
model = tf.keras.models.Model(input, [out1, out2])

# configure losses and metrics for each output
losses = {'out1': 'binary_crossentropy', 'out2': 'binary_crossentropy'}
weights = {'out1': 1.0, 'out2': 1.0}
metrics = {'out1': 'accuracy', 'out2': 'accuracy'}
model.compile(loss=losses, loss_weights=weights, optimizer='adam', metrics=metrics)

# fit
model.fit(x, [y1, y2], batch_size=10, epochs=10, verbose=2, validation_split=0.2)
```


### Relevant log output

```shell
2023-05-05 14:27:37.862813: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-05 14:27:38.294938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1242 MB memory:  -> device: 0, 
name: NVIDIA T500, pci bus id: 0000:01:00.0, compute capability: 7.5
Epoch 1/10
80/80 - 1s - loss: 1.3891 - out1_loss: 0.6953 - out2_loss: 0.6938 - out1_accuracy: 0.1125 - out2_accuracy: 0.1625 - val_loss: 1.3861 - val_out1_loss: 0.6933 - val_out2_loss: 0.6929 
- val_out1_accuracy: 0.1500 - val_out2_accuracy: 0.0500 - 1s/epoch - 18ms/step
Epoch 2/10
80/80 - 0s - loss: 1.3866 - out1_loss: 0.6931 - out2_loss: 0.6935 - out1_accuracy: 0.0625 - out2_accuracy: 0.0375 - val_loss: 1.3843 - val_out1_loss: 0.6918 - val_out2_loss: 0.6925 
- val_out1_accuracy: 0.2000 - val_out2_accuracy: 0.1000 - 313ms/epoch - 4ms/step
Epoch 3/10
80/80 - 0s - loss: 1.3872 - out1_loss: 0.6938 - out2_loss: 0.6934 - out1_accuracy: 0.1125 - out2_accuracy: 0.1250 - val_loss: 1.3837 - val_out1_loss: 0.6916 - val_out2_loss: 0.6920 
- val_out1_accuracy: 0.2000 - val_out2_accuracy: 0.0500 - 331ms/epoch - 4ms/step
Epoch 4/10
80/80 - 0s - loss: 1.3865 - out1_loss: 0.6929 - out2_loss: 0.6935 - out1_accuracy: 0.1125 - out2_accuracy: 0.1125 - val_loss: 1.3834 - val_out1_loss: 0.6916 - val_out2_loss: 0.6918 
- val_out1_accuracy: 0.2000 - val_out2_accuracy: 0.1000 - 335ms/epoch - 4ms/step
Epoch 5/10
80/80 - 0s - loss: 1.3859 - out1_loss: 0.6929 - out2_loss: 0.6930 - out1_accuracy: 0.0750 - out2_accuracy: 0.1125 - val_loss: 1.3832 - val_out1_loss: 0.6917 - val_out2_loss: 0.6915 
- val_out1_accuracy: 0.2000 - val_out2_accuracy: 0.1000 - 339ms/epoch - 4ms/step
Epoch 6/10
80/80 - 0s - loss: 1.3861 - out1_loss: 0.6931 - out2_loss: 0.6930 - out1_accuracy: 0.1125 - out2_accuracy: 0.0625 - val_loss: 1.3831 - val_out1_loss: 0.6919 - val_out2_loss: 0.6913 
- val_out1_accuracy: 0.2000 - val_out2_accuracy: 0.1500 - 333ms/epoch - 4ms/step
Epoch 7/10
80/80 - 0s - loss: 1.3860 - out1_loss: 0.6934 - out2_loss: 0.6926 - out1_accuracy: 0.1000 - out2_accuracy: 0.0625 - val_loss: 1.3830 - val_out1_loss: 0.6920 - val_out2_loss: 0.6911 
- val_out1_accuracy: 0.2500 - val_out2_accuracy: 0.1000 - 335ms/epoch - 4ms/step
Epoch 8/10
80/80 - 0s - loss: 1.3860 - out1_loss: 0.6929 - out2_loss: 0.6930 - out1_accuracy: 0.1000 - out2_accuracy: 0.1250 - val_loss: 1.3826 - val_out1_loss: 0.6918 - val_out2_loss: 0.6908 
- val_out1_accuracy: 0.2000 - val_out2_accuracy: 0.2000 - 342ms/epoch - 4ms/step
Epoch 9/10
80/80 - 0s - loss: 1.3861 - out1_loss: 0.6930 - out2_loss: 0.6931 - out1_accuracy: 0.0625 - out2_accuracy: 0.1250 - val_loss: 1.3824 - val_out1_loss: 0.6917 - val_out2_loss: 0.6907 
- val_out1_accuracy: 0.2000 - val_out2_accuracy: 0.1500 - 336ms/epoch - 4ms/step
Epoch 10/10
80/80 - 0s - loss: 1.3857 - out1_loss: 0.6930 - out2_loss: 0.6927 - out1_accuracy: 0.0750 - out2_accuracy: 0.1250 - val_loss: 1.3824 - val_out1_loss: 0.6919 - val_out2_loss: 0.6905 
- val_out1_accuracy: 0.2000 - val_out2_accuracy: 0.1000 - 349ms/epoch - 4ms/step
```
</details>"
60509,How does average pooling function work in TensorFlow?,"Let us assume a tensor like this:

```
x = tf.constant([[1., 2., 3.],
                  [4., 5., 6.],
                  [7., 8., 9.]])
```

To apply the average pooling function, I will do this:

```
x = tf.reshape(x, [1, 3, 3, 1])
avg_pool_2d = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),strides=(2, 2), padding='same')
avg_pool_2d(x)
```

The result is:

```
<tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=
array([[[[3. ],
         [4.5]],
        [[7.5],
         [9. ]]]], dtype=float32)>
```

I can follow the logic above:

```
(1+2+4+5)/4 = 3
(3+6)/2 = 4.5
(7+8)/2 = 7.5
(9/1) = 9
```

**I think the logic is:**  The pooling filter is usually situated inside the tensor to perform the pooling operator. But when the entire filter does not situate inside the tensor (see the below figure for an example), we need to specify the number of elements of the filter that are situated inside the tensor (a). The following figure illustrates the logic for a 4 by 3 tensor, with pooling filter and stride sizes of 2 by 2, and padding the same.

![image](https://user-images.githubusercontent.com/82632526/236491612-8b6b6fee-414a-4dd6-ba7d-98c77713a7c9.png)

However, it is not always like this. For example, suppose the following tensor:

```
y = tf.constant([[1., 2., 3., 4., 5.],
                         [6., 7., 8., 9., 10.]])
```

Then, I do this:

```
y = tf.reshape(y, [1, 2, 5, 1])
avg_pool_2d = tf.keras.layers.AveragePooling2D(pool_size=(4, 4),strides=(4, 4), padding='same')
avg_pool_2d(y)
```

The result is like this:

```
    <tf.Tensor: shape=(1, 1, 2, 1), dtype=float32, numpy=
array([[[[4.5 ],
         [7.]]]], dtype=float32)>
```

If I wanted to follow the logic for the first example, I expected the result to be like this:

```
(1+2+3+4+6+7+8+9)/8 = 5
(5+10)/2 = 7.5
```

I am using TensorFlow 2.8.0. What mistake am I making?"
60508,TensorFlow is failing with Bazel@HEAD,"https://buildkite.com/bazel/bazel-at-head-plus-downstream/builds/2997#0187e4ef-95bb-43c3-84ea-7cd94dbf0b7d

Platforms : Linux and Windows

Logs :  
```
ERROR: /var/lib/buildkite-agent/builds/bk-docker-5xrg/bazel-downstream-projects/tensorflow/tensorflow/cc/BUILD:766:22: Linking tensorflow/cc/ops/functional_ops_gen_cc [for tool] failed: (Exit 1): gcc failed: error executing command (from target //tensorflow/cc:ops/functional_ops_gen_cc)
  (cd /var/lib/buildkite-agent/.cache/bazel/_bazel_buildkite-agent/c5e474199d1c61ddf0091ab0c5156762/execroot/org_tensorflow && \
  exec env - \
    PATH=/var/lib/buildkite-agent/.cache/bazelisk/local/-tmp-tmpjryk49s9-bazel/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    ZERO_AR_DATE=1 \
  /usr/bin/gcc @bazel-out/k8-opt-exec-ST-86d428d70b9d/bin/tensorflow/cc/ops/functional_ops_gen_cc-2.params)
# Configuration: 480f85a7755fc62a440f8fbe1f9f17e1ba62abf41ed236d08b174eb7e701c839
# Execution platform: @local_execution_config_platform//:platform
bazel-out/k8-opt-exec-ST-86d428d70b9d/bin/_solib_k8/_U_S_Stensorflow_Scc_Cops_Sfunctional_Uops_Ugen_Ucc___Utensorflow/libtensorflow_framework.so.2: error: undefined reference to 'tsl::table::NewLRUCache(unsigned long)'
bazel-out/k8-opt-exec-ST-86d428d70b9d/bin/_solib_k8/_U_S_Stensorflow_Scc_Cops_Sfunctional_Uops_Ugen_Ucc___Utensorflow/libtensorflow_framework.so.2: error: undefined reference to 'tensorflow::GpuIdManager::TfToPlatformDeviceId(tsl::gtl::IntType<tsl::TfDeviceId_tag_, int>, tsl::gtl::IntType<tsl::PlatformDeviceId_tag_, int>*)'
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build
(04:36:51) INFO: Elapsed time: 1816.483s, 
```
Steps followed :  
```
1. git clone -v https://github.com/tensorflow/tensorflow.git
2. git reset cf9014ebbc63ae2e905930e333bde676b87f0167 --hard
3. export USE_BAZEL_VERSION=8087520b153832656695fef6f3654ed432642c98
4. bazel build //tensorflow/tools/pip_package:build_pip_package

```
CC @meteorcloudy @oquenchil 




 "
60506,Memory Leak in mkl_graph_util.h,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

Debian GNU/Linux 10

### Mobile device

_No response_

### Python version

3.9

### Bazel version

6.1.1

### GCC/Compiler version

clang 12.0.1

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

In [mkl_graph_util.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/graph/mkl_graph_util.h#L199C53-L200), we create a thread_local hash set using `new`. But it doesn't look like we ever free this memory. AddressSanitizer reports a memory leak when running a program that uses this code. Maybe we can change this to `std::unique_ptr`?

### Standalone code to reproduce the issue

```shell
// compiling with ASAN should leak.

#include ""tensorflow/core/public/session.h""

using namespace ::tensorflow;

int main(int argc, char** argv) {
  GraphDef gdef;
  Scope scope;
  auto op = ops::Add(scope, ops::Const(1), ops::Const(2));
  scope.ToGraphDef(&gdef);
  std::unique_ptr<Session> session = NewSession(SessionOptions());

  std::vector<Tensor> outputs;
  session->Run({}, {}, {}, &outputs);
  return 0;
}
```


### Relevant log output

```shell
Direct leak of 56 byte(s) in 1 object(s) allocated from:
    #0 0x14ffadad in malloc (/home/axlui/.cache/bazel/_bazel_axlui/f4685961e4a033eb3c5c8f3ed28f41d5/execroot/__main__/bazel-out/k8-opt/bin/cc/main_asan+0x14ffadad)
    #1 0x7f23a56450b4 in operator new(unsigned long) (/lib/x86_64-linux-gnu/libstdc++.so.6+0xa60b4)
    #2 0x2a67d4c6 in tensorflow::mkl_op_registry::IsMklOp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tensorflow::DataType) /proc/self/cwd/external/org_tensorflow/tensorflow/core/graph/mkl_graph_util.h:268:10
    #3 0x2a691503 in tensorflow::MklLayoutRewritePass::CheckForNodeRewrite(tensorflow::Node const*) const /proc/self/cwd/external/org_tensorflow/tensorflow/core/common_runtime/mkl_layout_pass.cc:3832:8
    #4 0x2a694b15 in tensorflow::MklLayoutRewritePass::RunPass(std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> >*) /proc/self/cwd/external/org_tensorflow/tensorflow/core/common_runtime/mkl_layout_pass.cc:4147:15
    #5 0x2a695b79 in tensorflow::MklLayoutRewritePass::Run(tensorflow::GraphOptimizationPassOptions const&)::$_15::operator()(std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> >*) const /proc/self/cwd/external/org_tensorflow/tensorflow/core/common_runtime/mkl_layout_pass.cc:4205:5
    #6 0x2a6958f2 in tensorflow::MklLayoutRewritePass::Run(tensorflow::GraphOptimizationPassOptions const&) /proc/self/cwd/external/org_tensorflow/tensorflow/core/common_runtime/mkl_layout_pass.cc:4218:7
    #7 0x2a77721a in tensorflow::OptimizationPassRegistry::RunGrouping(tensorflow::OptimizationPassRegistry::Grouping, tensorflow::GraphOptimizationPassOptions const&) /proc/self/cwd/external/org_tensorflow/tensorflow/core/common_runtime/optimization_registry.cc:73:26
    #8 0x289796a8 in tensorflow::DirectSession::CreateGraphs(tensorflow::BuildGraphOptions const&, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> > > > >*, std::unique_ptr<tensorflow::FunctionLibraryDefinition, std::default_delete<tensorflow::FunctionLibraryDefinition> >*, tensorflow::DirectSession::RunStateArgs*, absl::InlinedVector<tensorflow::DataType, 4ul, std::allocator<tensorflow::DataType> >*, absl::InlinedVector<tensorflow::DataType, 4ul, std::allocator<tensorflow::DataType> >*, long*) /proc/self/cwd/external/org_tensorflow/tensorflow/core/common_runtime/direct_session.cc:1729:3
    #9 0x28976dfc in tensorflow::DirectSession::CreateExecutors(tensorflow::CallableOptions const&, std::unique_ptr<tensorflow::DirectSession::ExecutorsAndKeys, std::default_delete<tensorflow::DirectSession::ExecutorsAndKeys> >*, std::unique_ptr<tensorflow::DirectSession::FunctionInfo, std::default_delete<tensorflow::DirectSession::FunctionInfo> >*, tensorflow::DirectSession::RunStateArgs*) /proc/self/cwd/external/org_tensorflow/tensorflow/core/common_runtime/direct_session.cc:1331:3
    #10 0x289722e2 in tensorflow::DirectSession::GetOrCreateExecutors(absl::Span<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const>, absl::Span<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const>, absl::Span<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const>, tensorflow::DirectSession::ExecutorsAndKeys**, tensorflow::DirectSession::RunStateArgs*) /proc/self/cwd/external/org_tensorflow/tensorflow/core/common_runtime/direct_session.cc:1573:3
    #11 0x2897014e in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*, tsl::thread::ThreadPoolOptions const&) /proc/self/cwd/external/org_tensorflow/tensorflow/core/common_runtime/direct_session.cc:873:3
    #12 0x2896fbd3 in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) /proc/self/cwd/external/org_tensorflow/tensorflow/core/common_runtime/direct_session.cc:842:10
    #13 0x2896c1eb in tensorflow::DirectSession::Run(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) /proc/self/cwd/external/org_tensorflow/tensorflow/core/common_runtime/direct_session.cc:472:10
    #14 0x1504aef5 in nn::NNInterface::Infer() /proc/self/cwd/cc/nn/nn_interface.cc:279:3
    #15 0x15049658 in nn::NNInterface::InferLoop() /proc/self/cwd/cc/nn/nn_interface.cc:267:5
    #16 0x1505f32b in void std::__invoke_impl<void, void (nn::NNInterface::*)(), nn::NNInterface*>(std::__invoke_memfun_deref, void (nn::NNInterface::*&&)(), nn::NNInterface*&&) /usr/lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/invoke.h:73:14
    #17 0x1505f264 in std::__invoke_result<void (nn::NNInterface::*)(), nn::NNInterface*>::type std::__invoke<void (nn::NNInterface::*)(), nn::NNInterface*>(void (nn::NNInterface::*&&)(), nn::NNInterface*&&) /usr/lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/invoke.h:95:14
    #18 0x1505f234 in void std::thread::_Invoker<std::tuple<void (nn::NNInterface::*)(), nn::NNInterface*> >::_M_invoke<0ul, 1ul>(std::_Index_tuple<0ul, 1ul>) /usr/lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/thread:264:13
    #19 0x1505f1f8 in std::thread::_Invoker<std::tuple<void (nn::NNInterface::*)(), nn::NNInterface*> >::operator()() /usr/lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/thread:271:11
    #20 0x1505f09c in std::thread::_State_impl<std::thread::_Invoker<std::tuple<void (nn::NNInterface::*)(), nn::NNInterface*> > >::_M_run() /usr/lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/thread:215:13
    #21 0x7f23a566decf  (/lib/x86_64-linux-gnu/libstdc++.so.6+0xceecf)
```
</details>"
60505,Is DTensor compatible with Horovod,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Want to check if DTensor model parallel compatible with Horovod? Any code examples or Benchmark of DTensor model parallel? 

### Standalone code to reproduce the issue

```shell
NA
```


### Relevant log output

_No response_</details>"
60504,Android Download page URL not available,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

On the frontpage of the README.md, the Android Download link pointed to https://bintray.com/google/tensorflow/tensorflow/_latestVersion now is not available and will redirect to another page.

### Standalone code to reproduce the issue

```shell
Document issue which easy to found on frontpage of this github project.
```


### Relevant log output

_No response_</details>"
60503,TensorFlow Refuses to See my GPU All The Time,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.25.1

### Custom Code

Yes

### OS Platform and Distribution

Linux, Ubuntu 22.04.6

### Mobile device

_No response_

### Python version

Python 3.11

### Bazel version

N/A*

### GCC/Compiler version

11.2

### CUDA/cuDNN version

12

### GPU model and memory

Nvidia GT 1030 2gb

### Current Behaviour?

<pre>``2023-05-05 02:37:58.234754: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2023-05-05 02:37:58.270803: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2023-05-05 02:37:58.271133: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-05 02:37:58.944800: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-05 02:37:59.878527: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-05 02:37:59.878936: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...``<pre>

### Standalone code to reproduce the issue

```shell
The above is the response every time even though the GPU exists in the PC.

The GT 1030 might not be the most powerful GPU out there, but it does have 344 Cuda Cores.

I also have the Cuda Toolkit installed, along with the latest drivers available (I'm surprised this card is still getting support).

To reproduce,run the following command in the terminal: ``python3 insert script name here.py (replace with the name of your .py script).

The output will tell you what has happened.

Just FYI, this also affects AMD Radeon Pro WX 5, 6 and 7000 series GPU's and their gaming variants i.e., my RX 6700 XT in this case running in WSL 1.

Godspeed.
```


### Relevant log output

_No response_</details>"
60501,tflite build on Windows crash if having multiple versions of Python in PATH,"### System information

* tesorflow 2.12.0
* windows-latest in github actions (log https://github.com/pharmpy/tflite-runtime-wheels/actions/runs/4817630805/jobs/8578588825)

### Describe the problem

Building tflite 2.12.0 using bazel on Windows having multiple versions of Python in `PATH` fails. This seems to be because the `where` command is used to find which Python is in `PATH` and this then returns multiple paths which gets concatenated together.  Having multiple Pythons in `PATH` works when building on MacOS and Linux.

### Source code / logs

This is the end of the output from the compilation with the error: (notice that the paths to the two Pythons have both been put into `PYTHON_BIN_PATH`

```
Repository rule local_python_configure defined at:
  D:/a/tflite-runtime-wheels/tflite-runtime-wheels/third_party/py/python_configure.bzl:279:41: in <toplevel>
ERROR: An error occurred during the fetch of repository 'local_execution_config_python':
   Traceback (most recent call last):
	File ""D:/a/tflite-runtime-wheels/tflite-runtime-wheels/third_party/py/python_configure.bzl"", line 212, column 22, in _create_local_python_repository
		_check_python_bin(repository_ctx, python_bin)
	File ""D:/a/tflite-runtime-wheels/tflite-runtime-wheels/third_party/py/python_configure.bzl"", line 145, column 25, in _check_python_bin
		auto_config_fail(""--define %s='%s' is not executable. Is it the python binary?"" % (
	File ""D:/a/tflite-runtime-wheels/tflite-runtime-wheels/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail
		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg))
Error in fail: Configuration Error: --define PYTHON_BIN_PATH='C:\\hostedtoolcache\\windows\\Python\\3.10.11\\x64\\python3.exe
C:\\hostedtoolcache\\windows\\Python\\3.9.13\\x64\\python3.exe' is not executable. Is it the python binary?
ERROR: D:/a/tflite-runtime-wheels/tflite-runtime-wheels/WORKSPACE:15:14: fetching local_python_configure rule //external:local_execution_config_python: Traceback (most recent call last):
	File ""D:/a/tflite-runtime-wheels/tflite-runtime-wheels/third_party/py/python_configure.bzl"", line 212, column 22, in _create_local_python_repository
		_check_python_bin(repository_ctx, python_bin)
	File ""D:/a/tflite-runtime-wheels/tflite-runtime-wheels/third_party/py/python_configure.bzl"", line 145, column 25, in _check_python_bin
		auto_config_fail(""--define %s='%s' is not executable. Is it the python binary?"" % (
	File ""D:/a/tflite-runtime-wheels/tflite-runtime-wheels/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail
		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg))
Error in fail: Configuration Error: --define PYTHON_BIN_PATH='C:\\hostedtoolcache\\windows\\Python\\3.10.11\\x64\\python3.exe
C:\\hostedtoolcache\\windows\\Python\\3.9.13\\x64\\python3.exe' is not executable. Is it the python binary?
ERROR: Analysis of target '//tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper' failed; build aborted: Configuration Error: --define PYTHON_BIN_PATH='C:\\hostedtoolcache\\windows\\Python\\3.10.11\\x64\\python3.exe
C:\\hostedtoolcache\\windows\\Python\\3.9.13\\x64\\python3.exe' is not executable. Is it the python binary?
INFO: Elapsed time: 357.021s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (10 packages loaded, 15 targets configured)
FAILED: Build did NOT complete successfully (10 packages loaded, 15 targets configured)
Error: Process completed with exit code 1.
```
"
60500,Regression: //bazel_pip/tensorflow/python/kernel_tests/nn_ops:pooling_ops_3d_test fails,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.10

### Bazel version

5.3.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

https://github.com/tensorflow/tensorflow/commit/f9becdcb81053e80b9c9034f3d4c31984f161dd1 introduced unit test failure when TF_ENABLE_ONEDNN_OPTS=1

### Standalone code to reproduce the issue

```shell
bazel test --build_tests_only --config=mkl_aarch64_threadpool --copt=-flax-vector-conversions --test_env=TF_ENABLE_ONEDNN_OPTS=1 --test_env=TF2_BEHAVIOR=1 --define=no_tensorflow_py_deps=true --test_lang_filters=py --flaky_test_attempts=3 --test_size_filters=small,medium --test_output=errors --verbose_failures=true --test_keep_going --notest_verbose_timeout_warnings --local_test_jobs=64 --test_tag_filters=-nopip,-no_pip,-oss_serial,-no_oss,-oss_excluded,-v1only,-benchmark-test,-no_aarch64,-no_oss_py38,-no_oss_py39,-no_oss_py310 -k -- //bazel_pip/tensorflow/... -//bazel_pip/tensorflow/compiler/tf2tensorrt/... -//bazel_pip/tensorflow/compiler/xrt/... -//bazel_pip/tensorflow/core/tpu/... -//bazel_pip/tensorflow/go/... -//bazel_pip/tensorflow/java/... -//bazel_pip/tensorflow/python/integration_testing/... -//bazel_pip/tensorflow/tools/toolchains/... -//bazel_pip/tensorflow/lite/... -//bazel_pip/tensorflow/python/kernel_tests/nn_ops:atrous_conv2d_test
```


### Relevant log output

```shell
======================================================================
FAIL: testAvgPool3dGradInvalidKsize (__main__.PoolingTest)
PoolingTest.testAvgPool3dGradInvalidKsize
----------------------------------------------------------------------
tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped____MklNativeAvgPool3DGrad_device_/job:localhost/replica:0/task:0/device:CPU:0}} Sliding window ksize for dimension 1 was zero. [Op:AvgPool3DGrad]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/tmpfs/bazel_output/_bazel_ubuntu/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/bazel_pip/tensorflow/python/kernel_tests/nn_ops/pooling_ops_3d_test_cpu.runfiles/org_tensorflow/bazel_pip/tensorflow/python/kernel_tests/nn_ops/pooling_ops_3d_test.py"", line 181, in testAvgPool3dGradInvalidKsize
    with self.assertRaisesRegex(
AssertionError: ""ksize must be positive, got: *"" does not match ""{{function_node __wrapped____MklNativeAvgPool3DGrad_device_/job:localhost/replica:0/task:0/device:CPU:0}} Sliding window ksize for dimension 1 was zero. [Op:AvgPool3DGrad]""

----------------------------------------------------------------------
Ran 33 tests in 22.409s

FAILED (failures=1, skipped=1)
================================================================================
```
</details>"
60497,Support CheckpointInputPipelineHook in Estimator MirrorStrategy,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf 1.15

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Failed to save pipeline info in checkpoint.

### Standalone code to reproduce the issue

```shell
mirrored_strategy = tf.distribute.MirroredStrategy()
    run_config = tf.estimator.RunConfig(
        model_dir=output_dir,
        save_checkpoints_steps=save_checkpoints_steps,
        train_distribute=mirrored_strategy,
        eval_distribute=mirrored_strategy,
        keep_checkpoint_max=max_ckp_num
    )

estimator.train(input_fn=input_fn, 
                hooks=[tf.data.experimental.CheckpointInputPipelineHook(estimator)],
                       steps=tf_args.test_train_step)
```


### Relevant log output

```shell
2023-05-03 21:53:52.048712: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at iterator_ops.cc:1081 : Failed precondition: RemoteCall is stateful.
```
</details>"
60495,GPU VRAM usage significantly higher for LSTM models on TensorFlow v1.X when compared to other frameworks,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Custom
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
-   **TensorFlow installed from (source or binary)**: binary
-   **TensorFlow version**: 2.5
-   **Python version**: 3.7.5
-   **GPU model and memory**:  NVIDIA Quadro RTX 4000 GPU with 8GB of VRAM
-  **CPU model**: Intel(R) Core(TM) i9-9900K CPU operating at 3.60GHz 
- **cuDNN version**: 7.6.5


### Describe the problem
Hello everybody.

I’ve been experimenting with different models and different frameworks, and I’ve noticed that, when using GPU, GPU VRAM consumption when using LSTM models on the IMDB dataset is significantly higher on TensorFlow v1.X compared to the PyTorch and Keras implementations. Note that I artificially increased the size of the testing set of the MNIST dataset by duplicating the set by 100 times for a total of 1 000 000 samples. I've done so so any difference in inference time may be clearer.


Here are boxplots that showcase the GPU VRAM consumption of numerous LSTM models:

![Screenshot 2023-05-04 015324 - Copy (3)](https://user-images.githubusercontent.com/132307143/236126233-d175314f-70d0-461e-b0ed-cbe59b34533f.png)


Any ideas on what may be causing this?
"
60494,Significantly higher RAM consumption for Keras Lenet-5 models (especially during inference) on Keras when compared to other frameworks,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Custom
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
-   **TensorFlow installed from (source or binary)**: binary
-   **TensorFlow version**: 2.5
-   **Python version**: 3.7.5
-   **GPU model and memory**:  NVIDIA Quadro RTX 4000 GPU with 8GB of VRAM
-  **CPU model**: Intel(R) Core(TM) i9-9900K CPU operating at 3.60GHz 
- **cuDNN version**: 7.6.5


### Describe the problem
Hello everybody.

I’ve been experimenting with different models and different frameworks, and I’ve noticed that, when using CPU, RAM consumption when using Lenet5 models on the MNIST dataset is significantly higher on Keras compared to the PyTorch and TensorFlow v1.X implementations, with RAM consumption during inference standing out. Note that I artificially increased the size of the testing set of the MNIST dataset by duplicating the set by 100 times for a total of 1 000 000 samples. I've done so so any difference in inference time may be clearer.

This also might be related to the slowdown observed during inference on Keras (https://github.com/tensorflow/tensorflow/issues/60462)

Here are boxplots that showcase the RAM consumption of numerous Lenet5 models:

![Screenshot 2023-05-04 015324 - Copy](https://user-images.githubusercontent.com/132307143/236123835-7c2c46c6-f1f6-4fe7-9b12-bea3750cb3c5.png)


Any ideas on what may be causing this?
"
60493,There are some installation problems such as 'tflite-model-maker'.,"**System information**
- OS Platform and Distribution: **macOS 13.3.1 (MacBook Pro M2)**
- TensorFlow installed from (source or binary): [tf-nightly 2.14.0.dev20230503](https://pypi.org/project/tf-nightly/)
- TensorFlow version (or github SHA if from source): **Version:  2.14.0-dev20230503** 
- Python Version(Default): **Python 3.11.3**


**Provide the text output from 'tflite-model-maker'**

```
# Summary: Successfully installed TensorFlow(2.14.0-dev20230503) but struggling about installing tflite-model-maker
#
sudo pip3 install tflite-model-maker
haesunglee@MacBookPro14-haesunglee ~ % sudo pip3 install tflite-model-maker
Password:
WARNING: The directory '/Users/haesunglee/Library/Caches/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.
Collecting tflite-model-maker
  Downloading tflite_model_maker-0.4.2-py3-none-any.whl (577 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 577.3/577.3 kB 6.4 MB/s eta 0:00:00
Collecting tf-models-official==2.3.0 (from tflite-model-maker)
  Downloading tf_models_official-2.3.0-py2.py3-none-any.whl (840 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 840.9/840.9 kB 7.0 MB/s eta 0:00:00
...
...
...
Collecting tflite-model-maker
  Downloading tflite_model_maker-0.1.0-py3-none-any.whl (84 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.6/84.6 kB 6.1 MB/s eta 0:00:00
ERROR: Cannot install tflite-model-maker==0.1.2, tflite-model-maker==0.2.0, tflite-model-maker==0.2.1, tflite-model-maker==0.2.2, tflite-model-maker==0.2.3, tflite-model-maker==0.2.4, tflite-model-maker==0.2.5, tflite-model-maker==0.3.0, tflite-model-maker==0.3.1, tflite-model-maker==0.3.2, tflite-model-maker==0.3.3, tflite-model-maker==0.3.4, tflite-model-maker==0.4.0, tflite-model-maker==0.4.1 and tflite-model-maker==0.4.2 because these package versions have conflicting dependencies.

The conflict is caused by:
    tflite-model-maker 0.4.2 depends on tflite-support>=0.4.2
    tflite-model-maker 0.4.1 depends on tflite-support-nightly
    tflite-model-maker 0.4.0 depends on tflite-support>=0.4.0
    tflite-model-maker 0.3.4 depends on tflite-support>=0.3.1
    tflite-model-maker 0.3.3 depends on tflite-support>=0.3.1
    tflite-model-maker 0.3.2 depends on tflite-support>=0.1.0rc4
    tflite-model-maker 0.3.1 depends on tflite-support>=0.1.0rc4
    tflite-model-maker 0.3.0 depends on tflite-support>=0.1.0rc4
    tflite-model-maker 0.2.5 depends on tflite-support==0.1.0rc4
    tflite-model-maker 0.2.4 depends on tflite-support==0.1.0rc4
    tflite-model-maker 0.2.3 depends on tflite-support==0.1.0rc3.dev2
    tflite-model-maker 0.2.2 depends on tflite-support==0.1.0rc3.dev2
    tflite-model-maker 0.2.1 depends on tflite-support==0.1.0rc3.dev2
    tflite-model-maker 0.2.0 depends on tflite-support==0.1.0rc3.dev2
    tflite-model-maker 0.1.2 depends on tflite-support==0.1.0rc3.dev2

To fix this you could try to:
1. loosen the range of package versions you've specified
2. remove package versions to allow pip attempt to solve the dependency conflict

ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts
haesunglee@MacBookPro14-haesunglee ~ % 
haesunglee@MacBookPro14-haesunglee ~ % 



```

Last year I successfully installed tflite-model-maker on my 1st mac. I'm still using it on my 1st mac.
Please help me update and fix the tflite-model-maker.

"
60489,How to write custom XLA op for tensorflow,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Feature Request

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Linux

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Hi,

I have custom tensorflow ops implemented in C++ [using this guide](https://www.tensorflow.org/guide/create_op) which are invoked during training. They work fine as long as XLA is not used, however I'm unable to get them working (and even a dummy custom xla op). 

The test op I have written is mentioned below and compiled by following the guide linked above. The compilation succeeds but I get following runtime error when the module with custom op is imported:

```
Non-OK-status: lookup_status status: NOT_FOUND: Op type not registered 'XlaTestOp' in binary running on <hostname>. Make sure the Op and Kernel are registered in the binary running in this process. 
```

I'm not able to find any more info how to resolve this issue. 



### Standalone code to reproduce the issue

```shell
C++ XLA op implementation:



#include ""tensorflow/compiler/tf2xla/shape_util.h""
#include ""tensorflow/compiler/tf2xla/xla_compiler.h""
#include ""tensorflow/compiler/tf2xla/xla_op_kernel.h""
#include ""tensorflow/compiler/tf2xla/xla_op_registry.h""
#include ""tensorflow/compiler/xla/client/xla_builder.h""
#include ""tensorflow/compiler/xla/service/custom_call_target_registry.h""
#include ""tensorflow/compiler/xla/service/hlo.pb.h""
#include ""tensorflow/compiler/xla/shape.h""
#include ""tensorflow/compiler/xla/xla_data.pb.h""
#include ""tensorflow/core/framework/op_kernel.h""
#include ""tensorflow/core/lib/hash/hash.h""
#include ""tensorflow/core/platform/human_readable_json.h""

void test_op(cudaStream_t stream, void** buffers, const char* opaque,
                     size_t opaque_len) {
  std::cout << ""executing test_op()\n"";
}



class XLATestOp : public XlaOpKernel {
public:
  explicit XLATestOp(OpKernelConstruction* ctx) : XlaOpKernel(ctx) { }

  void Compile(XlaOpKernelContext* ctx) override {

    ::xla::XlaBuilder* const builder = ctx->builder();
    ::xla::XlaOp input = ctx->Input(0);

    // use output aliasing to reuse input buffer for output
    std::vector<std::pair<::xla::ShapeIndex,
        std::pair<int64, ::xla::ShapeIndex>>> output_operand_aliasing = {
        {::xla::ShapeIndex{}, {0, ::xla::ShapeIndex{}}}
    };

    ::xla::Shape output_shape = builder->GetShape(input).value();

    ::xla::XlaOp output_op = builder->ReportErrorOrReturn(
        ::xla::CustomCall(
            builder,
            ""test_op"",
            {input},
            output_shape,
            /*opaque=*/ """",
            /*has_side_effect=*/false,
            output_operand_aliasing,
            /*literal=*/nullptr
        )
    );

    ctx->SetOutput(0, output_op);
  }
};


REGISTER_XLA_OP(Name(""XlaTestOp""), XLATestOp);
XLA_REGISTER_CUSTOM_CALL_TARGET(test_op, ""CUDA"");
```
```


### Relevant log output

_No response_</details>"
60485,XLA cumsum not supported for int8 and int16,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.14.0.dev20230503

### Custom Code

No

### OS Platform and Distribution

Google Colab

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

`tf.math.cumsum` is not supported for XLA with int8 and int16. I know that not all operations are supported by XLA, but this one seams so trivial. I assume it is an oversight.

### Standalone code to reproduce the issue

Google colab: https://colab.research.google.com/drive/1lqVjSjGmzYwLZfcfgVa8W02lWJ6XcqTk#scrollTo=ex1x-uaCfr6Y

```python
import tensorflow as tf

@tf.function(jit_compile=True)
def cumsum(x):
    return tf.math.cumsum(x)

print(cumsum(tf.constant([0, 1, 2], dtype=tf.dtypes.int8)))
```


### Relevant log output

```shell
InvalidArgumentError: Detected unsupported operations when trying to compile graph __inference_cumsum_7[_XlaMustCompile=true,config_proto=3175580994766145631,executor_type=11160318154034397263] on XLA_CPU_JIT: Cumsum (No registered 'Cumsum' OpKernel for XLA_CPU_JIT devices compatible with node {{node Cumsum}}
	 (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_INT8, Tidx=DT_INT32, exclusive=false, reverse=false){{node Cumsum}}
The op is created at: 
File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
  return _run_code(code, main_globals, None,
File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code
  exec(code, run_globals)
File ""/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py"", line 16, in <module>
  app.launch_new_instance()
File ""/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 992, in launch_instance
  app.start()
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 619, in start
  self.io_loop.start()
File ""/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 215, in start
  self.asyncio_loop.run_forever()
File ""/usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever
  self._run_once()
File ""/usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once
  handle._run()
File ""/usr/lib/python3.10/asyncio/events.py"", line 80, in _run
  self._context.run(self._callback, *self._args)
File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 687, in <lambda>
  lambda f: self._run_callback(functools.partial(callback, future))
File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 740, in _run_callback
  ret = callback()
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 821, in inner
  self.ctx_run(self.run)
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 782, in run
  yielded = self.gen.send(value)
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 377, in dispatch_queue
  yield self.process_one()
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 250, in wrapper
  runner = Runner(ctx_run, result, future, yielded)
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 748, in __init__
  self.ctx_run(self.run)
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 782, in run
  yielded = self.gen.send(value)
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 361, in process_one
  yield gen.maybe_future(dispatch(*args))
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
  yielded = ctx_run(next, result)
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 261, in dispatch_shell
  yield gen.maybe_future(handler(stream, idents, msg))
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
  yielded = ctx_run(next, result)
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 539, in execute_request
  self.do_execute(
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
  yielded = ctx_run(next, result)
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 302, in do_execute
  res = shell.run_cell(code, store_history=store_history, silent=silent)
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 539, in run_cell
  return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 2975, in run_cell
  result = self._run_cell(
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3030, in _run_cell
  return runner(coro)
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 78, in _pseudo_sync_runner
  coro.send(None)
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3257, in run_cell_async
  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3473, in run_ast_nodes
  if (await self.run_code(code, result,  async_=asy)):
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code
  exec(code_obj, self.user_global_ns, self.user_ns)
File ""<ipython-input-2-602a645c7a37>"", line 1, in <cell line: 1>
  print(cumsum(tf.constant([0, 1, 2], dtype=tf.dtypes.int8)))
File ""<ipython-input-1-9d904e83e71a>"", line 7, in cumsum
  return tf.math.cumsum(x)
	tf2xla conversion failed while converting __inference_cumsum_7[_XlaMustCompile=true,config_proto=3175580994766145631,executor_type=11160318154034397263]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions. [Op:__inference_cumsum_7]
```
</details>"
60483,Bug: tflite::PrintInterpreterState has hardcoded (short) line length in printf,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux 
-   **TensorFlow installed from (source or binary)**: source
-   **TensorFlow version (use command below)**: 2:12

### Problem Description
When using TensorFlow Lite's benchmark/profiler, both the options ```-print_postinvoke_state=true``` and ```--print_preinvoke_state=true``` are suggested for debugging. However, they both cause a call to ```tflite::PrintInterpreterState```, which truncates the names and AllocType of the tensors printed, independently of any parameter.
This renders the tool useless for debugging any models that have tensors with names longer than 25 characters (which is my case, some examples below).

Please make the total line length an argument in ```tflite::PrintInterpreterState``` and a command line argument in the benchmark, so that it can be increased if the default setting is not satisfactory.

Exact line of code where (at the latest commit at the time of writing this) the strings get truncated:  
https://github.com/tensorflow/tensorflow/blob/fbf60502e8186157325eb3f7053de4b1d1d7d5b5/tensorflow/lite/optional_debug_tools.cc#LL427C29-L427C32

### Source code / logs

Example output of tflite benchmark with options  ```-print_postinvoke_state=true --print_preinvoke_state=true```
```
Tensor  ID Name                      Type            AllocType          Size (Bytes/MB)    Shape      MemAddr-Offset
Tensor   0 serving_default_input:0   kTfLiteFloat32  kTfLiteArenaRw     71808    / 0.07 [1,187,96] [0, 71808)
Tensor   1 musicnn_classifier/fro... kTfLiteFloat32  kTfLiteMmapRo      816      / 0.00 [204] [3135892, 3136708)
Tensor   2 musicnn_classifier/fro... kTfLiteFloat32  kTfLiteMmapRo      816      / 0.00 [204] [3135064, 3135880)
Tensor   3 musicnn_classifier/fro... kTfLiteFloat32  kTfLiteMmapRo      204      / 0.00 [51] [3134848, 3135052)
Tensor   4 musicnn_classifier/fro... kTfLiteFloat32  kTfLiteMmapRo      204      / 0.00 [51] [3134632, 3134836)
Tensor   5 musicnn_classifier/fro... kTfLiteFloat32  kTfLiteMmapRo      204      / 0.00 [51] [3134416, 3134620)
Tensor   6 musicnn_classifier/bac... kTfLiteFloat32  kTfLiteMmapRo      800      / 0.00 [200] [3133604, 3134404)
Tensor   7 musicnn_classifier/bac... kTfLiteFloat32  kTfLiteMmapRo      1204800  / 1.15 [200,1506] [1928792, 3133592)
```
or
```
Tensor  52 musicnn_classifier/fro... kTfLiteInt32    ...itePersistentRo 12       / 0.00 [3] [-1, -1)   
```

*In all of these cases I can't figure out the full name of any tensor, and I have to guess the AllocType of Tensor 52*
"
60482,xla_sharding in new version,"Hi
What is the replacement for `from tensorflow.compiler.xla.experimental.xla_sharding import xla_sharding` in TF-2.12.0? For example, the function is `xla_sharding.split()`."
60479,Some unit tests fail pip test,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.10

### Bazel version

5.3.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

The unit tests //bazel_pip/tensorflow/dtensor/python/tests:multi_client_test_2gpus, //bazel_pip/tensorflow/dtensor/python/tests:multi_client_test_cpu, //bazel_pip/tensorflow/dtensor/python/tests:multi_client_test_nccl_2gpus and //bazel_pip/tensorflow/dtensor/python/tests:multi_client_test_nccl_local_2gpus fail since commit https://github.com/tensorflow/tensorflow/commit/540a08ea4ddc93a632dd8583ddeae5be5ee75bff
See https://github.com/tensorflow/tensorflow/actions/runs/4865981223/jobs/8676977815#step:5:29731

### Standalone code to reproduce the issue

```shell
bazel test --build_tests_only --config=mkl_aarch64_threadpool --copt=-flax-vector-conversions --test_env=TF_ENABLE_ONEDNN_OPTS=1 --test_env=TF2_BEHAVIOR=1 --define=no_tensorflow_py_deps=true --test_lang_filters=py --flaky_test_attempts=3 --test_size_filters=small,medium --test_output=errors --verbose_failures=true --test_keep_going --notest_verbose_timeout_warnings --local_test_jobs=64 --test_tag_filters=-nopip,-no_pip,-oss_serial,-no_oss,-oss_excluded,-v1only,-benchmark-test,-no_aarch64,-no_oss_py38,-no_oss_py39,-no_oss_py310 -k -- //bazel_pip/tensorflow/... -//bazel_pip/tensorflow/compiler/tf2tensorrt/... -//bazel_pip/tensorflow/compiler/xrt/... -//bazel_pip/tensorflow/core/tpu/... -//bazel_pip/tensorflow/go/... -//bazel_pip/tensorflow/java/... -//bazel_pip/tensorflow/python/integration_testing/... -//bazel_pip/tensorflow/tools/toolchains/... -//bazel_pip/tensorflow/lite/... -//bazel_pip/tensorflow/python/kernel_tests/nn_ops:atrous_conv2d_test
```


### Relevant log output

```shell
FAIL: //bazel_pip/tensorflow/dtensor/python/tests:multi_client_test_2gpus (see /tmpfs/bazel_output/_bazel_ubuntu/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/bazel_pip/tensorflow/dtensor/python/tests/multi_client_test_2gpus/test_attempts/attempt_1.log)
FAIL: //bazel_pip/tensorflow/dtensor/python/tests:multi_client_test_2gpus (see /tmpfs/bazel_output/_bazel_ubuntu/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/bazel_pip/tensorflow/dtensor/python/tests/multi_client_test_2gpus/test_attempts/attempt_2.log)
FAIL: //bazel_pip/tensorflow/dtensor/python/tests:multi_client_test_2gpus (see /tmpfs/bazel_output/_bazel_ubuntu/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/bazel_pip/tensorflow/dtensor/python/tests/multi_client_test_2gpus/test.log)

FAILED: //bazel_pip/tensorflow/dtensor/python/tests:multi_client_test_2gpus (Summary)
      /tmpfs/bazel_output/_bazel_ubuntu/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/bazel_pip/tensorflow/dtensor/python/tests/multi_client_test_2gpus/test.log
      /tmpfs/bazel_output/_bazel_ubuntu/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/bazel_pip/tensorflow/dtensor/python/tests/multi_client_test_2gpus/test_attempts/attempt_1.log
      /tmpfs/bazel_output/_bazel_ubuntu/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/bazel_pip/tensorflow/dtensor/python/tests/multi_client_test_2gpus/test_attempts/attempt_2.log
INFO: From Testing //bazel_pip/tensorflow/dtensor/python/tests:multi_client_test_2gpus:
==================== Test output for //bazel_pip/tensorflow/dtensor/python/tests:multi_client_test_2gpus:
2023-05-02 23:40:14.254127: I tensorflow/core/util/port.cc:116] Experimental oneDNN custom operations are on. If you experience issues, please turn them off by setting the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
Traceback (most recent call last):
  File ""/tmpfs/bazel_output/_bazel_ubuntu/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/bazel_pip/tensorflow/dtensor/python/tests/multi_client_test_2gpus.runfiles/org_tensorflow/bazel_pip/tensorflow/dtensor/python/tests/multi_client_test.py"", line 27, in <module>
    from tensorflow.dtensor.python.tests import multi_client_test_util
ImportError: cannot import name 'multi_client_test_util' from 'tensorflow.dtensor.python.tests' (/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/dtensor/python/tests/__init__.py)
================================================================================
==================== Test output for //bazel_pip/tensorflow/dtensor/python/tests:multi_client_test_2gpus:
2023-05-02 23:40:19.436998: I tensorflow/core/util/port.cc:116] Experimental oneDNN custom operations are on. If you experience issues, please turn them off by setting the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
Traceback (most recent call last):
  File ""/tmpfs/bazel_output/_bazel_ubuntu/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/bazel_pip/tensorflow/dtensor/python/tests/multi_client_test_2gpus.runfiles/org_tensorflow/bazel_pip/tensorflow/dtensor/python/tests/multi_client_test.py"", line 27, in <module>
    from tensorflow.dtensor.python.tests import multi_client_test_util
ImportError: cannot import name 'multi_client_test_util' from 'tensorflow.dtensor.python.tests' (/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/dtensor/python/tests/__init__.py)
================================================================================
==================== Test output for //bazel_pip/tensorflow/dtensor/python/tests:multi_client_test_2gpus:
2023-05-02 23:40:23.477863: I tensorflow/core/util/port.cc:116] Experimental oneDNN custom operations are on. If you experience issues, please turn them off by setting the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
Traceback (most recent call last):
  File ""/tmpfs/bazel_output/_bazel_ubuntu/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/bazel_pip/tensorflow/dtensor/python/tests/multi_client_test_2gpus.runfiles/org_tensorflow/bazel_pip/tensorflow/dtensor/python/tests/multi_client_test.py"", line 27, in <module>
    from tensorflow.dtensor.python.tests import multi_client_test_util
ImportError: cannot import name 'multi_client_test_util' from 'tensorflow.dtensor.python.tests' (/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/dtensor/python/tests/__init__.py)
================================================================================
```
</details>"
60472,"XLA size-inference integration bug (tf.where, tf.TensorArray, slice, loop)","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.14.0.dev20230502

### Custom Code

No

### OS Platform and Distribution

Google Colab

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

The attached code produces difference results for `jit_compile=True` and `jit_compile=False`, the XLA version produces the wrong results. The bug reproduces on both TF 2.12 and nightly (2.14.0.dev20230502) on Google Colab.

It appears to be an issue with `tf.size(tf.where(x == 1))`. While the visible front-end output is correct, I suspect there is an integration bug on the compiler back-end. When combined with a slice that is too big x[0:16] (`x.shape = (3, 5)`), plus many more code details, the output has same size as the input (`5` in the example). As if `tf.size(tf.where(x == 1)) == tf.size(x) == 5`, when actually `tf.size(tf.where(x == 1)) == 2`. I suspect this because changing the input shape to `(4, )` changes the output shape to `(4, )`.

This bug was very subtle and difficult to find, especially because `tf.print()` is not supported by XLA. I hope you appreciate it.

### Standalone code to reproduce the issue

Google Colab link: https://colab.research.google.com/drive/1cccEssWkCVLO515uf3RoOuEMDp5V-2ZZ?usp=sharing

```python
import tensorflow as tf

def fn(x, y):
    token_idx_to_mask = tf.where(x == 1)
    n_samples = tf.size(token_idx_to_mask)
    x_repeated = tf.repeat(tf.expand_dims(x, 0), n_samples, axis=0)

    predict_all_array = tf.TensorArray(x_repeated.dtype, size=1, infer_shape=False, element_shape=(None, ))
    for batch_i in tf.range(1):
        x_batch = x_repeated[0:(batch_i + 1)*16, ...]
        y_batch = x_batch[:, y]
        predict_all_array = predict_all_array.write(batch_i, y_batch)
    predicts_all = predict_all_array.concat()

    return n_samples, predicts_all

fn_jit = tf.function(reduce_retracing=True, jit_compile=True)(fn)
fn_std = tf.function(reduce_retracing=True)(fn)

print('XLA': fn_jit(tf.constant([0, 1, 1, 0, 0], dtype=tf.dtypes.int32), tf.constant(0)))
print('STD': fn_std(tf.constant([0, 1, 1, 0, 0], dtype=tf.dtypes.int32), tf.constant(0)))
```


### Relevant log output

```shell
XLA: (<tf.Tensor: shape=(), dtype=int32, numpy=2>, <tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 0, 0, 0, 0], dtype=int32)>)
STD: (<tf.Tensor: shape=(), dtype=int32, numpy=2>, <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 0], dtype=int32)>)
```

Expected results would be:

```shell
XLA: (<tf.Tensor: shape=(), dtype=int32, numpy=2>, <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 0], dtype=int32)>)
STD: (<tf.Tensor: shape=(), dtype=int32, numpy=2>, <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 0], dtype=int32)>)
```
</details>"
60470,[Tensorflow Lite] libtask_vision_jni.so crashes on Windows Subsystem for Android,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

tflite-gpu: 2.12.0, tflite-gpu-delegate-plugin: 0.4.3, tflite-task-vision: 0.4.3

### Custom Code

Yes

### OS Platform and Distribution

Windows Subsystem for Android

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

ObjectDetector.detect crashes on Windows Subsystem for Android. Works fine on Android Emulator and physical devices.

### Standalone code to reproduce the issue

```shell
// Tried nnapi, gpu and default (cpu)

val baseOptions = BaseOptions.builder().apply {
    // useNnapi()
    if (CompatibilityList().isDelegateSupportedOnThisDevice) {
        useGpu()
    }
}.build()
val objectDetectorOptions = ObjectDetector.ObjectDetectorOptions.builder().apply {
    setBaseOptions(baseOptions)
    setScoreThreshold(0.2f)
    setMaxResults(5)
}.build()

val tensorImage = ...
val objectDetector = ObjectDetector.createFromFileAndOptions(
    model, // used EfficientDet-Lite0
    objectDetectorOptions,
)
val detections = objectDetector.detect(tensorImage)
```


### Relevant log output

```shell
Fatal signal 11 (SIGSEGV), code 128 (SI_KERNEL), fault addr 0x0 in tid 9070 (DefaultDispatch), pid 9007 (mmar.havenwalls)
23:23:26.791 DEBUG              E  failed to readlink /proc/9070/fd/167: No such file or directory
23:23:26.799 crash_dump64       I  obtaining output fd from tombstoned, type: kDebuggerdTombstoneProto
23:23:26.799 tombstoned         I  received crash request for pid 9070
23:23:26.799 crash_dump64       I  performing dump of process 9007 (target tid = 9070)
23:23:26.896 DEBUG              A  *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
23:23:26.896 DEBUG              A  Build fingerprint: 'Windows/windows_x86_64/windows_x86_64:13/TQ2A.230305.008.C1/2303.40000.5.0:user/release-keys'
23:23:26.896 DEBUG              A  Revision: '0'
23:23:26.896 DEBUG              A  ABI: 'x86_64'
23:23:26.896 DEBUG              A  Timestamp: 2023-05-02 23:23:26.802005135+0900
23:23:26.896 DEBUG              A  Process uptime: 15s
23:23:26.896 DEBUG              A  Cmdline: com.ammar.havenwalls
23:23:26.896 DEBUG              A  pid: 9007, tid: 9070, name: DefaultDispatch  >>> com.ammar.havenwalls <<<
23:23:26.896 DEBUG              A  uid: 10069
23:23:26.896 DEBUG              A  signal 11 (SIGSEGV), code 128 (SI_KERNEL), fault addr 0x0000000000000000
23:23:26.896 DEBUG              A      rax 00007f0cedffabc0  rbx 00007f0e8ce1d0e0  rcx 00007f0e8ce1d120  rdx 00007f0cedffabc0
23:23:26.896 DEBUG              A      r8  00007f0e8ce1d0e0  r9  0000000000000003  r10 0000000000000003  r11 00007f0cefd1bff0
23:23:26.896 DEBUG              A      r12 0000000000012c18  r13 00007f0cee540000  r14 0000000000000000  r15 0000000000000000
23:23:26.896 DEBUG              A      rdi 0000000000012c18  rsi 00007f0cedb08b80
23:23:26.896 DEBUG              A      rbp 00007f0cf0ed8870  rsp 00007f0cf0ed8870  rip 00007f0cee527288
23:23:26.896 DEBUG              A  backtrace:
23:23:26.896 DEBUG              A        #00 pc 0000000000495288  /data/app/~~YZA5wZCbtPPgacPRPrHayg==/com.ammar.havenwalls-A6Pw6rODYvDy-YHQy_jHew==/lib/x86_64/libtask_vision_jni.so
23:23:26.896 DEBUG              A        #01 pc 00000000004ae031  /data/app/~~YZA5wZCbtPPgacPRPrHayg==/com.ammar.havenwalls-A6Pw6rODYvDy-YHQy_jHew==/lib/x86_64/libtask_vision_jni.so
23:23:26.896 DEBUG              A        #02 pc 00000000004e719f  /data/app/~~YZA5wZCbtPPgacPRPrHayg==/com.ammar.havenwalls-A6Pw6rODYvDy-YHQy_jHew==/lib/x86_64/libtask_vision_jni.so
23:23:26.896 DEBUG              A        #03 pc 00000000004ae2b4  /data/app/~~YZA5wZCbtPPgacPRPrHayg==/com.ammar.havenwalls-A6Pw6rODYvDy-YHQy_jHew==/lib/x86_64/libtask_vision_jni.so
23:23:26.896 DEBUG              A        #04 pc 000000000049b315  /data/app/~~YZA5wZCbtPPgacPRPrHayg==/com.ammar.havenwalls-A6Pw6rODYvDy-YHQy_jHew==/lib/x86_64/libtask_vision_jni.so
23:23:26.896 DEBUG              A        #05 pc 000000000041b6a9  /data/app/~~YZA5wZCbtPPgacPRPrHayg==/com.ammar.havenwalls-A6Pw6rODYvDy-YHQy_jHew==/lib/x86_64/libtask_vision_jni.so
23:23:26.896 DEBUG              A        #06 pc 000000000053767a  /data/app/~~YZA5wZCbtPPgacPRPrHayg==/com.ammar.havenwalls-A6Pw6rODYvDy-YHQy_jHew==/lib/x86_64/libtask_vision_jni.so
23:23:26.896 DEBUG              A        #07 pc 00000000005294eb  /data/app/~~YZA5wZCbtPPgacPRPrHayg==/com.ammar.havenwalls-A6Pw6rODYvDy-YHQy_jHew==/lib/x86_64/libtask_vision_jni.so
23:23:26.896 DEBUG              A        #08 pc 00000000004fbea4  /data/app/~~YZA5wZCbtPPgacPRPrHayg==/com.ammar.havenwalls-A6Pw6rODYvDy-YHQy_jHew==/lib/x86_64/libtask_vision_jni.so
23:23:26.896 DEBUG              A        #09 pc 000000000005ddc6  /data/app/~~YZA5wZCbtPPgacPRPrHayg==/com.ammar.havenwalls-A6Pw6rODYvDy-YHQy_jHew==/lib/x86_64/libtask_vision_jni.so
23:23:26.896 DEBUG              A        #10 pc 000000000005c51e  /data/app/~~YZA5wZCbtPPgacPRPrHayg==/com.ammar.havenwalls-A6Pw6rODYvDy-YHQy_jHew==/lib/x86_64/libtask_vision_jni.so
23:23:26.896 DEBUG              A        #11 pc 0000000000042a6b  /data/app/~~YZA5wZCbtPPgacPRPrHayg==/com.ammar.havenwalls-A6Pw6rODYvDy-YHQy_jHew==/lib/x86_64/libtask_vision_jni.so (Java_org_tensorflow_lite_task_vision_detector_ObjectDetector_detectNative+59)
23:23:26.896 DEBUG              A        #12 pc 000000000037d28b  /apex/com.android.art/lib64/libart.so (art_quick_generic_jni_trampoline+219) (BuildId: a5a11e17ddeb3fe4395528da1842c05c)
23:23:26.896 DEBUG              A        #13 pc 0000000000368fd9  /apex/com.android.art/lib64/libart.so (nterp_helper+2153) (BuildId: a5a11e17ddeb3fe4395528da1842c05c)
23:23:26.896 DEBUG              A        #14 pc 000000000041d2ea  [anon:dalvik-classes18.dex extracted in memory from /data/app/~~YZA5wZCbtPPgacPRPrHayg==/com.ammar.havenwalls-A6Pw6rODYvDy-YHQy_jHew==/base.apk!classes18.dex] (org.tensorflow.lite.task.vision.detector.ObjectDetector.detect+14)
23:23:26.896 DEBUG              A        #15 pc 0000000000369608  /apex/com.android.art/lib64/libart.so (nterp_helper+3736) (BuildId: a5a11e17ddeb3fe4395528da1842c05c)
23:23:26.896 DEBUG              A        #16 pc 000000000041d25c  [anon:dalvik-classes18.dex extracted in memory from /data/app/~~YZA5wZCbtPPgacPRPrHayg==/com.ammar.havenwalls-A6Pw6rODYvDy-YHQy_jHew==/base.apk!classes18.dex] (org.tensorflow.lite.task.vision.detector.ObjectDetector.access$1200+0)
23:23:26.896 DEBUG              A        #17 pc 00000000003687a8  /apex/com.android.art/lib64/libart.so (nterp_helper+56) (BuildId: a5a11e17ddeb3fe4395528da1842c05c)
23:23:26.896 DEBUG              A        #18 pc 000000000041ce6c  [anon:dalvik-classes18.dex extracted in memory from /data/app/~~YZA5wZCbtPPgacPRPrHayg==/com.ammar.havenwalls-A6Pw6rODYvDy-YHQy_jHew==/base.apk!classes18.dex] (org.tensorflow.lite.task.vision.detector.ObjectDetector$4.run+4)
23:23:26.896 DEBUG              A        #19 pc 0000000000369dbe  /apex/com.android.art/lib64/libart.so (nterp_helper+5710) (BuildId: a5a11e17ddeb3fe4395528da1842c05c)
23:23:26.896 DEBUG              A        #20 pc 000000000041ce4c  [anon:dalvik-classes18.dex extracted in memory from /data/app/~~YZA5wZCbtPPgacPRPrHayg==/com.ammar.havenwalls-A6Pw6rODYvDy-YHQy_jHew==/base.apk!classes18.dex] (org.tensorflow.lite.task.vision.detector.ObjectDetector$4.run+0)
23:23:26.896 DEBUG              A        #21 pc 000000000036ab40  /apex/com.android.art/lib64/libart.so (nterp_helper+9168) (BuildId: a5a11e17ddeb3fe4395528da1842c05c)
23:23:26.896 DEBUG              A        #22 pc 000000000041c83e  [anon:dalvik-classes18.dex extracted in memory from /data/app/~~YZA5wZCbtPPgacPRPrHayg==/com.ammar.havenwalls-A6Pw6rODYvDy-YHQy_jHew==/base.apk!classes18.dex] (org.tensorflow.lite.task.vision.core.BaseVisionTaskApi.run+54)
23:23:26.896 DEBUG              A        #23 pc 0000000000369608  /apex/com.android.art/lib64/libart.so (nterp_helper+3736) (BuildId: a5a11e17ddeb3fe4395528da1842c05c)
23:23:26.896 DEBUG              A        #24 pc 000000000041d33a  [anon:dalvik-classes18.dex extracted in memory from /data/app/~~YZA5wZCbtPPgacPRPrHayg==/com.ammar.havenwalls-A6Pw6rODYvDy-YHQy_jHew==/base.apk!classes18.dex] (org.tensorflow.lite.task.vision.detector.ObjectDetector.detect+10)
23:23:26.896 DEBUG              A        #25 pc 0000000000369608  /apex/com.android.art/lib64/libart.so (nterp_helper+3736) (BuildId: a5a11e17ddeb3fe4395528da1842c05c)
23:23:26.896 DEBUG              A        #26 pc 000000000041d314  [anon:dalvik-classes18.dex extracted in memory from /data/app/~~YZA5wZCbtPPgacPRPrHayg==/com.ammar.havenwalls-A6Pw6rODYvDy-YHQy_jHew==/base.apk!classes18.dex] (org.tensorflow.lite.task.vision.detector.ObjectDetector.detect+16)
23:23:26.896 DEBUG              A        #27 pc 0000000000369608  /apex/com.android.art/lib64/libart.so (nterp_helper+3736) (BuildId: a5a11e17ddeb3fe4395528da1842c05c)
23:23:26.896 DEBUG              A        #28 pc 0000000000033c92  /data/data/com.ammar.havenwalls/code_cache/.overlay/base.apk/classes17.dex (com.ammar.havenwalls.ui.crop.CropViewModel$detectObjects$1$2.invokeSuspend+178)
23:23:26.896 DEBUG              A        #29 pc 000000000202ea05  /memfd:jit-cache (deleted) (kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith+309)
23:23:26.896 DEBUG              A        #30 pc 0000000002014a9c  /memfd:jit-cache (deleted) (kotlinx.coroutines.DispatchedTask.run+2012)
23:23:26.896 DEBUG              A        #31 pc 000000000036a3ca  /apex/com.android.art/lib64/libart.so (nterp_helper+7258) (BuildId: a5a11e17ddeb3fe4395528da1842c05c)
23:23:26.896 DEBUG              A        #32 pc 000000000035644a  [anon:dalvik-classes18.dex extracted in memory from /data/app/~~YZA5wZCbtPPgacPRPrHayg==/com.ammar.havenwalls-A6Pw6rODYvDy-YHQy_jHew==/base.apk!classes18.dex] (kotlinx.coroutines.internal.LimitedDispatcher.run+26)
23:23:26.896 DEBUG              A        #33 pc 000000000036a421  /apex/com.android.art/lib64/libart.so (nterp_helper+7345) (BuildId: a5a11e17ddeb3fe4395528da1842c05c)
23:23:26.896 DEBUG              A        #34 pc 000000000035defe  [anon:dalvik-classes18.dex extracted in memory from /data/app/~~YZA5wZCbtPPgacPRPrHayg==/com.ammar.havenwalls-A6Pw6rODYvDy-YHQy_jHew==/base.apk!classes18.dex] (kotlinx.coroutines.scheduling.TaskImpl.run+6)
23:23:26.896 DEBUG              A        #35 pc 0000000000369608  /apex/com.android.art/lib64/libart.so (nterp_helper+3736) (BuildId: a5a11e17ddeb3fe4395528da1842c05c)
23:23:26.896 DEBUG              A        #36 pc 000000000035d002  [anon:dalvik-classes18.dex extracted in memory from /data/app/~~YZA5wZCbtPPgacPRPrHayg==/com.ammar.havenwalls-A6Pw6rODYvDy-YHQy_jHew==/base.apk!classes18.dex] (kotlinx.coroutines.scheduling.CoroutineScheduler.runSafely+2)
23:23:26.896 DEBUG              A        #37 pc 0000000000369608  /apex/com.android.art/lib64/libart.so (nterp_helper+3736) (BuildId: a5a11e17ddeb3fe4395528da1842c05c)
23:23:26.896 DEBUG              A        #38 pc 000000000035bd0e  [anon:dalvik-classes18.dex extracted in memory from /data/app/~~YZA5wZCbtPPgacPRPrHayg==/com.ammar.havenwalls-A6Pw6rODYvDy-YHQy_jHew==/base.apk!classes18.dex] (kotlinx.coroutines.scheduling.CoroutineScheduler$Worker.executeTask+34)
23:23:26.896 DEBUG              A        #39 pc 0000000000369608  /apex/com.android.art/lib64/libart.so (nterp_helper+3736) (BuildId: a5a11e17ddeb3fe4395528da1842c05c)
23:23:26.896 DEBUG              A        #40 pc 000000000035be3c  [anon:dalvik-classes18.dex extracted in memory from /data/app/~~YZA5wZCbtPPgacPRPrHayg==/com.ammar.havenwalls-A6Pw6rODYvDy-YHQy_jHew==/base.apk!classes18.dex] (kotlinx.coroutines.scheduling.CoroutineScheduler$Worker.runWorker+56)
23:23:26.896 DEBUG              A        #41 pc 0000000000369608  /apex/com.android.art/lib64/libart.so (nterp_helper+3736) (BuildId: a5a11e17ddeb3fe4395528da1842c05c)
23:23:26.896 DEBUG              A        #42 pc 000000000035bdec  [anon:dalvik-classes18.dex extracted in memory from /data/app/~~YZA5wZCbtPPgacPRPrHayg==/com.ammar.havenwalls-A6Pw6rODYvDy-YHQy_jHew==/base.apk!classes18.dex] (kotlinx.coroutines.scheduling.CoroutineScheduler$Worker.run+0)
23:23:26.896 DEBUG              A        #43 pc 0000000000372144  /apex/com.android.art/lib64/libart.so (art_quick_invoke_stub+756) (BuildId: a5a11e17ddeb3fe4395528da1842c05c)
23:23:26.896 DEBUG              A        #44 pc 00000000003f1b46  /apex/com.android.art/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+214) (BuildId: a5a11e17ddeb3fe4395528da1842c05c)
23:23:26.896 DEBUG              A        #45 pc 00000000007e596e  /apex/com.android.art/lib64/libart.so (art::JValue art::InvokeVirtualOrInterfaceWithJValues<art::ArtMethod*>(art::ScopedObjectAccessAlreadyRunnable const&, _jobject*, art::ArtMethod*, jvalue const*)+478) (BuildId: a5a11e17ddeb3fe4395528da1842c05c)
23:23:26.896 DEBUG              A        #46 pc 000000000083a0ac  /apex/com.android.art/lib64/libart.so (art::Thread::CreateCallback(void*)+1404) (BuildId: a5a11e17ddeb3fe4395528da1842c05c)
23:23:26.896 DEBUG              A        #47 pc 00000000000cca4a  /apex/com.android.runtime/lib64/bionic/libc.so (__pthread_start(void*)+58) (BuildId: 76cb658c499bceaab85932d1254e26df)
23:23:26.896 DEBUG              A        #48 pc 00000000000609f7  /apex/com.android.runtime/lib64/bionic/libc.so (__start_thread+55) (BuildId: 76cb658c499bceaab85932d1254e26df)
23:23:26.902 tombstoned         E  Tombstone written to: tombstone_03
```
</details>"
60469,Tensorflow 2.11 Memory Issue,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary  (Lambda Stack)

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

Driver Version: 525.105.17   CUDA Version: 12.0   

### GPU model and memory

A6000    System 1TB RAM 2TB Swap

### Current Behaviour?

I have multiple models that have worked fine for months or years under version <= 2.10.

With 2.11, I get this (below).

I have (on another system) tested 2.10 vs 2.11 by switching Docker containers.  The exact same code, data, etc, works with 2.10 but fails with 2.11.


training_cycle =  0


on_train_begin lr =  1.5625000182595272e-09
Epoch 1/100

---------------------------------------------------------------------------
ResourceExhaustedError                    Traceback (most recent call last)
<ipython-input-282-89befc246e8e> in <module>
     13 
     14     if training_validation_split != None:
---> 15         history = midas_model.fit (training_dataset, epochs=num_epochs, validation_data=validation_dataset, callbacks=[tensorboard_cb, dhc_scheduler])
     16     else:
     17         history = midas_model.fit (training_dataset, epochs=num_epochs, callbacks=[tensorboard_cb, dhc_scheduler])

/usr/lib/python3/dist-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)
     68             # To get the full stack trace, call:
     69             # `tf.debugging.disable_traceback_filtering()`
---> 70             raise e.with_traceback(filtered_tb) from None
     71         finally:
     72             del filtered_tb

/usr/lib/python3/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     50   try:
     51     ctx.ensure_initialized()
---> 52     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     53                                         inputs, attrs, num_outputs)
     54   except core._NotOkStatusException as e:

ResourceExhaustedError: Graph execution error:

Detected at node 'StatefulPartitionedCall' defined at (most recent call last):
    File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code
      exec(code, run_globals)
    File ""/usr/lib/python3/dist-packages/ipykernel_launcher.py"", line 16, in <module>
      app.launch_new_instance()
    File ""/usr/lib/python3/dist-packages/traitlets/config/application.py"", line 664, in launch_instance
      app.start()
    File ""/usr/lib/python3/dist-packages/ipykernel/kernelapp.py"", line 583, in start
      self.io_loop.start()
    File ""/usr/lib/python3/dist-packages/tornado/platform/asyncio.py"", line 132, in start
      self.asyncio_loop.run_forever()
    File ""/usr/lib/python3.8/asyncio/base_events.py"", line 570, in run_forever
      self._run_once()
    File ""/usr/lib/python3.8/asyncio/base_events.py"", line 1859, in _run_once
      handle._run()
    File ""/usr/lib/python3.8/asyncio/events.py"", line 81, in _run
      self._context.run(self._callback, *self._args)
    File ""/usr/lib/python3/dist-packages/tornado/ioloop.py"", line 758, in _run_callback
      ret = callback()
    File ""/usr/lib/python3/dist-packages/tornado/stack_context.py"", line 300, in null_wrapper
      return fn(*args, **kwargs)
    File ""/usr/lib/python3/dist-packages/tornado/gen.py"", line 1248, in inner
      self.run()
    File ""/usr/lib/python3/dist-packages/tornado/gen.py"", line 1162, in run
      yielded = self.gen.send(value)
    File ""/usr/lib/python3/dist-packages/ipykernel/kernelbase.py"", line 381, in dispatch_queue
      yield self.process_one()
    File ""/usr/lib/python3/dist-packages/tornado/gen.py"", line 346, in wrapper
      runner = Runner(result, future, yielded)
    File ""/usr/lib/python3/dist-packages/tornado/gen.py"", line 1095, in __init__
      self.run()
    File ""/usr/lib/python3/dist-packages/tornado/gen.py"", line 1162, in run
      yielded = self.gen.send(value)
    File ""/usr/lib/python3/dist-packages/ipykernel/kernelbase.py"", line 365, in process_one
      yield gen.maybe_future(dispatch(*args))
    File ""/usr/lib/python3/dist-packages/tornado/gen.py"", line 326, in wrapper
      yielded = next(result)
    File ""/usr/lib/python3/dist-packages/ipykernel/kernelbase.py"", line 268, in dispatch_shell
      yield gen.maybe_future(handler(stream, idents, msg))
    File ""/usr/lib/python3/dist-packages/tornado/gen.py"", line 326, in wrapper
      yielded = next(result)
    File ""/usr/lib/python3/dist-packages/ipykernel/kernelbase.py"", line 543, in execute_request
      self.do_execute(
    File ""/usr/lib/python3/dist-packages/tornado/gen.py"", line 326, in wrapper
      yielded = next(result)
    File ""/usr/lib/python3/dist-packages/ipykernel/ipkernel.py"", line 300, in do_execute
      res = shell.run_cell(code, store_history=store_history, silent=silent)
    File ""/usr/lib/python3/dist-packages/ipykernel/zmqshell.py"", line 536, in run_cell
      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
    File ""/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py"", line 2857, in run_cell
      result = self._run_cell(
    File ""/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py"", line 2886, in _run_cell
      return runner(coro)
    File ""/usr/lib/python3/dist-packages/IPython/core/async_helpers.py"", line 68, in _pseudo_sync_runner
      coro.send(None)
    File ""/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py"", line 3062, in run_cell_async
      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
    File ""/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py"", line 3254, in run_ast_nodes
      if (await self.run_code(code, result,  async_=asy)):
    File ""/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py"", line 3331, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File ""<ipython-input-282-89befc246e8e>"", line 15, in <module>
      history = midas_model.fit (training_dataset, epochs=num_epochs, validation_data=validation_dataset, callbacks=[tensorboard_cb, dhc_scheduler])
    File ""/usr/lib/python3/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/lib/python3/dist-packages/keras/engine/training.py"", line 1650, in fit
      tmp_logs = self.train_function(iterator)
    File ""/usr/lib/python3/dist-packages/keras/engine/training.py"", line 1249, in train_function
      return step_function(self, iterator)
    File ""/usr/lib/python3/dist-packages/keras/engine/training.py"", line 1233, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/usr/lib/python3/dist-packages/keras/engine/training.py"", line 1222, in run_step
      outputs = model.train_step(data)
    File ""/usr/lib/python3/dist-packages/keras/engine/training.py"", line 1027, in train_step
      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File ""/usr/lib/python3/dist-packages/keras/optimizers/optimizer_experimental/optimizer.py"", line 527, in minimize
      self.apply_gradients(grads_and_vars)
    File ""/usr/lib/python3/dist-packages/keras/optimizers/optimizer_experimental/optimizer.py"", line 1140, in apply_gradients
      return super().apply_gradients(grads_and_vars, name=name)
    File ""/usr/lib/python3/dist-packages/keras/optimizers/optimizer_experimental/optimizer.py"", line 634, in apply_gradients
      iteration = self._internal_apply_gradients(grads_and_vars)
    File ""/usr/lib/python3/dist-packages/keras/optimizers/optimizer_experimental/optimizer.py"", line 1166, in _internal_apply_gradients
      return tf.__internal__.distribute.interim.maybe_merge_call(
    File ""/usr/lib/python3/dist-packages/keras/optimizers/optimizer_experimental/optimizer.py"", line 1216, in _distributed_apply_gradients_fn
      distribution.extended.update(
    File ""/usr/lib/python3/dist-packages/keras/optimizers/optimizer_experimental/optimizer.py"", line 1211, in apply_grad_to_update_var
      return self._update_step_xla(grad, var, id(self._var_key(var)))
Node: 'StatefulPartitionedCall'
Out of memory while trying to allocate 6558708800 bytes.
BufferAssignment OOM Debugging.
BufferAssignment stats:
             parameter allocation:   18.32GiB
              constant allocation:         0B
        maybe_live_out allocation:   12.22GiB
     preallocated temp allocation:         0B
                 total allocation:   18.32GiB
Peak buffers:
	Buffer 1:
		Size: 6.11GiB
		Operator: op_name=""XLA_Args""
		Entry Parameter Subshape: f32[443156,3700]
		==========================

	Buffer 2:
		Size: 6.11GiB
		Operator: op_name=""XLA_Args""
		Entry Parameter Subshape: f32[443156,3700]
		==========================

	Buffer 3:
		Size: 6.11GiB
		Operator: op_name=""XLA_Args""
		Entry Parameter Subshape: f32[443156,3700]
		==========================

	Buffer 4:
		Size: 16B
		Operator: op_type=""AssignAddVariableOp"" op_name=""AssignAddVariableOp"" source_file=""/usr/lib/python3/dist-packages/keras/optimizers/optimizer_experimental/sgd.py"" source_line=182
		XLA Label: fusion
		Shape: (f32[443156,3700], f32[443156,3700])
		==========================

	Buffer 5:
		Size: 4B
		Operator: op_name=""XLA_Args""
		Entry Parameter Subshape: f32[]
		==========================


	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_train_function_30187]



### Standalone code to reproduce the issue

```shell
Here is the model.  I have tried reducing the 3,700 width down to 3,400 but that doesn't help.  This exact model (3,700) works fine with <= 2.10 and has for months.


Model: ""model""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 layer_0 (InputLayer)        [(None, 443156)]          0         
                                                                 
 layer_1 (Dense)             (None, 3700)              1639680900
                                                                 
 layer_2 (Dense)             (None, 3700)              13693700  
                                                                 
 layer_3 (Dense)             (None, 3700)              13693700  
                                                                 
 layer_4 (Dense)             (None, 3700)              13693700  
                                                                 
 layer_5 (Dense)             (None, 3700)              13693700  
                                                                 
 layer_6 (Dense)             (None, 3700)              13693700  
                                                                 
 layer_7 (Dense)             (None, 3700)              13693700  
                                                                 
 layer_8 (Dense)             (None, 3700)              13693700  
                                                                 
 layer_9 (Dense)             (None, 3700)              13693700  
                                                                 
 layer_10 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_11 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_12 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_13 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_14 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_15 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_16 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_17 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_18 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_19 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_20 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_21 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_22 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_23 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_24 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_25 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_26 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_27 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_28 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_29 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_30 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_31 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_32 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_33 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_34 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_35 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_36 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_37 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_38 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_39 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_40 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_41 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_42 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_43 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_44 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_45 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_46 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_47 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_48 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_49 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_50 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_51 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_52 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_53 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_54 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_55 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_56 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_57 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_58 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_59 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_60 (Dense)            (None, 3700)              13693700  

                                                                 
 layer_61 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_62 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_63 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_64 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_65 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_66 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_67 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_68 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_69 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_70 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_71 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_72 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_73 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_74 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_75 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_76 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_77 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_78 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_79 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_80 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_81 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_82 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_83 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_84 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_85 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_86 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_87 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_88 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_89 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_90 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_91 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_92 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_93 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_94 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_95 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_96 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_97 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_98 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_99 (Dense)            (None, 3700)              13693700  
                                                                 
 layer_100 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_101 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_102 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_103 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_104 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_105 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_106 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_107 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_108 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_109 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_110 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_111 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_112 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_113 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_114 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_115 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_116 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_117 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_118 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_119 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_120 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_121 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_122 (Dense)           (None, 3700)              13693700  
                                                                 

 layer_123 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_124 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_125 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_126 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_127 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_128 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_129 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_130 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_131 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_132 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_133 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_134 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_135 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_136 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_137 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_138 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_139 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_140 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_141 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_142 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_143 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_144 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_145 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_146 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_147 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_148 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_149 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_150 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_151 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_152 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_153 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_154 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_155 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_156 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_157 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_158 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_159 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_160 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_161 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_162 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_163 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_164 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_165 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_166 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_167 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_168 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_169 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_170 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_171 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_172 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_173 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_174 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_175 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_176 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_177 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_178 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_179 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_180 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_181 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_182 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_183 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_184 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_185 (Dense)           (None, 3700)              13693700  

                                                                 
 layer_186 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_187 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_188 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_189 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_190 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_191 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_192 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_193 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_194 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_195 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_196 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_197 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_198 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_199 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_200 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_201 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_202 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_203 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_204 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_205 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_206 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_207 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_208 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_209 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_210 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_211 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_212 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_213 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_214 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_215 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_216 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_217 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_218 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_219 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_220 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_221 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_222 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_223 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_224 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_225 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_226 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_227 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_228 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_229 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_230 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_231 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_232 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_233 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_234 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_235 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_236 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_237 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_238 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_239 (Dense)           (None, 3700)              13693700  
                                                                 
 layer_240 (Dense)           (None, 3700)              13693700  
                                                                 
 output_layer (Dense)        (None, 1)                 3701      
                                                                 
=================================================================
Total params: 4,912,478,901
Trainable params: 4,912,478,901
Non-trainable params: 0
_________________________________________________________________

​


midas_model.compile(loss=loss_list, optimizer = the_optimizer)


Here are some other potential items of interest.

the_optimizer = keras.optimizers.SGD (nesterov=True, momentum=0.9, learning_rate=initial_learning_rate)

Loss:  mae = tf.keras.losses.MeanAbsoluteError()
```


### Relevant log output

_No response_</details>"
60468,Graph/memory corruption involving custom_gradient of functions involving pow,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.9.1, 2.11, presumably more

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18.04, 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I have implemented `custom_gradient`s for `tf.function`s that smooth various Nth roots to avoid the singularity at `x=0`. I expect the value computed by the custom gradient to be used whenever that function is differentiated, and not substituted for any other value.

**HOWEVER** when the following conditions hold:
- input is a complex dtype
- the `GradientTape` is persistent
- the `tf.function` being differentiated invokes `tf.pow` directly (or indirectly through `**` operator)
- the argument is `0`

Then the custom gradient is invoked and the proper value is computed, but then discarded somewhere between returning from my custom gradient and returning from the gradient tape's internal computation.

Note: this problem does not appear when wrapping `sqrt()` as that has its own gradient implementation that presumably avoids `pow()`.

This seems likely related to numerical issues highlighted in tensorflow/tfjs#346, PLUS, some kind of graph/memory corruption, although why/how that would be conditional on the argument is beyond me.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf


@tf.function
@tf.custom_gradient
def cbrt(p):
  third = 1. / 3.
  real_part_nonnegative = tf.math.real(p) >= 0
  signs = tf.cast(tf.where(real_part_nonnegative, 1.0, -1.0), p.dtype)
  p = signs * p
  unsigned_root = p ** third

  @tf.function
  def grad(upstream):
    unsigned_root_plus_eps = unsigned_root + 1e-4
    denom = 3 * unsigned_root_plus_eps * unsigned_root_plus_eps + 1e-4
    droot_dp = (1. / denom)
    grad_root = droot_dp * upstream
    tf.print(""custom gradient:"", tf.math.real(grad_root), tf.math.imag(grad_root))
    return grad_root

  root = signs * unsigned_root
  return root, grad
  

if __name__ == ""__main__"":
  tf.config.run_functions_eagerly(True)
  x = tf.Variable([-1e-2, -1e-10, 0, 1e-10, 1e-2], dtype=tf.complex64)
  with tf.GradientTape(persistent=True) as tape:
    cbrt_x = cbrt(x)
  gradients = tape.gradient(cbrt_x, x)
  tf.print(""all good:"", tf.math.real(gradients), tf.math.imag(gradients))

  tf.config.run_functions_eagerly(False)
  with tf.GradientTape() as tape:
    cbrt_x = cbrt(x)
  gradients = tape.gradient(cbrt_x, x)
  tf.print(""all good:"", tf.math.real(gradients), tf.math.imag(gradients))

  @tf.function
  def now_again_in_graph_mode(y):
    with tf.GradientTape(persistent=True) as tape:
      cbrt_y = cbrt(y)
    return tape.gradient(cbrt_y, y)
  gradients = now_again_in_graph_mode(x)
  tf.print(""chaos reigns:"", tf.math.real(gradients), tf.math.imag(gradients))
```


### Relevant log output

```shell
custom gradient: [7.16964388 9905.4209 9997.00098 9905.4209 7.16964388] [0 0 0 0 0]
all good: [7.16964388 9905.4209 9997.00098 9905.4209 7.16964388] [0 0 0 0 0]
custom gradient: [7.16964388 9905.4209 9997.00098 9905.4209 7.16964388] [0 0 0 0 0]
all good: [7.16964388 9905.4209 9997.00098 9905.4209 7.16964388] [0 0 0 0 0]
custom gradient: [7.16964388 9905.4209 9997.00098 9905.4209 7.16964388] [0 0 0 0 0]
chaos reigns: [7.16964388 9905.4209 -nan 9905.4209 7.16964388] [0 0 -nan 0 0]
```
</details>"
60467,Is flag attribute available in TF-2.12.0?,"Hi,
I am working with the Bert training model, I faced the following error:
```
$ python3 create_pretraining_data.py --input_file=results/part-00000-of-00500   --output_file=tfrecord/part-00000-of-00500 --vocab_file=wiki/vocab.txt    --do_lower_case=True    --max_seq_length=512    --max_predictions_per_seq=76    --masked_lm_prob=0.15    --random_seed=12345    --dupe_factor=10
Traceback (most recent call last):
  File ""create_pretraining_data.py"", line 9, in <module>
    import tokenization
  File ""/disk1/training/language_model/tensorflow/bert/cleanup_scripts/tokenization.py"", line 13, in <module>
    import tensorflow.compat.v1 as tf
ModuleNotFoundError: No module named 'tensorflow'
```
Then I searched for available TF versions and installed the latest one:
```
$ pip index versions tensorflow
Available versions: 2.12.0, 2.11.1, 2.11.0, 2.10.1, 2.10.0, 2.9.3, 2.9.2, 2.9.1, 2.9.0, 2.8.4, 2.8.3, 2.8.2, 2.8.1, 2.8.0, 2.7.4, 2.7.3, 2.7.2, 2.7.1, 2.7.0, 2.6.5, 2.6.4, 2.6.3, 2.6.2, 2.6.1, 2.6.0, 2.5.3, 2.5.2, 2.5.1, 2.5.0, 2.4.4, 2.4.3, 2.4.2, 2.4.1, 2.4.0, 2.3.4, 2.3.3, 2.3.2, 2.3.1, 2.3.0, 2.2.3, 2.2.2, 2.2.1, 2.2.0
$ pip install --user tensorflow
Collecting tensorflow
  Downloading tensorflow-2.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)
...
Successfully installed absl-py-1.4.0 flatbuffers-23.3.3 google-auth-2.17.3 google-auth-oauthlib-1.0.0 grpcio-1.54.0 jax-0.4.8 keras-2.12.0 libclang-16.0.0 ml-dtypes-0.1.0 numpy-1.23.5 protobuf-4.22.3 scipy-1.10.1 tensorboard-2.12.2 tensorboard-data-server-0.7.0 tensorboard-plugin-wit-1.8.1 tensorflow-2.12.0 tensorflow-estimator-2.12.0 tensorflow-io-gcs-filesystem-0.32.0
```

I reran the command again and now see this error

```
$ python3 create_pretraining_data.py --input_file=results/part-00000-of-00500   --output_file=tfrecord/part-00000-of-00500 --vocab_file=wiki/vocab.txt    --do_lower_case=True    --max_seq_length=512    --max_predictions_per_seq=76    --masked_lm_prob=0.15    --random_seed=12345    --dupe_factor=10
2023-05-02 12:13:34.171692: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-02 12:13:34.670262: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Traceback (most recent call last):
  File ""create_pretraining_data.py"", line 12, in <module>
    flags = tf.flags
AttributeError: module 'tensorflow' has no attribute 'flags'
```


I am confused with the tensorflow versions and based on my searches, the `flag` is not available in TF-2.12.0. I am not sure though. Any idea about that?"
60466,Same isssue cannnot install !pip install -q tflite-model-maker,"              Same isssue cannnot install !pip install -q tflite-model-maker

_Originally posted by @UsamaHameed1 in https://github.com/tensorflow/tensorflow/issues/57086#issuecomment-1529139314_
            "
60465,About the Tensorflow C++ API documentation,The lack of documentation for the Tensorflow C++ API makes it difficult to build and use. Can you write how to use and build in README.md?
60464,Cannot load weights from h5 file ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Windows 10 22H2

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Hello, 

Thank you for all your hard work on tensorflow I was hoping you could help me with this issue I've been facing.
I'm having an issue getting the h5py package to import properly when I have gdal and tensorflow installed in the same conda environment.

 I have the latest version of  gdal  3.6.4 and tensorflow 2.12.0 installed in an anaconda environment and when I attempt to load model weights from an .h5 file it fails claiming that h5py is not installed in the environment. The thing is h5py is installed in the environment ( I have the outputs of conda list below), but both gdal and tensorflow both use it. I believe the issue is from a [recent update to gdal which places a lock on the hdf5 library at build time](https://github.com/OSGeo/gdal/pull/7357) and I was wondering if I could modify how tensorflow imports h5py to get around this problem?

Thank you for any help you can provide.

## Anaconda Environment to reproduce the issue
``` 
conda create -n test_env python=3.10 
conda activate test_env
conda install -c conda-forge gdal==3.6.4
pip install tensorflow==2.12.0
python test_script.py
```
## Outputs of conda list
```
# Name                    Version                   Build  Channel
absl-py                   1.4.0                    pypi_0    pypi
astunparse                1.6.3                    pypi_0    pypi
blosc                     1.21.3               h6c2663c_0
boost-cpp                 1.74.0               h5b4e17d_6    conda-forge
bzip2                     1.0.8                he774522_0
ca-certificates           2022.12.7            h5b45459_0    conda-forge
cachetools                5.3.0                    pypi_0    pypi
cairo                     1.16.0            h63a05c6_1001    conda-forge
certifi                   2022.12.7                pypi_0    pypi
cfitsio                   3.470                h2bbff1b_7
charset-normalizer        3.1.0                    pypi_0    pypi
curl                      7.88.1               h2bbff1b_0
expat                     2.5.0                h63175ca_1    conda-forge
flatbuffers               23.3.3                   pypi_0    pypi
fontconfig                2.14.1               h9c4af85_2
freetype                  2.10.4               h546665d_1    conda-forge
freexl                    1.0.6                ha8e266a_0    conda-forge
gast                      0.4.0                    pypi_0    pypi
gdal                      3.6.2           py310h1c2bfe4_1
geos                      3.8.0                h33f27b4_0
geotiff                   1.7.0                h4545760_1
glib                      2.69.1               h5dc1a3c_2
google-auth               2.17.3                   pypi_0    pypi
google-auth-oauthlib      1.0.0                    pypi_0    pypi
google-pasta              0.2.0                    pypi_0    pypi
grpcio                    1.54.0                   pypi_0    pypi
h5py                      3.8.0                    pypi_0    pypi
hdf4                      4.2.13               h712560f_2
hdf5                      1.10.6          nompi_h5268f04_1114    conda-forge
icu                       58.2                 ha925a31_3
idna                      3.4                      pypi_0    pypi
intel-openmp              2023.1.0         h57928b3_46319    conda-forge
jax                       0.4.8                    pypi_0    pypi
jpeg                      9e                   h8ffe710_2    conda-forge
kealib                    1.5.0                hde4a422_0
keras                     2.12.0                   pypi_0    pypi
krb5                      1.19.3               h1176d77_0    conda-forge
lcms2                     2.12                 h83e58a3_0
lerc                      3.0                  hd77b12b_0
libblas                   3.9.0                     8_mkl    conda-forge
libcblas                  3.9.0                     8_mkl    conda-forge
libclang                  16.0.0                   pypi_0    pypi
libcurl                   7.88.1               h86230a5_0
libdeflate                1.17                 h2bbff1b_0
libexpat                  2.5.0                h63175ca_1    conda-forge
libffi                    3.4.2                hd77b12b_6
libgdal                   3.6.2                h11d7215_1
libiconv                  1.17                 h8ffe710_0    conda-forge
libkml                    1.3.0             h9859afa_1014    conda-forge
liblapack                 3.9.0                     8_mkl    conda-forge
libnetcdf                 4.8.1                h6685c40_2
libpng                    1.6.39               h8cc25b3_0
libpq                     12.9                 hb652d5d_3
libspatialite             4.3.0a              h6ec8781_23
libssh2                   1.10.0               h680486a_2    conda-forge
libtiff                   4.5.0                h6c2663c_2
libwebp-base              1.2.4                h2bbff1b_1
libxml2                   2.10.3               h0ad7f3c_0
libzip                    1.8.0                h49b8836_0
lz4-c                     1.9.4                h2bbff1b_0
markdown                  3.4.3                    pypi_0    pypi
markupsafe                2.1.2                    pypi_0    pypi
mkl                       2020.4             hb70f87d_311    conda-forge
ml-dtypes                 0.1.0                    pypi_0    pypi
numpy                     1.22.3          py310hed7ac4c_2    conda-forge
oauthlib                  3.2.2                    pypi_0    pypi
openjpeg                  2.4.0                h4fc8c34_0
openssl                   1.1.1t               h2bbff1b_0
opt-einsum                3.3.0                    pypi_0    pypi
packaging                 23.1                     pypi_0    pypi
pcre                      8.45                 h0e60522_0    conda-forge
pcre2                     10.37                h0ff8eda_1
pip                       23.0.1          py310haa95532_0
pixman                    0.38.0            hfa6e2cd_1003    conda-forge
poppler                   22.12.0              h268424c_0
poppler-data              0.4.12               hd8ed1ab_0    conda-forge
postgresql                12.9                 hb652d5d_3
proj                      6.2.1                h3758d61_0
protobuf                  4.22.3                   pypi_0    pypi
pyasn1                    0.5.0                    pypi_0    pypi
pyasn1-modules            0.3.0                    pypi_0    pypi
python                    3.10.11              h966fe2a_2
python_abi                3.10                    2_cp310    conda-forge
qhull                     2020.2               h70d2c02_2    conda-forge
requests                  2.29.0                   pypi_0    pypi
requests-oauthlib         1.3.1                    pypi_0    pypi
rsa                       4.9                      pypi_0    pypi
scipy                     1.10.1                   pypi_0    pypi
setuptools                66.0.0          py310haa95532_0
six                       1.16.0                   pypi_0    pypi
sqlite                    3.41.2               h2bbff1b_0
tensorboard               2.12.2                   pypi_0    pypi
tensorboard-data-server   0.7.0                    pypi_0    pypi
tensorboard-plugin-wit    1.8.1                    pypi_0    pypi
tensorflow                2.12.0                   pypi_0    pypi
tensorflow-estimator      2.12.0                   pypi_0    pypi
tensorflow-intel          2.12.0                   pypi_0    pypi
tensorflow-io-gcs-filesystem 0.31.0                   pypi_0    pypi
termcolor                 2.3.0                    pypi_0    pypi
tiledb                    2.3.3                h3649cd2_2
tk                        8.6.12               h2bbff1b_0
typing-extensions         4.5.0                    pypi_0    pypi
tzdata                    2023c                h04d1e81_0
urllib3                   1.26.15                  pypi_0    pypi
vc                        14.2                 h21ff451_1
vs2015_runtime            14.27.29016          h5e58377_2
werkzeug                  2.3.3                    pypi_0    pypi
wheel                     0.38.4          py310haa95532_0
wrapt                     1.14.1                   pypi_0    pypi
xerces-c                  3.2.4                hd77b12b_0
xz                        5.2.10               h8cc25b3_1
zlib                      1.2.13               h8cc25b3_0
zstd                      1.5.5                hd43e919_0
```

### Standalone code to reproduce the issue

```shell
from osgeo import gdal

import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

# Create a simple model
model = Sequential([
    Dense(64, activation='relu', input_shape=(32,)),
    Dense(10)
])

# Compile the model
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# Print the model summary
model.summary()

# Save the initial weights to a file
model.save_weights('my_model_weights.h5')

# Change the weights of the model
for layer in model.layers:
    weights = layer.get_weights()  # list of numpy arrays
    weights = [weight * 0 for weight in weights]
    layer.set_weights(weights)

# Check that weights have been changed
print('Weights after resetting:')
print(model.layers[0].get_weights())

# Load the initial weights from the file
model.load_weights('my_model_weights.h5')

# Check that weights have been loaded
print('Weights after loading from file:')
print(model.layers[0].get_weights())
```


### Relevant log output

```shell
(test_env) > python sample_model.py
Model: ""sequential""
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 dense (Dense)               (None, 64)                2112

 dense_1 (Dense)             (None, 10)                650

=================================================================
Total params: 2,762
Trainable params: 2,762
Non-trainable params: 0
_________________________________________________________________
Traceback (most recent call last):
  File ""C:\1_USGS\1_CoastSeg\1_official_CoastSeg_repo\CoastSeg\src\coastseg\sample_model.py"", line 22, in <module>
    model.save_weights('my_model_weights.h5')
  File ""C:\Users\Sharon\anaconda3\envs\test_env\lib\site-packages\keras\utils\traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""C:\Users\Sharon\anaconda3\envs\test_env\lib\site-packages\keras\saving\legacy\save.py"", line 351, in save_weights
    raise ImportError(
ImportError: `save_weights` requires h5py when saving in hdf5, but h5py is not available. Try installing h5py package.
```
</details>"
60462,Inference significantly slower for Lenet5 models on both CPU and GPU when compared to other frameworks,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Custom
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
-   **TensorFlow installed from (source or binary)**: binary
-   **TensorFlow version**: 2.5
-   **Python version**: 3.7.5
-   **GPU model and memory**:  NVIDIA Quadro RTX 4000 GPU with 8GB of VRAM
-  **CPU model**: Intel(R) Core(TM) i9-9900K CPU operating at 3.60GHz 
- **cuDNN version**: 7.6.5

### Describe the problem
Hello everybody.

I’ve been experimenting with different models and different frameworks, and I’ve noticed that  inference with a Lenet5 model on the MNIST dataset is significantly slower on Keras (28% slower on CPU and 108% slower on GPU) compared to the PyTorch and TensorFlow v1.X implementations. Note that I artificially increased the size of the testing set of the MNIST dataset by duplicating the set by 100 times for a total of 1 000 000 samples. I've done so so any difference in inference time may be clearer. Also note that this is observed even when the amount of memory Keras can reserve is limited (i.e., `tf.config.experimental.set_memory_growth(gpu, True)`)

Here are boxplots that showcase the performance of numerous Lenet5 models:

![infer time](https://user-images.githubusercontent.com/132307143/235554144-86b4e9e2-56c1-4029-b84a-0c838176444c.png)


Any ideas on what may be causing this?
"
60460,Protocol Buffer error - critical I cannot import tensorflow,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Bug occurred today when reloading a jupyter notebook.

I previously tried to install cuML
https://docs.rapids.ai/install#pip-install

but the trace error seems unrelated to it, but mention `Protocol Buffers`:



### Standalone code to reproduce the issue

```shell
The error occurs when : 
`import tensorflow`

and consequently any other import of libraries based on tf, such as:
`import umap`
will fail.

I cannot use TF anymore.
```


### Relevant log output

```shell
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[33], line 1
----> 1 import tensorflow as tf
      2 #import umap

File /data0/home/h21/luas6629/venv/lib/python3.10/site-packages/tensorflow/__init__.py:37
     34 import sys as _sys
     35 import typing as _typing
---> 37 from tensorflow.python.tools import module_util as _module_util
     38 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     40 # Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.

File /data0/home/h21/luas6629/venv/lib/python3.10/site-packages/tensorflow/python/__init__.py:37
     29 # We aim to keep this file minimal and ideally remove completely.
     30 # If you are adding a new file with @tf_export decorators,
     31 # import it in modules_with_exports.py instead.
     32 
     33 # go/tf-wildcard-import
     34 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top
     36 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
---> 37 from tensorflow.python.eager import context
     39 # pylint: enable=wildcard-import
     40 
     41 # Bring in subpackages.
     42 from tensorflow.python import data

File /data0/home/h21/luas6629/venv/lib/python3.10/site-packages/tensorflow/python/eager/context.py:28
     25 from absl import logging
     26 import numpy as np
---> 28 from tensorflow.core.framework import function_pb2
     29 from tensorflow.core.protobuf import config_pb2
     30 from tensorflow.core.protobuf import coordination_config_pb2

File /data0/home/h21/luas6629/venv/lib/python3.10/site-packages/tensorflow/core/framework/function_pb2.py:16
     11 # @@protoc_insertion_point(imports)
     13 _sym_db = _symbol_database.Default()
---> 16 from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2
     17 from tensorflow.core.framework import node_def_pb2 as tensorflow_dot_core_dot_framework_dot_node__def__pb2
     18 from tensorflow.core.framework import op_def_pb2 as tensorflow_dot_core_dot_framework_dot_op__def__pb2

File /data0/home/h21/luas6629/venv/lib/python3.10/site-packages/tensorflow/core/framework/attr_value_pb2.py:16
     11 # @@protoc_insertion_point(imports)
     13 _sym_db = _symbol_database.Default()
---> 16 from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2
     17 from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2
     18 from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2

File /data0/home/h21/luas6629/venv/lib/python3.10/site-packages/tensorflow/core/framework/tensor_pb2.py:16
     11 # @@protoc_insertion_point(imports)
     13 _sym_db = _symbol_database.Default()
---> 16 from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2
     17 from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2
     18 from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2

File /data0/home/h21/luas6629/venv/lib/python3.10/site-packages/tensorflow/core/framework/resource_handle_pb2.py:16
     11 # @@protoc_insertion_point(imports)
     13 _sym_db = _symbol_database.Default()
---> 16 from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2
     17 from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2
     20 DESCRIPTOR = _descriptor.FileDescriptor(
     21   name='tensorflow/core/framework/resource_handle.proto',
     22   package='tensorflow',
   (...)
     26   ,
     27   dependencies=[tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_types__pb2.DESCRIPTOR,])

File /data0/home/h21/luas6629/venv/lib/python3.10/site-packages/tensorflow/core/framework/tensor_shape_pb2.py:36
     13 _sym_db = _symbol_database.Default()
     18 DESCRIPTOR = _descriptor.FileDescriptor(
     19   name='tensorflow/core/framework/tensor_shape.proto',
     20   package='tensorflow',
   (...)
     23   serialized_pb=_b('\n,tensorflow/core/framework/tensor_shape.proto\x12\ntensorflow\""z\n\x10TensorShapeProto\x12-\n\x03\x64im\x18\x02 \x03(\x0b\x32 .tensorflow.TensorShapeProto.Dim\x12\x14\n\x0cunknown_rank\x18\x03 \x01(\x08\x1a!\n\x03\x44im\x12\x0c\n\x04size\x18\x01 \x01(\x03\x12\x0c\n\x04name\x18\x02 \x01(\tB\x87\x01\n\x18org.tensorflow.frameworkB\x11TensorShapeProtosP\x01ZSgithub.com/tensorflow/tensorflow/tensorflow/go/core/framework/tensor_shape_go_proto\xf8\x01\x01\x62\x06proto3')
     24 )
     29 _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(
     30   name='Dim',
     31   full_name='tensorflow.TensorShapeProto.Dim',
     32   filename=None,
     33   file=DESCRIPTOR,
     34   containing_type=None,
     35   fields=[
---> 36     _descriptor.FieldDescriptor(
     37       name='size', full_name='tensorflow.TensorShapeProto.Dim.size', index=0,
     38       number=1, type=3, cpp_type=2, label=1,
     39       has_default_value=False, default_value=0,
     40       message_type=None, enum_type=None, containing_type=None,
     41       is_extension=False, extension_scope=None,
     42       serialized_options=None, file=DESCRIPTOR),
     43     _descriptor.FieldDescriptor(
     44       name='name', full_name='tensorflow.TensorShapeProto.Dim.name', index=1,
     45       number=2, type=9, cpp_type=9, label=1,
     46       has_default_value=False, default_value=_b("""").decode('utf-8'),
     47       message_type=None, enum_type=None, containing_type=None,
     48       is_extension=False, extension_scope=None,
     49       serialized_options=None, file=DESCRIPTOR),
     50   ],
     51   extensions=[
     52   ],
     53   nested_types=[],
     54   enum_types=[
     55   ],
     56   serialized_options=None,
     57   is_extendable=False,
     58   syntax='proto3',
     59   extension_ranges=[],
     60   oneofs=[
     61   ],
     62   serialized_start=149,
     63   serialized_end=182,
     64 )
     66 _TENSORSHAPEPROTO = _descriptor.Descriptor(
     67   name='TensorShapeProto',
     68   full_name='tensorflow.TensorShapeProto',
   (...)
    100   serialized_end=182,
    101 )
    103 _TENSORSHAPEPROTO_DIM.containing_type = _TENSORSHAPEPROTO

File /data0/home/h21/luas6629/venv/lib/python3.10/site-packages/google/protobuf/descriptor.py:560, in FieldDescriptor.__new__(cls, name, full_name, index, number, type, cpp_type, label, default_value, message_type, enum_type, containing_type, is_extension, extension_scope, options, serialized_options, has_default_value, containing_oneof, json_name, file, create_key)
    554 def __new__(cls, name, full_name, index, number, type, cpp_type, label,
    555             default_value, message_type, enum_type, containing_type,
    556             is_extension, extension_scope, options=None,
    557             serialized_options=None,
    558             has_default_value=True, containing_oneof=None, json_name=None,
    559             file=None, create_key=None):  # pylint: disable=redefined-builtin
--> 560   _message.Message._CheckCalledFromGeneratedFile()
    561   if is_extension:
    562     return _message.default_pool.FindExtensionByName(full_name)

TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
```
</details>"
60459,Docs problem. Notebook not found,"Hello, I am new to tensoflow and right now I am reading your gides, particularly guide about tf.function. There you have a link to guide for ""eager execution"", but I don't have access to it. Is it my problem or you have broken link?

Guide for tf.function: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/function.ipynb#scrollTo=J122XQYG7W6w

Broken link, which is in the first block of code: 
""In TensorFlow 2, [eager execution](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/eager.ipynb) is turned on by default.""

https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/eager.ipynb

Upd.
Also there is broken link here
https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/function.ipynb#scrollTo=ocxX-HVk7P2o

(See the [Transformer](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/transformer.ipynb) and [Deep Dream](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/deepdream.ipynb) tutorials for example).

first link does not work

Upd. 2
Another broken link
https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/function.ipynb#scrollTo=JeD2U-yrbfVb

To learn more, see the [tf.data: Build TensorFlow input pipelines](https://github.com/tensorflow/docs/blob/master/site/guide/data) guide."
60458,AutoGraph error when function is inside a match ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.13.0-dev20230501

### Custom Code

No

### OS Platform and Distribution

Google Colab

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When a `tf.data.Dataset.map` function is defined inside a `match` scope and used by `tf.data.Dataset.apply` the AutoGraph fails. This should not happen, as the match statement should be irrelevant to the function definition. I suspect this error happens in more AutoGraph cases than just `tf.data.Dataset.map` via `tf.data.Dataset.apply`.

The issue is reproduced on Google Colab with both nightly (2.13.0-dev20230501) and 2.12.

### Standalone code to reproduce the issue

See https://colab.research.google.com/drive/1x07aUgISrYQohsG7FYmc70LeYjVcGixG#scrollTo=auSxkDSinh3R

```python
import tensorflow as tf
parameter = 'a'

match parameter:
    case 'a':
        def fn(ds):
            return ds.map(lambda x: x + 1)

dataset = tf.data.Dataset.range(1) \
    .apply(fn)

for x in dataset:
    print(x)
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py"", line 427, in converted_call
    converted_f = _convert_actual(target_entity, program_ctx)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py"", line 269, in _convert_actual
    transformed, module, source_map = _TRANSPILER.transform(entity, program_ctx)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/pyct/transpiler.py"", line 282, in transform
    return self.transform_function(obj, user_context)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/pyct/transpiler.py"", line 466, in transform_function
    nodes, ctx = super(PyToPy, self).transform_function(fn, user_context)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/pyct/transpiler.py"", line 342, in transform_function
    node, source = parser.parse_entity(fn, future_features=future_features)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/pyct/parser.py"", line 145, in parse_entity
    return _parse_lambda(entity)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/pyct/parser.py"", line 275, in _parse_lambda
    lambda_nodes.extend(
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/pyct/parser.py"", line 275, in <genexpr>
    lambda_nodes.extend(
  File ""/usr/lib/python3.10/ast.py"", line 390, in walk
    todo.extend(iter_child_nodes(node))
  File ""/usr/lib/python3.10/ast.py"", line 272, in iter_child_nodes
    for name, field in iter_fields(node):
  File ""/usr/lib/python3.10/ast.py"", line 260, in iter_fields
    for field in node._fields:
AttributeError: 'NoneType' object has no attribute '_fields'
WARNING:tensorflow:AutoGraph could not transform <function fn.<locals>.<lambda> at 0x7f9fc56e9870> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: 'NoneType' object has no attribute '_fields'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2.13.0-dev20230501
Converted call: <function fn.<locals>.<lambda> at 0x7f9fc56e9870>
    args: (<tf.Tensor 'args_0:0' shape=() dtype=int64>,)
    kwargs: {}

Not allowed: <method-wrapper '__call__' of function object at 0x7f9fc56e9870>: default rule
Not allowed: <function fn.<locals>.<lambda> at 0x7f9fc56e9870>: default rule
<function fn.<locals>.<lambda> at 0x7f9fc56e9870> is not cached for subkey ConversionOptions[{}]
Error transforming entity <function fn.<locals>.<lambda> at 0x7f9fc56e9870>
WARNING: AutoGraph could not transform <function fn.<locals>.<lambda> at 0x7f9fc56e9870> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: 'NoneType' object has no attribute '_fields'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
tf.Tensor(1, shape=(), dtype=int64)
```
</details>"
60457,error message of tf.eye is inconsistent with doc,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.12.0

### Custom Code

Yes

### OS Platform and Distribution

win11

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

According to [doc](https://tensorflow.google.cn/api_docs/python/tf/eye), the param `num_rows` should be `Non-negative int32 scalar Tensor`. But below snippet code 1 indicates that the param `num_rows` cannot be zero which is inconsistent with doc. On the other hand, the param `num_rows` shouldnt be Bool Tensor, but when given bool tensor, `tf.eye` works, as below snippet code 2 shows.

### Standalone code to reproduce the issue

```shell
snippet code 1:

import tensorflow as tf
results={}
try:
  num_rows = ""1""
  results[""res""] = tf.eye(num_rows=num_rows)
except Exception as e:
  results[""err""] = ""Error:""+str(e)
print(results)
# results = Error:Arguments `num_rows` and `num_columns` must be positive integer values. Received: num_rows=1, num_columns=1
```

snippet code 2:
```
import tensorflow as tf
results={}
try:
  num_rows = True
  results[""res""] = tf.eye(num_rows=num_rows,)
except Exception as e:
  results[""err""] = ""Error:""+str(e)
print(results)
# results = {'res': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.]], dtype=float32)>}
```
```


### Relevant log output

_No response_</details>"
60456,creating float16 tensors from scalars is not giving the correct results,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.12, 2.13.0-dev20230501

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.8.10, 3.10.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When creating a tensor from a scalar and setting the dtype to `float16`, the tensor is created with the wrong contents
the bug starts appearing with numbers higher than 2048 up to the upper limit of `float16`.

### Standalone code to reproduce the issue

```shell
print(tf.__version__)
>>> 2.13.0-dev20230501
print(tf.convert_to_tensor(6552.,dtype=""float16""))
>>> tf.Tensor(6550.0, shape=(), dtype=float16)
```


### Relevant log output

_No response_</details>"
60455,Unable to download tflite-model-maker,"I'm experiencing difficulties when attempting to pip install tflite-model-maker in Google Colab with Python 3.10. I encounter one of the following three errors:

ERROR: Could not find a version that satisfies the requirement tflite-support>=0.4.2 (from tflite-model-maker) (from versions: 0.1.0a0.dev3, 0.1.0a0.dev4, 0.1.0a0.dev5, 0.1.0a0, 0.1.0a1)
ERROR: No matching distribution found for tflite-support>=0.4.2 (from tflite-model-maker)

ERROR: Unable to find version of scann==1.2.6

ERROR: Unable to find version of numba==0.53

I've also attempted to install tflite-model-maker-nightly and also cloning the source code from GitHub, but these methods have not resolved the issue.

If anyone could provide assistance or suggestions on how to successfully install tflite-model-maker, I would greatly appreciate it."
60453,How to download outdated versions of Tensorflow in Google Colab?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.5

### Custom Code

Yes

### OS Platform and Distribution

Colab

### Mobile device

_No response_

### Python version

3.11

### Bazel version

3.7.2

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I have a project using Mask RCNN, and the project only works with Tensorflow 2.5, but this version is outdated and the pip doesn't support this version anymore, when I'm trying to build on colab using the ""Build from Source"" documentation, 

When I try to use bazel I get an error while trying to build it

````
!bazel build --config=cuda //tensorflow/tools/pip_package:build_pip_package
````

Also when I try to use pip

````
!pip install git+https://github.com/tensorflow/tensorflow.git@r2.5
````

I get the following error
````
Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Collecting git+https://github.com/tensorflow/tensorflow.git@r2.5
  Cloning https://github.com/tensorflow/tensorflow.git (to revision r2.5) to /tmp/pip-req-build-f6mzej4m
  Running command git clone --filter=blob:none --quiet https://github.com/tensorflow/tensorflow.git /tmp/pip-req-build-f6mzej4m
  Running command git checkout -b r2.5 --track origin/r2.5
  Switched to a new branch 'r2.5'
  Branch 'r2.5' set up to track remote branch 'r2.5' from 'origin'.
  Resolved https://github.com/tensorflow/tensorflow.git to commit 959e9b2a0c06df945f9fb66bd367af8832ca0d28
ERROR: git+https://github.com/tensorflow/tensorflow.git@r2.5 does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.
````

### Standalone code to reproduce the issue

```shell
You can reproduce it by using this colab

https://colab.research.google.com/github/Huxwell/caffe-tf-torch-darknet-onnx/blob/main/tensorflow_from_source.ipynb
```


### Relevant log output

_No response_</details>"
60451,native windows is not supported,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

win 11 pro

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2/8.1

### GPU model and memory

laptop4060

### Current Behaviour?

使用conda安装完了tf==2.10.0后，可以使用GPU，用conda新建环境安装pytorch后，tensorflow环境下的GPU就不能使用了，请问怎么解决？

### Standalone code to reproduce the issue

```shell
(tensorflow) C:\Windows\System32>python -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""
[]
```


### Relevant log output

_No response_</details>"
60450,tf.compat.v1.nn.conv2d fails with assertion error: AttributeError: 'list' object has no attribute 'startswith',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Linux/Mac

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I was trying to use tf.compat.v1.nn.conv2d because of the compatibility issue in my project. However, tf.compat.v1.nn.conv2d fails with the following error message: 

```
AttributeError                            Traceback (most recent call last)
[<ipython-input-1-bc689394357a>](https://sjq4g20qeq-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230427-060111-RC00_527538942#) in <cell line: 4>()
      2 input = tf.Variable(tf.random_normal([10, 28, 28, 3]))
      3 filters = tf.Variable(tf.random_normal([5, 5, 3, 32]))
----> 4 tf.compat.v1.nn.conv2d(input, filters, [1,1,1,1], ""SAME"", ""NHWC"", [1,1,1,1])

1 frames
[/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/nn_ops.py](https://sjq4g20qeq-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20230427-060111-RC00_527538942#) in conv2d(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name, filters)
   2442   if data_format is None:
   2443     data_format = ""NHWC""
-> 2444   channel_index = 1 if data_format.startswith(""NC"") else 3
   2445 
   2446   strides = _get_sequence(strides, 2, channel_index, ""strides"")

AttributeError: 'list' object has no attribute 'startswith'
```

Looks like the argument `data_format` is a list instead of a string inside this API.

### Standalone code to reproduce the issue

```shell
import tensorflow.compat.v1 as tf
input = tf.Variable(tf.random_normal([10, 28, 28, 3]))
filters = tf.Variable(tf.random_normal([5, 5, 3, 32]))
tf.compat.v1.nn.conv2d(input, filters, [1,1,1,1], ""SAME"", ""NHWC"", [1,1,1,1])
```


### Relevant log output

_No response_</details>"
60449,error message of tf.constant_initializer is not accuracy,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.12.0

### Custom Code

Yes

### OS Platform and Distribution

win11

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When run below snippet code, it will throw an exception. It's correct to throw an exception, but I think the error message is not so accurate. The following error message is more for developers to see than to tell the user where the input parameter is wrong.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
results={}
try:
  value_0_tensor = tf.random.uniform([], dtype=tf.float32)
  value_0 = tf.identity(value_0_tensor)
  value = [value_0]
  arg_class = tf.constant_initializer(value=value,)
  arg_input_0 = 3
  arg_input_1 = 1
  arg_input = [arg_input_0,arg_input_1,]
  results[""res""] = arg_class(*arg_input)
except Exception as e:
  results[""err""] = ""Error:""+str(e)
print(results)
# results={'err': 'Error:TypeError: Scalar tensor has no `len()`\nTraceback (most recent call last):\n\n  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py"", line 1107, in __len__\n    raise TypeError(""Scalar tensor has no `len()`"")\n\nTypeError: Scalar tensor has no `len()`\n\n'}
```
```


### Relevant log output

_No response_</details>"
60444,Error message of tf.clip_by_norm is wrong,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.12.0

### Custom Code

Yes

### OS Platform and Distribution

win11

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

According to [doc](https://tensorflow.google.cn/versions/r2.4/api_docs/python/tf/clip_by_norm), the param `t` should be a Tensor or IndexedSlices, which must be a floating point type. But when `t` is [True, True], the error message indicates that `t` can be int type. More importantly, the error message when `t` is bool type is different from the error message when `t` is int type.

### Standalone code to reproduce the issue

```shell
snippet code 1:

import tensorflow as tf
results={}
try:
  t = [True,True]
  clip_norm = 1.0
  results[""res""] = tf.clip_by_norm(t=t,clip_norm=clip_norm,)
except Exception as e:
  results[""err""] = ""Error:""+str(e)
print(results)
#results={'err': ""Error:Value for attr 'T' of bool is not in the list of allowed values: bfloat16, half, float, double, uint8, int8, uint16, int16, int32, uint32, uint64, int64, complex64, complex128\n\t; NodeDef: {{node Mul}}; Op<name=Mul; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_UINT32, DT_UINT64, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; is_commutative=true> [Op:Mul]""}
```

snippet code 2:
```
import tensorflow as tf
results={}
try:
  t = [1,1,]
  clip_norm = 1.0
  results[""res""] = tf.clip_by_norm(t=t,clip_norm=clip_norm,)
except Exception as e:
  results[""err""] = ""Error:""+str(e)
print(results)
# results={'err': ""Error:Value for attr 'T' of int32 is not in the list of allowed values: bfloat16, half, float, double, complex64, complex128\n\t; NodeDef: {{node Sqrt}}; Op<name=Sqrt; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]> [Op:Sqrt]""}
```
```


### Relevant log output

_No response_</details>"
60441,Windows bazel says python is not an executable when building tflite,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   Trying to build TFLite
-  Windows 10
-   Windows Asus machine
-  source
-  r2.12
-   python 3.11.2
-   bazel 6.1.1
-   windows compiler
-   **CUDA/cuDNN version**: ---
-   Geforce GTX 960m
-  bazel build -c opt //tensorflow/lite:tensorflowlite


I'm trying to build c++ tflite for windows using bazel by following the [official documentation](https://www.tensorflow.org/install/source_windows). So far I've installed everything it want's me to and added them to PATH. Then I cloned the github repo and checked out to r2.12 branch.

Then I ran python ./configure.py and selected default for everything (said yes to override eigen strong inline). When doing so it declared that my python is located on C:\Users\Asus\AppData\Local\Programs\Python\Python311\python.exe.

After that running bazel build -c opt //tensorflow/lite:tensorflowlite on the directory where I've cloned tensorflow in cmd casuses this error:

Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=117
INFO: Reading rc options for 'build' from c:\users\asus\desktop\tensorflow\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/Users/Asus/AppData/Local/Programs/Python/Python311/python.exe
INFO: Reading rc options for 'build' from c:\users\asus\desktop\tensorflow\.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from c:\users\asus\desktop\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/Asus/AppData/Local/Programs/Python/Python311/python.exe --action_env PYTHON_LIB_PATH=C:/Users/Asus/AppData/Local/Programs/Python/Python311/Lib/site-packages --python_path=C:/Users/Asus/AppData/Local/Programs/Python/Python311/python.exe --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true
INFO: Reading rc options for 'build' from c:\users\asus\desktop\tensorflow\.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file c:\users\asus\desktop\tensorflow\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file c:\users\asus\desktop\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:windows in file c:\users\asus\desktop\tensorflow\.bazelrc: --copt=/W0 --host_copt=/W0 --copt=/Zc:__cplusplus --host_copt=/Zc:__cplusplus --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --features=compiler_param_file --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --cxxopt=/std:c++17 --host_cxxopt=/std:c++17 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/Zc:preprocessor --host_copt=/Zc:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file c:\users\asus\desktop\tensorflow\.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false
INFO: Repository local_config_python instantiated at:
  C:/users/asus/desktop/tensorflow/WORKSPACE:15:14: in <toplevel>
  C:/users/asus/desktop/tensorflow/tensorflow/workspace2.bzl:957:19: in workspace
  C:/users/asus/desktop/tensorflow/tensorflow/workspace2.bzl:104:21: in _tf_toolchains
Repository rule python_configure defined at:
  C:/users/asus/desktop/tensorflow/third_party/py/python_configure.bzl:298:35: in <toplevel>
ERROR: An error occurred during the fetch of repository 'local_config_python':
   Traceback (most recent call last):
        File ""C:/users/asus/desktop/tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl
                _create_local_python_repository(repository_ctx)
        File ""C:/users/asus/desktop/tensorflow/third_party/py/python_configure.bzl"", line 212, column 22, in _create_local_python_repository
                _check_python_bin(repository_ctx, python_bin)
        File ""C:/users/asus/desktop/tensorflow/third_party/py/python_configure.bzl"", line 145, column 25, in _check_python_bin
                auto_config_fail(""--define %s='%s' is not executable. Is it the python binary?"" % (
        File ""C:/users/asus/desktop/tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail
                fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg))
Error in fail: Configuration Error: --define PYTHON_BIN_PATH='C:/Users/Asus/AppData/Local/Programs/Python/Python311/python.exe' is not executable. Is it the python binary?
ERROR: C:/users/asus/desktop/tensorflow/WORKSPACE:15:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):
        File ""C:/users/asus/desktop/tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl
                _create_local_python_repository(repository_ctx)
        File ""C:/users/asus/desktop/tensorflow/third_party/py/python_configure.bzl"", line 212, column 22, in _create_local_python_repository
                _check_python_bin(repository_ctx, python_bin)
        File ""C:/users/asus/desktop/tensorflow/third_party/py/python_configure.bzl"", line 145, column 25, in _check_python_bin
                auto_config_fail(""--define %s='%s' is not executable. Is it the python binary?"" % (
        File ""C:/users/asus/desktop/tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail
                fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg))
Error in fail: Configuration Error: --define PYTHON_BIN_PATH='C:/Users/Asus/AppData/Local/Programs/Python/Python311/python.exe' is not executable. Is it the python binary?
INFO: Repository local_execution_config_python instantiated at:
  C:/users/asus/desktop/tensorflow/WORKSPACE:15:14: in <toplevel>
  C:/users/asus/desktop/tensorflow/tensorflow/workspace2.bzl:957:19: in workspace
  C:/users/asus/desktop/tensorflow/tensorflow/workspace2.bzl:94:27: in _tf_toolchains
  C:/users/asus/desktop/tensorflow/tensorflow/tools/toolchains/remote_config/configs.bzl:6:28: in initialize_rbe_configs
  C:/users/asus/desktop/tensorflow/tensorflow/tools/toolchains/remote_config/rbe_config.bzl:158:27: in _tensorflow_local_config
Repository rule local_python_configure defined at:
  C:/users/asus/desktop/tensorflow/third_party/py/python_configure.bzl:279:41: in <toplevel>
INFO: Repository go_sdk instantiated at:
  C:/users/asus/desktop/tensorflow/WORKSPACE:23:14: in <toplevel>
  C:/users/asus/desktop/tensorflow/tensorflow/workspace0.bzl:134:20: in workspace
  C:/users/asus/_bazel_asus/ddsftcyc/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps
  C:/users/asus/_bazel_asus/ddsftcyc/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains
  C:/users/asus/_bazel_asus/ddsftcyc/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk
Repository rule _go_download_sdk defined at:
  C:/users/asus/_bazel_asus/ddsftcyc/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>
ERROR: Analysis of target '//tensorflow/lite:tensorflowlite' failed; build aborted: Configuration Error: --define PYTHON_BIN_PATH='C:/Users/Asus/AppData/Local/Programs/Python/Python311/python.exe' is not executable. Is it the python binary?
INFO: Elapsed time: 224.163s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (32 packages loaded, 15 targets configured)
Looking at the main error part of this output : ERROR: Analysis of target '//tensorflow/lite:tensorflowlite' failed; build aborted: Configuration Error: --define PYTHON_BIN_PATH='C:/Users/Asus/AppData/Local/Programs/Python/Python311/python.exe' is not executable. Is it the python binary?

I checked whether python was there or not, it indeed is. Simply running C:/Users/Asus/AppData/Local/Programs/Python/Python311/python.exe in cmd indeed opens python.

So I checked the internet for some solutions, I've removed the ""python installers"" from my system, added python to PATH, tried the same steps with tensorflow source zip instead of cloning it, nothing works.

Some people suggested chaning some stuff inside the py directory of tensorflow but it didn't work aswell.

Why is this happening? What causes bazel to not see python even though it's there? How can I fix this and get a build with windows?
"
60440,We don't have your exam ready right now ," I had purchased the tensorflow exam and tried to start it, but it is showing that we don't have your exam ready right now.but in the candidate portal it is showing resume.so i am gonna post the screenshot of my problem 
<img width=""960"" alt=""Screenshot 2023-04-28 114949"" src=""https://user-images.githubusercontent.com/132153955/235311840-6b7d464f-46e7-4389-8135-db36f93b0605.png"">

"
60438,Inconsistency in SyncBatchNormalization/BatchNormalization(synchronized=True) Results during Distributed Training on CPUs,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04.5 LTS

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.7

### GPU model and memory

_No response_

### Current Behaviour?

I am reporting an issue encountered during the distributed training of a model with `SyncBatchNormalization` layers on CPUs. The issue presents as a relatively large inconsistency between the results produced when training the same model on a single device versus multiple devices.

We've prepared a reproduction script demonstrating this issue. We set the weights of the `SyncBatchNormalization`/`BatchNormalization` layer to reproduce this bug. After conducting a one-step training, the predictions from the identical model trained on a single CPU and two CPUs exhibited relatively large differences (measured by L-infinity distance, Linf = 0.00074467063). As a comparison, we build another model with exactly the same architecture but removing the `SyncBatchNormalization` layer, and the prediction difference from this model trained on a single CPU and two CPUs is Linf = 1.0131771e-09.

Another comparison experiment is on GPUs. We trained the same model containing `SyncBatchNormalization` layer on a single GPU and two GPUs. The Linf of model’s prediction result is 2.5331974e-07. It’s much smaller than 1CPU vs 2CPU (Linf = 0.00074467063). This is weird because I would expect the CPU executions to be more deterministic than GPUs, and I would expect larger inconsistencies in the GPU runs if the inconsistencies are caused by variance.

To ensure a controlled environment, we used the same training data for one-step training on both setups, and then evaluated the model. To guarantee that the distributed trainings are expected to produce the same results given different number of devices, we use `MirroredStrategy`, keep the global batch size the same, and use `tf.keras.losses.Reduction.AUTO` reduction. To make the difference more apparent given a limited amount of training data, we deliberately chose a relatively high learning rate (lr=10).

It's noteworthy that `SyncBatchNormalization` has been deprecated as of TensorFlow version 2.12. Therefore, we also conducted the same experiment using TensorFlow's nightly build (2.13.0-dev20230428), replacing `SyncBatchNormalization()` with `BatchNormalization(synchronized=True)`. This experiment still manifested the same inconsistency. The prediction from the model containing batchnorm layer trained on a single CPU and two CPUs exhibits a substantial difference of Linf = 0.00010111928, while for the model removing batchnorm layer it’s Linf = 6.212488e-36.


### Standalone code to reproduce the issue

```shell
For TF 2.11.0 CPU experiments, the model contains `SyncBatchNormalization` layer:
https://colab.research.google.com/drive/11EseXxq_uHweY7omt7JCr1mc-42Kf1H5?usp=sharing

For TF 2.11.0 GPU experiments, the model contains `SyncBatchNormalization` layer (fix seed to generate same inputs):
https://colab.research.google.com/drive/1m_gCWzb_OKVTYAMMs8YbdKUYM3aINNPV?usp=sharing 

For TF nightly 2.13.0-dev20230428, the model contains `BatchNormalization(synchronized=True)` layer:
https://colab.research.google.com/drive/1N-aXPcfckVb8fPDSmghMlzBOFEGq6RzM?usp=sharing
```


### Relevant log output

For TF 2.11.0:

```shell
contain syncbatchnorm:  True
1CPU vs 2CPU: 0.00074467063

contain syncbatchnorm:  False
1CPU vs 2CPU: 1.0131771e-09

contain syncbatchnorm:  True
1GPU vs 2GPU: 2.5331974e-07
```

For TF nightly 2.13.0-dev20230428:

```shell
contain syncbatchnorm:  True
1CPU vs 2CPU: 0.00010111928

contain syncbatchnorm:  False
1CPU vs 2CPU: 6.212488e-36
```

</details>"
60437,TypeError: Unable to serialize 16.0 to JSON. Unrecognized type <class 'tensorflow.python.framework.ops.EagerTensor'>.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.10 and 2.12

### Custom Code

No

### OS Platform and Distribution

Windows 10 

### Mobile device

_No response_

### Python version

3.11.3

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

RTX M960 4GB

### Current Behaviour?

I have been unable to save my transformer model using any of the Tensorflow complete model saving functions. While running Tensorflow 2.10 I received the following using message attempting to save using tf.keras.models.save_model:

TypeError: Unable to serialize 16.0 to JSON. Unrecognized type <class 'tensorflow.python.framework.ops.EagerTensor'>.

I was able to successfully save and read the model weights back in.  I was unable to recover it's normalization layer.

This issue was previously reported as a bug in 2.10

I uninstalled all python software and packages and the PyCharm IDE and installed python 3.11.3 and Tensorflow 2.12 and the latest PyCharm Community edition hoping that would correct the problem.  I used tf.keras.saving.save_model( ) using saved formats tf and keras. It did not correct the problem.  

The model I am trying to save is a simplied version to the Tensorflow Translator tutorial at https://www.tensorflow.org/text/tutorials/transformer

Here is a listing of the error message I received from PyCharm. It seems to have something to do with the python class tensorflow.python.framework.ops.EagerTensor Is there anything I can do with my model to dodge the issue

Thank you in advance for your help.

WARNING:absl:Found untraced functions such as _update_step_xla, dense_20_layer_call_fn, dense_20_layer_call_and_return_conditional_losses, positional_embedding_4_layer_call_fn, positional_embedding_4_layer_call_and_return_conditional_losses while saving (showing 5 of 165). These functions will not be directly callable after loading.

Traceback (most recent call last):
  File ""D:\Craig\Python\Projects\CraigsPackages\transformer_component_unit_tests.py"", line 333, in <module>
    return_pair = predict_investment_return(transformer, index, features, decoder_inputs,return_labels,  warm_up=warm_up)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\Craig\Python\Projects\CraigsPackages\transformer_component_unit_tests.py"", line 252, in predict_investment_return
    tf.keras.saving.save_model(transformer, ""transformer_model_save_test"", save_format=""tf"")
  File ""C:\Users\cpodc\AppData\Local\Programs\Python\Python311\Lib\site-packages\keras\saving\saving_api.py"", line 145, in save_model
    return legacy_sm_saving_lib.save_model(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\cpodc\AppData\Local\Programs\Python\Python311\Lib\site-packages\keras\utils\traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""C:\Users\cpodc\AppData\Local\Programs\Python\Python311\Lib\json\encoder.py"", line 200, in encode
    chunks = self.iterencode(o, _one_shot=True)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\cpodc\AppData\Local\Programs\Python\Python311\Lib\json\encoder.py"", line 258, in iterencode
    return _iterencode(o, 0)
           ^^^^^^^^^^^^^^^^^
TypeError: Unable to serialize 16.0 to JSON. Unrecognized type <class 'tensorflow.python.framework.ops.EagerTensor'>.



### Standalone code to reproduce the issue

```shell
tf.keras.saving.save_model(transformer, ""transformer_model_save_test"", save_format=""tf"")
tf.keras.saving.save_model(transformer, ""transformer_model_save_test"", save_format=""keras"")
```


### Relevant log output

```shell
WARNING:absl:Found untraced functions such as _update_step_xla, dense_20_layer_call_fn, dense_20_layer_call_and_return_conditional_losses, positional_embedding_4_layer_call_fn, positional_embedding_4_layer_call_and_return_conditional_losses while saving (showing 5 of 165). These functions will not be directly callable after loading.

Traceback (most recent call last):
  File ""D:\Craig\Python\Projects\CraigsPackages\transformer_component_unit_tests.py"", line 333, in <module>
    return_pair = predict_investment_return(transformer, index, features, decoder_inputs,return_labels,  warm_up=warm_up)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\Craig\Python\Projects\CraigsPackages\transformer_component_unit_tests.py"", line 252, in predict_investment_return
    tf.keras.saving.save_model(transformer, ""transformer_model_save_test"", save_format=""tf"")
  File ""C:\Users\cpodc\AppData\Local\Programs\Python\Python311\Lib\site-packages\keras\saving\saving_api.py"", line 145, in save_model
    return legacy_sm_saving_lib.save_model(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\cpodc\AppData\Local\Programs\Python\Python311\Lib\site-packages\keras\utils\traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""C:\Users\cpodc\AppData\Local\Programs\Python\Python311\Lib\json\encoder.py"", line 200, in encode
    chunks = self.iterencode(o, _one_shot=True)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\cpodc\AppData\Local\Programs\Python\Python311\Lib\json\encoder.py"", line 258, in iterencode
    return _iterencode(o, 0)
           ^^^^^^^^^^^^^^^^^
TypeError: Unable to serialize 16.0 to JSON. Unrecognized type <class 'tensorflow.python.framework.ops.EagerTensor'>.
```
</details>"
60435,Softmax API Mismatch on beta param between TF and TFL ,"In TFL's reference for Softmax, a beta param is supported to be multiplied with logits. [[Link](https://github.com/tensorflow/tensorflow/blob/fddb46a1b7f0136f64442629108961560b5e3486/tensorflow/lite/kernels/internal/reference/softmax.h#L53)] 

However, in TF, the beta seems has not been supported yet [[Link](https://www.tensorflow.org/api_docs/python/tf/nn/softmax)]. Is there any reason why it's only being supported in TFL not in TF now? 

More background: I'm trying to test the beta param from Softmax with a model converted from TF to TFL. So if it doesn't support beta in TF, how could I insert beta into the TFL model? 

Thanks, 

Jerry "
60434,Source file is missing tensorflow/lite/experimental/acceleration/configuration/configuration.fbs,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Only compiled flatbuffer header is available (i. e. `tensorflow/lite/experimental/acceleration/configuration/configuration_generated.h`).

We can not use this header in our monorepo, as using it mandates using specific version of flatbuffers library.

### Standalone code to reproduce the issue

```shell
ls tensorflow/lite/experimental/acceleration/configuration/configuration.fbs
```


### Relevant log output

_No response_</details>"
60431,Bug - pip install tflite-model-maker fails or hangs forever,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Tensorflow Version

v2.12.0-rc1-12-g0db597d0d75 2.12.0

### Custom Code

No

### OS Platform and Distribution, Mobile device, Python version, Bazel version, GCC/Compiler version, CUDA version

Default Colab Pro

### Current Behaviour?

```!pip install tflite-model-maker``` fails or hangs forever
The error occurs in tutorial notebook: https://colab.research.google.com/github/khanhlvg/tflite_raspberry_pi/blob/main/object_detection/Train_custom_model_tutorial.ipynb
I have tried multiple things in colab None/GPU runtime, 'Disconnect and delete runtime' after each failed attempt.:

`!pip install tflite-model-maker-nightly`

`pip install --no-dependencies tflite-model-maker` (imports will fail, manually installing them with `pip install tensorflowjs` won't help with `NotFoundError: /usr/local/lib/python3.10/dist-packages/tensorflow_decision_forests/tensorflow/ops/inference/inference.so: undefined symbol: _ZN3tsl6Status22MaybeAddSourceLocationENS_14SourceLocationE` error)

```
!pip install --upgrade pip
!wget https://raw.githubusercontent.com/tensorflow/examples/master/tensorflow_examples/lite/model_maker/requirements.txt
!pip install -r requirements.txt
```
results in:

```
ERROR: Could not find a version that satisfies the requirement tflite-support>=0.4.2 (from versions: 0.1.0a0.dev3, 0.1.0a0.dev4, 0.1.0a0.dev5, 0.1.0a0, 0.1.0a1)
ERROR: No matching distribution found for tflite-support>=0.4.2
```

I have also tried:
`pip install tflite-model-maker==0.4.2` got `ERROR: No matching distribution found for numba==0.53`
`pip install tflite-model-maker==0.4.1` got `ERROR: No matching distribution found for scann==1.2.6`
`pip install tflite-model-maker==0.4.0` got `ERROR: No matching distribution found for tflite-support>=0.4.0`


Related issues:
https://github.com/tensorflow/tensorflow/issues/59855
https://github.com/tensorflow/tensorflow/issues/51031





### Standalone code to reproduce the issue

```shell
`!pip install tflite-model-maker` in colab
```


### Relevant log output


tflite_modelmaker_debug2_
Notebook unstarred
1

Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Collecting tflite-model-maker
  Downloading tflite_model_maker-0.4.2-py3-none-any.whl (577 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 577.3/577.3 kB 11.3 MB/s eta 0:00:00
  Downloading tflite_model_maker-0.4.1-py3-none-any.whl (642 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 642.4/642.4 kB 28.3 MB/s eta 0:00:00
Collecting tf-models-official==2.3.0
  Downloading tf_models_official-2.3.0-py2.py3-none-any.whl (840 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 840.9/840.9 kB 12.0 MB/s eta 0:00:00
Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from tflite-model-maker) (1.22.4)
Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from tflite-model-maker) (6.0)
Requirement already satisfied: absl-py>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from tflite-model-maker) (1.4.0)
Requirement already satisfied: tensorflow-datasets>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tflite-model-maker) (4.8.3)
Requirement already satisfied: lxml>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from tflite-model-maker) (4.9.2)
Collecting tensorflow-addons>=0.11.2
  Downloading tensorflow_addons-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 591.0/591.0 kB 27.8 MB/s eta 0:00:00
Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tflite-model-maker) (1.16.0)
Collecting sentencepiece>=0.1.91
  Downloading sentencepiece-0.1.98-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 46.1 MB/s eta 0:00:00
Collecting tensorflow-hub<0.13,>=0.7.0
  Downloading tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 108.8/108.8 kB 10.7 MB/s eta 0:00:00
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1
  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128.0/128.0 kB 7.5 MB/s eta 0:00:00
Requirement already satisfied: pillow>=7.0.0 in /usr/local/lib/python3.10/dist-packages (from tflite-model-maker) (8.4.0)
Requirement already satisfied: tensorflow>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from tflite-model-maker) (2.12.0)
Collecting librosa==0.8.1
  Downloading librosa-0.8.1-py3-none-any.whl (203 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 203.8/203.8 kB 12.1 MB/s eta 0:00:00
Collecting neural-structured-learning>=1.3.1
  Downloading neural_structured_learning-1.4.0-py2.py3-none-any.whl (128 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128.6/128.6 kB 7.0 MB/s eta 0:00:00
Requirement already satisfied: Cython>=0.29.13 in /usr/local/lib/python3.10/dist-packages (from tflite-model-maker) (0.29.34)
Collecting tensorflowjs>=2.4.0
  Downloading tensorflowjs-4.4.0-py3-none-any.whl (85 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.1/85.1 kB 6.0 MB/s eta 0:00:00
Collecting matplotlib<3.5.0,>=3.0.3
  Downloading matplotlib-3.4.3.tar.gz (37.9 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 37.9/37.9 MB 9.8 MB/s eta 0:00:00
  Preparing metadata (setup.py) ... done
Collecting fire>=0.3.1
  Downloading fire-0.5.0.tar.gz (88 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.3/88.3 kB 6.4 MB/s eta 0:00:00
  Preparing metadata (setup.py) ... done
Collecting flatbuffers==1.12
  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)
Collecting tflite-model-maker
  Downloading tflite_model_maker-0.4.0-py3-none-any.whl (642 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 642.1/642.1 kB 34.6 MB/s eta 0:00:00
  Downloading tflite_model_maker-0.3.4-py3-none-any.whl (616 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 616.8/616.8 kB 33.1 MB/s eta 0:00:00
  Downloading tflite_model_maker-0.3.3-py3-none-any.whl (616 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 616.8/616.8 kB 30.7 MB/s eta 0:00:00
  Downloading tflite_model_maker-0.3.2-py3-none-any.whl (591 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 591.6/591.6 kB 22.7 MB/s eta 0:00:00
Requirement already satisfied: librosa>=0.5 in /usr/local/lib/python3.10/dist-packages (from tflite-model-maker) (0.10.0.post2)
Collecting tensorflow-hub<0.10>=0.8.0
  Downloading tensorflow_hub-0.9.0-py2.py3-none-any.whl (103 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103.3/103.3 kB 7.1 MB/s eta 0:00:00
Collecting tflite-model-maker
  Downloading tflite_model_maker-0.3.1-py3-none-any.whl (590 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 590.8/590.8 kB 32.7 MB/s eta 0:00:00
  Downloading tflite_model_maker-0.3.0-py3-none-any.whl (567 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 567.9/567.9 kB 23.1 MB/s eta 0:00:00
  Downloading tflite_model_maker-0.2.5-py3-none-any.whl (499 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 499.2/499.2 kB 4.4 MB/s eta 0:00:00
  Downloading tflite_model_maker-0.2.4-py3-none-any.whl (190 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 190.1/190.1 kB 11.9 MB/s eta 0:00:00
  Downloading tflite_model_maker-0.2.3-py3-none-any.whl (114 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.2/114.2 kB 10.0 MB/s eta 0:00:00
Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tflite-model-maker) (0.13.0)
  Downloading tflite_model_maker-0.2.2-py3-none-any.whl (103 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103.3/103.3 kB 7.8 MB/s eta 0:00:00
  Downloading tflite_model_maker-0.2.1-py3-none-any.whl (102 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.9/102.9 kB 2.3 MB/s eta 0:00:00
  Downloading tflite_model_maker-0.2.0-py3-none-any.whl (102 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.6/102.6 kB 8.0 MB/s eta 0:00:00
  Downloading tflite_model_maker-0.1.2-py3-none-any.whl (104 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 104.1/104.1 kB 4.8 MB/s eta 0:00:00
Collecting tf-nightly
  Downloading tf_nightly-2.13.0.dev20230428-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 524.1/524.1 MB 2.8 MB/s eta 0:00:00
Collecting tflite-model-maker
  Downloading tflite_model_maker-0.1.1-py3-none-any.whl (86 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.0/87.0 kB 9.6 MB/s eta 0:00:00
Collecting tflite-support==0.1.0a0
  Downloading tflite-support-0.1.0a0.tar.gz (258 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 258.8/258.8 kB 23.9 MB/s eta 0:00:00
  Preparing metadata (setup.py) ... done
Collecting tf-models-nightly
  Downloading tf_models_nightly-2.12.0.dev20230426-py2.py3-none-any.whl (2.6 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.6/2.6 MB 90.1 MB/s eta 0:00:00
Collecting pybind11>=2.4
  Using cached pybind11-2.10.4-py3-none-any.whl (222 kB)
Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker) (2.3)
Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker) (3.20.3)
Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker) (2.27.1)
Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker) (0.1.8)
Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker) (1.13.1)
Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker) (4.65.0)
Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker) (2.3.0)
Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker) (1.14.1)
Requirement already satisfied: etils[enp,epath]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker) (1.2.0)
Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker) (8.1.3)
Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker) (5.9.5)
Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets>=2.1.0->tflite-model-maker) (0.10.2)
Collecting immutabledict
  Downloading immutabledict-2.2.4-py3-none-any.whl (4.1 kB)
Collecting pyyaml<6.0,>=5.1
  Downloading PyYAML-5.4.1.tar.gz (175 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 175.1/175.1 kB 19.8 MB/s eta 0:00:00
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... done
Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from tf-models-nightly->tflite-model-maker) (4.7.0.72)
Collecting sacrebleu
  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 118.9/118.9 kB 12.3 MB/s eta 0:00:00
Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-nightly->tflite-model-maker) (9.0.0)
Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.10/dist-packages (from tf-models-nightly->tflite-model-maker) (2.84.0)
Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from tf-models-nightly->tflite-model-maker) (1.10.1)
Requirement already satisfied: tf-slim>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-nightly->tflite-model-maker) (1.1.0)
Collecting tensorflow-text-nightly
  Downloading tensorflow_text_nightly-2.13.0.dev20230424-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.5/6.5 MB 79.8 MB/s eta 0:00:00
Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from tf-models-nightly->tflite-model-maker) (4.1.3)
Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-nightly->tflite-model-maker) (1.5.3)
Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.10/dist-packages (from tf-models-nightly->tflite-model-maker) (1.5.13)
Collecting seqeval
  Downloading seqeval-1.2.2.tar.gz (43 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.6/43.6 kB 4.9 MB/s eta 0:00:00
  Preparing metadata (setup.py) ... done
Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from tf-models-nightly->tflite-model-maker) (2.0.6)
Requirement already satisfied: gin-config in /usr/local/lib/python3.10/dist-packages (from tf-models-nightly->tflite-model-maker) (0.5.0)
Collecting tensorflow-model-optimization>=0.4.1
  Downloading tensorflow_model_optimization-0.7.4-py2.py3-none-any.whl (240 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 240.6/240.6 kB 25.4 MB/s eta 0:00:00
Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from tf-models-nightly->tflite-model-maker) (3.7.1)
Collecting tf-nightly
  Downloading tf_nightly-2.13.0.dev20230427-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.0 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 524.0/524.0 MB 2.7 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230426-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.6 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 524.6/524.6 MB 2.4 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230425-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.7 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 524.7/524.7 MB 2.9 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230424-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.6 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 524.6/524.6 MB 2.6 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230420-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (522.9 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 522.9/522.9 MB 2.8 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230419-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (598.5 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 598.5/598.5 MB 2.5 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230418-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (598.3 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 598.3/598.3 MB 2.7 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230417-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (598.3 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 598.3/598.3 MB 2.5 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230416-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (598.3 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 598.3/598.3 MB 2.3 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230415-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (598.3 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 598.3/598.3 MB 2.3 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230414-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (598.3 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 598.3/598.3 MB 2.5 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230413-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (597.3 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 597.3/597.3 MB 2.3 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230412-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (597.2 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 597.2/597.2 MB 2.6 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230411-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (521.7 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 521.7/521.7 MB 2.6 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230410-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (596.9 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 596.9/596.9 MB 2.5 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230409-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (596.9 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 596.9/596.9 MB 2.5 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230408-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (597.0 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 597.0/597.0 MB 2.4 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230406-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (596.8 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 596.8/596.8 MB 2.5 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230405-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (596.8 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 596.8/596.8 MB 2.3 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230404-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (596.9 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 596.9/596.9 MB 2.3 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230403-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (596.9 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 596.9/596.9 MB 2.4 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230402-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (596.8 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 596.8/596.8 MB 2.5 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230401-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (596.8 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 596.8/596.8 MB 2.3 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230331-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (596.8 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 596.8/596.8 MB 2.3 MB/s eta 0:00:00
Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tf-nightly->tflite-model-maker) (23.1)
Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tf-nightly->tflite-model-maker) (1.54.0)
Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tf-nightly->tflite-model-maker) (3.3.0)
Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly->tflite-model-maker) (1.6.3)
Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tf-nightly->tflite-model-maker) (0.32.0)
Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly->tflite-model-maker) (16.0.0)
Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly->tflite-model-maker) (3.8.0)
Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tf-nightly->tflite-model-maker) (67.7.2)
Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tf-nightly->tflite-model-maker) (0.4.0)
  Downloading tf_nightly-2.13.0.dev20230330-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (596.8 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 596.8/596.8 MB 1.6 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230329-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (596.4 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 596.4/596.4 MB 1.4 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230328-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (596.3 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 596.3/596.3 MB 2.0 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230327-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (596.4 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 596.4/596.4 MB 2.3 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230326-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (596.4 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 596.4/596.4 MB 2.4 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230325-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (596.4 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 596.4/596.4 MB 2.5 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230324-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (596.4 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 596.4/596.4 MB 2.6 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230323-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (596.2 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 596.2/596.2 MB 2.3 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230322-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (596.0 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 596.0/596.0 MB 2.2 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230321-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (595.9 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 595.9/595.9 MB 2.4 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230319-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (595.8 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 595.8/595.8 MB 2.5 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230318-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (595.9 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 595.9/595.9 MB 2.6 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230316-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (595.8 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 595.8/595.8 MB 2.6 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230315-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (596.4 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 596.4/596.4 MB 2.3 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230314-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591.3 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 591.3/591.3 MB 2.2 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230313-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591.0 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 591.0/591.0 MB 2.5 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230312-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591.0 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 591.0/591.0 MB 2.4 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230310-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (590.9 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 590.9/590.9 MB 2.4 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230309-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.7 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 589.7/589.7 MB 2.4 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230308-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.5 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 589.5/589.5 MB 2.6 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230307-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.2 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 589.2/589.2 MB 2.1 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230306-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.1 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 589.1/589.1 MB 2.2 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230305-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.1 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 589.1/589.1 MB 2.3 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230304-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.1 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 589.1/589.1 MB 2.5 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230302-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.0 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 588.0/588.0 MB 2.4 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230301-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591.6 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 591.6/591.6 MB 2.3 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230228-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591.6 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 591.6/591.6 MB 2.6 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230227-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.4 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 589.4/589.4 MB 2.1 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230226-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.3 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 589.3/589.3 MB 2.3 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230224-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.6 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 589.6/589.6 MB 2.5 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230223-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.4 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 589.4/589.4 MB 2.3 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230222-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.3 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 589.3/589.3 MB 2.6 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230220-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.1 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 589.1/589.1 MB 2.4 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230219-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.1 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 589.1/589.1 MB 2.4 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230218-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.1 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 589.1/589.1 MB 2.2 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230217-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.1 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 589.1/589.1 MB 2.3 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230216-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.5 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 589.5/589.5 MB 2.4 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230215-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.4 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 589.4/589.4 MB 2.6 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230214-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.2 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 589.2/589.2 MB 2.2 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230213-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (587.8 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 587.8/587.8 MB 2.5 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230212-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (587.8 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 587.8/587.8 MB 2.6 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230211-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (587.7 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 587.7/587.7 MB 2.3 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230210-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (587.7 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 587.7/587.7 MB 2.4 MB/s eta 0:00:00
Collecting tb-nightly~=2.12.0.a
  Downloading tb_nightly-2.12.0a20230209-py3-none-any.whl (5.6 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 85.7 MB/s eta 0:00:00
Collecting tf-nightly
  Downloading tf_nightly-2.13.0.dev20230209-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (587.3 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 587.3/587.3 MB 2.3 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230208-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (586.9 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 586.9/586.9 MB 2.2 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230207-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (586.4 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 586.4/586.4 MB 2.5 MB/s eta 0:00:00
  Downloading tf_nightly-2.13.0.dev20230206-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 585.9/585.9 MB 1.7 MB/s eta 0:00:00
  Downloading tf_nightly-2.12.0.dev20230203-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 585.9/585.9 MB 2.2 MB/s eta 0:00:00
  Downloading tf_nightly-2.12.0.dev20230201-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (586.6 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 586.6/586.6 MB 2.3 MB/s eta 0:00:00
INFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.
INFO: pip is looking at multiple versions of tf-models-nightly to determine which version is compatible with other requirements. This could take a while.
Collecting tf-models-nightly
  Downloading tf_models_nightly-2.12.0.dev20230425-py2.py3-none-any.whl (2.6 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.6/2.6 MB 69.6 MB/s eta 0:00:00
INFO: pip is looking at multiple versions of sentencepiece to determine which version is compatible with other requirements. This could take a while.
Collecting sentencepiece
  Downloading sentencepiece-0.1.97-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 54.4 MB/s eta 0:00:00
INFO: pip is looking at multiple versions of pillow to determine which version is compatible with other requirements. This could take a while.
Collecting pillow
  Downloading Pillow-9.5.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 68.3 MB/s eta 0:00:00
INFO: pip is looking at multiple versions of fire to determine which version is compatible with other requirements. This could take a while.
Collecting fire
  Downloading fire-0.4.0.tar.gz (87 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.7/87.7 kB 9.9 MB/s eta 0:00:00
  Preparing metadata (setup.py) ... done
INFO: pip is looking at multiple versions of tensorflow-hub to determine which version is compatible with other requirements. This could take a while.
Collecting tensorflow-hub>=0.8.0
  Downloading tensorflow_hub-0.13.0-py2.py3-none-any.whl (100 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.6/100.6 kB 11.3 MB/s eta 0:00:00
INFO: pip is looking at multiple versions of tensorflow-datasets to determine which version is compatible with other requirements. This could take a while.
Collecting tensorflow-datasets>=2.1.0
  Downloading tensorflow_datasets-4.9.2-py3-none-any.whl (5.4 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.4/5.4 MB 89.6 MB/s eta 0:00:00
INFO: pip is looking at multiple versions of numpy to determine which version is compatible with other requirements. This could take a while.
Collecting numpy>=1.17.3
  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/17.3 MB 70.3 MB/s eta 0:00:00
  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.1/17.1 MB 74.9 MB/s eta 0:00:00
INFO: pip is looking at multiple versions of absl-py to determine which version is compatible with other requirements. This could take a while.
Collecting absl-py
  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 15.5 MB/s eta 0:00:00
INFO: pip is looking at multiple versions of tflite-support to determine which version is compatible with other requirements. This could take a while.
INFO: pip is looking at multiple versions of flatbuffers to determine which version is compatible with other requirements. This could take a while.
INFO: pip is looking at multiple versions of tflite-model-maker to determine which version is compatible with other requirements. This could take a while.
Collecting tflite-model-maker
  Downloading tflite_model_maker-0.1.0-py3-none-any.whl (84 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.6/84.6 kB 10.4 MB/s eta 0:00:00
ERROR: Cannot install tflite-model-maker==0.1.2, tflite-model-maker==0.2.0, tflite-model-maker==0.2.1, tflite-model-maker==0.2.2, tflite-model-maker==0.2.3, tflite-model-maker==0.2.4, tflite-model-maker==0.2.5, tflite-model-maker==0.3.0, tflite-model-maker==0.3.1, tflite-model-maker==0.3.2, tflite-model-maker==0.3.3, tflite-model-maker==0.3.4, tflite-model-maker==0.4.0, tflite-model-maker==0.4.1 and tflite-model-maker==0.4.2 because these package versions have conflicting dependencies.

The conflict is caused by:
    tflite-model-maker 0.4.2 depends on tflite-support>=0.4.2
    tflite-model-maker 0.4.1 depends on scann==1.2.6
    tflite-model-maker 0.4.0 depends on tflite-support>=0.4.0
    tflite-model-maker 0.3.4 depends on tflite-support>=0.3.1
    tflite-model-maker 0.3.3 depends on tflite-support>=0.3.1
    tflite-model-maker 0.3.2 depends on tflite-support>=0.1.0rc4
    tflite-model-maker 0.3.1 depends on tflite-support>=0.1.0rc4
    tflite-model-maker 0.3.0 depends on tflite-support>=0.1.0rc4
    tflite-model-maker 0.2.5 depends on tflite-support==0.1.0rc4
    tflite-model-maker 0.2.4 depends on tflite-support==0.1.0rc4
    tflite-model-maker 0.2.3 depends on tflite-support==0.1.0rc3.dev2
    tflite-model-maker 0.2.2 depends on tflite-support==0.1.0rc3.dev2
    tflite-model-maker 0.2.1 depends on tflite-support==0.1.0rc3.dev2
    tflite-model-maker 0.2.0 depends on tf-nightly==2.4.0.dev20200810
    tflite-model-maker 0.1.2 depends on tflite-support==0.1.0rc3.dev2

To fix this you could try to:
1. loosen the range of package versions you've specified
2. remove package versions to allow pip attempt to solve the dependency conflict

ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts"
60430,"TypeError ""unsupported operand type(s) for |: 'collections.OrderedDict' and 'collections.OrderedDict'"" is raised when training Sequential model","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

TF-nightly (2.13.0-dev20230427)

### Custom Code

No

### OS Platform and Distribution

MacOS

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

A bug happened!

TypeError ""unsupported operand type(s) for |: 'collections.OrderedDict' and 'collections.OrderedDict'"" is raised when training Sequential model composed of one Dense layer and training with numpy array type input dataset.

### Standalone code to reproduce the issue

```shell
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import SGD

import numpy as np

X = np.random.rand(10, 4)
y = np.random.rand(10)

model = Sequential()
model.add(Dense(3, input_dim=4))
model.add(Dense(1))
model.compile(loss=""mean_squared_error"", optimizer=SGD(learning_rate=0.001))
model.fit(X, y)   # raises TypeError!
```
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""../tf1.py"", line 14, in <module>
    model.fit(X, y)
  File "".../python3.8/site-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "".../python3.8/site-packages/tensorflow/core/function/capture/capture_container.py"", line 303, in capture_types
    return self._by_val_tracetype | self._by_ref_tracetype
TypeError: unsupported operand type(s) for |: 'collections.OrderedDict' and 'collections.OrderedDict'
```
</details>"
60429,tensorflow docker on WSL2: failed call to cuInit: CUDA_ERROR_NO_DEVICE,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

v2.12.0-rc1-12-g0db597d0d75 2.12.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04.06 on WSL2

### Mobile device

_No response_

### Python version

Python 3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.8/8.6

### GPU model and memory

NVIDIA GeForce RTX 2070 / 8GB

### Current Behaviour?

Windows host
* version: 10.0.22621.1555

WSL2/Ubuntu
* Ubuntu 18.04.6 LTS
* Kernel version: 5.10.102.1

NVIDIA Container Toolkit
* CLI version 1.13.1 (commit: 28b70663f1a2b982e59e83bcf1844177dc745208)

Docker (Ubuntu) 
* docker version: 23.0.5, build bc4487a
* image tag: tensorflow/tensorflow:latest-gpu
* image id: cbe3a4f4c2a0
* /etc/docker/daemon.json (already configured the NVIDIA Container Runtime. Docker is restarted to apply change as well)
```json
{
    ""runtimes"": {
        ""nvidia"": {
            ""args"": [],
            ""path"": ""nvidia-container-runtime""
        }
    }
}
```


When running in a container, tensorflow reports no GPU is found.
```
$ docker run -it --rm --gpus all tensorflow/tensorflow:latest-gpu \
    python -c 'import tensorflow as tf; gpus = tf.config.list_physical_devices(""GPU""); print(f"">>>> GPUs={gpus}"")'

2023-04-28 03:19:06.723090: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-28 03:19:08.020353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
>>>> GPUs=[]
```

However, `nvidia-smi` is able to print GPU info.
```
$ docker run -it --rm --gpus all tensorflow/tensorflow:latest-gpu nvidia-smi

+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 530.47                 Driver Version: 531.68       CUDA Version: 11.8     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 2070         On | 00000000:01:00.0  On |                  N/A |
|  0%   48C    P8               11W / 185W|    175MiB /  8192MiB |      2%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
```

### Standalone code to reproduce the issue

```shell
To reproduce the issue, refer to the `docker run` command described in the Current Behaviour.

Some hints
* When running the same configuration and commands on the Ubuntu host, GPU can be found as expected.
* Under WSL2, without running in a docker container, tensorflow is able to use GPU.
* Strange thing is when running inside the tensorflow container, tensorflow failed to find the GPU. However, nvidia-smi is able to see the GPU.
```


### Relevant log output

_No response_</details>"
60428,Tensorflow Quantum not installing in Colab,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.7

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu

### Mobile device

_No response_

### Python version

3.11, 3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

In attempting the tutorials in TFQ, Colab now says it does not recognize any TF less than 2.80 and therefore won't install TFQ 0.2.7. I attempted various other permutations of TF and TFQ versions but to no avail. 

### Standalone code to reproduce the issue

```shell
!pip install tensorflow==2.7.0 tensorflow-quantum==0.7.2
```


### Relevant log output

```shell
Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
ERROR: Could not find a version that satisfies the requirement tensorflow==2.7.0 (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0)
ERROR: No matching distribution found for tensorflow==2.7.0
```
</details>"
60427,TFLite converter does not support 4-dimensional input for dense operators ,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 pro
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.12.0

### 2. Code
Simple model
https://colab.research.google.com/drive/1IaVtPj4RO2QWV3V1CgmNPnb2hmggkLmV?usp=sharing

### 3. Failure after conversion
As you can see in the only the second dense operator is converted well (blue). The first dense op is broken down into primitive ops (red)

![image](https://user-images.githubusercontent.com/87983167/234969243-b6197275-a19a-41da-aaeb-f00b8d75fbca.png)

### 5. (optional) Any other info / logs
Likely related issue: https://github.com/tensorflow/tensorflow/issues/57691 
"
60426,AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7fa17d6e85e0> and will run it as-is,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CPU only

### GPU model and memory

_No response_

### Current Behaviour?

Running `tensorflow_decision_forests` I saw a warning in the output while running `model.fit()` which asked me to file a bug report.

Prediction seems okay, but I cannot be 100% sure about it.

Dataset files needed to reproduce the case:

[train.csv](https://github.com/tensorflow/tensorflow/files/11346644/train.csv)

[test.csv](https://github.com/tensorflow/tensorflow/files/11346645/test.csv)

List of Python modules:

[pip_list.txt](https://github.com/tensorflow/tensorflow/files/11346651/pip_list.txt)


### Standalone code to reproduce the issue

```shell
import pandas as pd
import tensorflow_decision_forests as tfdf


def type_conversion_tfdf(df_orig):
    df = df_orig.copy(deep=True)

    df[[""PassengerId_1"", ""PassengerId_2""]] = df[""PassengerId""].str.split(
        pat=""_"", n=1, expand=True
    )
    df[[""Name_First"", ""Name_Last""]] = df[""Name""].str.split(pat="" "", n=1, expand=True)
    df[[""Deck"", ""Cabin_Num"", ""Side""]] = df[""Cabin""].str.split(pat=""/"", n=2, expand=True)

    bool_to_int_columns = [""Transported"", ""CryoSleep"", ""VIP""]
    for c in bool_to_int_columns:
        if c in df.columns.to_list():
            df[c] = df[c].fillna(value=0).astype(bool).astype(int)

    categorical_columns = [
        ""HomePlanet"",
        ""Destination"",
        ""PassengerId_2"",
        ""Name_First"",
        ""Name_Last"",
        ""Deck"",
        ""Side"",
    ]
    for c in categorical_columns:
        # df[c] = df[c].astype(""category"")
        pass

    int_columns = [""Cabin_Num""]
    for c in int_columns:
        df[c] = df[c].fillna(value=0).astype(int)

    return df.drop(columns=[""PassengerId"", ""PassengerId_1"", ""Cabin"", ""Name""])


pd.options.mode.copy_on_write = True

train_df = pd.read_csv(""train.csv"")
test_df = pd.read_csv(""test.csv"")

train_df_tfdf = type_conversion_tfdf(train_df)
test_df_tfdf = type_conversion_tfdf(test_df)

train_ds_tfdf = tfdf.keras.pd_dataframe_to_tf_dataset(
    train_df_tfdf, label=""Transported""
)
test_ds_tfdf = tfdf.keras.pd_dataframe_to_tf_dataset(test_df_tfdf)

rf = tfdf.keras.RandomForestModel()
rf.fit(x=train_ds_tfdf)

predictions = rf.predict(test_ds_tfdf)
```


### Relevant log output

```shell
2023-04-27 10:53:42.381793: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-27 10:53:42.848387: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-04-27 10:53:43.514073: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-04-27 10:53:43.540254: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
Use /tmp/tmpsuc8grc3 as temporary training directory
Reading training dataset...
2023-04-27 10:53:43.603559: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_8' with dtype string and shape [8693]
         [[{{node Placeholder/_8}}]]
Training dataset read in 0:00:02.534852. Found 8693 examples.
Training model...
[INFO 23-04-27 10:54:06.0499 PDT kernel.cc:1242] Loading model from path /tmp/tmpsuc8grc3/model/ with prefix fd4ad06ed85c449e
[INFO 23-04-27 10:54:06.5441 PDT decision_forest.cc:660] Model loaded with 300 root(s), 292454 node(s), and 16 input feature(s).
[INFO 23-04-27 10:54:06.5441 PDT abstract_model.cc:1311] Engine ""RandomForestGeneric"" built
[INFO 23-04-27 10:54:06.5441 PDT kernel.cc:1074] Use fast generic engine
Model trained in 0:00:20.491433
Compiling model...
2023-04-27 10:54:06.630203: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype double and shape [8693]
         [[{{node Placeholder/_0}}]]
WARNING:tensorflow:AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7fa17d6e85e0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: could not get source code
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7fa17d6e85e0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: could not get source code
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
Model compiled.
2023-04-27 10:54:07.749903: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_13' with dtype double and shape [4277]
         [[{{node Placeholder/_13}}]]
5/5 [==============================] - 0s 60ms/step
```
</details>"
60425,"Initial training speed extreemly (~100x) slow for generated data, and become faster and faster as each epoch progress","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.8.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04.5 LTS

### Mobile device

_No response_

### Python version

3.9.1

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.1

### GPU model and memory

GeForce GTX TITAN X, 12G

### Current Behaviour?

My code is roughly like this:
```python
  for i in range(data_n):
    if not train_data[i].npz exists:
      data = generate train_data[i].npz and save to disk
    else:
      data = read train_data[i].npz from disk

    model.fit(data)
```

`data_n = 26`,  when `i` less than ~20, everything is normal.
as `i` reach about 20, something strange start to happen:

I found if the data.npz is generated, the initial training speed is *very* slow, e.g ~x100 slower than the normal speed (when directly read from .npz file), and as the training Epoch progress, it become faster and faster, until it eventually runs full power on GPU, e.g.
```
_________________________________________________________________                                                                                                                      
Epoch 1/1000                                                                                                                                                                           
732/732 [==============================] - 1524s 2s/step - loss: 3.0771 - accuracy: 0.2212 - val_loss: 3.0802 - val_accuracy: 0.2265                                                   
Epoch 2/1000                                                                                                                                                                           
732/732 [==============================] - 931s 1s/step - loss: 3.0538 - accuracy: 0.2246 - val_loss: 3.0623 - val_accuracy: 0.2303                                                    
Epoch 3/1000                                                                                                                                                                           
732/732 [==============================] - 833s 1s/step - loss: 3.0434 - accuracy: 0.2261 - val_loss: 3.0663 - val_accuracy: 0.2298                                                    
Epoch 4/1000                                                                                                                                                                           
732/732 [==============================] - 757s 1s/step - loss: 3.0339 - accuracy: 0.2275 - val_loss: 3.0618 - val_accuracy: 0.2309                                                    
Epoch 5/1000                                                                                                                                                                           
732/732 [==============================] - 663s 906ms/step - loss: 3.0316 - accuracy: 0.2279 - val_loss: 3.0692 - val_accuracy: 0.2297                                                 
Epoch 6/1000                                                                                                                                                                           
732/732 [==============================] - 572s 782ms/step - loss: 3.0254 - accuracy: 0.2288 - val_loss: 3.0683 - val_accuracy: 0.2298                                                 
Epoch 7/1000                                                                                                                                                                           
732/732 [==============================] - 406s 555ms/step - loss: 3.0201 - accuracy: 0.2291 - val_loss: 3.0638 - val_accuracy: 0.2305                                                 
Epoch 8/1000                                                                                                                                                                           
732/732 [==============================] - 367s 502ms/step - loss: 3.0148 - accuracy: 0.2301 - val_loss: 3.0595 - val_accuracy: 0.2289                                                 
Epoch 9/1000                                                                                                                                                                           
732/732 [==============================] - 321s 439ms/step - loss: 3.0098 - accuracy: 0.2311 - val_loss: 3.0599 - val_accuracy: 0.2299                                                 
Epoch 10/1000                                                                                                                                                                          
732/732 [==============================] - 304s 416ms/step - loss: 3.0058 - accuracy: 0.2318 - val_loss: 3.0608 - val_accuracy: 0.2301                                                 
Epoch 11/1000                                                                                                                                                                          
732/732 [==============================] - 286s 391ms/step - loss: 3.0053 - accuracy: 0.2317 - val_loss: 3.0562 - val_accuracy: 0.2320                                                 
Epoch 12/1000                                                                                                                                                                          
732/732 [==============================] - 268s 366ms/step - loss: 3.0010 - accuracy: 0.2315 - val_loss: 3.0563 - val_accuracy: 0.2323                                                 
Epoch 13/1000                                                                                                                                                                          
732/732 [==============================] - 252s 345ms/step - loss: 3.0003 - accuracy: 0.2320 - val_loss: 3.0518 - val_accuracy: 0.2332                                                 
Epoch 14/1000                                                                                                                                                                          
732/732 [==============================] - 233s 319ms/step - loss: 2.9963 - accuracy: 0.2321 - val_loss: 3.0488 - val_accuracy: 0.2315                                                 
Epoch 15/1000                                                                                                                                                                          
732/732 [==============================] - 217s 296ms/step - loss: 2.9926 - accuracy: 0.2326 - val_loss: 3.0515 - val_accuracy: 0.2329                                                 
Epoch 16/1000                                                                                                                                                                          
732/732 [==============================] - 198s 271ms/step - loss: 2.9881 - accuracy: 0.2341 - val_loss: 3.0514 - val_accuracy: 0.2327                                                 
Epoch 17/1000                                                                                                                                                                          
732/732 [==============================] - 181s 247ms/step - loss: 2.9883 - accuracy: 0.2326 - val_loss: 3.0527 - val_accuracy: 0.2333                                                 
Epoch 18/1000                                                                                                                                                                          
732/732 [==============================] - 165s 226ms/step - loss: 2.9868 - accuracy: 0.2338 - val_loss: 3.0496 - val_accuracy: 0.2307                                                 
Epoch 19/1000                                                                                                                                                                          
732/732 [==============================] - 153s 209ms/step - loss: 2.9831 - accuracy: 0.2333 - val_loss: 3.0438 - val_accuracy: 0.2336                                                 
Epoch 20/1000                                                                                                                                                                          
732/732 [==============================] - 137s 187ms/step - loss: 2.9817 - accuracy: 0.2338 - val_loss: 3.0442 - val_accuracy: 0.2330                    
Epoch 21/1000                                                                                                                                                                          
732/732 [==============================] - 123s 168ms/step - loss: 2.9779 - accuracy: 0.2340 - val_loss: 3.0434 - val_accuracy: 0.2344                                                 
Epoch 22/1000                                                                                                                                                                          
732/732 [==============================] - 113s 154ms/step - loss: 2.9789 - accuracy: 0.2344 - val_loss: 3.0370 - val_accuracy: 0.2341                                                 
Epoch 23/1000                                                                                                                                                                          
732/732 [==============================] - 102s 139ms/step - loss: 2.9743 - accuracy: 0.2351 - val_loss: 3.0392 - val_accuracy: 0.2356                                                 
Epoch 24/1000                                                                                                                                                                          
732/732 [==============================] - 93s 127ms/step - loss: 2.9777 - accuracy: 0.2349 - val_loss: 3.0517 - val_accuracy: 0.2336                                                  
Epoch 25/1000                                                                                                                                                                          
732/732 [==============================] - 82s 112ms/step - loss: 2.9716 - accuracy: 0.2352 - val_loss: 3.0279 - val_accuracy: 0.2369                                                  
Epoch 26/1000                                                                                                                                                                          
732/732 [==============================] - 79s 107ms/step - loss: 2.9709 - accuracy: 0.2351 - val_loss: 3.0363 - val_accuracy: 0.2348                                                  
Epoch 27/1000                                                                                                                                                                          
732/732 [==============================] - 67s 92ms/step - loss: 2.9672 - accuracy: 0.2354 - val_loss: 3.0344 - val_accuracy: 0.2343                                                   
Epoch 28/1000                                                                                                                                                                          
732/732 [==============================] - 55s 75ms/step - loss: 2.9664 - accuracy: 0.2350 - val_loss: 3.0418 - val_accuracy: 0.2337                                                   
Epoch 29/1000                                                                                                                                                                          
732/732 [==============================] - 45s 62ms/step - loss: 2.9690 - accuracy: 0.2356 - val_loss: 3.0439 - val_accuracy: 0.2340                                                   
Epoch 30/1000                                                                                                                                                                          
732/732 [==============================] - 32s 44ms/step - loss: 2.9639 - accuracy: 0.2364 - val_loss: 3.0333 - val_accuracy: 0.2350                                                   
Epoch 31/1000                                                                                                                                                                          
732/732 [==============================] - 28s 39ms/step - loss: 2.9631 - accuracy: 0.2362 - val_loss: 3.0373 - val_accuracy: 0.2353                                                   
Epoch 32/1000                                                                                                                                                                          
732/732 [==============================] - 28s 38ms/step - loss: 2.9626 - accuracy: 0.2368 - val_loss: 3.0391 - val_accuracy: 0.2343                                                   
Epoch 33/1000                                                                                                                                                                          
732/732 [==============================] - 26s 36ms/step - loss: 2.9598 - accuracy: 0.2355 - val_loss: 3.0339 - val_accuracy: 0.2356                                                   
Epoch 34/1000                                                                                                                                                                          
732/732 [==============================] - 26s 35ms/step - loss: 2.9581 - accuracy: 0.2368 - val_loss: 3.0337 - val_accuracy: 0.2349                                                   
Epoch 35/1000                                                                                                                                                                          
732/732 [==============================] - 25s 35ms/step - loss: 2.9601 - accuracy: 0.2366 - val_loss: 3.0306 - val_accuracy: 0.2363                            
```

As you can see, the 1st epoch took 1524s, while the 35th epoch took 25s; if the data is directly read from .npz an epoch took consistent ~18s.

Could this be a TF memory issue ? or it's a Python issue?

How and where I can debug such issue? or any suggestions I can try to improve the speed?

Thanks.

### Standalone code to reproduce the issue

```python
# My code is roughly like this:

  for i in range(data_n):
    if not train_data[i].npz exists:
      data = generate train_data[i].npz and save to disk
    else:
      data = read train_data[i].npz from disk

    model.fit(data)
```
```


### Relevant log output

_No response_</details>"
60420,Linux clang build issue,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

Latest master branch

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18.04.4

### Mobile device

_No response_

### Python version

3.9.16

### Bazel version

6.1.2

### GCC/Compiler version

Ubuntu clang version 16.0.1 

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Clang build failure (on Linux):

[1mERROR: ^[[0m/localdisk/gzhuang/work2/build/external/llvm-project/llvm/BUILD.bazel:320:11: Compiling llvm/lib/Option/ArgList.cpp failed: undeclared inclusion(s) in rule '@llvm-project//llvm:Option':
this rule is missing dependency declarations for the following files included by 'llvm/lib/Option/ArgList.cpp':
  'bazel-out/k8-opt/bin/external/llvm-project/llvm/Demangle.cppmap'
  'bazel-out/k8-opt/bin/external/llvm_terminfo/terminfo.cppmap'
  'bazel-out/k8-opt/bin/external/llvm_zlib/zlib.cppmap'
^[[32m[6,146 / 21,193]^[[0m 55 actions running


### Standalone code to reproduce the issue

```shell
Build Tensorflow from scratch with latest master branch (as of 4/24/2023) with build command

bazel --output_base=$build_dir build --config=opt --cxxopt=-stdlib=libc++ --copt=-O3 --strip=never -c opt //tensorflow/tools/pip_package:build_pip_package -j 56


Also set CC environment to clang++ (/usr/lib/llvm-16/bin/clang++)
```


### Relevant log output

```shell
See the ""Behaviour"" section above for the snapshot of log.
```
</details>"
60419,tf.raw_ops.GatherV2 api doc page does not properly describe the argument 'batch_dims' ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.12.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

The doc page for `tf.raw_ops.GatherV2` [here](https://www.tensorflow.org/api_docs/python/tf/raw_ops/GatherV2) does not describe how `batch_dims` is used in the operator. I found one stack overflow comment [here ](https://stackoverflow.com/questions/58194682/how-to-set-the-parameter-batch-dims-in-tf-gather-nd-tensorflow) that says:

``batch_dims=N informs TF that the first N dimensions of the tensor are batch dimensions...``

If this is indeed correct, it should be included in the doc page. Otherwise the behavior of `batch_dims` parameter is difficult to understand.

### Standalone code to reproduce the issue

```shell
Its a missing detail in the documentation for `tf.raw_ops.GatherV2.
```


### Relevant log output

_No response_</details>"
60417,AVX2 build failure on CPU with --config=mkl due to eigen update,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.13

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

5.3.0

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

AVX2 build is broken with with eigen update. Here is the commit id
https://github.com/tensorflow/tensorflow/commit/bc5f83612c6b4b96b652ac60e4138c61cbdb5fc3

### Standalone code to reproduce the issue

```shell
bazel build --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=1 --copt=-O3 --copt=-Wformat --copt=-Wformat-security --copt=-fstack-protector --copt=-fPIC --copt=-fpic --linkopt=-znoexecstack --linkopt=-zrelro --linkopt=-znow --linkopt=-fstack-protector --config=mkl --copt=-march=haswell //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
ERROR: /localdisk/amin/workspace/private-tensorflow/tensorflow/core/kernels/BUILD:5250:18: Compiling tensorflow/core/kernels/scatter_op.cc failed: (Exit 1): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 130 arguments skipped)
In file included from external/eigen_archive/Eigen/Core:180,
                 from ./tensorflow/tsl/framework/fixedpoint_types.h:21,
                 from ./tensorflow/tsl/framework/numeric_types.h:21,
                 from ./tensorflow/core/framework/numeric_types.h:24,
                 from ./tensorflow/core/framework/allocator.h:26,
                 from ./tensorflow/core/framework/op_kernel.h:27,
                 from tensorflow/core/kernels/scatter_op.cc:18:
external/eigen_archive/Eigen/src/Core/GenericPacketMath.h: In instantiation of 'Packet Eigen::internal::pmul(const Packet&, const Packet&) [with Packet = Eigen::internal::eigen_packet_wrapper<__vector(4) long long int, 5>]':
external/eigen_archive/Eigen/src/Core/functors/BinaryFunctors.h:81:26:   required from 'Packet Eigen::internal::scalar_product_op<LhsScalar, RhsScalar>::packetOp(const Packet&, const Packet&) const [with Packet = Eigen::internal::eigen_packet_wrapper<__vector(4) long long int, 5>; LhsScalar = long unsigned int; RhsScalar = long unsigned int]'
external/eigen_archive/Eigen/src/Core/functors/BinaryFunctors.h:752:30:   required from 'const Packet Eigen::internal::bind2nd_op<BinaryOp>::packetOp(const Packet&) const [with Packet = Eigen::internal::eigen_packet_wrapper<__vector(4) long long int, 5>; BinaryOp = Eigen::internal::scalar_product_op<long unsigned int, long unsigned int>]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h:517:73:   required from 'Eigen::TensorEvaluator<const Eigen::TensorCwiseUnaryOp<UnaryOp, ArgType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorCwiseUnaryOp<UnaryOp, ArgType>, Device>::packet(Eigen::TensorEvaluator<const Eigen::TensorCwiseUnaryOp<UnaryOp, ArgType>, Device>::Index) const [with int LoadMode = 0; UnaryOp = Eigen::internal::bind2nd_op<Eigen::internal::scalar_product_op<long unsigned int, long unsigned int> >; ArgType = const Eigen::TensorChippingOp<0, Eigen::TensorMap<Eigen::Tensor<long unsigned int, 2, 1, long int>, 16, Eigen::MakePointer> >; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorCwiseUnaryOp<UnaryOp, ArgType>, Device>::PacketReturnType = Eigen::internal::eigen_packet_wrapper<__vector(4) long long int, 5>; Eigen::TensorEvaluator<const Eigen::TensorCwiseUnaryOp<UnaryOp, ArgType>, Device>::Index = long int]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:180:5:   required from 'void Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::evalPacket(Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::Index) const [with LeftArgType = Eigen::TensorChippingOp<0, Eigen::TensorMap<Eigen::Tensor<long unsigned int, 2, 1, long int>, 16, Eigen::MakePointer> >; RightArgType = const Eigen::TensorCwiseUnaryOp<Eigen::internal::bind2nd_op<Eigen::internal::scalar_product_op<long unsigned int, long unsigned int> >, const Eigen::TensorChippingOp<0, Eigen::TensorMap<Eigen::Tensor<long unsigned int, 2, 1, long int>, 16, Eigen::MakePointer> > >; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::Index = long int]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:144:11:   required from 'static void Eigen::internal::TensorExecutor<Expression, Eigen::DefaultDevice, true, Eigen::internal::Off>::run(const Expression&, const Eigen::DefaultDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorChippingOp<0, Eigen::TensorMap<Eigen::Tensor<long unsigned int, 2, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::TensorCwiseUnaryOp<Eigen::internal::bind2nd_op<Eigen::internal::scalar_product_op<long unsigned int, long unsigned int> >, const Eigen::TensorChippingOp<0, Eigen::TensorMap<Eigen::Tensor<long unsigned int, 2, 1, long int>, 16, Eigen::MakePointer> > > >]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorBase.h:1211:65:   required from 'Derived& Eigen::TensorBase<Derived, AccessLevel>::operator=(const OtherDerived&) [with OtherDerived = Eigen::TensorCwiseUnaryOp<Eigen::internal::bind2nd_op<Eigen::internal::scalar_product_op<long unsigned int, long unsigned int> >, const Eigen::TensorChippingOp<0, Eigen::TensorMap<Eigen::Tensor<long unsigned int, 2, 1, long int>, 16, Eigen::MakePointer> > >; Derived = Eigen::TensorChippingOp<0, Eigen::TensorMap<Eigen::Tensor<long unsigned int, 2, 1, long int>, 16, Eigen::MakePointer> >; int AccessLevel = 1]'
./tensorflow/core/kernels/scatter_functor.h:87:7:   required from 'static void tensorflow::scatter_op::internal::Assign<tensorflow::scatter_op::UpdateOp::MUL>::RunScalar(Params, Update) [with Params = Eigen::TensorChippingOp<0, Eigen::TensorMap<Eigen::Tensor<long unsigned int, 2, 1, long int>, 16, Eigen::MakePointer> >; Update = long unsigned int]'
./tensorflow/core/kernels/scatter_functor.h:339:50:   required from 'Index tensorflow::functor::ScatterScalarFunctorBase<Device, T, Index, op>::operator()(tensorflow::OpKernelContext*, const Device&, typename tensorflow::TTypes<T>::Matrix, typename tensorflow::TTypes<T>::ConstScalar, typename tensorflow::TTypes<Index>::ConstFlat) [with Device = Eigen::ThreadPoolDevice; T = long unsigned int; Index = long int; tensorflow::scatter_op::UpdateOp op = tensorflow::scatter_op::UpdateOp::MUL; typename tensorflow::TTypes<T>::Matrix = Eigen::TensorMap<Eigen::Tensor<long unsigned int, 2, 1, long int>, 16, Eigen::MakePointer>; typename tensorflow::TTypes<T>::ConstScalar = Eigen::TensorMap<Eigen::TensorFixedSize<const long unsigned int, Eigen::Sizes<>, 1, long int>, 16, Eigen::MakePointer>; typename tensorflow::TTypes<Index>::ConstFlat = Eigen::TensorMap<Eigen::Tensor<const long int, 1, 1, long int>, 16, Eigen::MakePointer>]'
tensorflow/core/kernels/scatter_op.cc:133:36:   required from 'void tensorflow::ScatterUpdateOp<Device, T, Index, op>::DoCompute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = long unsigned int; Index = long int; tensorflow::scatter_op::UpdateOp op = tensorflow::scatter_op::UpdateOp::MUL]'
tensorflow/core/kernels/scatter_op.cc:91:7:   required from 'void tensorflow::ScatterUpdateOp<Device, T, Index, op>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = long unsigned int; Index = long int; tensorflow::scatter_op::UpdateOp op = tensorflow::scatter_op::UpdateOp::MUL]'
tensorflow/core/kernels/scatter_op.cc:87:8:   required from here
external/eigen_archive/Eigen/src/Core/GenericPacketMath.h:250:50: error: no match for 'operator*' (operand types are 'const Eigen::internal::eigen_packet_wrapper<__vector(4) long long int, 5>' and 'const Eigen::internal::eigen_packet_wrapper<__vector(4) long long int, 5>')
  250 | pmul(const Packet& a, const Packet& b) { return a*b; }
      |                                                 ~^~
In file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:79,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/framework/tensor.h:25,
                 from ./tensorflow/core/framework/device_base.h:26,
                 from ./tensorflow/core/framework/op_kernel.h:30,
                 from tensorflow/core/kernels/scatter_op.cc:18:
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorUInt128.h:138:35: note: candidate: 'template<class HL, class LL, class HR, class LR> Eigen::internal::TensorUInt128<long unsigned int, long unsigned int> Eigen::internal::operator*(const Eigen::internal::TensorUInt128<HL, LL>&, const Eigen::internal::TensorUInt128<HR, LR>&)'
  138 | TensorUInt128<uint64_t, uint64_t> operator * (const TensorUInt128<HL, LL>& lhs, const TensorUInt128<HR, LR>& rhs)
      |                                   ^~~~~~~~
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorUInt128.h:138:35: note:   template argument deduction/substitution failed:
In file included from external/eigen_archive/Eigen/Core:180,
                 from ./tensorflow/tsl/framework/fixedpoint_types.h:21,
                 from ./tensorflow/tsl/framework/numeric_types.h:21,
                 from ./tensorflow/core/framework/numeric_types.h:24,
                 from ./tensorflow/core/framework/allocator.h:26,
                 from ./tensorflow/core/framework/op_kernel.h:27,
                 from tensorflow/core/kernels/scatter_op.cc:18:
external/eigen_archive/Eigen/src/Core/GenericPacketMath.h:250:50: note:   'const Eigen::internal::eigen_packet_wrapper<__vector(4) long long int, 5>' is not derived from 'const Eigen::internal::TensorUInt128<HL, LL>'
  250 | pmul(const Packet& a, const Packet& b) { return a*b; }
      |                                                 ~^~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```
</details>"
60416,"Issue of ""type resource != float"" when trying to get frozen graph_def from a model of saved_model format","### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.6.1810 (Core)
- TensorFlow installation (pip package or built from source): pip install tensorflow
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.12.0

### 2. Code

I am trying to use the code of this function to get frozen_graph_def from a TF model in saved_model format: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python/convert_saved_model.py#L134

No error occurs during the process of conversion. But a bug is reported after I pass the frozen_graph_def to tf.import_graph_def()
```
import tensorflow_text
import tensorflow as tf
from convert_saved_model import freeze_saved_model
from tensorflow.python.saved_model import signature_constants

saved_model_dir = ""./universal-sentence-encoder-multilingual_3""
tag_set = ['serve']
signature_key = signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY
frozen_graph_def, in_tensors, out_tensors, graph = freeze_saved_model(saved_model_dir, input_arrays=None, input_shapes=None,
                       output_arrays=None, tag_set=tag_set, signature_key=signature_key)

tf.import_graph_def(frozen_graph_def, name='') 
```
The model is downloaded from this tfhub link: https://tfhub.dev/google/universal-sentence-encoder-multilingual/3
### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Can't be loaded by using tf.import_graph_def() API.


### 5. (optional) Any other info / logs
The code is run on Intel CLX Xeon CPU.

The log of this bug:
```
2023-04-26 22:31:36.666526: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-04-26 22:31:36.668374: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2023-04-26 22:31:36.708706: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2023-04-26 22:31:36.709089: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-26 22:31:38.113729: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/zehaohua/tensorflow/tensorflow/lite/python/convert_saved_model.py:42: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.saved_model.load` instead.
2023-04-26 22:31:40.329195: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
2023-04-26 22:31:42.536067: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2023-04-26 22:31:42.536209: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session
WARNING:tensorflow:From /home/zehaohua/miniconda3/envs/muse/lib/python3.8/site-packages/tensorflow/lite/python/util.py:306: convert_variables_to_constants (from tensorflow.python.framework.convert_to_constants) is deprecated and will be removed in a future version.
Instructions for updating:
This API was designed for TensorFlow v1. See https://www.tensorflow.org/guide/migrate for instructions on how to migrate your code to TensorFlow v2.
WARNING:tensorflow:From /home/zehaohua/miniconda3/envs/muse/lib/python3.8/site-packages/tensorflow/python/framework/convert_to_constants.py:952: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This API was designed for TensorFlow v1. See https://www.tensorflow.org/guide/migrate for instructions on how to migrate your code to TensorFlow v2.
Traceback (most recent call last):
  File ""/home/zehaohua/miniconda3/envs/muse/lib/python3.8/site-packages/tensorflow/python/framework/importer.py"", line 510, in _import_graph_def_internal
    results = c_api.TF_GraphImportGraphDefWithResults(
tensorflow.python.framework.errors_impl.InvalidArgumentError: input resource[0] expected type resource != float, the type of embeddings_sharded_0[0]
        In {{node EncoderDNN/EmbeddingLookup/EmbeddingLookupUnique/embedding_lookup/Gather}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""test_muse.py"", line 13, in <module>
    tf.import_graph_def(frozen_graph_def, name='')
  File ""/home/zehaohua/miniconda3/envs/muse/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py"", line 576, in new_func
    return func(*args, **kwargs)
  File ""/home/zehaohua/miniconda3/envs/muse/lib/python3.8/site-packages/tensorflow/python/framework/importer.py"", line 406, in import_graph_def
    return _import_graph_def_internal(
  File ""/home/zehaohua/miniconda3/envs/muse/lib/python3.8/site-packages/tensorflow/python/framework/importer.py"", line 515, in _import_graph_def_internal
    raise ValueError(str(e))
ValueError: input resource[0] expected type resource != float, the type of embeddings_sharded_0[0]
        In {{node EncoderDNN/EmbeddingLookup/EmbeddingLookupUnique/embedding_lookup/Gather}}

```
"
60415,Yolov8 tflite python output is not matching with tflite cpp output for float32 models,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installation (pip package or built from source):  built from source
- TensorFlow library (version, if pip package or github SHA, if built from source): tensorflow==2.4.2

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

Used following three files 
https://github.com/muhammedakyuzlu/yolov5-tflite-cpp/blob/main/src/main.cpp
https://github.com/craft-zhang/tensorflow-lite-cpp-examples/blob/main/yolov5.cc
https://github.com/craft-zhang/tensorflow-lite-cpp-examples/blob/main/yolov5.h



### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- I am using Yolov8 model, In python tflite model is predicting correctly but same model when we use it in cpp with above script its not able to give same outputtensor as like python's output
because of this problem models output scores, labels are going to 10000, 20000 like that. Kindly help us to sort this issue

"
60412,Include `run_hlo_module` in Tensorflow releases,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

`tensorflow/compiler/xla/tools/run_hlo_module` provides the ability to run HLO modules. Is it possible to include this in the TF releases so that we do not have to build from source?

### Standalone code to reproduce the issue

```shell
This is what we have to do currently:


git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
git checkout ""r${TF_BRANCH}""
bazel build -c opt --config=cuda tensorflow/compiler/xla/tools/run_hlo_module
```

It's preferred that `run_hlo_module` is included as a binary in Tensorflow releases.
```


### Relevant log output

_No response_</details>"
60411,Maximum aspect ratio of camera for Android Mobile app & Android TV to use feature of Pose estimation?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.7.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

A bug happened!

### Standalone code to reproduce the issue

```shell
What is the maximum aspect ratio of camera for Android Mobile app & Android TV to use feature of Pose estimation?
```


### Relevant log output

_No response_</details>"
60408,Missing installed C header,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

gcc-9.3.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

We install tensorflow via cmake (using conan).
For cmake we specify `-DTFLITE_ENABLE_INSTALL=ON` in order to install all files.

We have an simple C example where we just include tflite:
```
#include <tensorflow/lite/c/c_api.h>
```

This simple #include fails:
```
In file included from /CONAN/.conan/data/tensorflow-lite/2.12.0/conan_toolchain_catalog/build/package/3affdc49463f7dbc587d2b7b43afa94817639253/include/tensorflow/lite/interpreter.h:21,
                 from /home/EU.BSHG.COM/kleint/GIT/conan_toolchain_catalog/recipes/software/tensorflow-lite/2.12.0/test_package/hello.cpp:3:
/CONAN/.conan/data/tensorflow-lite/2.12.0/conan_toolchain_catalog/build/package/3affdc49463f7dbc587d2b7b43afa94817639253/include/tensorflow/lite/core/interpreter.h:55:10: fatal error: tensorflow/lite/profiling/telemetry/c/telemetry_setting_internal.h: No such file or directory
   55 | #include ""tensorflow/lite/profiling/telemetry/c/telemetry_setting_internal.h""
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
```

With current master I get similar issue:
```
In file included from /CONAN/.conan/data/tensorflow-lite/af92cf22/conan_toolchain_catalog/build/package/a31972e3f4561475d1be503229914494e93219e0/include/tensorflow/lite/core/async/async_signature_runner.h:23,
                 from /CONAN/.conan/data/tensorflow-lite/af92cf22/conan_toolchain_catalog/build/package/a31972e3f4561475d1be503229914494e93219e0/include/tensorflow/lite/core/interpreter.h:44,
                 from /CONAN/.conan/data/tensorflow-lite/af92cf22/conan_toolchain_catalog/build/package/a31972e3f4561475d1be503229914494e93219e0/include/tensorflow/lite/interpreter.h:21,
                 from /home/EU.BSHG.COM/kleint/GIT/conan_toolchain_catalog/recipes/software/tensorflow-lite/af92cf22/test_package/hello.cpp:3:
/CONAN/.conan/data/tensorflow-lite/af92cf22/conan_toolchain_catalog/build/package/a31972e3f4561475d1be503229914494e93219e0/include/tensorflow/lite/core/async/async_subgraph.h:24:10: fatal error: tensorflow/lite/core/async/interop/c/types.h: No such file or directory
   24 | #include ""tensorflow/lite/core/async/interop/c/types.h""
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.

```

### Standalone code to reproduce the issue

```shell
#include <tensorflow/lite/c/c_api.h>
```


### Relevant log output

_No response_</details>"
60407,"Cannot currently create HexagonDelegate, but was working in the past","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

org.tensorflow:tensorflow-lite:0.0.0-nightly-SNAPSHOT and org.tensorflow:tensorflow-lite-hexagon:0.0.0-nightly-SNAPSHOT with hexagon libraries v1.20.0.1

### Custom Code

Yes

### OS Platform and Distribution

Android 13

### Mobile device

Samsung Galaxy A71

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

The HexagonDelegate was working fine via a Java Android app on my device (Samsung Galaxy A71 with Snapdragon 730), but then i changed some things on the project (libraries versions, manifest) because i had issues with running some transformer models on the GPU and since then it is not working and produces the error:

Failed to load libhexagon_interface.so, Error: dlopen failed: library ""libadsprpc.so"" not found: needed by data/app/~~o6sPBobgR5g-DMgV2fEROg==/com.example.<project>-8QtHh6qQZAELc86wE5E8IA==/lib/arm64/libhexagon_interface.so in namespace classloader-namespace

I have also tried stable versions from 2.9.0 to 2.12.0 instead of the nightly snapshots and the error persists.

I can share my project if someone could look into it.

### Standalone code to reproduce the issue

```shell
On a Java project in Android Studio:

Follow the instructions in https://www.tensorflow.org/lite/android/delegates/hexagon#hexagon_delegate_java_api

build.gradle (:app):
def version = ""0.0.0-nightly-SNAPSHOT""
implementation ""org.tensorflow:tensorflow-lite:$version""
implementation ""org.tensorflow:tensorflow-lite-hexagon:$version""
implementation ""org.tensorflow:tensorflow-lite-select-tf-ops:$version""

After running the application on the device:

> adb shell ls /data/app/~~o6sPBobgR5g-DMgV2fEROg==/com.example.<project>-8QtHh6qQZAELc86wE5E8IA==/lib/arm64
libhexagon_interface.so
libhexagon_nn_skel.so
libhexagon_nn_skel_v65.so
libhexagon_nn_skel_v66.so
libtensorflowlite_flex_jni.so
libtensorflowlite_hexagon_jni.so
libtensorflowlite_jni.so

> adb shell cat /proc/cpuinfo | grep Hardware
Hardware        : Qualcomm Technologies, Inc SDMMAGPIE

> adb shell cat /sys/devices/soc0/soc_id
365

> adb shell getprop ro.product.device
a71

> adb shell getprop ro.board.platform
sm6150

Library ""libadsprpc.so"" is present in /system/vendor/lib

Code to create a HexagonDelegate:
Interpreter.Options finalOptions = new Interpreter.Options();
try {
    mHexagonDelegate = new HexagonDelegate(this);
    finalOptions.addDelegate(mHexagonDelegate);
    finalOptions.setUseXNNPACK(mOptions.useXNNPACK);
    mInterpreter = new Interpreter(mModel, finalOptions);
} catch (UnsupportedOperationException e) {
    // Hexagon delegate is not supported on this device.
    Log.e(TAG, ""Hexagon delegate is not supported on this device."");
    return;
}
```


### Relevant log output

```shell
Android Studio log:

2023-04-24 09:14:43.588 19191-19449 tflite                  com.example.<project>             E  Failed to load libhexagon_interface.so, Error: dlopen failed: library ""libadsprpc.so"" not found: needed by /data/app/~~o6sPBobgR5g-DMgV2fEROg==/com.example.<project>-8QtHh6qQZAELc86wE5E8IA==/lib/arm64/libhexagon_interface.so in namespace classloader-namespace
2023-04-24 09:14:43.588 19191-19449 tflite                  com.example.<project>             I  Hexagon Delegate is not supported.
```
</details>"
60406,"The dim size of inferred shape in GraphProperties is less than -1, which is inconsistent with TensorShapeProto","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

5.3.0

### GCC/Compiler version

11.3.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

### Behavior
Viewed dim size less than -1 after called `GraphProperties::InferStatically`, while in [TensorShapeProto](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/tensor_shape.proto#L17) the comment says either the dim size is greater than 0 or -1 (meaning unknown). 
```
message Dim {
    // Size of the tensor in that dimension.
    // This value must be >= -1, but values of -1 are reserved for ""unknown""
    // shapes (values of -1 mean ""unknown"" dimension). 
    int64 size = 1;
};
```
The two are inconsistent.
### The Cause
In [this code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/costs/graph_properties.cc#L118) which constructs the inferred dims, it assigns a negative id to unknown dimensions, starting at -2. 
```c++
// Assign a negative id to unknown dimensions, starting at -2 (the -1 id
// reserved by TensorFlow).
void ExtractValue(DimensionHandle d, int64_t* result) {
  if (!InferenceContext::ValueKnown(d)) {
    *result = -counter;
    counter++;
  } else {
    ...
  }
}
```
It can be seen from the code that size<=-1 means unknown, but the unknown size in [this code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/utils/symbolic_shapes.cc#L42) is identified with `==-1`.
`bool IsUnknown(const TensorShapeProto::Dim& dim) { return dim.size() == -1; }`
And the [`ShapeIsSymbolicallyDefined`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/utils/symbolic_shapes.cc#L44), which calls `IsUnknown `may cause bugs.
```c++
bool ShapeIsSymbolicallyDefined(const TensorShapeProto& shape) {
  return !shape.unknown_rank() &&
         std::all_of(
             shape.dim().begin(), shape.dim().end(),
             [](const TensorShapeProto::Dim& dim) { return !IsUnknown(dim); });
}
```
[`ShapesSymbolicallyEqual`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/utils/symbolic_shapes.cc#L76) function is the same problem. 
### Questions:
1. The inferred dim size is less than -1, inconsistent with TensorShapeProto, is this expected?
2. Is the `IsUnknown` function reasonable?
3. Do the `ShapeIsSymbolicallyDefined` and `ShapesSymbolicallyEqual` cause undefined behavior? I see some graph optimization path will call this function after shape inference.

### Standalone code to reproduce the issue
I manually print the shape inference log in the Tensorflow source code. First, patch my log into the Tensorflow source code, then compile the source code, and finally run the python script I gave. It can be seen from the log that the result of shape inference has dimensions less than -1.
```shell
git checkout v2.12.0

# apply patch
echo 'diff --git a/tensorflow/core/grappler/optimizers/arithmetic_optimizer.cc b/tensorflow/core/grappler/optimizers/arithmetic_optimizer.cc
index 40c27828d26..899a9d059cb 100644
--- a/tensorflow/core/grappler/optimizers/arithmetic_optimizer.cc
+++ b/tensorflow/core/grappler/optimizers/arithmetic_optimizer.cc
@@ -4452,6 +4452,26 @@ Status ArithmeticOptimizer::Optimize(Cluster* /*cluster*/,
     VLOG(1) << ""Shape inference failed."" << status.error_message();
   }

+  for (auto& node : optimized_graph_->node()) {
+    VLOG(0) << ""node_name: "" << node.name();
+    const auto& input_properties =
+        graph_properties_->GetInputProperties(node.name());
+    for (int i = 0; i < input_properties.size(); i++) {
+      auto& property = input_properties[i];
+      VLOG(0) << ""input"" << i << "": "";
+      const TensorShapeProto& tsp = property.shape();
+      if (tsp.unknown_rank()) {
+        VLOG(0) << ""unknown shape"";
+        continue;
+      }
+      VLOG(0) << ""input_rank: "" << tsp.dim_size();
+      for (int j = 0; j < tsp.dim_size(); j++) {
+        VLOG(0) << ""dim"" << j << "" size: "" << tsp.dim(j).size();
+      }
+    }
+    VLOG(0);
+  }
+
   // Perform the optimizations.
   TF_RETURN_IF_ERROR(SimplifyArithmeticOps(can_use_shapes));
   *optimized_graph = std::move(*optimized_graph_);
' | git apply

# build
bazel build //tensorflow/tools/pip_package:build_pip_package
...(continue to build)
```

```python
import numpy as np
import tensorflow as tf

@tf.function(input_signature=[tf.TensorSpec(shape=(None, None), dtype=tf.float32)])
def fun(x):
    y = tf.constant(np.ones((2, 4)), dtype=tf.float32)
    return tf.add(x, y)

output = fun(np.ones((2, 4), dtype=np.float32))
print(""output: "", output)
```


### Relevant log output

```shell
node_name: Add
input0:
input_rank: 2
dim0 size: 2
dim1 size: 4
input1:
input_rank: 2
dim0 size: -2
dim1 size: -3
```
</details>"
60405,ImportError: DLL load failed while importing _pywrap_dtensor_device: The specified procedure could not be found.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12

### Custom Code

No

### OS Platform and Distribution

Windows 10 

### Mobile device

_No response_

### Python version

3.10.4

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

no

### GPU model and memory

no

### Current Behaviour?

I just upgraded tensorflow to 2.12.x and I get the following error when I run a .py in my project:
ImportError: DLL load failed while importing _pywrap_dtensor_device: The specified procedure could not be found.

How can I solve that ??

### Standalone code to reproduce the issue

```shell
Traceback (most recent call last):
  File ""C:\Users\FRANK-PC\Documents\GitHub\Syntactic-analysis-system-of-Cuban-addresses\examples\generate_and_save_data_set.py"", line 3, in <module>
    from src.data_realism_converter.data_set_adapter import DataSetAdapter
  File ""C:\Users\FRANK-PC\Documents\GitHub\Syntactic-analysis-system-of-Cuban-addresses\src\data_realism_converter\data_set_adapter.py"", line 2, in <module>
    from keras.utils import pad_sequences, to_categorical
  File ""C:\Users\FRANK-PC\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\__init__.py"", line 21, in <module>
    from keras import models
  File ""C:\Users\FRANK-PC\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\models\__init__.py"", line 18, in <module>
    from keras.engine.functional import Functional
  File ""C:\Users\FRANK-PC\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\engine\functional.py"", line 26, in <module>
    from keras import backend
  File ""C:\Users\FRANK-PC\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\backend.py"", line 34, in <module>
    from keras.dtensor import dtensor_api as dtensor
  File ""C:\Users\FRANK-PC\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\dtensor\__init__.py"", line 22, in <module>
    from tensorflow.compat.v2.experimental import dtensor as dtensor_api
  File ""C:\Users\FRANK-PC\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\_api\v2\compat\v2\experimental\dtensor\__init__.py"", line 8, in <module>
    from tensorflow.dtensor.python.accelerator_util import initialize_accelerator_system
  File ""C:\Users\FRANK-PC\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\dtensor\python\accelerator_util.py"", line 24, in <module>
    from tensorflow.dtensor.python import tpu_util
  File ""C:\Users\FRANK-PC\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\dtensor\python\tpu_util.py"", line 24, in <module>
    from tensorflow.dtensor.python import dtensor_device
  File ""C:\Users\FRANK-PC\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\dtensor\python\dtensor_device.py"", line 27, in <module>
    from tensorflow.dtensor.python import layout as layout_lib
  File ""C:\Users\FRANK-PC\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\dtensor\python\layout.py"", line 25, in <module>
    from tensorflow.python import _pywrap_dtensor_device
ImportError: DLL load failed while importing _pywrap_dtensor_device: The specified procedure could not be found.
```


### Relevant log output

_No response_</details>"
60403,error: undefined reference to 'TfLiteGpuDelegateBindGlBufferToTensor',"Hi @impjdi the amazing porygon~,

I've checkout-ed the branch ""v2.10.0"" for deploying tflite on Android. In order to avoiding cpu-gpu memory copy, I used the function 'TfLiteGpuDelegateBindGlBufferToTensor' but got logs saying as the issue title during project configuration.

BTW, 
I had #include ""tensorflow/lite/delegates/gpu/cl/gpu_api_delegate.h""
I had compiled ""libtensorflowlite.so"", ""libtensorflowlite_gpu_delegate.so"", and all those .h included by my project

Please can you shed me some light?

Thanks~"
60401,Cuda memory error,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10

### Custom Code

Yes

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.8.16

### Bazel version

_No response_

### GCC/Compiler version

visual studio

### CUDA/cuDNN version

11.2/v8.2.1.32

### GPU model and memory

GTX 1650/ System Memory:8GB

### Current Behaviour?

I am trying to train a model for binary classification, with class1 dataset= 2716 and class 2 dataset= 2164, 
The training did happened successfully but during prediction I got error. 

The code is quite short so I will paste the whole code here, 

```
import matplotlib.pyplot as plt
from tensorflow.keras.models import Model
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense
from tensorflow.keras.applications import vgg16
from tensorflow.keras.optimizers import Adam, SGD
import tensorflow as tf
import scipy

import os
import cv2
from PIL import Image
import numpy as np

# i have habit of writing this on every program, since my GPU memory is only 4GB
if tf.config.list_physical_devices('GPU'):
    physical_devices = tf.config.list_physical_devices('GPU')
    tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)
    tf.config.experimental.set_virtual_device_configuration(physical_devices[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=12000)])


SIZE = 224

dataset = []
label = []

parasitized_images = os.listdir('Parasitized/')

for i, image_name in enumerate(parasitized_images):
    
    if (image_name.split('.')[1] == 'png'):
        image = cv2.imread('Parasitized/' + image_name)
        image = Image.fromarray(image, 'RGB')
        image = image.resize((SIZE, SIZE))
        dataset.append(np.array(image))
        label.append(1)
    
uninfected_images = os.listdir('Uninfected/')
for i, image_name in enumerate(uninfected_images):
    if (image_name.split('.')[1] == 'png'):
        image = cv2.imread('Uninfected/' + image_name)
        image = Image.fromarray(image, 'RGB')
        image = image.resize((SIZE, SIZE))
        dataset.append(np.array(image))
        label.append(0)
        
dataset = np.array(dataset)
label = np.array(label)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(dataset, label, test_size=0.2, random_state=0)

X_train = X_train.astype('float32') / 255
X_test = X_test.astype('float32') / 255     

from tensorflow.keras.utils import to_categorical

y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

def get_model(input_shape = (224, 224, 3)):
    
    with tf.device('/gpu:0'):
    
        vgg = vgg16.VGG16(weights='imagenet', include_top=False, input_shape = input_shape)
        
        for layer in vgg.layers[:-5]:
            print(layer.name)
            layer.trainable = False
            
        x = vgg.output
        x = GlobalAveragePooling2D()(x)
        x = Dense(2, activation=""softmax"")(x)
            
        model = Model(vgg.input, x)
            
        return model



model = get_model(input_shape = (224, 224, 3))

with tf.device('/cpu:0'):
    model.compile(loss=""categorical_crossentropy"",
                  optimizer = SGD(lr=0.0001, momentum=0.9), metrics=['accuracy'])

print(model.summary())
history = model.fit(X_train, y_train, batch_size=8, epochs=5, validation_data=(X_test, y_test))

#plot the training and validation accuracy and loss at each epoch
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'y', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()


acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
plt.plot(epochs, acc, 'y', label='Training acc')
plt.plot(epochs, val_acc, 'r', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

n=300  #Select the index of image to be loaded for testing
img = X_test[n]
plt.imshow(img)
input_img = np.expand_dims(img, axis=0) #Expand dims so the input is (num images, x, y, c)
print(""The prediction for this image is: "", np.argmax(model.predict(input_img)))
print(""The actual label for this image is: "", np.argmax(y_test[n]))

from sklearn.metrics import confusion_matrix
import seaborn as sns

y_pred = np.argmax(model.predict(X_test), axis=1)
cm=confusion_matrix(np.argmax(y_test, axis=1), y_pred)  
sns.heatmap(cm, annot=True)

#Identify all images classified as parasitized
parasited_image_idx = np.where(y_pred == 1)[0]

predicted_as_para=[]
for i in parasited_image_idx:
    par_img = X_test[i]
    #plt.imsave(""results_classified_as_para/para_""+str(i)+"".png"", par_img)
    predicted_as_para.append(par_img)
    
predicted_as_para = np.array(predicted_as_para)   

```
        
        
        
        
        
        
        
        
        
        
        

### Standalone code to reproduce the issue

```shell
from sklearn.metrics import confusion_matrix
import seaborn as sns

y_pred = np.argmax(model.predict(X_test), axis=1)
cm=confusion_matrix(np.argmax(y_test, axis=1), y_pred)  
sns.heatmap(cm, annot=True)
```


### Relevant log output

```shell
Traceback (most recent call last):

  Cell In[2], line 4
    y_pred = np.argmax(model.predict(X_test), axis=1)

  File D:\anaconda3\envs\tensorflow-gpu\lib\site-packages\keras\utils\traceback_utils.py:70 in error_handler
    raise e.with_traceback(filtered_tb) from None

  File D:\anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\framework\constant_op.py:102 in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)

InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.


2023-04-23 01:00:06.362905: I tensorflow/stream_executor/cuda/cuda_driver.cc:733] failed to allocate 8.41G (9029127168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-04-23 01:00:06.507445: I tensorflow/stream_executor/cuda/cuda_driver.cc:733] failed to allocate 8.41G (9029127168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-04-23 01:00:16.685829: I tensorflow/stream_executor/cuda/cuda_driver.cc:733] failed to allocate 8.41G (9029127168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-04-23 01:00:16.820275: I tensorflow/stream_executor/cuda/cuda_driver.cc:733] failed to allocate 8.41G (9029127168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-04-23 01:00:16.820335: W tensorflow/core/common_runtime/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 560.44MiB (rounded to 587661312)requested by op _EagerConst
If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. 
Current allocation summary follows.
Current allocation summary follows.
2023-04-23 01:00:16.820350: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] BFCAllocator dump for GPU_0_bfc
2023-04-23 01:00:16.820363: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (256): 	Total Chunks: 45, Chunks in use: 44. 11.2KiB allocated for chunks. 11.0KiB in use in bin. 740B client-requested in use in bin.
2023-04-23 01:00:16.820380: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (512): 	Total Chunks: 2, Chunks in use: 2. 1.0KiB allocated for chunks. 1.0KiB in use in bin. 1.0KiB client-requested in use in bin.
2023-04-23 01:00:16.820394: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (1024): 	Total Chunks: 4, Chunks in use: 4. 4.2KiB allocated for chunks. 4.2KiB in use in bin. 4.0KiB client-requested in use in bin.
2023-04-23 01:00:16.820405: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (2048): 	Total Chunks: 9, Chunks in use: 9. 20.0KiB allocated for chunks. 20.0KiB in use in bin. 18.0KiB client-requested in use in bin.
2023-04-23 01:00:16.820416: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (4096): 	Total Chunks: 4, Chunks in use: 4. 22.5KiB allocated for chunks. 22.5KiB in use in bin. 22.4KiB client-requested in use in bin.
2023-04-23 01:00:16.820426: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (8192): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-04-23 01:00:16.820437: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (16384): 	Total Chunks: 1, Chunks in use: 1. 30.5KiB allocated for chunks. 30.5KiB in use in bin. 30.5KiB client-requested in use in bin.
2023-04-23 01:00:16.820447: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (32768): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-04-23 01:00:16.820456: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (65536): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-04-23 01:00:16.820466: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (131072): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-04-23 01:00:16.820477: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (262144): 	Total Chunks: 2, Chunks in use: 2. 707.0KiB allocated for chunks. 707.0KiB in use in bin. 432.0KiB client-requested in use in bin.
2023-04-23 01:00:16.820488: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (524288): 	Total Chunks: 2, Chunks in use: 2. 1.14MiB allocated for chunks. 1.14MiB in use in bin. 1.14MiB client-requested in use in bin.
2023-04-23 01:00:16.820499: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (1048576): 	Total Chunks: 1, Chunks in use: 1. 1.97MiB allocated for chunks. 1.97MiB in use in bin. 1.12MiB client-requested in use in bin.
2023-04-23 01:00:16.820510: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (2097152): 	Total Chunks: 2, Chunks in use: 2. 4.50MiB allocated for chunks. 4.50MiB in use in bin. 4.50MiB client-requested in use in bin.
2023-04-23 01:00:16.820520: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (4194304): 	Total Chunks: 2, Chunks in use: 1. 10.06MiB allocated for chunks. 4.50MiB in use in bin. 4.50MiB client-requested in use in bin.
2023-04-23 01:00:16.820531: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (8388608): 	Total Chunks: 8, Chunks in use: 8. 72.00MiB allocated for chunks. 72.00MiB in use in bin. 72.00MiB client-requested in use in bin.
2023-04-23 01:00:16.820541: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (16777216): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-04-23 01:00:16.820551: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-04-23 01:00:16.820560: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (67108864): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-04-23 01:00:16.820572: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (134217728): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-04-23 01:00:16.820584: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (268435456): 	Total Chunks: 3, Chunks in use: 2. 3.22GiB allocated for chunks. 2.74GiB in use in bin. 2.74GiB client-requested in use in bin.
2023-04-23 01:00:16.820594: I tensorflow/core/common_runtime/bfc_allocator.cc:1056] Bin for 560.44MiB was 256.00MiB, Chunk State: 
2023-04-23 01:00:16.820607: I tensorflow/core/common_runtime/bfc_allocator.cc:1062]   Size: 497.67MiB | Requested Size: 9.00MiB | in_use: 0 | bin_num: 20, prev:   Size: 560.44MiB | Requested Size: 560.44MiB | in_use: 1 | bin_num: -1
2023-04-23 01:00:16.820615: I tensorflow/core/common_runtime/bfc_allocator.cc:1069] Next region of size 3553784832
2023-04-23 01:00:16.820623: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa00000 of size 1280 next 1
2023-04-23 01:00:16.820630: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa00500 of size 256 next 2
2023-04-23 01:00:16.820637: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa00600 of size 256 next 3
2023-04-23 01:00:16.820644: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa00700 of size 256 next 5
2023-04-23 01:00:16.820651: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa00800 of size 256 next 6
2023-04-23 01:00:16.820658: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa00900 of size 256 next 4
2023-04-23 01:00:16.820665: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa00a00 of size 256 next 7
2023-04-23 01:00:16.820672: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa00b00 of size 256 next 12
2023-04-23 01:00:16.820679: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa00c00 of size 256 next 10
2023-04-23 01:00:16.820686: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa00d00 of size 256 next 11
2023-04-23 01:00:16.820693: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa00e00 of size 512 next 15
2023-04-23 01:00:16.820700: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa01000 of size 256 next 16
2023-04-23 01:00:16.820707: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa01100 of size 256 next 19
2023-04-23 01:00:16.820714: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa01200 of size 256 next 50
2023-04-23 01:00:16.820722: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa01300 of size 256 next 22
2023-04-23 01:00:16.820729: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa01400 of size 256 next 20
2023-04-23 01:00:16.820736: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa01500 of size 256 next 21
2023-04-23 01:00:16.820743: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa01600 of size 1024 next 25
2023-04-23 01:00:16.820750: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa01a00 of size 256 next 26
2023-04-23 01:00:16.820757: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa01b00 of size 256 next 29
2023-04-23 01:00:16.820764: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa01c00 of size 1024 next 32
2023-04-23 01:00:16.820771: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa02000 of size 256 next 56
2023-04-23 01:00:16.820777: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa02100 of size 256 next 57
2023-04-23 01:00:16.820784: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa02200 of size 256 next 58
2023-04-23 01:00:16.820791: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa02300 of size 256 next 35
2023-04-23 01:00:16.820799: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa02400 of size 256 next 30
2023-04-23 01:00:16.820806: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa02500 of size 256 next 31
2023-04-23 01:00:16.820813: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa02600 of size 2048 next 37
2023-04-23 01:00:16.820820: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa02e00 of size 256 next 38
2023-04-23 01:00:16.820827: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa02f00 of size 256 next 41
2023-04-23 01:00:16.820834: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa03000 of size 2048 next 44
2023-04-23 01:00:16.820841: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa03800 of size 2048 next 8
2023-04-23 01:00:16.820848: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa04000 of size 256 next 54
2023-04-23 01:00:16.820855: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa04100 of size 512 next 17
2023-04-23 01:00:16.820863: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa04300 of size 1024 next 28
2023-04-23 01:00:16.820870: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa04700 of size 2048 next 39
2023-04-23 01:00:16.820877: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa04f00 of size 2048 next 49
2023-04-23 01:00:16.820884: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa05700 of size 2048 next 51
2023-04-23 01:00:16.820891: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa05f00 of size 2048 next 65
2023-04-23 01:00:16.820898: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa06700 of size 3072 next 52
2023-04-23 01:00:16.820908: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa07300 of size 256 next 47
2023-04-23 01:00:16.820916: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa07400 of size 6912 next 42
2023-04-23 01:00:16.820923: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa08f00 of size 281600 next 14
2023-04-23 01:00:16.820930: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aa4db00 of size 442368 next 18
2023-04-23 01:00:16.820937: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50aab9b00 of size 2064384 next 23
2023-04-23 01:00:16.820944: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50acb1b00 of size 589824 next 13
2023-04-23 01:00:16.820951: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50ad41b00 of size 4718592 next 33
2023-04-23 01:00:16.820957: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50b1c1b00 of size 256 next 59
2023-04-23 01:00:16.820964: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50b1c1c00 of size 256 next 60
2023-04-23 01:00:16.820971: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50b1c1d00 of size 256 next 61
2023-04-23 01:00:16.820978: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50b1c1e00 of size 256 next 62
2023-04-23 01:00:16.820985: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50b1c1f00 of size 3072 next 24
2023-04-23 01:00:16.820992: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50b1c2b00 of size 4096 next 48
2023-04-23 01:00:16.821000: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50b1c3b00 of size 31232 next 55
2023-04-23 01:00:16.821007: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50b1cb500 of size 4096 next 67
2023-04-23 01:00:16.821014: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50b1cc500 of size 256 next 68
2023-04-23 01:00:16.821021: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50b1cc600 of size 256 next 69
2023-04-23 01:00:16.821028: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50b1cc700 of size 256 next 70
2023-04-23 01:00:16.821034: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50b1cc800 of size 256 next 71
2023-04-23 01:00:16.821042: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50b1cc900 of size 256 next 72
2023-04-23 01:00:16.821050: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50b1cca00 of size 256 next 73
2023-04-23 01:00:16.821057: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50b1ccb00 of size 256 next 74
2023-04-23 01:00:16.821064: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50b1ccc00 of size 256 next 75
2023-04-23 01:00:16.821070: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50b1ccd00 of size 256 next 76
2023-04-23 01:00:16.821077: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50b1cce00 of size 256 next 81
2023-04-23 01:00:16.821084: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50b1ccf00 of size 256 next 78
2023-04-23 01:00:16.821091: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 50b1cd000 of size 256 next 85
2023-04-23 01:00:16.821098: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50b1cd100 of size 256 next 96
2023-04-23 01:00:16.821104: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50b1cd200 of size 256 next 86
2023-04-23 01:00:16.821111: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50b1cd300 of size 7936 next 93
2023-04-23 01:00:16.821118: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50b1cf200 of size 602112 next 89
2023-04-23 01:00:16.821125: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 50b262200 of size 5830912 next 36
2023-04-23 01:00:16.821132: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50b7f1b00 of size 2359296 next 27
2023-04-23 01:00:16.821140: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50ba31b00 of size 2359296 next 40
2023-04-23 01:00:16.821147: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50bc71b00 of size 9437184 next 34
2023-04-23 01:00:16.821154: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50c571b00 of size 9437184 next 46
2023-04-23 01:00:16.821161: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50ce71b00 of size 9437184 next 45
2023-04-23 01:00:16.821168: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50d771b00 of size 9437184 next 9
2023-04-23 01:00:16.821174: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50e071b00 of size 9437184 next 43
2023-04-23 01:00:16.821181: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 50e971b00 of size 2349441024 next 53
2023-04-23 01:00:16.821188: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 59aa0bb00 of size 9437184 next 63
2023-04-23 01:00:16.821195: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 59b30bb00 of size 9437184 next 64
2023-04-23 01:00:16.821202: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 59bc0bb00 of size 9437184 next 66
2023-04-23 01:00:16.821209: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 59c50bb00 of size 587661312 next 84
2023-04-23 01:00:16.821216: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 5bf57bb00 of size 521844992 next 18446744073709551615
2023-04-23 01:00:16.821223: I tensorflow/core/common_runtime/bfc_allocator.cc:1094]      Summary of in-use Chunks by size: 
2023-04-23 01:00:16.821232: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 44 Chunks of size 256 totalling 11.0KiB
2023-04-23 01:00:16.821241: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 512 totalling 1.0KiB
2023-04-23 01:00:16.821249: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 3 Chunks of size 1024 totalling 3.0KiB
2023-04-23 01:00:16.821258: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 1280 totalling 1.2KiB
2023-04-23 01:00:16.821267: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 7 Chunks of size 2048 totalling 14.0KiB
2023-04-23 01:00:16.821277: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 3072 totalling 6.0KiB
2023-04-23 01:00:16.821286: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 4096 totalling 8.0KiB
2023-04-23 01:00:16.821294: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 6912 totalling 6.8KiB
2023-04-23 01:00:16.821303: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 7936 totalling 7.8KiB
2023-04-23 01:00:16.821313: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 31232 totalling 30.5KiB
2023-04-23 01:00:16.821322: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 281600 totalling 275.0KiB
2023-04-23 01:00:16.821330: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 442368 totalling 432.0KiB
2023-04-23 01:00:16.821339: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 589824 totalling 576.0KiB
2023-04-23 01:00:16.821347: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 602112 totalling 588.0KiB
2023-04-23 01:00:16.821356: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 2064384 totalling 1.97MiB
2023-04-23 01:00:16.821365: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 2359296 totalling 4.50MiB
2023-04-23 01:00:16.821373: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 4718592 totalling 4.50MiB
2023-04-23 01:00:16.821382: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 8 Chunks of size 9437184 totalling 72.00MiB
2023-04-23 01:00:16.821390: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 587661312 totalling 560.44MiB
2023-04-23 01:00:16.821399: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 2349441024 totalling 2.19GiB
2023-04-23 01:00:16.821407: I tensorflow/core/common_runtime/bfc_allocator.cc:1101] Sum Total of in-use chunks: 2.82GiB
2023-04-23 01:00:16.821416: I tensorflow/core/common_runtime/bfc_allocator.cc:1103] total_region_allocated_bytes_: 3553784832 memory_limit_: 12582912000 available bytes: 9029127168 curr_region_allocation_bytes_: 25165824000
2023-04-23 01:00:16.821428: I tensorflow/core/common_runtime/bfc_allocator.cc:1109] Stats: 
Limit:                     12582912000
InUse:                      3026108672
MaxInUse:                   3553784320
NumAllocs:                      299589
MaxAllocSize:               2349441024
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-04-23 01:00:16.821441: W tensorflow/core/common_runtime/bfc_allocator.cc:491] **************************************************************************************______________
```
</details>"
60400,The inferred shape of `tf.RaggedTensor.row_lengths(axis=2)` in Keras graph is incorrect for ragged tensor with uniform row lengths,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

TF 2.12, TF nightly 2.13.0-dev20230420

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Assume a Keras graph gets a ragged tensor with uniform row lengths in the axis 1, so for example
```python
inputs = tf.keras.layers.Input([64, None], ragged=True)
```

Then `inputs.row_lengths(axis=2)` is a `KerasTensor` with a correct spec `RaggedTensorSpec(TensorShape([None, 64]), ...)`.

However, when you index the first axis using a ""full"" slice, i.e.
```python
inputs.row_lengths(axis=2)[:]
```
you should get the same tensor -- but you get a `KerasTensor` with an incorrect spec `RaggedTensorSpec(TensorShape([1, 64]), ...)`.

### Standalone code to reproduce the issue

A Colab notebook reproducing the issue both in TF 2.12.0 and in TF nightly 2.13.0-dev20230420 can be found at https://colab.research.google.com/drive/1RKvNdB_81yKZkfzDpefIPJU2pgqZuYUY?usp=sharing

The full source also follows:

```python
inputs = tf.keras.layers.Input([64, None], ragged=True)
print(inputs)

print(inputs.row_lengths(axis=1))
print(inputs.row_lengths(axis=1)[:]) # OK, is the same as above

print(inputs.row_lengths(axis=2))
print(inputs.row_lengths(axis=2)[:]) # Problem, should be the same as above
```


### Relevant log output

```python
KerasTensor(type_spec=RaggedTensorSpec(TensorShape([None, 64, None]), tf.float32, 2, tf.int64), name='input_1', description=""created by layer 'input_1'"")
KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.int64, name=None), name='input.row_lengths/sub:0', description=""created by layer 'input.row_lengths'"")
KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.int64, name=None), name='tf.__operators__.getitem/strided_slice:0', description=""created by layer 'tf.__operators__.getitem'"")
KerasTensor(type_spec=RaggedTensorSpec(TensorShape([None, 64]), tf.int64, 1, tf.int64), description=""created by layer 'input.row_lengths_2'"")
KerasTensor(type_spec=RaggedTensorSpec(TensorShape([1, 64]), tf.int64, 1, tf.int64), description=""created by layer 'tf.__operators__.ragged_getitem'"")
```
</details>"
60399,I am noticing lower validation accuracy on my dataset between Tensorflow 2.4 and Tensorflow 2.9,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.9 and 2.4

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I am trying to train an image classifier model using EfficientNetB1 on a custom dataset and I am trying out TensorFlow 2.4 and TensorFlow 2.9. I am using the exact same script with the same optimizer, augmentation, parameters, and dataset. I ran training 5 times and the results are around the same.

Results:

- TensorFlow 2.4: ~97-98% Accuracy on the validation set.

- TensorFlow 2.9: ~93-95% Accuracy on the validation set

More information: I am using Adam optimizer with 0.0001 lr, batch size of 16, using imagenet model weights, and categorical_crossentropy for my loss. I am using the same dataset on each version and I am using the same training script. I simply switch conda enviroments to TF 2.4 and 2.9.

Did something change between both versions that cause this discrepancy? Did the EfficientNet model weights change? Is the way the validation accuracy are calculated is different? Are the opimizers implementations are different?

I would appreciate your help and I would like some information on how to make it consistent between both versions. Thanks

### Standalone code to reproduce the issue

```shell
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'
import tensorflow as tf
from tensorflow.keras.applications import EfficientNetB1

model_base = EfficientNetB1(weights='imagenet',include_top=False, input_shape=(image_size, image_size, 3))
model.add(model_base)
model._name = ""EfficientNetB1""    
model.add(layers.Flatten())
model.add(tf.keras.layers.Dense(len(classes), activation='softmax'))

opt = Adam(learning_rate=1e-4)
model.compile(optimizer=opt, loss= 'categorical_crossentropy', metrics=['accuracy'])
```


### Relevant log output

_No response_</details>"
60398,bazel compile error: enumeration value ‘CUDNN_POINTWISE_RECIPROCAL’ not handled in switch,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

v2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Debian Bullseye

### Mobile device

_No response_

### Python version

3.9

### Bazel version

5.3.0

### GCC/Compiler version

gcc (Debian 10.2.1-6) 10.2.1 20210110

### CUDA/cuDNN version

cuda_11.6.r11.6/compiler.31057947_0

### GPU model and memory

RTX A4000

### Current Behaviour?

bazel build --config=cuda --config=mkl //tensorflow/tools/pip_package:build_pip_package
failed displaying following error messages

ERROR: /mnt/data/kentwork/src/AI/tensorflow/tensorflow/compiler/xla/stream_executor/cuda/BUILD:376:11: Compiling tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/compiler/xla/stream_executor/cuda/_objs/cudnn_plugin/cuda_dnn.pic.d ... (remaining 141 arguments skipped)
In file included from bazel-out/k8-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend_Operation.h:37,
                 from bazel-out/k8-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend_OperationGraph.h:36,
                 from bazel-out/k8-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend_Heuristics.h:31,
                 from bazel-out/k8-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend.h:101,
                 from tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:55:
bazel-out/k8-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend_PointWiseDesc.h: In member function ‘int64_t cudnn_frontend::PointWiseDesc_v8::getPortCount() const’:
bazel-out/k8-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend_PointWiseDesc.h:69:16: error: enumeration value ‘CUDNN_POINTWISE_RECIPROCAL’ not handled in switch [-Werror=switch]
   69 |         switch (mode) {
      |                ^
In file included from bazel-out/k8-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend_OperationGraph.h:36,
                 from bazel-out/k8-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend_Heuristics.h:31,
                 from bazel-out/k8-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend.h:101,
                 from tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:55:
bazel-out/k8-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend_Operation.h: In member function ‘cudnn_frontend::Operation_v8&& cudnn_frontend::OperationBuilder_v8::build_pointwise_op()’:
bazel-out/k8-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend_Operation.h:354:16: error: enumeration value ‘CUDNN_POINTWISE_RECIPROCAL’ not handled in switch [-Werror=switch]
  354 |         switch (m_operation.pointwise_mode) {
      |                ^
cc1plus: some warnings being treated as errors
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.

### Standalone code to reproduce the issue

```shell
bazel build --config=cuda --config=mkl //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

_No response_</details>"
60397,"segment_reduction_ops_gpu.cu.h error: no instance of overloaded function ""tensorflow::min""/""tensorflow::max"" matches the argument list","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Windows 10 1909

### Mobile device

_No response_

### Python version

Anaconda 2023.03

### Bazel version

5.3.0

### GCC/Compiler version

Visual Studio 2022 (build tools 14.35) + msys2-x86_64-20230318

### CUDA/cuDNN version

CUDA 11.8 + CUDNN 8.6.0 + TensorRT 8.5.3

### GPU model and memory

GTX 750 Ti 2GB

### Current Behaviour?

From previous discussion in https://github.com/tensorflow/tensorflow/issues/59918 and https://github.com/tensorflow/tensorflow/issues/59905, segment_reduction_ops_gpu.cu.h and other GPU header files have function overload errors when using MSVC + msys2 to compile. Run bazel command again just mean shuffle the compile action sequence. The compiler will reach the same errors when starts to compile segment_reduction_ops_gpu_x.cu.cc.

### Standalone code to reproduce the issue

```shell
1. download https://github.com/tensorflow/tensorflow/archive/refs/tags/v2.12.0.zip and extract
2. comment out Windows CUDA build rejection code in configure.py
3. run `python configure.py` to configure Windows CUDA build
4. run `bazel build --config=opt --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package`
```


### Relevant log output

```shell
.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(109): error: no instance of overloaded function ""tensorflow::min"" matches the argument list
            argument types are: (tsl::int32, int)
          detected during:
            instantiation of ""void tensorflow::SortedSegmentReductionCustomKernel<T,Index,OuterDimTileSize,ReductionF,AtomicReductionF>(Index, Index, Index, const Index *, const T
*, T *, Index, T) [with T=Eigen::half, Index=tsl::int32, OuterDimTileSize=8, ReductionF=tensorflow::NonAtomicProdOpGpu, AtomicReductionF=tensorflow::AtomicProdOpGpu]""
(750): here
            instantiation of ""void tensorflow::functor::SegmentReductionFunctor<T, Index, InitialValueF, EmptySegmentValueF, ReductionF>::operator()(tensorflow::OpKernelContext *,
const tensorflow::functor::GPUDevice &, Index, const tensorflow::TensorShape &, __nv_bool, tensorflow::TTypes<Index, 1, Eigen::DenseIndex>::ConstFlat, Index, const T *, tensorflow:
:TTypes<T, 2, Eigen::DenseIndex>::Tensor) [with T=Eigen::half, Index=tsl::int32, InitialValueF=tensorflow::functor::One<Eigen::half>, EmptySegmentValueF=tensorflow::functor::One<Ei
gen::half>, ReductionF=tensorflow::functor::Prod]""
E:\20220616_1800pm\_bazel_tensorflow\jsjos6dw\execroot\org_tensorflow\tensorflow\core\kernels\segment_reduction_ops_gpu_0.cu.cc(63): here

.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(109): error: no instance of overloaded function ""tensorflow::min"" matches the argument list
            argument types are: (tsl::int32, int)
          detected during:
            instantiation of ""void tensorflow::SortedSegmentReductionCustomKernel<T,Index,OuterDimTileSize,ReductionF,AtomicReductionF>(Index, Index, Index, const Index *, const T
*, T *, Index, T) [with T=Eigen::half, Index=tsl::int32, OuterDimTileSize=8, ReductionF=tensorflow::NonAtomicMinOpGpu, AtomicReductionF=tensorflow::AtomicMinOpGpu]""
(750): here
            instantiation of ""void tensorflow::functor::SegmentReductionFunctor<T, Index, InitialValueF, EmptySegmentValueF, ReductionF>::operator()(tensorflow::OpKernelContext *,
const tensorflow::functor::GPUDevice &, Index, const tensorflow::TensorShape &, __nv_bool, tensorflow::TTypes<Index, 1, Eigen::DenseIndex>::ConstFlat, Index, const T *, tensorflow:
:TTypes<T, 2, Eigen::DenseIndex>::Tensor) [with T=Eigen::half, Index=tsl::int32, InitialValueF=tensorflow::functor::Highest<Eigen::half>, EmptySegmentValueF=tensorflow::functor::Ze
ro<Eigen::half>, ReductionF=tensorflow::functor::Min]""
E:\20220616_1800pm\_bazel_tensorflow\jsjos6dw\execroot\org_tensorflow\tensorflow\core\kernels\segment_reduction_ops_gpu_0.cu.cc(63): here

.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(109): error: no instance of overloaded function ""tensorflow::min"" matches the argument list
            argument types are: (tsl::int32, int)
          detected during:
            instantiation of ""void tensorflow::SortedSegmentReductionCustomKernel<T,Index,OuterDimTileSize,ReductionF,AtomicReductionF>(Index, Index, Index, const Index *, const T
*, T *, Index, T) [with T=Eigen::half, Index=tsl::int32, OuterDimTileSize=8, ReductionF=tensorflow::NonAtomicMaxOpGpu, AtomicReductionF=tensorflow::AtomicMaxOpGpu]""
(750): here
            instantiation of ""void tensorflow::functor::SegmentReductionFunctor<T, Index, InitialValueF, EmptySegmentValueF, ReductionF>::operator()(tensorflow::OpKernelContext *,
const tensorflow::functor::GPUDevice &, Index, const tensorflow::TensorShape &, __nv_bool, tensorflow::TTypes<Index, 1, Eigen::DenseIndex>::ConstFlat, Index, const T *, tensorflow:
:TTypes<T, 2, Eigen::DenseIndex>::Tensor) [with T=Eigen::half, Index=tsl::int32, InitialValueF=tensorflow::functor::Lowest<Eigen::half>, EmptySegmentValueF=tensorflow::functor::Zer
o<Eigen::half>, ReductionF=tensorflow::functor::Max]""
E:\20220616_1800pm\_bazel_tensorflow\jsjos6dw\execroot\org_tensorflow\tensorflow\core\kernels\segment_reduction_ops_gpu_0.cu.cc(63): here

.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(109): error: no instance of overloaded function ""tensorflow::min"" matches the argument list
            argument types are: (tsl::int32, int)
          detected during:
            instantiation of ""void tensorflow::SortedSegmentReductionCustomKernel<T,Index,OuterDimTileSize,ReductionF,AtomicReductionF>(Index, Index, Index, const Index *, const T
*, T *, Index, T) [with T=tensorflow::bfloat16, Index=tsl::int32, OuterDimTileSize=8, ReductionF=tensorflow::NonAtomicSumOpGpu, AtomicReductionF=tensorflow::AtomicSumOpGpu]""
(750): here
            instantiation of ""void tensorflow::functor::SegmentReductionFunctor<T, Index, InitialValueF, EmptySegmentValueF, ReductionF>::operator()(tensorflow::OpKernelContext *,
const tensorflow::functor::GPUDevice &, Index, const tensorflow::TensorShape &, __nv_bool, tensorflow::TTypes<Index, 1, Eigen::DenseIndex>::ConstFlat, Index, const T *, tensorflow:
:TTypes<T, 2, Eigen::DenseIndex>::Tensor) [with T=tensorflow::bfloat16, Index=tsl::int32, InitialValueF=tensorflow::functor::Zero<tensorflow::bfloat16>, EmptySegmentValueF=tensorfl
ow::functor::Zero<tensorflow::bfloat16>, ReductionF=tensorflow::functor::Sum]""
E:\20220616_1800pm\_bazel_tensorflow\jsjos6dw\execroot\org_tensorflow\tensorflow\core\kernels\segment_reduction_ops_gpu_0.cu.cc(63): here

.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(151): error: no instance of overloaded function ""tensorflow::max"" matches the argument list
            argument types are: (tsl::int32, tsl::int32)
          detected during:
            instantiation of ""void tensorflow::SegmentMeanNormalizeKernel(SegmentId, Index, const Index *, T *) [with SegmentId=tsl::int32, Index=tsl::int32, T=tensorflow::bfloat16
]""
(166): here
            instantiation of ""tsl::Status tensorflow::LaunchSegmentMeanNormalizeKernel(const tensorflow::GPUDevice &, SegmentId, Index, const Index *, T *) [with SegmentId=tsl::int
32, Index=tsl::int32, T=tensorflow::bfloat16]""
(770): here
            instantiation of ""void tensorflow::functor::SegmentReductionFunctor<T, Index, InitialValueF, EmptySegmentValueF, ReductionF>::operator()(tensorflow::OpKernelContext *,
const tensorflow::functor::GPUDevice &, Index, const tensorflow::TensorShape &, __nv_bool, tensorflow::TTypes<Index, 1, Eigen::DenseIndex>::ConstFlat, Index, const T *, tensorflow:
:TTypes<T, 2, Eigen::DenseIndex>::Tensor) [with T=tensorflow::bfloat16, Index=tsl::int32, InitialValueF=tensorflow::functor::Zero<tensorflow::bfloat16>, EmptySegmentValueF=tensorfl
ow::functor::Zero<tensorflow::bfloat16>, ReductionF=tensorflow::functor::Sum]""
E:\20220616_1800pm\_bazel_tensorflow\jsjos6dw\execroot\org_tensorflow\tensorflow\core\kernels\segment_reduction_ops_gpu_0.cu.cc(63): here

.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(109): error: no instance of overloaded function ""tensorflow::min"" matches the argument list
            argument types are: (tsl::int32, int)
          detected during:
            instantiation of ""void tensorflow::SortedSegmentReductionCustomKernel<T,Index,OuterDimTileSize,ReductionF,AtomicReductionF>(Index, Index, Index, const Index *, const T
*, T *, Index, T) [with T=tensorflow::bfloat16, Index=tsl::int32, OuterDimTileSize=8, ReductionF=tensorflow::NonAtomicProdOpGpu, AtomicReductionF=tensorflow::AtomicProdOpGpu]""
(750): here
            instantiation of ""void tensorflow::functor::SegmentReductionFunctor<T, Index, InitialValueF, EmptySegmentValueF, ReductionF>::operator()(tensorflow::OpKernelContext *,
const tensorflow::functor::GPUDevice &, Index, const tensorflow::TensorShape &, __nv_bool, tensorflow::TTypes<Index, 1, Eigen::DenseIndex>::ConstFlat, Index, const T *, tensorflow:
:TTypes<T, 2, Eigen::DenseIndex>::Tensor) [with T=tensorflow::bfloat16, Index=tsl::int32, InitialValueF=tensorflow::functor::One<tensorflow::bfloat16>, EmptySegmentValueF=tensorflo
w::functor::One<tensorflow::bfloat16>, ReductionF=tensorflow::functor::Prod]""
E:\20220616_1800pm\_bazel_tensorflow\jsjos6dw\execroot\org_tensorflow\tensorflow\core\kernels\segment_reduction_ops_gpu_0.cu.cc(63): here

.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(109): error: no instance of overloaded function ""tensorflow::min"" matches the argument list
            argument types are: (tsl::int32, int)
          detected during:
            instantiation of ""void tensorflow::SortedSegmentReductionCustomKernel<T,Index,OuterDimTileSize,ReductionF,AtomicReductionF>(Index, Index, Index, const Index *, const T
*, T *, Index, T) [with T=tensorflow::bfloat16, Index=tsl::int32, OuterDimTileSize=8, ReductionF=tensorflow::NonAtomicMinOpGpu, AtomicReductionF=tensorflow::AtomicMinOpGpu]""
(750): here
            instantiation of ""void tensorflow::functor::SegmentReductionFunctor<T, Index, InitialValueF, EmptySegmentValueF, ReductionF>::operator()(tensorflow::OpKernelContext *,
const tensorflow::functor::GPUDevice &, Index, const tensorflow::TensorShape &, __nv_bool, tensorflow::TTypes<Index, 1, Eigen::DenseIndex>::ConstFlat, Index, const T *, tensorflow:
:TTypes<T, 2, Eigen::DenseIndex>::Tensor) [with T=tensorflow::bfloat16, Index=tsl::int32, InitialValueF=tensorflow::functor::Highest<tensorflow::bfloat16>, EmptySegmentValueF=tenso
rflow::functor::Zero<tensorflow::bfloat16>, ReductionF=tensorflow::functor::Min]""
E:\20220616_1800pm\_bazel_tensorflow\jsjos6dw\execroot\org_tensorflow\tensorflow\core\kernels\segment_reduction_ops_gpu_0.cu.cc(63): here

.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(109): error: no instance of overloaded function ""tensorflow::min"" matches the argument list
            argument types are: (tsl::int32, int)
          detected during:
            instantiation of ""void tensorflow::SortedSegmentReductionCustomKernel<T,Index,OuterDimTileSize,ReductionF,AtomicReductionF>(Index, Index, Index, const Index *, const T
*, T *, Index, T) [with T=tensorflow::bfloat16, Index=tsl::int32, OuterDimTileSize=8, ReductionF=tensorflow::NonAtomicMaxOpGpu, AtomicReductionF=tensorflow::AtomicMaxOpGpu]""
(750): here
            instantiation of ""void tensorflow::functor::SegmentReductionFunctor<T, Index, InitialValueF, EmptySegmentValueF, ReductionF>::operator()(tensorflow::OpKernelContext *,
const tensorflow::functor::GPUDevice &, Index, const tensorflow::TensorShape &, __nv_bool, tensorflow::TTypes<Index, 1, Eigen::DenseIndex>::ConstFlat, Index, const T *, tensorflow:
:TTypes<T, 2, Eigen::DenseIndex>::Tensor) [with T=tensorflow::bfloat16, Index=tsl::int32, InitialValueF=tensorflow::functor::Lowest<tensorflow::bfloat16>, EmptySegmentValueF=tensor
flow::functor::Zero<tensorflow::bfloat16>, ReductionF=tensorflow::functor::Max]""
E:\20220616_1800pm\_bazel_tensorflow\jsjos6dw\execroot\org_tensorflow\tensorflow\core\kernels\segment_reduction_ops_gpu_0.cu.cc(63): here

.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(109): error: no instance of overloaded function ""tensorflow::min"" matches the argument list
            argument types are: (tsl::int32, int)
          detected during:
            instantiation of ""void tensorflow::SortedSegmentReductionCustomKernel<T,Index,OuterDimTileSize,ReductionF,AtomicReductionF>(Index, Index, Index, const Index *, const T
*, T *, Index, T) [with T=float, Index=tsl::int32, OuterDimTileSize=8, ReductionF=tensorflow::NonAtomicSumOpGpu, AtomicReductionF=tensorflow::AtomicSumOpGpu]""
(750): here
            instantiation of ""void tensorflow::functor::SegmentReductionFunctor<T, Index, InitialValueF, EmptySegmentValueF, ReductionF>::operator()(tensorflow::OpKernelContext *,
const tensorflow::functor::GPUDevice &, Index, const tensorflow::TensorShape &, __nv_bool, tensorflow::TTypes<Index, 1, Eigen::DenseIndex>::ConstFlat, Index, const T *, tensorflow:
:TTypes<T, 2, Eigen::DenseIndex>::Tensor) [with T=float, Index=tsl::int32, InitialValueF=tensorflow::functor::Zero<float>, EmptySegmentValueF=tensorflow::functor::Zero<float>, Redu
ctionF=tensorflow::functor::Sum]""
E:\20220616_1800pm\_bazel_tensorflow\jsjos6dw\execroot\org_tensorflow\tensorflow\core\kernels\segment_reduction_ops_gpu_0.cu.cc(63): here

.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(151): error: no instance of overloaded function ""tensorflow::max"" matches the argument list
            argument types are: (tsl::int32, tsl::int32)
          detected during:
            instantiation of ""void tensorflow::SegmentMeanNormalizeKernel(SegmentId, Index, const Index *, T *) [with SegmentId=tsl::int32, Index=tsl::int32, T=float]""
(166): here
            instantiation of ""tsl::Status tensorflow::LaunchSegmentMeanNormalizeKernel(const tensorflow::GPUDevice &, SegmentId, Index, const Index *, T *) [with SegmentId=tsl::int
32, Index=tsl::int32, T=float]""
(770): here
            instantiation of ""void tensorflow::functor::SegmentReductionFunctor<T, Index, InitialValueF, EmptySegmentValueF, ReductionF>::operator()(tensorflow::OpKernelContext *,
const tensorflow::functor::GPUDevice &, Index, const tensorflow::TensorShape &, __nv_bool, tensorflow::TTypes<Index, 1, Eigen::DenseIndex>::ConstFlat, Index, const T *, tensorflow:
:TTypes<T, 2, Eigen::DenseIndex>::Tensor) [with T=float, Index=tsl::int32, InitialValueF=tensorflow::functor::Zero<float>, EmptySegmentValueF=tensorflow::functor::Zero<float>, Redu
ctionF=tensorflow::functor::Sum]""
E:\20220616_1800pm\_bazel_tensorflow\jsjos6dw\execroot\org_tensorflow\tensorflow\core\kernels\segment_reduction_ops_gpu_0.cu.cc(63): here

.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(109): error: no instance of overloaded function ""tensorflow::min"" matches the argument list
            argument types are: (tsl::int32, int)
          detected during:
            instantiation of ""void tensorflow::SortedSegmentReductionCustomKernel<T,Index,OuterDimTileSize,ReductionF,AtomicReductionF>(Index, Index, Index, const Index *, const T
*, T *, Index, T) [with T=float, Index=tsl::int32, OuterDimTileSize=8, ReductionF=tensorflow::NonAtomicProdOpGpu, AtomicReductionF=tensorflow::AtomicProdOpGpu]""
(750): here
            instantiation of ""void tensorflow::functor::SegmentReductionFunctor<T, Index, InitialValueF, EmptySegmentValueF, ReductionF>::operator()(tensorflow::OpKernelContext *,
const tensorflow::functor::GPUDevice &, Index, const tensorflow::TensorShape &, __nv_bool, tensorflow::TTypes<Index, 1, Eigen::DenseIndex>::ConstFlat, Index, const T *, tensorflow:
:TTypes<T, 2, Eigen::DenseIndex>::Tensor) [with T=float, Index=tsl::int32, InitialValueF=tensorflow::functor::One<float>, EmptySegmentValueF=tensorflow::functor::One<float>, Reduct
ionF=tensorflow::functor::Prod]""
E:\20220616_1800pm\_bazel_tensorflow\jsjos6dw\execroot\org_tensorflow\tensorflow\core\kernels\segment_reduction_ops_gpu_0.cu.cc(63): here

.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(109): error: no instance of overloaded function ""tensorflow::min"" matches the argument list
            argument types are: (tsl::int32, int)
          detected during:
            instantiation of ""void tensorflow::SortedSegmentReductionCustomKernel<T,Index,OuterDimTileSize,ReductionF,AtomicReductionF>(Index, Index, Index, const Index *, const T
*, T *, Index, T) [with T=float, Index=tsl::int32, OuterDimTileSize=8, ReductionF=tensorflow::NonAtomicMinOpGpu, AtomicReductionF=tensorflow::AtomicMinOpGpu]""
(750): here
            instantiation of ""void tensorflow::functor::SegmentReductionFunctor<T, Index, InitialValueF, EmptySegmentValueF, ReductionF>::operator()(tensorflow::OpKernelContext *,
const tensorflow::functor::GPUDevice &, Index, const tensorflow::TensorShape &, __nv_bool, tensorflow::TTypes<Index, 1, Eigen::DenseIndex>::ConstFlat, Index, const T *, tensorflow:
:TTypes<T, 2, Eigen::DenseIndex>::Tensor) [with T=float, Index=tsl::int32, InitialValueF=tensorflow::functor::Highest<float>, EmptySegmentValueF=tensorflow::functor::Zero<float>, R
eductionF=tensorflow::functor::Min]""
E:\20220616_1800pm\_bazel_tensorflow\jsjos6dw\execroot\org_tensorflow\tensorflow\core\kernels\segment_reduction_ops_gpu_0.cu.cc(63): here

.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(54): error: no instance of overloaded function ""tensorflow::min"" matches the argument list
            argument types are: (float, const float)
          detected during:
            instantiation of ""void tensorflow::NonAtomicMinOpGpu::operator()(T *, const T &) [with T=float]""
(125): here
            instantiation of ""void tensorflow::SortedSegmentReductionCustomKernel<T,Index,OuterDimTileSize,ReductionF,AtomicReductionF>(Index, Index, Index, const Index *, const T
*, T *, Index, T) [with T=float, Index=tsl::int32, OuterDimTileSize=8, ReductionF=tensorflow::NonAtomicMinOpGpu, AtomicReductionF=tensorflow::AtomicMinOpGpu]""
(750): here
            instantiation of ""void tensorflow::functor::SegmentReductionFunctor<T, Index, InitialValueF, EmptySegmentValueF, ReductionF>::operator()(tensorflow::OpKernelContext *,
const tensorflow::functor::GPUDevice &, Index, const tensorflow::TensorShape &, __nv_bool, tensorflow::TTypes<Index, 1, Eigen::DenseIndex>::ConstFlat, Index, const T *, tensorflow:
:TTypes<T, 2, Eigen::DenseIndex>::Tensor) [with T=float, Index=tsl::int32, InitialValueF=tensorflow::functor::Highest<float>, EmptySegmentValueF=tensorflow::functor::Zero<float>, R
eductionF=tensorflow::functor::Min]""
E:\20220616_1800pm\_bazel_tensorflow\jsjos6dw\execroot\org_tensorflow\tensorflow\core\kernels\segment_reduction_ops_gpu_0.cu.cc(63): here

.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(109): error: no instance of overloaded function ""tensorflow::min"" matches the argument list
            argument types are: (tsl::int32, int)
          detected during:
            instantiation of ""void tensorflow::SortedSegmentReductionCustomKernel<T,Index,OuterDimTileSize,ReductionF,AtomicReductionF>(Index, Index, Index, const Index *, const T
*, T *, Index, T) [with T=float, Index=tsl::int32, OuterDimTileSize=8, ReductionF=tensorflow::NonAtomicMaxOpGpu, AtomicReductionF=tensorflow::AtomicMaxOpGpu]""
(750): here
            instantiation of ""void tensorflow::functor::SegmentReductionFunctor<T, Index, InitialValueF, EmptySegmentValueF, ReductionF>::operator()(tensorflow::OpKernelContext *,
const tensorflow::functor::GPUDevice &, Index, const tensorflow::TensorShape &, __nv_bool, tensorflow::TTypes<Index, 1, Eigen::DenseIndex>::ConstFlat, Index, const T *, tensorflow:
:TTypes<T, 2, Eigen::DenseIndex>::Tensor) [with T=float, Index=tsl::int32, InitialValueF=tensorflow::functor::Lowest<float>, EmptySegmentValueF=tensorflow::functor::Zero<float>, Re
ductionF=tensorflow::functor::Max]""
E:\20220616_1800pm\_bazel_tensorflow\jsjos6dw\execroot\org_tensorflow\tensorflow\core\kernels\segment_reduction_ops_gpu_0.cu.cc(63): here

.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(53): error: no instance of overloaded function ""tensorflow::max"" matches the argument list
            argument types are: (float, const float)
          detected during:
            instantiation of ""void tensorflow::NonAtomicMaxOpGpu::operator()(T *, const T &) [with T=float]""
(125): here
            instantiation of ""void tensorflow::SortedSegmentReductionCustomKernel<T,Index,OuterDimTileSize,ReductionF,AtomicReductionF>(Index, Index, Index, const Index *, const T
*, T *, Index, T) [with T=float, Index=tsl::int32, OuterDimTileSize=8, ReductionF=tensorflow::NonAtomicMaxOpGpu, AtomicReductionF=tensorflow::AtomicMaxOpGpu]""
(750): here
            instantiation of ""void tensorflow::functor::SegmentReductionFunctor<T, Index, InitialValueF, EmptySegmentValueF, ReductionF>::operator()(tensorflow::OpKernelContext *,
const tensorflow::functor::GPUDevice &, Index, const tensorflow::TensorShape &, __nv_bool, tensorflow::TTypes<Index, 1, Eigen::DenseIndex>::ConstFlat, Index, const T *, tensorflow:
:TTypes<T, 2, Eigen::DenseIndex>::Tensor) [with T=float, Index=tsl::int32, InitialValueF=tensorflow::functor::Lowest<float>, EmptySegmentValueF=tensorflow::functor::Zero<float>, Re
ductionF=tensorflow::functor::Max]""
E:\20220616_1800pm\_bazel_tensorflow\jsjos6dw\execroot\org_tensorflow\tensorflow\core\kernels\segment_reduction_ops_gpu_0.cu.cc(63): here

.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(109): error: no instance of overloaded function ""tensorflow::min"" matches the argument list
            argument types are: (tsl::int32, int)
          detected during:
            instantiation of ""void tensorflow::SortedSegmentReductionCustomKernel<T,Index,OuterDimTileSize,ReductionF,AtomicReductionF>(Index, Index, Index, const Index *, const T
*, T *, Index, T) [with T=double, Index=tsl::int32, OuterDimTileSize=8, ReductionF=tensorflow::NonAtomicSumOpGpu, AtomicReductionF=tensorflow::AtomicSumOpGpu]""
(750): here
            instantiation of ""void tensorflow::functor::SegmentReductionFunctor<T, Index, InitialValueF, EmptySegmentValueF, ReductionF>::operator()(tensorflow::OpKernelContext *,
const tensorflow::functor::GPUDevice &, Index, const tensorflow::TensorShape &, __nv_bool, tensorflow::TTypes<Index, 1, Eigen::DenseIndex>::ConstFlat, Index, const T *, tensorflow:
:TTypes<T, 2, Eigen::DenseIndex>::Tensor) [with T=double, Index=tsl::int32, InitialValueF=tensorflow::functor::Zero<double>, EmptySegmentValueF=tensorflow::functor::Zero<double>, R
eductionF=tensorflow::functor::Sum]""
E:\20220616_1800pm\_bazel_tensorflow\jsjos6dw\execroot\org_tensorflow\tensorflow\core\kernels\segment_reduction_ops_gpu_0.cu.cc(63): here

.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(151): error: no instance of overloaded function ""tensorflow::max"" matches the argument list
            argument types are: (tsl::int32, tsl::int32)
          detected during:
            instantiation of ""void tensorflow::SegmentMeanNormalizeKernel(SegmentId, Index, const Index *, T *) [with SegmentId=tsl::int32, Index=tsl::int32, T=double]""
(166): here
            instantiation of ""tsl::Status tensorflow::LaunchSegmentMeanNormalizeKernel(const tensorflow::GPUDevice &, SegmentId, Index, const Index *, T *) [with SegmentId=tsl::int
32, Index=tsl::int32, T=double]""
(770): here
            instantiation of ""void tensorflow::functor::SegmentReductionFunctor<T, Index, InitialValueF, EmptySegmentValueF, ReductionF>::operator()(tensorflow::OpKernelContext *,
const tensorflow::functor::GPUDevice &, Index, const tensorflow::TensorShape &, __nv_bool, tensorflow::TTypes<Index, 1, Eigen::DenseIndex>::ConstFlat, Index, const T *, tensorflow:
:TTypes<T, 2, Eigen::DenseIndex>::Tensor) [with T=double, Index=tsl::int32, InitialValueF=tensorflow::functor::Zero<double>, EmptySegmentValueF=tensorflow::functor::Zero<double>, R
eductionF=tensorflow::functor::Sum]""
E:\20220616_1800pm\_bazel_tensorflow\jsjos6dw\execroot\org_tensorflow\tensorflow\core\kernels\segment_reduction_ops_gpu_0.cu.cc(63): here

.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(109): error: no instance of overloaded function ""tensorflow::min"" matches the argument list
            argument types are: (tsl::int32, int)
          detected during:
            instantiation of ""void tensorflow::SortedSegmentReductionCustomKernel<T,Index,OuterDimTileSize,ReductionF,AtomicReductionF>(Index, Index, Index, const Index *, const T
*, T *, Index, T) [with T=double, Index=tsl::int32, OuterDimTileSize=8, ReductionF=tensorflow::NonAtomicProdOpGpu, AtomicReductionF=tensorflow::AtomicProdOpGpu]""
(750): here
            instantiation of ""void tensorflow::functor::SegmentReductionFunctor<T, Index, InitialValueF, EmptySegmentValueF, ReductionF>::operator()(tensorflow::OpKernelContext *,
const tensorflow::functor::GPUDevice &, Index, const tensorflow::TensorShape &, __nv_bool, tensorflow::TTypes<Index, 1, Eigen::DenseIndex>::ConstFlat, Index, const T *, tensorflow:
:TTypes<T, 2, Eigen::DenseIndex>::Tensor) [with T=double, Index=tsl::int32, InitialValueF=tensorflow::functor::One<double>, EmptySegmentValueF=tensorflow::functor::One<double>, Red
uctionF=tensorflow::functor::Prod]""
E:\20220616_1800pm\_bazel_tensorflow\jsjos6dw\execroot\org_tensorflow\tensorflow\core\kernels\segment_reduction_ops_gpu_0.cu.cc(63): here

.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(109): error: no instance of overloaded function ""tensorflow::min"" matches the argument list
            argument types are: (tsl::int32, int)
          detected during:
            instantiation of ""void tensorflow::SortedSegmentReductionCustomKernel<T,Index,OuterDimTileSize,ReductionF,AtomicReductionF>(Index, Index, Index, const Index *, const T
*, T *, Index, T) [with T=double, Index=tsl::int32, OuterDimTileSize=8, ReductionF=tensorflow::NonAtomicMinOpGpu, AtomicReductionF=tensorflow::AtomicMinOpGpu]""
(750): here
            instantiation of ""void tensorflow::functor::SegmentReductionFunctor<T, Index, InitialValueF, EmptySegmentValueF, ReductionF>::operator()(tensorflow::OpKernelContext *,
const tensorflow::functor::GPUDevice &, Index, const tensorflow::TensorShape &, __nv_bool, tensorflow::TTypes<Index, 1, Eigen::DenseIndex>::ConstFlat, Index, const T *, tensorflow:
:TTypes<T, 2, Eigen::DenseIndex>::Tensor) [with T=double, Index=tsl::int32, InitialValueF=tensorflow::functor::Highest<double>, EmptySegmentValueF=tensorflow::functor::Zero<double>
, ReductionF=tensorflow::functor::Min]""
E:\20220616_1800pm\_bazel_tensorflow\jsjos6dw\execroot\org_tensorflow\tensorflow\core\kernels\segment_reduction_ops_gpu_0.cu.cc(63): here

.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(54): error: no instance of overloaded function ""tensorflow::min"" matches the argument list
            argument types are: (double, const double)
          detected during:
            instantiation of ""void tensorflow::NonAtomicMinOpGpu::operator()(T *, const T &) [with T=double]""
(125): here
            instantiation of ""void tensorflow::SortedSegmentReductionCustomKernel<T,Index,OuterDimTileSize,ReductionF,AtomicReductionF>(Index, Index, Index, const Index *, const T
*, T *, Index, T) [with T=double, Index=tsl::int32, OuterDimTileSize=8, ReductionF=tensorflow::NonAtomicMinOpGpu, AtomicReductionF=tensorflow::AtomicMinOpGpu]""
(750): here
            instantiation of ""void tensorflow::functor::SegmentReductionFunctor<T, Index, InitialValueF, EmptySegmentValueF, ReductionF>::operator()(tensorflow::OpKernelContext *,
const tensorflow::functor::GPUDevice &, Index, const tensorflow::TensorShape &, __nv_bool, tensorflow::TTypes<Index, 1, Eigen::DenseIndex>::ConstFlat, Index, const T *, tensorflow:
:TTypes<T, 2, Eigen::DenseIndex>::Tensor) [with T=double, Index=tsl::int32, InitialValueF=tensorflow::functor::Highest<double>, EmptySegmentValueF=tensorflow::functor::Zero<double>
, ReductionF=tensorflow::functor::Min]""
E:\20220616_1800pm\_bazel_tensorflow\jsjos6dw\execroot\org_tensorflow\tensorflow\core\kernels\segment_reduction_ops_gpu_0.cu.cc(63): here

.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(109): error: no instance of overloaded function ""tensorflow::min"" matches the argument list
            argument types are: (tsl::int32, int)
          detected during:
            instantiation of ""void tensorflow::SortedSegmentReductionCustomKernel<T,Index,OuterDimTileSize,ReductionF,AtomicReductionF>(Index, Index, Index, const Index *, const T
*, T *, Index, T) [with T=double, Index=tsl::int32, OuterDimTileSize=8, ReductionF=tensorflow::NonAtomicMaxOpGpu, AtomicReductionF=tensorflow::AtomicMaxOpGpu]""
(750): here
            instantiation of ""void tensorflow::functor::SegmentReductionFunctor<T, Index, InitialValueF, EmptySegmentValueF, ReductionF>::operator()(tensorflow::OpKernelContext *,
const tensorflow::functor::GPUDevice &, Index, const tensorflow::TensorShape &, __nv_bool, tensorflow::TTypes<Index, 1, Eigen::DenseIndex>::ConstFlat, Index, const T *, tensorflow:
:TTypes<T, 2, Eigen::DenseIndex>::Tensor) [with T=double, Index=tsl::int32, InitialValueF=tensorflow::functor::Lowest<double>, EmptySegmentValueF=tensorflow::functor::Zero<double>,
 ReductionF=tensorflow::functor::Max]""
E:\20220616_1800pm\_bazel_tensorflow\jsjos6dw\execroot\org_tensorflow\tensorflow\core\kernels\segment_reduction_ops_gpu_0.cu.cc(63): here

.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(53): error: no instance of overloaded function ""tensorflow::max"" matches the argument list
            argument types are: (double, const double)
          detected during:
            instantiation of ""void tensorflow::NonAtomicMaxOpGpu::operator()(T *, const T &) [with T=double]""
(125): here
            instantiation of ""void tensorflow::SortedSegmentReductionCustomKernel<T,Index,OuterDimTileSize,ReductionF,AtomicReductionF>(Index, Index, Index, const Index *, const T
*, T *, Index, T) [with T=double, Index=tsl::int32, OuterDimTileSize=8, ReductionF=tensorflow::NonAtomicMaxOpGpu, AtomicReductionF=tensorflow::AtomicMaxOpGpu]""
(750): here
            instantiation of ""void tensorflow::functor::SegmentReductionFunctor<T, Index, InitialValueF, EmptySegmentValueF, ReductionF>::operator()(tensorflow::OpKernelContext *,
const tensorflow::functor::GPUDevice &, Index, const tensorflow::TensorShape &, __nv_bool, tensorflow::TTypes<Index, 1, Eigen::DenseIndex>::ConstFlat, Index, const T *, tensorflow:
:TTypes<T, 2, Eigen::DenseIndex>::Tensor) [with T=double, Index=tsl::int32, InitialValueF=tensorflow::functor::Lowest<double>, EmptySegmentValueF=tensorflow::functor::Zero<double>,
 ReductionF=tensorflow::functor::Max]""
E:\20220616_1800pm\_bazel_tensorflow\jsjos6dw\execroot\org_tensorflow\tensorflow\core\kernels\segment_reduction_ops_gpu_0.cu.cc(63): here

.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(206): error: no instance of overloaded function ""tensorflow::max"" matches the argument list
            argument types are: (ptrdiff_t, ptrdiff_t)
          detected during:
            instantiation of ""void tensorflow::SegmentOffsetsKernel(Tindex, Tsegmentids, const Tsegmentids *, Tindex *) [with Tindex=int, Tsegmentids=ptrdiff_t]""
(233): here
            instantiation of ""tsl::Status tensorflow::LaunchSegmentOffsetsKernel(const tensorflow::GPUDevice &, Tindex, Tsegmentids, const Tsegmentids *, Tindex *) [with Tindex=int
, Tsegmentids=ptrdiff_t]""
(570): here
            instantiation of ""tsl::Status tensorflow::SegmentReduceGPUImpl<Treducevec,Tvec,Tindex,Tsegmentids,ReduceOp,Tinit>(tensorflow::OpKernelContext *, Tindex, Tindex, Tsegmen
tids, ReduceOp, Tinit, Tinit, __nv_bool, __nv_bool, const Tvec *, const Tsegmentids *, const Tindex *, const Tinit *, Tvec *) [with Treducevec=tensorflow::AlignedVector<float, 8>,
Tvec=tensorflow::AlignedVector<Eigen::half, 8>, Tindex=int, Tsegmentids=ptrdiff_t, ReduceOp=tensorflow::functor::Sum, Tinit=Eigen::half]""
(617): here
            instantiation of ""tsl::Status tensorflow::SegmentReduceGPUVectorized<Treduce>::Impl<vec_size>::operator()(tensorflow::OpKernelContext *, Tindex, Tindex, Tsegmentids, Re
duceOp, T, T, __nv_bool, __nv_bool, const T *, const Tsegmentids *, const Tindex *, const T *, T *) [with Treduce=float, vec_size=8, T=Eigen::half, Tindex=int, Tsegmentids=ptrdiff_
t, ReduceOp=tensorflow::functor::Sum]""
.\tensorflow/core/util/gpu_kernel_helper.h(329): here
            instantiation of ""tsl::Status tensorflow::detail::DispatchToVectorizedHelper<VecSize, Functor>::operator()(int64_t, Args &&...) const [with VecSize=8LL, Functor=tensorf
low::SegmentReduceGPUVectorized<float>::Impl, Args=<tensorflow::OpKernelContext *&, int &, int &, ptrdiff_t &, tensorflow::functor::Sum &, Eigen::half &, Eigen::half &, __nv_bool &
, __nv_bool &, const Eigen::half *&, const int64_t *&, const tsl::int32 *&, const Eigen::half *&, Eigen::half *&>]""
.\tensorflow/core/util/gpu_kernel_helper.h(364): here
            instantiation of ""tsl::Status tensorflow::DispatchToVectorized<T,Functor,Args...>(int64_t, Args &&...) [with T=Eigen::half, Functor=tensorflow::SegmentReduceGPUVectoriz
ed<float>::Impl, Args=<tensorflow::OpKernelContext *&, tsl::int32 &, tsl::int32 &, int64_t &, tensorflow::functor::Sum &, Eigen::half &, Eigen::half &, __nv_bool &, __nv_bool &, co
nst Eigen::half *&, const int64_t *&, const tsl::int32 *&, const Eigen::half *&, Eigen::half *&>]""
(647): here
            instantiation of ""tsl::Status tensorflow::SegmentReduceGPU<Treduce,T,Tindex,Tsegmentids,ReduceOp>(tensorflow::OpKernelContext *, Tindex, Tindex, Tsegmentids, ReduceOp,
T, T, __nv_bool, __nv_bool, const T *, const Tsegmentids *, const Tindex *, const T *, T *) [with Treduce=float, T=Eigen::half, Tindex=tsl::int32, Tsegmentids=int64_t, ReduceOp=ten
sorflow::functor::Sum]""
(904): here
            instantiation of ""tsl::Status tensorflow::functor::SparseSegmentReductionFunctor<T, Index, SegmentId>::operator()(tensorflow::OpKernelContext *, __nv_bool, __nv_bool, T
, tensorflow::TTypes<T, 2, Eigen::DenseIndex>::ConstTensor, tensorflow::TTypes<Index, 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<SegmentId, 1, Eigen::DenseIndex>::ConstVec
, tensorflow::TTypes<T, 2, Eigen::DenseIndex>::Tensor) [with T=Eigen::half, Index=tsl::int32, SegmentId=int64_t]""
E:\20220616_1800pm\_bazel_tensorflow\jsjos6dw\execroot\org_tensorflow\tensorflow\core\kernels\segment_reduction_ops_gpu_0.cu.cc(95): here

.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(206): error: no instance of overloaded function ""tensorflow::min"" matches the argument list
            argument types are: (<error-type>, ptrdiff_t)
          detected during:
            instantiation of ""void tensorflow::SegmentOffsetsKernel(Tindex, Tsegmentids, const Tsegmentids *, Tindex *) [with Tindex=int, Tsegmentids=ptrdiff_t]""
(233): here
            instantiation of ""tsl::Status tensorflow::LaunchSegmentOffsetsKernel(const tensorflow::GPUDevice &, Tindex, Tsegmentids, const Tsegmentids *, Tindex *) [with Tindex=int
, Tsegmentids=ptrdiff_t]""
(570): here
            instantiation of ""tsl::Status tensorflow::SegmentReduceGPUImpl<Treducevec,Tvec,Tindex,Tsegmentids,ReduceOp,Tinit>(tensorflow::OpKernelContext *, Tindex, Tindex, Tsegmen
tids, ReduceOp, Tinit, Tinit, __nv_bool, __nv_bool, const Tvec *, const Tsegmentids *, const Tindex *, const Tinit *, Tvec *) [with Treducevec=tensorflow::AlignedVector<float, 8>,
Tvec=tensorflow::AlignedVector<Eigen::half, 8>, Tindex=int, Tsegmentids=ptrdiff_t, ReduceOp=tensorflow::functor::Sum, Tinit=Eigen::half]""
(617): here
            instantiation of ""tsl::Status tensorflow::SegmentReduceGPUVectorized<Treduce>::Impl<vec_size>::operator()(tensorflow::OpKernelContext *, Tindex, Tindex, Tsegmentids, Re
duceOp, T, T, __nv_bool, __nv_bool, const T *, const Tsegmentids *, const Tindex *, const T *, T *) [with Treduce=float, vec_size=8, T=Eigen::half, Tindex=int, Tsegmentids=ptrdiff_
t, ReduceOp=tensorflow::functor::Sum]""
.\tensorflow/core/util/gpu_kernel_helper.h(329): here
            instantiation of ""tsl::Status tensorflow::detail::DispatchToVectorizedHelper<VecSize, Functor>::operator()(int64_t, Args &&...) const [with VecSize=8LL, Functor=tensorf
low::SegmentReduceGPUVectorized<float>::Impl, Args=<tensorflow::OpKernelContext *&, int &, int &, ptrdiff_t &, tensorflow::functor::Sum &, Eigen::half &, Eigen::half &, __nv_bool &
, __nv_bool &, const Eigen::half *&, const int64_t *&, const tsl::int32 *&, const Eigen::half *&, Eigen::half *&>]""
.\tensorflow/core/util/gpu_kernel_helper.h(364): here
            instantiation of ""tsl::Status tensorflow::DispatchToVectorized<T,Functor,Args...>(int64_t, Args &&...) [with T=Eigen::half, Functor=tensorflow::SegmentReduceGPUVectoriz
ed<float>::Impl, Args=<tensorflow::OpKernelContext *&, tsl::int32 &, tsl::int32 &, int64_t &, tensorflow::functor::Sum &, Eigen::half &, Eigen::half &, __nv_bool &, __nv_bool &, co
nst Eigen::half *&, const int64_t *&, const tsl::int32 *&, const Eigen::half *&, Eigen::half *&>]""
(647): here
            instantiation of ""tsl::Status tensorflow::SegmentReduceGPU<Treduce,T,Tindex,Tsegmentids,ReduceOp>(tensorflow::OpKernelContext *, Tindex, Tindex, Tsegmentids, ReduceOp,
T, T, __nv_bool, __nv_bool, const T *, const Tsegmentids *, const Tindex *, const T *, T *) [with Treduce=float, T=Eigen::half, Tindex=tsl::int32, Tsegmentids=int64_t, ReduceOp=ten
sorflow::functor::Sum]""
(904): here
            instantiation of ""tsl::Status tensorflow::functor::SparseSegmentReductionFunctor<T, Index, SegmentId>::operator()(tensorflow::OpKernelContext *, __nv_bool, __nv_bool, T
, tensorflow::TTypes<T, 2, Eigen::DenseIndex>::ConstTensor, tensorflow::TTypes<Index, 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<SegmentId, 1, Eigen::DenseIndex>::ConstVec
, tensorflow::TTypes<T, 2, Eigen::DenseIndex>::Tensor) [with T=Eigen::half, Index=tsl::int32, SegmentId=int64_t]""
E:\20220616_1800pm\_bazel_tensorflow\jsjos6dw\execroot\org_tensorflow\tensorflow\core\kernels\segment_reduction_ops_gpu_0.cu.cc(95): here

.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(657): error: no instance of overloaded function ""tensorflow::max"" matches the argument list
            argument types are: (tsl::int32, tsl::int32)
          detected during:
            instantiation of ""void tensorflow::SegmentWeightsKernel(SegmentId, tensorflow::SparseSegmentReductionOperation, const Index *, T *) [with SegmentId=tsl::int32, Index=ts
l::int32, T=Eigen::half]""
(674): here
            instantiation of ""tsl::Status tensorflow::LaunchSegmentWeightsKernel(const tensorflow::GPUDevice &, SegmentId, tensorflow::SparseSegmentReductionOperation, const Index
*, T *) [with SegmentId=tsl::int32, Index=tsl::int32, T=Eigen::half]""
(941): here
            instantiation of ""void tensorflow::functor::SparseSegmentGradFunctor<tensorflow::functor::GPUDevice, T, Index, SegmentId>::operator()(tensorflow::OpKernelContext *, ten
sorflow::SparseSegmentReductionOperation, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix, tensorflow::TTypes<Index, 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<Se
gmentId, 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Matrix) [with T=Eigen::half, Index=tsl::int32, SegmentId=tsl::int32]""
E:\20220616_1800pm\_bazel_tensorflow\jsjos6dw\execroot\org_tensorflow\tensorflow\core\kernels\segment_reduction_ops_gpu_0.cu.cc(101): here

.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(657): error: no instance of overloaded function ""tensorflow::max"" matches the argument list
            argument types are: (tsl::int32, tsl::int32)
          detected during:
            instantiation of ""void tensorflow::SegmentWeightsKernel(SegmentId, tensorflow::SparseSegmentReductionOperation, const Index *, T *) [with SegmentId=int64_t, Index=tsl::
int32, T=Eigen::half]""
(674): here
            instantiation of ""tsl::Status tensorflow::LaunchSegmentWeightsKernel(const tensorflow::GPUDevice &, SegmentId, tensorflow::SparseSegmentReductionOperation, const Index
*, T *) [with SegmentId=int64_t, Index=tsl::int32, T=Eigen::half]""
(941): here
            instantiation of ""void tensorflow::functor::SparseSegmentGradFunctor<tensorflow::functor::GPUDevice, T, Index, SegmentId>::operator()(tensorflow::OpKernelContext *, ten
sorflow::SparseSegmentReductionOperation, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix, tensorflow::TTypes<Index, 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<Se
gmentId, 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Matrix) [with T=Eigen::half, Index=tsl::int32, SegmentId=int64_t]""
E:\20220616_1800pm\_bazel_tensorflow\jsjos6dw\execroot\org_tensorflow\tensorflow\core\kernels\segment_reduction_ops_gpu_0.cu.cc(101): here

.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(206): error: no instance of overloaded function ""tensorflow::max"" matches the argument list
            argument types are: (int, int)
          detected during:
            instantiation of ""void tensorflow::SegmentOffsetsKernel(Tindex, Tsegmentids, const Tsegmentids *, Tindex *) [with Tindex=ptrdiff_t, Tsegmentids=int]""
(233): here
            instantiation of ""tsl::Status tensorflow::LaunchSegmentOffsetsKernel(const tensorflow::GPUDevice &, Tindex, Tsegmentids, const Tsegmentids *, Tindex *) [with Tindex=ptr
diff_t, Tsegmentids=int]""
(570): here
            instantiation of ""tsl::Status tensorflow::SegmentReduceGPUImpl<Treducevec,Tvec,Tindex,Tsegmentids,ReduceOp,Tinit>(tensorflow::OpKernelContext *, Tindex, Tindex, Tsegmen
tids, ReduceOp, Tinit, Tinit, __nv_bool, __nv_bool, const Tvec *, const Tsegmentids *, const Tindex *, const Tinit *, Tvec *) [with Treducevec=tensorflow::AlignedVector<Eigen::half
, 8>, Tvec=tensorflow::AlignedVector<Eigen::half, 8>, Tindex=ptrdiff_t, Tsegmentids=int, ReduceOp=cub::Sum, Tinit=Eigen::half]""
(617): here
            instantiation of ""tsl::Status tensorflow::SegmentReduceGPUVectorized<Treduce>::Impl<vec_size>::operator()(tensorflow::OpKernelContext *, Tindex, Tindex, Tsegmentids, Re
duceOp, T, T, __nv_bool, __nv_bool, const T *, const Tsegmentids *, const Tindex *, const T *, T *) [with Treduce=Eigen::half, vec_size=8, T=Eigen::half, Tindex=ptrdiff_t, Tsegment
ids=int, ReduceOp=cub::Sum]""
.\tensorflow/core/util/gpu_kernel_helper.h(329): here
            instantiation of ""tsl::Status tensorflow::detail::DispatchToVectorizedHelper<VecSize, Functor>::operator()(int64_t, Args &&...) const [with VecSize=8LL, Functor=tensorf
low::SegmentReduceGPUVectorized<Eigen::half>::Impl, Args=<tensorflow::OpKernelContext *&, ptrdiff_t &, ptrdiff_t &, int &, cub::Sum &, Eigen::half &, Eigen::half &, __nv_bool &, __
nv_bool &, const Eigen::half *&, const tsl::int32 *&, const int64_t *&, const Eigen::half *&, Eigen::half *&>]""
.\tensorflow/core/util/gpu_kernel_helper.h(364): here
            instantiation of ""tsl::Status tensorflow::DispatchToVectorized<T,Functor,Args...>(int64_t, Args &&...) [with T=Eigen::half, Functor=tensorflow::SegmentReduceGPUVectoriz
ed<Eigen::half>::Impl, Args=<tensorflow::OpKernelContext *&, int64_t &, int64_t &, tsl::int32 &, cub::Sum &, Eigen::half &, Eigen::half &, __nv_bool &, __nv_bool &, const Eigen::ha
lf *&, const tsl::int32 *&, const int64_t *&, const Eigen::half *&, Eigen::half *&>]""
(647): here
            instantiation of ""tsl::Status tensorflow::SegmentReduceGPU<Treduce,T,Tindex,Tsegmentids,ReduceOp>(tensorflow::OpKernelContext *, Tindex, Tindex, Tsegmentids, ReduceOp,
T, T, __nv_bool, __nv_bool, const T *, const Tsegmentids *, const Tindex *, const T *, T *) [with Treduce=Eigen::half, T=Eigen::half, Tindex=int64_t, Tsegmentids=tsl::int32, Reduce
Op=cub::Sum]""
(976): here
            instantiation of ""void tensorflow::functor::SparseSegmentGradFunctor<tensorflow::functor::GPUDevice, T, Index, SegmentId>::operator()(tensorflow::OpKernelContext *, ten
sorflow::SparseSegmentReductionOperation, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix, tensorflow::TTypes<Index, 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<Se
gmentId, 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Matrix) [with T=Eigen::half, Index=tsl::int32, SegmentId=int64_t]""
E:\20220616_1800pm\_bazel_tensorflow\jsjos6dw\execroot\org_tensorflow\tensorflow\core\kernels\segment_reduction_ops_gpu_0.cu.cc(101): here

.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(206): error: no instance of overloaded function ""tensorflow::min"" matches the argument list
            argument types are: (<error-type>, int)
          detected during:
            instantiation of ""void tensorflow::SegmentOffsetsKernel(Tindex, Tsegmentids, const Tsegmentids *, Tindex *) [with Tindex=ptrdiff_t, Tsegmentids=int]""
(233): here
            instantiation of ""tsl::Status tensorflow::LaunchSegmentOffsetsKernel(const tensorflow::GPUDevice &, Tindex, Tsegmentids, const Tsegmentids *, Tindex *) [with Tindex=ptr
diff_t, Tsegmentids=int]""
(570): here
            instantiation of ""tsl::Status tensorflow::SegmentReduceGPUImpl<Treducevec,Tvec,Tindex,Tsegmentids,ReduceOp,Tinit>(tensorflow::OpKernelContext *, Tindex, Tindex, Tsegmen
tids, ReduceOp, Tinit, Tinit, __nv_bool, __nv_bool, const Tvec *, const Tsegmentids *, const Tindex *, const Tinit *, Tvec *) [with Treducevec=tensorflow::AlignedVector<Eigen::half
, 8>, Tvec=tensorflow::AlignedVector<Eigen::half, 8>, Tindex=ptrdiff_t, Tsegmentids=int, ReduceOp=cub::Sum, Tinit=Eigen::half]""
(617): here
            instantiation of ""tsl::Status tensorflow::SegmentReduceGPUVectorized<Treduce>::Impl<vec_size>::operator()(tensorflow::OpKernelContext *, Tindex, Tindex, Tsegmentids, Re
duceOp, T, T, __nv_bool, __nv_bool, const T *, const Tsegmentids *, const Tindex *, const T *, T *) [with Treduce=Eigen::half, vec_size=8, T=Eigen::half, Tindex=ptrdiff_t, Tsegment
ids=int, ReduceOp=cub::Sum]""
.\tensorflow/core/util/gpu_kernel_helper.h(329): here
            instantiation of ""tsl::Status tensorflow::detail::DispatchToVectorizedHelper<VecSize, Functor>::operator()(int64_t, Args &&...) const [with VecSize=8LL, Functor=tensorf
low::SegmentReduceGPUVectorized<Eigen::half>::Impl, Args=<tensorflow::OpKernelContext *&, ptrdiff_t &, ptrdiff_t &, int &, cub::Sum &, Eigen::half &, Eigen::half &, __nv_bool &, __
nv_bool &, const Eigen::half *&, const tsl::int32 *&, const int64_t *&, const Eigen::half *&, Eigen::half *&>]""
.\tensorflow/core/util/gpu_kernel_helper.h(364): here
            instantiation of ""tsl::Status tensorflow::DispatchToVectorized<T,Functor,Args...>(int64_t, Args &&...) [with T=Eigen::half, Functor=tensorflow::SegmentReduceGPUVectoriz
ed<Eigen::half>::Impl, Args=<tensorflow::OpKernelContext *&, int64_t &, int64_t &, tsl::int32 &, cub::Sum &, Eigen::half &, Eigen::half &, __nv_bool &, __nv_bool &, const Eigen::ha
lf *&, const tsl::int32 *&, const int64_t *&, const Eigen::half *&, Eigen::half *&>]""
(647): here
            instantiation of ""tsl::Status tensorflow::SegmentReduceGPU<Treduce,T,Tindex,Tsegmentids,ReduceOp>(tensorflow::OpKernelContext *, Tindex, Tindex, Tsegmentids, ReduceOp,
T, T, __nv_bool, __nv_bool, const T *, const Tsegmentids *, const Tindex *, const T *, T *) [with Treduce=Eigen::half, T=Eigen::half, Tindex=int64_t, Tsegmentids=tsl::int32, Reduce
Op=cub::Sum]""
(976): here
            instantiation of ""void tensorflow::functor::SparseSegmentGradFunctor<tensorflow::functor::GPUDevice, T, Index, SegmentId>::operator()(tensorflow::OpKernelContext *, ten
sorflow::SparseSegmentReductionOperation, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix, tensorflow::TTypes<Index, 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<Se
gmentId, 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Matrix) [with T=Eigen::half, Index=tsl::int32, SegmentId=int64_t]""
E:\20220616_1800pm\_bazel_tensorflow\jsjos6dw\execroot\org_tensorflow\tensorflow\core\kernels\segment_reduction_ops_gpu_0.cu.cc(101): here

.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(657): error: no instance of overloaded function ""tensorflow::max"" matches the argument list
            argument types are: (tsl::int32, tsl::int32)
          detected during:
            instantiation of ""void tensorflow::SegmentWeightsKernel(SegmentId, tensorflow::SparseSegmentReductionOperation, const Index *, T *) [with SegmentId=tsl::int32, Index=ts
l::int32, T=tensorflow::bfloat16]""
(674): here
            instantiation of ""tsl::Status tensorflow::LaunchSegmentWeightsKernel(const tensorflow::GPUDevice &, SegmentId, tensorflow::SparseSegmentReductionOperation, const Index
*, T *) [with SegmentId=tsl::int32, Index=tsl::int32, T=tensorflow::bfloat16]""
(941): here
            instantiation of ""void tensorflow::functor::SparseSegmentGradFunctor<tensorflow::functor::GPUDevice, T, Index, SegmentId>::operator()(tensorflow::OpKernelContext *, ten
sorflow::SparseSegmentReductionOperation, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix, tensorflow::TTypes<Index, 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<Se
gmentId, 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Matrix) [with T=tensorflow::bfloat16, Index=tsl::int32, SegmentId=tsl::int32]""
E:\20220616_1800pm\_bazel_tensorflow\jsjos6dw\execroot\org_tensorflow\tensorflow\core\kernels\segment_reduction_ops_gpu_0.cu.cc(101): here

.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(657): error: no instance of overloaded function ""tensorflow::max"" matches the argument list
            argument types are: (tsl::int32, tsl::int32)
          detected during:
            instantiation of ""void tensorflow::SegmentWeightsKernel(SegmentId, tensorflow::SparseSegmentReductionOperation, const Index *, T *) [with SegmentId=int64_t, Index=tsl::
int32, T=tensorflow::bfloat16]""
(674): here
            instantiation of ""tsl::Status tensorflow::LaunchSegmentWeightsKernel(const tensorflow::GPUDevice &, SegmentId, tensorflow::SparseSegmentReductionOperation, const Index
*, T *) [with SegmentId=int64_t, Index=tsl::int32, T=tensorflow::bfloat16]""
(941): here
            instantiation of ""void tensorflow::functor::SparseSegmentGradFunctor<tensorflow::functor::GPUDevice, T, Index, SegmentId>::operator()(tensorflow::OpKernelContext *, ten
sorflow::SparseSegmentReductionOperation, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix, tensorflow::TTypes<Index, 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<Se
gmentId, 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Matrix) [with T=tensorflow::bfloat16, Index=tsl::int32, SegmentId=int64_t]""
E:\20220616_1800pm\_bazel_tensorflow\jsjos6dw\execroot\org_tensorflow\tensorflow\core\kernels\segment_reduction_ops_gpu_0.cu.cc(101): here

.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(657): error: no instance of overloaded function ""tensorflow::max"" matches the argument list
            argument types are: (tsl::int32, tsl::int32)
          detected during:
            instantiation of ""void tensorflow::SegmentWeightsKernel(SegmentId, tensorflow::SparseSegmentReductionOperation, const Index *, T *) [with SegmentId=tsl::int32, Index=ts
l::int32, T=float]""
(674): here
            instantiation of ""tsl::Status tensorflow::LaunchSegmentWeightsKernel(const tensorflow::GPUDevice &, SegmentId, tensorflow::SparseSegmentReductionOperation, const Index
*, T *) [with SegmentId=tsl::int32, Index=tsl::int32, T=float]""
(941): here
            instantiation of ""void tensorflow::functor::SparseSegmentGradFunctor<tensorflow::functor::GPUDevice, T, Index, SegmentId>::operator()(tensorflow::OpKernelContext *, ten
sorflow::SparseSegmentReductionOperation, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix, tensorflow::TTypes<Index, 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<Se
gmentId, 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Matrix) [with T=float, Index=tsl::int32, SegmentId=tsl::int32]""
E:\20220616_1800pm\_bazel_tensorflow\jsjos6dw\execroot\org_tensorflow\tensorflow\core\kernels\segment_reduction_ops_gpu_0.cu.cc(101): here

.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(657): error: no instance of overloaded function ""tensorflow::max"" matches the argument list
            argument types are: (tsl::int32, tsl::int32)
          detected during:
            instantiation of ""void tensorflow::SegmentWeightsKernel(SegmentId, tensorflow::SparseSegmentReductionOperation, const Index *, T *) [with SegmentId=int64_t, Index=tsl::
int32, T=float]""
(674): here
            instantiation of ""tsl::Status tensorflow::LaunchSegmentWeightsKernel(const tensorflow::GPUDevice &, SegmentId, tensorflow::SparseSegmentReductionOperation, const Index
*, T *) [with SegmentId=int64_t, Index=tsl::int32, T=float]""
(941): here
            instantiation of ""void tensorflow::functor::SparseSegmentGradFunctor<tensorflow::functor::GPUDevice, T, Index, SegmentId>::operator()(tensorflow::OpKernelContext *, ten
sorflow::SparseSegmentReductionOperation, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix, tensorflow::TTypes<Index, 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<Se
gmentId, 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Matrix) [with T=float, Index=tsl::int32, SegmentId=int64_t]""
E:\20220616_1800pm\_bazel_tensorflow\jsjos6dw\execroot\org_tensorflow\tensorflow\core\kernels\segment_reduction_ops_gpu_0.cu.cc(101): here

.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(657): error: no instance of overloaded function ""tensorflow::max"" matches the argument list
            argument types are: (tsl::int32, tsl::int32)
          detected during:
            instantiation of ""void tensorflow::SegmentWeightsKernel(SegmentId, tensorflow::SparseSegmentReductionOperation, const Index *, T *) [with SegmentId=tsl::int32, Index=ts
l::int32, T=double]""
(674): here
            instantiation of ""tsl::Status tensorflow::LaunchSegmentWeightsKernel(const tensorflow::GPUDevice &, SegmentId, tensorflow::SparseSegmentReductionOperation, const Index
*, T *) [with SegmentId=tsl::int32, Index=tsl::int32, T=double]""
(941): here
            instantiation of ""void tensorflow::functor::SparseSegmentGradFunctor<tensorflow::functor::GPUDevice, T, Index, SegmentId>::operator()(tensorflow::OpKernelContext *, ten
sorflow::SparseSegmentReductionOperation, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix, tensorflow::TTypes<Index, 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<Se
gmentId, 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Matrix) [with T=double, Index=tsl::int32, SegmentId=tsl::int32]""
E:\20220616_1800pm\_bazel_tensorflow\jsjos6dw\execroot\org_tensorflow\tensorflow\core\kernels\segment_reduction_ops_gpu_0.cu.cc(101): here

.\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(657): error: no instance of overloaded function ""tensorflow::max"" matches the argument list
            argument types are: (tsl::int32, tsl::int32)
```
</details>"
60395,Tensorflow GPU segfaults on M1 mac,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

M1 mac, Ventura OS

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Fatal Python error: Segmentation fault. 

### Standalone code to reproduce the issue

```shell
# Download script
curl https://raw.githubusercontent.com/abreheret/tensorflow-models/master/tutorials/image/mnist/convolutional.py -o model.py

# Make script compatible with Tensorflow 2.0
sed -i 's/import tensorflow as tf/import tensorflow.compat.v1 as tf\ntf.disable_eager_execution()/g' model.py

# Run script
python model.py
```


### Relevant log output

```shell
Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.
Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.
Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.
Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
Metal device set to: Apple M1

systemMemory: 16.00 GB
maxCacheSize: 5.33 GB

2023-04-21 23:54:30.086459: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
Initialized!
Fatal Python error: Segmentation fault

Thread 0x00000001eb19a500 (most recent call first):
  File ""/Users/ashok/miniconda/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1454 in _call_tf_sessionrun
  File ""/Users/ashok/miniconda/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1361 in _run_fn
  File ""/Users/ashok/miniconda/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1378 in _do_call
  File ""/Users/ashok/miniconda/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1371 in _do_run
  File ""/Users/ashok/miniconda/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1191 in _run
  File ""/Users/ashok/miniconda/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 968 in run
  File ""/Users/ashok/Desktop/model.py"", line 314 in main
  File ""/Users/ashok/miniconda/lib/python3.10/site-packages/absl/app.py"", line 254 in _run_main
  File ""/Users/ashok/miniconda/lib/python3.10/site-packages/absl/app.py"", line 308 in run
  File ""/Users/ashok/miniconda/lib/python3.10/site-packages/tensorflow/python/platform/app.py"", line 36 in run
  File ""/Users/ashok/Desktop/model.py"", line 353 in <module>

Extension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, google._upb._message, tensorflow.python.framework.fast_tensor_util, _cffi_backend, h5py._errors, h5py.defs, h5py._objects, h5py.h5, h5py.h5r, h5py.utils, h5py.h5s, h5py.h5ac, h5py.h5p, h5py.h5t, h5py._conv, h5py.h5z, h5py._proxy, h5py.h5a, h5py.h5d, h5py.h5ds, h5py.h5g, h5py.h5i, h5py.h5f, h5py.h5fd, h5py.h5pl, h5py.h5o, h5py.h5l, h5py._selector, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._isolve._iterative, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg._cythonized_array_utils, scipy.linalg._flinalg, scipy.linalg._solve_toeplitz, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_lapack, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, PIL._imaging, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pandas._libs.hashing, pandas._libs.tslib, pandas._libs.ops, pandas._libs.arrays, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.index, pandas._libs.internals, pandas._libs.join, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, scipy.ndimage._nd_image, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, _ni_label, scipy.ndimage._ni_label (total: 114)
fish: Job 1, 'python ~/Desktop/model.py' terminated by signal SIGSEGV (Address boundary error)
```
</details>"
60394,ValueError: Unexpected result of `predict_function` (Empty batch_outputs),"I have the below model I'm working on. The intention is to forecast the 'Index' field based on the impacts from the fields A,B,C and D. The Date field is of the type 'MM/DD/YYYY'

--------------------------------------------------------------------------------------------------------------------------------------------------
`#Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense, LSTM

#Load the dataset
df = pd.read_csv('New Final.csv', usecols=['Date', 'Index', 'A', 'B', 'C', 'D'])
df = df.sort_values('Date')
df = df.set_index('Date')

#Normalize the data using MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(df)

#Split data into training and testing sets
training_data_len = int(len(scaled_data) * 0.8)
train_data = scaled_data[0:training_data_len, :]
test_data = scaled_data[training_data_len:, :]

#Prepare the data for training
def create_dataset(dataset, time_step=1):
    data_X, data_y = [], []
    for i in range(len(dataset) - time_step):
        a = dataset[i:(i + time_step), :]
        data_X.append(a)
        data_y.append(dataset[i + time_step, 0])
    return np.array(data_X), np.array(data_y)

time_steps = 60
X_train, y_train = create_dataset(train_data, time_steps)
X_test, y_test = create_dataset(test_data, time_steps)

#Build the model
model = Sequential()
model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))
model.add(LSTM(units=50, return_sequences=True))
model.add(LSTM(units=50))
model.add(Dense(units=1))

#Compile the model
#model.compile(optimizer='adam', loss='mean_squared_error')
model.compile(loss='mean_squared_error', optimizer='adam', run_eagerly=True)


#Train the model
model.fit(X_train, y_train, epochs=100, batch_size=32)

#Evaluate the model
train_predictions = model.predict(X_train)
test_predictions = model.predict(X_test, batch_size=1)

#Convert the predictions back to original form
train_predictions = scaler.inverse_transform(train_predictions)
y_train = scaler.inverse_transform([y_train])
test_predictions = scaler.inverse_transform(test_predictions)
y_test = scaler.inverse_transform([y_test])

#Create a dataframe to store the predictions
train_predict_df = pd.DataFrame(train_predictions, columns=['IndexI'], index=df.iloc[time_steps:training_data_len, :].index)
test_predict_df = pd.DataFrame(test_predictions, columns=['Index'], index=df.iloc[training_data_len+time_steps:-1, :].index)

#Merge the predicted values with the original dataset
df_train_predict = pd.concat([df.iloc[time_steps:training_data_len, :], train_predict_df], axis=1)
df_test_predict = pd.concat([df.iloc[training_data_len+time_steps:-1, :], test_predict_df], axis=1)

#Import the necessary libraries
import matplotlib.pyplot as plt

#Define the x-axis labels
x_labels = []
for year in range(2009, 2023):
    for week in range(1, 53):
        x_labels.append(str(year) + '-W' + str(week))

#Create the plot
plt.plot(df['Index'].values, color='purple')
plt.plot(df_train_predict['Index'], color='green')
plt.plot(df_test_predict['Index'], color='yellow')

#Set the x-axis labels
plt.xticks(np.arange(0, len(df), 52), x_labels[::52], rotation=90)

#Set the plot title and axis labels
plt.title('Actual vs. Predicted Index Values')
plt.xlabel('Weeks of the Year')
plt.ylabel('SCFI Values')

#Display the plot
plt.show()`

--------------------------------------------------------------------------------------------------------------------------------------------------

The training outcomes I get is below.

![Outcomes](https://user-images.githubusercontent.com/65387688/233699866-4e3ea9ee-e257-4afa-8889-83c7d0f00f12.PNG)

Im getting the following error when Im trying to predict the model,
![Error](https://user-images.githubusercontent.com/65387688/233701783-a8a632cc-d9a3-4b22-9f6c-9c9db9448022.PNG)

Please tell me the possible reason for the above error and how can I fix it?


"
60392,[MSVC]Tensorflow build process takes 11 hours with 2589785 commit,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

master branch, commit: 2589785

### Custom Code

No

### OS Platform and Distribution

Windows Server 2022

### Mobile device

_No response_

### Python version

3.9

### Bazel version

5.3.0

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

It took 11 hours to build this project with 2589785 commits, however with 160f2bd, it took 6 hours.

**2589785 Commit Build log:**
INFO: Build completed successfully, 34131 total actions
INFO: Build completed successfully, 34131 total actions
##[debug] Command #6 exited with code [0].
 2023-04-12T07:34:32.2365460-07:00  Command ran in 11 hours, 17 minutes, 46 seconds

**160f2bd Commit Build log:**
INFO: Build completed successfully, 16950 total actions
INFO: Build completed successfully, 16950 total actions
##[debug] Command #6 exited with code [0].
 2023-04-14T23:52:20.2671170-07:00  Command ran in 5 hours, 27 minutes, 51 seconds

### Standalone code to reproduce the issue

```shell
git clone https://github.com/tensorflow/tensorflow.git F:\gitP\tensorflow\tensorflow
cd F:\gitP\tensorflow\tensorflow
git reset --hard 2589785
pip3 install -r tensorflow/tools/ci_build/release/requirements_common.txt 2>&1
set PATH=F:\gitP\tensorflow\tensorflow\..\tools;%path%
set PATH=F:\gitP\tensorflow\tensorflow\..\tools\msys64\usr\bin;%path%
yes """" 2>nul | python ./configure.py 2>&1
set BAZEL_VC=C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC
set BAZEL_VC_FULL_VERSION=14.35.32215
set PATH=F:\gitP\tensorflow\tensorflow\..\tools;%path%
set PATH=F:\gitP\tensorflow\tensorflow\..\tools\msys64\usr\bin;%path%
bazel --output_user_root F:\bazelTemp build --jobs 8 --config=opt --local_ram_resources=2048 --subcommands //tensorflow/tools/pip_package:build_pip_package 2>&1
```


### Relevant log output

_No response_</details>"
60391,Unequal strides support recently removed for DepthwiseConv2D,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**: 2.9 and 2.11
-   **Python version**: 3.8
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**: 11.2 and 8.1
-   **GPU model and memory**:
-   **Exact command to reproduce**:

import tensorflow as tf
import numpy as np
layer1 = tf.keras.layers.DepthwiseConv2D(depth_multiplier=2,kernel_size=(1,9),strides=(1,2))
print(layer1(np.ones((100, 28, 28, 1), dtype=np.float32)))

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

The command would run without any error when I was using it with TF2.11 version installed previously. On the newly installed TF2.9 version, however, the code throws an error as:
InvalidArgumentError: Exception encountered when calling layer 'depthwise_conv2d_1' (type DepthwiseConv2D).
{{function_node __wrapped__DepthwiseConv2dNative_device_/job:localhost/replica:0/task:0/device:CPU:0}} Current implementation only supports equal length strides in the row and column dimensions. [Op:DepthwiseConv2dNative]
Call arguments received by layer 'depthwise_conv2d_1' (type DepthwiseConv2D):
  • inputs=tf.Tensor(shape=(100, 28, 28, 1), dtype=float32)

Seeing the documentation, the error is expected for version 2.12, but should work for versions 2.9 and 2.11
https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/layers/DepthwiseConv2D
https://www.tensorflow.org/versions/r2.11/api_docs/python/tf/keras/layers/DepthwiseConv2D
https://www.tensorflow.org/versions/r2.12/api_docs/python/tf/keras/layers/DepthwiseConv2D

Is it that the implementation existing in the earlier versions has been removed recently for all previous and current versions?

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
60389,Tensorflow won't let pytorch import,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Docker image ubuntu:20.04 and above

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Tensorflow 2.12.0 won't let torch import, causes some sort of deadlock, if tensorflow is imported first

### Standalone code to reproduce the issue

```shell
Code to reproduce the error:

docker run --rm -it ubuntu:22.04
// once inside
apt-get update && apt-get install python3
apt-get install pip
pip install tensorflow -y
pip install torch --extra-index-url https://download.pytorch.org/whl/cu118 -y
python3
import tensorflow
import torch
```
Note: the issue will persist even if you install torch-cpu, also on any Ubuntu:20.04 and above
```


### Relevant log output

_No response_</details>"
60387,Compile tensorflow with static cuda libs,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

No

### OS Platform and Distribution

Centos &

### Mobile device

_No response_

### Python version

Python-3.9.16

### Bazel version

bazel-5.3.0

### GCC/Compiler version

9.3.1 

### CUDA/cuDNN version

11.2

### GPU model and memory

Quadro RTX 4000

### Current Behaviour?

I am trying to link TF with cuda static libs (cublas, cufft, cusparse etc). I have made some changes in the following files:
1. tensorflow/lite/toco/BUILD b/tensorflow/lite/toco/BUILD
2. tensorflow/tensorflow.bzl
3. tensorflow/compiler/mlir/tools/kernel_gen/BUILD
4. third_party/gpus/cuda_configure.bzl
In the first 3 files I have added inkopts = if_not_windows([""-lm"", ""-Wl,-ldl""]) + lrt_if_needed() + [""-L/usr/local/cuda/lib64"", ""-L/usr/local/cuda/extras/CUPTI/lib64"", ""-lcuda"", ""-lcudart"", ""-lcublas"",""-lcublasLt"",""-lculibos"",""-lcufft"", ""-lcudnn"", ""-lcurand"", ""-lcupti"", ""-lcusolver"", ""-lcusparse""],
In the fourth I have changed static = False to static = True. 

### Standalone code to reproduce the issue

```shell
https://drive.google.com/file/d/1vGyJD-1OWWlE2tYu4qbglMklSfep6x4T/view?usp=share_link
```


### Relevant log output

```shell
bazel build //tensorflow/tools/pip_package:build_pip_package 
$TEST_TMPDIR defined: output root default is '/tmp/bazel_manospavl' and max_idle_secs default is '15'.
$TEST_TMPDIR defined: output root default is '/tmp/bazel_manospavl' and max_idle_secs default is '15'.
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=104
INFO: Reading rc options for 'build' from /tmp/manospavl/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /tmp/manospavl/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false
INFO: Reading rc options for 'build' from /tmp/manospavl/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3.9 --action_env PYTHON_LIB_PATH=/opt/rh/llvm-toolset-7/root/usr/lib/python2.7/site-packages --python_path=/usr/local/bin/python3.9 --action_env PYTHONPATH=/opt/rh/devtoolset-8/root/usr/lib64/python2.7/site-packages:/opt/rh/devtoolset-8/root/usr/lib/python2.7/site-packages:/opt/rh/llvm-toolset-7/root/usr/lib/python2.7/site-packages --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=7.5 --action_env LD_LIBRARY_PATH=/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/opt/rh/devtoolset-9/root/usr/lib64/dyninst:/opt/rh/devtoolset-9/root/usr/lib/dyninst:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/archive/users/gxanth/gcc-12.1.0/lib64:/opt/altera/intelFPGA/20.3/hld/board/de5a_net_ddr4/linux64/lib:/opt/altera/intelFPGA/20.3/hld/host/linux64/lib:/opt/altera/intelFPGA/20.3/hld/board/de5a_net_ddr4/tests/extlibs/lib:/usr/local/cuda-11.2/extras/CUPTI/lib64:/usr/local/cuda-11.2/lib64:/usr/lib64/atlas:/usr/local/lib:/lib64:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/opt/rh/devtoolset-9/root/usr/lib64/dyninst:/opt/rh/devtoolset-9/root/usr/lib/dyninst:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/archive/users/gxanth/gcc-12.1.0/lib64:/opt/altera/intelFPGA/20.3/hld/board/de5a_net_ddr4/linux64/lib:/opt/altera/intelFPGA/20.3/hld/host/linux64/lib:/opt/altera/intelFPGA/20.3/hld/board/de5a_net_ddr4/tests/extlibs/lib:/usr/local/cuda-11.2/extras/CUPTI/lib64:/usr/local/cuda-11.2/lib64:/usr/lib64/atlas:/usr/local/lib:/lib64:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/opt/rh/devtoolset-9/root/usr/lib64/dyninst:/opt/rh/devtoolset-9/root/usr/lib/dyninst:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/archive/users/gxanth/gcc-12.1.0/lib64:/opt/altera/intelFPGA/20.3/hld/board/de5a_net_ddr4/linux64/lib:/opt/altera/intelFPGA/20.3/hld/host/linux64/lib:/opt/altera/intelFPGA/20.3/hld/board/de5a_net_ddr4/tests/extlibs/lib:/usr/local/cuda-11.2/lib64:/usr/lib64/atlas:/usr/local/lib:/lib64:/opt/rh/devtoolset-8/root/usr/lib64:/opt/rh/devtoolset-8/root/usr/lib:/opt/rh/devtoolset-8/root/usr/lib64/dyninst:/opt/rh/devtoolset-8/root/usr/lib/dyninst:/opt/rh/devtoolset-8/root/usr/lib64:/opt/rh/devtoolset-8/root/usr/lib:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/opt/rh/devtoolset-9/root/usr/lib64/dyninst:/opt/rh/devtoolset-9/root/usr/lib/dyninst:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/archive/users/gxanth/gcc-12.1.0/lib64:/opt/altera/intelFPGA/20.3/hld/board/de5a_net_ddr4/linux64/lib:/opt/altera/intelFPGA/20.3/hld/host/linux64/lib:/opt/altera/intelFPGA/20.3/hld/board/de5a_net_ddr4/tests/extlibs/lib:/usr/local/cuda-11.2/lib64:/usr/lib64/atlas:/usr/local/lib:/lib64:/opt/rh/llvm-toolset-7/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/opt/rh/devtoolset-9/root/usr/lib64/dyninst:/opt/rh/devtoolset-9/root/usr/lib/dyninst:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/usr/mpi/gcc/openmpi-4.0.3rc4/lib64 --action_env GCC_HOST_COMPILER_PATH=/opt/rh/devtoolset-9/root/usr/bin/gcc --config=cuda
INFO: Reading rc options for 'build' from /tmp/manospavl/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /tmp/manospavl/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /tmp/manospavl/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda in file /tmp/manospavl/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:linux in file /tmp/manospavl/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /tmp/manospavl/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (558 packages loaded, 38371 targets configured).
INFO: Found 1 target...
ERROR: /tmp/manospavl/tensorflow/tensorflow/BUILD:1426:19: Executing genrule //tensorflow:tf_python_api_gen_v2 failed: (Exit 1): bash failed: error executing command /bin/bash -c ... (remaining 1 argument skipped)
2023-04-20 16:53:52.189168: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Traceback (most recent call last):
  File ""/tmp/bazel_manospavl/_bazel_manospavl/3b86cb968a28ba76b0db07c39e753f80/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /tmp/bazel_manospavl/_bazel_manospavl/3b86cb968a28ba76b0db07c39e753f80/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/../libtensorflow_framework.so.2: undefined symbol: cublasLtMatmulDescDestroy

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/tmp/bazel_manospavl/_bazel_manospavl/3b86cb968a28ba76b0db07c39e753f80/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 22, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/tmp/bazel_manospavl/_bazel_manospavl/3b86cb968a28ba76b0db07c39e753f80/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 36, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""/tmp/bazel_manospavl/_bazel_manospavl/3b86cb968a28ba76b0db07c39e753f80/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 77, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""/tmp/bazel_manospavl/_bazel_manospavl/3b86cb968a28ba76b0db07c39e753f80/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /tmp/bazel_manospavl/_bazel_manospavl/3b86cb968a28ba76b0db07c39e753f80/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/../libtensorflow_framework.so.2: undefined symbol: cublasLtMatmulDescDestroy


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /tmp/manospavl/tensorflow/tensorflow/python/tools/BUILD:98:10 Middleman _middlemen/tensorflow_Spython_Stools_Simport_Upb_Uto_Utensorboard-runfiles failed: (Exit 1): bash failed: error executing command /bin/bash -c ... (remaining 1 argument skipped)
INFO: Elapsed time: 29.342s, Critical Path: 6.86s
INFO: 2 processes: 2 internal.
FAILED: Build did NOT complete successfully
```
</details>"
60386,Limiting graph serialization depth for composite models.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.12

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I am building a model generator that composes models by putting together assemblies of simpler modules. When exporting such a model, I noticed that all component modules (submodules/sugraphs) are serialized and exported as well and are available at the load time, so if my model is composed of say P, Q the graph for P and Q is also serialized and same for all the children of P and Q. 
 
 1. My understanding is that the top-level graph is self-contained and does not need the graph structures of submodules, exporting them is wasteful if I only need to run the compute function at the top-level graph.
 
 2. Is there a way I could achieve resource restriction using ` tf.saved_model.save`; I only mean to save and work with top level graphs. Ideally, I think it would be great to be able to provide a maximum depth at which the graph serialization/export is done so it excludes everything below this depth.
 
 Any guidance on how to achieve this will be greatly appreciated. Please see the code snippet and the comments for more information, the code demonstrates that a composite model takes considerably more disk space than an equivalent monolithic model, did not look into memory and compute performance as yet...


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1FeP_bhVwQ223A9GGts1v--FgXDNeb6jY?usp=sharing
```


### Relevant log output

```shell
https://gist.github.com/RameezI/3d51ab59d745d08b62daec77a3a5439c
```
</details>"
60385,IndexError: tuple index out of range in Quantum LSTM Hybrid Model,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.7.0

### Custom Code

Yes

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I have created a simple VQC and combined it with Classsical Tensorflow layers using KerasLayer. My model compiles perfectly but when I try to train it using toy data, it returns a input_shape() error. 

I have already tried a bunch of tricks to change the shape of my data to prevent the error but it creates more error or same error presists. 

**Packages:**

1. PennyLane
2. Tensorflow
3. Numpy

It'll be significantly appreciated if someone can help me. 

### Standalone code to reproduce the issue

```shell
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'


import tensorflow as tf
import numpy as np
import pennylane as qml
from pennylane.templates.layers import StronglyEntanglingLayers

# Define the quantum circuit
n_qubits = 4
dev = qml.device(""default.qubit"", wires=n_qubits)

@qml.qnode(dev)
def quantum_layer(inputs, weights):
    print(""Inputs shape:"", inputs.shape)
    print(""Weights shape:"", weights.shape)
    # weights=weights[0]
    qml.StronglyEntanglingLayers(weights, wires=range(n_qubits))
    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]

n_layers = 2
features=1
timesteps = 10 

weight_shapes = {""weights"": (n_layers, n_qubits)}
# Define the neural network model
def create_model():
    model = tf.keras.Sequential([
        tf.keras.layers.LSTM(4, input_shape=(timesteps, features)),
        tf.keras.layers.Reshape((1, 4)),
        tf.keras.layers.Lambda(lambda x: x[:, -1, :]),
        qml.qnn.KerasLayer(quantum_layer, weight_shapes, output_dim=n_qubits),
        tf.keras.layers.Dense(16, activation='relu'),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])
    return model

# Compile the model and print the summary
model = create_model()
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()

# Define the toy dataset
X = np.random.rand(100, timesteps, features)
y = np.random.randint(0, 2, size=(100, 1))

# Train the model
model.fit(X, y, epochs=10, batch_size=10)
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""f:/CS/QLSTM/qml.py"", line 49, in <module>
    model.fit(X, y, epochs=10, batch_size=10)
  File ""C:\Users\DELL\AppData\Roaming\Python\Python38\site-packages\keras\utils\traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""C:\Users\DELL\AppData\Roaming\Python\Python38\site-packages\pennylane\qnn\keras.py"", line 302, in call
    reconstructor.append(self.call(x))
  File ""C:\Users\DELL\AppData\Roaming\Python\Python38\site-packages\pennylane\qnn\keras.py"", line 305, in call
    return self._evaluate_qnode(inputs)
  File ""C:\Users\DELL\AppData\Roaming\Python\Python38\site-packages\pennylane\qnn\keras.py"", line 320, in _evaluate_qnode
    return self.qnode(**kwargs)
  File ""C:\Users\DELL\AppData\Roaming\Python\Python38\site-packages\pennylane\qnode.py"", line 842, in __call__
    self.construct(args, kwargs)
  File ""C:\Users\DELL\AppData\Roaming\Python\Python38\site-packages\pennylane\qnode.py"", line 751, in construct
1, in wrapper
    result = fn(*args, **kwargs)
  File ""f:/CS/QLSTM/qml.py"", line 19, in quantum_layer
    qml.StronglyEntanglingLayers(weights, wires=range(n_qubits))
  File ""C:\Users\DELL\AppData\Roaming\Python\Python38\site-packages\pennylane\templates\layers\strongly_entangling.py"", line 143, in __init__
    if shape[2] != 3:
IndexError: Exception encountered when calling layer ""keras_layer"" (type KerasLayer).

tuple index out of range

Call arguments received:
  • inputs=tf.Tensor(shape=(10, 4), dtype=float32)
```
</details>"
60384,Performance using prefetch does not improve!,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11.1

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04.2 LTS

### Mobile device

_No response_

### Python version

3.9.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

12.0

### GPU model and memory

_No response_

### Current Behaviour?

Hi all, the problem I have is about _tf.data_ and mainly about the fact that _prefetch_ optimization does not improve perfomance in any case (and it almost seems that it is not used).
Specifically, I did the tests using different environments (with different GPUs and different CPUs), using different versions of tensorflow (I used tf-2.10.0 and tf-2.11.0), using different batch-size, using different data-processing/data-augmentation operations, using different parameters of _num_parallel_calls_ (with fixed number and with AUTOTUNE) and the results are the same: I have no advantage using prefetching.
This seems strange to me, because at least theoretically a speed-up should be there. I used Cifar10 as the dataset, and the data are initially stored in a TFRecordDataset. PS: I already see this [guide](https://www.tensorflow.org/guide/data_performance)
Thank you in advance 

### Standalone code to reproduce the issue

```shell

import numpy as np
import tensorflow as tf
import tensorflow.keras as keras
import tensorflow.keras.layers as layers
from keras.utils.layer_utils import count_params
import time
from datetime import datetime

def get_model_data_augmentation_CPU():
    model = tf.keras.Sequential([
      layers.Conv2D(64, 3, activation='relu'),
      layers.MaxPooling2D(),
      layers.Dropout(0.1),
      layers.Conv2D(128, 3, activation='relu'),
      layers.MaxPooling2D(),
      layers.Dropout(0.1),
      layers.Conv2D(128, 3, activation='relu'),
      layers.MaxPooling2D(),
      layers.Dropout(0.2),
      layers.Flatten(),
      layers.Dense(256, activation='relu'),
      layers.Dropout(0.3),
      layers.Dense(10)
    ])
    adam_opt = keras.optimizers.Adam(learning_rate=0.001)
    model.compile(optimizer = adam_opt,
                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                  metrics=['accuracy'])
    return model

def parse_image_function(example_proto):
    image_feature_description = {
        'image': tf.io.FixedLenFeature([], tf.string),
        'label': tf.io.FixedLenFeature([], tf.int64)
    }
    features = tf.io.parse_single_example(
                  example_proto, image_feature_description)
    image = tf.io.decode_raw(features['image'], tf.float32)
    image = tf.reshape(image, [32, 32, 3])
    label = tf.cast(features['label'], tf.int64)
    return image, label

# Set Memory Growth of tensorflow equals true
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        # Currently, memory growth needs to be the same across GPUs
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        logical_gpus = tf.config.list_logical_devices('GPU')
        print(len(gpus), ""Physical GPUs,"", len(logical_gpus), ""Logical GPUs"")
    except RuntimeError as e:
        # Memory growth must be set before GPUs have been initialized
        print(e)

NUM_CLASSES = 10 # Cifar10 has 10 classes
BATCH_SIZE = 32
EPOCHS=5 
AUTOTUNE = tf.data.AUTOTUNE

TFRECORD_FILE_TRAIN = # YOUR_PATH
TFRECORD_FILE_VALIDATION= # YOUR_PATH
TFRECORD_FILE_TEST = # YOUR_PATH
SIZE_DATASET_TRAIN= 40000
STEPS_PER_EPOCH = (SIZE_DATASET_TRAIN*2)//BATCH_SIZE
VALIDATION_STEP = 10000//BATCH_SIZE
model = get_model_data_augmentation_CPU() # Get the Keras model
data_augmentation = tf.keras.Sequential([
  layers.RandomFlip(""horizontal_and_vertical""),
  layers.RandomRotation(0.2),
  layers.RandomZoom(0.2)
])
dataset_train = tf.data.TFRecordDataset(TFRECORD_FILE_TRAIN, num_parallel_reads=4)
dataset_train = dataset_train.repeat(2*EPOCHS)
dataset_train = dataset_train.map(parse_image_function, num_parallel_calls=8) # decode image from Tfrecord
dataset_train = dataset_train.map(lambda x,y: (data_augmentation(x, training=True),y), num_parallel_calls=8)
dataset_train = dataset_train.batch(BATCH_SIZE)
# Comment or uncomment following line to see differences 
dataset_train = dataset_train.prefetch(1) # I tried also with other buffer_size

# Training of the network
start_time = time.time() # Measure the start time of the training
history = model.fit(
    dataset_train,
    epochs=EPOCHS,
    steps_per_epoch=STEPS_PER_EPOCH,
)
end_time = time.time() 
training_time = end_time-start_time
print(f""Execution time: {(end_time-start_time)} sec"")
```


### Relevant log output

_No response_</details>"
60369,`tf.bitcast` throws assertion on `osx-64` and `osx-arm64` with `2.11.1`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

TF 2.11

### Custom Code

No

### OS Platform and Distribution

`osx-64`, `osx-arm64`

### Mobile device

_No response_

### Python version

3.10.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

For some reason, with version `2.11` (and `2.11.1`) the `bitcast` function hits an assertion in `casts.h`, no matter what the input. This only happens on `osx-64` and `osx-arm64`, and that too only with this version - older versions work correctly.

I also tried installing this version from `conda-forge` and `anaconda`, and both have the same behavior. So in summary, regardless of whether you install this from source or from the common `conda` channels the behavior is the same.

This **does not happen** with the versions installed via `pip`.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

x = [1., 2., 3.]
tf.bitcast(x, tf.qint8)
```


### Relevant log output

```shell
Assertion failed: (f == nullptr || dynamic_cast<To>(f) != nullptr), function down_cast, file casts.h, line 58.
```
</details>"
60368,Tensorflow 2.11.1 bazel failed with option framework_shared_object=false,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf2.11.1

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

5.3.0

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Tried to build Tensorflow 2.11.1 (and Tensorflow 2.12.0) using Tensorflow docker image: tensorflow/tensorflow:devel with option framework_shared_object=false. The build fails with the following error. (It's a build for CPU only).

Using the same option, previous versions of Tensorflow (such as v2.10.1) build without error. 






### Standalone code to reproduce the issue

```shell
docker pull tensorflow/tensorflow:devel
docker run -it -w /tensorflow -v /path/to/tensorflow:/tensorflow -v $PWD:/mnt \
    -e HOST_PERMS=""\\((id -u):\\)(id -g)"" tensorflow/tensorflow:devel bash

# within the container
bazel build --config opt --define framework_shared_object=false //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
ERROR: /tensorflow/tensorflow/python/BUILD:889:29: Linking tensorflow/python/gen_ragged_math_ops_py_wrappers_cc failed: (Exit 1): gcc failed: error executing command /usr/bin/gcc @bazel-out/k8-opt/bin/tensorflow/python/gen_ragged_math_ops_py_wrappers_cc-2.params
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/autotuning_proto_cc_impl/autotuning.pb.o:autotuning.pb.cc:function InitDefaultsscc_info_AutotuneResult_FailureResult_tensorflow_2fcore_2fprotobuf_2fautotuning_2eproto(): error: undefined reference to 'stream_executor::dnn::_AlgorithmProto_default_instance_'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/autotuning_proto_cc_impl/autotuning.pb.o:autotuning.pb.cc:function InitDefaultsscc_info_AutotuneResult_tensorflow_2fcore_2fprotobuf_2fautotuning_2eproto(): error: undefined reference to 'stream_executor::dnn::_AlgorithmProto_default_instance_'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/autotuning_proto_cc_impl/autotuning.pb.o:autotuning.pb.cc:function tensorflow::AutotuneResult_FailureResult::MergeFrom(tensorflow::AutotuneResult_FailureResult const&): error: undefined reference to 'stream_executor::dnn::AlgorithmProto* google::protobuf::Arena::CreateMaybeMessage<stream_executor::dnn::AlgorithmProto>(google::protobuf::Arena*)'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/autotuning_proto_cc_impl/autotuning.pb.o:autotuning.pb.cc:function tensorflow::AutotuneResult_FailureResult::MergeFrom(tensorflow::AutotuneResult_FailureResult const&): error: undefined reference to 'stream_executor::dnn::_AlgorithmProto_default_instance_'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/autotuning_proto_cc_impl/autotuning.pb.o:autotuning.pb.cc:function tensorflow::AutotuneResult_FailureResult::MergeFrom(tensorflow::AutotuneResult_FailureResult const&): error: undefined reference to 'stream_executor::dnn::AlgorithmProto::MergeFrom(stream_executor::dnn::AlgorithmProto const&)'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/autotuning_proto_cc_impl/autotuning.pb.o:autotuning.pb.cc:function tensorflow::AutotuneResult_FailureResult::MergeFrom(tensorflow::AutotuneResult_FailureResult const&): error: undefined reference to 'stream_executor::dnn::AlgorithmProto::MergeFrom(stream_executor::dnn::AlgorithmProto const&)'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/autotuning_proto_cc_impl/autotuning.pb.o:autotuning.pb.cc:function tensorflow::AutotuneResult::MergeFrom(tensorflow::AutotuneResult const&): error: undefined reference to 'stream_executor::dnn::AlgorithmProto* google::protobuf::Arena::CreateMaybeMessage<stream_executor::dnn::AlgorithmProto>(google::protobuf::Arena*)'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/autotuning_proto_cc_impl/autotuning.pb.o:autotuning.pb.cc:function tensorflow::AutotuneResult::MergeFrom(tensorflow::AutotuneResult const&): error: undefined reference to 'stream_executor::dnn::_AlgorithmProto_default_instance_'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/autotuning_proto_cc_impl/autotuning.pb.o:autotuning.pb.cc:function tensorflow::AutotuneResult::MergeFrom(tensorflow::AutotuneResult const&): error: undefined reference to 'stream_executor::dnn::AlgorithmProto::MergeFrom(stream_executor::dnn::AlgorithmProto const&)'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/autotuning_proto_cc_impl/autotuning.pb.o:autotuning.pb.cc:function tensorflow::AutotuneResult::MergeFrom(tensorflow::AutotuneResult const&): error: undefined reference to 'stream_executor::dnn::AlgorithmProto::MergeFrom(stream_executor::dnn::AlgorithmProto const&)'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/autotuning_proto_cc_impl/autotuning.pb.o:autotuning.pb.cc:function tensorflow::AutotuneResult_FailureResult::MergePartialFromCodedStream(google::protobuf::io::CodedInputStream*): error: undefined reference to 'stream_executor::dnn::AlgorithmProto* google::protobuf::Arena::CreateMaybeMessage<stream_executor::dnn::AlgorithmProto>(google::protobuf::Arena*)'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/autotuning_proto_cc_impl/autotuning.pb.o:autotuning.pb.cc:function tensorflow::AutotuneResult_FailureResult::MergePartialFromCodedStream(google::protobuf::io::CodedInputStream*): error: undefined reference to 'stream_executor::dnn::AlgorithmProto::MergePartialFromCodedStream(google::protobuf::io::CodedInputStream*)'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/autotuning_proto_cc_impl/autotuning.pb.o:autotuning.pb.cc:function tensorflow::AutotuneResult_FailureResult::InternalSerializeWithCachedSizesToArray(unsigned char*) const: error: undefined reference to 'stream_executor::dnn::AlgorithmProto::InternalSerializeWithCachedSizesToArray(unsigned char*) const'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/autotuning_proto_cc_impl/autotuning.pb.o:autotuning.pb.cc:function tensorflow::AutotuneResult_FailureResult::ByteSizeLong() const: error: undefined reference to 'stream_executor::dnn::AlgorithmProto::ByteSizeLong() const'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/autotuning_proto_cc_impl/autotuning.pb.o:autotuning.pb.cc:function tensorflow::AutotuneResult::MergePartialFromCodedStream(google::protobuf::io::CodedInputStream*): error: undefined reference to 'stream_executor::dnn::AlgorithmProto* google::protobuf::Arena::CreateMaybeMessage<stream_executor::dnn::AlgorithmProto>(google::protobuf::Arena*)'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/autotuning_proto_cc_impl/autotuning.pb.o:autotuning.pb.cc:function tensorflow::AutotuneResult::MergePartialFromCodedStream(google::protobuf::io::CodedInputStream*): error: undefined reference to 'stream_executor::dnn::AlgorithmProto::MergePartialFromCodedStream(google::protobuf::io::CodedInputStream*)'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/autotuning_proto_cc_impl/autotuning.pb.o:autotuning.pb.cc:function tensorflow::AutotuneResult::InternalSerializeWithCachedSizesToArray(unsigned char*) const: error: undefined reference to 'stream_executor::dnn::AlgorithmProto::InternalSerializeWithCachedSizesToArray(unsigned char*) const'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/autotuning_proto_cc_impl/autotuning.pb.o:autotuning.pb.cc:function tensorflow::AutotuneResult::ByteSizeLong() const: error: undefined reference to 'stream_executor::dnn::AlgorithmProto::ByteSizeLong() const'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/autotuning_proto_cc_impl/autotuning.pb.o:autotuning.pb.cc:descriptor_table_tensorflow_2fcore_2fprotobuf_2fautotuning_2eproto_deps: error: undefined reference to 'descriptor_table_tensorflow_2fcompiler_2fxla_2fstream_5fexecutor_2fdnn_2eproto'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/autotuning_proto_cc_impl/autotuning.pb.o:autotuning.pb.cc:scc_info_AutotuneResult_FailureResult_tensorflow_2fcore_2fprotobuf_2fautotuning_2eproto: error: undefined reference to 'scc_info_AlgorithmProto_tensorflow_2fcompiler_2fxla_2fstream_5fexecutor_2fdnn_2eproto'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/autotuning_proto_cc_impl/autotuning.pb.o:autotuning.pb.cc:scc_info_AutotuneResult_tensorflow_2fcore_2fprotobuf_2fautotuning_2eproto: error: undefined reference to 'scc_info_AlgorithmProto_tensorflow_2fcompiler_2fxla_2fstream_5fexecutor_2fdnn_2eproto'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/conv_autotuning_proto_cc_impl/conv_autotuning.pb.o:conv_autotuning.pb.cc:function InitDefaultsscc_info_ConvolutionProto_tensorflow_2fcore_2fprotobuf_2fconv_5fautotuning_2eproto(): error: undefined reference to 'stream_executor::dnn::_TensorDescriptorProto_default_instance_'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/conv_autotuning_proto_cc_impl/conv_autotuning.pb.o:conv_autotuning.pb.cc:function InitDefaultsscc_info_ConvolutionProto_tensorflow_2fcore_2fprotobuf_2fconv_5fautotuning_2eproto(): error: undefined reference to 'stream_executor::dnn::_ConvolutionDescriptorProto_default_instance_'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/conv_autotuning_proto_cc_impl/conv_autotuning.pb.o:conv_autotuning.pb.cc:function tensorflow::ConvolutionProto::MergeFrom(tensorflow::ConvolutionProto const&): error: undefined reference to 'stream_executor::dnn::TensorDescriptorProto::MergeFrom(stream_executor::dnn::TensorDescriptorProto const&)'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/conv_autotuning_proto_cc_impl/conv_autotuning.pb.o:conv_autotuning.pb.cc:function tensorflow::ConvolutionProto::MergeFrom(tensorflow::ConvolutionProto const&): error: undefined reference to 'stream_executor::dnn::TensorDescriptorProto::MergeFrom(stream_executor::dnn::TensorDescriptorProto const&)'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/conv_autotuning_proto_cc_impl/conv_autotuning.pb.o:conv_autotuning.pb.cc:function tensorflow::ConvolutionProto::MergeFrom(tensorflow::ConvolutionProto const&): error: undefined reference to 'stream_executor::dnn::TensorDescriptorProto::MergeFrom(stream_executor::dnn::TensorDescriptorProto const&)'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/conv_autotuning_proto_cc_impl/conv_autotuning.pb.o:conv_autotuning.pb.cc:function tensorflow::ConvolutionProto::MergeFrom(tensorflow::ConvolutionProto const&): error: undefined reference to 'stream_executor::dnn::ConvolutionDescriptorProto::MergeFrom(stream_executor::dnn::ConvolutionDescriptorProto const&)'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/conv_autotuning_proto_cc_impl/conv_autotuning.pb.o:conv_autotuning.pb.cc:function tensorflow::ConvolutionProto::MergeFrom(tensorflow::ConvolutionProto const&): error: undefined reference to 'stream_executor::dnn::TensorDescriptorProto* google::protobuf::Arena::CreateMaybeMessage<stream_executor::dnn::TensorDescriptorProto>(google::protobuf::Arena*)'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/conv_autotuning_proto_cc_impl/conv_autotuning.pb.o:conv_autotuning.pb.cc:function tensorflow::ConvolutionProto::MergeFrom(tensorflow::ConvolutionProto const&): error: undefined reference to 'stream_executor::dnn::_TensorDescriptorProto_default_instance_'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/conv_autotuning_proto_cc_impl/conv_autotuning.pb.o:conv_autotuning.pb.cc:function tensorflow::ConvolutionProto::MergeFrom(tensorflow::ConvolutionProto const&): error: undefined reference to 'stream_executor::dnn::TensorDescriptorProto* google::protobuf::Arena::CreateMaybeMessage<stream_executor::dnn::TensorDescriptorProto>(google::protobuf::Arena*)'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/conv_autotuning_proto_cc_impl/conv_autotuning.pb.o:conv_autotuning.pb.cc:function tensorflow::ConvolutionProto::MergeFrom(tensorflow::ConvolutionProto const&): error: undefined reference to 'stream_executor::dnn::_TensorDescriptorProto_default_instance_'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/conv_autotuning_proto_cc_impl/conv_autotuning.pb.o:conv_autotuning.pb.cc:function tensorflow::ConvolutionProto::MergeFrom(tensorflow::ConvolutionProto const&): error: undefined reference to 'stream_executor::dnn::TensorDescriptorProto* google::protobuf::Arena::CreateMaybeMessage<stream_executor::dnn::TensorDescriptorProto>(google::protobuf::Arena*)'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/conv_autotuning_proto_cc_impl/conv_autotuning.pb.o:conv_autotuning.pb.cc:function tensorflow::ConvolutionProto::MergeFrom(tensorflow::ConvolutionProto const&): error: undefined reference to 'stream_executor::dnn::_TensorDescriptorProto_default_instance_'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/conv_autotuning_proto_cc_impl/conv_autotuning.pb.o:conv_autotuning.pb.cc:function tensorflow::ConvolutionProto::MergeFrom(tensorflow::ConvolutionProto const&): error: undefined reference to 'stream_executor::dnn::ConvolutionDescriptorProto* google::protobuf::Arena::CreateMaybeMessage<stream_executor::dnn::ConvolutionDescriptorProto>(google::protobuf::Arena*)'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/conv_autotuning_proto_cc_impl/conv_autotuning.pb.o:conv_autotuning.pb.cc:function tensorflow::ConvolutionProto::MergeFrom(tensorflow::ConvolutionProto const&): error: undefined reference to 'stream_executor::dnn::_ConvolutionDescriptorProto_default_instance_'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/conv_autotuning_proto_cc_impl/conv_autotuning.pb.o:conv_autotuning.pb.cc:function tensorflow::ConvolutionProto::MergePartialFromCodedStream(google::protobuf::io::CodedInputStream*): error: undefined reference to 'stream_executor::dnn::TensorDescriptorProto* google::protobuf::Arena::CreateMaybeMessage<stream_executor::dnn::TensorDescriptorProto>(google::protobuf::Arena*)'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/conv_autotuning_proto_cc_impl/conv_autotuning.pb.o:conv_autotuning.pb.cc:function tensorflow::ConvolutionProto::MergePartialFromCodedStream(google::protobuf::io::CodedInputStream*): error: undefined reference to 'stream_executor::dnn::ConvolutionDescriptorProto::MergePartialFromCodedStream(google::protobuf::io::CodedInputStream*)'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/conv_autotuning_proto_cc_impl/conv_autotuning.pb.o:conv_autotuning.pb.cc:function tensorflow::ConvolutionProto::MergePartialFromCodedStream(google::protobuf::io::CodedInputStream*): error: undefined reference to 'stream_executor::dnn::TensorDescriptorProto::MergePartialFromCodedStream(google::protobuf::io::CodedInputStream*)'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/conv_autotuning_proto_cc_impl/conv_autotuning.pb.o:conv_autotuning.pb.cc:function tensorflow::ConvolutionProto::MergePartialFromCodedStream(google::protobuf::io::CodedInputStream*): error: undefined reference to 'stream_executor::dnn::ConvolutionDescriptorProto* google::protobuf::Arena::CreateMaybeMessage<stream_executor::dnn::ConvolutionDescriptorProto>(google::protobuf::Arena*)'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/conv_autotuning_proto_cc_impl/conv_autotuning.pb.o:conv_autotuning.pb.cc:function tensorflow::ConvolutionProto::InternalSerializeWithCachedSizesToArray(unsigned char*) const: error: undefined reference to 'stream_executor::dnn::TensorDescriptorProto::InternalSerializeWithCachedSizesToArray(unsigned char*) const'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/conv_autotuning_proto_cc_impl/conv_autotuning.pb.o:conv_autotuning.pb.cc:function tensorflow::ConvolutionProto::InternalSerializeWithCachedSizesToArray(unsigned char*) const: error: undefined reference to 'stream_executor::dnn::TensorDescriptorProto::InternalSerializeWithCachedSizesToArray(unsigned char*) const'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/conv_autotuning_proto_cc_impl/conv_autotuning.pb.o:conv_autotuning.pb.cc:function tensorflow::ConvolutionProto::InternalSerializeWithCachedSizesToArray(unsigned char*) const: error: undefined reference to 'stream_executor::dnn::TensorDescriptorProto::InternalSerializeWithCachedSizesToArray(unsigned char*) const'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/conv_autotuning_proto_cc_impl/conv_autotuning.pb.o:conv_autotuning.pb.cc:function tensorflow::ConvolutionProto::InternalSerializeWithCachedSizesToArray(unsigned char*) const: error: undefined reference to 'stream_executor::dnn::ConvolutionDescriptorProto::InternalSerializeWithCachedSizesToArray(unsigned char*) const'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/conv_autotuning_proto_cc_impl/conv_autotuning.pb.o:conv_autotuning.pb.cc:function tensorflow::ConvolutionProto::ByteSizeLong() const: error: undefined reference to 'stream_executor::dnn::TensorDescriptorProto::ByteSizeLong() const'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/conv_autotuning_proto_cc_impl/conv_autotuning.pb.o:conv_autotuning.pb.cc:function tensorflow::ConvolutionProto::ByteSizeLong() const: error: undefined reference to 'stream_executor::dnn::TensorDescriptorProto::ByteSizeLong() const'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/conv_autotuning_proto_cc_impl/conv_autotuning.pb.o:conv_autotuning.pb.cc:function tensorflow::ConvolutionProto::ByteSizeLong() const: error: undefined reference to 'stream_executor::dnn::TensorDescriptorProto::ByteSizeLong() const'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/conv_autotuning_proto_cc_impl/conv_autotuning.pb.o:conv_autotuning.pb.cc:function tensorflow::ConvolutionProto::ByteSizeLong() const: error: undefined reference to 'stream_executor::dnn::ConvolutionDescriptorProto::ByteSizeLong() const'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/conv_autotuning_proto_cc_impl/conv_autotuning.pb.o:conv_autotuning.pb.cc:descriptor_table_tensorflow_2fcore_2fprotobuf_2fconv_5fautotuning_2eproto_deps: error: undefined reference to 'descriptor_table_tensorflow_2fcompiler_2fxla_2fstream_5fexecutor_2fdnn_2eproto'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/conv_autotuning_proto_cc_impl/conv_autotuning.pb.o:conv_autotuning.pb.cc:scc_info_ConvolutionProto_tensorflow_2fcore_2fprotobuf_2fconv_5fautotuning_2eproto: error: undefined reference to 'scc_info_TensorDescriptorProto_tensorflow_2fcompiler_2fxla_2fstream_5fexecutor_2fdnn_2eproto'
bazel-out/k8-opt/bin/tensorflow/core/protobuf/_objs/conv_autotuning_proto_cc_impl/conv_autotuning.pb.o:conv_autotuning.pb.cc:scc_info_ConvolutionProto_tensorflow_2fcore_2fprotobuf_2fconv_5fautotuning_2eproto: error: undefined reference to 'scc_info_ConvolutionDescriptorProto_tensorflow_2fcompiler_2fxla_2fstream_5fexecutor_2fdnn_2eproto'
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /tensorflow/tensorflow/lite/python/BUILD:68:10 Middleman _middlemen/tensorflow_Slite_Spython_Stflite_Uconvert-runfiles failed: (Exit 1): gcc failed: error executing command /usr/bin/gcc @bazel-out/k8-opt/bin/tensorflow/python/gen_ragged_math_ops_py_wrappers_cc-2.params
INFO: Elapsed time: 152.054s, Critical Path: 49.12s
INFO: 376 processes: 39 internal, 337 local.
FAILED: Build did NOT complete successfully
```
</details>"
60367,Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/conv.cc:343 input->dims->size != 4 (3 != 4),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

MacOS

### Mobile device

Linux (Android Oneplus)

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I am working on this noise and echo cancellation on android. I used this tflite model created via https://colab.research.google.com/drive/1HzGdovqo0gg_xW1QL7ygbkYlWqbyMaKL?usp=sharing#scrollTo=2kRjDbp7og1u

Now I want to use a ShortArray() audioData and want to pass and get a ShortArray() data back with  removed echo and noise.

I tried creating, tflite model, predict function works there, I incorporated it in android but whenever data is passed in the array it returns the error

```Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/conv.cc:343 input->dims->size != 4 (3 != 4)```

### Standalone code to reproduce the issue

```shell
snippet has this


#@INPUT
print(test_audio) gives 

<tf.Tensor: shape=(6, 12000, 1), dtype=float32, numpy=
array([[[-0.00097656],
        [-0.01428223],
        [-0.02471924],
        ...,
        [ 0.14346313],
        [ 0.13012695],
        [ 0.14242554]]]], dtype=float32)>
```

```
predict_tflite_with_array(tf.convert_to_tensor(test_audio, np.float32)) # @To predict
```

```
#@Output
<tf.Tensor: shape=(68800,), dtype=float32, numpy=
array([-0.00645937, -0.01239021, -0.01816257, ..., -0.02149353,
       -0.0514788 , -0.04442336], dtype=float32)>
```

```
def predict_tflite_with_array(test_audio):
  input_index = interpreter.get_input_details()[0][""index""]
  output_index = interpreter.get_output_details()[0][""index""]

  preds = []
  for i in test_audio:
    interpreter.set_tensor(input_index, tf.expand_dims(i,0))
    interpreter.invoke()
    predictions = interpreter.get_tensor(output_index)
    preds.append(predictions)

  predictions = tf.squeeze(tf.stack(preds,axis=1))
  final_op = tf.reshape(predictions[:-1],((predictions.shape[0]-1)*predictions.shape[1],1))
  final_op = tf.concat((tf.squeeze(final_op),predictions[-1][-diff:]),axis=0)
  return final_op

```



I used this tflite model in android to run this

```
aaptOptions {
        noCompress ""model.tflite""
    }

implementation 'org.tensorflow:tensorflow-lite:2.12.0'
```
```
 @Throws(IOException::class)
    private fun loadModelFile(activity: Activity): MappedByteBuffer? {
        val fileDescriptor: AssetFileDescriptor = activity.assets.openFd(""model.tflite"")
        val inputStream = FileInputStream(fileDescriptor.fileDescriptor)
        val fileChannel = inputStream.channel
        val startOffset = fileDescriptor.startOffset
        val declaredLength = fileDescriptor.declaredLength
        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength)
    }

    try {
            viewModel.tflite = loadModelFile(requireActivity())?.let {
                Interpreter(it) }
            Log.d(""tflite"", ""Model initiated"")
        } catch (ex: Exception) {
            ex.printStackTrace()
            Log.d(""tflite"", ""Model initiation Failed $ex"")
        }
```

```
private fun applyModel(inputAudioData: ShortArray): ShortArray {
        val inputVal = FloatArray(inputAudioData.size)

        for (i in inputAudioData.indices) {
            inputVal[i] = inputAudioData[i].toFloat()
        }
        val inputFloatArray = Array(1){
            inputVal
        }
        val outputFloatArray = Array(1) {
            FloatArray(inputAudioData.size)
        }

        Log.d(""tflite"", ""Model input data: ${inputFloatArray.toString()}"")

        tflite!!.run(inputFloatArray, outputFloatArray)
        Log.d(""tflite"", ""Model output data: ${outputFloatArray.toString()}"")

        val outputAudioData = ShortArray(inputFloatArray.size)

        for (i in outputFloatArray[0].indices) {
            outputAudioData[i] = outputFloatArray[0][i].toInt().toShort()
        }
        Log.d(""tflite"", ""Model returning data: ${outputAudioData.toString()}"")

        return outputAudioData
    }
```

  [1]: https://colab.research.google.com/drive/1HzGdovqo0gg_xW1QL7ygbkYlWqbyMaKL?usp=sharing#scrollTo=2kRjDbp7og1u
```


### Relevant log output

```shell
but whenever the app is running its crashing with this error


 java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/conv.cc:343 input->dims->size != 4 (3 != 4)
                                                                                                    Node number 5 (CONV_2D) failed to prepare.
                                                                                                    	at org.tensorflow.lite.NativeInterpreterWrapper.allocateTensors(Native Method)
                                                                                                    	at org.tensorflow.lite.NativeInterpreterWrapper.allocateTensorsIfNeeded(NativeInterpreterWrapper.java:308)
                                                                                                    	at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:248)
                                                                                                    	at org.tensorflow.lite.InterpreterImpl.runForMultipleInputsOutputs(InterpreterImpl.java:101)
                                                                                                    	at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:77)
                                                                                                    	at org.tensorflow.lite.InterpreterImpl.run(InterpreterImpl.java:94)
                                                                                                    	at org.tensorflow.lite.Interpreter.run(Interpreter.java:77)
```
```
</details>"
60366,"NaN is not propagated in `tf.pow(tf.constant(1.0), tf.sqrt(tf.constant(-1.0)))`","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Currently `tf.pow(x, y)` returns 1.0 (at least on CPU) when `x` is 1.0 and `y` is NaN, instead of propagating NaN like most other math ops. Can someone confirm which is the intended behavior? Thanks


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

x = tf.constant(1.0)
y = tf.sqrt(tf.constant(-1.0))
z = tf.pow(x, y)
print(z)
```


### Relevant log output

_No response_</details>"
60364,TFLite model maker installation issue in kaggle,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): kaggle
- TensorFlow installation (pip package or built from source): !pip install tflite-model-maker
- TensorFlow library (version, if pip package or github SHA, if built from source):2.8

### 2. Code

[[Provide code to help us reproduce your issues using one of the following options:](https://www.kaggle.com/code/iamprateek/tflite-model-maker-test/notebook)](https://www.kaggle.com/code/iamprateek/tflite-model-maker-test/notebook)

###3. ERROR: `Cannot uninstall 'llvmlite'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.`"
60363,Tensorflow model training stalls with MirroredStrategy on multiple GPUs,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

Sagemaker ml.p3.8xlarge - 4x V100

### Current Behaviour?

When using `MirroredStrategy`, initializing training (after calling `fit`) takes an incredibly long time (over an hour), and often it stalls completely. See the attached logs - it stalls after the 4 `Init COMPLETE` messages are logged.

I'm unable to share the specific model and training code as it is proprietary. However, it's based around the Keras model `InceptionResNetV2`.

I'll try to work on putting together a shareable test-case, but in the meantime is there any way I can try and diagnose the issue myself?

### Standalone code to reproduce the issue

```shell
TBD
```


### Relevant log output

```shell
INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK
Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0
INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK
Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0
INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')
INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).

...

INFO:tensorflow:batch_all_reduce: 499 all-reduces with algorithm = nccl, num_packs = 1
INFO:tensorflow:batch_all_reduce: 499 all-reduces with algorithm = nccl, num_packs = 1
INFO:tensorflow:batch_all_reduce: 499 all-reduces with algorithm = nccl, num_packs = 1
INFO:tensorflow:batch_all_reduce: 499 all-reduces with algorithm = nccl, num_packs = 1
4e409fc75fa0:30:152 [3] NCCL INFO Bootstrap : Using eth0:172.17.0.2<0>
4e409fc75fa0:30:152 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
4e409fc75fa0:30:152 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
4e409fc75fa0:30:152 [0] NCCL INFO cudaDriverVersion 11080
NCCL version 2.16.5+cuda11.8
4e409fc75fa0:30:536 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
4e409fc75fa0:30:536 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1

4e409fc75fa0:30:536 [0] nccl_net_ofi_init:1444 NCCL WARN NET/OFI Only EFA provider is supported

4e409fc75fa0:30:536 [0] nccl_net_ofi_init:1483 NCCL WARN NET/OFI aws-ofi-nccl initialization failed
4e409fc75fa0:30:536 [0] NCCL INFO NET/IB : No device found.
4e409fc75fa0:30:536 [0] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.2<0>
4e409fc75fa0:30:536 [0] NCCL INFO Using network Socket
4e409fc75fa0:30:539 [3] NCCL INFO Using network Socket
4e409fc75fa0:30:537 [1] NCCL INFO Using network Socket
4e409fc75fa0:30:538 [2] NCCL INFO Using network Socket
4e409fc75fa0:30:538 [2] NCCL INFO Trees [0] 1/-1/-1->2->3 [1] 3/-1/-1->2->1 [2] 1/-1/-1->2->3 [3] 3/-1/-1->2->1 [4] 1/-1/-1->2->3 [5] 3/-1/-1->2->1 [6] 1/-1/-1->2->3 [7] 3/-1/-1->2->1
4e409fc75fa0:30:538 [2] NCCL INFO P2P Chunksize set to 524288
4e409fc75fa0:30:537 [1] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] 2/-1/-1->1->-1 [2] -1/-1/-1->1->2 [3] 2/-1/-1->1->-1 [4] -1/-1/-1->1->2 [5] 2/-1/-1->1->-1 [6] -1/-1/-1->1->2 [7] 2/-1/-1->1->-1
4e409fc75fa0:30:537 [1] NCCL INFO P2P Chunksize set to 524288
4e409fc75fa0:30:536 [0] NCCL INFO Channel 00/08 :    0   1   2   3
4e409fc75fa0:30:536 [0] NCCL INFO Channel 01/08 :    0   3   2   1
4e409fc75fa0:30:536 [0] NCCL INFO Channel 02/08 :    0   3   1   2
4e409fc75fa0:30:536 [0] NCCL INFO Channel 03/08 :    0   2   1   3
4e409fc75fa0:30:536 [0] NCCL INFO Channel 04/08 :    0   1   2   3
4e409fc75fa0:30:536 [0] NCCL INFO Channel 05/08 :    0   3   2   1
4e409fc75fa0:30:536 [0] NCCL INFO Channel 06/08 :    0   3   1   2
4e409fc75fa0:30:539 [3] NCCL INFO Trees [0] 2/-1/-1->3->0 [1] 0/-1/-1->3->2 [2] 2/-1/-1->3->0 [3] 0/-1/-1->3->2 [4] 2/-1/-1->3->0 [5] 0/-1/-1->3->2 [6] 2/-1/-1->3->0 [7] 0/-1/-1->3->2
4e409fc75fa0:30:539 [3] NCCL INFO P2P Chunksize set to 524288
4e409fc75fa0:30:536 [0] NCCL INFO Channel 07/08 :    0   2   1   3
4e409fc75fa0:30:536 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1 [1] -1/-1/-1->0->3 [2] 3/-1/-1->0->-1 [3] -1/-1/-1->0->3 [4] 3/-1/-1->0->-1 [5] -1/-1/-1->0->3 [6] 3/-1/-1->0->-1 [7] -1/-1/-1->0->3
4e409fc75fa0:30:536 [0] NCCL INFO P2P Chunksize set to 524288
4e409fc75fa0:30:539 [3] NCCL INFO Channel 00/0 : 3[1e0] -> 0[1b0] via P2P/direct pointer
4e409fc75fa0:30:537 [1] NCCL INFO Channel 00/0 : 1[1c0] -> 2[1d0] via P2P/direct pointer
4e409fc75fa0:30:539 [3] NCCL INFO Channel 03/0 : 3[1e0] -> 0[1b0] via P2P/direct pointer
4e409fc75fa0:30:537 [1] NCCL INFO Channel 02/0 : 1[1c0] -> 2[1d0] via P2P/direct pointer
4e409fc75fa0:30:538 [2] NCCL INFO Channel 00/0 : 2[1d0] -> 3[1e0] via P2P/direct pointer
4e409fc75fa0:30:539 [3] NCCL INFO Channel 04/0 : 3[1e0] -> 0[1b0] via P2P/direct pointer
4e409fc75fa0:30:537 [1] NCCL INFO Channel 04/0 : 1[1c0] -> 2[1d0] via P2P/direct pointer
4e409fc75fa0:30:536 [0] NCCL INFO Channel 00/0 : 0[1b0] -> 1[1c0] via P2P/direct pointer
4e409fc75fa0:30:538 [2] NCCL INFO Channel 04/0 : 2[1d0] -> 3[1e0] via P2P/direct pointer
4e409fc75fa0:30:539 [3] NCCL INFO Channel 07/0 : 3[1e0] -> 0[1b0] via P2P/direct pointer
4e409fc75fa0:30:537 [1] NCCL INFO Channel 06/0 : 1[1c0] -> 2[1d0] via P2P/direct pointer
4e409fc75fa0:30:536 [0] NCCL INFO Channel 04/0 : 0[1b0] -> 1[1c0] via P2P/direct pointer
4e409fc75fa0:30:538 [2] NCCL INFO Channel 02/0 : 2[1d0] -> 0[1b0] via P2P/direct pointer
4e409fc75fa0:30:539 [3] NCCL INFO Channel 02/0 : 3[1e0] -> 1[1c0] via P2P/direct pointer
4e409fc75fa0:30:537 [1] NCCL INFO Channel 03/0 : 1[1c0] -> 3[1e0] via P2P/direct pointer
4e409fc75fa0:30:536 [0] NCCL INFO Channel 03/0 : 0[1b0] -> 2[1d0] via P2P/direct pointer
4e409fc75fa0:30:538 [2] NCCL INFO Channel 06/0 : 2[1d0] -> 0[1b0] via P2P/direct pointer
4e409fc75fa0:30:539 [3] NCCL INFO Channel 06/0 : 3[1e0] -> 1[1c0] via P2P/direct pointer
4e409fc75fa0:30:537 [1] NCCL INFO Channel 07/0 : 1[1c0] -> 3[1e0] via P2P/direct pointer
4e409fc75fa0:30:536 [0] NCCL INFO Channel 07/0 : 0[1b0] -> 2[1d0] via P2P/direct pointer
4e409fc75fa0:30:538 [2] NCCL INFO Channel 01/0 : 2[1d0] -> 1[1c0] via P2P/direct pointer
4e409fc75fa0:30:536 [0] NCCL INFO Channel 01/0 : 0[1b0] -> 3[1e0] via P2P/direct pointer
4e409fc75fa0:30:538 [2] NCCL INFO Channel 03/0 : 2[1d0] -> 1[1c0] via P2P/direct pointer
4e409fc75fa0:30:536 [0] NCCL INFO Channel 02/0 : 0[1b0] -> 3[1e0] via P2P/direct pointer
4e409fc75fa0:30:539 [3] NCCL INFO Channel 01/0 : 3[1e0] -> 2[1d0] via P2P/direct pointer
4e409fc75fa0:30:537 [1] NCCL INFO Channel 01/0 : 1[1c0] -> 0[1b0] via P2P/direct pointer
4e409fc75fa0:30:538 [2] NCCL INFO Channel 05/0 : 2[1d0] -> 1[1c0] via P2P/direct pointer
4e409fc75fa0:30:536 [0] NCCL INFO Channel 05/0 : 0[1b0] -> 3[1e0] via P2P/direct pointer
4e409fc75fa0:30:539 [3] NCCL INFO Channel 05/0 : 3[1e0] -> 2[1d0] via P2P/direct pointer
4e409fc75fa0:30:537 [1] NCCL INFO Channel 05/0 : 1[1c0] -> 0[1b0] via P2P/direct pointer
4e409fc75fa0:30:538 [2] NCCL INFO Channel 07/0 : 2[1d0] -> 1[1c0] via P2P/direct pointer
4e409fc75fa0:30:536 [0] NCCL INFO Channel 06/0 : 0[1b0] -> 3[1e0] via P2P/direct pointer
4e409fc75fa0:30:538 [2] NCCL INFO Connected all rings
4e409fc75fa0:30:537 [1] NCCL INFO Connected all rings
4e409fc75fa0:30:537 [1] NCCL INFO Channel 01/0 : 1[1c0] -> 2[1d0] via P2P/direct pointer
4e409fc75fa0:30:539 [3] NCCL INFO Connected all rings
4e409fc75fa0:30:536 [0] NCCL INFO Connected all rings
4e409fc75fa0:30:537 [1] NCCL INFO Channel 03/0 : 1[1c0] -> 2[1d0] via P2P/direct pointer
4e409fc75fa0:30:537 [1] NCCL INFO Channel 05/0 : 1[1c0] -> 2[1d0] via P2P/direct pointer
4e409fc75fa0:30:537 [1] NCCL INFO Channel 07/0 : 1[1c0] -> 2[1d0] via P2P/direct pointer
4e409fc75fa0:30:538 [2] NCCL INFO Channel 01/0 : 2[1d0] -> 3[1e0] via P2P/direct pointer
4e409fc75fa0:30:538 [2] NCCL INFO Channel 02/0 : 2[1d0] -> 3[1e0] via P2P/direct pointer
4e409fc75fa0:30:538 [2] NCCL INFO Channel 03/0 : 2[1d0] -> 3[1e0] via P2P/direct pointer
4e409fc75fa0:30:539 [3] NCCL INFO Channel 01/0 : 3[1e0] -> 0[1b0] via P2P/direct pointer
4e409fc75fa0:30:538 [2] NCCL INFO Channel 05/0 : 2[1d0] -> 3[1e0] via P2P/direct pointer
4e409fc75fa0:30:539 [3] NCCL INFO Channel 02/0 : 3[1e0] -> 0[1b0] via P2P/direct pointer
4e409fc75fa0:30:538 [2] NCCL INFO Channel 06/0 : 2[1d0] -> 3[1e0] via P2P/direct pointer
4e409fc75fa0:30:539 [3] NCCL INFO Channel 05/0 : 3[1e0] -> 0[1b0] via P2P/direct pointer
4e409fc75fa0:30:538 [2] NCCL INFO Channel 07/0 : 2[1d0] -> 3[1e0] via P2P/direct pointer
4e409fc75fa0:30:539 [3] NCCL INFO Channel 06/0 : 3[1e0] -> 0[1b0] via P2P/direct pointer
4e409fc75fa0:30:536 [0] NCCL INFO Channel 00/0 : 0[1b0] -> 3[1e0] via P2P/direct pointer
4e409fc75fa0:30:536 [0] NCCL INFO Channel 03/0 : 0[1b0] -> 3[1e0] via P2P/direct pointer
4e409fc75fa0:30:536 [0] NCCL INFO Channel 04/0 : 0[1b0] -> 3[1e0] via P2P/direct pointer
4e409fc75fa0:30:536 [0] NCCL INFO Channel 07/0 : 0[1b0] -> 3[1e0] via P2P/direct pointer
4e409fc75fa0:30:539 [3] NCCL INFO Channel 00/0 : 3[1e0] -> 2[1d0] via P2P/direct pointer
4e409fc75fa0:30:539 [3] NCCL INFO Channel 02/0 : 3[1e0] -> 2[1d0] via P2P/direct pointer
4e409fc75fa0:30:538 [2] NCCL INFO Channel 00/0 : 2[1d0] -> 1[1c0] via P2P/direct pointer
4e409fc75fa0:30:539 [3] NCCL INFO Channel 03/0 : 3[1e0] -> 2[1d0] via P2P/direct pointer
4e409fc75fa0:30:538 [2] NCCL INFO Channel 02/0 : 2[1d0] -> 1[1c0] via P2P/direct pointer
4e409fc75fa0:30:539 [3] NCCL INFO Channel 04/0 : 3[1e0] -> 2[1d0] via P2P/direct pointer
4e409fc75fa0:30:538 [2] NCCL INFO Channel 04/0 : 2[1d0] -> 1[1c0] via P2P/direct pointer
4e409fc75fa0:30:539 [3] NCCL INFO Channel 06/0 : 3[1e0] -> 2[1d0] via P2P/direct pointer
4e409fc75fa0:30:538 [2] NCCL INFO Channel 06/0 : 2[1d0] -> 1[1c0] via P2P/direct pointer
4e409fc75fa0:30:539 [3] NCCL INFO Channel 07/0 : 3[1e0] -> 2[1d0] via P2P/direct pointer
4e409fc75fa0:30:537 [1] NCCL INFO Connected all trees
4e409fc75fa0:30:537 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
4e409fc75fa0:30:537 [1] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer
4e409fc75fa0:30:536 [0] NCCL INFO Connected all trees
4e409fc75fa0:30:538 [2] NCCL INFO Connected all trees
4e409fc75fa0:30:538 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
4e409fc75fa0:30:538 [2] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer
4e409fc75fa0:30:536 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
4e409fc75fa0:30:536 [0] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer
4e409fc75fa0:30:539 [3] NCCL INFO Connected all trees
4e409fc75fa0:30:539 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
4e409fc75fa0:30:539 [3] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer
4e409fc75fa0:30:538 [2] NCCL INFO comm 0x7f7cd93df490 rank 2 nranks 4 cudaDev 2 busId 1d0 commId 0xb631c8061e2f5303 - Init COMPLETE
4e409fc75fa0:30:536 [0] NCCL INFO comm 0x7f7cf2520cf0 rank 0 nranks 4 cudaDev 0 busId 1b0 commId 0xb631c8061e2f5303 - Init COMPLETE
4e409fc75fa0:30:539 [3] NCCL INFO comm 0x7f7cd93e1f20 rank 3 nranks 4 cudaDev 3 busId 1e0 commId 0xb631c8061e2f5303 - Init COMPLETE
4e409fc75fa0:30:537 [1] NCCL INFO comm 0x7f7ce1c85c50 rank 1 nranks 4 cudaDev 1 busId 1c0 commId 0xb631c8061e2f5303 - Init COMPLETE
```
</details>"
60362,Tensorflow 2.12 bazel failed!Cuda11.2+cuDNN8.1+Tensorrt7.2 under Ubuntu2004,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf2.12

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu2004

### Mobile device

_No response_

### Python version

3.8

### Bazel version

5.3

### GCC/Compiler version

9.4

### CUDA/cuDNN version

CUDA11.2/cuDNN8.1

### GPU model and memory

_No response_

##Current Behaviour?

TensorRT Version: 7.2.3.4 CUDA Version: 11.2 CUDNN Version: 8.1.1.33-1 Operating System: Ubuntu-20.04

https://github.com/tensorflow/tensorflow/blob/master/.bazelrc

Bazel build latest 2.12 source code failed:


##Standalone code to reproduce the issue

```shell
# Configuration: 605fee2c8aa1b68d8dcb7abeac1c0e77048ffd56710c9ebb1fee2729853263d3
# Execution platform: @local_execution_config_platform//:platform
tensorflow/compiler/xla/stream_executor/cuda/cuda_graph.cc: In function ‘tsl::StatusOr<stream_executor::gpu::OwnedCudaGraph> stream_executor::gpu::CaptureCudaGraph(stream_executor::Stream*, absl::lts_20220623::AnyInvocable<tsl::Status()>, cudaStreamCaptureMode)’:
tensorflow/compiler/xla/stream_executor/cuda/cuda_graph.cc:135:21: **error**: ‘cudaGraphDebugDotFlagsVerbose’ was not declared in this scope
  135 |         int flags = cudaGraphDebugDotFlagsVerbose;
      |                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~
tensorflow/compiler/xla/stream_executor/cuda/cuda_graph.cc:136:24: **error**: ‘cudaGraphDebugDotPrint’ was not declared in this scope
  136 |         if (auto err = cudaGraphDebugDotPrint(graph, file.c_str(), flags);
      |                        ^~~~~~~~~~~~~~~~~~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 11358.490s, Critical Path: 270.65s
INFO: 26270 processes: 9762 internal, 16508 local.
FAILED: Build did NOT complete successfully
```


### Relevant log output

_No response_</details>"
60361,Need Add Perl to Supported API Languages,"This page lists all the languages with support for the TensorFlow API:
https://www.tensorflow.org/api_docs

Thanks to an official grant from The Perl Foundation, we have recently released a Perl API:
https://github.com/EntropyOrg/perl-AI-TensorFlow-Libtensorflow
https://metacpan.org/dist/AI-TensorFlow-Libtensorflow

How do we go about getting Perl added to the list?"
60358,Training speed and GPU utilization 50% lower after upgrading from 2.9 to 2.12,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

0.27.1

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.8/8.6

### GPU model and memory

A100

### Current Behaviour?

After upgrading TF from 2.9 to 2.12, when training our point segmentation model, the GPU utilization dropped from 70% to 30%, and the training speed is 50% slower. Is there any suggestion why this may happen? And what's the best way to debug it?

### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

_No response_</details>"
60351,//tensorflow/python/client:session_partial_run_test is flaky,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

5.3.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

Unit test reports as FLAKY or FAILED.

See https://source.cloud.google.com/results/invocations/dea422ff-7e14-4fc1-b324-0129ecd7ffbc/log or https://github.com/tensorflow/tensorflow/actions/runs/4731924097/jobs/8397430880#step:5:23224

### Standalone code to reproduce the issue

```shell
docker exec tf bazel --bazelrc=/usertools/cpu.bazelrc test --config=rbe --config=pycpp --config=build_event_export
```


### Relevant log output

```shell
======================================================================
ERROR: testPartialRunMissingPlaceholderFeedExceptionDist (__main__.PartialRunTest)
PartialRunTest.testPartialRunMissingPlaceholderFeedExceptionDist
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/client/session_partial_run_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1379, in _do_call
    return fn(*args)
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/client/session_partial_run_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1369, in _prun_fn
    return self._call_tf_sessionprun(handle, feed_dict, fetch_list)
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/client/session_partial_run_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1460, in _call_tf_sessionprun
    return tf_session.TF_SessionPRun_wrapper(self._session, handle, feed_dict,
tensorflow.python.framework.errors_impl.InternalError: From /job:localhost/replica:0/task:0:
ValidateDevices called before initialization.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/client/session_partial_run_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1629, in decorated
    return f(self, *args, **kwargs)
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/client/session_partial_run_test.runfiles/org_tensorflow/tensorflow/python/client/session_partial_run_test.py"", line 269, in testPartialRunMissingPlaceholderFeedExceptionDist
    self.RunTestPartialRunMissingPlaceholderFeedException(
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/client/session_partial_run_test.runfiles/org_tensorflow/tensorflow/python/client/session_partial_run_test.py"", line 119, in RunTestPartialRunMissingPlaceholderFeedException
    sess.partial_run(handle, fetches[0])
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/client/session_partial_run_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1026, in partial_run
    return self._run(handle, fetches, feed_dict, None, None)
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/client/session_partial_run_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1192, in _run
    results = self._do_run(handle, final_targets, final_fetches,
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/client/session_partial_run_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1375, in _do_run
    return self._do_call(_prun_fn, handle, feeds, fetches)
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/client/session_partial_run_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1398, in _do_call
    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter
tensorflow.python.framework.errors_impl.InternalError: Graph execution error:

From /job:localhost/replica:0/task:0:
ValidateDevices called before initialization.

----------------------------------------------------------------------
Ran 25 tests in 2.625s

FAILED (errors=1, skipped=1)
================================================================================
```
</details>"
60350,Use optimized Convolution2DTransposeBias Layer in tflite,"### 1. System information

- Windows 11 x64
- tensorflow==2.11.0

### 2. Code
```python
import tensorflow as tf
from tensorflow.keras.layers import (Conv2D, Conv2DTranspose, Input)
from tensorflow.keras.models import Model

input_size=(144, 256, 3)

inputs = Input(input_size, batch_size=1)
conv2d = Conv2D(16, kernel_size=(3, 3), strides=(2, 2), padding='same', use_bias=True)(inputs)

segment = Conv2DTranspose(2, kernel_size=(2, 2), strides=(2, 2), use_bias=True)(conv2d)

model = Model(inputs, segment, name=""test"")

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_types = [tf.float16]
tflite_model = converter.convert()

with open('model.tflite', 'wb') as f:
  f.write(tflite_model)
```

### 3. Failure after conversion

I want to convert a keras model to tflite while using optimized operations such as ""Convolution2DTransposeBias"". Those are discussed here: [Link](https://github.com/google/mediapipe/issues/245) 


### 5. (optional) Any other info / logs
How can I tell tflite to use those custom op after converting? 

Thanks in advance!
"
60348,data and ml module missing,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2

### Custom Code

No

### OS Platform and Distribution

mac

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

A bug happened!

### Standalone code to reproduce the issue

```shell
from data import BodyPart
from ml import Movenet
```


### Relevant log output

_No response_</details>"
60345,"RuntimeError: Given shapes, [1,784] and [784,100], are not broadcastable.Node number 1 (MUL) failed to prepare.","### 1. System information

I am using Google Colab. Gives the output Linux a0c9eeb98a07 5.10.147+ #1 SMP Sat Dec 10 16:00:40 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux, when I run uname -a.

Tensorflow Version: 2.12.0


### 2. Code
Reference [TensorFlow Model Colab](https://colab.research.google.com/drive/1Qq2CdEKDbV-Y5y2EFV3LxQgSPly_PbdT?usp=sharing): The basic conversion to TFLite without any representative dataset works just fine. However, the problematic portions are cells 4, and 5, where I am trying to perform a TFLite conversion with a representative dataset.  


### 3. Failure after conversion
It throws the following error.
```
WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.
INFO:tensorflow:Assets written to: /tmp/tmpz4fb25rp/assets
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
[<ipython-input-6-3bd48f77d98d>](https://localhost:8080/#) in <cell line: 10>()
      8 converter.inference_output_type = tf.uint8
      9 
---> 10 tflite_model_quant = converter.convert()
     11 
     12 interpreter = tf.lite.Interpreter(model_content=tflite_model_quant)

13 frames
[/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/lite.py](https://localhost:8080/#) in convert(self)
   1895         Invalid quantization parameters.
   1896     """"""
-> 1897     return super(TFLiteConverterV2, self).convert()
   1898 
   1899 

[/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/lite.py](https://localhost:8080/#) in wrapper(self, *args, **kwargs)
    960   def wrapper(self, *args, **kwargs):
    961     # pylint: disable=protected-access
--> 962     return self._convert_and_export_metrics(convert_func, *args, **kwargs)
    963     # pylint: enable=protected-access
    964 

[/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/lite.py](https://localhost:8080/#) in _convert_and_export_metrics(self, convert_func, *args, **kwargs)
    938     self._save_conversion_params_metric()
    939     start_time = time.process_time()
--> 940     result = convert_func(self, *args, **kwargs)
    941     elapsed_time_ms = (time.process_time() - start_time) * 1000
    942     if result:

[/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/lite.py](https://localhost:8080/#) in convert(self)
   1534     """"""
   1535     if self.experimental_lower_to_saved_model:
-> 1536       saved_model_convert_result = self._convert_as_saved_model()
   1537       if saved_model_convert_result:
   1538         return saved_model_convert_result

[/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/lite.py](https://localhost:8080/#) in _convert_as_saved_model(self)
   1514       if self.saved_model_dir:
   1515         self._validate_inputs(graph_def, input_tensors)
-> 1516         return self._convert_from_saved_model(graph_def)
   1517     finally:
   1518       shutil.rmtree(temp_dir, True)

[/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/lite.py](https://localhost:8080/#) in _convert_from_saved_model(self, graph_def)
   1129 
   1130     result = _convert_saved_model(**converter_kwargs)
-> 1131     return self._optimize_tflite_model(
   1132         result, quant_mode, quant_io=self.experimental_new_quantizer)
   1133 

[/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/convert_phase.py](https://localhost:8080/#) in wrapper(*args, **kwargs)
    213       except Exception as error:
    214         report_error_message(str(error))
--> 215         raise error from None  # Re-throws the exception.
    216 
    217     return wrapper

[/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/convert_phase.py](https://localhost:8080/#) in wrapper(*args, **kwargs)
    203     def wrapper(*args, **kwargs):
    204       try:
--> 205         return func(*args, **kwargs)
    206       except ConverterError as converter_error:
    207         if converter_error.errors:

[/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/lite.py](https://localhost:8080/#) in _optimize_tflite_model(self, model, quant_mode, quant_io)
    897         q_allow_float = quant_mode.is_allow_float()
    898         q_variable_quantization = quant_mode.enable_mlir_variable_quantization
--> 899         model = self._quantize(model, q_in_type, q_out_type, q_activations_type,
    900                                q_bias_type, q_allow_float,
    901                                q_variable_quantization)

[/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/lite.py](https://localhost:8080/#) in _quantize(self, result, input_type, output_type, activations_type, bias_type, allow_float, enable_variable_quantization)
    636                                                 custom_op_registerers_by_func)
    637     if self._experimental_calibrate_only or self.experimental_new_quantizer:
--> 638       calibrated = calibrate_quantize.calibrate(
    639           self.representative_dataset.input_gen)
    640 

[/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/convert_phase.py](https://localhost:8080/#) in wrapper(*args, **kwargs)
    213       except Exception as error:
    214         report_error_message(str(error))
--> 215         raise error from None  # Re-throws the exception.
    216 
    217     return wrapper

[/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/convert_phase.py](https://localhost:8080/#) in wrapper(*args, **kwargs)
    203     def wrapper(*args, **kwargs):
    204       try:
--> 205         return func(*args, **kwargs)
    206       except ConverterError as converter_error:
    207         if converter_error.errors:

[/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/optimize/calibrator.py](https://localhost:8080/#) in calibrate(self, dataset_gen)
    224       dataset_gen: A generator that generates calibration samples.
    225     """"""
--> 226     self._feed_tensors(dataset_gen, resize_input=True)
    227     return self._calibrator.Calibrate()

[/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/optimize/calibrator.py](https://localhost:8080/#) in _feed_tensors(self, dataset_gen, resize_input)
    127                                      signature_key)
    128           else:
--> 129             self._calibrator.Prepare([list(s.shape) for s in input_array])
    130         else:
    131           if signature_key is not None:

RuntimeError: Given shapes, [1,784] and [784,100], are not broadcastable.Node number 1 (MUL) failed to prepare.
```
Following is the netron image and the problematic node info.
![image](https://user-images.githubusercontent.com/37201355/232685234-0761417e-37e6-494d-ac04-77079c1f96dd.png)

I do not suspect the issue to be with the broadcasting of scalar r in the multiplication with W1_1, because the code [here](https://colab.research.google.com/drive/1jOs_vcne2-vzf2SnB-6-PRkdGk3s9wq3?usp=sharing) works just fine. The only difference in this new code is that the __call__ implementation only takes r as the input argument and returns the product of r and W1_1, which does not emit the error shown above, even though there is a broadcast."
60344,"""Affected Versions"" discrepancies between TFSA and GHSA","### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: N/A
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: N/A
-   **TensorFlow installed from (source or binary)**: N/A
-   **TensorFlow version (use command below)**: N/A
-   **Python version**: N/A
-   **Bazel version (if compiling from source)**: N/A
-   **GCC/Compiler version (if compiling from source)**: N/A
-   **CUDA/cuDNN version**: N/A
-   **GPU model and memory**: N/A
-   **Exact command to reproduce**: N/A


### Describe the problem
There are ""affected versions"" discrepancies between [GHSA](https://github.com/tensorflow/tensorflow/security) and [TFSA](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/security) security advisories. I don't really expect this to be resolved, as the affected versions differ on almost every advisory, but perhaps it could be made clear which one of these advisories users should view as the source of truth? The security readme mentions that the TFSA list might be sunset, and only GHSA will be used, but is that definitely the case? Before that happens, which repository for security advisories should be viewed as the truth, since they are both still getting published to? Thank you!

### Source code / logs
An example of this is the [GHSA entry](https://github.com/tensorflow/tensorflow/security/advisories/GHSA-rww7-2gpw-fv6j) for CVE-2022-23572, which lists the affected versions as `< 2.8.0`, while the [TFSA Entry](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-035.md) lists the affected version as `>= 2.6.0, < 2.8.0` in the [security readme](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/security). 
"
60343,build from source in jetson nano bazel report error,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

r2.6

### Custom Code

Yes

### OS Platform and Distribution

linux ubuntu18.04 jetpack461

### Mobile device

_No response_

### Python version

3.8

### Bazel version

3.7.2

### GCC/Compiler version

7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

i want install tensorflow-cpu==2.6 on jetson nano and python>=3.8 jetpack 4.61

### Standalone code to reproduce the issue

```shell
git checkout r2.6
./configure 
You have bazel 3.7.2- (@non-git) installed.
Please specify the location of python. [Default is /home/jetson/miniforge3/envs/py38/bin/python3]: 


Found possible Python library paths:
  /home/jetson/miniforge3/envs/py38/lib/python3.8/site-packages
Please input the desired Python library path to use.  Default is [/home/jetson/miniforge3/envs/py38/lib/python3.8/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: 
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: 
No CUDA support will be enabled for TensorFlow.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: 
Clang will not be downloaded.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: 
Not configuring the WORKSPACE for Android builds.

bazel --output_base=/home/jetson/experiment/tensorflow_pkg build //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by com.google.devtools.build.lib.unsafe.StringUnsafe (file:/home/jetson/.cache/bazel/_bazel_jetson/install/55993bca119e0c9410de59d485012347/A-server.jar) to constructor java.lang.String(byte[],byte)
WARNING: Please consider reporting this to the maintainers of com.google.devtools.build.lib.unsafe.StringUnsafe
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
```
</details>"
60341,Importing Tensorflow on Databricks after setting a log configuration causes next cell to hang indefinitely,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Databricks runtime 11.3 LTS ML

### Mobile device

_No response_

### Python version

3.9.5

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

On Databricks: importing tensorflow after setting a logz.io logging configuration casues the next cell to hang indefinitely roughly 50% of the time.

In the screenshot below, I've cancelled a Databricks command after it spent over 10 minutes calculating `1+1` after a tensorflow import. 

![tensorflow import example](https://user-images.githubusercontent.com/13903284/232449271-3fae830a-3b30-480f-bbb5-ba01de79a181.png)

This only happens when I've set a logz.io logging configuration before importing tensorflow, as shown in the **Standalone code to reproduce this issue**.  Tensorflow seems to pick up the log configuration, as the import outputs this message when the logz.io-token is invalid:

![image](https://user-images.githubusercontent.com/13903284/232451242-382f6b68-96fc-46c7-8a8d-f5846eafb3a6.png)

And this _somehow_ seems to be related to the issue, as I've only had it happen after I've set the log configuration.

The same issue occurs when using tf-nightly. It can be observed by adding a magic command: `%pip install tf-nightly` to the beginning of the standalone code below. Doing so reveals that tensorboard and logzio-python-handler have several common dependencies, and there could in principle be underlying conflicts. But please note that the issue also occurs with tensorflow 2.9.1 and logzio-python-handler 3.1.1, which were specifically chosen to not have version-conflicts in the common transitive dependencies:
```
tensorboard 2.9.1 requires google-auth-oauthlib<0.5,>=0.4.1, but you have google-auth-oauthlib 1.0.0 which is incompatible.
tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.22.3 which is incompatible.
tensorboard 2.9.1 requires tensorboard-data-server<0.7.0,>=0.6.0, but you have tensorboard-data-server 0.7.0 which is incompatible.
```

Do anyone have ideas about this? Does tensorflow have some internal logging that might cause conflicts?

**Expected behaviour**
Running a cell after importing tensorflow should just work.

### Standalone code to reproduce the issue

```shell
# Databricks notebook source
# MAGIC %pip install logzio-python-handler==3.1.1

# COMMAND ----------

import logging
import logging.config


try:
  token = dbutils.secrets.get('default', 'logzio_token')
except:
  token = ""test_value""

LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'logzioFormat': {
            'format': '{""component"": ""test_comp"", ""res_cluster"": ""test_cluster""}',
            'validate': False
        }
    },
    'handlers': {
        'logzio': {
            'class': 'logzio.handler.LogzioHandler',
            'level': 'INFO',
            'formatter': 'logzioFormat',
            'token': f'{token}',
            'logzio_type': 'Databricks',
            'logs_drain_timeout': 1,
            'url': 'https://listener-eu.logz.io:8071',
            'debug': False
        }
    },
    'loggers': {
        '': {
            'level': 'INFO',
            'handlers': ['logzio'],
            'propagate': True
        }
    }
}

logging.config.dictConfig(LOGGING)
logger = logging.getLogger('DatabricksLogger')

# COMMAND ----------

import tensorflow as tf

# COMMAND ----------

1+1
```


### Relevant log output

_No response_</details>"
60340,Training data format for EfficientNet,"I would like to train a EfficientNetB7 model from scratch to classify 2D arrays into two 2 classes, but it seems like I did not prepare my data in the correct format. Currently my `x_train` is a list of float64 arrays with a 600x600 size, my `y_train` is a list of integers that are either 0 or 1. Of course `x_train` and `y_train` have the same length. This is what I have so far:
```
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.applications import EfficientNetB7
from tensorflow.keras.models import Sequential

base_model = EfficientNetB7(include_top=False, weights=None, input_shape=(600,600,1), classes=2)
base_model.trainable = True
model = Sequential()
model.add(base_model)
model.add(Flatten())
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history = model.fit(x_train, y_train, batch_size=8, epochs=100)
```
The last line currently gives me an error: 
```
ValueError: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {""<class 'numpy.ndarray'>""}), (<class 'list'> containing values of types {""<class 'int'>""})
```
What would be the right format for training data? Any help is appreciated! "
60339,*Lookup layers scale as vocab size increases.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

v2.11.0-rc2-17-gd5b57ca93e5 2.11.0

### Custom Code

No

### OS Platform and Distribution

os platform: Linux-4.19.0-23-cloud-amd64-x86_64-with-debian-10.13 linux distribution: ('debian', '10.13', '')

### Mobile device

_No response_

### Python version

python version: 3.7.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2   

### GPU model and memory

_No response_

### Current Behaviour?

String/IntegerLookup layers underneath use the statichashtable. 
I have noticed as the vocab size increases, performance decreases, as I assumed it was basically a hash table with a O(1) lookup time, this shouldn't be the case. Is my assumption wrong on the time complexity?

### Standalone code to reproduce the issue

```shell
test = tf.keras.layers.StringLookup(name=f""{100000}_string_lookup"", vocabulary = list(map(str, range(100000))))
%timeit test(""23"")
>> 75.3 ms ± 430 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

test_100 = tf.keras.layers.StringLookup(name=f""{100}_string_lookup"", vocabulary = list(map(str, range(100))))
%timeit test_10(""23"")
>> 610 µs ± 21.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

testi = tf.keras.layers.IntegerLookup(name=f""{100000}_int_lookup"", vocabulary = list(range(100000)))
testi_100 = tf.keras.layers.IntegerLookup(name=f""{100}_string_lookup"", vocabulary = list(range(100)))

%timeit testi(23)
%timeit testi_100(23)

>>75.9 ms ± 421 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
>>816 µs ± 21.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```


### Relevant log output

```shell
75.3 ms ± 430 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
610 µs ± 21.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

75.9 ms ± 421 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
816 µs ± 21.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```
</details>"
60338,Accuracy is not working in the compile method for TensorFlow Keras Sequential,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf v2.12.0-rc1-12-g0db597d0d75 2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Arch Linux | BUILD_ID: rolling | Linux kernel 6.2.10

### Mobile device

_No response_

### Python version

Python 3.10.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

cuda 12.1.0-3 | cudnn 8.8.0.121-1

### GPU model and memory

GPU: NVIDIA GeForce GTX 1080 | Memory: 8388608 kBytes

### Current Behaviour?

I expect the same result if I used the metric with quotes and if I used without quotes. For me, it means there is something wrong at code level, maybe I am wrong.

The word accuracy with quotes, I got these values:

```python
self.__model.compile(
    optimizer=Adam(),
    loss=categorical_crossentropy,
    metrics=['accuracy', Precision()]
)
```
![Figure_2_with_quotes](https://user-images.githubusercontent.com/831380/232268485-e99d2947-47b5-4657-858a-72a253497c84.png)


The word accuracy without quotes, I got these values:

```python
from tensorflow.python.keras.metrics import accuracy, Precision
self.__model.compile(
    optimizer=Adam(),
    loss=categorical_crossentropy,
    metrics=[accuracy, Precision()]
)
```

![Figure_2_without_quotes](https://user-images.githubusercontent.com/831380/232268342-6ce45fef-c389-4ff3-9632-6e9fa78c8303.png)

---

Note:

I can't reproduce my example in TF Nightly (v1.12.1-92737-g0ae43a3fd1f 2.13.0-dev20230415) because it throws an error: `AttributeError: module 'tensorflow.python.distribute.input_lib' has no attribute 'DistributedDatasetInterface'. Did you mean: 'DistributedDatasetSpec'?`

### Standalone code to reproduce the issue

```shell
As, I show in this Jupyter Notebook. With the same data, if I change the metrics in the compile method with quotes `'accuracy'` or without quotes `accuracy`, it shows different results.

I repeat three times these steps in the Jupyter notebook, to avoid any fluctuation.

Bug-TensorFlow-Keras-Sequential-compile-metrics.ipynb
https://colab.research.google.com/drive/1sle0JaEl-hdRCKeMQCUdY43nNWnYY4mB
```


### Relevant log output

```shell
Without quotes:

{'loss': [1808.695068359375, 202.96279907226562, 54.72672653198242], 'accuracy': [0.5349747538566589, 0.4256841242313385, 0.16613984107971191], 'precision_9': [0.2498059868812561, 0.24983149766921997, 0.2497267723083496], 'val_loss': [443.4054870605469, 110.1395034790039, 11.466686248779297], 'val_accuracy': [0.532325029373169, 0.38062500953674316, 0.001075000036507845], 'val_precision_9': [0.24899999797344208, 0.24692469835281372, 0.25175511837005615]}

{'loss': [1874.009521484375, 148.31224060058594, 2.5201101303100586], 'accuracy': [0.5312250256538391, 0.35342004895210266, 0.0018018229166045785], 'precision_11': [0.25014790892601013, 0.249727264046669, 0.2505243122577667], 'val_loss': [175.01815795898438, 16.236042022705078, 1.3863223791122437], 'val_accuracy': [0.44843751192092896, 0.018687499687075615, 0.0], 'val_precision_11': [0.24866242706775665, 0.2497398555278778, 0.0]}

{'loss': [1835.9310302734375, 164.72079467773438, 33.215553283691406], 'accuracy': [0.5334452986717224, 0.38880130648612976, 0.09400103986263275], 'precision_13': [0.25037264823913574, 0.2497129887342453, 0.25012683868408203], 'val_loss': [331.0498046875, 54.01085662841797, 2.3827459812164307], 'val_accuracy': [0.4865500032901764, 0.18593749403953552, 0.0], 'val_precision_13': [0.24895000457763672, 0.2504253089427948, 0.2584269642829895]}


With quotes:

{'loss': [2104.91552734375, 218.6924591064453, 41.532161712646484], 'accuracy': [0.24953334033489227, 0.25047290325164795, 0.2501385509967804], 'precision_10': [0.24953436851501465, 0.2504711151123047, 0.25022196769714355], 'val_loss': [299.38861083984375, 87.65056610107422, 1.386319637298584], 'val_accuracy': [0.24914999306201935, 0.2533999979496002, 0.24940000474452972], 'val_precision_10': [0.24914999306201935, 0.25342532992362976, 0.0]}

{'loss': [1932.6285400390625, 215.28610229492188, 62.687828063964844], 'accuracy': [0.25043436884880066, 0.24967291951179504, 0.24986353516578674], 'precision_12': [0.25043463706970215, 0.2496640384197235, 0.24984458088874817], 'val_loss': [281.5032653808594, 66.43962860107422, 21.2304744720459], 'val_accuracy': [0.2502000033855438, 0.25029999017715454, 0.25209999084472656], 'val_precision_12': [0.2502000033855438, 0.2503626048564911, 0.25167566537857056]}

{'loss': [1990.3583984375, 205.70851135253906, 56.19692611694336], 'accuracy': [0.2503645718097687, 0.24988020956516266, 0.24988438189029694], 'precision_14': [0.25036510825157166, 0.2498749941587448, 0.24987071752548218], 'val_loss': [414.74072265625, 72.6060562133789, 28.769975662231445], 'val_accuracy': [0.24705000221729279, 0.25224998593330383, 0.24639999866485596], 'val_precision_14': [0.24705000221729279, 0.2522878348827362, 0.24636809527873993]}
```
</details>"
60337,have a bug about AVX2 FMA,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12

### Custom Code

Yes

### OS Platform and Distribution

ubuntu 2004

### Mobile device

_No response_

### Python version

Python3.8.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.3

### GPU model and memory

_No response_

### Current Behaviour?

 don't train,I want to fix it

### Standalone code to reproduce the issue

```shell
2023-04-16 18:41:59.167857: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Segmentation fault (core dumped)
```


### Relevant log output

```shell
2023-04-16 18:41:59.167857: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Segmentation fault (core dumped)
```
</details>"
60336,TPU Tensorflow mapping string label to int with ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12

### Custom Code

Yes

### OS Platform and Distribution

Colab

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

None

### GPU model and memory

None

### Current Behaviour?

I currently get an error when trying to get my batch from a tf.dataset. I am mapping the string label in the tfrecord into int with tf.lookup.StaticHashTable. 
```
InternalError: failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:37044: Failed to connect to remote host: Connection refused
Additional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:
```

Because of that I can't get the batch of my dataset, and train a model with TPU. It works fine with GPU.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1vAADMl5fBulmSnbmbOTrMjyzAYAgHhFl?authuser=1#scrollTo=_zv9OlXbIqDf
```


### Relevant log output

```shell
AttributeError                            Traceback (most recent call last)

/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/ops/iterator_ops.py in _next_internal(self)
    786         # Fast path for the case `self._structure` is not a nested structure.
--> 787         return self._element_spec._from_compatible_tensor_list(ret)  # pylint: disable=protected-access
    788       except AttributeError:

AttributeError: 'tuple' object has no attribute '_from_compatible_tensor_list'


During handling of the above exception, another exception occurred:

InternalError                             Traceback (most recent call last)

13 frames

InternalError: failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:37044: Failed to connect to remote host: Connection refused
Additional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:
:UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:37044: Failed to connect to remote host: Connection refused {grpc_status:14, created_time:""2023-04-16T09:15:30.550805248+00:00""}
Executing non-communication op <MakeIterator> originally returned UnavailableError, and was replaced by InternalError to avoid invoking TF network error handling logic.


During handling of the above exception, another exception occurred:

InternalError                             Traceback (most recent call last)

/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/executor.py in wait(self)
     63   def wait(self):
     64     """"""Waits for ops dispatched in this executor to finish.""""""
---> 65     pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)
     66 
     67   def clear_error(self):

InternalError: failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:37044: Failed to connect to remote host: Connection refused
Additional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:
:UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:37044: Failed to connect to remote host: Connection refused {grpc_status:14, created_time:""2023-04-16T09:15:30.550805248+00:00""}
Executing non-communication op <MakeIterator> originally returned UnavailableError, and was replaced by InternalError to avoid invoking TF network error handling logic.
```
</details>"
60335,Smart watch ,<spam removed>
60334,Is there some way to set the data type of the tflite model to int8 here?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

linux ubuntu 20

### Mobile device

linux ubuntu 20 

### Python version

3.10.6

### Bazel version

No bazel

### GCC/Compiler version

9.4

### CUDA/cuDNN version

No cuda 

### GPU model and memory

No Gpu

### Current Behaviour?

I'm new to tensorflow, I was doing mlir related work before, I'm trying to download a model to test what I'm doing, but I'm having a problem, the conv and matmul I implemented on mlir only support int8, I'd like to ask if there is a way to convert the data type of the tflite model here to int8.

### Standalone code to reproduce the issue

```shell
I just want to ask if there is a way to convert data types.
```


### Relevant log output

```shell
No log.
```
</details>"
60333,AttributeError: module 'tensorflow' has no attribute '__version__',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12

### Custom Code

Yes

### OS Platform and Distribution

win10

### Mobile device

_No response_

### Python version

Python3.8.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.7

### GPU model and memory

_No response_

### Current Behaviour?

A bug happened!

### Standalone code to reproduce the issue

```shell
code is too long
```


### Relevant log output

```shell
(reflow1) PS E:\Googledownload\RectifiedFlow-main\ImageGeneration> python ./main.py --config ./configs/rectified_flow/cifar10_rf_gaussian_ddpmpp.py --eval_folder eval --mode eval --workdir ./logs/1_rectified_flow --config.eval.enable_sampling  --config.eval.batch_size 250 --config.eval.num_samples 3000 --config.eval.begin_ckpt 8
Traceback (most recent call last):
  File ""./main.py"", line 18, in <module>
    import run_lib
  File ""E:\Googledownload\RectifiedFlow-main\ImageGeneration\run_lib.py"", line 26, in <module>
    import tensorflow_gan as tfgan
  File ""D:\conda\envs\reflow1\lib\site-packages\tensorflow_gan\__init__.py"", line 108, in <module>
    _ensure_tf_install()
  File ""D:\conda\envs\reflow1\lib\site-packages\tensorflow_gan\__init__.py"", line 60, in _ensure_tf_install
    if (distutils.version.LooseVersion(tf.__version__) <
AttributeError: module 'tensorflow' has no attribute '__version__'
```
</details>"
60332,KerasLegacyOptimizer fails type check in keras.optimizers.get (ValueError: Could not interpret optimizer identifier),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

RTX 2070

### Current Behaviour?

I am trying to create a customer optimizer using KerasLegacyOptimizer as a lot of the examples in https://github.com/tensorflow/addons/tree/master/tensorflow_addons/optimizers are using. Looks like we are failing at this line https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/optimizers.py#L118

Any ideas?

### Standalone code to reproduce the issue

```shell
from tensorflow.python.keras.models import Sequential
from tensorflow_addons.optimizers import KerasLegacyOptimizer


class CustomerOptimizer(KerasLegacyOptimizer):
    def __init__(
        self,
        name: str = ""CustomerOptimize"",
        **kwargs,
    ):
        super().__init__(name, **kwargs)


model = Sequential()
optimizer = CustomerOptimizer()
model.compile(optimizer=optimizer)
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""C:\Users\Ben\PycharmProjects\AutoTab\test2.py"", line 17, in <module>
    model.compile(optimizer=optimizer)
  File ""C:\Users\Ben\PycharmProjects\AutoTab\venv\Lib\site-packages\tensorflow\python\keras\engine\training.py"", line 568, in compile
    self.optimizer = self._get_optimizer(optimizer)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Ben\PycharmProjects\AutoTab\venv\Lib\site-packages\tensorflow\python\keras\engine\training.py"", line 606, in _get_optimizer
    return nest.map_structure(_get_single_optimizer, optimizer)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Ben\PycharmProjects\AutoTab\venv\Lib\site-packages\tensorflow\python\util\nest.py"", line 917, in map_structure
    structure[0], [func(*x) for x in entries],
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Ben\PycharmProjects\AutoTab\venv\Lib\site-packages\tensorflow\python\util\nest.py"", line 917, in <listcomp>
    structure[0], [func(*x) for x in entries],
                   ^^^^^^^^
  File ""C:\Users\Ben\PycharmProjects\AutoTab\venv\Lib\site-packages\tensorflow\python\keras\engine\training.py"", line 597, in _get_single_optimizer
    opt = optimizers.get(opt)
          ^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Ben\PycharmProjects\AutoTab\venv\Lib\site-packages\tensorflow\python\keras\optimizers.py"", line 131, in get
    raise ValueError(
ValueError: Could not interpret optimizer identifier: <__main__.CustomerOptimizer object at 0x0000024363063350>
```
</details>"
60330,element-wise multiplication overflow with large dimension tensors,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.9.3

### Custom Code

Yes

### OS Platform and Distribution

ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.6/8

### GPU model and memory

single A100 80G

### Current Behaviour?

tested with tensorflow==2.9.3 and numpy==1.24.2 on single A100 80G GPU. If use small memory GPU, you may get OOM before reproducing the issue.

when using dimension (524288, 16, 9, 32), get illegal memory.
when using dimension (524288, 16, 8, 32), get Mismatched elements: 1024 / 2147483648 (4.77e-05%)
when using dimension (524288, 16, 7, 32), get correct values.

same behavior on eager mode and graph mode.
note: one related issue has been reported https://github.com/keras-team/tf-keras/issues/124

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

def test_mul_eager(input_shape):
    rng = np.random.RandomState(42)

    grad = rng.exponential(size=input_shape).astype(np.float32)
    grad_loss = rng.exponential(size=(input_shape[0],1,1,1)).astype(np.float32)
    
    with tf.device('/GPU:0'):
        tf_grad = tf.convert_to_tensor(grad)
        tf_grad_loss = tf.convert_to_tensor(grad_loss)
        out = tf_grad * tf_grad_loss
        #tf.print(""==== shape "", tf_grad.shape, tf_grad_loss.shape, out.shape)


    with tf.device('/CPU:0'):
        out_cpu = tf.identity(out)
        tf_grad_cpu = tf.identity(tf_grad)
        tf_grad_loss_cpu = tf.identity(tf_grad_loss)
    #
    np.testing.assert_allclose(grad, tf_grad_cpu.numpy(), rtol=1e-5, atol=1e-4)
    np.testing.assert_allclose(grad_loss, tf_grad_loss_cpu.numpy(), rtol=1e-5, atol=1e-4)
    np.testing.assert_allclose(grad*grad_loss, out_cpu.numpy(), rtol=1e-5, atol=1e-4)

@tf.function
def compute_mul(b,t,u,v):
    with tf.device('/CPU:0'):
        x = tf.random.normal((1,t,u,v), dtype=tf.float32)
        y = tf.random.normal((1,1,1,1), dtype=tf.float32)
        tf_grad = tf.tile(x, (b,1,1,1))
        tf_grad_loss = tf.tile(y, (b,1,1,1))
    with tf.device('/GPU:0'):
        out = tf_grad * tf_grad_loss
        #out = tf.raw_ops.Mul(x=tf_grad, y=tf_grad_loss)
        #out = tf.multiply(tf_grad, tf_grad_loss)
    with tf.device('/CPU:0'):
        out_cpu = tf.identity(out)
        tf_grad_cpu = tf.identity(tf_grad)
        tf_grad_loss_cpu = tf.identity(tf_grad_loss)
    
    return out_cpu, tf_grad_cpu, tf_grad_loss_cpu

def test_mul_graph(input_shape):
    b,t,u,v = input_shape
    out_cpu, tf_grad_cpu, tf_grad_loss_cpu = compute_mul(b,t,u,v)
    
    np.testing.assert_allclose(tf_grad_cpu.numpy()*tf_grad_loss_cpu.numpy(), out_cpu.numpy(), rtol=1e-5, atol=1e-4)

if __name__ == '__main__':
    #input_shape = (524288, 16, 7, 32) # pass <2^31
    input_shape = (524288, 16, 8, 32) # value mismatch at 2^31 
    #input_shape = (524288, 16, 9, 32) # illegal memory access
    
    #test_mul_eager(input_shape)
    test_mul_graph(input_shape)
```


### Relevant log output

```shell
when using (524288, 16, 8, 32), got
Traceback (most recent call last):
  File ""multiplication_mismatch.py"", line 93, in <module>
    test_mul_graph(input_shape)
  File ""multiplication_mismatch.py"", line 85, in test_mul_graph
    np.testing.assert_allclose(tf_grad_cpu.numpy()*tf_grad_loss_cpu.numpy(), out_cpu.numpy(), rtol=1e-5, atol=1e-4)
  File ""/fs/scratch/work/yu_fang/warp-transducer/venv_tf_29/lib/python3.8/site-packages/numpy/testing/_private/utils.py"", line 1592, in assert_allclose
    assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
  File ""/usr/lib/python3.8/contextlib.py"", line 75, in inner
    return func(*args, **kwds)
  File ""/fs/scratch/work/yu_fang/warp-transducer/venv_tf_29/lib/python3.8/site-packages/numpy/testing/_private/utils.py"", line 862, in assert_array_compare
    raise AssertionError(msg)
AssertionError:
Not equal to tolerance rtol=1e-05, atol=0.0001

Mismatched elements: 1024 / 2147483648 (4.77e-05%)
Max absolute difference: 2.2970564
Max relative difference: 0.
 x: array([[[[ 1.605678, -0.261173, -1.222985, ..., -1.186496, -0.111071,
           0.792078],
         [ 0.307934,  0.016565, -0.576156, ..., -0.17745 , -0.993849,...
 y: array([[[[ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,
           0.      ],
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,...
```
</details>"
60329,Getting error while importing tensorflow,"```
import os
import pickle
import numpy as np
from tqdm.notebook import tqdm

from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical, plot_model
from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add
```
I was trying to import above modules in my PC

Getting below error as no module found.
Installed and reinstalled Tensorflow several times even in virtual environment this error is not going anywhere please help!
```
File D:\PY\Lib\site-packages\tensorflow\compiler\jit\ops\xla_ops.py:13
     11 from tensorflow.python.eager import execute as _execute
     12 from tensorflow.python.framework import dtypes as _dtypes
---> 13 from tensorflow.security.fuzzing.py import annotation_types as _atypes
     15 from tensorflow.python.framework import op_def_registry as _op_def_registry
     16 from tensorflow.python.framework import ops as _ops

ModuleNotFoundError: No module named 'tensorflow.security'
```


"
60328,Unit test failures on ARM_CI,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

5.3.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

Since the commit https://github.com/tensorflow/tensorflow/commit/11c29fb2c3e508c3459b55d6357d8ff1d18412ea ARM_CI has been showing unit test failures on
//bazel_pip/tensorflow/python/compiler/xla:xla_test_gpu
//bazel_pip/tensorflow/python/data/experimental/kernel_tests:checkpoint_input_pipeline_hook_test
//bazel_pip/tensorflow/python/distribute:parameter_server_strategy_test_cpu
//bazel_pip/tensorflow/python/compiler/xla:xla_test_cpu
//bazel_pip/tensorflow/python/distribute:parameter_server_strategy_test_gpu
//bazel_pip/tensorflow/core/platform:ram_file_system_test

### Standalone code to reproduce the issue

```shell
bazel --bazelrc /usertools/aarch64.bazelrc test --config=mkl_aarch64_threadpool --copt=-flax-vector-conversions --test_env=TF_ENABLE_ONEDNN_OPTS=1 --test_env=TF2_BEHAVIOR=1 --define=tf_api_version=2 --flaky_test_attempts=3 --test_output=errors --verbose_failures=true --test_keep_going --jobs=75 --notest_verbose_timeout_warnings --build_tests_only -- //tensorflow/core/platform:ram_file_system_test //tensorflow/python/distribute:parameter_server_strategy_test_gpu //tensorflow/python/compiler/xla:xla_test_cpu //tensorflow/python/distribute:parameter_server_strategy_test_cpu //tensorflow/python/data/experimental/kernel_tests:checkpoint_input_pipeline_hook_test //tensorflow/python/compiler/xla:xla_test_gpu
```


### Relevant log output

```shell
All tests fail with similar backtrace

INFO: From Testing //bazel_pip/tensorflow/core/platform:ram_file_system_test:
  File ""/tmpfs/bazel_output/_bazel_ubuntu/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/bazel_pip/tensorflow/core/platform/ram_file_system_test.runfiles/org_tensorflow/bazel_pip/tensorflow/core/platform/ram_file_system_test.py"", line 21, in <module>
    from tensorflow.python.estimator.estimator import Estimator
  File ""/tmpfs/bazel_output/_bazel_ubuntu/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/bazel_pip/tensorflow/core/platform/ram_file_system_test.runfiles/org_tensorflow/tensorflow/python/estimator/estimator.py"", line 22, in <module>
    from tensorflow_estimator.python.estimator import estimator
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>
    from tensorflow_estimator._api.v1 import estimator
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 11, in <module>
    from tensorflow_estimator._api.v1.estimator import tpu
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow_estimator/_api/v1/estimator/tpu/__init__.py"", line 12, in <module>
    from tensorflow_estimator.python.estimator.tpu.tpu_estimator import TPUEstimator
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 118, in <module>
    to_proto=resource_variable_ops._to_proto_fn,  # pylint: disable=protected-access
AttributeError: module 'tensorflow.python.ops.resource_variable_ops' has no attribute '_to_proto_fn'
```
</details>"
60326,ImportError: libtensorflow_cc.so.2: cannot open shared object file: No such file or directory,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.13.0

### Custom Code

Yes

### OS Platform and Distribution

Linux

### Mobile device

_No response_

### Python version

3.9

### Bazel version

6.1.1

### GCC/Compiler version

9.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I build the lastest TensorFlow code from source successfully with
`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`

Then I generate a TensorFlow whl successfully with
`./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg`

But when I pip install this whl, import tensorflow got ""ImportError: libtensorflow_cc.so.2: cannot open shared object file: No such file or directory"". 

I found the whl file generated is only 80.64M. But I think it should be about 200M.

But there has libtensorflow_cc.so.2 file under path tensorflow/bazel-bin/tensorflow. I don't know why it wasn't packed into the whl file.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/data1/envs/xtc3.9/lib/python3.9/site-packages/tensorflow/__init__.py"", line 38, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/data1/envs/xtc3.9/lib/python3.9/site-packages/tensorflow/python/__init__.py"", line 36, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""/data1/envs/xtc3.9/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 26, in <module>
    self_check.preload_check()
  File ""/data1/envs/xtc3.9/lib/python3.9/site-packages/tensorflow/python/platform/self_check.py"", line 63, in preload_check
    from tensorflow.python.platform import _pywrap_cpu_feature_guard
ImportError: libtensorflow_cc.so.2: cannot open shared object file: No such file or directory
```
</details>"
60325,Issue with obtaining files from this repo via Google Colab,"![Screenshot 2023-04-14 162503](https://user-images.githubusercontent.com/93562563/231988319-e5946ae6-30fe-4237-9617-c08e8ee8104c.png)
"
60324,Custom loss function with multiple arguments from generator,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I would like to know if it is possible to create a loss function not only get y_true and y_pred as parameters. 
So basically, I want to return 4 parameters in the custom generator but these 4 parameters are all used to calculate one single loss function. I haven't found any example or document about this issue.

### Standalone code to reproduce the issue

```shell
ValueError: Found unexpected losses or metrics that do not correspond to any Model output: dict_keys(['custom_metric']). Valid mode output names: ['association']. Received struct is: {'custom_metric': ((<tf.Tensor 'IteratorGetNext:2' shape=(None, None, None, None) dtype=float32>, <tf.Tensor 'IteratorGetNext:3' shape=(None, None, None, None) dtype=float32>, <tf.Tensor 'IteratorGetNext:4' shape=(None, None, None, None) dtype=float32>, <tf.Tensor 'IteratorGetNext:5' shape=(None, None, None, None) dtype=float32>, <tf.Tensor 'IteratorGetNext:6' shape=(None, None, None, None) dtype=float32>, <tf.Tensor 'IteratorGetNext:7' shape=(None, None, None, None) dtype=float32>), (<tf.Tensor 'IteratorGetNext:8' shape=(None, None, None) dtype=float32>, <tf.Tensor 'IteratorGetNext:9' shape=(None, None, None) dtype=float32>, <tf.Tensor 'IteratorGetNext:10' shape=(None, None, None) dtype=float32>, <tf.Tensor 'IteratorGetNext:11' shape=(None, None, None) dtype=float32>, <tf.Tensor 'IteratorGetNext:12' shape=(None, None, None) dtype=float32>, <tf.Tensor 'IteratorGetNext:13' shape=(None, None, None) dtype=float32>), (<tf.Tensor 'IteratorGetNext:14' shape=(None, None) dtype=int32>, <tf.Tensor 'IteratorGetNext:15' shape=(None, None) dtype=int32>, <tf.Tensor 'IteratorGetNext:16' shape=(None, None) dtype=int32>, <tf.Tensor 'IteratorGetNext:17' shape=(None, None) dtype=int32>, <tf.Tensor 'IteratorGetNext:18' shape=(None, None) dtype=int32>, <tf.Tensor 'IteratorGetNext:19' shape=(None, None) dtype=int32>), (<tf.Tensor 'IteratorGetNext:20' shape=(None, None) dtype=int32>, <tf.Tensor 'IteratorGetNext:21' shape=(None, None) dtype=int32>, <tf.Tensor 'IteratorGetNext:22' shape=(None, None) dtype=int32>, <tf.Tensor 'IteratorGetNext:23' shape=(None, None) dtype=int32>, <tf.Tensor 'IteratorGetNext:24' shape=(None, None) dtype=int32>, <tf.Tensor 'IteratorGetNext:25' shape=(None, None) dtype=int32>))}.
```


### Relevant log output

_No response_</details>"
60320,protobuf have a problem,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf2.11.0

### Custom Code

Yes

### OS Platform and Distribution

win10

### Mobile device

_No response_

### Python version

3.9.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.7

### GPU model and memory

_No response_

### Current Behaviour?

I don't know if there is a problem with my download or the version of tensorflow, that is, how to recompile protobuf. Can I download the tensorflow compressed package directly from GitHub and uninstall tensorflow-Intel without uninstalling the gpu version.

### Standalone code to reproduce the issue

```shell
PS E:\Googledownload\RectifiedFlow-main> & D:/conda/envs/reflow/python.exe e:/Googledownload/RectifiedFlow-main/ImageGeneration/1.py
Traceback (most recent call last):
  File ""e:\Googledownload\RectifiedFlow-main\ImageGeneration\1.py"", line 1, in <module>
    import tensorflow as tf
  File ""D:\conda\envs\reflow\lib\site-packages\tensorflow\__init__.py"", line 37, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""D:\conda\envs\reflow\lib\site-packages\tensorflow\python\__init__.py"", line 37, in <module>
    from tensorflow.python.eager import context
  File ""D:\conda\envs\reflow\lib\site-packages\tensorflow\python\eager\context.py"", line 28, in <module>
    from tensorflow.core.framework import function_pb2
  File ""D:\conda\envs\reflow\lib\site-packages\tensorflow\core\framework\function_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2
  File ""D:\conda\envs\reflow\lib\site-packages\tensorflow\core\framework\attr_value_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2
  File ""D:\conda\envs\reflow\lib\site-packages\tensorflow\core\framework\tensor_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2
  File ""D:\conda\envs\reflow\lib\site-packages\tensorflow\core\framework\resource_handle_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2
  File ""D:\conda\envs\reflow\lib\site-packages\tensorflow\core\framework\tensor_shape_pb2.py"", line 36, in <module>
    _descriptor.FieldDescriptor(
  File ""D:\conda\envs\reflow\lib\site-packages\google\protobuf\descriptor.py"", line 561, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
```


### Relevant log output

_No response_</details>"
60316,bug feedback,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf2.5

### Custom Code

Yes

### OS Platform and Distribution

Windows 10 21H2 9044.2604

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.6

### GPU model and memory

_No response_

### Current Behaviour?

A robustness problem happened!
When the number of inputs is greater than 1, the tanh function can still compute the inputs without throwing any exception.
But in other framework like PyTorch, I can get exception message immediately.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import torch
import mindspore as ms

if __name__ == '__main__':

    try:
        print(tf.tanh(tf.ones(2, 2), tf.ones(2, 2)))
    except Exception as e:
        print(e)

    try:
        print(torch.tanh(torch.ones(2, 2), torch.ones(2, 2)))
    except Exception as e:
        print(e)

    try:
        print(ms.ops.Tanh(ms.ops.Ones()((2, 2), ms.float32)), ms.ops.Ones()((2, 2), ms.float32))
    except Exception as e:
        print(e)
```


### Relevant log output

```shell
tf.Tensor([0.76159416 0.76159416], shape=(2,), dtype=float64)
tanh() takes 1 positional argument but 2 were given
too many positional arguments
```
</details>"
60315,bug feedback,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

tf2.5

### Custom Code

Yes

### OS Platform and Distribution

Windows 10 21H2 9044.2604

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.6

### GPU model and memory

_No response_

### Current Behaviour?

A bug happened!
When the input of Conv2D is inf, in_channels and out_channels are greater than 128, the output  is supposed to be inf, but got nan in fact.
And I can get correct output in PyTorch.

### Standalone code to reproduce the issue

```shell
data = np.array([np.inf for _ in range(1 * 128 * 6 * 6)])
data = np.reshape(data, newshape=[1, 128, 6, 6])
conv = tf.keras.layers.Conv2D(filters=256, kernel_size=3)
print(conv(data))
```


### Relevant log output

```shell
tf.Tensor(
[[[[nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]]

  [[nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]]

  [[nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]]

  ...

  [[nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]]

  [[nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]]

  [[nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]
   [nan nan nan ... nan nan nan]]]], shape=(1, 126, 4, 256), dtype=float32)
```
</details>"
60314,Backpropagation for all zero label vectors in softmax/crossentropy,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.13.0-dev20230413

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04.1 LTS

### Mobile device

_No response_

### Python version

Python 3.10.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

no GPU Used

### GPU model and memory

no GPU Used

### Current Behaviour?

The First approach to building a Softmax layer yields a non zero gradient in backpropagation for an all zero target vector. The second approach yields a zero gradient. I would expect the behaviour of the latter, as is mathematically correct. 
I am a bit unsure whether this is intentended behaviour or not. If it is intended, where does the difference come from? 

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import keras

x = tf.constant([[0.1,0.90]])
y = tf.constant([[0.0,.0]])

# First approach 
with tf.GradientTape() as g:
  g.watch(x)
  z = keras.Sequential(
         keras.layers.Dense(2, activation = ""softmax"", use_bias = False)
      )(x)
  z = tf.losses.categorical_crossentropy(y, z)
dz_dx = g.gradient(z, x)
print(dz_dx)



x = tf.constant([[0.1,0.90]])
y = tf.constant([[0.0,.0]])

# Second approach 
with tf.GradientTape() as g:
  g.watch(x)
  z = keras.Sequential(
         [keras.layers.Dense(2, activation = ""linear"", use_bias = False),   
          keras.layers.Softmax(axis = -1)]
      )(x)
  z = tf.losses.categorical_crossentropy(y, z)
dz_dx = g.gradient(z, x)
print(dz_dx)
```


### Relevant log output

_No response_</details>"
60313,"TensorFlow Lite in Play Services, app is crashing when creating interpreter","**System information**
- Android Device information: samsung a13 (5793 events, 31% of total events), samsung m11q, Redmi olivewood, samsung j6lte, samsung m13, samsung j7velte, samsung on7xelte
- TensorFlow Lite in Play Services SDK version: play-services-tflite-java:16.0.1, play-services-tflite-gpu:16.1.0
- Google Play Services version: don't know, never happened in dev devices.

**Standalone code to reproduce the issue**
```
InterpreterApi.create(new File(modelFile),
                        new InterpreterApi.Options()
                                .setRuntime(TfLiteRuntime.FROM_SYSTEM_ONLY)
                                .addDelegateFactory(new GpuDelegateFactory()));
```

**Any other info / logs**
We have started receiving this crash from 23 March 2023, app was updated a month ago before this(21st feb). When we stopped using GPU Delegate, toggled through Firebase Remote Config, the error went away.

Samsung a13 device alone has registered 5793 crashes, which is 31% of total events, based on Play Store Console. title in the console is ""[libtensorflowlite_jni_gms_client.so] Java_com_google_android_gms_tflite_NativeInterpreterWrapper_createInterpreter""

backtrace:
```
  #00  pc 0x000000000015643a  /data/user_de/0/com.google.android.gms/app_chimera/m/0000003c/dl-TfliteDynamiteDynamite.integ_224210505100300.apk
  #01  pc 0x0000000000156415  /data/user_de/0/com.google.android.gms/app_chimera/m/0000003c/dl-TfliteDynamiteDynamite.integ_224210505100300.apk
  #02  pc 0x0000000000059961  /data/user_de/0/com.google.android.gms/app_chimera/m/000000bd/dl-TfliteGpuDynamite.optional_231015100300.apk
  #03  pc 0x0000000000159279  /data/user_de/0/com.google.android.gms/app_chimera/m/0000003c/dl-TfliteDynamiteDynamite.integ_224210505100300.apk
  #04  pc 0x000000000014f4bf  /data/user_de/0/com.google.android.gms/app_chimera/m/0000003c/dl-TfliteDynamiteDynamite.integ_224210505100300.apk
  #05  pc 0x000000000014da61  /data/user_de/0/com.google.android.gms/app_chimera/m/0000003c/dl-TfliteDynamiteDynamite.integ_224210505100300.apk
  #06  pc 0x000000000014d8db  /data/user_de/0/com.google.android.gms/app_chimera/m/0000003c/dl-TfliteDynamiteDynamite.integ_224210505100300.apk
  #07  pc 0x0000000000036181  /data/user_de/0/com.google.android.gms/app_chimera/m/0000003c/dl-TfliteDynamiteDynamite.integ_224210505100300.apk
  #08  pc 0x000000000001b477  /data/app/~~RdHSBgE6dMlsRCnHjLYXZw==/com.example-Z6wg41mIr1QI0UuN7e6zkQ==/lib/arm/libtensorflowlite_jni_gms_client.so
  #09  pc 0x000000000001a609  /data/app/~~RdHSBgE6dMlsRCnHjLYXZw==/com.example-Z6wg41mIr1QI0UuN7e6zkQ==/lib/arm/libtensorflowlite_jni_gms_client.so
  #10  pc 0x000000000001a673  /data/app/~~RdHSBgE6dMlsRCnHjLYXZw==/com.example-Z6wg41mIr1QI0UuN7e6zkQ==/lib/arm/libtensorflowlite_jni_gms_client.so
  #11  pc 0x000000000001abc1  /data/app/~~RdHSBgE6dMlsRCnHjLYXZw==/com.example-Z6wg41mIr1QI0UuN7e6zkQ==/lib/arm/libtensorflowlite_jni_gms_client.so
  #12  pc 0x0000000000017d61  /data/app/~~RdHSBgE6dMlsRCnHjLYXZw==/com.example-Z6wg41mIr1QI0UuN7e6zkQ==/lib/arm/libtensorflowlite_jni_gms_client.so (Java_com_google_android_gms_tflite_NativeInterpreterWrapper_createInterpreter+452)
  #13  pc 0x000000000014e2b5  /data/app/~~RdHSBgE6dMlsRCnHjLYXZw==/com.example-Z6wg41mIr1QI0UuN7e6zkQ==/oat/arm/base.odex (art_jni_trampoline+100)
  #14  pc 0x00000000000edf38  /apex/com.android.art/lib/libart.so (nterp_helper+2728)
  #15  pc 0x00000000009af384  /data/app/~~RdHSBgE6dMlsRCnHjLYXZw==/com.example-Z6wg41mIr1QI0UuN7e6zkQ==/oat/arm/base.vdex (com.google.android.gms.tflite.NativeInterpreterWrapper.zzl+528)
  #16  pc 0x00000000000ee688  /apex/com.android.art/lib/libart.so (nterp_helper+4600)
  #17  pc 0x00000000009af08e  /data/app/~~RdHSBgE6dMlsRCnHjLYXZw==/com.example-Z6wg41mIr1QI0UuN7e6zkQ==/oat/arm/base.vdex (com.google.android.gms.tflite.NativeInterpreterWrapper.<init>+86)
  #18  pc 0x00000000000edf80  /apex/com.android.art/lib/libart.so (nterp_helper+2800)
  #19  pc 0x00000000009afbd2  /data/app/~~RdHSBgE6dMlsRCnHjLYXZw==/com.example-Z6wg41mIr1QI0UuN7e6zkQ==/oat/arm/base.vdex (com.google.android.gms.tflite.zzd.<init>+18)
  #20  pc 0x00000000000edf80  /apex/com.android.art/lib/libart.so (nterp_helper+2800)
  #21  pc 0x00000000009aef44  /data/app/~~RdHSBgE6dMlsRCnHjLYXZw==/com.example-Z6wg41mIr1QI0UuN7e6zkQ==/oat/arm/base.vdex (com.google.android.gms.tflite.InterpreterFactoryImpl.create+24)
  #22  pc 0x00000000000ee92c  /apex/com.android.art/lib/libart.so (nterp_helper+5276)
  #23  pc 0x000000000040f87e  /data/app/~~RdHSBgE6dMlsRCnHjLYXZw==/com.example-Z6wg41mIr1QI0UuN7e6zkQ==/oat/arm/base.vdex (androidx.appcompat.widget.a0.b+230)
  #24  pc 0x00000000000ed4c8  /apex/com.android.art/lib/libart.so (nterp_helper+56)
  #25  pc 0x00000000009b6ce0  /data/app/~~RdHSBgE6dMlsRCnHjLYXZw==/com.example-Z6wg41mIr1QI0UuN7e6zkQ==/oat/arm/base.vdex (com.google.firebase.samples.apps.mlkit.NsfwImageProcessor.createInterpreter+104)
  #26  pc 0x00000000000edf80  /apex/com.android.art/lib/libart.so (nterp_helper+2800)
  #27  pc 0x00000000009b6d0a  /data/app/~~RdHSBgE6dMlsRCnHjLYXZw==/com.example-Z6wg41mIr1QI0UuN7e6zkQ==/oat/arm/base.vdex (com.google.firebase.samples.apps.mlkit.NsfwImageProcessor.lambda$new$0)
  #28  pc 0x00000000000edf80  /apex/com.android.art/lib/libart.so (nterp_helper+2800)
  #29  pc 0x00000000009b6bbc  /data/app/~~RdHSBgE6dMlsRCnHjLYXZw==/com.example-Z6wg41mIr1QI0UuN7e6zkQ==/oat/arm/base.vdex (com.google.firebase.samples.apps.mlkit.NsfwImageProcessor.a)
  #30  pc 0x00000000000ed4c8  /apex/com.android.art/lib/libart.so (nterp_helper+56)
  #31  pc 0x00000000004123f0  /data/app/~~RdHSBgE6dMlsRCnHjLYXZw==/com.example-Z6wg41mIr1QI0UuN7e6zkQ==/oat/arm/base.vdex (com.applovin.exoplayer2.b.a0.run+44)
  #32  pc 0x0000000000c964b7  /data/misc/apexdata/com.android.art/dalvik-cache/arm/boot.oat (android.os.Handler.dispatchMessage+70)
  #33  pc 0x0000000000c99fb7  /data/misc/apexdata/com.android.art/dalvik-cache/arm/boot.oat (android.os.Looper.loopOnce+886)
  #34  pc 0x0000000000c99b5b  /data/misc/apexdata/com.android.art/dalvik-cache/arm/boot.oat (android.os.Looper.loop+1034)
  #35  pc 0x0000000000c989bd  /data/misc/apexdata/com.android.art/dalvik-cache/arm/boot.oat (android.os.HandlerThread.run+1748)
  #36  pc 0x00000000003bdbd5  /apex/com.android.art/lib/libart.so (art_quick_invoke_stub_internal+68)
  #37  pc 0x00000000003bd677  /apex/com.android.art/lib/libart.so (void art::quick_invoke_reg_setup<false>(art::ArtMethod*, unsigned int*, unsigned int, art::Thread*, art::JValue*, char const*) (.__uniq.192663596067446536341070919852553954320.llvm.16200230356545185596)+158)
  #38  pc 0x0000000000288f1d  /apex/com.android.art/lib/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+136)
  #39  pc 0x00000000003128a9  /apex/com.android.art/lib/libart.so (art::(anonymous namespace)::InvokeWithArgArray(art::ScopedObjectAccessAlreadyRunnable const&, art::ArtMethod*, art::(anonymous namespace)::ArgArray*, art::JValue*, char const*) (.__uniq.245181933781456475607640333933569312899.llvm.1951482298021755071)+40)
  #40  pc 0x00000000003127b5  /apex/com.android.art/lib/libart.so (art::JValue art::InvokeVirtualOrInterfaceWithJValues<art::ArtMethod*>(art::ScopedObjectAccessAlreadyRunnable const&, _jobject*, art::ArtMethod*, jvalue const*)+164)
  #41  pc 0x000000000033dc8b  /apex/com.android.art/lib/libart.so (art::JValue art::InvokeVirtualOrInterfaceWithJValues<_jmethodID*>(art::ScopedObjectAccessAlreadyRunnable const&, _jobject*, _jmethodID*, jvalue const*)+42)
  #42  pc 0x00000000002f0bad  /apex/com.android.art/lib/libart.so (art::Thread::CreateCallback(void*)+424)
  #43  pc 0x00000000000afc5f  /apex/com.android.runtime/lib/bionic/libc.so (__pthread_start(void*)+40)
  #44  pc 0x0000000000065ef1  /apex/com.android.runtime/lib/bionic/libc.so (__start_thread+30)

```

"
60312,"Stride-1 tf.nn.conv2d with XLA is 1.5x slower then without XLA, as far as stride-2 tf.nn.depthwise_conv2d","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.12.0, 2.13.0-dev20230412

### Custom Code

Yes

### OS Platform and Distribution

Google Colab

### Mobile device

_No response_

### Python version

Google Colab

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

_No response_

### Current Behaviour?

See example below to reproduce. Here is speed test results:

stride-1 conv 40.66
stride-1 conv_jit 64.11  // conv2d is slower with JIT but only if stride=1
stride-2 conv 40.18
stride-2 conv_jit 28.05  // when stride=2 it is FASTER with JIT

stride-1 dwconv 9.82
stride-1 dwconv_jit 5.72  // dwconv is faster with JIT but only if stride=1
stride-2 dwconv 2.59
stride-2 dwconv_jit 4.2  // when stride=2 it is SLOWER with JIT

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1zqqPVVKt4ILRA1rCoWjB1uOtB3D0hDc-?usp=sharing
```


### Relevant log output

_No response_</details>"
60309,Training stopping because of  BufferError: Existing exports of data: object cannot be re-sized or something wrong with tornado,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

NAME=""CentOS Linux"" VERSION=""7 (Core)""

### Mobile device

NAME=""CentOS Linux"" VERSION=""7 (Core)""

### Python version

3.9.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

 11.8.0 

### GPU model and memory

_No response_

### Current Behaviour?

The model training would just stop abruptly 

https://colab.research.google.com/drive/1WiqyF7dCdnNBIANEY80Pxw_mVz4fyV-S?usp=sharing

### Standalone code to reproduce the issue

```shell
Voxelmoprh library training
```


### Relevant log output

```shell
(tf) vr-lab@pop-os:~$ jupyter notebook

  _   _          _      _
 | | | |_ __  __| |__ _| |_ ___
 | |_| | '_ \/ _` / _` |  _/ -_)
  \___/| .__/\__,_\__,_|\__\___|
       |_|
                       
Read the migration plan to Notebook 7 to learn about the new features and the actions to take if you are using extensions.

https://jupyter-notebook.readthedocs.io/en/latest/migrate_to_notebook7.html

Please note that updating to Notebook 7 might break some of your extensions.

[I 00:02:49.290 NotebookApp] Serving notebooks from local directory: /home/vr-lab
[I 00:02:49.290 NotebookApp] Jupyter Notebook 6.5.4 is running at:
[I 00:02:49.290 NotebookApp] http://localhost:8888/?token=697572ae046e4388d22c7be946cefcb261064994d2f99466
[I 00:02:49.290 NotebookApp]  or http://127.0.0.1:8888/?token=697572ae046e4388d22c7be946cefcb261064994d2f99466
[I 00:02:49.290 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 00:02:49.334 NotebookApp] 
    
    To access the notebook, open this file in a browser:
        file:///home/vr-lab/.local/share/jupyter/runtime/nbserver-405435-open.html
    Or copy and paste one of these URLs:
        http://localhost:8888/?token=697572ae046e4388d22c7be946cefcb261064994d2f99466
     or http://127.0.0.1:8888/?token=697572ae046e4388d22c7be946cefcb261064994d2f99466
[I 00:03:15.170 NotebookApp] Kernel started: 4915aa8a-d4aa-4d50-885f-810d53eae7db, name: python3
[I 00:03:20.670 NotebookApp] Kernel restarted: 4915aa8a-d4aa-4d50-885f-810d53eae7db
[W 00:03:20.684 NotebookApp] Replacing stale connection: 4915aa8a-d4aa-4d50-885f-810d53eae7db:e6146c4b818f471185049a02ac632f6d
[W 00:03:21.180 NotebookApp] zmq message arrived on closed channel
[I 00:03:21.181 NotebookApp] Starting buffering for 4915aa8a-d4aa-4d50-885f-810d53eae7db:e6146c4b818f471185049a02ac632f6d
[I 00:03:21.183 NotebookApp] Restoring connection for 4915aa8a-d4aa-4d50-885f-810d53eae7db:e6146c4b818f471185049a02ac632f6d
[I 00:03:21.689 NotebookApp] Replaying 1 buffered messages
[E 00:03:21.761 NotebookApp] Uncaught exception, closing connection.
    Traceback (most recent call last):
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/iostream.py"", line 702, in _handle_events
        self._handle_write()
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/iostream.py"", line 976, in _handle_write
        self._write_buffer.advance(num_bytes)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/iostream.py"", line 182, in advance
        assert 0 < size <= self._size
    AssertionError
[W 00:03:21.764 NotebookApp] Write error on <socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6>: [Errno 9] Bad file descriptor
[W 00:03:21.766 NotebookApp] zmq message arrived on closed channel
[W 00:03:21.767 NotebookApp] zmq message arrived on closed channel
Exception in callback None()
handle: <Handle cancelled>
Traceback (most recent call last):
  File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/asyncio/events.py"", line 80, in _run
    self._context.run(self._callback, *self._args)
  File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/platform/asyncio.py"", line 206, in _handle_events
    handler_func(fileobj, events)
  File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/iostream.py"", line 702, in _handle_events
    self._handle_write()
  File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/iostream.py"", line 976, in _handle_write
    self._write_buffer.advance(num_bytes)
  File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/iostream.py"", line 182, in advance
    assert 0 < size <= self._size
AssertionError
[I 00:03:21.768 NotebookApp] Starting buffering for 4915aa8a-d4aa-4d50-885f-810d53eae7db:e6146c4b818f471185049a02ac632f6d
2023-04-11 00:03:22.084618: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-11 00:03:22.225493: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[I 00:03:22.803 NotebookApp] Restoring connection for 4915aa8a-d4aa-4d50-885f-810d53eae7db:e6146c4b818f471185049a02ac632f6d
[I 00:03:22.803 NotebookApp] Replaying 1 buffered messages
2023-04-11 00:03:22.815590: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/vr-lab/anaconda3/envs/tf/lib/:/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/nvidia/cudnn/lib
2023-04-11 00:03:22.815709: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/vr-lab/anaconda3/envs/tf/lib/:/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/nvidia/cudnn/lib
2023-04-11 00:03:22.815716: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-04-11 00:03:25.015062: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12776 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:af:00.0, compute capability: 8.6
2023-04-11 00:03:40.078576: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8600
[I 00:05:15.159 NotebookApp] Saving file at /Music/HybridMorph Please don't delete/HybridMorph_proof of concept.ipynb
Task exception was never retrieved
future: <Task finished name='Task-76' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/websocket.py:1090> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/websocket.py"", line 1092, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/asyncio/tasks.py"", line 256, in __step
    result = coro.send(None)
  File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/websocket.py"", line 1094, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
[E 01:03:52.904 NotebookApp] Exception in callback <bound method WebSocketMixin.send_ping of ZMQChannelsHandler(4915aa8a-d4aa-4d50-885f-810d53eae7db)>
    Traceback (most recent call last):
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/ioloop.py"", line 921, in _run
        val = self.callback()
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/notebook/base/zmqhandlers.py"", line 188, in send_ping
        self.ping(b'')
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/websocket.py"", line 445, in ping
        self.ws_connection.write_ping(data)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/websocket.py"", line 1101, in write_ping
        self._write_frame(True, 0x9, data)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/websocket.py"", line 1061, in _write_frame
        return self.stream.write(frame)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/iostream.py"", line 540, in write
        self._write_buffer.append(data)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/iostream.py"", line 157, in append
        b += data  # type: ignore
    BufferError: Existing exports of data: object cannot be re-sized
[E 01:13:22.812 NotebookApp] Uncaught exception in ZMQStream callback
    Traceback (most recent call last):
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py"", line 584, in _run_callback
        f = callback(*args, **kwargs)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py"", line 308, in stream_callback
        return callback(self, msg)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/notebook/services/kernels/handlers.py"", line 572, in _on_zmq_reply
        super()._on_zmq_reply(stream, msg)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/notebook/base/zmqhandlers.py"", line 256, in _on_zmq_reply
        self.write_message(msg, binary=isinstance(msg, bytes))
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/websocket.py"", line 339, in write_message
        return self.ws_connection.write_message(message, binary=binary)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/websocket.py"", line 1086, in write_message
        fut = self._write_frame(True, opcode, message, flags=flags)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/websocket.py"", line 1061, in _write_frame
        return self.stream.write(frame)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/iostream.py"", line 540, in write
        self._write_buffer.append(data)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/iostream.py"", line 157, in append
        b += data  # type: ignore
    BufferError: Existing exports of data: object cannot be re-sized
[E 01:13:22.815 NotebookApp] Uncaught exception in zmqstream callback
    Traceback (most recent call last):
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py"", line 634, in _handle_events
        self._handle_recv()
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py"", line 663, in _handle_recv
        self._run_callback(callback, msg)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py"", line 584, in _run_callback
        f = callback(*args, **kwargs)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py"", line 308, in stream_callback
        return callback(self, msg)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/notebook/services/kernels/handlers.py"", line 572, in _on_zmq_reply
        super()._on_zmq_reply(stream, msg)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/notebook/base/zmqhandlers.py"", line 256, in _on_zmq_reply
        self.write_message(msg, binary=isinstance(msg, bytes))
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/websocket.py"", line 339, in write_message
        return self.ws_connection.write_message(message, binary=binary)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/websocket.py"", line 1086, in write_message
        fut = self._write_frame(True, opcode, message, flags=flags)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/websocket.py"", line 1061, in _write_frame
        return self.stream.write(frame)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/iostream.py"", line 540, in write
        self._write_buffer.append(data)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/iostream.py"", line 157, in append
        b += data  # type: ignore
    BufferError: Existing exports of data: object cannot be re-sized
[E 01:13:22.815 NotebookApp] Exception in callback functools.partial(<function ZMQStream._update_handler.<locals>.<lambda> at 0x7f1de4ff4b80>)
    Traceback (most recent call last):
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/ioloop.py"", line 740, in _run_callback
        ret = callback()
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py"", line 718, in <lambda>
        self.io_loop.add_callback(lambda: self._handle_events(self.socket, 0))
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py"", line 634, in _handle_events
        self._handle_recv()
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py"", line 663, in _handle_recv
        self._run_callback(callback, msg)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py"", line 584, in _run_callback
        f = callback(*args, **kwargs)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py"", line 308, in stream_callback
        return callback(self, msg)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/notebook/services/kernels/handlers.py"", line 572, in _on_zmq_reply
        super()._on_zmq_reply(stream, msg)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/notebook/base/zmqhandlers.py"", line 256, in _on_zmq_reply
        self.write_message(msg, binary=isinstance(msg, bytes))
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/websocket.py"", line 339, in write_message
        return self.ws_connection.write_message(message, binary=binary)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/websocket.py"", line 1086, in write_message
        fut = self._write_frame(True, opcode, message, flags=flags)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/websocket.py"", line 1061, in _write_frame
        return self.stream.write(frame)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/iostream.py"", line 540, in write
        self._write_buffer.append(data)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/iostream.py"", line 157, in append
        b += data  # type: ignore
    BufferError: Existing exports of data: object cannot be re-sized
```
</details>"
60308,"use ESRGAN super resolution on iOS device by TensorFlowLiteSwift, the output is not identical as the output in python env.","I use the sample image for ESRGAN lr-1 
![lr1](https://user-images.githubusercontent.com/7868069/231623423-1431788d-dc0c-451e-9e52-d811f410276c.jpg)
and inference by TensorFlowLiteSwift, compared with python env in Colab , the input pixel data array was identical, but the output pixel data was different totally。partial output data：
iOS:    [[[ 59. 78. 70.] [ 77. 137. 42.] [ 76. 136. 79.]  ...
Colab:[[[ 59. 79. 71.] [ 72. 103.  72.] [ 86. 124.  65.] ..."
60302,Issue created for Rollback of PR #60180: Use a custom rule for API files generation,"Merged PR #60180 is rolled back in ec0ba2461436eb45881a6e76ca29ed49dcd21a66.
    Please follow up with the reviewer and close this issue once its resolved."
60301,[tosa] [mlir] tfl.IfOp isn’t supported in tf-mlir-translate.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.5 LTS  
- TensorFlow installed from (source or binary): source
- TensorFlow version (or github SHA if from source): 45f08a0fcc90145b9c2b7057310762d6b0ebae85

**Standalone code to reproduce the issue** 
```
tensorflow/bazel-bin/tensorflow/compiler/mlir/lite/flatbuffer_translate --tflite-flatbuffer-to-mlir onlyif/test_onlyif_1_f32/model.tflite --output-arrays=PartitionedCall:0 -o onlyif/test_onlyif_1_f32/test_tflite.preopt.mlir
```

test_tflite.preopt.mlir output: 
```
  func.func @main(%arg0: tensor<1xf32> {tf_saved_model.index_path = [""placeholder_0""]}) -> (tensor<*xf32> {tf_saved_model.index_path = [""output_0""]}) attributes {tf.entry_function = {inputs = ""serving_default_placeholder_0:0"", outputs = ""PartitionedCall:0""}, tf_saved_model.exported_names = [""serving_default""]} {
    %0 = ""tfl.pseudo_const""() {value = dense<5.000000e+00> : tensor<f32>} : () -> tensor<f32>
    %1 = tfl.less(%arg0, %0) : (tensor<1xf32>, tensor<f32>) -> tensor<1xi1>
    %2 = ""tfl.pseudo_const""() {value = dense<> : tensor<0xi32>} : () -> tensor<0xi32>
    %3 = ""tfl.reshape""(%1, %2) : (tensor<1xi1>, tensor<0xi32>) -> tensor<i1>
    // !!!!! THIS IS NOT tfl.If !!!!!!
    %4 = ""tf.If""(%3, %arg0, %0) {else_branch = @cond_false_70, is_stateless = false, then_branch = @cond_true_60} : (tensor<i1>, tensor<1xf32>, tensor<f32>) -> tensor<*xf32>
    return %4 : tensor<*xf32>
  }
```


**Any other info / logs**
- The goal is to generate tfl.IfOp in MLIR
- seems a lot of TF::IfOp is used everywhere and a TFL::IfOp is only instantiated during the flatbuffer export
- Some TODOs here: 
    - https://github.com/tensorflow/tensorflow/blob/ba8b52cad65f882bed982517a7b7cf60c598efab/tensorflow/compiler/mlir/lite/flatbuffer_export.cc#L572
    - https://github.com/tensorflow/tensorflow/blob/524101303ff158b939923b36805474f2e4118a49/tensorflow/compiler/mlir/lite/transforms/pin_ops_with_side_effects.cc#L48




Could anyone update the progress of the support of this operator? 
"
60300,[tosa] [mlir] tfl.relu_0_to_1 mlir dialect can’t be generated,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.5 LTS  
- TensorFlow installed from (source or binary): source
- TensorFlow version (or github SHA if from source): 45f08a0fcc90145b9c2b7057310762d6b0ebae85

**Standalone code to reproduce the issue** 
```
tensorflow/bazel-bin/tensorflow/compiler/mlir/lite/flatbuffer_translate --tflite-flatbuffer-to-mlir relu0To1/test_relu0To1_13x21x3_qu8/model.tflite --output-arrays=PartitionedCall:0 -o relu0To1/test_relu0To1_13x21x3_qu8/test_tflite.preopt.mlir
```

**Any other info / logs**

- The goal is to generate [tfl.relu_0_to_1](https://www.tensorflow.org/mlir/tfl_ops#tflrelu_0_to_1_mlirtflrelu0to1op) op in tfl dialect 
- The kernel was added here: https://github.com/tensorflow/tensorflow/commit/f11ab18461943d15fe562ce5bd07c3d1a50fd0de
- Myself added the MLIR dialect here: https://github.com/tensorflow/tensorflow/commit/485f680eccae0a5f3f7b83319b1ddc3426fc601a
- From the comments here https://github.com/tensorflow/tensorflow/pull/58078#pullrequestreview-1142611030 saying that Google will be finishing this. 

Could anyone update the progress of the support of this operator? 
"
60298,Issue created for Rollback of PR #60180: Use a custom rule for API files generation,"Merged PR #60180 is rolled back in 6767f70a301c62205e6fa1e590bfab3cfccd3978.
    Please follow up with the reviewer and close this issue once its resolved."
60297,<spam>,
60295,[MLIR-HLO] Missing legalization for mhlo.scatter to standard MLIR,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

main branch

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

See below.

### Standalone code to reproduce the issue

```shell
mhlo.scatter
```


### Relevant log output

_No response_</details>

### Problem Statement

Is there a pass (sequence) that can lower the `mhlo.scatter` operation to standard MLIR dialects, such as linalg, tensor, arith, and/or scf?

The goal is to ultimately lower to the LLVM dialect and perform codegen with LLVM. I wasn't able to find a pass that converts the `mhlo.scatter` op out of the MLIR-HLO dialect domain. Most other ops can be converter via passes like `--hlo-legalize-to-linalg`, `--mhlo-legalize-to-std`, or `--mhlo-legalize-control-flow`. 

(Duplicate of https://github.com/tensorflow/mlir-hlo/issues/64 since I'm not sure that repository is monitored for issues.)"
60294,tensorflow.python.framework.errors_impl.NotFoundError: Key conv1/kernel not found in checkpoint,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Windows x64

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


Hi everyone, I'm a novice programmer, I decided to create a neural network for facial recognition.
I have encountered such an error.

```shell
C:\Users\wefy2\AppData\Local\Programs\Python\Python310\python.exe C:/Users/wefy2/PycharmProjects/CNN-Facial-Recognition-master1/test_face_id.py
WARNING:tensorflow:From C:\Users\wefy2\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From C:\Users\wefy2\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\layers\normalization\batch_normalization.py:581: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2023-04-11 23:01:13.568545: W tensorflow/c/c_api.cc:300] Operation '{name:'conv_dw_12_bn/gamma/Assign' id:1939 op device:{requested: '', assigned: ''} def:{{{node conv_dw_12_bn/gamma/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](conv_dw_12_bn/gamma, conv_dw_12_bn/gamma/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2023-04-11 23:01:15.059810: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at save_restore_v2_ops.cc:228 : NOT_FOUND: Key conv1/kernel not found in checkpoint
WARNING:tensorflow:Restoring an object-based checkpoint using a name-based saver. This may be somewhat fragile, and will re-build the Saver. Instead, consider loading object-based checkpoints using tf.train.Checkpoint().
Traceback (most recent call last):
  File ""C:\Users\wefy2\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\client\session.py"", line 1378, in _do_call
    return fn(*args)
  File ""C:\Users\wefy2\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\client\session.py"", line 1361, in _run_fn
    return self._call_tf_sessionrun(options, feed_dict, fetch_list,
  File ""C:\Users\wefy2\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\client\session.py"", line 1454, in _call_tf_sessionrun
    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,
tensorflow.python.framework.errors_impl.NotFoundError: Key conv1/kernel not found in checkpoint
	 [[{{node save/RestoreV2}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\wefy2\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\training\saver.py"", line 1418, in restore
    sess.run(self.saver_def.restore_op_name,
  File ""C:\Users\wefy2\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\client\session.py"", line 968, in run
    result = self._run(None, fetches, feed_dict, options_ptr,
  File ""C:\Users\wefy2\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\client\session.py"", line 1191, in _run
    results = self._do_run(handle, final_targets, final_fetches,
  File ""C:\Users\wefy2\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\client\session.py"", line 1371, in _do_run
    return self._do_call(_run_fn, feeds, fetches, targets, options,
  File ""C:\Users\wefy2\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\client\session.py"", line 1397, in _do_call
    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter
tensorflow.python.framework.errors_impl.NotFoundError: Graph execution error:

Detected at node 'save/RestoreV2' defined at (most recent call last):
    File ""C:\Users\wefy2\PycharmProjects\CNN-Facial-Recognition-master1\test_face_id.py"", line 66, in <module>
      test_nn.load_network()
    File ""C:\Users\wefy2\PycharmProjects\CNN-Facial-Recognition-master1\test_face_id.py"", line 25, in load_network
      saver = tf.train.Saver()
Node: 'save/RestoreV2'
Key conv1/kernel not found in checkpoint
	 [[{{node save/RestoreV2}}]]

Original stack trace for 'save/RestoreV2':
  File ""C:\Users\wefy2\PycharmProjects\CNN-Facial-Recognition-master1\test_face_id.py"", line 66, in <module>
    test_nn.load_network()
  File ""C:\Users\wefy2\PycharmProjects\CNN-Facial-Recognition-master1\test_face_id.py"", line 25, in load_network
    saver = tf.train.Saver()
  File ""C:\Users\wefy2\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\training\saver.py"", line 934, in __init__
    self.build()
  File ""C:\Users\wefy2\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\training\saver.py"", line 946, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""C:\Users\wefy2\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\training\saver.py"", line 974, in _build
    self.saver_def = self._builder._build_internal(  # pylint: disable=protected-access
  File ""C:\Users\wefy2\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\training\saver.py"", line 543, in _build_internal
    restore_op = self._AddRestoreOps(filename_tensor, saveables,
  File ""C:\Users\wefy2\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\training\saver.py"", line 360, in _AddRestoreOps
    all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,
  File ""C:\Users\wefy2\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\training\saver.py"", line 611, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""C:\Users\wefy2\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\ops\gen_io_ops.py"", line 1604, in restore_v2
    _, _, _op, _outputs = _op_def_library._apply_op_helper(
  File ""C:\Users\wefy2\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 795, in _apply_op_helper
    op = g._create_op_internal(op_type_name, inputs, dtypes=None,
  File ""C:\Users\wefy2\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\framework\ops.py"", line 3814, in _create_op_internal
    ret = Operation(


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\wefy2\PycharmProjects\CNN-Facial-Recognition-master1\test_face_id.py"", line 66, in <module>
    test_nn.load_network()
  File ""C:\Users\wefy2\PycharmProjects\CNN-Facial-Recognition-master1\test_face_id.py"", line 27, in load_network
    saver.restore(self.sess, path)
  File ""C:\Users\wefy2\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\training\saver.py"", line 1444, in restore
    self._object_restore_saver = saver_from_object_based_checkpoint(
  File ""C:\Users\wefy2\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\training\saver.py"", line 1826, in saver_from_object_based_checkpoint
    raise errors.NotFoundError(
tensorflow.python.framework.errors_impl.NotFoundError: 

Existing variables not in the checkpoint: conv1/kernel, conv1_bn/beta, conv1_bn/gamma, conv1_bn/moving_mean, conv1_bn/moving_variance, conv_dw_1/depthwise_kernel, conv_dw_10/depthwise_kernel, conv_dw_10_bn/beta, conv_dw_10_bn/gamma, conv_dw_10_bn/moving_mean, conv_dw_10_bn/moving_variance, conv_dw_11/depthwise_kernel, conv_dw_11_bn/beta, conv_dw_11_bn/gamma, conv_dw_11_bn/moving_mean, conv_dw_11_bn/moving_variance, conv_dw_12/depthwise_kernel, conv_dw_12_bn/beta, conv_dw_12_bn/gamma, conv_dw_12_bn/moving_mean, conv_dw_12_bn/moving_variance, conv_dw_13/depthwise_kernel, conv_dw_13_bn/beta, conv_dw_13_bn/gamma, conv_dw_13_bn/moving_mean, conv_dw_13_bn/moving_variance, conv_dw_1_bn/beta, conv_dw_1_bn/gamma, conv_dw_1_bn/moving_mean, conv_dw_1_bn/moving_variance, conv_dw_2/depthwise_kernel, conv_dw_2_bn/beta, conv_dw_2_bn/gamma, conv_dw_2_bn/moving_mean, conv_dw_2_bn/moving_variance, conv_dw_3/depthwise_kernel, conv_dw_3_bn/beta, conv_dw_3_bn/gamma, conv_dw_3_bn/moving_mean, conv_dw_3_bn/moving_variance, conv_dw_4/depthwise_kernel, conv_dw_4_bn/beta, conv_dw_4_bn/gamma, conv_dw_4_bn/moving_mean, conv_dw_4_bn/moving_variance, conv_dw_5/depthwise_kernel, conv_dw_5_bn/beta, conv_dw_5_bn/gamma, conv_dw_5_bn/moving_mean, conv_dw_5_bn/moving_variance, conv_dw_6/depthwise_kernel, conv_dw_6_bn/beta, conv_dw_6_bn/gamma, conv_dw_6_bn/moving_mean, conv_dw_6_bn/moving_variance, conv_dw_7/depthwise_kernel, conv_dw_7_bn/beta, conv_dw_7_bn/gamma, conv_dw_7_bn/moving_mean, conv_dw_7_bn/moving_variance, conv_dw_8/depthwise_kernel, conv_dw_8_bn/beta, conv_dw_8_bn/gamma, conv_dw_8_bn/moving_mean, conv_dw_8_bn/moving_variance, conv_dw_9/depthwise_kernel, conv_dw_9_bn/beta, conv_dw_9_bn/gamma, conv_dw_9_bn/moving_mean, conv_dw_9_bn/moving_variance, conv_pw_1/kernel, conv_pw_10/kernel, conv_pw_10_bn/beta, conv_pw_10_bn/gamma, conv_pw_10_bn/moving_mean, conv_pw_10_bn/moving_variance, conv_pw_11/kernel, conv_pw_11_bn/beta, conv_pw_11_bn/gamma, conv_pw_11_bn/moving_mean, conv_pw_11_bn/moving_variance, conv_pw_12/kernel, conv_pw_12_bn/beta, conv_pw_12_bn/gamma, conv_pw_12_bn/moving_mean, conv_pw_12_bn/moving_variance, conv_pw_13/kernel, conv_pw_13_bn/beta, conv_pw_13_bn/gamma, conv_pw_13_bn/moving_mean, conv_pw_13_bn/moving_variance, conv_pw_1_bn/beta, conv_pw_1_bn/gamma, conv_pw_1_bn/moving_mean, conv_pw_1_bn/moving_variance, conv_pw_2/kernel, conv_pw_2_bn/beta, conv_pw_2_bn/gamma, conv_pw_2_bn/moving_mean, conv_pw_2_bn/moving_variance, conv_pw_3/kernel, conv_pw_3_bn/beta, conv_pw_3_bn/gamma, conv_pw_3_bn/moving_mean, conv_pw_3_bn/moving_variance, conv_pw_4/kernel, conv_pw_4_bn/beta, conv_pw_4_bn/gamma, conv_pw_4_bn/moving_mean, conv_pw_4_bn/moving_variance, conv_pw_5/kernel, conv_pw_5_bn/beta, conv_pw_5_bn/gamma, conv_pw_5_bn/moving_mean, conv_pw_5_bn/moving_variance, conv_pw_6/kernel, conv_pw_6_bn/beta, conv_pw_6_bn/gamma, conv_pw_6_bn/moving_mean, conv_pw_6_bn/moving_variance, conv_pw_7/kernel, conv_pw_7_bn/beta, conv_pw_7_bn/gamma, conv_pw_7_bn/moving_mean, conv_pw_7_bn/moving_variance, conv_pw_8/kernel, conv_pw_8_bn/beta, conv_pw_8_bn/gamma, conv_pw_8_bn/moving_mean, conv_pw_8_bn/moving_variance, conv_pw_9/kernel, conv_pw_9_bn/beta, conv_pw_9_bn/gamma, conv_pw_9_bn/moving_mean, conv_pw_9_bn/moving_variance

Variables names when this checkpoint was written which don't exist now: Adam/m/conv2d/bias, Adam/m/conv2d/kernel, Adam/m/conv2d_1/bias, Adam/m/conv2d_1/kernel, Adam/m/conv2d_2/bias, Adam/m/conv2d_2/kernel, Adam/m/conv2d_3/bias, Adam/m/conv2d_3/kernel, Adam/m/dense/bias, Adam/m/dense/kernel, Adam/m/dense_1/bias, Adam/m/dense_1/kernel, Adam/v/conv2d/bias, Adam/v/conv2d/kernel, Adam/v/conv2d_1/bias, Adam/v/conv2d_1/kernel, Adam/v/conv2d_2/bias, Adam/v/conv2d_2/kernel, Adam/v/conv2d_3/bias, Adam/v/conv2d_3/kernel, Adam/v/dense/bias, Adam/v/dense/kernel, Adam/v/dense_1/bias, Adam/v/dense_1/kernel, conv2d/bias, conv2d/kernel, conv2d_1/bias, conv2d_1/kernel, conv2d_2/bias, conv2d_2/kernel, conv2d_3/bias, conv2d_3/kernel, count, iteration, learning_rate, total

(4 variable name(s) did match)

Could not find some variables in the checkpoint (see names above). Saver was attempting to load an object-based checkpoint (saved using tf.train.Checkpoint or tf.keras.Model.save_weights) using variable names. If the checkpoint was written with eager execution enabled, it's possible that variable names have changed (for example missing a '_1' suffix). It's also possible that there are new variables which did not exist when the checkpoint was written. You can construct a Saver(var_list=...) with only the variables which previously existed, and if variable names have changed you may need to make this a dictionary with the old names as keys. If you're using an Estimator, you'll need to return a tf.train.Saver inside a tf.train.Scaffold from your model_fn.

Process finished with exit code 1
```

I suspect that this problem is that the error occurs when loading the weights of the test_nn model from the saved checkpoints. It reports that some variables in the saved checkpoint do not correspond to variables in the model, because there are no corresponding stored values for them. Or because of a mismatch of tensorflow library versions

mit_data_processing.py
```shell
import cv2
import glob
import numpy as np

save_to = 'C:\\Users\\wefy2\\PycharmProjects\\CNN-Facial-Recognition-master1\\data'
all_faces = [img for img in glob.glob('C:\\Users\\wefy2\\PycharmProjects\\CNN-Facial-Recognition-master1\\data\\gt_db\\s*\\*.jpg')]

faces_x = []
faces_y = []

faceCascade = cv2.CascadeClassifier('data\\haarcascade_frontalface.xml')

for i, face in enumerate(all_faces):
    image = cv2.imread(face)
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    faces = faceCascade.detectMultiScale(gray, 1.3, 5)

    if len(faces) == 1:
        x, y, w, h = faces[0]
        cropped_img = image[y:y + h, x:x + w]

        faces_x.append(cv2.resize(cropped_img, (128, 128)))
        faces_y.append(int(face.split('\\')[-2][1:]))

    print('Finished: ', i, ' Out of: ', len(all_faces))


faces_x, faces_y = np.array(faces_x), np.array(faces_y)

np.save(save_to + 'x_train', faces_x)
np.save(save_to + 'y_train', faces_y)
```
train_face_id.py
```
import numpy as np
import tensorflow as tf
import tensorflow_addons as tfa

# loading data
faces_x = np.load('datax_train.npy')
faces_y = np.load('datay_train.npy')
faces_x = tf.expand_dims(faces_x, axis=0)
faces_y = tf.expand_dims(faces_y, axis=0)
train_dataset = tf.data.Dataset.from_tensor_slices((faces_x, faces_y))
print('Faces were loaded successfully.')

print (tf.__version__)
# Construct the fully connected hashing layers
model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same',
                           activation='relu', input_shape=(128, 128, 3)),
    tf.keras.layers.MaxPooling2D(pool_size=2),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same',
                           activation='relu', input_shape=(128, 128, 3)),
    tf.keras.layers.MaxPooling2D(pool_size=2),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Conv2D(filters=32, kernel_size=2,
                           padding='same', activation='relu'),
    tf.keras.layers.MaxPooling2D(pool_size=2),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Conv2D(filters=32, kernel_size=2,
                           padding='same', activation='relu'),
    tf.keras.layers.MaxPooling2D(pool_size=2),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(128, activation='sigmoid')
])


# Compile the model
model.compile(
    optimizer=tf.keras.optimizers.Adam(0.001),
    loss=tfa.losses.TripletSemiHardLoss(margin=3.0))
print(model.summary())
print('Model Compiled Successfully.')


# Train the model
print('Training has started.')
history = model.fit(train_dataset, epochs=10, verbose=1)


# Save the model
model.save('models/face_id_model')
print('Training is finished.')
```

test_face_id.py
```
import numpy as np
import cv2
import tensorflow.compat.v1 as tf

tf.disable_v2_behavior()


class FaceID:
    def __init__(self):
        model = tf.keras.Sequential()
        net = tf.keras.applications.MobileNet(input_shape=(128, 128, 3), weights='imagenet', include_top=False)
        model.add(net)
        model.add(tf.keras.layers.GlobalAveragePooling2D())
        self.features_extractor = model

        self.x_holder = tf.placeholder(shape=[None, 1024], dtype=tf.float32)
        fc_1 = tf.layers.Dense(units=512, activation=tf.nn.relu)(self.x_holder)
        fc_2 = tf.layers.Dense(units=128, activation=tf.nn.sigmoid)(fc_1)

        self.face_id = fc_2

        self.sess = None

    def load_network(self, path='models\\face_id_model\\variables\\variables'):
        saver = tf.train.Saver()
        self.sess = tf.Session()
        saver.restore(self.sess, path)

    def get_id(self, imgs):
        imgs = imgs.reshape((-1, 128, 128, 3))
        features = self.features_extractor.predict(imgs)
        embeds = self.sess.run([self.face_id], feed_dict={self.x_holder: features})

        return embeds[0]


class FaceExtractor:
    def __init__(self, cascade_path='data\\haarcascade_frontalface.xml'):
        self.faceCascade = cv2.CascadeClassifier(cascade_path)

    def extract_single_face_from_path(self, img_path):
        image = cv2.imread(img_path)
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        faces = self.faceCascade.detectMultiScale(gray, 1.3, 5)

        if len(faces) == 1:
            x, y, w, h = faces[0]
            cropped_img = image[y:y + h, x:x + w]
            return cv2.resize(cropped_img, (128, 128))
        else:
            faces = self.faceCascade.detectMultiScale(gray, 1.3, 10)
            if len(faces) == 1:
                x, y, w, h = faces[0]
                cropped_img = image[y:y + h, x:x + w]
                return cv2.resize(cropped_img, (128, 128))

        return None

    def faces_from_image(self, image):
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        return self.faceCascade.detectMultiScale(gray, 1.3, 5)


test_nn = FaceID()
face_ex = FaceExtractor()
test_nn.load_network()
ref_face = face_ex.extract_single_face_from_path(""ref.jpg"")
ref_face_hash = test_nn.get_id(ref_face)[0]
cap = cv2.VideoCapture(0)

while True:
    ret, frame = cap.read()
    faces = face_ex.faces_from_image(frame)

    for face in faces:
        x, y, w, h = face
        cropped_face = cv2.resize(frame[y:y + h, x:x + w], (128, 128))
        cropped_hash = test_nn.get_id(cropped_face)[0]

        cv2.rectangle(frame, (x, y), (x + w, y + h), 1, 3)

        distance_1 = np.sum(np.power(ref_face_hash - cropped_hash, 2))

        if distance_1 <= 3:
            cv2.putText(frame, 'ref ', (x, y + h + 30), cv2.FONT_HERSHEY_SIMPLEX, 1, 1, 2, cv2.LINE_AA)
        else:
            cv2.putText(frame, 'Nan ', (x, y + h + 30), cv2.FONT_HERSHEY_SIMPLEX, 1, 1, 2, cv2.LINE_AA)

    cv2.imshow('My FaceID', frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        ret, frame = cap.read()
        break
```
```


### Relevant log output

_No response_</details>"
60291,tf.random.truncated_normal (inside graph) crashed TPUv4 pod," ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.10 TPU Pod

### Current Behaviour?


I found ```tf.random.truncated_normal``` will crash the TPUv4 Pod during training (graph computation).

```tf.random.truncated_normal``` is fine when not in a graph (e.g. variable initialization).

```tf.random.normal``` and ```tf.random.uniform``` are fine during training.


### Relevant log output
"
60289,Respect Keras layer names for output operations in Concrete Function,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When converting a Keras model to concrete function, you can preserve the input name by creating a named TensorSpec, but the outputs are always created for you by just slapping `tf.identity` on top of whatever you had there, even if it was a custom named `tf.identity` operation. Since many converters rely on concrete functions to make their own representation (TFLite, ONNX, CoreML, etc), this behavior messes up the output operation names, often making them inconsistent with each other. 
There's currently no workaround for that. You *can* access previous graph nodes by calling a layer  named like {model_name}/{output_layer_name} when doing inference on frozen graph itself, but it won't help you in any way to convert the model.

So I'd be happy to see one of those things as a solution to that:
1) Add an option to explicitly specify the TensorSpec for outputs, just the way we do it for inputs. This would be the most obvious and convenient way of doing it from a user standpoint
2) Don't add new identity operations on top of existing ones. More of a kludge, but would get the job done
3) Add an option to rename operations in concrete function post factum.
4) Add an option to cut off the operations in concrete function past a certain node. 
5) Add an option to convert a graph into a concrete function. Since you can directly modify graphs, this could work as well


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

# Build simple model
inputs = tf.keras.Input((224, 224, 3), name='custom_input_layer')
x = tf.keras.layers.Flatten()(inputs)
x = tf.keras.layers.Dense(512, activation='relu')(x)
x = tf.keras.layers.Dense(256, activation='relu')(x)
x = tf.keras.layers.Dense(128, activation='relu')(x)
x = tf.keras.layers.Dense(1, activation='sigmoid', name='custom_output_layer')(x)

model = tf.keras.Model(inputs=inputs, outputs=x, name='my_custom_model_name')
model.summary()

input_tensors = [tf.TensorSpec(shape=inp.shape, dtype=tf.float32, name=inp.name) for inp in model.inputs]
concrete_function = tf.function(lambda x: model(x)).get_concrete_function(x=input_tensors)

print(concrete_function.inputs)  # we can see 'custom_input_layer:0' is there. So is the 'true' output 'my_custom_model_name/custom_output_layer/BiasAdd/ReadVariableOp/resource:0'
print(concrete_function.outputs)  # pesky Identity node gets inserted
```


### Relevant log output

```shell
Model: ""my_custom_model_name""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 custom_input_layer (InputLa  [(None, 224, 224, 3)]    0         
 yer)                                                            
                                                                 
 flatten (Flatten)           (None, 150528)            0         
                                                                 
 dense (Dense)               (None, 512)               77070848  
                                                                 
 dense_1 (Dense)             (None, 256)               131328    
                                                                 
 dense_2 (Dense)             (None, 128)               32896     
                                                                 
 custom_output_layer (Dense)  (None, 1)                129       
                                                                 
=================================================================
Total params: 77,235,201
Trainable params: 77,235,201
Non-trainable params: 0
_________________________________________________________________
[<tf.Tensor 'custom_input_layer:0' shape=(None, 224, 224, 3) dtype=float32>, <tf.Tensor 'my_custom_model_name/dense/MatMul/ReadVariableOp/resource:0' shape=() dtype=resource>, <tf.Tensor 'my_custom_model_name/dense/BiasAdd/ReadVariableOp/resource:0' shape=() dtype=resource>, <tf.Tensor 'my_custom_model_name/dense_1/MatMul/ReadVariableOp/resource:0' shape=() dtype=resource>, <tf.Tensor 'my_custom_model_name/dense_1/BiasAdd/ReadVariableOp/resource:0' shape=() dtype=resource>, <tf.Tensor 'my_custom_model_name/dense_2/MatMul/ReadVariableOp/resource:0' shape=() dtype=resource>, <tf.Tensor 'my_custom_model_name/dense_2/BiasAdd/ReadVariableOp/resource:0' shape=() dtype=resource>, <tf.Tensor 'my_custom_model_name/custom_output_layer/MatMul/ReadVariableOp/resource:0' shape=() dtype=resource>, <tf.Tensor 'my_custom_model_name/custom_output_layer/BiasAdd/ReadVariableOp/resource:0' shape=() dtype=resource>]
[<tf.Tensor 'Identity:0' shape=(None, 1) dtype=float32>]
```
</details>"
60288,"""Failed to connect to remote host: Connection refused"" on Colab TPU","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.12.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I got an error ""Failed to connect to remote host: Connection refused"" when I use TPU to train my model on Colab, but it works if I use GPU to train.
```


### Standalone code to reproduce the issue

```shell
My notebook is
https://colab.research.google.com/drive/1YUMZQe-z5cc9PAgGEch3xuEuloclNSRC?usp=sharing
```


### Relevant log output

```shell
Epoch 1/50
   1/1240 [..............................] - ETA: 10:24:26
---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
<ipython-input-14-d0d9f97c5872> in <cell line: 2>()
      1 # Train the model
----> 2 history = model.fit(train_generator, validation_data=val_generator, epochs=50, verbose=1, shuffle=True)

1 frames
/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/ops.py in _numpy(self)
   1126       return self._numpy_internal()
   1127     except core._NotOkStatusException as e:  # pylint: disable=protected-access
-> 1128       raise core._status_to_exception(e) from None  # pylint: disable=protected-access
   1129 
   1130   @property

InternalError: 9 root error(s) found.
  (0) INTERNAL: {{function_node __inference_train_function_107490}} failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:50612: Failed to connect to remote host: Connection refused
Additional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:
:UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:50612: Failed to connect to remote host: Connection refused {created_time:""2023-04-11T06:57:27.796149613+00:00"", grpc_status:14}
	 [[{{node MultiDeviceIteratorGetNextFromShard}}]]
Executing non-communication op <MultiDeviceIteratorGetNextFromShard> originally returned UnavailableError, and was replaced by InternalError to avoid invoking TF network error handling logic.
	 [[RemoteCall]]
	 [[IteratorGetNextAsOptional]]
	 [[strided_slice_91/_338]]
  (1) INTERNAL: {{function_node __inference_train_function_107490}} failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:50612: Failed to connect to remote host: Connection refused
Additional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:
:UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:50612: Failed to connect to remote host: Connection refused {created_time:""2023-04-11T06:57:27.796149613+00:00"", grpc_status:14}
	 [[{{node MultiDeviceIteratorGetNextFromShard}}]]
Executing non-communication op <MultiDeviceIteratorGetNextFromShard> originally returned UnavailableError, and was replaced by InternalError to avoid invoking TF network error handling logic.
	 [[RemoteCall]]
	 [[IteratorGetNextAsOptional]]
	 [[strided_slice_72/_312]]
  (2) INTERNAL: {{function_node __inference_train_function_107490}} failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:50612: Failed to connect to remote host: Connection refused
Additional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:
:UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:50612: Failed to connect to remote host: Connection refused {created_time:""2023-04-11T06:57:27.796149613+00:00"", grpc_status:14}
	 [[{{node MultiDeviceIteratorGetNextFromShard}}]]
Executing non-communication op <MultiDeviceIteratorGetNextFromShard> originally returned UnavailableError, and was replaced by InternalError to avoid invoking TF network error handling logic.
	 [[RemoteCall]]
	 [[IteratorGetNextAsOptional]]
	 [[strided_slice_37/_266]]
  (3) INTERNAL: {{function_node __inference_train_function_107490}} failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:50612: Failed to connect to remote host: Connection refused
Additional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:
:UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:50612: Failed to connect to remote host: Connection refused {created_time:""2023-04-11T06:57:27.796149613+00:00"", grpc_status:14}
	 [[{{node MultiDeviceIteratorGetNextFromShard}}]]
Ex ... [truncated]
```
</details>"
60287,tfp.stats.histogram weights argument bug when using multiple axes,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Tensorflow throws a ValueError when using weights and multiple axes at the same time, saying that the input and weights arguments should have the same shape, even though they already have.

Hint: Does not happen when using only one axis.
```


### Standalone code to reproduce the issue

```shell
import tensorflow_probability as tfp
import tensorflow as tf
x = tf.constant([[1, 2, 3], [1, 2, 3.]])
w = tf.constant([[1, 1, 1], [1, 1, 1.]])
e = tf.constant([0, 2.5, 10])
axis = (0, 1)
tfp.stats.histogram(x=x, edges=e, axis=axis, weights=w)
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/opt/.pycharm_helpers/pydev/pydevconsole.py"", line 364, in runcode
    coro = func()
  File ""<input>"", line 1, in <module>
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/stats/quantiles.py"", line 393, in histogram
    raise ValueError('Number of dimensions of `x` and `weights` must '
ValueError: Number of dimensions of `x` and `weights` must coincide. Found: x has 2, weights has 1
```
</details>"
60286,bitcast op testcase bug,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

master

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
gpu xla testcase：
 XLA_TEST_F(Resnet50FusionTest, module_0615_entry_bitcast) {                         
   const char* testcase = R""(                                                        
   ¦ HloModule m, is_scheduled=true                                                  
   ¦ ENTRY out_of_fusion {                                                           
   ¦ ¦ p0 = f32[1]{0} parameter(0)                                                   
   ¦ ¦ ROOT res = f32[] bitcast(f32[1]{0} p0)                                        
   ¦ }                                                                               
   )"";                                                                               
   auto module = ParseAndReturnVerifiedModule(testcase).value();                     
   EXPECT_TRUE(RunAndCompareNoHloPasses(std::move(module), ErrorSpec(1e-5)));    
 }     
will cause following bug：
2023-04-07 07:42:59.490547: I tensorflow/compiler/xla/service/platform_util.cc:72] platform Host present but no XLA compiler available: could not find registered compiler for platform Host -- was support for that platform linked in?
2023-04-07 07:42:59.659576: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1162] failed to enqueue async memcpy from device to host: CUDA_ERROR_INVALID_VALUE: invalid argument; host dst: 0x7fff996f4360; GPU src: 0x7f4e8da00000; size: 4=0x4
2023-04-07 07:42:59.659624: I tensorflow/compiler/xla/stream_executor/stream.cc:2535] INTERNAL: Unknown error
2023-04-07 07:42:59.659679: I tensorflow/compiler/xla/stream_executor/stream.cc:2539] [stream=0x5562acc10570,impl=0x5562acbb8160] INTERNAL: stream did not block host until done; was already in an error state
2023-04-07 07:42:59.659694: I tensorflow/compiler/xla/stream_executor/stream.cc:2535] INTERNAL: Unknown error
2023-04-07 07:42:59.659703: I tensorflow/compiler/xla/stream_executor/stream.cc:2539] [stream=0x5562acc10570,impl=0x5562acbb8160] INTERNAL: stream did not block host until done; was already in an error state
2023-04-07 07:42:59.659711: W tensorflow/compiler/xla/stream_executor/stream.cc:277] Error blocking host until done in stream destructor: INTERNAL: stream did not block host until done; was already in an error state
2023-04-07 07:42:59.659745: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:755] failed to free device memory at 0x7f4e8da00000; result: CUDA_ERROR_INVALID_VALUE: invalid argument
tensorflow/compiler/xla/service/gpu/tests/gpu_kernel_tiling_test.cc:58: Failure
Value of: RunAndCompareNoHloPasses(std::move(module), ErrorSpec(1e-5))
  Actual: false (INTERNAL: stream did not block host until done; was already in an error state)
Expected: true
```


### Standalone code to reproduce the issue

```shell
see current behaviour
```


### Relevant log output

```shell
2023-04-07 07:42:59.490547: I tensorflow/compiler/xla/service/platform_util.cc:72] platform Host present but no XLA compiler available: could not find registered compiler for platform Host -- was support for that platform linked in?
2023-04-07 07:42:59.659576: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1162] failed to enqueue async memcpy from device to host: CUDA_ERROR_INVALID_VALUE: invalid argument; host dst: 0x7fff996f4360; GPU src: 0x7f4e8da00000; size: 4=0x4
2023-04-07 07:42:59.659624: I tensorflow/compiler/xla/stream_executor/stream.cc:2535] INTERNAL: Unknown error
2023-04-07 07:42:59.659679: I tensorflow/compiler/xla/stream_executor/stream.cc:2539] [stream=0x5562acc10570,impl=0x5562acbb8160] INTERNAL: stream did not block host until done; was already in an error state
2023-04-07 07:42:59.659694: I tensorflow/compiler/xla/stream_executor/stream.cc:2535] INTERNAL: Unknown error
2023-04-07 07:42:59.659703: I tensorflow/compiler/xla/stream_executor/stream.cc:2539] [stream=0x5562acc10570,impl=0x5562acbb8160] INTERNAL: stream did not block host until done; was already in an error state
2023-04-07 07:42:59.659711: W tensorflow/compiler/xla/stream_executor/stream.cc:277] Error blocking host until done in stream destructor: INTERNAL: stream did not block host until done; was already in an error state
2023-04-07 07:42:59.659745: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:755] failed to free device memory at 0x7f4e8da00000; result: CUDA_ERROR_INVALID_VALUE: invalid argument
tensorflow/compiler/xla/service/gpu/tests/gpu_kernel_tiling_test.cc:58: Failure
Value of: RunAndCompareNoHloPasses(std::move(module), ErrorSpec(1e-5))
  Actual: false (INTERNAL: stream did not block host until done; was already in an error state)
Expected: true
```
</details>"
60285,linked page is not valid!,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
TRFL in
https://www.tensorflow.org/resources/libraries-extensions
is not valid:)
```


### Standalone code to reproduce the issue

```shell
TRFL in
https://www.tensorflow.org/resources/libraries-extensions
is not valid:)
```


### Relevant log output

_No response_</details>"
60284,Anyway to set the attr of operation after added to the gragh in C API?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

tf2.11

### Custom Code

No

### OS Platform and Distribution

Windows and Linux

### Current Behaviour?

Hello, I'm a developer of [Tensorflow.NET](https://github.com/SciSharp/TensorFlow.NET), which is a tensorflow binding for dotnet. When I implemented some feature, I could not find a C API to add attributes to an operation that has already been created and added to the graph. However, there is indeed such C API for python. It's located [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/python_api.h#L33).

Among exported C APIs of `libtensorflow`, only the following API was found:

```c
TF_CAPI_EXPORT extern void TF_SetAttrValueProto(TF_OperationDescription* desc,
                                                const char* attr_name,
                                                const void* proto,
                                                size_t proto_len,
                                                TF_Status* status);
```



However, `TF_OperationDescription` is released after adding the operation to graph by calling `TF_FinishOperation`. Therefore I can't use the API to add attributes.

Is there any other way for us to add attributes for operations which have been added to graph?


### Standalone code to reproduce the issue

```shell
The code is wrote in csharp. I'll give a minimal example if needed.
```


### Relevant log output

_No response_</details>"
60283,windows link error,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

No

### OS Platform and Distribution

Windows 10 professional

### Mobile device

_No response_

### Python version

3.8.5 64bit

### Bazel version

5.3.0

### GCC/Compiler version

visual studio 2019

### CUDA/cuDNN version

_No response_

### GPU model and memory

cpu model

### Current Behaviour?

```shell
A bug happened!when I compile TensorFlow 2.12.0  to produce Windows C++ API files. I have tried severl times. The same erro happended. I hope you can help me,thank you
```


### Standalone code to reproduce the issue

```shell
I used Windows 10 operating system, Visual Studio 2019 C++, Bazel 5.3.0, and TensorFlow 2.12.0 to compile Windows C++ API files. My Bazel build command is: ""G:\Bazel5\Bazel --output_user_root=g:\tfoutPut4 build --config=opt --define=no_tensorflow_py_deps=true --jobs=8 --subcommands //tensorflow:tensorflow_cc.dll //tensorflow:install_headers > log.txt 2> err.txt"". The compilation was successful, but the linking failed with the error message: ""ERROR: G:/tensorflow/tensorflow/BUILD:1219:21: Linking tensorflow/tensorflow_cc.dll failed: (Exit 1120): link.exe failed: error executing command"".
```


### Relevant log output

```shell
ERROR: G:/tensorflow/tensorflow/BUILD:1219:21: Linking tensorflow/tensorflow_cc.dll failed: (Exit 1120): link.exe failed: error executing command 
  cd /d G:/tfoutput4/ic7qrvhc/execroot/org_tensorflow
  SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\ATLMFC\lib\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\lib\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.19041.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.19041.0\um\x64
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\\Extensions\Microsoft\IntelliCode\CLI;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\\x64;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\HTML Help Workshop;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\Tools\devinit;C:\Program Files (x86)\Windows Kits\10\bin\10.0.19041.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\\MSBuild\Current\Bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\Tools\;;C:\Windows\system32;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\Llvm\x64\bin;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/ProgramData/Anaconda3/python.exe
    SET PYTHON_LIB_PATH=C:/ProgramData/Anaconda3/Lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\yisir\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TMP=C:\Users\yisir\AppData\Local\Temp
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\bin\HostX64\x64\link.exe @bazel-out/x64_windows-opt/bin/tensorflow/tensorflow_cc.dll-2.params
# Configuration: 86b0c99414660dd887a605e34d6ccaa371dc6e6122db6d3c5d156e9013ebeb17
# Execution platform: @local_execution_config_platform//:platform
INFO: Elapsed time: 8232.786s, Critical Path: 458.51s
INFO: 15506 processes: 3617 internal, 11889 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully

The following is an excerpt from another log file stdout-23850 int my dirctory G:/tfoutput4/ic7qrvhc/execroot/org_tensorflow/bazel-out/_tmp/actions/ :

tensorflow_cc.dll.if.exp: error LNK2001: unresolved external symbol ""public: int __cdecl tensorflow::BytesList::value_size(void)const "" (?value_size@BytesList@tensorflow@@QEBAHXZ)
tensorflow_cc.dll.if.exp: error LNK2001: unresolved external symbol ""public: int __cdecl tensorflow::FloatList::value_size(void)const "" (?value_size@FloatList@tensorflow@@QEBAHXZ)
tensorflow_cc.dll.if.exp: error LNK2001: unresolved external symbol ""public: int __cdecl tensorflow::Int64List::value_size(void)const "" (?value_size@Int64List@tensorflow@@QEBAHXZ)
ibtensorflow_framework.so.2.12.0.if.lib(libtensorflow_framework.so.2.12.0): error LNK2005: TF_NewBufferFromString already defined in tf_buffer.lib(tf_buffer.obj)
libtensorflow_framework.so.2.12.0.if.lib(libtensorflow_framework.so.2.12.0): error LNK2005: TF_NewTensor already defined in tf_tensor.lib(tf_tensor.obj)
libtensorflow_framework.so.2.12.0.if.lib(libtensorflow_framework.so.2.12.0): error LNK2005: TF_TensorData already defined in tf_tensor.lib(tf_tensor.obj)
Creating library bazel-out/x64_windows-opt/bin/tensorflow/tensorflow_cc.dll.if.lib and object bazel-out/x64_windows-opt/bin/tensorflow/tensorflow_cc.dll.if.exp
LINK: warning LNK4217: symbol ""TFE_NewContextOptions"" (defined in ""c_api.lo.lib(c_api.obj)"") already imported in ""pass_utils.lib(utils.obj)"" (function ""?InitializeTFRuntime@quant@mlir@@YAPEAUTFE_Context@@XZ"" (?InitializeTFRuntime@quant@mlir@@YAPEAUTFE_Context@@XZ))
LINK: warning LNK4286: symbol ""TFE_NewContextOptions"" (defined in ""c_api.lo.lib(c_api.obj)"") already imported in ""tf_dialect_passes.lo.lib(constant_fold.obj)""
```
</details>"
60282,Enabling XNNPACK with Raspberry Pi Zero/W,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

No

### OS Platform and Distribution

Linux Raspberrypi OS 32-bit (Debian bullseye)

### Mobile device

Raspberry Pi Zero W

### Python version

3.9.2

### Bazel version

cmake 3.18.4

### GCC/Compiler version

GNU c++ (Raspbian 10.2.1-6+rpi1) 10.2.1 20210110

### CUDA/cuDNN version

NA

### GPU model and memory

NA

### Current Behaviour?
The tf-lite build instructions for Raspberry Pi Zero/Zero W state that the following should be part
 of the CFLAGS/CXXFLAGS:
`-march=armv6 -mfpu=vfp -mfloat-abi=hard -funsafe-math-optimizations`

As per the [README for xxnpack](https://github.com/google/XNNPACK#raspberry-pi), XNNPACK supports running on the armv6 with vpf that's the Raspberry Pi Zero W. However all build instructions for Raspberry Pi Zero request explicitly disabling xnnpack. Given the support for rpi0 in xnnpack documentation, I tried to build tf-lite with xnnpack enabled.

When the xnnpack sub-build is enabled, the following conflicting CFLAGS are added to the compiler invocation during the xnnpack sub-build:
``` -marm  -march=armv8.2-a+dotprod -mfpu=neon-fp-armv8```

Please document/extend the cmake and build instructions to allow tf-lite to build correctly with xnnpack enabled for the Raspberry Pi Zero/Zero W.

### Standalone code to reproduce the issue

```shell
cmake \
   -DCMAKE_C_FLAGS='-march=armv6 -mfpu=vfp -mfloat-abi=hard -funsafe-math-optimizations -I/usr/include/python3.9 -I/usr/lib/python3/dist-packages/pybind11/include -I/usr/lib/python3/dist-packages/numpy/core/include' \
   -DCMAKE_CXX_FLAGS='-march=armv6 -mfpu=vfp -mfloat-abi=hard -funsafe-math-optimizations -I/usr/include/python3.9 -I/usr/lib/python3/dist-packages/pybind11/include -I/usr/lib/python3/dist-packages/numpy/core/include'  \
   -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \
   -DCMAKE_SYSTEM_NAME=Linux \
   -DCMAKE_SYSTEM_PROCESSOR=armv6  \
   -DTFLITE_ENABLE_XNNPACK=ON \
   /home/samveen/tensorflow/build/../tensorflow/lite
...
cmake --build . --verbose -t _pywrap_tensorflow_interpreter_wrapper
```


### Relevant log output

```shell
/usr/bin/gmake  -f _deps/xnnpack-build/CMakeFiles/microkernels-prod.dir/build.make _deps/xnnpack-build/CMakeFiles/microkernels-prod.dir/build
gmake[3]: Entering directory '/home/samveen/tensorflow/build/gen/tflite_pip/python3/cmake_build'
[ 47%] Building C object _deps/xnnpack-build/CMakeFiles/microkernels-prod.dir/src/amalgam/neondot.c.o
cd /home/samveen/tensorflow/build/gen/tflite_pip/python3/cmake_build/_deps/xnnpack-build && /usr/bin/cc -DEIGEN_MPL2_ONLY -DFXDIV_USE_INLINE_ASSEMBLY=0 -DNOMINMAX=1 -DPTHREADPOOL_NO_DEPRECATED_API=1 -DXNN_ENABLE_ARM_BF16=1 -DXNN_ENABLE_ARM_DOTPROD=1 -DXNN_ENABLE_ARM_FP16_SCALAR=1 -DXNN_ENABLE_ARM_FP16_VECTOR=1 -DXNN_ENABLE_ASSEMBLY=1 -DXNN_ENABLE_DWCONV_MULTIPASS=0 -DXNN_ENABLE_GEMM_M_SPECIALIZATION=1 -DXNN_ENABLE_JIT=0 -DXNN_ENABLE_MEMOPT=1 -DXNN_ENABLE_RISCV_VECTOR=1 -DXNN_ENABLE_SPARSE=1 -I/home/samveen/tensorflow/build/gen/tflite_pip/python3/cmake_build/xnnpack/src -I/home/samveen/tensorflow/build/gen/tflite_pip/python3/cmake_build/pthreadpool-source/include -I/home/samveen/tensorflow/build/gen/tflite_pip/python3/cmake_build/FXdiv-source/include -I/home/samveen/tensorflow/build/gen/tflite_pip/python3/cmake_build/FP16-source/include -march=armv6 -mfpu=vfp -mfloat-abi=hard -funsafe-math-optimizations -I/usr/include/python3.9 -I/usr/lib/python3/dist-packages/pybind11/include -I/usr/lib/python3/dist-packages/numpy/core/include -O3 -DNDEBUG -fPIC -Wno-psabi -O2 -pthread -std=c99  -marm  -march=armv8.2-a+dotprod -mfpu=neon-fp-armv8  -o CMakeFiles/microkernels-prod.dir/src/amalgam/neondot.c.o -c /home/samveen/tensorflow/build/gen/tflite_pip/python3/cmake_build/xnnpack/src/amalgam/neondot.c
/tmp/ccotLSur.s: Assembler messages:
/tmp/ccotLSur.s:63: Error: selected processor does not support `vsdot.s8 q8,q12,d7[0]' in ARM mode
/tmp/ccotLSur.s:65: Error: selected processor does not support `vsdot.s8 q9,q10,d7[0]' in ARM mode
/tmp/ccotLSur.s:68: Error: selected processor does not support `vsdot.s8 q11,q10,d7[0]' in ARM mode
/tmp/ccotLSur.s:71: Error: selected processor does not support `vsdot.s8 q14,q10,d7[0]' in ARM mode
/tmp/ccotLSur.s:74: Error: selected processor does not support `vsdot.s8 q8,q10,d7[1]' in ARM mode
/tmp/ccotLSur.s:77: Error: selected processor does not support `vsdot.s8 q9,q10,d7[1]' in ARM mode
/tmp/ccotLSur.s:80: Error: selected processor does not support `vsdot.s8 q11,q10,d7[1]' in ARM mode
...
```
</details>"
60281,M1 mac LLVM bug for tensor multiplication ,"### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.8.0, 2.11

### Custom Code

No

### OS Platform and Distribution

MaxOS 12.3.1 

### Python version

3.9, 3.10

### GPU model and memory

M1 Max 26 cores GPU 32 GB ram

### Current Behaviour?

```shell
For some reason, it does not allow me to matrix multiply those 2 tensors (on M1, Colab works just fine)
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
class EncoderBlock(tf.keras.layers.Layer):
    def __init__(self,**kwargs):
        super().__init__(**kwargs)
        self.n_heads = 2
    def build(self, input_shape):
        self.WQ = self.add_weight(""WQ_encoder"", (self.n_heads, input_shape[-1], 10))
        # however, an easy fix is the using the following:
        # self.WQ = self.add_weight(""WQ_encoder"", (1, self.n_heads, input_shape[-1], 10))
        # but clearly there is a problem, as the broadcast should take place nicely like on Colab
    def call(self, inputs, mask, *args):
        inputs = inputs[:,None,...]
        _ = tf.matmul(inputs,self.WQ)
        # ^^^^^^^^^^^^^^^^^^^^^^^^^^^

EncoderBlock()(np.random.randn(3,10,13), np.ones((3,10)))
```


### Relevant log output

```shell
-:5:10: error: incompatible dimensions
-:5:10: error: invalid shape
LLVM ERROR: Failed to infer result type(s).
```"
60280,Cannot use extended batch shape with channel_first Conv2D,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 1804

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Given input tensor for shape, say [batch_dim_1 = 4, batch_dim_2 = 5, channels = 6, Height = 7, Width = 8] to a channels_first (keras) Conv2D of stride 2, the ""_get_sequence"" function at Line 1288 in nn_ops.py sets strides to [1, 2, 2, 1], which is wrong. Strides should be [1, 1, 2, 2] for the input tensor will be reshaped to [batch_dim_1 * batch_dim_2, channels, Height, Width] later in squeeze_batch_dim in nn_ops.py.
```


### Standalone code to reproduce the issue

```shell
As described above.
```


### Relevant log output

_No response_</details>"
60279,AttributeError: 'Delegate' object has no attribute '_library',"**System information**
- ubuntu 18.04.6 lts
- tflite-runtime installed from Source, Using 'sudo apt-get install python3-tflite-runtime' [version: 2.5.0]
- Python Verison: 3.6
- edgetpu-compiler 16 amd64

**Provide the text output from tflite_convert**
syncx@syncx:~/cubesatnet_improvised/tflite/python/examples/classification$ python3 classify_image.py \
>   --model models/mobilenet_v2_1.0_224_inat_bird_quant_edgetpu.tflite \
>   --labels models/inat_bird_labels.txt \
>   --input images/parrot.jpg
Traceback (most recent call last):
  File ""classify_image.py"", line 122, in <module>
    main()
  File ""classify_image.py"", line 99, in main
    interpreter = make_interpreter(args.model)
  File ""classify_image.py"", line 73, in make_interpreter
    {'device': device[0]} if device else {})
  File ""/usr/lib/python3/dist-packages/tflite_runtime/interpreter.py"", line 160, in load_delegate
    delegate = Delegate(library, options)
  File ""/usr/lib/python3/dist-packages/tflite_runtime/interpreter.py"", line 89, in __init__
    self._library = ctypes.pydll.LoadLibrary(library)
  File ""/usr/lib/python3.6/ctypes/__init__.py"", line 426, in LoadLibrary
    return self._dlltype(name)
  File ""/usr/lib/python3.6/ctypes/__init__.py"", line 348, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: libedgetpu.so.1: cannot open shared object file: No such file or directory
Exception ignored in: <bound method Delegate.__del__ of <tflite_runtime.interpreter.Delegate object at 0x7fa771c0ae48>>
Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/tflite_runtime/interpreter.py"", line 124, in __del__
    if self._library is not None:
AttributeError: 'Delegate' object has no attribute '_library'

**Standalone code to reproduce the issue** 
python3 classify_image.py \
  --model models/mobilenet_v2_1.0_224_inat_bird_quant_edgetpu.tflite \
  --labels models/inat_bird_labels.txt \
  --input images/parrot.jpg

I am trying coral dev board micro. So, I tried ""https://github.com/google-coral/tflite/tree/master/python/examples/classification"" in this example. It produces the above error. I followed the instructions properly and tried everything I found on Google, but I am unsure what is occurring with this issue.

"
60278,A check fail can be triggered in MatrixSolve,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.13.0-dev20230406

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.8

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The following code can trigger a crash in `tf.raw_ops.MatrixSolve` due to check-fail(multiply overflow) in the latest version of TensorFlow.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
with tf.device(""CPU""):
    adjoint = False
    matrix = tf.random.uniform([0, 3, 15, 7847250026211090813, 11], dtype=tf.float64, minval=-1024, maxval=1024)
    rhs = tf.random.uniform([11], dtype=tf.float64, minval=-1024, maxval=1024)
    res = tf.raw_ops.MatrixSolve(
        adjoint=adjoint,
        matrix=matrix,
        rhs=rhs,
    )
```


### Relevant log output

```shell
2023-04-09 14:04:15.420939: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-04-09 14:04:15.468322: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-09 14:04:16.265727: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-04-09 14:04:17.807591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1638] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14561 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:2f:00.0, compute capability: 7.0
2023-04-09 14:04:17.844083: F tensorflow/core/framework/tensor_shape.cc:201] Non-OK-status: InitDims(dim_sizes) status: INVALID_ARGUMENT: Encountered overflow when multiplying 7847250026211090813 with 11, result: -1
Aborted (core dumped)
```
</details>"
60277,A check fail can be triggered in TridiagonalMatMul,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.13.0-dev20230406

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.8

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The following code can trigger a crash in `tf.raw_ops.TridiagonalMatMul` due to check-fail(multiply overflow) in the latest version of TensorFlow.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
with tf.device(""CPU""):
    superdiag = tf.random.uniform([5, 13, 5, 0, 6291163412844499803, 7], dtype=tf.float64, minval=-1024, maxval=1024)
    maindiag = tf.random.uniform([], dtype=tf.float64, minval=-1024, maxval=1024)
    subdiag = tf.random.uniform([9, 3, 3], dtype=tf.float64, minval=-1024, maxval=1024)
    rhs = tf.random.uniform([0, 3, 2, 8, 12, 8], dtype=tf.float64, minval=-1024, maxval=1024)
    res = tf.raw_ops.TridiagonalMatMul(
        superdiag=superdiag,
        maindiag=maindiag,
        subdiag=subdiag,
        rhs=rhs,
    )
```


### Relevant log output

```shell
2023-04-09 14:03:29.795879: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-04-09 14:03:29.841868: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-09 14:03:30.668006: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-04-09 14:03:32.219934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1638] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14561 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:2f:00.0, compute capability: 7.0
2023-04-09 14:03:32.257496: F tensorflow/core/framework/tensor_shape.cc:201] Non-OK-status: InitDims(dim_sizes) status: INVALID_ARGUMENT: Encountered overflow when multiplying 6291163412844499803 with 7, result: -1
Aborted (core dumped)
```
</details>"
60276,A check fail can be triggered in Gather,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.13.0-dev20230406

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.8

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The following code can trigger a crash in `tf.raw_ops.Gather` due to check-fail in the latest version of TensorFlow.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
with tf.device(""GPU:0""):
    validate_indices = False
    params = tf.saturate_cast(tf.random.uniform([13, 15, 7, 13, 14], minval=-1024, maxval=1024, dtype=tf.int64), dtype=tf.half)
    indices = tf.saturate_cast(tf.random.uniform([11, 12, 6, 15, 11, 3], minval=-1024, maxval=1024, dtype=tf.int64), dtype=tf.int64)
    res = tf.raw_ops.Gather(
        validate_indices=validate_indices,
        params=params,
        indices=indices,
    )
```


### Relevant log output

```shell
2023-04-09 13:58:03.392638: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-04-09 13:58:03.438587: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-09 13:58:04.254390: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-04-09 13:58:05.786420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1638] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14561 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:2f:00.0, compute capability: 7.0
2023-04-09 13:58:06.047489: F ./tensorflow/core/util/gpu_launch_config.h:160] Check failed: work_element_count >= 0 (0 vs. -549025096)
Aborted (core dumped)
```
</details>"
60275,A check fail can be triggered in MatrixLogarithm,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.13.0-dev20230406

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.8

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The following code can trigger a crash in `tf.raw_ops.MatrixLogarithm` due to check-fail in the latest version of TensorFlow.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
with tf.device(""CPU""):
    input = tf.complex(tf.random.uniform([0, 9, 13, 15, 13, 8440370290997831992], dtype=tf.float32, minval=-1024, maxval=1024),tf.random.uniform([0, 9, 13, 15, 13, 8440370290997831992], dtype=tf.float32, minval=-1024, maxval=1024))
    res = tf.raw_ops.MatrixLogarithm(
        input=input,
    )
```


### Relevant log output

```shell
2023-04-09 13:53:45.787590: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-04-09 13:53:45.833409: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-09 13:53:46.651440: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-04-09 13:53:48.212756: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1638] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14561 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:2f:00.0, compute capability: 7.0
2023-04-09 13:53:48.249022: F tensorflow/core/framework/tensor_shape.cc:201] Non-OK-status: InitDims(dim_sizes) status: INVALID_ARGUMENT: Encountered overflow when multiplying 13 with 8440370290997831992, result: -1
Aborted (core dumped)
```
</details>"
60274,A check fail can be triggered in Cholesky,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.13.0-dev20230406

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.8

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The following code can trigger a crash in `tf.raw_ops.Cholesky` due to check-fail in the latest version of TensorFlow.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
with tf.device(""CPU""):
    input = tf.complex(tf.random.uniform([6, 1, 0, 11, 5241981715460094077, 8], dtype=tf.float64, minval=-1024, maxval=1024),tf.random.uniform([6, 1, 0, 11, 5241981715460094077, 8], dtype=tf.float64, minval=-1024, maxval=1024))
    res = tf.raw_ops.Cholesky(
        input=input,
    )
```


### Relevant log output

```shell
2023-04-09 13:50:29.887381: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-04-09 13:50:29.932917: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-09 13:50:30.753764: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-04-09 13:50:32.285931: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1638] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14561 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:2f:00.0, compute capability: 7.0
2023-04-09 13:50:32.321965: F tensorflow/core/framework/tensor_shape.cc:201] Non-OK-status: InitDims(dim_sizes) status: INVALID_ARGUMENT: Encountered overflow when multiplying 5241981715460094077 with 8, result: -1
Aborted (core dumped)
```
</details>"
60273,A check fail can be triggered in UnicodeDecodeWithOffsets,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.13.0-dev20230406

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.8

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The following code can trigger a crash in `tf.raw_ops.UnicodeDecodeWithOffsets` due to check-fail in the latest version of TensorFlow.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
with tf.device(""GPU:0""):
    input_encoding = ""l9""
    errors = ""strict""
    replacement_char = 965
    replace_control_characters = True
    Tsplits = tf.int32
    input = tf.strings.unicode_encode(np.random.randint(0, 128, size=[2, 15, 16, 1, 6, 18], dtype=np.int32), output_encoding='UTF-8')
    res = tf.raw_ops.UnicodeDecodeWithOffsets(
        input_encoding=input_encoding,
        errors=errors,
        replacement_char=replacement_char,
        replace_control_characters=replace_control_characters,
        Tsplits=Tsplits,
        input=input,
    )
```


### Relevant log output

```shell
2023-04-09 13:47:10.094296: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-04-09 13:47:10.141240: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-09 13:47:10.947591: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-04-09 13:47:12.490727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1638] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14561 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:2f:00.0, compute capability: 7.0
2023-04-09 13:47:12.779782: F tensorflow/core/framework/tensor.cc:770] Check failed: dtype() == expected_dtype (9 vs. 3) int32 expected, got int64
Aborted (core dumped)
```
</details>"
60271,M2 GPU utilization decays from 50% to 10% in non batched inference for huggingface distilbert-base-cased,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tensorflow-macos 2.9, tensorflow-metal 0.5.0

### Custom Code

Yes

### OS Platform and Distribution

MacOS 13.3

### Mobile device

_No response_

### Python version

Python 3.10.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

N/A

### GPU model and memory

Apple M2 Max (unified memory)

### Current Behaviour?

```shell
MacBook Pro M2 Max 96gb 
macOS 13.3 
tensorflow-macos 2.9.0 
tensorflow-metal 0.5.0

GPU utilization should hold steady when running inference for HuggingFace TFDistilBertForSequenceClassification from pretrained 'distilbert-base-cased'.
Instead utilization dropped steadily from 50% to 10% (and sometimes, below 2%). 
It becomes excruciatingly slow.
```


### Standalone code to reproduce the issue

```shell
# if needed, from HuggingFace
!pip install transformers
!pip install datasets

from transformers import AutoTokenizer, TFDistilBertForSequenceClassification
from datasets import load_dataset
from tqdm import tqdm

imdb = load_dataset('imdb')
sentences = imdb['train']['text'][:500]

tokenizer = AutoTokenizer.from_pretrained(""distilbert-base-cased"")
model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-cased')

for i, sentence in tqdm(enumerate(sentences)):
  inputs = tokenizer(sentence, truncation=True, return_tensors='tf')
  output = model(inputs).logits
  pred = np.argmax(output.numpy(), axis=1)

  if i % 100 == 0:
    print(f""len(input_ids): {inputs['input_ids'].shape[-1]}"")
```


### Relevant log output

```shell
Output from print:

Metal device set to: Apple M2 Max

systemMemory: 96.00 GB
maxCacheSize: 36.00 GB

3it [00:00, 10.87it/s]
len(input_ids): 391
101it [00:13,  6.38it/s]
len(input_ids): 215
201it [00:34,  4.78it/s]
len(input_ids): 237
301it [00:55,  4.26it/s]
len(input_ids): 256
401it [01:54,  1.12it/s]
len(input_ids): 55
500it [03:40,  2.27it/s]
```
</details>"
60270,"Doc Feature Request: For tf 2, explain how to distribute a pretrained model across multiple nodes.","When I read the original TF paper, I was told that

> Dataflow simplifies distributed execution, because it makes communication between subcomputations explicit.
> It enables the same TensorFlow program to be deployed to a cluster of GPUs for training, a cluster of TPUs for serving, and a cellphone for mobile inference. 
> Each operation resides on a particular device, such as a CPU or GPU in a particular task. A device is responsible for executing a kernel for each operation assigned to it.
> (...)
> The TensorFlow runtime places operations on devices, subject to implicit or explicit constraints in the graph.
> The placement algorithm computes a feasible set of devices for each operation, calculates the sets of operations that must be colocated, and selects a satisfying device for each colocation group.

I work with small models, so I have never need more than one device. But now large models are all the rage, and a lot of competition is about just getting the weights of a pretrained model and run them. So I want cluster all the GPUs of a room to just run a LLM, either ad-hoc, keras or HF. 

I would expect a tutorial for this to be available somewhere. It is not. I guess one still needs to configure a cluster description and, if such option is available still in 2.x, launch an script in each machine to join them into a cluster. And then somehow launch a master script in some machine that loads the model, deploys its graph across the cluster, and runs it. Without needing any mirroring or any parameter collection, so that the current objects in distribute.strategy seem very big for such task. Perhaps eventually the example could include some ClusterResolver for common cloud services, but ideally it should be for in-home heterogeneous architecture.

Of course it could be still sensible to do such tutorial with the distribute.strategy tools, if it were considered as a first introduction to distribution of tasks, and that the next natural thing is to learn also to do a loss calculation, a backpropagation, and a training step. 
"
60269,Tensorflow not recognizing GPU in Spyder,"I have followed this YouTube tutorial since I want to use GPU for tensorflow model training: https://www.youtube.com/watch?v=yLVFwAaFACk&ab_channel=DigitalSreeni. However, I was still not able to detect GPU of my computer in Spyder:

```
from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())

[name: ""/device:CPU:0""
device_type: ""CPU""
memory_limit: 268435456
locality {
}
incarnation: 17504734730771000338
xla_global_id: -1
]
```
My system is Windows 11. My tensorflow version is 2.12.0. My GPUs are NVIDIA GeForce RTX 2060 and Intel® UHD Graphics 630. My NVIDIA CUDA version is 12.1. I used cudnn-windows-x86_64-8.8.1.3_cuda12-archive. My Anaconda is fully updated. My Spyder version is 5.4.2.
Any help will be much appreciated!"
60268,Hey why you all are stopping the tensorflow GPU support for windows. It sucks you know. I am here sitting infront of my computer since 12 hours and trying to analyze whats going wrong and why my GPU is not showing on the tensorflow. And after some reserch I found that you are stopping the suppport . WHY?????????????????????????,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
60267,TF Hangs when calculating validation sample weights,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

unknown 2.9.2

### Custom Code

Yes

### OS Platform and Distribution

MacOS 12.4

### Mobile device

NA

### Python version

Python 3.10.5

### Bazel version

NA

### GCC/Compiler version

NA

### CUDA/cuDNN version

NA

### GPU model and memory

NA

### Current Behaviour?

While using `validation_data` in `model.fit`, TF hangs when it's finished the first epoch (but before updating the progress bar with the validation loss). 

Specifically, this happens when a 3-tuple is passed to `validation_data`, where the third element is an array specifying the sample weights.

This happens even if the sample weights are all `1.0`.

### Standalone code to reproduce the issue

```shell
import numpy as np
import keras
import tensorflow as tf
import sklearn.model_selection

X = np.random.randint(low=300, high=900, size=(215699, 30, 30))
y = np.random.randint(low=0, high=51, size=(215699,))
X_trn, X_val, y_trn, y_val = sklearn.model_selection.train_test_split(X, y)

m = tf.keras.Sequential(
    [
        keras.layers.Input(shape=X.shape[1:]),
        keras.layers.Flatten(),
        keras.layers.Dense(units=64, activation=""relu"",),
        keras.layers.Dense(units=64, activation=""relu"",),
        keras.layers.Dense(len(np.unique(y))),
    ]
)
m.compile(
    optimizer=keras.optimizers.Adam(1e-5),
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
)

sample_weights_val = np.ones(y_val.shape)

history = m.fit(
    X_trn,
    y_trn,
    validation_data=(X_val, y_val, sample_weights_val),
    batch_size=128,
    epochs=5,
)
```


### Relevant log output

```shell
Epoch 1/5

2023-04-08 13:18:01.491389: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
2023-04-08 13:18:01.621051: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.

1264/1264 [==============================] - ETA: 0s - loss: 124.2101
```

At this point Tensorflow stops and I've got to interrupt it in order to get control back.
</details>"
60266,tf.GradientTape.gradients() does not support graph control flow operations like tf.cond or tf.while at this time. Use tf.gradients() instead,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
60264,Cannot compile TensorFlow 2.10 with GPu support on Windows,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.10

### Custom Code

No

### OS Platform and Distribution

Windows Server 2016

### Mobile device

_No response_

### Python version

3.8.8

### Bazel version

5.1.0

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.8/8.6

### GPU model and memory

Nvidia GeForce GTX 1080 Ti

### Current Behaviour?

```shell
I am running into issues with bazel when trying to compile TensorFlow 2.10 with CUDA 11.8. I know that this configuration has not been tested, but I need to compile TF with GPU support on native Windows since we are integrating it into our software in my company. Hence the 2.10 version which is the last one with native Windows GPU support. I build against CUDA 11.8 since it is one that support the latest Nvidia GPUs. I used Bazel 5.1.0 since it is the one that apparently should be used with TensorFlow 2.10.

Basically, everything went fine with the configuration until I try to lauch bazel with the following command:

bazel --output_user_root=D:/TensorFlow/bazel-out build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package

Before that everything went well with the configure.py script, which automatically found Python and CUDA 11.8.

The first error message I get is this one :
ERROR: D:/tensorflow/tensorflow/WORKSPACE:15:14: fetching llvm_configure rule //external:llvm-project
File ""D:/tensorflow/bazel-out/zfk46uyn/external/llvm-raw/utils/bazel/configure.bzl"", line 74, column 25, in _llvm_configure_impl
                _overlay_directories(repository_ctx)
        File ""D:/tensorflow/bazel-out/zfk46uyn/external/llvm-raw/utils/bazel/configure.bzl"", line 63, column 13, in _overlay_directories
                fail((""Failed to execute overlay script: '{cmd}'\n"" +
Error in fail: Failed to execute overlay script: 'C:/Python38/python.exe D:/tensorflow/bazel-out/zfk46uyn/external/llvm-raw/utils/bazel/overlay_directories.py --src D:/tensorflow/bazel-out/zfk46uyn/external/llvm-raw --overlay D:/tensorflow/bazel-out/zfk46uyn/external/llvm-raw/utils/bazel/llvm-project-overlay --target .'

Everything subsequent error seem to depend on that first error. I added the full log to this message.

Many thanks for your kind help.
```


### Standalone code to reproduce the issue

```shell
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
git checkout r2.10
python ./configure.py
bazel --output_user_root=D:/TensorFlow/bazel-out build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=120
INFO: Reading rc options for 'build' from d:\tensorflow\tensorflow\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/Python38/python.exe
INFO: Reading rc options for 'build' from d:\tensorflow\tensorflow\.bazelrc:
  'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false
INFO: Reading rc options for 'build' from d:\tensorflow\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Python38/python.exe --action_env PYTHON_LIB_PATH=C:/Python38/lib/site-packages --python_path=C:/Python38/python.exe --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.8 --action_env CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.8 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.5,5.2,6.1,7.0,7.5,8.6,8.9 --config=cuda --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions
INFO: Reading rc options for 'build' from d:\tensorflow\tensorflow\.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file d:\tensorflow\tensorflow\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file d:\tensorflow\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda in file d:\tensorflow\tensorflow\.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:opt in file d:\tensorflow\tensorflow\.tf_configure.bazelrc: --copt=/arch:AVX --host_copt=/arch:AVX
INFO: Found applicable config definition build:cuda in file d:\tensorflow\tensorflow\.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:windows in file d:\tensorflow\tensorflow\.bazelrc: --copt=/W0 --host_copt=/W0 --copt=/Zc:__cplusplus --host_copt=/Zc:__cplusplus --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --features=compiler_param_file --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --cxxopt=/std:c++17 --host_cxxopt=/std:c++17 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file d:\tensorflow\tensorflow\.bazelrc: --define framework_shared_object=false --experimental_link_static_libraries_once=false
INFO: Repository llvm-project instantiated at:
  D:/tensorflow/tensorflow/WORKSPACE:15:14: in <toplevel>
  D:/tensorflow/tensorflow/tensorflow/workspace2.bzl:889:21: in workspace
  D:/tensorflow/tensorflow/tensorflow/workspace2.bzl:527:15: in _tf_repositories
  D:/tensorflow/tensorflow/third_party/llvm/setup.bzl:22:19: in llvm_setup
Repository rule llvm_configure defined at:
  D:/tensorflow/bazel-out/zfk46uyn/external/llvm-raw/utils/bazel/configure.bzl:84:33: in <toplevel>
ERROR: An error occurred during the fetch of repository 'llvm-project':
   Traceback (most recent call last):
        File ""D:/tensorflow/bazel-out/zfk46uyn/external/llvm-raw/utils/bazel/configure.bzl"", line 74, column 25, in _llvm_configure_impl
                _overlay_directories(repository_ctx)
        File ""D:/tensorflow/bazel-out/zfk46uyn/external/llvm-raw/utils/bazel/configure.bzl"", line 63, column 13, in _overlay_directories
                fail((""Failed to execute overlay script: '{cmd}'\n"" +
Error in fail: Failed to execute overlay script: 'C:/Python38/python.exe D:/tensorflow/bazel-out/zfk46uyn/external/llvm-raw/utils/bazel/overlay_directories.py --src D:/tensorflow/bazel-out/zfk46uyn/external/llvm-raw --overlay D:/tensorflow/bazel-out/zfk46uyn/external/llvm-raw/utils/bazel/llvm-project-overlay --target .'
Exited with code 1
stdout:

stderr:
Traceback (most recent call last):
  File ""D:/tensorflow/bazel-out/zfk46uyn/external/llvm-raw/utils/bazel/overlay_directories.py"", line 92, in <module>
    main(parse_arguments())
  File ""D:/tensorflow/bazel-out/zfk46uyn/external/llvm-raw/utils/bazel/overlay_directories.py"", line 80, in main
    _symlink_abs(os.path.join(args.overlay, relpath),
  File ""D:/tensorflow/bazel-out/zfk46uyn/external/llvm-raw/utils/bazel/overlay_directories.py"", line 64, in _symlink_abs
    os.symlink(os.path.abspath(from_path), os.path.abspath(to_path))
OSError: [WinError 1314] Le client ne dispose pas dÆun privilÞge nÚcessaire: 'D:\\tensorflow\\bazel-out\\zfk46uyn\\external\\llvm-raw\\utils\\bazel\\llvm-project-overlay\\.bazelignore' -> 'D:\\tensorflow\\bazel-out\\zfk46uyn\\external\\llvm-project\\.bazelignore'

ERROR: D:/tensorflow/tensorflow/WORKSPACE:15:14: fetching llvm_configure rule //external:llvm-project: Traceback (most recent call last):
        File ""D:/tensorflow/bazel-out/zfk46uyn/external/llvm-raw/utils/bazel/configure.bzl"", line 74, column 25, in _llvm_configure_impl
                _overlay_directories(repository_ctx)
        File ""D:/tensorflow/bazel-out/zfk46uyn/external/llvm-raw/utils/bazel/configure.bzl"", line 63, column 13, in _overlay_directories
                fail((""Failed to execute overlay script: '{cmd}'\n"" +
Error in fail: Failed to execute overlay script: 'C:/Python38/python.exe D:/tensorflow/bazel-out/zfk46uyn/external/llvm-raw/utils/bazel/overlay_directories.py --src D:/tensorflow/bazel-out/zfk46uyn/external/llvm-raw --overlay D:/tensorflow/bazel-out/zfk46uyn/external/llvm-raw/utils/bazel/llvm-project-overlay --target .'
Exited with code 1
stdout:

stderr:
Traceback (most recent call last):
  File ""D:/tensorflow/bazel-out/zfk46uyn/external/llvm-raw/utils/bazel/overlay_directories.py"", line 92, in <module>
    main(parse_arguments())
  File ""D:/tensorflow/bazel-out/zfk46uyn/external/llvm-raw/utils/bazel/overlay_directories.py"", line 80, in main
    _symlink_abs(os.path.join(args.overlay, relpath),
  File ""D:/tensorflow/bazel-out/zfk46uyn/external/llvm-raw/utils/bazel/overlay_directories.py"", line 64, in _symlink_abs
    os.symlink(os.path.abspath(from_path), os.path.abspath(to_path))
OSError: [WinError 1314] Le client ne dispose pas dÆun privilÞge nÚcessaire: 'D:\\tensorflow\\bazel-out\\zfk46uyn\\external\\llvm-raw\\utils\\bazel\\llvm-project-overlay\\.bazelignore' -> 'D:\\tensorflow\\bazel-out\\zfk46uyn\\external\\llvm-project\\.bazelignore'

INFO: Repository flatbuffers instantiated at:
  D:/tensorflow/tensorflow/WORKSPACE:15:14: in <toplevel>
  D:/tensorflow/tensorflow/tensorflow/workspace2.bzl:882:28: in workspace
  D:/tensorflow/tensorflow/tensorflow/workspace2.bzl:61:16: in _initialize_third_party
  D:/tensorflow/tensorflow/third_party/flatbuffers/workspace.bzl:6:20: in repo
  D:/tensorflow/tensorflow/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  D:/tensorflow/tensorflow/third_party/repo.bzl:89:35: in <toplevel>
ERROR: D:/tensorflow/tensorflow/tensorflow/tools/pip_package/BUILD:278:10: //tensorflow/tools/pip_package:build_pip_package depends on //tensorflow/compiler/mlir/tensorflow:gen_mlir_passthrough_op_py in repository @ which failed to fetch. no such package '@llvm-project//mlir': Failed to execute overlay script: 'C:/Python38/python.exe D:/tensorflow/bazel-out/zfk46uyn/external/llvm-raw/utils/bazel/overlay_directories.py --src D:/tensorflow/bazel-out/zfk46uyn/external/llvm-raw --overlay D:/tensorflow/bazel-out/zfk46uyn/external/llvm-raw/utils/bazel/llvm-project-overlay --target .'
Exited with code 1
stdout:

stderr:
Traceback (most recent call last):
  File ""D:/tensorflow/bazel-out/zfk46uyn/external/llvm-raw/utils/bazel/overlay_directories.py"", line 92, in <module>
    main(parse_arguments())
  File ""D:/tensorflow/bazel-out/zfk46uyn/external/llvm-raw/utils/bazel/overlay_directories.py"", line 80, in main
    _symlink_abs(os.path.join(args.overlay, relpath),
  File ""D:/tensorflow/bazel-out/zfk46uyn/external/llvm-raw/utils/bazel/overlay_directories.py"", line 64, in _symlink_abs
    os.symlink(os.path.abspath(from_path), os.path.abspath(to_path))
OSError: [WinError 1314] Le client ne dispose pas dÆun privilÞge nÚcessaire: 'D:\\tensorflow\\bazel-out\\zfk46uyn\\external\\llvm-raw\\utils\\bazel\\llvm-project-overlay\\.bazelignore' -> 'D:\\tensorflow\\bazel-out\\zfk46uyn\\external\\llvm-project\\.bazelignore'

ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed
INFO: Elapsed time: 1.558s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (38 packages loaded, 2 targets configured)
    currently loading: tensorflow/lite/python ... (2 packages)
    Fetching https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/flatbuffers/archive/v2.0.6.tar.gz
```
</details>"
60263,Windows bazel says python is not an executable when building,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

r2.12

### Custom Code

No

### OS Platform and Distribution

Widnows 10

### Mobile device

Asus pc

### Python version

3.11

### Bazel version

6.1.1

### GCC/Compiler version

msbuild 17.5.1+f6fdcf537

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I'm trying to build c++ tflite for windows using bazel by following the [official documentation][1]. So far I've installed everything it want's me to and added them to `PATH`. Then I cloned the github repo and checked out to `r2.12` branch.

Then I ran `python ./configure.py` and selected default for everything (said yes to override eigen strong inline). When doing so it declared that my python is located on `C:\Users\Asus\AppData\Local\Programs\Python\Python311\python.exe`.

After that running `bazel build -c opt //tensorflow/lite:tensorflowlite` on the directory where I've cloned tensorflow in cmd casuses the below error:


I checked whether python was there or not, it indeed is. Simply running `C:/Users/Asus/AppData/Local/Programs/Python/Python311/python.exe` in cmd indeed opens python. 

So I checked the internet for some solutions, I've removed the ""python installers"" from my system, added python to PATH, tried the same steps with tensorflow source zip instead of cloning it, nothing works. 

Some people suggested chaning some stuff inside the `py` directory of tensorflow but it didn't work aswell.

Why is this happening? What causes bazel to not see python even though it's there? How can I fix this and get a build with windows?


  [1]: https://www.tensorflow.org/install/source_windows
```


### Standalone code to reproduce the issue

```shell
Steps I've followed: 

- run:
`pip3 install -U six numpy wheel packaging
pip3 install -U keras_preprocessing --no-deps`

- Install bazel 6.1.1 and add it to PATH.

- Install MSYS2 and add bin to PATH.

- Install visual studio and setup the needed tools.

-git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow

- git checkout r2.12

- bazel build -c opt //tensorflow/lite:tensorflowlite
```


### Relevant log output

```shell
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=117
INFO: Reading rc options for 'build' from c:\users\asus\desktop\tensorflow\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/Users/Asus/AppData/Local/Programs/Python/Python311/python.exe
INFO: Reading rc options for 'build' from c:\users\asus\desktop\tensorflow\.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from c:\users\asus\desktop\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/Asus/AppData/Local/Programs/Python/Python311/python.exe --action_env PYTHON_LIB_PATH=C:/Users/Asus/AppData/Local/Programs/Python/Python311/Lib/site-packages --python_path=C:/Users/Asus/AppData/Local/Programs/Python/Python311/python.exe --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true
INFO: Reading rc options for 'build' from c:\users\asus\desktop\tensorflow\.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file c:\users\asus\desktop\tensorflow\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file c:\users\asus\desktop\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:windows in file c:\users\asus\desktop\tensorflow\.bazelrc: --copt=/W0 --host_copt=/W0 --copt=/Zc:__cplusplus --host_copt=/Zc:__cplusplus --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --features=compiler_param_file --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --cxxopt=/std:c++17 --host_cxxopt=/std:c++17 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/Zc:preprocessor --host_copt=/Zc:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file c:\users\asus\desktop\tensorflow\.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false
INFO: Repository local_config_python instantiated at:
  C:/users/asus/desktop/tensorflow/WORKSPACE:15:14: in <toplevel>
  C:/users/asus/desktop/tensorflow/tensorflow/workspace2.bzl:957:19: in workspace
  C:/users/asus/desktop/tensorflow/tensorflow/workspace2.bzl:104:21: in _tf_toolchains
Repository rule python_configure defined at:
  C:/users/asus/desktop/tensorflow/third_party/py/python_configure.bzl:298:35: in <toplevel>
ERROR: An error occurred during the fetch of repository 'local_config_python':
   Traceback (most recent call last):
        File ""C:/users/asus/desktop/tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl
                _create_local_python_repository(repository_ctx)
        File ""C:/users/asus/desktop/tensorflow/third_party/py/python_configure.bzl"", line 212, column 22, in _create_local_python_repository
                _check_python_bin(repository_ctx, python_bin)
        File ""C:/users/asus/desktop/tensorflow/third_party/py/python_configure.bzl"", line 145, column 25, in _check_python_bin
                auto_config_fail(""--define %s='%s' is not executable. Is it the python binary?"" % (
        File ""C:/users/asus/desktop/tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail
                fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg))
Error in fail: Configuration Error: --define PYTHON_BIN_PATH='C:/Users/Asus/AppData/Local/Programs/Python/Python311/python.exe' is not executable. Is it the python binary?
ERROR: C:/users/asus/desktop/tensorflow/WORKSPACE:15:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):
        File ""C:/users/asus/desktop/tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl
                _create_local_python_repository(repository_ctx)
        File ""C:/users/asus/desktop/tensorflow/third_party/py/python_configure.bzl"", line 212, column 22, in _create_local_python_repository
                _check_python_bin(repository_ctx, python_bin)
        File ""C:/users/asus/desktop/tensorflow/third_party/py/python_configure.bzl"", line 145, column 25, in _check_python_bin
                auto_config_fail(""--define %s='%s' is not executable. Is it the python binary?"" % (
        File ""C:/users/asus/desktop/tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail
                fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg))
Error in fail: Configuration Error: --define PYTHON_BIN_PATH='C:/Users/Asus/AppData/Local/Programs/Python/Python311/python.exe' is not executable. Is it the python binary?
INFO: Repository local_execution_config_python instantiated at:
  C:/users/asus/desktop/tensorflow/WORKSPACE:15:14: in <toplevel>
  C:/users/asus/desktop/tensorflow/tensorflow/workspace2.bzl:957:19: in workspace
  C:/users/asus/desktop/tensorflow/tensorflow/workspace2.bzl:94:27: in _tf_toolchains
  C:/users/asus/desktop/tensorflow/tensorflow/tools/toolchains/remote_config/configs.bzl:6:28: in initialize_rbe_configs
  C:/users/asus/desktop/tensorflow/tensorflow/tools/toolchains/remote_config/rbe_config.bzl:158:27: in _tensorflow_local_config
Repository rule local_python_configure defined at:
  C:/users/asus/desktop/tensorflow/third_party/py/python_configure.bzl:279:41: in <toplevel>
INFO: Repository go_sdk instantiated at:
  C:/users/asus/desktop/tensorflow/WORKSPACE:23:14: in <toplevel>
  C:/users/asus/desktop/tensorflow/tensorflow/workspace0.bzl:134:20: in workspace
  C:/users/asus/_bazel_asus/ddsftcyc/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps
  C:/users/asus/_bazel_asus/ddsftcyc/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains
  C:/users/asus/_bazel_asus/ddsftcyc/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk
Repository rule _go_download_sdk defined at:
  C:/users/asus/_bazel_asus/ddsftcyc/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>
ERROR: Analysis of target '//tensorflow/lite:tensorflowlite' failed; build aborted: Configuration Error: --define PYTHON_BIN_PATH='C:/Users/Asus/AppData/Local/Programs/Python/Python311/python.exe' is not executable. Is it the python binary?
INFO: Elapsed time: 224.163s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (32 packages loaded, 15 targets configured)
```
</details>"
60262,Collected Tensorflow profiles are not recognized in Tensorboard,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.13.0-dev20230406

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 22.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

n/a

### GCC/Compiler version

n/a

### CUDA/cuDNN version

11.8.0/8.6.0.163

### GPU model and memory

NVIDIA GeForce RTX 3080 Ti 12GiB

### Current Behaviour?

I am following the tutorial at https://www.tensorflow.org/tutorials/quickstart/beginner

I modified the code according to the instructions at https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras in order to enable profiling for a range of batches during training.

With this change, training seems to proceed as normal, with the logs indicating that a profiler session is created, and a profile is collected.

The logs directory contains one non-empty `plugins/profile/<date>/<host>.xplane.pb` file.

But when I run tensorboard (either main or tb-nightly) on the logs, it fails to detect a profile (the Profile tab is missing from the UI). I also confirm I ran `pip install -U tensorboard-plugin-profile` first.

I would have expected one of these two outcomes: either (a) tensorboard would show me the profiles, or (b) if something went wrong either when collecting or displaying the profiles, an error message would have indicated it so I can fix the issue.

### Standalone code to reproduce the issue

```shell
# The code is at https://www.tensorflow.org/tutorials/quickstart/beginner
# I change the model.fit() call to use the Tensorboard callback to collect a profile:

log_dir = ""logs/fit/"" + datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"")
tensorboard_callback = tf.keras.callbacks.TensorBoard(
    log_dir=log_dir,
    histogram_freq=1,
    profile_batch=(500, 600))
model.fit(x_train, y_train, epochs=5, callbacks=[tensorboard_callback])
```

### Relevant log output

```shell
2023-04-06 23:17:28.048863: I tensorflow/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.
2023-04-06 23:17:28.048880: I tensorflow/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.
2023-04-06 23:17:28.048915: I tensorflow/compiler/xla/backends/profiler/gpu/cupti_tracer.cc:1671] Profiler found 1 GPUs
2023-04-06 23:17:28.237604: I tensorflow/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.
2023-04-06 23:17:28.237742: I tensorflow/compiler/xla/backends/profiler/gpu/cupti_tracer.cc:1805] CUPTI activity buffer flushed
Epoch 1/5
2023-04-06 23:17:28.747772: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f08c0180cf0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-04-06 23:17:28.747785: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3080 Ti, Compute Capability 8.6
2023-04-06 23:17:28.751189: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2023-04-06 23:17:28.834436: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:426] Loaded cuDNN version 8600
2023-04-06 23:17:28.868033: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory
2023-04-06 23:17:28.900180: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
 522/1875 [=======>......................] - ETA: 5s - loss: 0.4875 - accuracy: 0.8590
2023-04-06 23:17:30.991051: I tensorflow/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.
2023-04-06 23:17:30.991106: I tensorflow/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.
 645/1875 [=========>....................] - ETA: 4s - loss: 0.4499 - accuracy: 0.8701
2023-04-06 23:17:31.542500: I tensorflow/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.
2023-04-06 23:17:31.545123: I tensorflow/compiler/xla/backends/profiler/gpu/cupti_tracer.cc:1805] CUPTI activity buffer flushed
2023-04-06 23:17:31.570874: I tensorflow/compiler/xla/backends/profiler/gpu/cupti_collector.cc:541]  GpuTracer has collected 6158 callback api events and 5891 activity events. 
2023-04-06 23:17:31.598454: I tensorflow/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.
1875/1875 [==============================] - 8s 4ms/step - loss: 0.3017 - accuracy: 0.9121
Epoch 2/5
1875/1875 [==============================] - 7s 4ms/step - loss: 0.1441 - accuracy: 0.9570
Epoch 3/5
1875/1875 [==============================] - 7s 4ms/step - loss: 0.1075 - accuracy: 0.9685
Epoch 4/5
1875/1875 [==============================] - 6s 3ms/step - loss: 0.0878 - accuracy: 0.9732
Epoch 5/5
1875/1875 [==============================] - 6s 3ms/step - loss: 0.0737 - accuracy: 0.9771
```
</details>"
60261,[lite]Compiler build warnings ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

No

### OS Platform and Distribution

Linux Raspberrypi OS 32-bit (bullseye)  - (Linux 6.1.19+ \# 1637 Tue Mar 14 11:01:56 GMT 2023 armv6l GNU/Linux)

### Mobile device

Raspberry Pi Zero W

### Python version

3.9.2

### Bazel version

CMake build used

### GCC/Compiler version

GNU c++ (Raspbian 10.2.1-6+rpi1) 10.2.1 20210110

### CUDA/cuDNN version

_N.A._

### GPU model and memory

_N.A._

### Current Behaviour?

I get the following warning while building tensorflow-lite with cmake on a Raspberry Pi Zero W running Raspberrypi OS lite 32-bit (Debian bullseye).
```
[ 54%] Building CXX object CMakeFiles/tensorflow-lite.dir/kernels/conv3d.cc.o
/usr/bin/c++ -DCPUINFO_SUPPORTED_PLATFORM=1 -DEIGEN_MPL2_ONLY -DNOMINMAX=1 -DPTHREADPOOL_NO_DEPRECATED_API=1 -I/home/samveen/tensorflow -I/home/samveen/tensorflow/build/gemmlowp -I/home/samveen/tensorflow/build/eigen -I/home/samveen/tensorflow/build/neon2sse -I/home/samveen/tensorflow/build/abseil-cpp -I/home/samveen/tensorflow/build/farmhash/src -I/home/samveen/tensorflow/build/flatbuffers/include -I/home/samveen/tensorflow/build/ruy -I/home/samveen/tensorflow/build/cpuinfo/include -I/home/samveen/tensorflow/build/pthreadpool-source/include -march=armv6 -mfpu=vfp -mfloat-abi=hard -funsafe-math-optimizations -O3 -DNDEBUG -fPIC -DEIGEN_NEON_GEBP_NR=4 -DTFL_STATIC_LIBRARY_BUILD -pthread -std=gnu++17 -o CMakeFiles/tensorflow-lite.dir/kernels/conv3d.cc.o -c /home/samveen/tensorflow/tensorflow/lite/kernels/conv3d.cc
In file included from /home/samveen/tensorflow/build/eigen/Eigen/Core:266,
                 from /home/samveen/tensorflow/third_party/eigen3/Eigen/Core:1,
                 from /home/samveen/tensorflow/tensorflow/lite/kernels/internal/optimized/optimized_ops.h:39,
                 from /home/samveen/tensorflow/tensorflow/lite/kernels/conv3d.cc:25:
/home/samveen/tensorflow/build/eigen/Eigen/src/Core/functors/UnaryFunctors.h: In instantiation of ‘Eigen::internal::scalar_unary_pow_op<Scalar, ExponentScalar, false, false, false, false>::scalar_unary_pow_op(const ExponentScalar&) [with Scalar = float; ExponentScalar = double]’:
/home/samveen/tensorflow/build/eigen/Eigen/src/Core/../plugins/ArrayCwiseUnaryOps.h:714:66:   required from ‘Eigen::ArrayBase<Derived>::UnaryPowReturnType<ScalarExponent> Eigen::ArrayBase<Derived>::pow(const ScalarExponent&) const [with ScalarExponent = double; Derived = Eigen::ArrayWrapper<Eigen::Map<Eigen::Matrix<float, -1, -1>, 0, Eigen::Stride<0, 0> > >; Eigen::ArrayBase<Derived>::UnaryPowReturnType<ScalarExponent> = Eigen::CwiseUnaryOp<Eigen::internal::scalar_unary_pow_op<float, double, false, false, false, false>, const Eigen::ArrayWrapper<Eigen::Map<Eigen::Matrix<float, -1, -1>, 0, Eigen::Stride<0, 0> > > >]’
/home/samveen/tensorflow/tensorflow/lite/kernels/internal/optimized/optimized_ops.h:3327:78:   required from here
/home/samveen/tensorflow/build/eigen/Eigen/src/Core/functors/UnaryFunctors.h:1218:27: warning: ‘std::enable_if_t<(! IsExactlyRepresentable), void> Eigen::internal::scalar_unary_pow_op<Scalar, ExponentScalar, false, false, false, false>::check_is_representable() const [with bool IsExactlyRepresentable = false; Scalar = float; ExponentScalar = double; std::enable_if_t<(! IsExactlyRepresentable), void> = void]’ is deprecated [-Wdeprecated-declarations]
 1218 |     check_is_representable();
      |     ~~~~~~~~~~~~~~~~~~~~~~^~
/home/samveen/tensorflow/build/eigen/Eigen/src/Core/functors/UnaryFunctors.h:1214:68: note: declared here
 1214 |   EIGEN_DEPRECATED std::enable_if_t<!IsExactlyRepresentable, void> check_is_representable() const {}
      |                                                                    ^~~~~~~~~~~~~~~~~~~~~~
```



### Standalone code to reproduce the issue
Standard build on a Pi Zero W running Raspberry Pi OS lite 32-bit (Debian bullseye)
```shell
mkdir build && cd build
ARMCC_FLAGS=""-march=armv6 -mfpu=vfp -mfloat-abi=hard -funsafe-math-optimizations""
cmake -DCMAKE_C_FLAGS=""${ARMCC_FLAGS}"" -DCMAKE_CXX_FLAGS=""${ARMCC_FLAGS}""  \
        -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON -DCMAKE_SYSTEM_NAME=Linux -DCMAKE_SYSTEM_PROCESSOR=armv6 \
        -DTFLITE_ENABLE_XNNPACK=OFF ../tensorflow/lite/
cmake --build .
```

### Relevant log output

```shell
[ 54%] Building CXX object CMakeFiles/tensorflow-lite.dir/kernels/conv3d.cc.o
/usr/bin/c++ -DCPUINFO_SUPPORTED_PLATFORM=1 -DEIGEN_MPL2_ONLY -DNOMINMAX=1 -DPTHREADPOOL_NO_DEPRECATED_API=1 -I/home/samveen/tensorflow -I/home/samveen/tensorflow/build/gemmlowp -I/home/samveen/tensorflow/build/eigen -I/home/samveen/tensorflow/build/neon2sse -I/home/samveen/tensorflow/build/abseil-cpp -I/home/samveen/tensorflow/build/farmhash/src -I/home/samveen/tensorflow/build/flatbuffers/include -I/home/samveen/tensorflow/build/ruy -I/home/samveen/tensorflow/build/cpuinfo/include -I/home/samveen/tensorflow/build/pthreadpool-source/include -march=armv6 -mfpu=vfp -mfloat-abi=hard -funsafe-math-optimizations -O3 -DNDEBUG -fPIC -DEIGEN_NEON_GEBP_NR=4 -DTFL_STATIC_LIBRARY_BUILD -pthread -std=gnu++17 -o CMakeFiles/tensorflow-lite.dir/kernels/conv3d.cc.o -c /home/samveen/tensorflow/tensorflow/lite/kernels/conv3d.cc
In file included from /home/samveen/tensorflow/build/eigen/Eigen/Core:266,
                 from /home/samveen/tensorflow/third_party/eigen3/Eigen/Core:1,
                 from /home/samveen/tensorflow/tensorflow/lite/kernels/internal/optimized/optimized_ops.h:39,
                 from /home/samveen/tensorflow/tensorflow/lite/kernels/conv3d.cc:25:
/home/samveen/tensorflow/build/eigen/Eigen/src/Core/functors/UnaryFunctors.h: In instantiation of ‘Eigen::internal::scalar_unary_pow_op<Scalar, ExponentScalar, false, false, false, false>::scalar_unary_pow_op(const ExponentScalar&) [with Scalar = float; ExponentScalar = double]’:
/home/samveen/tensorflow/build/eigen/Eigen/src/Core/../plugins/ArrayCwiseUnaryOps.h:714:66:   required from ‘Eigen::ArrayBase<Derived>::UnaryPowReturnType<ScalarExponent> Eigen::ArrayBase<Derived>::pow(const ScalarExponent&) const [with ScalarExponent = double; Derived = Eigen::ArrayWrapper<Eigen::Map<Eigen::Matrix<float, -1, -1>, 0, Eigen::Stride<0, 0> > >; Eigen::ArrayBase<Derived>::UnaryPowReturnType<ScalarExponent> = Eigen::CwiseUnaryOp<Eigen::internal::scalar_unary_pow_op<float, double, false, false, false, false>, const Eigen::ArrayWrapper<Eigen::Map<Eigen::Matrix<float, -1, -1>, 0, Eigen::Stride<0, 0> > > >]’
/home/samveen/tensorflow/tensorflow/lite/kernels/internal/optimized/optimized_ops.h:3327:78:   required from here
/home/samveen/tensorflow/build/eigen/Eigen/src/Core/functors/UnaryFunctors.h:1218:27: warning: ‘std::enable_if_t<(! IsExactlyRepresentable), void> Eigen::internal::scalar_unary_pow_op<Scalar, ExponentScalar, false, false, false, false>::check_is_representable() const [with bool IsExactlyRepresentable = false; Scalar = float; ExponentScalar = double; std::enable_if_t<(! IsExactlyRepresentable), void> = void]’ is deprecated [-Wdeprecated-declarations]
 1218 |     check_is_representable();
      |     ~~~~~~~~~~~~~~~~~~~~~~^~
/home/samveen/tensorflow/build/eigen/Eigen/src/Core/functors/UnaryFunctors.h:1214:68: note: declared here
 1214 |   EIGEN_DEPRECATED std::enable_if_t<!IsExactlyRepresentable, void> check_is_representable() const {}
      |                                                                    ^~~~~~~~~~~~~~~~~~~~~~
```
</details>"
60256,pip install tensorflow for Mac M1 Python3.11 matching distribution not found - nightly build does,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12

### Custom Code

No

### OS Platform and Distribution

Mac OSX M2

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
please could you support mac M1's in a way where we dont have to build from source

tf-nightly is working out of the box but standard pip tensorflow is currently not supported
```


### Standalone code to reproduce the issue

```shell
pip install tensorflow
ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensorflow

pip install tensorflow-macos
ERROR: Could not find a version that satisfies the requirement tensorflow-macos (from versions: none)
ERROR: No matching distribution found for tensorflow-macos

pip install tf-nightly 
Downloading tf_nightly-2.13.0.dev20230406-cp311-cp311-macosx_12_0_arm64.whl (2.1 kB)
```


### Relevant log output

_No response_</details>"
60255,Use after free in propagator_state.cc,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf2.12

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Pointer `next_iter` from function `PropagatorState::FrameState::IncrementIteration` is passed 
as the 1st parameter into `ActivateLoopInvs` where it is passed as the 1st parameter into 
`AdjustOutstandingOpsLocked`, then inside this function it is passed into `CleanupIterations` 
where it is deleted.

Then in `PropagatorState::FrameState::IncrementIteration` this possibly freed pointer is used 
in `return`-statement.

This behavior was introduced by https://github.com/tensorflow/tensorflow/commit/ae2a0e5c473f2a575767262021c26852d22886f8. 
Before this commit, no `return` was performed on the possibly freed pointer.
```


### Standalone code to reproduce the issue

```shell
Bug was found by Svace static analysis tool.
```


### Relevant log output

_No response_</details>"
60254,WSL2 fit function not works tensorflow 2.12.0,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

v2.12.0-rc1-12-g0db597d0d75 2.12.0

### Custom Code

No

### OS Platform and Distribution

WSL2 Ubuntu / Windows 10 19044 64bit

### Mobile device

_No response_

### Python version

3.10.9-3.10.10

### Bazel version

-

### GCC/Compiler version

-

### CUDA/cuDNN version

11.8.0 / 8.6.0.163

### GPU model and memory

1050ti 4GB

### Current Behaviour?

```shell
Installed all using this instructions https://www.tensorflow.org/install/pip?hl=ru#windows-wsl2
Python see my GPU, but model.fit function not works.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.python.client import device_lib
import numpy as np
print(tf.__version__)

print(f""Tensor Flow Version: {tf.__version__}"")
print(f""Keras Version: {tf.keras.__version__}"")
print()
gpu = len(tf.config.list_physical_devices('GPU'))>0
print(""GPU is"", ""available"" if gpu else ""NOT AVAILABLE"")

samples = np.array([
    [u'Россия', 0],
    [u'Вчера смотрел в кино - потрясающий фильм! Актёры высшие, невероятные декорации, безудержный драйв на протяжении всего фильма. Давно не испытывал такого восторга от просмотра! 10/10', 1],
    [u'Норм фильм,в своём стиле не понимаю что другие ожидали))одно смутило когда сцена в клубе все танчили пока бойня была типо ниче не замечая а как картежника завалили все с истериками побежали,типа хуясе тут все в настаящую))))да и пёсель зачетный))', 1],
    [u'Да пипец блин, меня хватило на 10 минут. Это днище', 0],
    [u'Бредовый фильм не советую', 0],
])

test = np.array([
    [u'Фильм говно', 0],
    [u'Классный фильм', 1],
    [u'Не советую к просмотру', 0],
    [u'Тупой фильм', 0],
])

train_text = []
train_label = []

test_text = []
test_label = []

for sample in samples:
    train_text.append(sample[0])
    train_label.append(float(sample[1]))

for tst in test:
    test_text.append(tst[0])
    test_label.append(float(tst[1]))

dataset = {'train': 0, 'test': 0}

dataset['train'] = tf.data.Dataset.from_tensor_slices((train_text, train_label))
dataset['test'] = tf.data.Dataset.from_tensor_slices((test_text, test_label))

train_dataset, test_dataset = dataset['train'], dataset['test']

for text, lable in train_dataset.take(2):
    print(text)

BUFFER_SIZE = 10000
BATCH_SIZE = 128

train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

VOCAB_SIZE = 20000
encoder = tf.keras.layers.TextVectorization(
    standardize='lower',
    max_tokens=VOCAB_SIZE,
    encoding='utf-8')
encoder.adapt(train_dataset.map(lambda text, label: text))


model = tf.keras.Sequential([
    encoder,
    tf.keras.layers.Embedding(
        input_dim=len(encoder.get_vocabulary()),
        output_dim=64,
        # Use masking to handle the variable sequence lengths
        mask_zero=True),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1)
])

model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              optimizer=tf.keras.optimizers.Adam(1e-4),
              metrics=['accuracy'])

history = model.fit(train_dataset, epochs=250,
                    validation_data=test_dataset)
                

sample_text = 'меня хватило на 10 минут'
predictions = model.predict(np.array([sample_text]))
print(predictions)
```


### Relevant log output

```shell
2023-04-06 12:06:58.601576: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-06 12:07:00.057818: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2.12.0
Tensor Flow Version: 2.12.0
Keras Version: 2.12.0

2023-04-06 12:07:01.276945: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-04-06 12:07:01.462465: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-04-06 12:07:01.462581: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
GPU is available
2023-04-06 12:07:01.477422: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-04-06 12:07:01.477546: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-04-06 12:07:01.477680: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-04-06 12:07:06.851452: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-04-06 12:07:06.852698: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-04-06 12:07:06.852765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.
2023-04-06 12:07:06.852885: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-04-06 12:07:06.873685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2519 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1
2023-04-06 12:07:17.597288: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [92]
         [[{{node Placeholder/_1}}]]
tf.Tensor(b'\xd0\xa0\xd0\xbe\xd1\x81\xd1\x81\xd0\xb8\xd1\x8f', shape=(), dtype=string)
tf.Tensor(b'\xd0\x92\xd1\x87\xd0\xb5\xd1\x80\xd0\xb0 \xd1\x81\xd0\xbc\xd0\xbe\xd1\x82\xd1\x80\xd0\xb5\xd0\xbb \xd0\xb2 \xd0\xba\xd0\xb8\xd0\xbd\xd0\xbe - \xd0\xbf\xd0\xbe\xd1\x82\xd1\x80\xd1\x8f\xd1\x81\xd0\xb0\xd1\x8e\xd1\x89\xd0\xb8\xd0\xb9 \xd1\x84\xd0\xb8\xd0\xbb\xd1\x8c\xd0\xbc! \xd0\x90\xd0\xba\xd1\x82\xd1\x91\xd1\x80\xd1\x8b \xd0\xb2\xd1\x8b\xd1\x81\xd1\x88\xd0\xb8\xd0\xb5, \xd0\xbd\xd0\xb5\xd0\xb2\xd0\xb5\xd1\x80\xd0\xbe\xd1\x8f\xd1\x82\xd0\xbd\xd1\x8b\xd0\xb5 \xd0\xb4\xd0\xb5\xd0\xba\xd0\xbe\xd1\x80\xd0\xb0\xd1\x86\xd0\xb8\xd0\xb8, \xd0\xb1\xd0\xb5\xd0\xb7\xd1\x83\xd0\xb4\xd0\xb5\xd1\x80\xd0\xb6\xd0\xbd\xd1\x8b\xd0\xb9 \xd0\xb4\xd1\x80\xd0\xb0\xd0\xb9\xd0\xb2 \xd0\xbd\xd0\xb0 \xd0\xbf\xd1\x80\xd0\xbe\xd1\x82\xd1\x8f\xd0\xb6\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb8 \xd0\xb2\xd1\x81\xd0\xb5\xd0\xb3\xd0\xbe \xd1\x84\xd0\xb8\xd0\xbb\xd1\x8c\xd0\xbc\xd0\xb0. \xd0\x94\xd0\xb0\xd0\xb2\xd0\xbd\xd0\xbe \xd0\xbd\xd0\xb5 \xd0\xb8\xd1\x81\xd0\xbf\xd1\x8b\xd1\x82\xd1\x8b\xd0\xb2\xd0\xb0\xd0\xbb \xd1\x82\xd0\xb0\xd0\xba\xd0\xbe\xd0\xb3\xd0\xbe \xd0\xb2\xd0\xbe\xd1\x81\xd1\x82\xd0\xbe\xd1\x80\xd0\xb3\xd0\xb0 \xd0\xbe\xd1\x82 \xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd0\xbc\xd0\xbe\xd1\x82\xd1\x80\xd0\xb0! 10/10', shape=(), dtype=string)2023-04-06 12:07:18.366287: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [92]
         [[{{node Placeholder/_1}}]]
2023-04-06 12:07:18.366750: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [92]
         [[{{node Placeholder/_0}}]]
2023-04-06 12:07:23.301274: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [92]
         [[{{node Placeholder/_1}}]]
2023-04-06 12:07:23.301919: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [92]
         [[{{node Placeholder/_0}}]]
Epoch 1/250
2023-04-06 12:07:26.986857: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]
         [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]
2023-04-06 12:07:30.238269: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]
         [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]
2023-04-06 12:07:31.901964: W tensorflow/core/common_runtime/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:
type_id: TFT_OPTIONAL
args {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_TENSOR
    args {
      type_id: TFT_INT32
    }
  }
}
 is neither a subtype nor a supertype of the combined inputs preceding it:
type_id: TFT_OPTIONAL
args {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_TENSOR
    args {
      type_id: TFT_FLOAT
    }
  }
}

        while inferring type of node 'cond_40/output/_23'
2023-04-06 12:07:38.520941: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600
2023-04-06 12:07:39.881521: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x1da1c010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-04-06 12:07:39.881632: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce GTX 1050 Ti, Compute Capability 6.1
2023-04-06 12:07:39.994222: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2023-04-06 12:07:40.402391: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:530] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.
Searched for CUDA in the following directories:
  ./cuda_sdk_lib
  /usr/local/cuda-11.8
  /usr/local/cuda
  .
You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2023-04-06 12:07:40.405914: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-04-06 12:07:40.408337: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-04-06 12:07:40.408428: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:GPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INTERNAL: libdevice not found at ./libdevice.10.bc
         [[{{node StatefulPartitionedCall_10}}]]
2023-04-06 12:07:40.444717: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-04-06 12:07:40.445387: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-04-06 12:07:40.475982: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-04-06 12:07:40.476520: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-04-06 12:07:40.509245: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-04-06 12:07:40.509756: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
Traceback (most recent call last):
  File ""/home/yatebyaeby/test.py"", line 168, in <module>
    history = model.fit(train_dataset, epochs=250,
  File ""/home/yatebyaeby/miniconda3/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/yatebyaeby/miniconda3/lib/python3.10/site-packages/tensorflow/python/eager/execute.py"", line 52, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InternalError: Graph execution error:

Detected at node 'StatefulPartitionedCall_10' defined at (most recent call last):
    File ""/home/yatebyaeby/test.py"", line 168, in <module>
      history = model.fit(train_dataset, epochs=250,
    File ""/home/yatebyaeby/miniconda3/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/home/yatebyaeby/miniconda3/lib/python3.10/site-packages/keras/engine/training.py"", line 1685, in fit
      tmp_logs = self.train_function(iterator)
    File ""/home/yatebyaeby/miniconda3/lib/python3.10/site-packages/keras/engine/training.py"", line 1284, in train_function
      return step_function(self, iterator)
    File ""/home/yatebyaeby/miniconda3/lib/python3.10/site-packages/keras/engine/training.py"", line 1268, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/home/yatebyaeby/miniconda3/lib/python3.10/site-packages/keras/engine/training.py"", line 1249, in run_step
      outputs = model.train_step(data)
    File ""/home/yatebyaeby/miniconda3/lib/python3.10/site-packages/keras/engine/training.py"", line 1054, in train_step
      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File ""/home/yatebyaeby/miniconda3/lib/python3.10/site-packages/keras/optimizers/optimizer.py"", line 543, in minimize
      self.apply_gradients(grads_and_vars)
    File ""/home/yatebyaeby/miniconda3/lib/python3.10/site-packages/keras/optimizers/optimizer.py"", line 1174, in apply_gradients
      return super().apply_gradients(grads_and_vars, name=name)
    File ""/home/yatebyaeby/miniconda3/lib/python3.10/site-packages/keras/optimizers/optimizer.py"", line 650, in apply_gradients
      iteration = self._internal_apply_gradients(grads_and_vars)
    File ""/home/yatebyaeby/miniconda3/lib/python3.10/site-packages/keras/optimizers/optimizer.py"", line 1200, in _internal_apply_gradients
      return tf.__internal__.distribute.interim.maybe_merge_call(
    File ""/home/yatebyaeby/miniconda3/lib/python3.10/site-packages/keras/optimizers/optimizer.py"", line 1250, in _distributed_apply_gradients_fn
      distribution.extended.update(
    File ""/home/yatebyaeby/miniconda3/lib/python3.10/site-packages/keras/optimizers/optimizer.py"", line 1245, in apply_grad_to_update_var
      return self._update_step_xla(grad, var, id(self._var_key(var)))
Node: 'StatefulPartitionedCall_10'
libdevice not found at ./libdevice.10.bc
         [[{{node StatefulPartitionedCall_10}}]] [Op:__inference_train_function_14740]
```
</details>"
60253,"Does tensorflow lite not support compilation with both parameters set on windows? ""--cpu=x64_x86_windows --define tflite_with_xnnpack=true""","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

r2.9,r2.10,2.11

### Custom Code

No

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
run: bazel --output_user_root=./../tensorflow_build build -s --config=opt --define tflite_with_xnnpack=true --cpu=x64_x86_windows //tensorflow/lite:tensorflowlite.dll
```


### Standalone code to reproduce the issue

```shell
result: 
ERROR: C:/users/togrey/desktop/tensorflow_build/master1/gaunzc4b/external/XNNPACK/BUILD.bazel:4762:26: configurable attribute ""deps"" in @XNNPACK//:amalgam_microkernels doesn't match this configuration. Would a default condition help?

Conditions checked:
 @XNNPACK//:aarch32
 @XNNPACK//:aarch64
 @XNNPACK//:x86
 @XNNPACK//:emscripten_wasm
 @XNNPACK//:emscripten_wasmsimd
 @XNNPACK//:emscripten_wasmrelaxedsimd
 @XNNPACK//:riscv
To see a condition's definition, run: bazel query --output=build <condition label>.

On tf version r2.9:
ERROR: C:/users/togrey/desktop/tensorflow_build/2.11/gaunzc4b/external/cpuinfo/BUILD.bazel:103:11: configurable attribute ""srcs"" in @cpuinfo//:cpuinfo_impl doesn't match this configuration. Would a default condition help?

Conditions checked:
 @cpuinfo//:linux_x86_64
 @cpuinfo//:linux_arm
 @cpuinfo//:linux_armhf
 @cpuinfo//:linux_armv7a
 @cpuinfo//:linux_armeabi
 @cpuinfo//:linux_aarch64
 @cpuinfo//:linux_mips64
 @cpuinfo//:linux_riscv64
 @cpuinfo//:linux_s390x
 @cpuinfo//:macos_x86_64
 @cpuinfo//:macos_arm64
 @cpuinfo//:windows_x86_64
 @cpuinfo//:android_armv7
 @cpuinfo//:android_arm64
 @cpuinfo//:android_x86
 @cpuinfo//:android_x86_64
 @cpuinfo//:ios_x86_64
 @cpuinfo//:ios_x86
 @cpuinfo//:ios_armv7
 @cpuinfo//:ios_arm64
 @cpuinfo//:ios_arm64e
 @cpuinfo//:ios_sim_arm64
 @cpuinfo//:watchos_x86_64
 @cpuinfo//:watchos_x86
 @cpuinfo//:watchos_armv7k
 @cpuinfo//:watchos_arm64_32
 @cpuinfo//:tvos_x86_64
 @cpuinfo//:tvos_arm64
 @cpuinfo//:emscripten_wasm

To see a condition's definition, run: bazel query --output=build <condition label>.
```


### Relevant log output

_No response_</details>"
60247,output Tensor data pointer is NULL before calling Invoke(),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf2.10

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04.4 LTS

### Mobile device

_No response_

### Python version

Python 3.8.10

### Bazel version

bazel 6.1.1

### GCC/Compiler version

gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hello,

I run into the following issue, I try to retrieve the data pointer from the output tensor of my tensorflow lite model before running Invoke() and it is NULL.

Is it the expected behavior?

I would have supposed that after calling AllocateTensors() the tensors could be used.
```


### Standalone code to reproduce the issue

```shell
I run the following code:
class ModelTfLite
{
public:
    ModelTfLite(const std::string& path);
    ~ModelTfLite() = default;

    bool Invoke();

private:
    std::unique_ptr<tflite::FlatBufferModel> model_;
    tflite::ops::builtin::BuiltinOpResolver resolver_;
    std::unique_ptr<tflite::Interpreter> interpreter_;
};

ModelTfLite::ModelTfLite(const std::string& path)
{
    model_ = tflite::FlatBufferModel::BuildFromFile(path.c_str());
    assert(model_ != nullptr);

    tflite::InterpreterBuilder builder(*model_, resolver_);
    builder(&interpreter_);
    assert(interpreter_ != nullptr);
}

bool ModelTfLite::Invoke()
{
    TfLiteStatus status;

    interpreter_->AllocateTensors();

    void* inputTensor = interpreter_->input_tensor(0)->data.data;
    void* outputTensor = interpreter_->output_tensor(0)->data.data;
    std::cout << ""Before Invoke inputTensor= "" << inputTensor << "" outputTensor= "" << outputTensor << ""\n"" ;

    status = interpreter_->Invoke();

    inputTensor = interpreter_->input_tensor(0)->data.data;
    outputTensor = interpreter_->output_tensor(0)->data.data;
    std::cout << ""After Invoke inputTensor= "" << inputTensor << "" outputTensor= "" << outputTensor << ""\n"" ;

    if(status != kTfLiteOk) {
        std::cout << ""Failed to run Invoke(): "" << status << ""\n"" ;
        return false;
    }
    return true;
}

main:
int main(int argc, char* argv[]) {
    if (argc != 2) {
        fprintf(stderr, ""testmodel <tflite model>\n"");
        return 1;
    }
    const char* filename = argv[1];

    ModelTfLite m(filename);

    m.Invoke();
    return 0;
}
```


### Relevant log output

```shell
Before Invoke inputTensor= 0x56253a528800 outputTensor= 0
After Invoke inputTensor= 0x56253ab84d00 outputTensor= 0x56253ab84e00
```
</details>"
60245,Why training speed per step decrease when number of workers increase in distributed training on CPUs?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 16.04

### Mobile device

None

### Python version

3.7

### Bazel version

None

### GCC/Compiler version

None

### CUDA/cuDNN version

None

### GPU model and memory

None

### Current Behaviour?

When I follow the tutorial, [Multi-worker training with Keras](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras), using CPUs to do training. I found that the speed of each step(given the batch size of each worker is constant) decrease when I increase the number of workers. I know there will be communication cost when the number of workers is increasing, but I run it in the same physical node and the speed drop sharply. I expect that the speed of each step should be similar or just decrease within a small proportion when increasing the workers. Or could you please give an explanation?


### Standalone code to reproduce the issue

Just follow the tutorial in colab: [link](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/distribute/multi_worker_with_keras.ipynb)
and increase the number of workers.


### Relevant log output

_No response_</details>"
60244,Why training speed per step decrease when number of workers increase in distributed training on multiple cpus?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 16.04

### Mobile device

None

### Python version

3.7

### Bazel version

None

### GCC/Compiler version

None

### CUDA/cuDNN version

None

### GPU model and memory

None

### Current Behaviour?

```shell
When I follow the tutorial, [Distrited training with Keras](https://www.tensorflow.org/tutorials/distribute/keras), using CPU to do training. I found that the speed of each step(given the batch size of each worker is constant) decrease when I increase the number of workers. I know there will be communication cost when the number of workers is increasing, but I run it in the same physical node and the speed drop sharply. I expect that the speed of each step should be similar or just decrease within a small proportion when increasing the workers.
```


### Standalone code to reproduce the issue

```shell
Just follow the tutorial in colab: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/distribute/keras.ipynb
and increase the number of workers.
```


### Relevant log output

_No response_</details>"
60243,Cannot Install tensorflow-gpu on python=3.10.11,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

No

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
pip install tensorflow-gpu
```


### Relevant log output

```shell
Collecting tensorflow-gpu
  Downloading tensorflow-gpu-2.12.0.tar.gz (2.6 kB)
  Preparing metadata (setup.py) ... error
  error: subprocess-exited-with-error
  
  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─> [41 lines of output]
      Traceback (most recent call last):
        File ""C:\Users\shiva\OneDrive\Desktop\LipNet\venv\lib\site-packages\setuptools\_vendor\packaging\requirements.py"", line 35, in __init__  
          parsed = parse_requirement(requirement_string)
        File ""C:\Users\shiva\OneDrive\Desktop\LipNet\venv\lib\site-packages\setuptools\_vendor\packaging\_parser.py"", line 64, in parse_requirement
          return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))
        File ""C:\Users\shiva\OneDrive\Desktop\LipNet\venv\lib\site-packages\setuptools\_vendor\packaging\_parser.py"", line 82, in _parse_requirement
          url, specifier, marker = _parse_requirement_details(tokenizer)
        File ""C:\Users\shiva\OneDrive\Desktop\LipNet\venv\lib\site-packages\setuptools\_vendor\packaging\_parser.py"", line 126, in _parse_requirement_details
          marker = _parse_requirement_marker(
        File ""C:\Users\shiva\OneDrive\Desktop\LipNet\venv\lib\site-packages\setuptools\_vendor\packaging\_parser.py"", line 147, in _parse_requirement_marker
          tokenizer.raise_syntax_error(
        File ""C:\Users\shiva\OneDrive\Desktop\LipNet\venv\lib\site-packages\setuptools\_vendor\packaging\_tokenizer.py"", line 163, in raise_syntax_error
          raise ParserSyntaxError(
      setuptools.extern.packaging._tokenizer.ParserSyntaxError: Expected end or semicolon (after name and no valid version specifier)
          python_version>""3.7""
  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.
```
</details>"
60241,Tensorflow-gpu version 2.12.0 is not available,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf2.12.0

### Custom Code

No

### OS Platform and Distribution

Win11

### Mobile device

_No response_

### Python version

3.10.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
No version 2.12.0 of the tensorflow-gpu was found.
```


### Standalone code to reproduce the issue

```shell
Not have
```


### Relevant log output

_No response_</details>"
60239,"In tf.data.experimental.enable_debug_mode, tf.data.Dataset.ragged_batch fails with an error","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

TF 2.12.0, TF nightly 2.13.0-dev20230404

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Consider the following code creating ragged batches using `tf.data.Dataset.ragged_batch`:

```python
data = tf.data.Dataset.from_tensor_slices(tf.ragged.constant([[1, 2], [3]]))
list(data.ragged_batch(2))
```

The above code works fine in normal mode. However, if you enable debug mode using `tf.data.experimental.enable_debug_mode()`, the same code crashes with an error.

### Standalone code to reproduce the issue

I reproduced the error in https://colab.research.google.com/drive/1nf1BHjssx2YhF0ZbbgPg1QSALSS4Z89r?usp=sharing , both for TF 2.12.0 and TF nightly 2.13.0-dev20230404.

The code for triggering the bug is the following:

```python
tf.data.experimental.enable_debug_mode()
data = tf.data.Dataset.from_tensor_slices(tf.ragged.constant([[1, 2], [3]]))
list(data.ragged_batch(2))
```

### Relevant log output

Here is the error printed by TF 2.12.0

```
---------------------------------------------------------------------------

InvalidArgumentError                      Traceback (most recent call last)

<ipython-input-3-34b7e4bb8c4b> in <cell line: 4>()
      2 tf.data.experimental.enable_debug_mode()
      3 data = tf.data.Dataset.from_tensor_slices(tf.ragged.constant([[1, 2], [3]]))
----> 4 list(data.ragged_batch(2))

3 frames

/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   6651 def raise_from_not_ok_status(e, name):
   6652   e.message += ("" name: "" + str(name if name is not None else """"))
-> 6653   raise core._status_to_exception(e) from None  # pylint: disable=protected-access
   6654 
   6655 

InvalidArgumentError: {{function_node __wrapped__IteratorGetNext_output_types_1_device_/job:localhost/replica:0/task:0/device:CPU:0}} ValueError: Value [1 2] is not convertible to a tensor with dtype <dtype: 'variant'> and shape ().
Traceback (most recent call last):

  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/util/structure.py"", line 347, in reduce_fn
    component = ops.convert_to_tensor(component, spec.dtype)

  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/profiler/trace.py"", line 183, in wrapped
    return func(*args, **kwargs)

  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/ops.py"", line 1440, in convert_to_tensor
    return tensor_conversion_registry.convert(

  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py"", line 209, in convert
    return overload(dtype, name)  #  pylint: disable=not-callable

  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/ops.py"", line 1335, in __tf_tensor__
    return super().__tf_tensor__(dtype, name)

  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/ops.py"", line 967, in __tf_tensor__
    raise ValueError(

ValueError: Tensor conversion requested dtype variant for Tensor with dtype int32: <tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>


During handling of the above exception, another exception occurred:


Traceback (most recent call last):

  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/script_ops.py"", line 266, in __call__
    return func(device, token, args)

  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/script_ops.py"", line 144, in __call__
    outputs = self._call(device, args)

  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/script_ops.py"", line 151, in _call
    ret = self._func(*args)

  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/impl/api.py"", line 643, in wrapper
    return func(*args, **kwargs)

  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/ops/structured_function.py"", line 213, in py_function_wrapper
    ret = structure.to_tensor_list(self._output_structure, ret)

  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/util/structure.py"", line 410, in to_tensor_list
    return _to_tensor_list_helper(

  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/util/structure.py"", line 360, in _to_tensor_list_helper
    return functools.reduce(

  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/util/structure.py"", line 349, in reduce_fn
    raise ValueError(

ValueError: Value [1 2] is not convertible to a tensor with dtype <dtype: 'variant'> and shape ().


	 [[{{node EagerPyFunc}}]] [Op:IteratorGetNext] name:
```
</details>"
60237,tensorflow-2.12.0 not support GPU,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

v2.12.0-rc1-12-g0db597d0d75 2.12.0

### Custom Code

No

### OS Platform and Distribution

Windows 10

### Mobile device

-

### Python version

3.10.10

### Bazel version

-

### GCC/Compiler version

-

### CUDA/cuDNN version

cuda_11.8.0_522.06_windows / cudnn-windows-x86_64-8.6.0.163_cuda11

### GPU model and memory

1050ti 4 GB

### Current Behaviour?

```shell
Supporting GPU calcs. Ive tried version tf 2.10, all works, but 2.10 have some issues, that 2.12 dont have
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.python.client import device_lib
import numpy as np
print(tf.__version__)

print(f""Tensor Flow Version: {tf.__version__}"")
print(f""Keras Version: {tf.keras.__version__}"")
print()
gpu = len(tf.config.list_physical_devices('GPU'))>0
print(""GPU is"", ""available"" if gpu else ""NOT AVAILABLE"")
```


### Relevant log output

```shell
2.12.0
Tensor Flow Version: 2.12.0
Keras Version: 2.12.0

GPU is NOT AVAILABLE
```
</details>"
60236,Distributed training using multiple GPUs hangs when enabling eager execution,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf 2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04.5 LTS

### Mobile device

_No response_

### Python version

3.8.10 (docker tensorflow:devel-gpu)

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.7

### GPU model and memory

_No response_

### Current Behaviour?

```shell
After adding the following code to enable eager execution:
`
tf.config.run_functions_eagerly(True)
tf.data.experimental.enable_debug_mode()
`

Distributed training using MirroredStrategy with more than 2 GPUs hangs forever.

Using 1 GPU does not trigger the bug. Using MirroredStrategy with multiple logical CPUs also does not trigger the bug.

Whether adding the second line (`tf.data.experimental.enable_debug_mode()`) or not does not matter.

The model we used is a simple Sequential model trained on CIFAR-10 dataset.
The model structure is given as follows:
`
Input - GlobalMaxPool2D - BatchNormalization- Dense
`

Current behavior:
Using tf 2.11.0, the program hangs at the line of code:
`loss = model.fit(train_input, train_label)`
Also, the program cannot be stopped gracefully by keyboard interruption.

Using tf nightly, the program reports an error (see below).

Expected behavior:
The program should print the values of loss as an eager tensor.
```


### Standalone code to reproduce the issue

```shell
The code is also available at https://colab.research.google.com/drive/1BiAxZEO9IGak_4XOeZcyXRhJEByZ-X6B?usp=sharing

import keras
import tensorflow as tf

tf.config.run_functions_eagerly(True)
tf.data.experimental.enable_debug_mode()

if __name__ == ""__main__"":
    # Using at least 2 GPU triggers the bug
    strategy = tf.distribute.MirroredStrategy([""/GPU:0"", ""/GPU:1""])

    # Training data, batch_size=240
    train_input = tf.random.uniform(shape=(240, 32, 32, 3))
    train_label = tf.random.uniform(shape=(240, 10), minval=0, maxval=2, dtype=tf.int32)

    with strategy.scope():
        model = keras.Sequential([
            keras.layers.Input(shape=(32, 32, 3)),
            keras.layers.GlobalMaxPool2D(),
            keras.layers.BatchNormalization(axis=-1),
            keras.layers.Dense(10, ), ])
        optimizer = keras.optimizers.Adam(learning_rate=0.1)
        loss = tf.keras.losses.MeanSquaredError(
            reduction=tf.keras.losses.Reduction.NONE)
        model.compile(optimizer=optimizer, loss=loss)

    # One step training
    loss = model.fit(train_input, train_label)
    print(loss)
```


### Relevant log output

```shell
-------------- Output using tf 2.11.0 -----------------------------
tf-docker /mnt/src/reproduce > python ./reproduce_eager.py 
2023-04-05 03:23:45.884503: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-05 03:23:48.001421: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-05 03:23:53.074754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9636 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:3d:00.0, compute capability: 7.5
2023-04-05 03:23:53.076188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9636 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:3f:00.0, compute capability: 7.5
2023-04-05 03:23:53.077442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 9636 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:40:00.0, compute capability: 7.5
2023-04-05 03:23:53.078666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 9636 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:41:00.0, compute capability: 7.5
2023-04-05 03:23:53.079887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 9636 MB memory:  -> device: 4, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:61:00.0, compute capability: 7.5
2023-04-05 03:23:53.081109: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 9636 MB memory:  -> device: 5, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:62:00.0, compute capability: 7.5
2023-04-05 03:23:53.082252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 9636 MB memory:  -> device: 6, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:63:00.0, compute capability: 7.5
2023-04-05 03:23:53.083395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 9636 MB memory:  -> device: 7, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:64:00.0, compute capability: 7.5
WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.
(program hangs here)
--------------------------------------------------------------------

-------------- Output using tf nightly -----------------------------
tf-docker /mnt/src/reproduce > python ./reproduce_eager.py 
2023-04-05 03:29:24.607254: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-05 03:29:25.559092: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-04-05 03:29:32.898888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9598 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:3d:00.0, compute capability: 7.5
2023-04-05 03:29:32.900404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9598 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:3f:00.0, compute capability: 7.5
2023-04-05 03:29:32.901638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 9598 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:40:00.0, compute capability: 7.5
2023-04-05 03:29:32.902870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 9598 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:41:00.0, compute capability: 7.5
2023-04-05 03:29:32.904072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 9598 MB memory:  -> device: 4, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:61:00.0, compute capability: 7.5
2023-04-05 03:29:32.905328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 9598 MB memory:  -> device: 5, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:62:00.0, compute capability: 7.5
2023-04-05 03:29:32.906501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 9598 MB memory:  -> device: 6, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:63:00.0, compute capability: 7.5
2023-04-05 03:29:32.907653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 9598 MB memory:  -> device: 7, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:64:00.0, compute capability: 7.5
2023-04-05 03:29:33.578635: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_9' with dtype int32 and shape [240,10]
         [[{{node Placeholder/_9}}]]
2023-04-05 03:29:33.578971: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_9' with dtype int32 and shape [240,10]
         [[{{node Placeholder/_9}}]]
WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.
2023-04-05 03:29:35.301668: E tensorflow/core/common_runtime/base_collective_executor.cc:249] BaseCollectiveExecutor::StartAbort UNKNOWN: Error invoking NCCL: unhandled cuda error
Traceback (most recent call last):
  File ""./reproduce_eager.py"", line 27, in <module>
    loss = model.fit(train_input, train_label)
  File ""/usr/local/lib/python3.8/dist-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 364, in map
    return self._map_async(func, iterable, mapstar, chunksize).get()
  File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 771, in get
    raise self._value
  File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 125, in worker
    result = (True, func(*args, **kwds))
  File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 48, in mapstar
    return list(map(*args))
tensorflow.python.framework.errors_impl.UnknownError: {{function_node __wrapped__CollectiveReduceV2_Nordering_token_1_device_/job:localhost/replica:0/task:0/device:GPU:1}} Collective ops is aborted by: Error invoking NCCL: unhandled cuda error
The error could be from a previous operation. Restart your program to reset. [Op:CollectiveReduceV2] name:
--------------------------------------------------------------------
```
</details>"
60235,Metadata server host isn't configurable with tf.io.gfile,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

latest

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

setting the env var `GCE_METADATA_IP` to something other than `http://metadata.google.internal` has no effect to get the default service account token. This is currently hardcoded here https://github.com/tensorflow/tensorflow/blob/aaa18fec1cba172911259ca7b619fe32e4667fac/tensorflow/tsl/platform/cloud/compute_engine_metadata_client.cc#L28

Expected:
Respect the `GCE_METADATA_IP` env variable to be able to override `http://metadata.google.internal`

Note that TPU client already does this correctly: https://github.com/tensorflow/tensorflow/pull/40317

### Standalone code to reproduce the issue

```shell
export GCE_METADATA_IP=http://127.0.0.1:8000

import tensorflow as tf

tf.io.gfile.listdir(""gs://sam-poc"")
```


### Relevant log output

_No response_</details>"
60234,TFLite NNAPI Delegate converts INT8 UnidirectionalSequenceLSTM to incorrect NN operation type,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.12.0

### Custom Code

No

### OS Platform and Distribution

Android 12, aarch64

### Mobile device

Pixel 4 xl

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am trying to run a basic LSTM TFLite model with NNAPI delegate to explore the acceleration from Snapdragon 855's DSP Hexagon 690. I converted the simple LSTM model with full integer post-training quantization with intention to maximize hardware acceleration support, and ran this model on Pixel 4xl (snapdragon 855) with the latest pre-downloaded TFLite benchmark binary tool.

I am able to run other non-8bit model - env setup is correct. But 8bit model encountered error ` Unsupported input operand type for UNIDIRECTIONAL_SEQUENCE_LSTM op: TENSOR_QUANT8_ASYMM_SIGNED`.

I looked at NNAPI's operation support doc. It seems that operation `ANEURALNETWORKS_QUANTIZED_LSTM` is supported with the int8 inputs/outputs weights. But logcat suggests that TFLite NNAPI Model building process has converted the 8bit TFlite `UnidirectionalSequenceLSTM` to the non-quantized version `ANEURALNETWORKS_UNIDIRECTIONAL_SEQUENCE_LSTM`. Could this incorrect TFLite->NN operation conversion led to [this error](https://android.googlesource.com/platform/frameworks/ml/+/master/nn/common/operations/UnidirectionalSequenceLSTM.cpp#156)?
```


### Standalone code to reproduce the issue

```shell
### To make the dummy TFLite model

def representative_dataset():
""""""Just to make dummy input data for full integer quantization""""""
    for _ in range(100):
        data = np.random.rand(8, 16)
        yield [data.astype(np.float32)]

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense

# make keras model
units = 512
batch_size = 8

model_in = Input(shape=(16,), batch_size=batch_size)
model = Model(model_in, Dense(units, activation=""relu"")(LSTM(736)(Embedding(4001, units,)(model_in))))

# convert to TFLite format
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset
converter._experimental_default_to_single_batch_in_tensor_list_ops = True
tflite_model = converter.convert()
``` 

### To run on device with TFLite benchmark tool
Whether to turn `disable_nnapi_cpu`  to true or false makes no difference.
```
./android_aarch64_benchmark_model --graph=/data/local/model/lstm_w_emb_and_dense_3.8b.tflite --use_nnapi=true --verbose=true
```
```


### Relevant log output

```shell
--------- beginning of main
2023-04-04 17:24:18.427 19036-19036 tflite                  pid-19036                            I  STARTING!
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Log parameter values verbosely: [1]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Min num runs: [50]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Min runs duration (seconds): [1]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Max runs duration (seconds): [150]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Inter-run delay (seconds): [-1]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Number of prorated runs per second: [-1]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Num threads: [-1]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Use caching: [0]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Benchmark name: []
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Output prefix: []
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Min warmup runs: [1]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Min warmup runs duration (seconds): [0.5]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Run w/o invoking kernels: [0]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Report the peak memory footprint: [0]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Memory footprint check interval (ms): [50]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Graph: [/data/local/model/lstm_w_emb_and_dense_3.8b.tflite]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Input layers: []
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Input shapes: []
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Input value ranges: []
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Input value files: []
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Allow fp16: [0]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Require full delegation: [0]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Enable op profiling: [0]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Max initial profiling buffer entries: [1024]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Allow dynamic increase on profiling buffer entries: [0]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  CSV File to export profiling data to: []
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Print pre-invoke interpreter state: [0]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Print post-invoke interpreter state: [0]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Release dynamic tensor memory: [0]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Optimize memory usage for large tensors: [0]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Disable delegate clustering: [0]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  File path to export outputs layer to: []
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  print out all supported flags: [0]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  #threads used for CPU inference: [-1]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Max number of delegated partitions: [0]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Min nodes per partition: [0]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Index of the first node that could be delegated: [0]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Index of the first node that could be delegated: [2147483647]
2023-04-04 17:24:18.428 19036-19036 tflite                  pid-19036                            I  Directory for delegate serialization: []
2023-04-04 17:24:18.429 19036-19036 tflite                  pid-19036                            I  Model-specific token/key for delegate serialization.: []
2023-04-04 17:24:18.429 19036-19036 tflite                  pid-19036                            I  External delegate path: []
2023-04-04 17:24:18.429 19036-19036 tflite                  pid-19036                            I  External delegate options: []
2023-04-04 17:24:18.429 19036-19036 tflite                  pid-19036                            I  Use gpu: [0]
2023-04-04 17:24:18.429 19036-19036 tflite                  pid-19036                            I  Allow lower precision in gpu: [1]
2023-04-04 17:24:18.429 19036-19036 tflite                  pid-19036                            I  Enable running quant models in gpu: [1]
2023-04-04 17:24:18.429 19036-19036 tflite                  pid-19036                            I  Prefer maximizing the throughput in gpu: [0]
2023-04-04 17:24:18.429 19036-19036 tflite                  pid-19036                            I  GPU backend: []
2023-04-04 17:24:18.429 19036-19036 tflite                  pid-19036                            I  Use Hexagon: [0]
2023-04-04 17:24:18.429 19036-19036 tflite                  pid-19036                            I  Hexagon lib path: [/data/local/tmp]
2023-04-04 17:24:18.429 19036-19036 tflite                  pid-19036                            I  Hexagon profiling: [0]
2023-04-04 17:24:18.429 19036-19036 tflite                  pid-19036                            I  Use NNAPI: [1]
2023-04-04 17:24:18.429 19036-19036 tflite                  pid-19036                            I  NNAPI execution preference: []
2023-04-04 17:24:18.429 19036-19036 tflite                  pid-19036                            I  Model execution priority in nnapi: []
2023-04-04 17:24:18.429 19036-19036 tflite                  pid-19036                            I  NNAPI accelerator name: []
2023-04-04 17:24:18.475 19036-19036 Manager                 pid-19036                            I  DeviceManager::DeviceManager
2023-04-04 17:24:18.475 19036-19036 Manager                 pid-19036                            I  findAvailableDevices
2023-04-04 17:24:18.475 19036-19036 ProcessState            pid-19036                            D  Binder ioctl to enable oneway spam detection failed: Invalid argument
2023-04-04 17:24:17.843     0-0     <no-tag>                kernel                               I  c7  19036 binder: 19036:19036 ioctl 40046210 7ff47b1524 returned -22
2023-04-04 17:24:18.476 19036-19036 hw-ProcessState         pid-19036                            D  Binder ioctl to enable oneway spam detection failed: Invalid argument
2023-04-04 17:24:17.844     0-0     <no-tag>                kernel                               I  c6  19036 binder: 19036:19036 ioctl 40046210 7ff47b1484 returned -22
2023-04-04 17:24:18.480 19036-19036 Manager                 pid-19036                            I  Found interface qti-default
2023-04-04 17:24:18.480 19036-19036 Manager                 pid-19036                            I  Found interface qti-dsp
2023-04-04 17:24:18.480 19036-19036 Manager                 pid-19036                            I  Found interface qti-gpu
2023-04-04 17:24:18.480 19036-19036 Manager                 pid-19036                            I  Found interface google-edgetpu
2023-04-04 17:24:18.480 19036-19036 tflite                  pid-19036                            I  NNAPI accelerators available: [qti-default,qti-dsp,qti-gpu,google-edgetpu,nnapi-reference]
2023-04-04 17:24:18.480 19036-19036 tflite                  pid-19036                            I  Disable NNAPI cpu: [0]
2023-04-04 17:24:18.480 19036-19036 tflite                  pid-19036                            I  Allow fp16 in NNAPI: [0]
2023-04-04 17:24:18.480 19036-19036 tflite                  pid-19036                            I  Allow dynamic dimensions in NNAPI: [0]
2023-04-04 17:24:18.480 19036-19036 tflite                  pid-19036                            I  Use burst mode in NNAPI: [0]
2023-04-04 17:24:18.480 19036-19036 tflite                  pid-19036                            I  Use xnnpack: [0]
2023-04-04 17:24:18.480 19036-19036 tflite                  pid-19036                            I  Loaded model /data/local/model/lstm_w_emb_and_dense_3.8b.tflite
2023-04-04 17:24:18.480 19036-19036 tflite                  pid-19036                            I  Initialized TensorFlow Lite runtime.
2023-04-04 17:24:18.481 19036-19036 tflite                  pid-19036                            I  Created TensorFlow Lite delegate for NNAPI.
2023-04-04 17:24:18.481 19036-19036 tflite                  pid-19036                            I  NNAPI delegate created.
2023-04-04 17:24:18.482 19036-19036 tflite                  pid-19036                            I  Replacing 6 node(s) with delegate (TfLiteNnapiDelegate) node, yielding 1 partitions for the whole graph.
2023-04-04 17:24:18.482 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 2 size 2048512
2023-04-04 17:24:18.482 19036-19036 TypeManager             pid-19036                            I  TypeManager::TypeManager
2023-04-04 17:24:18.482 19036-19036 TypeManager             pid-19036                            I  Failed to read /vendor/etc/nnapi_extensions_app_allowlist ; No app allowlisted for vendor extensions use.
2023-04-04 17:24:18.482 19036-19036 TypeManager             pid-19036                            I  NNAPI Vendor extensions enabled: 1
2023-04-04 17:24:18.482 19036-19036 TypeManager             pid-19036                            I  Registered extension com.google.edgetpu_precompiled
2023-04-04 17:24:18.482 19036-19036 ModelBuilder            pid-19036                            I  Saving large value
2023-04-04 17:24:18.482 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 3 size 4
2023-04-04 17:24:18.482 19036-19036 ModelBuilder            pid-19036                            I  Copied small value to offset 0
2023-04-04 17:24:18.482 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 5 size 376832
2023-04-04 17:24:18.482 19036-19036 ModelBuilder            pid-19036                            I  Saving large value
2023-04-04 17:24:18.482 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 6 size 376832
2023-04-04 17:24:18.482 19036-19036 ModelBuilder            pid-19036                            I  Saving large value
2023-04-04 17:24:18.482 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 7 size 376832
2023-04-04 17:24:18.482 19036-19036 ModelBuilder            pid-19036                            I  Saving large value
2023-04-04 17:24:18.482 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 8 size 376832
2023-04-04 17:24:18.482 19036-19036 ModelBuilder            pid-19036                            I  Saving large value
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 9 size 541696
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  Saving large value
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 10 size 541696
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  Saving large value
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 11 size 541696
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  Saving large value
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 12 size 541696
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  Saving large value
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 13 size 0
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 14 size 0
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 15 size 0
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 16 size 2944
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  Saving large value
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 17 size 2944
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  Saving large value
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 18 size 2944
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  Saving large value
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 19 size 2944
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  Saving large value
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 20 size 0
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 21 size 0
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 24 size 4
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  Copied small value to offset 4
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 25 size 4
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  Copied small value to offset 8
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 26 size 4
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  Copied small value to offset 12
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 27 size 1
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  Copied small value to offset 16
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 28 size 0
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 29 size 0
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 30 size 0
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            I  setOperandValue for operand 31 size 0
2023-04-04 17:24:18.483 19036-19036 Operations              pid-19036                            E  NN_RET_CHECK failed (packages/modules/NeuralNetworks/common/operations/UnidirectionalSequenceLSTM.cpp:160): Unsupported input operand type for UNIDIRECTIONAL_SEQUENCE_LSTM op: TENSOR_QUANT8_ASYMM_SIGNED
2023-04-04 17:24:18.483 19036-19036 ModelBuilder            pid-19036                            E  Invalid Operation: NN_RET_CHECK failed (packages/modules/NeuralNetworks/common/operations/UnidirectionalSequenceLSTM.cpp:160): Unsupported input operand type for UNIDIRECTIONAL_SEQUENCE_LSTM op: TENSOR_QUANT8_ASYMM_SIGNED
2023-04-04 17:24:18.483 19036-19036 tflite                  pid-19036                            E  NN API returned error ANEURALNETWORKS_BAD_DATA at line 1131 while adding operation.
2023-04-04 17:24:18.483 19036-19036 tflite                  pid-19036                            E  Node number 6 (TfLiteNnapiDelegate) failed to prepare.
2023-04-04 17:24:18.483 19036-19036 tflite                  pid-19036                            E  Restored original execution plan after delegate application failure.
2023-04-04 17:24:18.484 19036-19036 tflite                  pid-19036                            E  Failed to apply NNAPI delegate.
2023-04-04 17:24:18.484 19036-19036 tflite                  pid-19036                            E  Benchmarking failed.
```
</details>"
60232,TFlite Convert Fails to check for bias type int32 ,"
TFlite converter allows bias type int32; but interpreter fails to allocate tensors. 

1) [Colab to reproduce issue]https://gist.github.com/Naveen-Dodda/2427001bc4b8fb1e05f0a4d84d201e4e): Demonstrate how the model is built and converted to tflite.

### 3. Failure after conversion
 The conversion works fine but Interpreter fails to allocate tensors. I am wondering if this issue can be fixed.

RuntimeError                              Traceback (most recent call last)
<ipython-input-8-514827761d7e> in <cell line: 5>()
      3 signatures = quantizer.get_signature_list()
      4 print(signatures)
----> 5 quantizer.allocate_tensors()

/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/interpreter.py in allocate_tensors(self)
    505   def allocate_tensors(self):
    506     self._ensure_safe()
--> 507     return self._interpreter.AllocateTensors()
    508 
    509   def _safe_to_run(self):

RuntimeError: tensorflow/lite/kernels/depthwise_conv.cc:149 bias->type != kTfLiteInt64 (INT32 != INT64)Node number 1 (DEPTHWISE_CONV_2D) failed to prepare.Failed to apply the default TensorFlow Lite delegate indexed at 0

### 5. (optional) Any other info / logs

https://github.com/tensorflow/tensorflow/blob/v2.12.0/tensorflow/lite/python/lite.py#L479

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/depthwise_conv.cc#L149

"
60229,"Problem on installation document, wrong $LD_LIBRARY_PATH","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

tf v2.12

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
bug:
mkdir -p $CONDA_PREFIX/etc/conda/activate.d
CUDNN_PATH=$(dirname $(python -c ""import nvidia.cudnn;print(nvidia.cudnn.__file__)""))
echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/:$CUDNN_PATH/lib' > $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh

symptom:
error on creating $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh, causing the $LD_LIBRARY_PATH cannot be set properly, using single quotes instead of double quotes

solution:
CUDNN_PATH=$(dirname $(python -c ""import nvidia.cudnn;print(nvidia.cudnn.__file__)""))
echo ""export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/:${CUDNN_PATH}/lib"" > $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh
```


### Standalone code to reproduce the issue

```shell
mkdir -p $CONDA_PREFIX/etc/conda/activate.d
CUDNN_PATH=$(dirname $(python -c ""import nvidia.cudnn;print(nvidia.cudnn.__file__)""))
echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/:$CUDNN_PATH/lib' > $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh
```


### Relevant log output

_No response_</details>"
60228,INVALID_ARGUMENT: You must feed a value for placeholder tensor while creating Dataset iterator,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.12.0

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA Toolkit 11.8, cuDNN 8.6.0

### GPU model and memory

NVIDIA GeForce RTX 3060 Mobile, 6GB

### Current Behaviour?

```shell
I get messages like the one below

2023-04-04 17:05:27.821386: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
         [[{{node Placeholder/_0}}]]
```
while creating dataset iterator. While the code works, such message is annoying when the iterator is created in a loop, as it happens every time.

```python

### Standalone code to reproduce the issue

import tensorflow as tf

def generator():
    while True:
        yield (1, 0.1)

dtypes = (tf.int32, tf.float32)
shapes = ((), ())
dataset = tf.data.Dataset.from_generator(generator, dtypes, output_shapes=shapes)

iter(dataset.take(1))
```


### Relevant log output

```shell
2023-04-04 17:13:32.866627: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-04-04 17:13:32.894884: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-04 17:13:33.371610: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-04-04 17:14:59.902102: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-04-04 17:14:59.920891: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-04-04 17:14:59.921057: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-04-04 17:14:59.922666: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-04-04 17:14:59.922823: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-04-04 17:14:59.922907: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-04-04 17:15:00.284459: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-04-04 17:15:00.284586: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-04-04 17:15:00.284664: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-04-04 17:15:00.284741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4069 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6
2023-04-04 17:15:18.283352: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
         [[{{node Placeholder/_0}}]]
```
</details>"
60223,Null pointer dereference in lmhlo_to_cpu_runtime.cc,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.12

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Variable `dict` may be `nullptr` and is dereferenced on line 149 in `tensorflow/compiler/xla/mlir/backends/cpu/transforms/lmhlo_to_cpu_runtime.cc`.  

`dict` is initialized on line 146 and may equal `nullptr`. Then it is dereferenced on line 149.  
```


### Standalone code to reproduce the issue

```shell
Bug was found by Svace static analysis tool.
```


### Relevant log output

_No response_</details>"
60220,Null pointer dereference in conv_ops_fused_int8.cc,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.12

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Pointer `side_input_ptr` is dereferenced and passed as the first argument into a call to `std::fmaf` in `tensorflow/core/kernels/conv_ops_fused_int8.cc`.  

If we are at 0th iteration in a `for`-loop (line 210) and `side_input_base == nullptr` then `col == 0` and `side_input_ptr` will also equal `nullptr` (line 265). After assign to `side_input_ptr`, this pointer is dereferenced on line 269.
```


### Standalone code to reproduce the issue

```shell
Bug was found by Svace static analysis tool.
```


### Relevant log output

_No response_</details>"
60218,TensorFlow Lite Converter Issue,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
60217,Bazel build issue,"ERROR: Config value 'opt' is not defined in any .rc file
comes out."
60216,TensorFlow 2.12.0 depends on an older Numpy version than TensorFlow 2.11 does,"```bash
pip install tensorflow==2.11.1 numpy==1.24.2
```

works fine (as does `pip install tensorflow==2.11.0 numpy==1.24.2`).

But

```bash
pip install tensorflow==2.12.0 numpy==1.24.2
```

does not:

```
ERROR: Cannot install numpy==1.24.2 and tensorflow==2.12.0 because these package versions have conflicting dependencies.

The conflict is caused by:
    The user requested numpy==1.24.2
    tensorflow 2.12.0 depends on numpy<1.24 and >=1.22

To fix this you could try to:
1. loosen the range of package versions you've specified
2. remove package versions to allow pip attempt to solve the dependency conflict
```

To me, this looks unintended (I did not find anything in the documentation or release notes about it), so I've opened this issue. In case this behavior actually is intended (and not a bug in the dependency declarations), please just close the issue. (A remark in the docs would be cool though.)"
60214,api for model parallelism,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
By using tf.distribute, only the data parallel is documented in the website. https://www.tensorflow.org/guide/distributed_training

Now, what are the APIs that can be used for model parallelism? 

In huggingface, it is documented that, there are many types of parallelism. Read here https://huggingface.co/docs/transformers/v4.15.0/parallelism

What the solutions tensorflow provides for large language model training/inference for consumer level GPU. GPU that has 16GB, 24GB v-ram.

Is it possible to do [Tensor-Parallel](https://huggingface.co/docs/transformers/v4.15.0/parallelism#tensor-parallelism) in Tensorflow.

How model parallism can be applied with APIs in Tensorflow and Keras?
```


### Standalone code to reproduce the issue

```shell
mirrored_strategy = tf.distribute.MirroredStrategy()
cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver()
```


### Relevant log output

_No response_</details>"
60213,Cannot pip install tensorflow examples,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am trying tensorflow segmentation example from [here](https://www.tensorflow.org/tutorials/images/segmentation) on Google Colab and I get an error in the first cell with pip install.
```


### Standalone code to reproduce the issue

```shell
From Jupyter notebook:

`!pip install git+https://github.com/tensorflow/examples.git`
```


### Relevant log output

```shell
Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Collecting git+https://github.com/tensorflow/examples.git
  Cloning https://github.com/tensorflow/examples.git to /tmp/pip-req-build-xe1kper2
  Running command git clone --filter=blob:none --quiet https://github.com/tensorflow/examples.git /tmp/pip-req-build-xe1kper2
  Resolved https://github.com/tensorflow/examples.git to commit 5bc9f1ed519146242db5e71f00d9d39d52a308c8
  error: subprocess-exited-with-error
  
  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─> See above for output.
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  Preparing metadata (setup.py) ... error
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.

Output of GIT Version and TF version:

`import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)`:

v2.12.0-rc1-12-g0db597d0d75 2.12.0
```
</details>"
60212,Keras model with Sparse Input failed to process symbolic Sparse Input after being saved and loaded again,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf 2.4+

### Custom Code

Yes

### OS Platform and Distribution

CentOS 

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
As shown in the test codes, we followed the official Google wiki to construct a Keras model with Sparse Input and validate that it can run infer on SparseTensor.

Then we save this model into SavedModel format and reload it. And we expect the loaded KerasLayer can be stitched into a new Keras model for finetuning.

However, we met with error when trying to run the KerasLayer with symbolic Sparse Input:
TypeError: signature_wrapper(*, args_0_1, args_0_2, args_0) missing required arguments: args_0, args_0_1, args_0_2

This error occurred in all TF versions from 2.4 to 2.11.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import tensorflow_hub as hub

# Try Google official codes to construct a model
# https://www.tensorflow.org/guide/sparse_tensor#tfkeras
x = tf.keras.Input(shape=(1000,), sparse=True)
y = tf.keras.layers.Dense(4)(x)
model = tf.keras.Model(x, y)

sparse_data = tf.sparse.SparseTensor(
    indices = [(0,0),(0,1),(0,2),
               (4,3),(5,0),(5,1)],
    values = [1,1,1,1,1,1],
    dense_shape = (6,1000)
)
print(sparse_data)

print(model.predict(sparse_data))

model_dir = ""./test""
model.save(model_dir)

model_1 = hub.KerasLayer(model_dir, trainable=False, signature=""serving_default"")

inputs = tf.keras.Input(shape=(1000,), sparse=True)
embs = model_1(inputs)
```


### Relevant log output

```shell
2023-04-03 11:56:06.381971: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-04-03 11:56:09.089355: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-04-03 11:56:09.090767: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2023-04-03 11:56:09.100227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1714] Found device 0 with properties: 
pciBusID: 0001:00:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 149.31GiB/s
2023-04-03 11:56:09.100265: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-04-03 11:56:09.106741: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-04-03 11:56:09.106802: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-04-03 11:56:09.110060: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2023-04-03 11:56:09.110473: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2023-04-03 11:56:09.113208: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2023-04-03 11:56:09.114508: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2023-04-03 11:56:09.114656: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /export/apps/xtools/oracle-instantclient
2023-04-03 11:56:09.114678: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1751] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-04-03 11:56:09.115066: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-03 11:56:09.116084: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-04-03 11:56:09.116154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1255] Device interconnect StreamExecutor with strength 1 edge matrix:
2023-04-03 11:56:09.116169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261]      
SparseTensor(indices=tf.Tensor(
[[0 0]
 [0 1]
 [0 2]
 [4 3]
 [5 0]
 [5 1]], shape=(6, 2), dtype=int64), values=tf.Tensor([1 1 1 1 1 1], shape=(6,), dtype=int32), dense_shape=tf.Tensor([   6 1000], shape=(2,), dtype=int64))
2023-04-03 11:56:09.209265: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2023-04-03 11:56:09.209956: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2596985000 Hz
[[-0.01467296 -0.01330906  0.08810526 -0.08027062]
 [ 0.          0.          0.          0.        ]
 [ 0.          0.          0.          0.        ]
 [ 0.          0.          0.          0.        ]
 [-0.04658424  0.04584154 -0.06923811 -0.01754887]
 [-0.09175565 -0.05856525  0.04386244 -0.03397611]]
2023-04-03 11:56:09.322841: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
Traceback (most recent call last):
  File ""test.py"", line 30, in <module>
    embs = video_nhfc_model(inputs)
  File ""/data/src/mmsearch-modeling/build/modeling-pcv2/environments/development-venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 952, in __call__
    input_list)
  File ""/data/src/mmsearch-modeling/build/modeling-pcv2/environments/development-venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1091, in _functional_construction_call
    inputs, input_masks, args, kwargs)
  File ""/data/src/mmsearch-modeling/build/modeling-pcv2/environments/development-venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 822, in _keras_tensor_symbolic_call
    return self._infer_output_signature(inputs, args, kwargs, input_masks)
  File ""/data/src/mmsearch-modeling/build/modeling-pcv2/environments/development-venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 863, in _infer_output_signature
    outputs = call_fn(inputs, *args, **kwargs)
  File ""/data/src/mmsearch-modeling/build/modeling-pcv2/environments/development-venv/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py"", line 670, in wrapper
    raise e.ag_error_metadata.to_exception(e)
TypeError: in user code:

    /data/src/mmsearch-modeling/build/modeling-pcv2/environments/development-venv/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py:237 call  *
        result = smart_cond.smart_cond(training,
    /data/src/mmsearch-modeling/build/modeling-pcv2/environments/development-venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py:1669 __call__  **
        return self._call_impl(args, kwargs)
    /data/src/mmsearch-modeling/build/modeling-pcv2/environments/development-venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py:1685 _call_impl
        raise structured_err
    /data/src/mmsearch-modeling/build/modeling-pcv2/environments/development-venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py:1679 _call_impl
        cancellation_manager)
    /data/src/mmsearch-modeling/build/modeling-pcv2/environments/development-venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py:1756 _call_with_structured_signature
        self._structured_signature_check_missing_args(args, kwargs)
    /data/src/mmsearch-modeling/build/modeling-pcv2/environments/development-venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py:1780 _structured_signature_check_missing_args
        "", "".join(sorted(missing_arguments))))

    TypeError: signature_wrapper(*, args_0_1, args_0_2, args_0) missing required arguments: args_0, args_0_1, args_0_2
```
</details>"
60211,RPC server that batches requests,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.8.0

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Currently the only implemented `GrpcServer`, that implements `Server` does not handle server side batching. Perhaps we would want an additional implementation that supports server side batching?

This could be done similar to how SEED-RL wrote their batching layer (https://github.com/google-research/seed_rl/tree/master/grpc).

`Server`: https://github.com/tensorflow/tensorflow/blob/071ef3748e5fdb67a0438f357f37b962edae83e2/tensorflow/python/distribute/experimental/rpc/rpc_ops.py#L51

`GrpcServer`: https://github.com/tensorflow/tensorflow/blob/071ef3748e5fdb67a0438f357f37b962edae83e2/tensorflow/python/distribute/experimental/rpc/rpc_ops.py#L260
```


### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

_No response_</details>"
60210,Tensorflow Lite library is crashing in WASM library at 3rd inference,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.7.0

### Custom Code

Yes

### OS Platform and Distribution

Emscripten, Ubuntu 18.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hello! I have C++ code that I want to deploy as WASM library and this code contains TFLite library. I have compiled TFLite library with XNNPack support using Emscripten toolchain quite easy, so no issue there. I have a leight-weight convolution+dense model that runs perfectly on Desktop, but I am starting having problems in the browser.

In 99% of cases I have an error on the third inference:

Uncaught RuntimeError: memory access out of bounds

Through some trivial debugging I have found out that the issue comes from _interpreter->Invoke() method. Does not matter if I put any input or not, I just need to call Invoke() three times and I have a crash.

First thing first: I decided to add more memory to my WASM library by adding this line to CMake:

SET(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -s TOTAL_STACK=134217728 -s TOTAL_MEMORY=268435456"")
SET(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -s TOTAL_STACK=134217728 -s TOTAL_MEMORY=268435456"")

128 MB and 256 MB in total for 1 MB model - I think this is more than enough. And on top of that, I am allowing Memory Growth. But unfortunately, I have exactly the same issue.

I am beating on this problem for 2 weeks straight and at this stage I have no clue how to fix it. Also I have tried to set custom allocation using TfLiteCustomAllocation but in this case I have a crash on the very first inference. I guess I was not using it right, but unfortunately I couldn't find even one tutorial describing how to apply custom allocation in TFLite.

I said that I have a crash in 99% of cases. There was one time when WASM library worked and inference worked as well. It happens just randomly once, and I couldn't reproduce it anymore.
```


### Standalone code to reproduce the issue

```shell
Here is the code that does TFLite inference


#include <cstdlib>
#include ""tflite_model.h""
#include <iostream>

#include ""tensorflow/lite/interpreter.h""
#include ""tensorflow/lite/util.h""

namespace tracker {

#ifdef EMSCRIPTEN
	void TFLiteModel::init(std::stringstream& stream) {

		std::string img_str = stream.str();
		std::vector<char> img_model_data(img_str.size());
		std::copy(img_str.begin(), img_str.end(), img_model_data.begin());

		_model = tflite::FlatBufferModel::BuildFromBuffer(img_str.data(), img_str.size());
#else
	void TFLiteModel::init(const std::string& path) {
		_model = tflite::FlatBufferModel::BuildFromFile(path.c_str());

#endif

		tflite::ops::builtin::BuiltinOpResolver resolver;
		tflite::InterpreterBuilder(*_model, resolver)(&_interpreter);

		_interpreter->AllocateTensors();

		/*for (int i = 0; i < _interpreter->tensors_size(); i++) {
			TfLiteTensor* tensor = _interpreter->tensor(i);

			if (tensor->allocation_type == kTfLiteArenaRw || tensor->allocation_type == kTfLiteArenaRwPersistent) {

				int aligned_bytes = tensor->bytes + (tflite::kDefaultTensorAlignment - tensor->bytes % tflite::kDefaultTensorAlignment) % tflite::kDefaultTensorAlignment;

				TfLiteCustomAllocation customAlloc;
				int result = posix_memalign(&customAlloc.data, tflite::kDefaultTensorAlignment, tensor->bytes);
				if (result != 0 || customAlloc.data == NULL) {
					std::cout << ""posix_memalign does not work!\n"";
				}

				TfLiteStatus st = _interpreter->SetCustomAllocationForTensor(i, customAlloc);
				std::cout << ""status = "" << st << std::endl;
				if (tensor->bytes % tflite::kDefaultTensorAlignment != 0) {
					std::cout << ""bad! i "" << i << "", size "" << tensor->bytes << std::endl;
				}
				_allocations.push_back(customAlloc);
			}
		}
		exit(0);*/
	}

	void TFLiteModel::forward(const cv::Mat& img_input, const std::vector<float>& lms_input) {

		float* model_in = _interpreter->typed_input_tensor<float>(0);
		std::memcpy(model_in, img_input.data, img_input.total() * img_input.elemSize());

		float* lms_in = _interpreter->typed_input_tensor<float>(1);
		std::memcpy(lms_in, lms_input.data(), sizeof(float) * lms_input.size());
		
		_interpreter->Invoke();
	}

	float* TFLiteModel::out() {
		return _interpreter->typed_output_tensor<float>(0);
	}

	std::vector<int> TFLiteModel::getOutputShape() const {
		TfLiteTensor* outtensor = _interpreter->output_tensor(0);
		TfLiteIntArray* dims = outtensor->dims;

		std::vector<int> sh;
		for (int i = 0; i < dims->size; i++) {
			sh.push_back(dims->data[i]);
		}

		return sh;
	}
}
```


### Relevant log output

_No response_</details>"
60209,Python3.11 arm64 wheels for tensorflow-macos,"### Current Behaviour?


For the package tensorflow 2.12 we have [precompiled wheels for python3.11](https://pypi.org/project/tensorflow/#files) but [not for the package tensorflow-macos](https://pypi.org/project/tensorflow-macos/#files). This means that we can use TF 2.12 + Python3.11 on macOS x86 (Intel mac) but not on arm64 (M1 mac)

-  ✔️  `TF 2.12 Python3.10 macOS arm64` (package [tensorflow-macos](https://pypi.org/project/tensorflow-macos/#files))
`tensorflow_macos-2.12.0-cp310-cp310-macosx_12_0_arm64.whl`
-  ✔️  `TF 2.12 Python3.11 macOS x86` (package [tensorflow](https://pypi.org/project/tensorflow/#files))
`tensorflow-2.12.0-cp311-cp311-macosx_10_15_x86_64.whl`
- ❌ `TF 2.12 Python3.11 macOS arm64` (package [tensorflow-macos](https://pypi.org/project/tensorflow-macos/#files))

### Standalone code to reproduce the issue

```shell
$ uname -m
arm64

$ python -V
Python 3.11.0

$ python3.11 -m pip install tensorflow-macos==2.12.0
ERROR: Could not find a version that satisfies the requirement tensorflow-macos==2.12.0 (from versions: none)
ERROR: No matching distribution found for tensorflow-macos==2.12.0
```"
60208,"XNNPACK/BUILD.bazel:4762:26: configurable attribute ""deps"" in @XNNPACK//:amalgam_microkernels doesn't match this configuration. Would a default condition help?","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.13.0

### Custom Code

No

### OS Platform and Distribution

macMonterey 12.4

### Mobile device

_No response_

### Python version

3.11.2

### Bazel version

5.3.0

### GCC/Compiler version

N

### CUDA/cuDNN version

N

### GPU model and memory

N

### Current Behaviour?

```shell
I try to build iOS framework that support armv7 only on M1 MacBook with script

bazel build   --ios_minimum_os='10.0' --config=ios_armv7 -c opt --cxxopt=--std=c++17  //tensorflow/lite/ios:TensorFlowLiteC_framework
external/XNNPACK/BUILD.bazel:4762:26: configurable attribute ""deps"" in @XNNPACK//:amalgam_microkernels doesn't match this configuration. Would a default condition help?

Conditions checked:
 @XNNPACK//:aarch32
 @XNNPACK//:aarch64
 @XNNPACK//:x86
 @XNNPACK//:emscripten_wasm
 @XNNPACK//:emscripten_wasmsimd
 @XNNPACK//:emscripten_wasmrelaxedsimd
 @XNNPACK//:riscv

To see a condition's definition, run: bazel query --output=build <condition label>.

This instance of @XNNPACK//:amalgam_microkernels has configuration identifier 4cdd8f2. To inspect its configuration, run: bazel config 4cdd8f2.

For more help, see https://docs.bazel.build/configurable-attributes.html#why-doesnt-my-select-choose-what-i-expect.

ERROR: Analysis of target '//tensorflow/lite/ios:TensorFlowLiteC_framework' failed; build aborted: 
INFO: Elapsed time: 0.142s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (1 packages loaded, 0 targets configured)

```


### Standalone code to reproduce the issue

```shell
I try to build iOS framework that support armv7 only on M1 MacBook with script

bazel build   --ios_minimum_os='10.0' --config=ios_armv7 -c opt --cxxopt=--std=c++17  //tensorflow/lite/ios:TensorFlowLiteC_framework
```


### Relevant log output

```shell
-
```
</details>"
60207,Writing custom ReduceOp,"I would like take a MirroredVariable, shape=(32,60),  with 3 replica and reduce is first as a difference of pairs and than L2 norm it. 

Is there a way of writing such a custom ReduceOp? 

https://github.com/tensorflow/tensorflow/blob/0db597d0d758aba578783b5bf46c889700a45085/tensorflow/python/distribute/reduce_util.py#L23-L47"
60206,Support for CUDA 12 for newer GPUS.,"Hello,
I have an rrx 3060, and I have Cuda 12.1 installed along with Cudnn.
I can't get Tensorflow to detect my gpu in Python.
As I have read in the docs you must have Cuda 11.8, but I am using another Nvidia app that requires CUDA 12, and I am afraid if I downgrade the CUDA version, I might encounter issues.
Please can someone help or guide me to another methkd of using Tensorflow locally along with my gpu.
I have Windows 11."
60205,Tensorflow import error in virtual environment (vsc) - No module named 'tensorflow',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

python 3.7.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory 
HINT: This error might have occurred since this system does not have Windows Long Path support enabled. You can find information on how to enable this at https://pip.pypa.io/warnings/enable-long-paths
```


### Standalone code to reproduce the issue

```shell
Code : 

import cv2
import numpy as np
import tensorflow as tf

model = tf.keras.models.load_model('keras_model.h5')

video = cv2.VideoCapture(0)

while True:

    check,frame = video.read()

    img = cv2.resize(frame,(224,224))

    test_image = np.array(img, dtype=np.float32)
    test_image = np.expand_dims(test_image, axis=0)

    # 3. Normalizing the image
    normalised_image = test_image/255.0

    # Predict Result
    prediction = model.predict(normalised_image)

    print(""Prediction : "", prediction)
        
    cv2.imshow(""Result"",frame)
            
    key = cv2.waitKey(1)

    if key == 32:
        print(""Closing"")
        break

video.release()


This is the location of the tensorflow package on my C drive : 

C:\Users\Asus\Downloads\PRO-C110-Student-Boilerplate-main (1)\PRO-C110-Student-Boilerplate-main\venv\Scripts\tensorboard.exe


Method of installing tensorflow : 

pip install --ignore-installed --upgrade tensorflow
I did try: uninstalling and reinstalling protobuf but it makes no difference, neither does pip3
```


### Relevant log output

_No response_</details>"
60204,pip install tensorflow==2.11.1 not working on AWS arm64 ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.1

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When I run `pip install tensorflow==2.11.1` on an aws m6g.large instance (arm64 graviton) it throws the following error:

ERROR: Could not find a version that satisfies the requirement tensorflow-cpu-aws==2.11.1; platform_system == ""Linux"" and (platform_machine == ""arm64"" or platform_machine == ""aarch64"") (from tensorflow) (from versions: 2.9.1, 2.10.0rc0, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.12.0rc1, 2.12.0)
ERROR: No matching distribution found for tensorflow-cpu-aws==2.11.1; platform_system == ""Linux"" and (platform_machine == ""arm64"" or platform_machine == ""aarch64"")

I can see this is the case by navigating here: https://pypi.org/project/tensorflow-cpu-aws/2.11.1/#files

for some reason it was only built for python 3.7 and is lacking binaries for all other python versions. tensorflow 2.11.0 and 2.12.0 both work but we wanted to use 2.11.1 in this case, perhaps an error during that particular build?

For example compare to this: https://pypi.org/project/tensorflow-cpu-aws/2.11.0/#files or the 2.12.0
```


### Standalone code to reproduce the issue

```shell
`pip install tensorflow==2.11.1` on python 3.10 on an aws m6g instance
```


### Relevant log output

_No response_</details>"
60203,Detected unsupported operations when trying to compile graph XLA_GPU_JIT: ResizeNearestNeighborGrad,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.12

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When defining blocks like the following:


self.conv_64 = tf.keras.Sequential([
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())
    model.add(layers.UpSampling2D(2))
])

when training with @tf.function(jit_compile=True) got this error:

Detected unsupported operations when trying to compile graph __inference_train_step_8942[_XlaMustCompile=true,config_proto=6001324581131673121,executor_type=11160318154034397263] on XLA_GPU_JIT: ResizeNearestNeighborGrad (No registered 'ResizeNearestNeighborGrad' OpKernel for XLA_GPU_JIT devices compatible with node {{node gradient_tape/sequential_4/up_sampling2d_2/resize/ResizeNearestNeighborGrad}}){{node gradient_tape/sequential_4/up_sampling2d_2/resize/ResizeNearestNeighborGrad}}
```


### Standalone code to reproduce the issue

```shell
Use the DCGAN code https://www.tensorflow.org/tutorials/generative/dcgan

Use generator with UpSampling:

def make_generator_model():
    model = tf.keras.Sequential()
    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Reshape((7, 7, 256)))
    assert model.output_shape == (None, 7, 7, 256)  # Note: None is the batch size

    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))
    assert model.output_shape == (None, 7, 7, 128)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.UpSampling2D(2))

    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))
    assert model.output_shape == (None, 28, 28, 1)

    return model


and train with jit=True:

# Notice the use of `tf.function`
# This annotation causes the function to be ""compiled"".
@tf.function(jit_compile=True)
def train_step(images):
    noise = tf.random.normal([BATCH_SIZE, noise_dim])

    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
      generated_images = generator(noise, training=True)

      real_output = discriminator(images, training=True)
      fake_output = discriminator(generated_images, training=True)

      gen_loss = generator_loss(fake_output)
      disc_loss = discriminator_loss(real_output, fake_output)

    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))
```
This is a shame because it limits the potential of XLA in GANs

### Relevant log output


```
</details>"
60202,[tensorflow-io-gcs-filesystem] Missing p39 win32 package for 0.32.0,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.1

### Custom Code

No

### OS Platform and Distribution

Windows

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The PyPI server is currently lacking a win32 wheel for python 3.9 for sub-package tensorflow-io-gcs-filesystem.
```


### Standalone code to reproduce the issue

```shell
$ conda create -n tf39 python=3.9
$ conda activate tf39
$ pip install tensorflow-io-gcs-filesystem==0.32.0
```


### Relevant log output

```shell
ERROR: Could not find a version that satisfies the requirement tensorflow-io-gcs-filesystem==0.32.0 (from versions: 0.23.1, 0.24.0, 0.25.0, 0.26.0, 0.27.0, 0.28.0, 0.29.0, 0.30.0, 0.31.0)
ERROR: No matching distribution found for tensorflow-io-gcs-filesystem==0.32.0
```
</details>"
60201,A check fail can be triggered in DepthwiseConv2dNativeBackpropInput,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.13.0-dev20230331

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.8

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The following code can trigger a crash in `tf.raw_ops.DepthwiseConv2dNativeBackpropInput` due to check-fail in the latest version of TensorFlow.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
with tf.device(""CPU""):
    strides = [1, 0, 1, 1]
    padding = ""VALID""
    explicit_paddings = []
    data_format = ""NHWC""
    dilations = [1, 0, 77, 1, 64]
    input_sizes = tf.saturate_cast(tf.random.uniform([3], minval=-1024, maxval=1024, dtype=tf.int64), dtype=tf.int32)
    filter = tf.random.uniform([16, 3, 3, 5], dtype=tf.bfloat16, minval=-1024, maxval=1024)
    out_backprop = tf.random.uniform([1, 0, 0, 1], dtype=tf.bfloat16, minval=-1024, maxval=1024)
    res = tf.raw_ops.DepthwiseConv2dNativeBackpropInput(
        strides=strides,
        padding=padding,
        explicit_paddings=explicit_paddings,
        data_format=data_format,
        dilations=dilations,
        input_sizes=input_sizes,
        filter=filter,
        out_backprop=out_backprop,
    )
```


### Relevant log output

```shell
2023-04-01 16:37:53.415934: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-04-01 16:37:53.465275: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-01 16:37:54.257156: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-04-01 16:37:55.809665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14561 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:2f:00.0, compute capability: 7.0
2023-04-01 16:37:55.854746: F tensorflow/core/kernels/mkl/mkl_conv_grad_input_ops.cc:537] Non-OK-status: tensor::MakeShape(input_tensor, &input_tf_shape) status: INVALID_ARGUMENT: Dimension -510 must be >= 0
Aborted (core dumped)
```
</details>"
60200,A check fail can be triggered in ThreadUnsafeUnigramCandidateSampler,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.13.0-dev20230331

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.8

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The following code can trigger a crash in `tf.raw_ops.ThreadUnsafeUnigramCandidateSampler` due to check-fail in the latest version of TensorFlow.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
with tf.device(""GPU:0""):
    num_true = 11
    num_sampled = 2
    unique = False
    range_max = 7612169259283414040
    seed = -111
    seed2 = -11
    true_classes = tf.saturate_cast(tf.random.uniform([12, 11], minval=-1024, maxval=1024, dtype=tf.int64), dtype=tf.int64)
    res = tf.raw_ops.ThreadUnsafeUnigramCandidateSampler(
        num_true=num_true,
        num_sampled=num_sampled,
        unique=unique,
        range_max=range_max,
        seed=seed,
        seed2=seed2,
        true_classes=true_classes,
    )
```


### Relevant log output

```shell
2023-04-01 16:33:33.009606: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-04-01 16:33:33.057487: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-01 16:33:33.874853: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-04-01 16:33:35.397082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14561 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:2f:00.0, compute capability: 7.0
2023-04-01 16:33:40.359234: F tensorflow/core/kernels/range_sampler.cc:183] Check failed: range < kint32max (7612169259283414040 vs. 2147483647)
Aborted (core dumped)
```
</details>"
60199,A check fail can be triggered in MatrixDeterminant,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.13.0-dev20230331

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.8

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The following code can trigger a crash in `tf.raw_ops.MatrixDeterminant` due to check-fail in the latest version of TensorFlow.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
with tf.device(""CPU""):
    input = tf.complex(tf.random.uniform([0, 1, 7678600331551628182, 15], dtype=tf.float32, minval=-18446744073709551615, maxval=18446744073709551615),tf.random.uniform([0, 1, 7678600331551628182, 15], dtype=tf.float32, minval=-18446744073709551615, maxval=18446744073709551615))
    res = tf.raw_ops.MatrixDeterminant(
        input=input,
    )
```


### Relevant log output

```shell
2023-04-01 16:27:43.185649: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-04-01 16:27:43.233461: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-01 16:27:44.034698: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-04-01 16:27:45.582999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14561 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:2f:00.0, compute capability: 7.0
2023-04-01 16:27:45.620825: F tensorflow/core/framework/tensor_shape.cc:201] Non-OK-status: InitDims(dim_sizes) status: INVALID_ARGUMENT: Encountered overflow when multiplying 7678600331551628182 with 15, result: -1
Aborted (core dumped)
```
</details>"
60198,A check fail can be triggered in TridiagonalSolve,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.13.0-dev20230331

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.8

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The following code can trigger a crash in `tf.raw_ops.TridiagonalSolve` due to check-fail in the latest version of TensorFlow.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
with tf.device(""GPU:0""):
    partial_pivoting = True
    perturb_singular = False
    diagonals = tf.complex(tf.random.uniform([2, 13, 0, 13, 857005098598382018], dtype=tf.float32, minval=-1024, maxval=1024),tf.random.uniform([2, 13, 0, 13, 857005098598382018], dtype=tf.float32, minval=-1024, maxval=1024))
    rhs = tf.complex(tf.random.uniform([15, 15, 12, 1, 10, 16], dtype=tf.float32, minval=-1024, maxval=1024),tf.random.uniform([15, 15, 12, 1, 10, 16], dtype=tf.float32, minval=-1024, maxval=1024))
    res = tf.raw_ops.TridiagonalSolve(
        partial_pivoting=partial_pivoting,
        perturb_singular=perturb_singular,
        diagonals=diagonals,
        rhs=rhs,
    )
```


### Relevant log output

```shell
2023-04-01 16:22:38.864472: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-04-01 16:22:38.915491: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-01 16:22:39.745143: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-04-01 16:22:41.321428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14561 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:2f:00.0, compute capability: 7.0
2023-04-01 16:22:41.585403: F tensorflow/core/framework/tensor_shape.cc:201] Non-OK-status: InitDims(dim_sizes) status: INVALID_ARGUMENT: Encountered overflow when multiplying 13 with 857005098598382018, result: -7305677791930585382
Aborted (core dumped)
```
</details>"
60197,A check fail can be triggered in LearnedUnigramCandidateSampler,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.13.0-dev20230331

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.8

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The following code can trigger a crash in `tf.raw_ops.LearnedUnigramCandidateSampler` due to check-fail in the latest version of TensorFlow.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
with tf.device(""GPU:0""):
    num_true = 13
    num_sampled = 48
    unique = True
    range_max = 3031324185113192368
    seed = 93
    seed2 = 11
    true_classes = tf.saturate_cast(tf.random.uniform([14, 13], minval=-1024, maxval=1024, dtype=tf.int64), dtype=tf.int64)
    res = tf.raw_ops.LearnedUnigramCandidateSampler(
        num_true=num_true,
        num_sampled=num_sampled,
        unique=unique,
        range_max=range_max,
        seed=seed,
        seed2=seed2,
        true_classes=true_classes,
    )
```


### Relevant log output

```shell
2023-04-01 16:20:32.160750: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-04-01 16:20:32.211959: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-01 16:20:33.026789: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-04-01 16:20:34.550122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14561 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:2f:00.0, compute capability: 7.0
2023-04-01 16:20:39.689581: F tensorflow/core/kernels/range_sampler.cc:183] Check failed: range < kint32max (3031324185113192368 vs. 2147483647)
Aborted (core dumped)
```
</details>"
60196,A check fail can be triggered in StatelessRandomGammaV2,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.13.0-dev20230331

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.8

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The following code can trigger a crash in `tf.raw_ops.StatelessRandomGammaV2` due to check-fail in the latest version of TensorFlow.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
with tf.device(""GPU:0""):
    shape = [94, 47, 76, 127, 90]
    seed = tf.saturate_cast(tf.random.uniform([2], minval=-1024, maxval=1024, dtype=tf.int64), dtype=tf.int32)
    alpha = tf.saturate_cast(tf.random.uniform([], minval=-1024, maxval=1024, dtype=tf.int64), dtype=tf.half)
    res = tf.raw_ops.StatelessRandomGammaV2(
        shape=shape,
        seed=seed,
        alpha=alpha,
    )
```


### Relevant log output

```shell
2023-04-01 16:17:25.774398: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-04-01 16:17:25.823488: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-01 16:17:26.620798: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-04-01 16:17:28.156561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14561 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:2f:00.0, compute capability: 7.0
2023-04-01 16:17:28.418600: F ./tensorflow/core/util/gpu_launch_config.h:129] Check failed: work_element_count >= 0 (0 vs. -457139056)
Aborted (core dumped)
```
</details>"
60195,A check fail can be triggered in Conv3DBackpropFilterV2,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.13.0-dev20230331

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.5

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The following code can trigger a crash in `tf.raw_ops.Conv3DBackpropFilterV2` due to check-fail in the latest version of TensorFlow.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
with tf.device(""CPU""):
    strides = [1, 1, 1, 1, 1]
    padding = ""VALID""
    data_format = ""NCDHW""
    dilations = [1, 1, 1, 1, 1]
    input = tf.random.uniform([1], dtype=tf.bfloat16, minval=-1024, maxval=1024)
    filter_sizes = tf.saturate_cast(tf.random.uniform([1], minval=-1024, maxval=1024, dtype=tf.int64), dtype=tf.int32)
    out_backprop = tf.random.uniform([2, 12, 3, 5, 10], dtype=tf.bfloat16, minval=-1024, maxval=1024)
    res = tf.raw_ops.Conv3DBackpropFilterV2(
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilations=dilations,
        input=input,
        filter_sizes=filter_sizes,
        out_backprop=out_backprop,
    )
```


### Relevant log output

```shell
2023-04-01 15:05:13.595051: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-04-01 15:05:13.597635: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2023-04-01 15:05:13.640719: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2023-04-01 15:05:13.641231: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-01 15:05:14.394924: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2.13.0-dev20230331
2023-04-01 15:05:15.220833: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-04-01 15:05:15.267117: F ./tensorflow/core/util/tensor_format.h:427] Check failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 1, 1, C
Aborted (core dumped)
```
</details>"
60194,Issue OS,desc:
60192,"Masked LSTM and GRU using cuDNN 8.1+ give randomly corrupted results and crashes, even during inference.","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

TF 2.12, TF 2.11, TF nightly, TF 2.8

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04, Colab

### Mobile device

_No response_

### Python version

3.9, 3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

cuDNN 8.1 or newer

### GPU model and memory

I have observed the issue on GeForce 1080 Ti, GeForce 3090 RTX, A40, T4 from Collab.

### Current Behaviour?

When LSTM or GRU with mask use cuDNN 8.1+ implementation, it randomly give corrupted results (even during inference), and sometimes even crash with error `CUDA_ERROR_ILLEGAL_ADDRESS`.

It has taken me quite some time to be able to reproduce the problem, but I have found it (it manifests reliably on Colab).

Important comments:

- Of course, the problem manifests only when a GPU is available (the CPU implementation is fine).

- The bug manifests only with cuDNN 8.1+, because older cuDNN use different RNN methods. I even recompiled TF 2.11 and TF 2.8 with CUDA 11.1 and cuDNN 8.0, and they work fine.

- The bug manifests only when a `mask` is passed to the GRU and LSTM (the masked RNN calls also use a specific code path).

- The problem is not data-dependent, the same batch sometimes does and sometimes does not trigger the bug (I use the same batch in the example below).

- Sometimes the RNN call end with a `CUDA_ERROR_ILLEGAL_ADDRESS` and crash the program.

- Different GPU models differ in how frequently the corruption happens -- cards with CC 6.1 seem to trigger it more often; cards with CC 8.6 seem to trigger it less, but they still do

- When the dimensionalities of the cells are larger, the problem manifests more often.

- Note that before the recent Colab update, it used cuDNN 8.0.[56] for a very long time (I assume specific Colab packages were being build), so the bug was not manifesting there; but now it does.

My initial guess is that the different RNN calls somehow share memory and sometimes overwrite it (and maybe sometimes is the memory freed from one place and being accessed from the other place, causing the crash).

### Standalone code to reproduce the issue

The Colab notebook showing the problem with TF 2.12.0: https://colab.research.google.com/drive/17a4AcbGf9CyCl4de_vPEbB3QlTwxlV1b?usp=sharing

The Colab notebook showing the problem with TF nightly https://colab.research.google.com/drive/1ONQ7EBF9iLkSmmJE3yb04nSNbhW9fnlV?usp=sharing

The code triggering the problem:

```python
import numpy as np
import tensorflow as tf
print(tf.__version__)

def create_model(use_mask: bool) -> tf.keras.Model:
    inputs = tf.keras.layers.Input(shape=[None], dtype=tf.int32)
    if use_mask:
        mask = inputs >= 0
    else:
        mask = None
    h = tf.keras.layers.Embedding(64, 2048)(tf.math.maximum(inputs, 0))
    h1 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(2048, return_sequences=True), merge_mode=""sum"")(h, mask=mask)
    h2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(2048, return_sequences=True), merge_mode=""sum"")(h, mask=mask)
    h = tf.keras.layers.Dense(10)(h1 + h2)
    return tf.keras.Model(inputs, h)

# Data
data = tf.data.Dataset.from_tensor_slices([[j if j <= i else -1 for j in range(64)] for i in range(64)]).batch(64)

# However, when masking is used, even prediction on GPU gives different result.
# It also sometimes crases with the error `CUDA_ERROR_ILLEGAL_ADDRESS`.
# The full error log is copied below.
# If `use_mask=False` is passed, no problem happens.

# Models
tf.keras.utils.set_random_seed(42)
model = create_model(use_mask=True)

# Run prediction
gold = None
for i in range(100):
    result = model.predict(data, verbose=0)
    if gold is None:
        gold = result
    print(""Batch {}, max difference {}, mean difference {}"".format(i, np.max(np.abs(gold - result)), np.mean(np.abs(gold - result))))
```


### Relevant log output

```shell
Batch 0, max difference 0.0, mean difference 0.0
Batch 1, max difference 0.0, mean difference 0.0
Batch 2, max difference 0.0, mean difference 0.0
Batch 3, max difference 0.0, mean difference 0.0
Batch 4, max difference 0.0, mean difference 0.0
Batch 5, max difference 0.0, mean difference 0.0
Batch 6, max difference 1.6253925561904907, mean difference 0.032524894922971725
Batch 7, max difference 1.5961412191390991, mean difference 0.034154243767261505
Batch 8, max difference 0.0, mean difference 0.0
Batch 9, max difference 0.13439376652240753, mean difference 0.009293164126574993
Batch 10, max difference 0.0, mean difference 0.0
Batch 11, max difference 0.0, mean difference 0.0
Batch 12, max difference 0.0, mean difference 0.0
Batch 13, max difference 0.0, mean difference 0.0
Batch 14, max difference 0.0, mean difference 0.0
Batch 15, max difference 0.0, mean difference 0.0
Batch 16, max difference 0.0, mean difference 0.0
Batch 17, max difference 0.0, mean difference 0.0
Batch 18, max difference 0.0, mean difference 0.0
Batch 19, max difference 0.0, mean difference 0.0
Batch 20, max difference 0.0900668054819107, mean difference 0.006251291837543249
Batch 21, max difference 1.516040325164795, mean difference 0.03404051065444946
Batch 22, max difference 0.0, mean difference 0.0
Batch 23, max difference 0.09510315954685211, mean difference 0.004909028299152851
Batch 24, max difference 0.0, mean difference 0.0
Batch 25, max difference 0.0, mean difference 0.0
Batch 26, max difference 0.0, mean difference 0.0
Batch 27, max difference 0.10816850513219833, mean difference 0.006311381701380014
Batch 28, max difference 0.06991208344697952, mean difference 0.004763560835272074
Batch 29, max difference 0.0, mean difference 0.0
Batch 30, max difference 0.0, mean difference 0.0
Batch 31, max difference 0.12369412928819656, mean difference 0.0054976968094706535
Batch 32, max difference 1.114426612854004, mean difference 0.01574256643652916
Batch 33, max difference 1.1078299283981323, mean difference 0.01561223715543747
Batch 34, max difference 0.0, mean difference 0.0
Batch 35, max difference 0.07631748914718628, mean difference 0.004826296120882034
Batch 36, max difference 0.10820074379444122, mean difference 0.006503588054329157
Batch 37, max difference 0.09048224240541458, mean difference 0.006238402798771858
Batch 38, max difference 0.0, mean difference 0.0
Batch 39, max difference 0.0, mean difference 0.0
Batch 40, max difference 0.1100485697388649, mean difference 0.014095092192292213
Batch 41, max difference 0.007873136550188065, mean difference 0.0007940558716654778
Batch 42, max difference 0.007873136550188065, mean difference 0.0007940558716654778
Batch 43, max difference 0.007873136550188065, mean difference 0.0007940558716654778
Batch 44, max difference 0.06849975883960724, mean difference 0.004877315368503332
Batch 45, max difference 0.007873136550188065, mean difference 0.0007940558716654778
Batch 46, max difference 0.007873136550188065, mean difference 0.0007940558716654778
Batch 47, max difference 0.09048224240541458, mean difference 0.006395397242158651
Batch 48, max difference 0.12369412928819656, mean difference 0.005698652006685734
Batch 49, max difference 0.007873136550188065, mean difference 0.0007940558716654778
Batch 50, max difference 0.007873136550188065, mean difference 0.0007940558716654778
Batch 51, max difference 1.1203747987747192, mean difference 0.016059130430221558
Batch 52, max difference 0.007873136550188065, mean difference 0.0007940558716654778
Batch 53, max difference 0.007873136550188065, mean difference 0.0007940558716654778
Batch 54, max difference 0.007873136550188065, mean difference 0.0007940558716654778
Batch 55, max difference 0.007873136550188065, mean difference 0.0007940558716654778
Batch 56, max difference 0.07624031603336334, mean difference 0.0050272345542907715
Batch 57, max difference 1.6275964975357056, mean difference 0.032646626234054565
Batch 58, max difference 0.043441060930490494, mean difference 0.0046346113085746765
Batch 59, max difference 0.007873136550188065, mean difference 0.0007940558716654778
Batch 60, max difference 0.08132395148277283, mean difference 0.0063618021085858345
```

Then the following error appeared and crashed the program:

```shell
Mar 31, 2023, 10:30:50 PM	WARNING	2023-03-31 20:30:50.700469: E tensorflow/compiler/xla/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Mar 31, 2023, 10:30:50 PM	WARNING	2023-03-31 20:30:50.700546: F tensorflow/core/common_runtime/device/device_event_mgr.cc:223] Unexpected Event status: 1
```
</details>"
60191,Fail to compile TF 2.12.0 with XCode 14.3 due to Compiler flag in boringssl/src/crypto/x509,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

No

### OS Platform and Distribution

macOS 13.3

### Mobile device

_No response_

### Python version

3.10

### Bazel version

5.3.0

### GCC/Compiler version

XCode 14.3

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Using standard compiling procedure (no special flags), compilation of the external library: boringssl/src/crypto/x509 fails. Log attached below.
```


### Standalone code to reproduce the issue

```shell
Compile TF 2.12.0 using MacOS 13.x and XCode 14.3 (not earlier).
```


### Relevant log output

```shell
: /private/var/tmp/_bazel_alex/dc1a9368c8e4ba5b96348c2850b37ab0/external/boringssl/BUILD:161:11: Compiling src/crypto/x509/t_x509.c [for host] failed: (Exit 1): cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -Wunused-but-set-parameter -Wno-free-nonheap-object -fcolor-diagnostics ... (remaining 44 arguments skipped)
external/boringssl/src/crypto/x509/t_x509.c:321:18: error: variable 'l' set but not used [-Werror,-Wunused-but-set-variable]
    int ret = 0, l, i;
                 ^
1 error generated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```
</details>"
60188,Tflite benchmark app build issues,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.8

### Custom Code

Yes

### OS Platform and Distribution

macos

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Trying to build the android benchmark app from source using bazel

I did the configuration setup as follow:

➜  tensorflow git:(master) ✗ ./configure                                                                                                         
You have bazel 5.3.0 installed.
Please specify the location of python. [Default is /Library/Frameworks/Python.framework/Versions/3.8/bin/python3]: /Library/Frameworks/Python.framework/Versions/3.8/bin/python3


Found possible Python library paths:
  /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages
Please input the desired Python library path to use.  Default is [/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: n
No CUDA support will be enabled for TensorFlow.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: n
Clang will not be downloaded.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: y
Searching for NDK and SDK installations.

Please specify the home path of the Android NDK to use. [Default is /library/Android/Sdk/ndk-bundle]: /Library/Android/sdk/ndk/21.4.7075529


Please specify the (min) Android NDK API level to use. [Available levels: ['16', '17', '18', '19', '21', '22', '23', '24', '26', '27', '28', '29', '30']] [Default is 26]: 21


Please specify the home path of the Android SDK to use. [Default is /library/Android/Sdk]: /library/Android/Sdk


Please specify the Android SDK API level to use. [Available levels: ['23', '28', '33', '33-ext5']] [Default is 33-ext5]: 28


Please specify an Android build tools version to use. [Available versions: ['28.0.3', '30.0.0', '33.0.2']] [Default is 33.0.2]: 30.0.0


Do you wish to build TensorFlow with iOS support? [y/N]: n
No iOS support will be enabled for TensorFlow.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
	--config=v1          	# Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=nogcp       	# Disable GCP support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished
```


### Standalone code to reproduce the issue

```shell
bazel build -c opt --config=android_arm64 tensorflow/lite/tools/benchmark/android:benchmark_model --verbose_failures
```


### Relevant log output

```shell
# Execution platform: @local_execution_config_platform//:platform
tensorflow/lite/delegates/gpu/delegate.cc:796:7: error: use of undeclared identifier 'AHardwareBuffer_acquire'; did you mean 'AHardwareBuffer_Plane'?
      AHardwareBuffer_acquire(ahwb);
      ^
external/androidndk/ndk/sysroot/usr/include/android/hardware_buffer.h:320:3: note: 'AHardwareBuffer_Plane' declared here
} AHardwareBuffer_Plane;
  ^
tensorflow/lite/delegates/gpu/delegate.cc:804:9: error: use of undeclared identifier 'AHardwareBuffer_release'; did you mean 'AHardwareBuffer_Plane'?
        AHardwareBuffer_release(b);
        ^
external/androidndk/ndk/sysroot/usr/include/android/hardware_buffer.h:320:3: note: 'AHardwareBuffer_Plane' declared here
} AHardwareBuffer_Plane;
  ^
tensorflow/lite/delegates/gpu/delegate.cc:812:7: error: use of undeclared identifier 'AHardwareBuffer_describe'
      AHardwareBuffer_describe(uptr_ahwb.get(), &desc_ahwb);
      ^
tensorflow/lite/delegates/gpu/delegate.cc:1194:18: error: use of undeclared identifier 'AHardwareBuffer_lock'
          return AHardwareBuffer_lock(buffer, this->usage_, -1 /* fence */,
                 ^
tensorflow/lite/delegates/gpu/delegate.cc:1221:24: error: use of undeclared identifier 'AHardwareBuffer_unlock'
                return AHardwareBuffer_unlock(buffer, nullptr /* fence */);
                       ^
5 errors generated.
Target //tensorflow/lite/tools/benchmark/android:benchmark_model failed to build
INFO: Elapsed time: 306.142s, Critical Path: 27.97s
INFO: 648 processes: 17 internal, 631 local.
FAILED: Build did NOT complete successfully
```
</details>"
60187,Tensorflow build error with v2.12.0,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 22.04 (Docker)

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

5.3.0

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When I try to build tensorflow from source, error occured by two file.

- tensorflow/compiler/tf2tensorrt/common/utils.cc:209:10
- tensorflow/compiler/tf2tensorrt/convert/weights.cc:61:10

Since there was no default statement in switch, I just add something like, default: break; and the error has gone.

It is okay to add just defualt and do nothing? or should I use some flags that makes compiler ignore w/o-switch?

Thanks!
```


### Standalone code to reproduce the issue

```shell
Same above.
```


### Relevant log output

```shell
ERROR: /tmp/tensorflow/tensorflow/compiler/tf2tensorrt/BUILD:565:16: Compiling tensorflow/compiler/tf2tensorrt/convert/weights.cc failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/compiler/tf2tensorrt/_objs/trt_weights/weights.pic.d ... (remaining 180 arguments skipped)
tensorflow/compiler/tf2tensorrt/convert/weights.cc: In member function ‘size_t tensorflow::tensorrt::convert::TRT_ShapedWeights::size_bytes() const’:
tensorflow/compiler/tf2tensorrt/convert/weights.cc:61:10: error: enumeration value ‘kFP8’ not handled in switch [-Werror=switch]
   61 |   switch (type_) {
      |          ^
cc1plus: some warnings being treated as errors
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```
</details>"
60185,TFTRT Convert using trt.TrtGraphConverterV2 failed due to “Attribute _tftrt_convert_function was not found”,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf2.8.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.6/8

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
convert my custom model using trt.TrtGraphConverterV2. There is no benefit in comparing the performance before and after conversion. Inspect the tf log, found that the conversion actually failed with the log “Attribute _tftrt_convert_function was not found”
```


### Relevant log output

```shell
2023-03-31 15:29:29.663291: I tensorflow/compiler/tf2tensorrt/convert/trt_optimization_pass.cc:167] Attribute _tftrt_convert_function was not found.
2023-03-31 15:29:29.663297: I tensorflow/compiler/tf2tensorrt/convert/trt_optimization_pass.cc:375] Not optimizing this grappler item: TRTEngineOp_0_2_native_segment
```
</details>"
60184,Linker Errors while building tensorflow from source,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

Debian GNU/Linux 10

### Mobile device

_No response_

### Python version

3.7

### Bazel version

6.1.1

### GCC/Compiler version

LLVM 12

### CUDA/cuDNN version

11, 8

### GPU model and memory

T4

### Current Behaviour?

```shell
I'm trying to build tensorflow from source via the tutorial (using configure.py). However, when I do so, I run into linker errors, such as


ld.lld: error: undefined symbol: tensorflow::Tensor::Tensor()
>>> referenced by nn_interface.cc
>>>               nn_interface.o:(nn::NNInterface::Infer(tensorflow::Scope&, tensorflow::ClientSession&)) in archive bazel-out/k8-opt/bin/cc/nn/libnn_interface.a
>>> referenced by nn_interface.cc
>>>               nn_interface.o:(nn::NNInterface::Infer(tensorflow::Scope&, tensorflow::ClientSession&)) in archive bazel-out/k8-opt/bin/cc/nn/libnn_interface.a
>>> referenced by nn_interface.cc
>>>               nn_interface.o:(nn::NNInterface::Infer(tensorflow::Scope&, tensorflow::ClientSession&)) in archive bazel-out/k8-opt/bin/cc/nn/libnn_interface.a
>>> referenced 3 more times


I have tried running this multiple times, with both gcc/llvm, and via the config script/through a dependency in my project. When I build TF as a monolithic build, everything works fine, but when I try to build using the TF defaults, I always run into linker issues. What could be going on?
```


### Standalone code to reproduce the issue

```shell
git clone <tensorflow>
cd tensorflow
./configure
```


### Relevant log output

```shell
ld.lld: error: undefined symbol: tensorflow::Tensor::Tensor()
>>> referenced by nn_interface.cc
>>>               nn_interface.o:(nn::NNInterface::Infer(tensorflow::Scope&, tensorflow::ClientSession&)) in archive bazel-out/k8-opt/bin/cc/nn/libnn_interface.a
>>> referenced by nn_interface.cc
>>>               nn_interface.o:(nn::NNInterface::Infer(tensorflow::Scope&, tensorflow::ClientSession&)) in archive bazel-out/k8-opt/bin/cc/nn/libnn_interface.a
>>> referenced by nn_interface.cc
>>>               nn_interface.o:(nn::NNInterface::Infer(tensorflow::Scope&, tensorflow::ClientSession&)) in archive bazel-out/k8-opt/bin/cc/nn/libnn_interface.a
>>> referenced 3 more times
```
</details>"
60183,Please update/fix the tutorial of how to automatically add cuda/dnn path into environment when activate conda environment,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.12

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

Linux Ubuntu 22.04.2

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hi guys,

First thanks for the support from TF 2.0 and cuda. I think there is a typo in tutorial which results the cuda would not be found by TF. In the main tutorail from https://www.tensorflow.org/install/pip#linux_setup (access from 2023.3.31). 

Where you mention ""For your convenience it is recommended that you automate it with the following commands. The system paths will be automatically configured when you activate this conda environment.""

and the code be given is:

""mkdir -p $CONDA_PREFIX/etc/conda/activate.d
CUDNN_PATH=$(dirname $(python -c ""import nvidia.cudnn;print(nvidia.cudnn.__file__)""))
echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/:$CUDNN_PATH/lib' > $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh""

This is somewhat problematic as this would only put ""export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/:$CUDNN_PATH/lib"" inside the file ""env_vars.sh"",
and when I activate the conda environment, the cuda path was not automatically loaded simply as ""CUDNN_PATH"" was not defined. This would then result ""GPU not found etc.. no GPU, cuda cannot be load etc.. fix issue etc..."". 

I believe the fix should be (may not be universal correct but works for me,please help check)""
""
mkdir -p $CONDA_PREFIX/etc/conda/activate.d
echo 'CUDNN_PATH=$(dirname $(python -c ""import nvidia.cudnn;print(nvidia.cudnn.__file__)""))' > $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh
echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/:$CUDNN_PATH/lib' >> $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh
""

In this case, the file ""env_vars.sh"" would contain:

""
CUDNN_PATH=$(dirname $(python -c ""import nvidia.cudnn;print(nvidia.cudnn.__file__)""))
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/:$CUDNN_PATH/lib
""

Now finally, everytime I activate the conda environment, I no longer need to manually set up the environment path for CUDA. 

I know this is somewhat fundamental, but this could be misleading to those high level devloper, and this could cause that CUDA cannot find GPU error without much detailed info. So please consider fixing this in the tutorial webpage: https://www.tensorflow.org/install/pip#linux_setup


Plus, another issue that one needs to update conda before install cuda, but if one is using environment module where the Anaconda was not installed in the global envrionment. Then one needs to be root or other account which has access to update the conda for that specific Anaconda version. 

Specifically, say the Anaconda was installed in 

/home/software/GlobalModules/apps/binapps/anaconda3/2020.07

If we activate conda envrionment and run:
conda upgrade -n base conda

it will return error no permissions (as the software was centrally distributed that user has no write access to the global installed pacakge (imaging that many users are sharing a HPC). 

In which case, one has to be the root user or other user has write access to the folder where we install the Anaconda to update the conda for this version 

However, this is not an issue of tensorflow of course, as it will only happen if one is using envrionment module. I provide here just in case anyone fails to update conda to install cuda etc.., as if the update of conda fails, then it will fail to install cuda somehow for no reason. Thanks.
```


### Standalone code to reproduce the issue

```shell
Note that I have installed environment module so this may not happen when only a universal conda was installed. 

#connect to some server
ssh -X -p 3060 user@somelinuxserver.com 

#environment module load anaconda
module load apps/binapps/anaconda3/2020.07

#activate (assume this venvPy3_8 was created following tutorial from https://www.tensorflow.org/install/pip#linux_setup)
conda activate venvPy3_8

#Try install cuda in virtual envrionment (as suggested)
conda install -c conda-forge cudatoolkit=11.8.0
pip install nvidia-cudnn-cu11==8.6.0.163

#Specify envirionment path (suppose to make my life easier but in fact not)
mkdir -p $CONDA_PREFIX/etc/conda/activate.d
echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/:$CUDNN_PATH/lib' > $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh

#pip install tensorflow==2.12.*

#Verify CPU setup (should return the CPU and not note that GPU not found)
python3 -c ""import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))""
#No GPU found etc...

#Veerify GPU setup (should return the physical GPU if successful)
python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""
#No GPU found etc...

#Fix, try instead (Go above step of enrionment path)
#Specify envirionment path 
mkdir -p $CONDA_PREFIX/etc/conda/activate.d
echo 'CUDNN_PATH=$(dirname $(python -c ""import nvidia.cudnn;print(nvidia.cudnn.__file__)""))' > $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh
echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/:$CUDNN_PATH/lib' >> $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh
```


### Relevant log output

```shell
Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
```
</details>"
60181,How to evaluate a pretrained TF mobilenet_v2 saved_model for accuracy on test dataset,"
### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 20.04
-   **TensorFlow installed from (source or binary)**: binary
-   **TensorFlow version (use command below)**:2.11
-   **Python version**: 3.9

### Describe the problem
How can i use a pretrained saved_model and find its accuracy on a test dataset?

I have mobilenet_v2 saved model which is sourced from https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/classification/5

I have an imagenet validation dataset consisting of 50000 images, and a labels.txt file consisting of ground truth labels for those 50000 images. 

I also have ImageNetLabels.txt sourced from https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt consisting of 1001 imagenet classes. 

How do I preprocess this data so that i can run evaluate() function to find test_data loss and accuracy of this pretrained model?

I am currently using the below script, but it doesn't seem to work:

```
import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
import os

m = tf.keras.Sequential([hub.KerasLayer(""https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/classification/4"", output_shape=(1001,))])
m.build([None, 224, 224, 3])# Batch input shape.
images = '/home/Downloads/ILSVRC2012_img_val'
classes = '/home/Documents/ImageNetLabels.txt'
labels ='/home/Documents/val.txt'

with open(labels, 'r') as f:
	label_name = [line.strip() for line in f.readlines()]

class_map = {}
with open(classes, 'r') as f:
	classes = [line.strip() for line in f]
	for i, class_name in enumerate(classes):
		class_map[class_name] = i

test_labels=[]
for label in label_name:
	if label in class_map:
		test_labels.append(class_map[label])
	else:
		print(f""label '{label} not found in class_map"")

image_paths = [os.path.join(images, filename) for filename in os.listdir(images)]
dataset = tf.data.Dataset.from_tensor_slices((image_paths, test_labels))
def preprocess_image(image_path):
	image = tf.io.read_file(image_path)
	image = tf.image.decode_jpeg(image, channels=3)
	image = tf.image.resize(image, [224,224])
	image = tf.image.convert_image_dtype(image, tf.float32)
	image /= 255.0
	return image

dataset = dataset.map(lambda image_path, label: (preprocess_image(image_path), label))

dataset = dataset.batch(batch_size=32)
m.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
loss, accuracy = m.evaluate(dataset)
print('loss: ', loss)
print('accuracy: ', accuracy)
```

I get the below error here:

> Traceback (most recent call last):
>   File ""test1234.py"", line 74, in <module>
>     loss, accuracy = m.evaluate(dataset)
>   File ""/home/mtk/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py"", line 70, in error_handler
>     raise e.with_traceback(filtered_tb) from None
>   File ""/tmp/__autograph_generated_filemqiwcebs.py"", line 15, in tf__test_function
>     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
> ValueError: in user code:
> 
>     File ""/home/mtk/.local/lib/python3.8/site-packages/keras/engine/training.py"", line 1820, in test_function  *
>         return step_function(self, iterator)
>     File ""/home/mtk/.local/lib/python3.8/site-packages/keras/engine/training.py"", line 1804, in step_function  **
>         outputs = model.distribute_strategy.run(run_step, args=(data,))
>     File ""/home/mtk/.local/lib/python3.8/site-packages/keras/engine/training.py"", line 1792, in run_step  **
>         outputs = model.test_step(data)
>     File ""/home/mtk/.local/lib/python3.8/site-packages/keras/engine/training.py"", line 1758, in test_step
>         self.compute_loss(x, y, y_pred, sample_weight)
>     File ""/home/mtk/.local/lib/python3.8/site-packages/keras/engine/training.py"", line 1082, in compute_loss
>         return self.compiled_loss(
>     File ""/home/mtk/.local/lib/python3.8/site-packages/keras/engine/compile_utils.py"", line 265, in __call__
>         loss_value = loss_obj(y_t, y_p, sample_weight=sw)
>     File ""/home/mtk/.local/lib/python3.8/site-packages/keras/losses.py"", line 152, in __call__
>         losses = call_fn(y_true, y_pred)
>     File ""/home/mtk/.local/lib/python3.8/site-packages/keras/losses.py"", line 284, in call  **
>         return ag_fn(y_true, y_pred, **self._fn_kwargs)
>     File ""/home/mtk/.local/lib/python3.8/site-packages/keras/losses.py"", line 2004, in categorical_crossentropy
>         return backend.categorical_crossentropy(
>     File ""/home/mtk/.local/lib/python3.8/site-packages/keras/backend.py"", line 5532, in categorical_crossentropy
>         target.shape.assert_is_compatible_with(output.shape)
> 
>     ValueError: Shapes (None, 1) and (None, 1001) are incompatible

I assume something is wrong in the way I preprocess my data though, but not sure how to go about it. Some insights would be nice
thanks"
60179,Build failed `not all outputs were created or valid` on `darwin/amd64`,"### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

faf4a8eb61d6344997e1860de30e6eae434b4411

### Custom Code

No

### OS Platform and Distribution

MacOS 13.3 (22E252) Apple M1 Pro

### Mobile device

N/A

### Python version

3.11

### Bazel version

bazel 5.3.0

### GCC/Compiler version

Apple clang version 14.0.0 (clang-1400.0.29.202)

### CUDA/cuDNN version

N/A

### GPU model and memory

16GB RAM, no NVIDIA GPU

### Current Behaviour?

<details>
<summary>Build logs</summary>

```
$ bazel build --verbose_failures //tensorflow/tools/pip_package:build_pip_package
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=156
INFO: Reading rc options for 'build' from /Users/ckpn/Mine/projects/python/bert_tst/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /Users/ckpn/Mine/projects/python/bert_tst/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /Users/ckpn/Mine/projects/python/bert_tst/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/opt/homebrew/opt/python@3.11/bin/python3.11 --action_env PYTHON_LIB_PATH=/opt/homebrew/opt/python@3.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages --python_path=/opt/homebrew/opt/python@3.11/bin/python3.11
INFO: Reading rc options for 'build' from /Users/ckpn/Mine/projects/python/bert_tst/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /Users/ckpn/Mine/projects/python/bert_tst/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /Users/ckpn/Mine/projects/python/bert_tst/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:macos in file /Users/ckpn/Mine/projects/python/bert_tst/tensorflow/.bazelrc: --apple_platform_type=macos --copt=-DGRPC_BAZEL_BUILD --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/6330447c2509c3669d64ae753c8030be1a38dc72.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found                                                                                 
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (606 packages loaded, 44253 targets configured).                                     
INFO: Found 1 target...                                                                                                                                     
ERROR: /Users/ckpn/Mine/projects/python/bert_tst/tensorflow/tensorflow/BUILD:1128:21: declared output 'tensorflow/libtensorflow_framework.2.dylib' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)                                                         
ERROR: /Users/ckpn/Mine/projects/python/bert_tst/tensorflow/tensorflow/BUILD:1128:21: Executing genrule //tensorflow:libtensorflow_framework.2.dylib_sym failed: not all outputs were created or valid                                                                                                                  
realpath: illegal option -- -                                                                                                                               
usage: realpath [-q] [path ...]                                                                                                                             
Target //tensorflow/tools/pip_package:build_pip_package failed to build                                                                                     
INFO: Elapsed time: 589.942s, Critical Path: 78.69s                                                                                                         
INFO: 4653 processes: 875 internal, 3778 local.                                                                                                             
FAILED: Build did NOT complete successfully
```

</details>

Expect no error

### Standalone code to reproduce the issue

1. Prepare a computer with Mac M1 chip
2. Do `git clone https://github.com/tensorflow/tensorflow`
3. Install dependencies such as clang and bazel
4. Do `bazel build --verbose_failures //tensorflow/tools/pip_package:build_pip_package`


### Relevant log output

*See above*
"
60178,Could not find any cuda.h matching version '' in any subdirectory,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

v2.8.0

### Custom Code

Yes

### OS Platform and Distribution

linux ubuntu 2204

### Mobile device

linux ubuntu 2204

### Python version

3.10

### Bazel version

6.1.1

### GCC/Compiler version

11.3.0

### CUDA/cuDNN version

ananconda3  2023.03

### GPU model and memory

11.3

### Current Behaviour?

```shell
I wanted to configure tensorflow,then after running . /configure command, I get the error ""Could not find any cudn.h matching version '10' in any subdirectory"", and I don't know how to handle this error.
```


### Standalone code to reproduce the issue

```shell
(base) emma@emma-virtual-machine:~/tensorflow$ ./configure
You have bazel 3.1.0 installed.
Please specify the location of python. [Default is /usr/local/anaconda3/bin/python3]: 


Found possible Python library paths:
  /usr/local/anaconda3/lib/python3.10/site-packages
Please input the desired Python library path to use.  Default is [/usr/local/anaconda3/lib/python3.10/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: 
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: 
No TensorRT support will be enabled for TensorFlow.

Could not find any cuda.h matching version '' in any subdirectory:
        ''
        'include'
        'include/cuda'
        'include/*-linux-gnu'
        'extras/CUPTI/include'
        'include/cuda/CUPTI'
        'local/cuda/extras/CUPTI/include'
of:
        '/lib/x86_64-linux-gnu'
        '/usr'
        '/usr/lib/x86_64-linux-gnu/libfakeroot'
        '/usr/local/lib'
Asking for detailed CUDA configuration...

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]: 


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]:
```


### Relevant log output

_No response_</details>"
60177,TF2 keras model.fit() VS model.evaluate print different loss and AUC,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.7.3

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
My model generate 3 binary outputs with MMoE module, and I define a custom metric to combine these three AUC in callbacks. As I am using model.fit to train my dataset,  it prints the final training AUC in the last epoch is 0.98. While I am using model.evaluate() on the SAME training dataset, it turns out the AUC is only 0.65. Can anyone can help me with it? Btw, I do not use BatchNormalization and Dropout layers, and also the batch_size is the same for both fitting and evaluation process.

I used exactly same dataset and same batch_size, I really don’t know why it occurs?
```


### Standalone code to reproduce the issue

```shell
class MergeMetrics(tf.keras.callbacks.Callback):
    def __init__(self,**kargs):
        super(MergeMetrics,self).__init__(**kargs)

    def on_epoch_begin(self,epoch, logs={}):
        return

    def on_epoch_end(self, epoch, logs={}):
        logs['merge_auc'] = 0.7 *logs[""output_1_auc""]+ 0.2*logs[""output_2_auc""] + 0.1*logs[""output_3_auc""]


model = Modelname(some_parameters)
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate, beta_1, beta_2),
            loss={
                ""output_1_auc"": ‘binary_crossentropy’
                ""output_2_auc"": ‘binary_crossentropy’
                ""output_3_auc"": ‘binary_crossentropy’
            },
            loss_weights=[1,1,1],
            metrics={
                ""output_1_auc"": [tf.keras.metrics.AUC(name='auc')],
                ""output_2_auc"": [tf.keras.metrics.AUC(name='auc')],
                ""output_3_auc"": [tf.keras.metrics.AUC(name='auc')]})


checkpoint = MergeMetrics()
checkpoint_filepath = os.path.join(path,'model')
model_check = tf.keras.callbacks.ModelCheckpoint(
        filepath = checkpoint_filepath,
        # filepath = path,
        monitor= ""merge_auc"",
        save_best_only = True,
        mode='max',
        save_weights_only = True,
        save_freq=""epoch"")

train_dataset = tf.data.Dataset.from_tensor_slices((train_data,                                           (y_train.output_1, y_train.output_2, y_train.output_3))).shuffle(10*batch_size).batch(batch_size)

model.fit(train_dataset, epochs=5, callbacks=[checkpoint,model_check])
model.load_weights(checkpoint_filepath) ## load weights of the best epoch
auc  = model.evaluate(train_data, [y_train.output_1, y_train.output_2, y_train.output_3], batch_size=batch_size)
```


### Relevant log output

```shell
RESULT:
Fit process : output_1_auc:0.9862,  output_2_auc: 0.9665, output_3_auc:0.5014, merge_auc: 0.9338
Evaluation process: output_1_auc: 0.7124500870704651, output_2_auc:0.6924731135368347, output_3_auc: 0.6774155497550964
```
</details>"
60176,Docs do not mention discontinuation of support for Python 3.7 in TF 2.11.1,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.11.1

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The release notes for TensorFlow 2.12.0 state that support for Python 3.7 has been discontinued. However, it seems that Python 3.7 support has also been removed from TensorFlow 2.11.1, despite this not being mentioned in the release notes or documentation. As a result, to maintain compatibility, we were forced to limit our application to using TensorFlow 2.11.0.
```


### Standalone code to reproduce the issue

```shell
NA
```


### Relevant log output

_No response_</details>"
60175,Building TF from source instructions clarification wrt Python packages,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Feature Request

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

TF 2.11

### Custom Code

No

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

5.3

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hi! Following the instructions on https://www.tensorflow.org/install/source usually works. However, this time I ran into an issue that took a while to figure out. It turns out to be an issue with the protobuf version:

protobuf                     3.19.4 => no good
protobuf                     3.20.3 => good 

In the current instructions, the only Python related info is:

sudo apt install python3-dev python3-pip

and 

pip install -U --user pip numpy wheel packaging requests opt_einsum
pip install -U --user keras_preprocessing --no-deps

Is there a reason not to refer to the following requirement specs?

pip install -r ./tensorflow/tools/ci_build/release/requirements_common.txt
pip install -r ./tensorflow/tools/ci_build/release/requirements_ubuntu.txt

Which would have avoided the issue for me... If not, I propose to add that.

Cheers!
```


### Standalone code to reproduce the issue

```shell
bazel build --verbose_failures //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
ERROR: /home/ubuntu/repos/mirror-tensorflow/tensorflow/BUILD:1635:19: Executing genrule //tensorflow:tf_python_api_gen_v2 failed: (Exit 1): bash failed: error executing command 
  (cd /home/ubuntu/.cache/bazel/_bazel_ubuntu/0ef2ae1c374389fefaed577dece28985/execroot/org_tensorflow && \
  exec env - \
    PATH=/home/ubuntu/.cache/bazelisk/downloads/bazelbuild/bazel-5.3.0-linux-arm64/bin:/home/ubuntu/.vscode-server/bin/ee2b180d582a7f601fa6ecfdad8d9fd269ab1884/bin/remote-cli:/home/ubuntu/.local/bin:/home/ubuntu/bin:/home/ubuntu/.local/bin:/home/ubuntu/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/ubuntu/bin:/home/ubuntu/bin \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3.10/dist-packages \
    TF2_BEHAVIOR=1 \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/aarch64-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2 --root_init_template=tensorflow/api_template.__init__.py --apidir=bazel-out/aarch64-opt/bin/tensorflow_api/v2/ --apiname=tensorflow --apiversion=2  --compat_apiversion=1 --compat_apiversion=2  --compat_init_template=tensorflow/compat_template_v1.__init__.py --compat_init_template=tensorflow/compat_template.__init__.py --packages=tensorflow.python,tensorflow.dtensor.python.accelerator_util,tensorflow.dtensor.python.api,tensorflow.dtensor.python.config,tensorflow.dtensor.python.d_checkpoint,tensorflow.dtensor.python.d_variable,tensorflow.dtensor.python.input_util,tensorflow.dtensor.python.layout,tensorflow.dtensor.python.mesh_util,tensorflow.dtensor.python.tpu_util,tensorflow.dtensor.python.save_restore,tensorflow.lite.python.analyzer,tensorflow.lite.python.lite,tensorflow.lite.python.authoring.authoring,tensorflow.python.modules_with_exports --output_package=tensorflow._api.v2 --use_relative_imports=True --loading=static --loading=default bazel-out/aarch64-opt/bin/tensorflow/_api/v2/v2.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/autograph/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/decorator/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/dispatch/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/distribute/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/distribute/combinations/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/distribute/interim/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/distribute/multi_process_runner/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/eager_context/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/feature_column/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/function/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/graph_util/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/mixed_precision/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/monitoring/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/nest/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/smart_cond/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/test/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/test/combinations/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/tf2/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/train/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/types/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/types/data/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/saved_model/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/saved_model/load/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/tracking/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__operators__/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/audio/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/autograph/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/autograph/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/autodiff/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/bitwise/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/config/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/config/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/config/optimizer/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/config/threading/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/data/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/data/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/data/experimental/service/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/debugging/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/debugging/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/distribute/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/distribute/cluster_resolver/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/distribute/coordinator/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/distribute/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/distribute/experimental/coordinator/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/distribute/experimental/partitioners/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/distribute/experimental/rpc/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/dtypes/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/dtypes/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/errors/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/experimental/extension_type/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/experimental/dtensor/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/experimental/numpy/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/experimental/numpy/random/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/experimental/tensorrt/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/experimental/dlpack/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/feature_column/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/io/gfile/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/graph_util/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/image/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/io/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/queue/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/linalg/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/linalg/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/lite/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/lite/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/lite/experimental/authoring/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/lite/experimental/microfrontend/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/lite/experimental/microfrontend/python/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/lite/experimental/microfrontend/python/ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/lookup/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/lookup/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/math/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/math/special/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/mlir/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/mlir/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/nest/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/nn/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/nn/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/profiler/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/profiler/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/profiler/experimental/client/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/profiler/experimental/server/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/quantization/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/ragged/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/random/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/random/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/raw_ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/saved_model/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/saved_model/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/sets/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/signal/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/sparse/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/strings/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/summary/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/summary/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/sysconfig/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/test/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/test/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/tpu/experimental/embedding/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/tpu/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/tpu/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/train/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/train/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/types/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/types/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/types/experimental/distributed/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/version/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/xla/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/xla/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/autograph/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/decorator/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/dispatch/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/distribute/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/distribute/combinations/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/distribute/interim/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/distribute/multi_process_runner/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/eager_context/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/feature_column/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/function/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/graph_util/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/mixed_precision/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/monitoring/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/nest/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/smart_cond/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/test/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/test/combinations/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/tf2/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/train/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/types/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/types/data/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/saved_model/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/saved_model/load/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/tracking/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__operators__/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/audio/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/autograph/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/autograph/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/autodiff/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/bitwise/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/compat/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/config/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/config/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/config/optimizer/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/config/threading/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/data/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/data/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/data/experimental/service/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/debugging/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/debugging/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/distribute/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/distribute/cluster_resolver/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/distribute/coordinator/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/distribute/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/distribute/experimental/coordinator/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/distribute/experimental/partitioners/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/distribute/experimental/rpc/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/dtypes/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/dtypes/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/errors/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/experimental/extension_type/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/experimental/dtensor/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/experimental/numpy/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/experimental/numpy/random/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/experimental/tensorrt/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/experimental/dlpack/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/feature_column/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/io/gfile/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/graph_util/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/image/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/io/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/queue/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/linalg/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/linalg/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/lite/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/lite/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/lite/experimental/authoring/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/lite/experimental/microfrontend/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/lite/experimental/microfrontend/python/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/lite/experimental/microfrontend/python/ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/lookup/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/lookup/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/math/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/math/special/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/mlir/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/mlir/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/nest/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/nn/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/nn/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/profiler/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/profiler/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/profiler/experimental/client/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/profiler/experimental/server/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/quantization/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/ragged/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/random/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/random/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/raw_ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/saved_model/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/saved_model/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/sets/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/signal/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/sparse/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/strings/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/summary/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/summary/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/sysconfig/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/test/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/test/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/tpu/experimental/embedding/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/tpu/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/tpu/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/train/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/train/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/types/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/types/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/types/experimental/distributed/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/version/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/xla/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/xla/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/__internal__/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/__internal__/types/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/__internal__/types/data/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/app/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/audio/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/autograph/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/autograph/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/bitwise/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/compat/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/config/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/config/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/config/optimizer/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/config/threading/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/data/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/data/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/data/experimental/service/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/debugging/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/debugging/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/distribute/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/distribute/cluster_resolver/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/distribute/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/distributions/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/dtypes/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/dtypes/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/errors/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/experimental/extension_type/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/feature_column/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/gfile/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/io/gfile/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/graph_util/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/image/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/io/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/queue/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/initializers/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/layers/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/layers/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/linalg/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/linalg/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lite/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lite/constants/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lite/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lite/experimental/authoring/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lite/experimental/microfrontend/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lite/experimental/microfrontend/python/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lite/experimental/microfrontend/python/ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/logging/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lookup/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lookup/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/losses/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/manip/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/math/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/math/special/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/metrics/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/mixed_precision/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/mixed_precision/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/mlir/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/mlir/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/nest/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/nn/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/nn/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/nn/rnn_cell/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/profiler/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/python_io/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/quantization/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/ragged/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/random/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/random/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/raw_ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/resource_loader/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/strings/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/builder/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/constants/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/loader/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/main_op/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/signature_constants/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/signature_def_utils/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/tag_constants/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/utils/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/sets/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/signal/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/sparse/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/spectral/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/summary/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/sysconfig/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/test/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/test/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/tpu/experimental/embedding/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/tpu/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/tpu/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/train/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/train/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/train/queue_runner/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/types/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/types/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/user_ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/version/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/xla/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/xla/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/compat/v1/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/compat/v2/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/compat/v1/compat/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/compat/v2/compat/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/compat/v1/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/compat/v2/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/compat/v1/compat/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/compat/v2/compat/__init__.py')
# Configuration: 6e760a068cbc11a6cc46e726655f369e4c10c58a67d5a62b8baed49245ffc3a5
# Execution platform: @local_execution_config_platform//:platform
Traceback (most recent call last):
  File ""/home/ubuntu/.cache/bazel/_bazel_ubuntu/0ef2ae1c374389fefaed577dece28985/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 22, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/home/ubuntu/.cache/bazel/_bazel_ubuntu/0ef2ae1c374389fefaed577dece28985/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 37, in <module>
    from tensorflow.python.eager import context
  File ""/home/ubuntu/.cache/bazel/_bazel_ubuntu/0ef2ae1c374389fefaed577dece28985/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/eager/context.py"", line 28, in <module>
    from tensorflow.core.framework import function_pb2
  File ""/home/ubuntu/.cache/bazel/_bazel_ubuntu/0ef2ae1c374389fefaed577dece28985/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/core/framework/function_pb2.py"", line 5, in <module>
    from google.protobuf.internal import builder as _builder
ImportError: cannot import name 'builder' from 'google.protobuf.internal' (/home/ubuntu/.local/lib/python3.10/site-packages/google/protobuf/internal/__init__.py)
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: /home/ubuntu/repos/mirror-tensorflow/tensorflow/lite/python/BUILD:72:10 Middleman _middlemen/tensorflow_Slite_Spython_Stflite_Uconvert-runfiles failed: (Exit 1): bash failed: error executing command 
  (cd /home/ubuntu/.cache/bazel/_bazel_ubuntu/0ef2ae1c374389fefaed577dece28985/execroot/org_tensorflow && \
  exec env - \
    PATH=/home/ubuntu/.cache/bazelisk/downloads/bazelbuild/bazel-5.3.0-linux-arm64/bin:/home/ubuntu/.vscode-server/bin/ee2b180d582a7f601fa6ecfdad8d9fd269ab1884/bin/remote-cli:/home/ubuntu/.local/bin:/home/ubuntu/bin:/home/ubuntu/.local/bin:/home/ubuntu/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/ubuntu/bin:/home/ubuntu/bin \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3.10/dist-packages \
    TF2_BEHAVIOR=1 \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/aarch64-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2 --root_init_template=tensorflow/api_template.__init__.py --apidir=bazel-out/aarch64-opt/bin/tensorflow_api/v2/ --apiname=tensorflow --apiversion=2  --compat_apiversion=1 --compat_apiversion=2  --compat_init_template=tensorflow/compat_template_v1.__init__.py --compat_init_template=tensorflow/compat_template.__init__.py --packages=tensorflow.python,tensorflow.dtensor.python.accelerator_util,tensorflow.dtensor.python.api,tensorflow.dtensor.python.config,tensorflow.dtensor.python.d_checkpoint,tensorflow.dtensor.python.d_variable,tensorflow.dtensor.python.input_util,tensorflow.dtensor.python.layout,tensorflow.dtensor.python.mesh_util,tensorflow.dtensor.python.tpu_util,tensorflow.dtensor.python.save_restore,tensorflow.lite.python.analyzer,tensorflow.lite.python.lite,tensorflow.lite.python.authoring.authoring,tensorflow.python.modules_with_exports --output_package=tensorflow._api.v2 --use_relative_imports=True --loading=static --loading=default bazel-out/aarch64-opt/bin/tensorflow/_api/v2/v2.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/autograph/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/decorator/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/dispatch/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/distribute/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/distribute/combinations/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/distribute/interim/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/distribute/multi_process_runner/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/eager_context/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/feature_column/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/function/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/graph_util/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/mixed_precision/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/monitoring/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/nest/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/smart_cond/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/test/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/test/combinations/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/tf2/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/train/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/types/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/types/data/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/saved_model/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/saved_model/load/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/tracking/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__operators__/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/audio/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/autograph/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/autograph/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/autodiff/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/bitwise/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/config/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/config/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/config/optimizer/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/config/threading/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/data/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/data/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/data/experimental/service/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/debugging/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/debugging/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/distribute/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/distribute/cluster_resolver/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/distribute/coordinator/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/distribute/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/distribute/experimental/coordinator/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/distribute/experimental/partitioners/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/distribute/experimental/rpc/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/dtypes/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/dtypes/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/errors/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/experimental/extension_type/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/experimental/dtensor/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/experimental/numpy/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/experimental/numpy/random/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/experimental/tensorrt/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/experimental/dlpack/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/feature_column/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/io/gfile/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/graph_util/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/image/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/io/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/queue/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/linalg/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/linalg/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/lite/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/lite/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/lite/experimental/authoring/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/lite/experimental/microfrontend/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/lite/experimental/microfrontend/python/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/lite/experimental/microfrontend/python/ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/lookup/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/lookup/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/math/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/math/special/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/mlir/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/mlir/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/nest/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/nn/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/nn/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/profiler/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/profiler/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/profiler/experimental/client/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/profiler/experimental/server/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/quantization/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/ragged/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/random/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/random/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/raw_ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/saved_model/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/saved_model/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/sets/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/signal/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/sparse/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/strings/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/summary/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/summary/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/sysconfig/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/test/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/test/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/tpu/experimental/embedding/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/tpu/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/tpu/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/train/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/train/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/types/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/types/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/types/experimental/distributed/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/version/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/xla/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/xla/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/autograph/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/decorator/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/dispatch/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/distribute/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/distribute/combinations/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/distribute/interim/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/distribute/multi_process_runner/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/eager_context/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/feature_column/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/function/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/graph_util/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/mixed_precision/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/monitoring/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/nest/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/smart_cond/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/test/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/test/combinations/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/tf2/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/train/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/types/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/types/data/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/saved_model/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/saved_model/load/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/tracking/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__operators__/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/audio/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/autograph/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/autograph/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/autodiff/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/bitwise/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/compat/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/config/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/config/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/config/optimizer/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/config/threading/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/data/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/data/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/data/experimental/service/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/debugging/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/debugging/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/distribute/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/distribute/cluster_resolver/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/distribute/coordinator/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/distribute/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/distribute/experimental/coordinator/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/distribute/experimental/partitioners/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/distribute/experimental/rpc/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/dtypes/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/dtypes/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/errors/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/experimental/extension_type/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/experimental/dtensor/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/experimental/numpy/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/experimental/numpy/random/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/experimental/tensorrt/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/experimental/dlpack/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/feature_column/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/io/gfile/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/graph_util/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/image/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/io/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/queue/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/linalg/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/linalg/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/lite/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/lite/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/lite/experimental/authoring/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/lite/experimental/microfrontend/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/lite/experimental/microfrontend/python/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/lite/experimental/microfrontend/python/ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/lookup/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/lookup/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/math/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/math/special/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/mlir/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/mlir/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/nest/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/nn/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/nn/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/profiler/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/profiler/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/profiler/experimental/client/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/profiler/experimental/server/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/quantization/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/ragged/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/random/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/random/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/raw_ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/saved_model/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/saved_model/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/sets/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/signal/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/sparse/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/strings/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/summary/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/summary/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/sysconfig/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/test/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/test/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/tpu/experimental/embedding/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/tpu/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/tpu/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/train/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/train/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/types/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/types/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/types/experimental/distributed/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/version/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/xla/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/xla/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/__internal__/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/__internal__/types/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/__internal__/types/data/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/app/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/audio/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/autograph/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/autograph/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/bitwise/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/compat/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/config/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/config/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/config/optimizer/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/config/threading/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/data/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/data/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/data/experimental/service/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/debugging/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/debugging/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/distribute/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/distribute/cluster_resolver/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/distribute/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/distributions/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/dtypes/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/dtypes/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/errors/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/experimental/extension_type/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/feature_column/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/gfile/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/io/gfile/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/graph_util/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/image/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/io/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/queue/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/initializers/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/layers/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/layers/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/linalg/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/linalg/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lite/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lite/constants/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lite/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lite/experimental/authoring/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lite/experimental/microfrontend/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lite/experimental/microfrontend/python/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lite/experimental/microfrontend/python/ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/logging/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lookup/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lookup/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/losses/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/manip/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/math/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/math/special/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/metrics/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/mixed_precision/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/mixed_precision/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/mlir/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/mlir/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/nest/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/nn/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/nn/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/nn/rnn_cell/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/profiler/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/python_io/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/quantization/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/ragged/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/random/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/random/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/raw_ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/resource_loader/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/strings/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/builder/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/constants/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/loader/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/main_op/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/signature_constants/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/signature_def_utils/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/tag_constants/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/utils/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/sets/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/signal/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/sparse/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/spectral/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/summary/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/sysconfig/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/test/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/test/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/tpu/experimental/embedding/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/tpu/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/tpu/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/train/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/train/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/train/queue_runner/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/types/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/types/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/user_ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/version/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/xla/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/xla/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/compat/v1/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/compat/v2/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/compat/v1/compat/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/compat/v2/compat/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/compat/v1/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/compat/v2/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/compat/v1/compat/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/compat/v2/compat/__init__.py')
# Configuration: 6e760a068cbc11a6cc46e726655f369e4c10c58a67d5a62b8baed49245ffc3a5
# Execution platform: @local_execution_config_platform//:platform
INFO: Elapsed time: 1090.456s, Critical Path: 327.25s
INFO: 16173 processes: 1835 internal, 14338 local.
FAILED: Build did NOT complete successfully
```
</details>"
60174,Dataset is never fully read when caching to disk,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

v2.12.0-rc1-12-g0db597d0d75 2.12.0

### Custom Code

No

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

Python 3.10.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When caching a dataset to disk with `dataset.cache(""some_folder"")`, the dataset is never fully cached according to the log message. I have reproduced this consistently in my application code and in the MWE below.

When testing this MWE, the message does not appear on subsequent runs unless I delete the cache files, which makes me think Tensorflow is actually able to them, and this might be a logging issue.

When caching to memory (`dataset.cache()`), no log message is generated.



### Standalone code to reproduce the issue

```python
import tensorflow as tf

dataset = tf.data.Dataset.range(1_000_000, dtype=tf.float32)
dataset=dataset.cache(""cached"")
for _ in dataset:
    pass # Exhaust the dataset to force it to cache
```


### Relevant log output

```text
2023-03-30 12:15:57.441876: W tensorflow/core/kernels/data/cache_dataset_ops.cc:296] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
```
</details>"
60172,TensorFlow cannot use GPU.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.11.2

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.8/8.6

### GPU model and memory

_No response_

### Current Behaviour?

```shell
import tensorflow as tf
print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))
tf.config.list_physical_devices('GPU')
tf.config.list_physical_devices()
Num GPUs Available:  0
[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]
```


### Standalone code to reproduce the issue

```shell
I install CUDA 11.8 and CUDNN 8.6, but TensorFlow cannot use GPU.I also set %PATH% variable.
```


### Relevant log output

_No response_</details>"
60171,Are there plans to release TF 2.9.4 to address CVEs?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.9.3

### Custom Code

No

### OS Platform and Distribution

RHEL 8

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
We're using TF 2.9.3 in our product images. Our latest twistlock scans reveal several security findings. Are there plans to release a 2.9.4 that addresses these CVEs?

Findings are:
| CVE | URL |
| --- | --- |
| CVE-2023-25669	| https://nvd.nist.gov/vuln/detail/CVE-2023-25669|
| CVE-2023-25673	| https://nvd.nist.gov/vuln/detail/CVE-2023-25673|
| CVE-2023-25674	| https://nvd.nist.gov/vuln/detail/CVE-2023-25674|
| CVE-2023-27579	| https://nvd.nist.gov/vuln/detail/CVE-2023-27579|
| CVE-2023-25667	| https://nvd.nist.gov/vuln/detail/CVE-2023-25667|
| CVE-2023-25675	| https://nvd.nist.gov/vuln/detail/CVE-2023-25675|
| CVE-2023-25670	| https://nvd.nist.gov/vuln/detail/CVE-2023-25670|
| CVE-2023-25671	| https://nvd.nist.gov/vuln/detail/CVE-2023-25671|
| CVE-2023-25672	| https://nvd.nist.gov/vuln/detail/CVE-2023-25672|
| CVE-2023-25801	| https://nvd.nist.gov/vuln/detail/CVE-2023-25801|
| CVE-2023-25676	| https://nvd.nist.gov/vuln/detail/CVE-2023-25676|
| CVE-2023-25668	| https://nvd.nist.gov/vuln/detail/CVE-2023-25668|
| CVE-2023-25666	| https://nvd.nist.gov/vuln/detail/CVE-2023-25666|
| CVE-2023-25665	| https://nvd.nist.gov/vuln/detail/CVE-2023-25665|
| CVE-2023-25664	| https://nvd.nist.gov/vuln/detail/CVE-2023-25664|
| CVE-2023-25663	| https://nvd.nist.gov/vuln/detail/CVE-2023-25663|
| CVE-2023-25662	| https://nvd.nist.gov/vuln/detail/CVE-2023-25662|
| CVE-2023-25661	| https://nvd.nist.gov/vuln/detail/CVE-2023-25661|
| CVE-2023-25660	| https://nvd.nist.gov/vuln/detail/CVE-2023-25660|
| CVE-2023-25659	| https://nvd.nist.gov/vuln/detail/CVE-2023-25659|
| CVE-2023-25658	| https://nvd.nist.gov/vuln/detail/CVE-2023-25658|
```


### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

_No response_</details>"
60170,Non deterministic training of LSTM with recurrent_dropout>0,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

tf 2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 22.04.2 LTS

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?
The training of a LSTM model with recurrent_droput>0 is not deterministic: setting the op_determinism configuration  (https://www.tensorflow.org/api_docs/python/tf/config/experimental/enable_op_determinism) different runs of creation and train of the same model produce different model (different training history).
In contrast, deterministic training is obtained with recurrent_dropout=0 and also also with the  dropout>0.

The code below has been used to reproduce the bug.
Note that the  `det_session` function is used each time before the creation of the model.
This function contains the suggested setting from https://www.tensorflow.org/api_docs/python/tf/config/experimental/enable_op_determinism, but I have tested this code, with the same results, also with the following version in which suggested determinism sets from various issues or sites  have been added: 

```python
def det_session():
    os.environ['PYTHONHASHSEED'] = str(1)
    rn.seed(1)
    np.random.seed(1)
    tf.random.set_seed(1)
    tf.keras.utils.set_random_seed(1)
    tf.config.experimental.enable_op_determinism()
```


### Standalone code to reproduce the issue

```python
import tensorflow as tf
from tensorflow.keras.losses import mean_squared_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import RMSprop


def det_session():
    tf.keras.utils.set_random_seed(1)
    tf.config.experimental.enable_op_determinism()


def create_model(inputs_shape):
    # Define the model
    model = Sequential()
    model.add(tf.keras.layers.LSTM(4, input_shape=(inputs_shape[1], inputs_shape[2]), dropout=0.0, recurrent_dropout=0.1))
    # Compile the model
    model.compile(optimizer=RMSprop(1e-3), loss=mean_squared_error)
    # Give a summary
    model.summary()
    return model


if __name__ == '__main__':
    inputs = tf.random.normal([32, 10, 8])
    outputs = tf.random.normal([32, 4])

    # Create and train the model the first time
    det_session()
    model = create_model(inputs.shape)
    history_0 = model.fit(inputs, outputs, epochs=10)
    # Create and train the model more times and check the loss history
    for _ in range(10):
        # Create and train the model again and check the history loss
        det_session()
        model = create_model(inputs.shape)
        history = model.fit(inputs, outputs, epochs=10)
        assert history.history['loss'] == history_0.history['loss'], 'Losses history does not corresponds'
```


### Relevant log output

_No response_</details>"
60168,[TF-TRT QAT Explicit precision] Assertion outScales.size() == 1 when converting QAT TF model,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?
I am currently working through supporting TF-TRT with explicit quant / dequant nodes, and am in the final stages of making it work. Here is a summary of the fixes I landed: 
- Implement a explicit convert for `FakeQuantWithMinMaxVars` keras API
- Support Conv2D with a tensor input in explicit conversion (transpose 2nd input to `KCRS` format) to be compatible with TRT conv layer

The conversion actually runs through, but I get an error when building the engine almost immediately: 
```shell
Internal Error (Assertion outScales.size() == 1 failed. )
```


### Standalone code to reproduce the issue

I am building the code from a custom TF-TRT branch off of r2.9, with minimal non-breaking changes. I can clean up my changes and open a PR on r2.9 so people can understand the changes better.


### Relevant log output

```shell
W20230329 16:29:04.094142 99658 quantization_ops.cc:309] FakeQuantWithMinMaxVars has narrow_range=true, but for TensorRT conversion, narrow_range=false is recommended.
W20230329 16:29:04.138859 99658 quantization_ops.cc:309] FakeQuantWithMinMaxVars has narrow_range=true, but for TensorRT conversion, narrow_range=false is recommended.
E20230329 16:29:05.544306 99658 convert_nodes.cc:7274] use_explicit_precision: 1
E20230329 16:29:05.544360 99658 convert_nodes.cc:7280] Build cuda engine
E20230329 16:29:05.545313 99658 convert_nodes.cc:1287] Setting TensorRT network name to TF:2.9.1, TRT:8.5.1-Precision:INT8, Calibration:0, Max-Batch-Size:1, Max-Workspace-Size:4294967296
E20230329 16:29:12.417982 99658 trt_logger.cc:40] DefaultLogger 2: [pointWiseV2Builder.cpp::toExpr::279] Error Code 2: Internal Error (Assertion outScales.size() == 1 failed. )
W20230329 16:29:12.464977 99658 trt_engine_op.cc:1047] TF-TRT Warning: Engine creation for TRTEngineOp_000_000 failed. The native segment will be used instead. Reason: INTERNAL: Failed to build TensorRT engine
W20230329 16:29:12.465102 99658 trt_engine_op.cc:888] TF-TRT Warning: Engine retrieval for input shapes: [[1,7,12], [1,7,15], [1,7,12], [1,6,5,550400], [7,3,1208,1920], [1,15,1,1]] failed. Running native segment for TRTEngineOp_000_000
```
</details>

Following up from this issue: https://github.com/tensorflow/tensorflow/issues/59711
It can possibly help solve both. "
60167,Compilation times double after commit removing distinct_host_configuration=false option,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

master at commit bdacdadc5

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

5.3.0

### GCC/Compiler version

Ubuntu 9.4.0-1ubuntu1~20.04.1

### CUDA/cuDNN version

CUDA 11.8, CUDNN 8.6

### GPU model and memory

N/A

### Current Behaviour?

```shell
Since commit [fa2a678486](https://github.com/tensorflow/tensorflow/commit/fa2a6784860c40e3b189f21208154b851af2fee8) removed the `--distinct_host_configuration=false` option build times have doubled.

TensorFlow builds now take 114 minutes on a machine with dual 20-core CPUs.
Disabling distinct host configuration, the same build takes 62 minutes.

This significantly increases the stress on build and test CIs as well as reducing developer productivity.
```


### Standalone code to reproduce the issue

```shell
I'm building using the sig-build docker environment as documented [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/tf_sig_build_dockerfiles).
```


### Relevant log output

_No response_</details>"
60166,tf.linalg.diag_part() is slow on TPU,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Google colab

### Mobile device

_No response_

### Python version

3.9.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


I am working on implementing efficient attention mechanisms using tensorflow. (such as longformer).

The local interaction in the longformer only requires tokens to interact with their neighboring ones, which could reduce the computation overhead calculating the full dot product between two tensors.

The problem I am currently experiencing is that tf.linalg.diag_part(), a function used for extracting the band diagonal from the key tensor, works extremely slow on TPU. 

In my test, the efficient attention takes 52sec to compute, whereas it takes 2sec to compute the regular dot product attention.

I tried warping it on @tf.function decorator jit_compile=True, but it is still slower than the full dot product calculation.

Using CPU, the efficient attention gets x4-5 performance boost depending on the settings.

There may be some limitations in tf.linalg.diag_part() in parallel use. The source code is written in C++, which I lack expertise in. 

Is there any way to enhance the performance of tf.linalg.diag_part() in parallel processing?


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from functools import partial

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
print(""All devices: "", tf.config.list_logical_devices('TPU'))
strategy = tf.distribute.TPUStrategy(resolver)


def split_head(input, B, N, D, num_heads):
    assert D/num_heads == D//num_heads, ""D must be divisible by num_heads""
   
    x = tf.reshape(input, 
               (B, N, num_heads, D//num_heads))
    return tf.transpose(x, perm = (0, 2, 1, 3))
       
B = 2048
N = 1024
D = 1024
H = 32
b_k = 15
input = tf.ones((B, N, D))

with strategy.scope():
      
 #tf.reshape(tf.range(B*N*D), (B,N,D))
  q = input
  k = input
  split_head = partial(split_head, B= B, N=N, D= D)
  q = split_head(q, num_heads= H)
  k = split_head(k, num_heads= H)
  print(B,N,D)
  k_T = tf.transpose(k, perm =(0, 1, 3, 2))
  print(k_T.shape)
  band_k = extract_band(input = k, b_k = b_k)
  print(band_k.shape)
  if N >= D//H:
      attention = q@tf.transpose(band_k, perm = (0, 1, 3, 2))
      print(attention.shape)
      attention = tf.reduce_mean(attention, axis = 1) # combine head
      attention_map = tf.linalg.diag(tf.transpose(attention, perm = (0, 2, 1) ), k = (-b_k//2, b_k//2))
```


### Relevant log output

_No response_</details>"
60165,tf.estimator.BestExporter / tf.compat.v1.gfile.Rename not working for saved_models in S3,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.12

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04.2

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When either using tf.estimator.BestExporter or using tf.compat.v1.gfile.Rename directly when the source and destination are folders in S3, an error is thrown. This error seems to go back to Tensorflow 2.6 when the S3 support was moved into tensorflow_io. In Tensorflow 2.5 this behaves properly. It also functions properly if the folders are local.

Right now if you are using Tensorflow >= 2.6 and using a tf.estimator.BestExporter with the output being written to S3, an exception is thrown
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import tensorflow_io as tfio

SOURCE_DIR = 's3://.../best_exporter/1'
DEST_DIR = 's3://.../best_exporter/old'

tf.compat.v1.gfile.Rename(SOURCE_DIR, DEST_DIR)
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/home/j99ca/.config/JetBrains/PyCharm2022.3/scratches/tf2_s3_rename_test.py"", line 14, in <module>
    tf.compat.v1.gfile.Rename(SOURCE_DIR, DEST_DIR)
  File ""/home/j99ca/venvs/tf2-12/lib/python3.10/site-packages/tensorflow/python/lib/io/file_io.py"", line 606, in rename
    rename_v2(oldname, newname, overwrite)
  File ""/home/j99ca/venvs/tf2-12/lib/python3.10/site-packages/tensorflow/python/lib/io/file_io.py"", line 622, in rename_v2
    _pywrap_file_io.RenameFile(
tensorflow.python.framework.errors_impl.FailedPreconditionError: Source is a directory or empty file
```
</details>"
60164,JVP using tf.autodiff.ForwardAccumulator becomes None under graph execution,"I'm new to Tensorflow. I'm trying to compute a Jacobian-Vector Product using tf.autodiff.ForwardAccumulator with a train function looks something like the code below. The jvp looks fine under eager execution. However, the jvp becomes a list of Nones when activate graph execution using @tf.function. 
What could be the issue here?

### Tensorflow Version

tf 2.6.0

### Python version

3.7

### Train code

```shell
@tf.function
def train(self, data):
        with tf.GradientTape() as upper_tape:
            loss1= self.loss1(data)
        grad1 = upper_tape.gradient(loss1, self.net1.variables)

        with tf.autodiff.ForwardAccumulator(primals=self.net1.variables, tangents=grad1) as acc:
            with tf.GradientTape() as lower_tape:
                loss2 = self.loss2(data)
            grad2 = lower_tape.gradient(loss2, self.net2.variables)

        final_grad = acc.jvp(grad2)
        self.optimizer.apply_gradients(zip(final_grad, self.net2.variables))
```

grad1 and grad2 are correctly computed under both eager mode and graph mode. The only problem is with the jvp."
60163,How to solve the backpropagation problem of tf.signal.rfft?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.11

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I x wanted to implement the Hilbert transform in the model built by tensorflowd, but I found no callable API. so I implemented it myself using tf.signal.rfft and tf.signal.ifft. However, when I was training, I found the following error reported:
LookupError: gradient registry has no entry for: RFFT3D
I think it may be that tensorflow has not defined the corresponding backpropagation method for rfft3d yet. I want to provide my code train.py  &  model.py
```


### Standalone code to reproduce the issue

```
def hilbert_transform(x, N=None, axis=-1):
    if x.dtype == tf.complex64 or x.dtype == tf.complex128:
        raise ValueError(""x must be real."")
    if N is None:
        N = x.shape[axis]
    if N <= 0:
        raise ValueError(""N must be positive."")
    x_br = tf.signal.rfft3d(x[:, :, :1])
    x_bi = tf.signal.rfft3d(x[:, :, 1:])
    x_complex = concatenate([x_br, x_bi], axis=-1)

    h = np.zeros(N)
    if N % 2 == 0:
        h[0] = h[N // 2] = 1
        h[1:N // 2] = 2
    else:
        h[0] = 1
        h[1:(N + 1) // 2] = 2
    if x.shape.ndims > 1:
        ind = [tf.newaxis] * x.shape.ndims
        ind[axis] = slice(None)
        h = h[tuple(ind)]
    x_complex_h = x_complex * h
    x_bj = tf.signal.ifft3d(x_complex_h[:, :, :1])
    x_bk = tf.signal.ifft3d(x_complex_h[:, :, 1:])
    x_hilbert = concatenate([tf.math.imag(x_bj), tf.math.imag(x_bk)], axis=-1)
    return x_hilbert
```


### Relevant log output

_No response_</details>"
60162,test,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

mate

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
test
```


### Relevant log output

```shell
test
```
</details>"
60159,Tired to convert Y tensor failed also convert none to tensor,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

Windows

### Mobile device

_No response_

### Python version

3.9

### Bazel version

no

### GCC/Compiler version

no

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
from tensorflow.python.keras.layers import Input, Dense
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.optimizers import Nadam
import numpy as np

ipt = Input(shape=(4,))
out = Dense(1, activation='sigmoid')(ipt)

model = Model(ipt, out)
model.compile(optimizer=Nadam(lr=1e-4), loss='binary_crossentropy')

X = np.random.randn(32,4)
Y = np.random.randint(0,2,(32,1))
model.train_on_batch(X,Y)from tensorflow.python.keras.layers import Input, Dense
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.optimizers import Nadam
import numpy as np

ipt = Input(shape=(4,))
out = Dense(1, activation='sigmoid')(ipt)

model = Model(ipt, out)
model.compile(optimizer=Nadam(lr=1e-4), loss='binary_crossentropy')

X = np.random.randn(32,4)
Y = np.random.randint(0,2,(32,1))
model.train_on_batch(X,Y)
```


### Relevant log output

```shell
File ""<ipython-input-1-2db039c052cf>"", line 20, in <module>
    model.train_on_batch(X,Y)
  File ""D:\Anaconda\envs\tf2_env\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 1017, in train_on_batch
    self._make_train_function()
  File ""D:\Anaconda\envs\tf2_env\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 2116, in _make_train_function
    params=self._collected_trainable_weights, loss=self.total_loss)
  File ""D:\Anaconda\envs\tf2_env\lib\site-packages\tensorflow_core\python\keras\optimizers.py"", line 653, in get_updates
    grads = self.get_gradients(loss, params)
  File ""D:\Anaconda\envs\tf2_env\lib\site-packages\tensorflow_core\python\keras\optimizers.py"", line 92, in get_gradients
    if None in grads:
  File ""D:\Anaconda\envs\tf2_env\lib\site-packages\tensorflow_core\python\ops\math_ops.py"", line 1336, in tensor_equals
    return gen_math_ops.equal(self, other)
  File ""D:\Anaconda\envs\tf2_env\lib\site-packages\tensorflow_core\python\ops\gen_math_ops.py"", line 3626, in equal
    name=name)
  File ""D:\Anaconda\envs\tf2_env\lib\site-packages\tensorflow_core\python\framework\op_def_library.py"", line 545, in _apply_op_helper
    (input_name, err))

ValueError: Tried to convert 'y' to a tensor and failed. Error: None values not supported.
```
</details>"
60158,DepthToSpace produce GPU address fault with Metal delegate,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

No

### OS Platform and Distribution

macOS Ventura 13.2.1

### Mobile device

iPhone12

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am testing 2x and 4x super-resolution using tflite on iphone using metal delegate.
The two models use almost the same network and determine the multiplier in DepthToSpace,
but 'GPU address fault' occurs at 4x.

--Execution of the command buffer was aborted due to an error during execution. Caused GPU Address Fault Error (0000000b:kIOGPUCommandBufferCallbackErrorPageFault)

When I checked the detailed log through the shader validation option, 'bad memory access (over access)' was occurring.

-- buffer: <unnamed>, length:4915200, resident:Read Write
-- [GPUDebug] Invalid device load executing kernel function ""ComputeFunction"" encoder: """", dispatch: 0, at offset 4915232
-- file:///program_source:35:0 - ComputeFunction()

The peculiarity is that the fault address offset is not constant.
(ex: 4915248, 4917808, 4918176, 4917792, 4917776, 4918240, 4915216, ...)

Here is the tflite I used
https://drive.google.com/drive/folders/1LAQjbJeXaeGQrhNc-fDiFRfsIyO5tGBR?usp=share_link
```


### Standalone code to reproduce the issue

```shell
It is 1-channel(luminance) super-resolution(2x, 4x) model with 320x240 resolution.
2x is working well, and 4x is not working. I/O type is float32
```


### Relevant log output

```shell
* with Metal API Validation option
2023-03-28 10:45:30.501062+0900 EditorDemo[2735:416512] TensorFlow Lite version : 2.11.0
2023-03-28 10:45:30.503471+0900 EditorDemo[2735:416512] Created TensorFlow Lite delegate for Metal.
INFO: Created TensorFlow Lite delegate for Metal.
2023-03-28 10:45:30.513366+0900 EditorDemo[2735:416512] Initialized TensorFlow Lite runtime.
INFO: Initialized TensorFlow Lite runtime.

2023-03-28 10:45:33.801156+0900 EditorDemo[2735:416822] Execution of the command buffer was aborted due to an error during execution. Caused GPU Address Fault Error (0000000b:kIOGPUCommandBufferCallbackErrorPageFault)

2023-03-28 10:45:33.802032+0900 EditorDemo[2735:416822] Execution of the command buffer was aborted due to an error during execution. Caused GPU Address Fault Error (0000000b:kIOGPUCommandBufferCallbackErrorPageFault)

* with Metal Shader Validation option
2023-03-28 10:49:39.145090+0900 EditorDemo[2747:418356] TensorFlow Lite version : 2.11.0
2023-03-28 10:49:39.145399+0900 EditorDemo[2747:418356] Created TensorFlow Lite delegate for Metal.
INFO: Created TensorFlow Lite delegate for Metal.
2023-03-28 10:49:39.146572+0900 EditorDemo[2747:418356] Initialized TensorFlow Lite runtime.
INFO: Initialized TensorFlow Lite runtime.

2023-03-28 10:49:45.131812+0900 EditorDemo[2747:418729] [GPUDebug] Invalid device load executing kernel function ""ComputeFunction"" encoder: """", dispatch: 0, at offset 4915232
file:///program_source:35:0 - ComputeFunction()
buffer: <unnamed>, length:4915200, resident:Read Write

2023-03-28 10:49:45.134406+0900 EditorDemo[2747:418729] [GPUDebug] Invalid device load executing kernel function ""ComputeFunction"" encoder: """", dispatch: 0, at offset 4915232
file:///program_source:35:0 - ComputeFunction()
buffer: <unnamed>, length:4915200, resident:Read Write
```
</details>"
60157,spam removed,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
60156,Tensorflow cann't detect or use CUDA but pytorch can in windows 11 and python 3.9.10,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.9.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.6r11.6/8.6

### GPU model and memory

_No response_

### Current Behaviour?

```shell
import tensorflow as tf
print(tf.test.is_build_with_cuda()) #  false is returned
tf.config.list_physical_devices('GPU') # [] is returned

import torch
torch.cuda.is_available() # true is returned
print(torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")) #'cuda:0' is returned
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
print(tf.test.is_build_with_cuda()) # result show false
tf.config.list_physical_devices('GPU') # result show []
```


### Relevant log output

_No response_</details>"
60154,[Feature Request] MatrixSquareRoot for tflite,"Tensorflow lite currently will not compile graphs containing `tf.linalg.sqrtm`.  If you try, you get 

```
Some ops in the model are custom ops, See instructions to implement custom ops: https://www.tensorflow.org/lite/guide/ops_custom 
Custom ops: MatrixSquareRoot
Details:
	tf.MatrixSquareRoot(tensor<3x3xf32>) -> (tensor<3x3xf32>) : {T = f32, device = """"}
```

As a work around, I am using an approximate solution:

```
def tf_denmann_beavers_sqrtm(matrix: TensorInvCovMat, n_iter=10):
    """"""
    Approximate the matrix-square-root by Denmann Beavers iteration
        https://en.wikipedia.org/wiki/Square_root_of_a_matrix#By_Denman%E2%80%93Beavers_iteration
    Convergence is not guaranteed.  Use at your own risk!
    """"""
    ym = matrix
    zm = tf.eye(tf.shape(matrix[0])[0], dtype=matrix.dtype)
    for i in range(n_iter):
        ym_ = 0.5 * (ym + tf.linalg.inv(zm))
        zm = 0.5 * (zm + tf.linalg.inv(ym))
        ym = ym_
    return ym
```

But it is not ideal.  

Can we please get `tf.linalg.sqrtm` included as a standard tflite op?  "
60153,Correct way to implement RNN if looping over tensor is yet not allowed,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11

### Custom Code

No

### OS Platform and Distribution

MaxOS 12.3.1 

### Mobile device

-

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

M1 Max 26 cores GPU 32 GB ram

### Current Behaviour?

```shell
it throws : Iterating over a symbolic `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature. yet the code in my opinion can be easily converted to a graph, just conditioned on the `tf.shape(inputs)[1]`... this would allow building RNNs that accepts long and short inputs, without having to pad it, which is extremely useful if the length of the strings in the dataset is very skewed, and we have many short phrases and few long ones
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
class MyModel1(tf.keras.Model):
    def __init__(self, input_text_processor, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.input_text_processor = input_text_processor
    def call(self, data, training=True, max_len=50):
        inputs = self.input_text_processor(data)
        for _ in tf.range(tf.shape(inputs)[1]):
            pass
        return []

    def train_step(self, data):
        self.call(data)
        return {""loss"" : 0}
dataset = [
    ""hi"", ""what's up"", ""what's the weather""
]
input_text_processor = tf.keras.layers.TextVectorization()
input_text_processor.adapt(dataset)
with tf.device(""/CPU:0""):
    model = MyModel1(input_text_processor)
    model.compile(tf.optimizers.Adam(), loss=tf.keras.losses.SparseCategoricalCrossentropy())
    hist = model.fit(dataset, epochs=5)
```


### Relevant log output

```shell
Epoch 1/5
Traceback (most recent call last):
  File ""/Users/username/ml/tensorflow-journey/39-rnn-enc-dec-attention/rnn-enc-dec-attention.py"", line 23, in <module>
    hist = model.fit(dataset, epochs=5)
  File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal-3-10/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal-3-10/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py"", line 1269, in autograph_handler
    raise e.ag_error_metadata.to_exception(e)
tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: in user code:

    File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal-3-10/lib/python3.10/site-packages/keras/engine/training.py"", line 1249, in train_function  *
        return step_function(self, iterator)
    File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal-3-10/lib/python3.10/site-packages/keras/engine/training.py"", line 1233, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal-3-10/lib/python3.10/site-packages/keras/engine/training.py"", line 1222, in run_step  **
        outputs = model.train_step(data)
    File ""/Users/username/ml/tensorflow-journey/39-rnn-enc-dec-attention/rnn-enc-dec-attention.py"", line 13, in train_step
        self.call(data)
    File ""/Users/username/ml/tensorflow-journey/39-rnn-enc-dec-attention/rnn-enc-dec-attention.py"", line 8, in call
        for _ in tf.range(tf.shape(inputs)[1]):

    OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.
```
</details>"
60152,weighted_metrics argument for compile() doesn't account for weights,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12.0-dev20221213

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.7.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When using weighted_metrics, values that would be expected with no weighting are seen.

I expected that when I use sample_weight in fit() and pass the same function to the ""metrics"" and ""weighted_metrics"" in compile(), the scores would differ, but they are the same.
```


### Standalone code to reproduce the issue

```shell
import os
# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import numpy as np
import tensorflow as tf

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

# add channel axis
x_train = x_train[..., np.newaxis]
x_test = x_test[..., np.newaxis]

# subsample
train_idx = np.random.choice(len(x_train), 1000, replace=0)
test_idx = np.random.choice(len(x_test), 1000, replace=0)
x_train = x_train[train_idx]
x_test = x_test[test_idx]

# convert 1-hot
y_train = tf.one_hot(y_train[train_idx], 10)
y_test = tf.one_hot(y_test[test_idx], 10)

# define the model
def get_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Input((28, 28, 1)),
        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(32, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    return model

##################### first, no weighting #####################

# initialize model
model = get_model()

# compile
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['categorical_crossentropy'],
              weighted_metrics=['categorical_crossentropy'])

# train the model
history = model.fit(x_train, y_train,
                    validation_data = (x_train, y_train),
                    epochs=5,
                    steps_per_epoch=5,
                    verbose=0)
h = history.history
print('No weighting:')
print('final_loss:',h['loss'][-1])
print('final_categorical_crossentropy ():',h['categorical_crossentropy'][-1])
print('final_weighted_categorical_crossentropy ():',h['weighted_categorical_crossentropy'][-1])



##################### add weights #####################

# categorical_crossentropy should differ from weighted_categorical_crossentropy



# define sample weights
sample_weights = np.ones((len(x_train), 1))*2
val_sample_weights = np.ones((len(x_test), 1))*2

# initialize model
model = get_model()

# compile
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['categorical_crossentropy'],
              weighted_metrics=['categorical_crossentropy'])

# train the model
history = model.fit(x_train, y_train,
                    sample_weight = sample_weights,
                    validation_data = (x_train, y_train, val_sample_weights),
                    epochs=5,
                    steps_per_epoch=5,
                    verbose=0)
h = history.history
print('\nWith Weighting:')
print('final_loss:',h['loss'][-1])
print('final_categorical_crossentropy ():',h['categorical_crossentropy'][-1])
print('final_weighted_categorical_crossentropy ():',h['weighted_categorical_crossentropy'][-1])
```


### Relevant log output

```shell
No weighting:
final_loss: 2.071871042251587
final_categorical_crossentropy (): 2.071871042251587
final_weighted_categorical_crossentropy (): 2.071871042251587

With Weighting:
final_loss: 3.7742838859558105
final_categorical_crossentropy (): 1.8871419429779053
final_weighted_categorical_crossentropy (): 1.8871419429779053
```
</details>"
60151,Build TensorFlow Lite for iOS failed!!!!,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. `bazel build --config=ios_arm64 -c opt --cxxopt=--std=c++17 \
  //tensorflow/lite/ios:TensorFlowLiteC_framework
❯ bazel build --incompatible_run_shell_command_string=false --verbose_failures --config=ios_arm64 -c opt //tensorflow/lite/ios:TensorFlowLiteCMetal_framework
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=170
INFO: Reading rc options for 'build' from /Users/thao/Desktop/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /Users/thao/Desktop/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false
INFO: Reading rc options for 'build' from /Users/thao/Desktop/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/Users/thao/miniforge3/bin/python --action_env PYTHON_LIB_PATH=/Users/thao/miniforge3/lib/python3.10/site-packages --python_path=/Users/thao/miniforge3/bin/python
INFO: Reading rc options for 'build' from /Users/thao/Desktop/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /Users/thao/Desktop/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /Users/thao/Desktop/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:ios_arm64 in file /Users/thao/Desktop/tensorflow/.bazelrc: --config=ios --cpu=ios_arm64
INFO: Found applicable config definition build:ios in file /Users/thao/Desktop/tensorflow/.bazelrc: --apple_platform_type=ios --apple_bitcode=embedded --copt=-fembed-bitcode --copt=-Wno-c++11-narrowing --noenable_platform_specific_config --copt=-w --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --define=with_xla_support=false
INFO: Build option --cxxopt has changed, discarding analysis cache.
ERROR: /private/var/tmp/_bazel_thao/26d40dc75f2c247e7283b353a9ab184f/external/local_config_cc/BUILD:48:19: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'ios_arm64'
ERROR: /private/var/tmp/_bazel_thao/26d40dc75f2c247e7283b353a9ab184f/external/local_config_cc/BUILD:48:19: Analysis of target '@local_config_cc//:toolchain' failed
ERROR: Analysis of target '//tensorflow/lite/ios:TensorFlowLiteCMetal_framework' failed; build aborted: 
INFO: Elapsed time: 45.455s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (66 packages loaded, 1118 targets configured)`

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
MacOS-M1Max : 13.3
Tensorflow:2.9.2
Python: 3.10.0



### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
60150,Cannot install tensorflow modules use pip,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Darwin
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: N/A
-   **TensorFlow installed from (source or binary)**: `pip3 install tf-nightly`
-   **TensorFlow version (use command below)**: `2.13.0.dev20230325`
-   **Python version**: `Python 3.11.2`
-   **Bazel version (if compiling from source)**: no bazel installed
-   **GCC/Compiler version (if compiling from source)**: Not compiling from source
  ```bash
  $ clang --version
  Apple clang version 13.1.6 (clang-1316.0.21.2.3)
  Target: arm64-apple-darwin21.2.0
  Thread model: posix
  InstalledDir: /Library/Developer/CommandLineTools/usr/bin
  ```
-   **CUDA/cuDNN version**: 
-   **GPU model and memory**: 16GB memory. Mac M1 built in GPU?
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh
P.S: I don't think this script work, because it gave me my python2 version, not python3

You can obtain the TensorFlow version with:

```bash
$ python3 -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
v1.12.1-91636-g1057ad694db 2.13.0-dev20230325
```

### Describe the problem

I am following this document: <https://www.tensorflow.org/text/tutorials/classify_text_with_bert#setup>

So when I try to install tensorflow, it gave me mo matching distribution found errors.

### Source code / logs

```bash
$ pip3 install tensorflow                                                                                               
ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensorflow
```
```bash
$ pip3 install -q -U ""tensorflow-text==2.11.*""
ERROR: Could not find a version that satisfies the requirement tensorflow-text==2.11.* (from versions: none)
ERROR: No matching distribution found for tensorflow-text==2.11.*
$ pip3 install -q tf-models-official==2.11.0
ERROR: Could not find a version that satisfies the requirement opencv-python-headless==4.5.2.52 (from tf-models-official) (from versions: 3.4.10.37, 3.4.11.39, 3.4.11.41, 3.4.11.43, 3.4.11.45, 3.4.13.47, 3.4.15.55, 3.4.16.59, 3.4.17.61, 3.4.17.63, 3.4.18.65, 4.3.0.38, 4.4.0.40, 4.4.0.42, 4.4.0.44, 4.4.0.46, 4.5.1.48, 4.5.3.56, 4.5.4.58, 4.5.4.60, 4.5.5.62, 4.5.5.64, 4.6.0.66, 4.7.0.68, 4.7.0.72)
ERROR: No matching distribution found for opencv-python-headless==4.5.2.52
```"
60149,tf.data.Dataset.from_generator crashes with abortion,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.13.0-dev20230208

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tf.data.Dataset.from_generator crashes with abortion
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
ds = tf.data.Dataset.from_tensors([1]).repeat(-1)
def gen():
  for _ in ds:
    yield _
ds = tf.data.Dataset.from_generator(
    gen, output_types=tf.int32)
list(ds.take(2).as_numpy_iterator())
```


### Relevant log output

```shell
2023-03-28 12:06:28.440209: F tensorflow/tsl/platform/default/env.cc:74] Check failed: ret == 0 (11 vs. 0)Thread tf_data_private_threadpool creation via pthread_create() failed.
Aborted (core dumped)
```
</details>"
60148,TensorArray.gather crash with Segmentation fault,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.13.0-dev20230208

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
TensorArray.gather crash with Segmentation fault
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

tf = tf.compat.v2
tf.enable_v2_behavior()

@tf.function
def foo():
    ta = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)
    ta.write(0, tf.constant(0))
    return ta.gather([0])

foo()
```


### Relevant log output

```shell
Segmentation fault (core dumped)
```
</details>"
60147,tf.raw_ops.ResourceScatterUpdate crash with abortion,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.13.0-dev20230208

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tf.raw_ops.ResourceScatterUpdate crash with abortion
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

init = np.random.rand(20)
update = np.random.rand(20)

resource = tf.Variable(init, dtype=tf.float32)
resource_var = resource.handle
indices = np.array([1, 3, 5], dtype=np.int32)
tf.raw_ops.ResourceScatterUpdate(resource=resource_var, indices=indices, updates=update)
```


### Relevant log output

```shell
2023-03-28 11:39:22.735062: F tensorflow/core/framework/tensor.cc:770] Check failed: dtype() == expected_dtype (1 vs. 2) double expected, got float
Aborted (core dumped)
```
</details>"
60144,GPU install error ,"I am using wsl2 
GPU Quadro p1000

When installing TensorFlow 2.12 through pip and conda (https://www.tensorflow.org/install/pip) after installing that build was cpu (onednn) don't use cuda (cudnn) and the error said 1) Don't have cuda driver 2)Don't open tensorrt lib 

I have tried 3 times but every time build cpu based not gpu based.

"
60141,ImportError: Traceback,"I have this error after installing Tensorflow, when I try to import it.

ImportError                               Traceback (most recent call last)
File ~\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py:62
     61 try:
---> 62   from tensorflow.python._pywrap_tensorflow_internal import *
     63 # This try catch logic is because there is no bazel equivalent for py_extension.
     64 # Externally in opensource we must enable exceptions to load the shared object
     65 # by exposing the PyInit symbols with pybind. This error will only be
     66 # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.
     67 
     68 # This logic is used in other internal projects using py_extension.

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: Routine di inizializzazione della libreria di collegamento dinamico (DLL) non riuscita.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
Cell In[1], line 1
----> 1 import tensorflow as tf

File ~\anaconda3\lib\site-packages\tensorflow\__init__.py:37
     34 import sys as _sys
     35 import typing as _typing
---> 37 from tensorflow.python.tools import module_util as _module_util
     38 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     40 # Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.

File ~\anaconda3\lib\site-packages\tensorflow\python\__init__.py:36
     27 import traceback
     29 # We aim to keep this file minimal and ideally remove completely.
     30 # If you are adding a new file with @tf_export decorators,
     31 # import it in modules_with_exports.py instead.
     32 
     33 # go/tf-wildcard-import
     34 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top
---> 36 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
     37 from tensorflow.python.eager import context
     39 # pylint: enable=wildcard-import
     40 
     41 # Bring in subpackages.

File ~\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py:77
     75     sys.setdlopenflags(_default_dlopen_flags)
     76 except ImportError:
---> 77   raise ImportError(
     78       f'{traceback.format_exc()}'
     79       f'\n\nFailed to load the native TensorFlow runtime.\n'
     80       f'See https://www.tensorflow.org/install/errors '
     81       f'for some common causes and solutions.\n'
     82       f'If you need help, create an issue '
     83       f'at https://github.com/tensorflow/tensorflow/issues '
     84       f'and include the entire stack trace above this error message.')

ImportError: Traceback (most recent call last):
  File ""C:\Users\user\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: Routine di inizializzazione della libreria di collegamento dinamico (DLL) non riuscita.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
"
60140,Internal error on Attention over LSTM with ragged input,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04 LTS on Windows WSL2

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.7/8.5

### GPU model and memory

_No response_

### Current Behaviour?

```shell
No problem if LSTM layer is skipped. However, if LSTM (or GRU) layer is included, got an internal error.
```


### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

# -----------------------------------------------------------------------------

samples = [[[0.1], [0.2], [0.3]],
           [[0.4], [0.5], [0.6], [0.7]],
           [[0.8], [0.9]]]

targets = np.array([[0.25], [0.50], [0.75]], dtype=np.float32)

# -----------------------------------------------------------------------------

row_lengths = [len(s) for s in samples]

samples = np.concatenate(samples, axis=0)

samples = tf.RaggedTensor.from_row_lengths(samples, row_lengths)

# -----------------------------------------------------------------------------

layer_1 = tf.keras.layers.Input(shape=(None, 1), dtype=tf.float32, ragged=True)

layer_2 = tf.keras.layers.LSTM(1, return_sequences=True)(layer_1)

layer_3 = tf.keras.layers.Attention()([layer_2, layer_2])

layer_4 = tf.keras.layers.Lambda(lambda x: tf.reduce_sum(x, axis=1))(layer_3)

model = tf.keras.models.Model([layer_1], [layer_4])

model.summary()

# -----------------------------------------------------------------------------

model.compile(optimizer='adam', loss='mse')

model.fit(samples, targets, batch_size=3, epochs=1, verbose=1)
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/home/user/model.py"", line 38, in <module>
    model.fit(samples, targets, batch_size=3, epochs=1, verbose=1)
  File ""/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py"", line 52, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InternalError: Graph execution error:

Detected at node 'zeros_like_5' defined at (most recent call last):
    File ""/home/user/model.py"", line 38, in <module>
      model.fit(samples, targets, batch_size=3, epochs=1, verbose=1)
    File ""/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.10/dist-packages/keras/engine/training.py"", line 1650, in fit
      tmp_logs = self.train_function(iterator)
    File ""/usr/local/lib/python3.10/dist-packages/keras/engine/training.py"", line 1249, in train_function
      return step_function(self, iterator)
    File ""/usr/local/lib/python3.10/dist-packages/keras/engine/training.py"", line 1233, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/usr/local/lib/python3.10/dist-packages/keras/engine/training.py"", line 1222, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.10/dist-packages/keras/engine/training.py"", line 1027, in train_step
      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File ""/usr/local/lib/python3.10/dist-packages/keras/optimizers/optimizer_experimental/optimizer.py"", line 526, in minimize
      grads_and_vars = self.compute_gradients(loss, var_list, tape)
    File ""/usr/local/lib/python3.10/dist-packages/keras/optimizers/optimizer_experimental/optimizer.py"", line 259, in compute_gradients
      grads = tape.gradient(loss, var_list)
Node: 'zeros_like_5'
Detected at node 'zeros_like_5' defined at (most recent call last):
    File ""/home/user/model.py"", line 38, in <module>
      model.fit(samples, targets, batch_size=3, epochs=1, verbose=1)
    File ""/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.10/dist-packages/keras/engine/training.py"", line 1650, in fit
      tmp_logs = self.train_function(iterator)
    File ""/usr/local/lib/python3.10/dist-packages/keras/engine/training.py"", line 1249, in train_function
      return step_function(self, iterator)
    File ""/usr/local/lib/python3.10/dist-packages/keras/engine/training.py"", line 1233, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/usr/local/lib/python3.10/dist-packages/keras/engine/training.py"", line 1222, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.10/dist-packages/keras/engine/training.py"", line 1027, in train_step
      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File ""/usr/local/lib/python3.10/dist-packages/keras/optimizers/optimizer_experimental/optimizer.py"", line 526, in minimize
      grads_and_vars = self.compute_gradients(loss, var_list, tape)
    File ""/usr/local/lib/python3.10/dist-packages/keras/optimizers/optimizer_experimental/optimizer.py"", line 259, in compute_gradients
      grads = tape.gradient(loss, var_list)
Node: 'zeros_like_5'
2 root error(s) found.
  (0) INTERNAL:  No unary variant unary_op function found for op ZEROS_LIKE Variant type_name: RaggedTensorVariant for device type: GPU
         [[{{node zeros_like_5}}]]
         [[model/attention/RaggedSoftmax/RowPartitionFromRowLengths_2/assert_non_negative/assert_less_equal/Assert/AssertGuard/pivot_f/_168/_379]]
  (1) INTERNAL:  No unary variant unary_op function found for op ZEROS_LIKE Variant type_name: RaggedTensorVariant for device type: GPU
         [[{{node zeros_like_5}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_10894]
```
</details>"
60137,Issue in TF Docs and it's tools with dataclasses module in Python 3.6 and earlier,"Currently, the TensorFlow documentation tools requires using the `dataclasses` module. However, the `dataclasses` module is not included in the standard library in Python 3.6 and earlier, which can cause compatibility issues for users of these versions of Python.
https://github.com/tensorflow/docs/blob/master/setup.py#L36:L38
```python
# Dataclasses is in-built from py >=3.7. This version is a backport for py 3.6.
if (sys.version_info.major, sys.version_info.minor) == (3, 6):
  REQUIRED_PKGS.append('dataclasses')
```
But in the **nbfmt** tool  the `notebook_utils.py` file,
https://github.com/tensorflow/docs/blob/master/tools/tensorflow_docs/tools/nbfmt/notebook_utils.py#L119:L124
```python
@dataclasses.dataclass
class CellCopyStats:
  processed_cells: int = 0
  updated_cells: int = 0
  unmatched_target_cells: list[str] = dataclasses.field(default_factory=list)
  unmatched_source_cells: list[str] = dataclasses.field(default_factory=list)
  out_of_order_target_cells: list[str] = dataclasses.field(default_factory=list)
```
 `list` is used instead of the `typing.List` which will cause `TypeError: 'type' object is not subscriptable`, I think `typing.List` should be used to ensure backward compatibility.
 
To avoid these issues, the documentation should :
1.  use the `typing.List` class instead of `list` to ensure backward compatibility with older versions of Python.
2.  Require the installation of the `dataclasses` package for users of Python 3.6 and earlier, instead of requiring it for python 3.6 only, like the following: 
```python 
import sys

if sys.version_info < (3, 7):
    REQUIRED_PKGS.append('dataclasses')
```
**I'm ready to start fixing it in a PR.**"
60136,The Windows MSVC to clang migration linker issue,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.12

### Custom Code

No

### OS Platform and Distribution

Microsoft Windows Server 2019 Datacenter

### Mobile device

_No response_

### Python version

Python 3.10

### Bazel version

5.3.0

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

#### Background
We have switch the TensorFlow compilation from MSVC to clang-cl and resolved two compile errors. The compilation is complete but there is a link issue before we can complete package TensorFlow to wheel.

#### How to reproduce
1. Fix the const expression error
    a. Add file to third_party\tf_runtime_clangcl.patch with content 
```shell
diff --git a/include/tfrt/support/std_mutex.h b/include/tfrt/support/std_mutex.h
index 6238d097..9fb24279 100644
--- a/include/tfrt/support/std_mutex.h
+++ b/include/tfrt/support/std_mutex.h
@@ -50,7 +50,7 @@ class TFRT_CAPABILITY(""mutex"") mutex {
  
  private:
   friend class mutex_lock;
-  std::mutex mu_;
+  std::mutex mu_{};
 };
 
 // Wrap std::unique_lock<std::mutex> with support for thread annotations.
```

    b. Update workspace file at third_party\tf_runtime\workspace.bzl at line 19.

```shell
patch_file = [""//third_party:tf_runtime_clangcl.patch""],
```

2. Bypass the Google ABSL compilation error
    a. Create file at third_party\absl\comd_google_absl_remove_static_assert.patch

```shell
diff --git a/absl/meta/type_traits.h b/absl/meta/type_traits.h
index d886cb30..819f87b4 100644
--- a/absl/meta/type_traits.h
+++ b/absl/meta/type_traits.h
@@ -495,9 +495,7 @@ struct is_trivially_copy_assignable
                     absl::is_copy_assignable<T>::value> {
 #ifdef ABSL_HAVE_STD_IS_TRIVIALLY_ASSIGNABLE
  private:
-  static constexpr bool compliant =
-      std::is_trivially_copy_assignable<T>::value ==
-      is_trivially_copy_assignable::value;
+  static constexpr bool compliant = true;
   static_assert(compliant || std::is_trivially_copy_assignable<T>::value,
                 ""Not compliant with std::is_trivially_copy_assignable; ""
                 ""Standard: false, Implementation: true"");
```

    b. Modify the file at third_party\absl\workspace.bzl at line 46

```shell
patch_file = [""//third_party/absl:com_google_absl_fix_mac_and_nvcc_build.patch"", ""//third_party/absl:comd_google_absl_remove_static_assert.patch""],
```


### Standalone code to reproduce the issue

#### Build with command 
```shell
/> bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures --compiler=clang-cl
```


### Relevant log output

```shell
\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures --compiler=clang-cl --copt=/clang:-Weverything  --config=windows
WARNING: Ignoring JAVA_HOME, because it must point to a JDK, not a JRE.
WARNING: The following configs were expanded more than once: [monolithic]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=189
INFO: Reading rc options for 'build' from d:\...\msvc_to_clang\tensorflow\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=D:/.../msvc_to_clang/venv310/Scripts/python.exe
INFO: Reading rc options for 'build' from d:\...\msvc_to_clang\tensorflow\.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from d:\...\msvc_to_clang\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=D:/.../msvc_to_clang/venv310/Scripts/python.exe --action_env PYTHON_LIB_PATH=D:/.../msvc_to_clang/venv310/lib/site-packages --python_path=D:/.../msvc_to_clang/venv310/Scripts/python.exe --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true --define=tf_use_clang_cl_instead_of_msvc=true
INFO: Reading rc options for 'build' from d:\...\msvc_to_clang\tensorflow\.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file d:\...\msvc_to_clang\tensorflow\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file d:\...\msvc_to_clang\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:opt in file d:\...\msvc_to_clang\tensorflow\.tf_configure.bazelrc: --copt=/arch:AVX --host_copt=/arch:AVX
INFO: Found applicable config definition build:windows in file d:\...\msvc_to_clang\tensorflow\.bazelrc: --copt=/W0 --host_copt=/W0 --copt=/Zc:__cplusplus --host_copt=/Zc:__cplusplus --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --features=compiler_param_file --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --cxxopt=/std:c++17 --host_cxxopt=/std:c++17 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/Zc:preprocessor --host_copt=/Zc:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file
INFO: Found applicable config definition build:monolithic in file d:\...\msvc_to_clang\tensorflow\.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false
INFO: Found applicable config definition build:windows in file d:\...\msvc_to_clang\tensorflow\.bazelrc: --copt=/W0 --host_copt=/W0 --copt=/Zc:__cplusplus --host_copt=/Zc:__cplusplus --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --features=compiler_param_file --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --cxxopt=/std:c++17 --host_cxxopt=/std:c++17 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/Zc:preprocessor --host_copt=/Zc:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file
INFO: Found applicable config definition build:monolithic in file d:\...\msvc_to_clang\tensorflow\.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (594 packages loaded, 33130 targets configured).
INFO: Found 1 target...
ERROR: D:/.../msvc_to_clang/tensorflow/tensorflow/distribute/experimental/rpc/kernels/BUILD:60:21: Linking tensorflow/distribute/experimental/rpc/kernels/gen_gen_rpc_ops_py_wrappers_cc.exe failed: (Exit 1): lld-link.exe failed: error executing command
  cd /d C:/users/...sha/_bazel_...sha/mlvocmwh/execroot/org_tensorflow
  SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\MSVC\14.29.30133\lib\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.19041.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.19041.0\um\x64;d:\...\msvc_to_clang\LLVM\lib\clang\15.0.6\lib\windows
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\MSVC\14.29.30133\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\Tools\devinit;C:\Program Files (x86)\Windows Kits\10\bin\10.0.19041.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\\MSBuild\Current\Bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\Tools\;;C:\Windows\system32;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=D:/.../msvc_to_clang/venv310/Scripts/python.exe
    SET PYTHON_LIB_PATH=D:/.../msvc_to_clang/venv310/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\...sha\AppData\Local\Temp\2
    SET TF2_BEHAVIOR=1
    SET TMP=C:\Users\...sha\AppData\Local\Temp\2
  d:\...\msvc_to_clang\LLVM\bin\lld-link.exe @bazel-out/x64_windows-opt/bin/tensorflow/distribute/experimental/rpc/kernels/gen_gen_rpc_ops_py_wrappers_cc.exe-2.params
# Configuration: cfe8788e4ffcaa7fd26e4e99620edbfd250b250962129096b14aa1fc721dc89b
# Execution platform: @local_execution_config_platform//:platform
lld-link: warning: ignoring unknown argument '-lm'
lld-link: warning: ignoring unknown argument '-lpthread'
lld-link: warning: ignoring unknown argument '-lm'
lld-link: warning: ignoring unknown argument '-lpthread'
lld-link: warning: ignoring unknown argument '-lm'
lld-link: warning: allocator_registry_impl.lo.lib(cpu_allocator_impl.obj): locally defined symbol imported: struct std::atomic<int> tsl::profiler::internal::g_trace_level (defined in traceme_recorder_impl.lo.lib(traceme_recorder.obj)) [LNK4217]
lld-link: warning: utils.lib(utils.obj): locally defined symbol imported: char const *const tensorflow::DEVICE_CPU (defined in tensor.lo.lib(types.obj)) [LNK4217]
lld-link: warning: utils.lib(utils.obj): locally defined symbol imported: char const *const tensorflow::DEVICE_GPU (defined in tensor.lo.lib(types.obj)) [LNK4217]
lld-link: warning: memory_optimizer.lib(memory_optimizer.obj): locally defined symbol imported: char const *const tensorflow::DEVICE_CPU (defined in tensor.lo.lib(types.obj)) [LNK4217]
lld-link: warning: memory_optimizer.lib(memory_optimizer.obj): locally defined symbol imported: char const *const tensorflow::DEVICE_GPU (defined in tensor.lo.lib(types.obj)) [LNK4217]
lld-link: warning: arithmetic_optimizer.lib(arithmetic_optimizer.obj): locally defined symbol imported: char const *const tensorflow::DEVICE_CPU (defined in tensor.lo.lib(types.obj)) [LNK4217]
lld-link: warning: arithmetic_optimizer.lib(arithmetic_optimizer.obj): locally defined symbol imported: char const *const tensorflow::DEVICE_GPU (defined in tensor.lo.lib(types.obj)) [LNK4217]
lld-link: warning: pin_to_host_optimizer.lib(pin_to_host_optimizer.obj): locally defined symbol imported: char const *const tensorflow::DEVICE_CPU (defined in tensor.lo.lib(types.obj)) [LNK4217]
lld-link: warning: pin_to_host_optimizer.lib(pin_to_host_optimizer.obj): locally defined symbol imported: char const *const tensorflow::DEVICE_GPU (defined in tensor.lo.lib(types.obj)) [LNK4217]
lld-link: warning: gpu_id_impl.lib(gpu_id_manager.obj): locally defined symbol imported: char const *const tensorflow::DEVICE_GPU (defined in tensor.lo.lib(types.obj)) [LNK4217]
lld-link: warning: bfc_allocator.lib(bfc_allocator.obj): locally defined symbol imported: struct std::atomic<int> tsl::profiler::internal::g_trace_level (defined in traceme_recorder_impl.lo.lib(traceme_recorder.obj)) [LNK4217]
lld-link: warning: Pass.lib(pass.obj): locally defined symbol imported: char const *const tensorflow::DEVICE_CPU (defined in tensor.lo.lib(types.obj)) [LNK4217]
lld-link: warning: Pass.lib(pass.obj): locally defined symbol imported: char const *const tensorflow::DEVICE_GPU (defined in tensor.lo.lib(types.obj)) [LNK4217]
lld-link: warning: pdll_utils.lib(utils.obj): locally defined symbol imported: char const *const tensorflow::DEVICE_CPU (defined in tensor.lo.lib(types.obj)) [LNK4217]
lld-link: error: undefined symbol: struct mlir::LogicalResult __cdecl mlir::tfg::InferReturnTypeComponentsForTFOp(class std::optional<class mlir::Location>, class mlir::Operation *, class mlir::ValueRange, __int64, class llvm::function_ref<class mlir::Attribute __cdecl(class mlir::Value)>, class llvm::function_ref<class tensorflow::shape_inference::ShapeHandle __cdecl(class tensorflow::shape_inference::InferenceContext &, class mlir::OpResult)>, class llvm::function_ref<class mlir::Type __cdecl(int)>, class llvm::function_ref<class tsl::Status __cdecl(class mlir::Operation *, class llvm::StringRef, class tensorflow::OpRegistrationData const *, bool, class google::protobuf::Map<class std::basic_string<char, struct std::char_traits<char>, class std::allocator<char>>, class tensorflow::AttrValue> *)>, class llvm::SmallVectorImpl<class mlir::ShapedTypeComponents> &)
>>> referenced by Pass.lib(pass.obj):(public: __cdecl `public: virtual void __cdecl mlir::tfg::ShapeInference::runOnOperation(void)'::`1'::<lambda_5>::operator()(class mlir::Operation *) const)
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 1397.717s, Critical Path: 378.14s
INFO: 10556 processes: 821 internal, 9735 local.
FAILED: Build did NOT complete successfully
```
</details>"
60135,Make time series_from_array() more intuitive to use,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.10.0 (Tensorflow-macos)

### Custom Code

Yes

### OS Platform and Distribution

MacOS 13.2.1

### Mobile device

_No response_

### Python version

3.10.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

TimeseriesGenerator() is deprecated, and Tensorflow docs encourage the use of time series_from_array() instead. However, this is not intuitive to use, requiring far more boilerplate code to achieve the same effect.

In addition, the results are not as expected. I spent hours debugging my code to realise time series_from_array() is not behaving as expected. Using the code below, I would expect there to be 6 different inputs and outputs, however, there are only 3. Running the same code with TimeseriesGenerator(), without the [:-4] and [4:] indexing, produces the expected 6 inputs and outputs.


### Standalone code to reproduce the issue

```shell
x = np.zeros((10, 3))
y = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])

test = keras.utils.timeseries_dataset_from_array(x[:-4], y[4:], sequence_length=4, batch_size=2000)
list(test.as_numpy_iterator())
```


### Relevant log output

```shell
[(array([[[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],
  
         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]],
  
         [[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]]]),
  array([5, 6, 7]))]
```

### Code for TimeseriesGenerator() (expected output)

```Python
test = keras.preprocessing.sequence.TimeseriesGenerator(x, y, length=4)
test[0]
```

### Expected output

```shell
(array([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]],
 
        [[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]],
 
        [[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]],
 
        [[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]],
 
        [[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]],
 
        [[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]]),
 array([ 5,  6,  7,  8,  9, 10]))
```

</details>"
60133,test,
60131,"Memory leak in forward pass (e.g., of ResNet50 model) with TensorFlow 2.12.0 and Python 3.11","The following minimal example reproduces the memory leak I ran into. (No GPU, just CPU.)

`memleak.py`:

```python3
import numpy as np
import psutil
import tensorflow as tf

model = tf.keras.applications.ResNet50()  # VGG19 seems to not leak.

# tf.config.threading.set_inter_op_parallelism_threads(0) and tf.config.threading.set_intra_op_parallelism_threads(0) do not help.

inp = (np.random.rand(1, 224, 224, 3) * 255).astype('uint8')

for run in range(1, 9999999):
    model(inp)
    memory_usage_in_MiB = psutil.Process().memory_info().rss / (1024 * 1024)
    print(f'Memory usage after {run} run(s) (in MiB): {memory_usage_in_MiB:.3f}', flush=True)
```

`Dockerfile`:
```Dockerfile
FROM python:3.11.2

RUN pip install --no-cache-dir tensorflow==2.12.0 psutil==5.9.4

# Disable the Docker cache from this stage on, see https://stackoverflow.com/a/58801213/1866775
ADD ""https://www.random.org/cgi-bin/randbyte?nbytes=10&format=h"" skipcache

ADD ./memleak.py /
RUN python /memleak.py
```

[Output](https://gist.github.com/Dobiasd/ba800b40e117aa13d97deb44761888f6) (`docker build --rm .`):
```
Memory usage after 1 run(s) (in MiB): 604.324
Memory usage after 2 run(s) (in MiB): 606.906
Memory usage after 3 run(s) (in MiB): 606.906
Memory usage after 4 run(s) (in MiB): 606.906
Memory usage after 5 run(s) (in MiB): 606.906
Memory usage after 6 run(s) (in MiB): 607.164
Memory usage after 7 run(s) (in MiB): 607.164
Memory usage after 8 run(s) (in MiB): 607.164
Memory usage after 9 run(s) (in MiB): 607.164
Memory usage after 10 run(s) (in MiB): 607.164
Memory usage after 11 run(s) (in MiB): 607.422
Memory usage after 12 run(s) (in MiB): 607.422
[...]
Memory usage after 498 run(s) (in MiB): 626.242
Memory usage after 499 run(s) (in MiB): 626.242
Memory usage after 500 run(s) (in MiB): 626.242
Memory usage after 501 run(s) (in MiB): 626.500
Memory usage after 502 run(s) (in MiB): 626.500
[...]
[...]
Memory usage after 1996 run(s) (in MiB): 683.477
Memory usage after 1997 run(s) (in MiB): 683.734
Memory usage after 1998 run(s) (in MiB): 683.734
Memory usage after 1999 run(s) (in MiB): 683.734
Memory usage after 2000 run(s) (in MiB): 683.734
Memory usage after 2001 run(s) (in MiB): 683.734
[...]
Memory usage after 9996 run(s) (in MiB): 960.258
Memory usage after 9997 run(s) (in MiB): 960.508
Memory usage after 9998 run(s) (in MiB): 960.508
Memory usage after 9999 run(s) (in MiB): 960.508
Memory usage after 10000 run(s) (in MiB): 960.508
Memory usage after 10001 run(s) (in MiB): 960.508
[...]
Memory usage after 24997 run(s) (in MiB): 1547.840
Memory usage after 24998 run(s) (in MiB): 1547.840
Memory usage after 24999 run(s) (in MiB): 1534.230
Memory usage after 25000 run(s) (in MiB): 1532.348
Memory usage after 25001 run(s) (in MiB): 1533.441
Memory usage after 25002 run(s) (in MiB): 1544.711
[...]
```

When using the same TensorFlow version (`2.12.0`) but with Python `3.10.10` (instead of `3.11.2`), the memory usage does not grow."
60130,URL fetch failure when tried to fetch the pretrained neural nets (ResNet) stored in tensorflow,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.11

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04, Linux 5.4.0

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hi,

When I tried to import the pre-trained ResNet in tensorflow, it always gave me 403 forbidden error which I have no clue to solve. What might cause this error?
```


### Standalone code to reproduce the issue

```shell
The code is:

base_model = tf.keras.applications.ResNet50(weights = 'imagenet', include_top = False, input_shape = (512,512,3))
for layer in base_model.layers:
    layer.trainable = False
```


### Relevant log output

_No response_</details>"
60128,tf.distribute.MultiWorkerMirroredStrategy does not replicate tf.Variable among workers,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.9

### Custom Code

Yes

### OS Platform and Distribution

tensorflow/tensorflow:2.11.0

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A variable is created with `synchronization=tf.VariableSynchronization.ON_WRITE`. But if `.assign(1)` is called the value is not distributed among workers.

Expected behavior: the variable is set to `1` on all workers.

There might be a lack of understanding on my side on how the distribution is working.
```


### Standalone code to reproduce the issue

```shell
`src/minimal-example.py`:


#!/usr/bin/env python3
# COPYRIGHT HSLU 2023 PFAEFFLI, ALL RIGHTS RESERVED
import json
import logging
import os
import time
from pathlib import Path

import tensorflow as tf
os.environ[
    ""CUDA_VISIBLE_DEVICES""
] = ""-1""  # Prevent that any GPUs are allocated unintendendly


def _is_chief(task_type, task_id, cluster_spec):
    return (
        task_type is None
        or task_type == ""chief""
        or (
            task_type == ""worker""
            and task_id == 0
            and ""chief"" not in cluster_spec.as_dict()
        )
    )

def train():
    if ""WORKER"" not in os.environ:
        logging.warning(""Missing env 'WORKER'"")
        wname = ""UNK""
    wname = os.environ[""WORKER""]

    # Logger
    FORMAT = ""%(asctime)s - "" + wname + "" %(message)s""
    logging.basicConfig(format=FORMAT)
    logger = logging.getLogger(wname)
    logger.setLevel(logging.INFO)
    logger.info(f""{tf.version.GIT_VERSION}, {tf.version.VERSION}"")

    # Load arguments
    tf_config = json.loads(os.environ[""TF_CONFIG""])
    cluster_spec = tf.train.ClusterSpec(tf_config[""cluster""])
    # Debug info to check if tf_config is set correctly
    logger.info(f""{wname}: TF_CONFIG {os.environ['TF_CONFIG']}"")

    # Prepare model
    strategy = tf.distribute.MultiWorkerMirroredStrategy()
    with strategy.scope():
        dist_is_ready = tf.Variable(
            initial_value=tf.constant(0, dtype=tf.dtypes.int64),
            name=""dist_is_ready"",
            synchronization=tf.VariableSynchronization.ON_WRITE,
            aggregation=tf.VariableAggregation.SUM,
        )

    logger.info(f""{wname}: Replicas in sync: {strategy.num_replicas_in_sync}"")
    logger.info(f""{wname}: {dist_is_ready}"")

    task_type, task_id = (
        strategy.cluster_resolver.task_type,
        strategy.cluster_resolver.task_id,
    )
    if _is_chief(task_type, task_id, cluster_spec):
        logger.info(f""{wname}: Assign ready"")
        dist_is_ready.assign(1)
    else:
        while dist_is_ready.numpy() < 1:
            logger.error(f""{wname}: dist_is_ready is {dist_is_ready.numpy()}..."")
            logger.error(f""{wname}: Wait for chief..."")
            time.sleep(5)

    logger.info(f""{wname}: All done. dist_is_ready is {dist_is_ready.numpy()}."")


if __name__ == ""__main__"":
    train()
```
`docker/Dockerfile`

```
FROM tensorflow/tensorflow

RUN pip install --quiet --upgrade --pre tf-nightly
RUN mkdir -p /app/log

COPY ./src/* /app/

WORKDIR /app
```

`docker-compose.yml`

```
services:
  worker0:
    image: local/worker
    build:
      dockerfile: ./docker/Dockerfile
      context: ./
    command: [""./minimal-example.py""]
    networks:
      - cluster
    environment:
      TF_CONFIG: '{""cluster"": {""worker"": [""worker0:30000"", ""worker1:30000""]}, ""task"":
        {""type"": ""worker"", ""index"": 0}}'
      TF_LOGDIR: /app/log
      WORKER: worker0
    volumes:
      - ${PWD}/log/worker0:/app/log

  worker1:
    image: local/worker
    build:
      dockerfile: ./docker/Dockerfile
      context: ./
    command: [""./minimal-example.py""]
    networks:
      - cluster
    environment:
      TF_CONFIG: '{""cluster"": {""worker"": [""worker0:30000"", ""worker1:30000""]}, ""task"":
        {""type"": ""worker"", ""index"": 1}}'
      TF_LOGDIR: /app/log
      WORKER: worker1
    volumes:
      - ${PWD}/log/worker1:/app/log

networks:
  cluster: {}
```

Start with
`docker compose build && docker compose up`
```


### Relevant log output

```shell
distributed-learning-worker1-1  | 2023-03-27 07:52:46,080 - worker1 worker1: dist_is_ready is 0...
distributed-learning-worker1-1  | 2023-03-27 07:52:46,080 - worker1 worker1: Wait for chief...
distributed-learning-worker0-1  | 2023-03-27 07:52:46,082 - worker0 worker0: All done. dist_is_ready is 1.
distributed-learning-worker0-1 exited with code 0
distributed-learning-worker1-1  | 2023-03-27 07:52:51,087 - worker1 worker1: dist_is_ready is 0...
distributed-learning-worker1-1  | 2023-03-27 07:52:51,087 - worker1 worker1: Wait for chief...
```
```
</details>"
60127,GPU is not supported on qualcomm adreno 702 GPU device. How to enable that?,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 10
- TensorFlow installed from (source or binary): source
- TensorFlow version (or github SHA if from source):2.9.2


**GPU is not supported on qualcomm adreno 702 GPU device.
    On qualcomm adreno 702 GPU device,[ the tensorflow example android app](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android_java)  can not work and it give us the toast ""GPU is not supported on this device."".
    Look into the tensorflow source code , and notice that int the tensorflow source code path tensorflow_src/tensorflow/lite/experimental/acceleration/compatibility,that give a way to add GPU device to make GPU work.**

**1. Following the tensorflow_src/tensorflow/lite/experimental/acceleration/compatibility/README.md, but it can not well on GPU, and the android app will crash. We do not know how to resolve this ?**

Very thanks for your reply.

"
60126,"Build failure - stablehlo ""File name too long""","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git 1dce1ddd62b4d9f7434bffa347defe0ad286bfab Mar 26

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 22.04.2 LTS

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

5.3.0

### GCC/Compiler version

11.3.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
TF build fails due to an issue with stablehlo integration.

$ bazel build //tensorflow/tools/pip_package:build_pip_package


g++ (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0
Copyright (C) 2021 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

uday@polymage-gpu-laptop:~/csa/courses/e0255/2023/e0255-asst-2/tensorflow$ bazel build //tensorflow/tools/pip_package:build_pip_package
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=146
INFO: Reading rc options for 'build' from /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3
INFO: Reading rc options for 'build' from /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:linux in file /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Build options --copt and --linkopt have changed, discarding analysis cache.
INFO: Repository stablehlo instantiated at:
  /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/WORKSPACE:15:14: in <toplevel>
  /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/tensorflow/workspace2.bzl:965:28: in workspace
  /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/tensorflow/workspace2.bzl:83:14: in _initialize_third_party
  /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/third_party/stablehlo/workspace.bzl:11:20: in repo
  /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/third_party/repo.bzl:89:35: in <toplevel>
INFO: Repository 'stablehlo' used the following cache hits instead of downloading the corresponding file.
 * Hash '1295d499727ed77e34f8c3c82f7fc68d95ca6044da82579ac9d51b510c5b2cd7' for https://storage.googleapis.com/mirror.tensorflow.org/github.com/openxla/stablehlo/archive/ef7a111784a2a5574de0b1165e3bdfc5397dce5a.zip
If the definition of 'stablehlo' was updated, verify that the hashes were also updated.
ERROR: An error occurred during the fetch of repository 'stablehlo':
   Traceback (most recent call last):
	File ""/home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/third_party/repo.bzl"", line 73, column 33, in _tf_http_archive_impl
		ctx.download_and_extract(
Error in download_and_extract: java.io.IOException: Error extracting /home/uday/.cache/bazel/_bazel_uday/77f8878d03275bdd27457b5be533415a/external/stablehlo/temp12753699981416543916/ef7a111784a2a5574de0b1165e3bdfc5397dce5a.zip to /home/uday/.cache/bazel/_bazel_uday/77f8878d03275bdd27457b5be533415a/external/stablehlo/temp12753699981416543916: /home/uday/.cache/bazel/_bazel_uday/77f8878d03275bdd27457b5be533415a/external/stablehlo/stablehlo/testdata/vmap_dot_general_batch_dimensions_lhs_float32_8_4_3_3_4__rhs_float32_4_8_3_4_2__dimensionnumbers____4_3___3_2_____0_1___1_0_dynamic.mlir.0_9_0.bc (File name too long)
ERROR: /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/WORKSPACE:15:14: fetching _tf_http_archive rule //external:stablehlo: Traceback (most recent call last):
	File ""/home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/third_party/repo.bzl"", line 73, column 33, in _tf_http_archive_impl
		ctx.download_and_extract(
Error in download_and_extract: java.io.IOException: Error extracting /home/uday/.cache/bazel/_bazel_uday/77f8878d03275bdd27457b5be533415a/external/stablehlo/temp12753699981416543916/ef7a111784a2a5574de0b1165e3bdfc5397dce5a.zip to /home/uday/.cache/bazel/_bazel_uday/77f8878d03275bdd27457b5be533415a/external/stablehlo/temp12753699981416543916: /home/uday/.cache/bazel/_bazel_uday/77f8878d03275bdd27457b5be533415a/external/stablehlo/stablehlo/testdata/vmap_dot_general_batch_dimensions_lhs_float32_8_4_3_3_4__rhs_float32_4_8_3_4_2__dimensionnumbers____4_3___3_2_____0_1___1_0_dynamic.mlir.0_9_0.bc (File name too long)
ERROR: /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/WORKSPACE:15:14: fetching _tf_http_archive rule //external:stablehlo: Traceback (most recent call last):
	File ""/home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/third_party/repo.bzl"", line 73, column 33, in _tf_http_archive_impl
		ctx.download_and_extract(
Error in download_and_extract: java.io.IOException: Error extracting /home/uday/.cache/bazel/_bazel_uday/77f8878d03275bdd27457b5be533415a/external/stablehlo/temp12753699981416543916/ef7a111784a2a5574de0b1165e3bdfc5397dce5a.zip to /home/uday/.cache/bazel/_bazel_uday/77f8878d03275bdd27457b5be533415a/external/stablehlo/temp12753699981416543916: /home/uday/.cache/bazel/_bazel_uday/77f8878d03275bdd27457b5be533415a/external/stablehlo/stablehlo/testdata/vmap_dot_general_batch_dimensions_lhs_float32_8_4_3_3_4__rhs_float32_4_8_3_4_2__dimensionnumbers____4_3___3_2_____0_1___1_0_dynamic.mlir.0_9_0.bc (File name too long)
INFO: Repository icu instantiated at:
  /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/WORKSPACE:15:14: in <toplevel>
  /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/tensorflow/workspace2.bzl:965:28: in workspace
  /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/tensorflow/workspace2.bzl:72:8: in _initialize_third_party
  /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/third_party/icu/workspace.bzl:8:20: in repo
  /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/third_party/repo.bzl:89:35: in <toplevel>
INFO: Repository go_sdk instantiated at:
  /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/WORKSPACE:23:14: in <toplevel>
  /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/tensorflow/workspace0.bzl:135:20: in workspace
  /home/uday/.cache/bazel/_bazel_uday/77f8878d03275bdd27457b5be533415a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps
  /home/uday/.cache/bazel/_bazel_uday/77f8878d03275bdd27457b5be533415a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains
  /home/uday/.cache/bazel/_bazel_uday/77f8878d03275bdd27457b5be533415a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk
Repository rule _go_download_sdk defined at:
  /home/uday/.cache/bazel/_bazel_uday/77f8878d03275bdd27457b5be533415a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>
INFO: Repository XNNPACK instantiated at:
  /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/WORKSPACE:15:14: in <toplevel>
  /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/tensorflow/workspace2.bzl:972:21: in workspace
  /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/tensorflow/workspace2.bzl:144:20: in _tf_repositories
  /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/third_party/repo.bzl:89:35: in <toplevel>
INFO: Repository boringssl instantiated at:
  /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/WORKSPACE:15:14: in <toplevel>
  /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/tensorflow/workspace2.bzl:972:21: in workspace
  /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/tensorflow/workspace2.bzl:565:20: in _tf_repositories
  /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/third_party/repo.bzl:89:35: in <toplevel>
ERROR: /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/tensorflow/tools/pip_package/BUILD:192:10: //tensorflow/tools/pip_package:licenses depends on @stablehlo//:LICENSE in repository @stablehlo which failed to fetch. no such package '@stablehlo//': java.io.IOException: Error extracting /home/uday/.cache/bazel/_bazel_uday/77f8878d03275bdd27457b5be533415a/external/stablehlo/temp12753699981416543916/ef7a111784a2a5574de0b1165e3bdfc5397dce5a.zip to /home/uday/.cache/bazel/_bazel_uday/77f8878d03275bdd27457b5be533415a/external/stablehlo/temp12753699981416543916: /home/uday/.cache/bazel/_bazel_uday/77f8878d03275bdd27457b5be533415a/external/stablehlo/stablehlo/testdata/vmap_dot_general_batch_dimensions_lhs_float32_8_4_3_3_4__rhs_float32_4_8_3_4_2__dimensionnumbers____4_3___3_2_____0_1___1_0_dynamic.mlir.0_9_0.bc (File name too long)
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: 
INFO: Elapsed time: 1.345s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded, 4299 targets configured)
```

```
$ java --version
openjdk 11.0.18 2023-01-17
OpenJDK Runtime Environment (build 11.0.18+10-post-Ubuntu-0ubuntu122.04)
OpenJDK 64-Bit Server VM (build 11.0.18+10-post-Ubuntu-0ubuntu122.04, mixed mode, sharing)
```
```


### Standalone code to reproduce the issue

```shell
Clone official TF repo at 1dce1ddd62b4d9f7434bffa347defe0ad286bfab (Mar 26):

Use default configure:

$ bazel build //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```
g++ (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0
Copyright (C) 2021 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

uday@polymage-gpu-laptop:~/csa/courses/e0255/2023/e0255-asst-2/tensorflow$ bazel build //tensorflow/tools/pip_package:build_pip_package
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=146
INFO: Reading rc options for 'build' from /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3
INFO: Reading rc options for 'build' from /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:linux in file /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Build options --copt and --linkopt have changed, discarding analysis cache.
INFO: Repository stablehlo instantiated at:
  /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/WORKSPACE:15:14: in <toplevel>
  /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/tensorflow/workspace2.bzl:965:28: in workspace
  /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/tensorflow/workspace2.bzl:83:14: in _initialize_third_party
  /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/third_party/stablehlo/workspace.bzl:11:20: in repo
  /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/third_party/repo.bzl:89:35: in <toplevel>
INFO: Repository 'stablehlo' used the following cache hits instead of downloading the corresponding file.
 * Hash '1295d499727ed77e34f8c3c82f7fc68d95ca6044da82579ac9d51b510c5b2cd7' for https://storage.googleapis.com/mirror.tensorflow.org/github.com/openxla/stablehlo/archive/ef7a111784a2a5574de0b1165e3bdfc5397dce5a.zip
If the definition of 'stablehlo' was updated, verify that the hashes were also updated.
ERROR: An error occurred during the fetch of repository 'stablehlo':
   Traceback (most recent call last):
	File ""/home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/third_party/repo.bzl"", line 73, column 33, in _tf_http_archive_impl
		ctx.download_and_extract(
Error in download_and_extract: java.io.IOException: Error extracting /home/uday/.cache/bazel/_bazel_uday/77f8878d03275bdd27457b5be533415a/external/stablehlo/temp12753699981416543916/ef7a111784a2a5574de0b1165e3bdfc5397dce5a.zip to /home/uday/.cache/bazel/_bazel_uday/77f8878d03275bdd27457b5be533415a/external/stablehlo/temp12753699981416543916: /home/uday/.cache/bazel/_bazel_uday/77f8878d03275bdd27457b5be533415a/external/stablehlo/stablehlo/testdata/vmap_dot_general_batch_dimensions_lhs_float32_8_4_3_3_4__rhs_float32_4_8_3_4_2__dimensionnumbers____4_3___3_2_____0_1___1_0_dynamic.mlir.0_9_0.bc (File name too long)
ERROR: /home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/WORKSPACE:15:14: fetching _tf_http_archive rule //external:stablehlo: Traceback (most recent call last):
	File ""/home/uday/csa/courses/e0255/2023/e0255-asst-2/tensorflow/third_party/repo.bzl"", line 73, column 33, in _tf_http_archive_impl
		ctx.download_and_extract(
Error in download_and_extract: java.io.IOException: Error extracting /home/uday/.cache/bazel/_bazel_uday/77f8878d03275bdd27457b5be533415a/external/stablehlo/temp12753699981416543916/ef7a111784a2a5574de0b1165e3bdfc5397dce5a.zip to /home/uday/.cache/bazel/_bazel_uday/77f8878d03275bdd27457b5be533415a/external/stablehlo/temp12753699981416543916: /home/uday/.cache/bazel/_bazel_uday/77f8878d03275bdd27457b5be533415a/external/stablehlo/stablehlo/testdata/vmap_dot_general_batch_dimensions_lhs_float32_8_4_3_3_4__rhs_float32_4_8_3_4_2__dimensionnumbers____4_3___3_2_____0_1___1_0_dynamic.mlir.0_9_0.bc (File name too long)
```
```
</details>"
60125,Questions about int4 quantization,"Hi, all.
Since it is an inquiry rather than an issue, I will not write a template.

Looking at the kernel side code of tflite, I saw that int4 for filter is supported in several op kernels; conv2d, depthwise-conv2d, fully-connected.

Could you tell me if there are plans to support int4 quantization in the tflite converter or support int4 for each op's input as well as filter, and if so, what milestones do you have?

Thank you :)"
60124,tf.raw_ops.QuantizeAndDequantizeV4 (and v3) crash (abort),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.13.0-dev20230208

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tf.raw_ops.QuantizeAndDequantizeV4 (and v3) crash (abort)
```


### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf
tf.raw_ops.QuantizeAndDequantizeV4(input=[np.ones((10))], input_min=-1, input_max=[-1, 1], range_given=True)
```


### Relevant log output

```shell
2023-03-26 22:24:03.910007: F tensorflow/core/framework/tensor.cc:778] Check failed: 1 == NumElements() (1 vs. 2)Must have a one element tensor
Aborted (core dumped)
```
</details>"
60123,tf.raw_ops.ResourceScatterDiv crash with abortion,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.13.0-dev20230208

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

8.2.4

### GPU model and memory

_No response_

### Current Behaviour?

```shell
2.13.0-dev20230208
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.python.eager import context

input1 = tf.raw_ops.VarHandleOp(dtype=tf.int32, shape=[2, 3], shared_name=context.anonymous_name())
input2 = tf.constant([],dtype=tf.float32)
output = tf.raw_ops.ResourceScatterDiv(resource=input1, indices=[0], updates=input2)
```


### Relevant log output

```shell
2023-03-26 22:11:23.135344: F tensorflow/core/framework/tensor.cc:770] Check failed: dtype() == expected_dtype (3 vs. 1) float expected, got int32
Aborted (core dumped)
```
</details>"
60122,tf.raw_ops.AssignSubVariableOp crash with abortion,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.13.0-dev20230208

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tf.raw_ops.AssignSubVariableOp crash with abortion
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.python.eager import context

input1 = tf.raw_ops.VarHandleOp(dtype=tf.int32, shape=[2, 3], shared_name=context.anonymous_name())
input2 = tf.constant([],dtype=tf.float32)

tf.raw_ops.AssignSubVariableOp(resource=input1, value=input2)
```


### Relevant log output

```shell
2023-03-26 21:58:20.336369: F tensorflow/core/framework/tensor.cc:770] Check failed: dtype() == expected_dtype (3 vs. 1) float expected, got int32
Aborted (core dumped)
```
</details>"
60121,tf.raw_ops.AssignAddVariableOp  crash with abortion,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.13.0-dev20230208

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tf.raw_ops.AssignAddVariableOp  crash with abortion
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.python.eager import context

input1 = tf.raw_ops.VarHandleOp(dtype=tf.int32, shape=[2, 3], shared_name=context.anonymous_name())
input2 = tf.constant([],dtype=tf.float32)

tf.raw_ops.AssignAddVariableOp(resource=input1, value=input2)
```


### Relevant log output

```shell
2023-03-26 18:39:30.729731: F tensorflow/core/framework/tensor.cc:770] Check failed: dtype() == expected_dtype (3 vs. 1) float expected, got int32
Aborted (core dumped
```
</details>"
60120,Thread-safety for `tensorflow::Tensor`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Debian GNU/Linux 10

### Mobile device

_No response_

### Python version

3.9

### Bazel version

6.1.0

### GCC/Compiler version

LLVM 12

### CUDA/cuDNN version

11.6

### GPU model and memory

T4

### Current Behaviour?

```shell
Are there any methods on `tensorflow::Tensor` objects that are thread-safe? Specifically, any of

SubSlice()
flat(), unaligned_flat(), shaped().
```


### Standalone code to reproduce the issue

```shell
std::vector<Tensor> output;
Mutex mu;
bool ready = false;

void Eval() {
  output = {Tensor(DataType::DT_HALF, {2, 16}); // one entry per thread
  nn_evaluator_.Infer(<some_input>, <some_names>, &output);

  mu.Lock();
  ready = true;
  mu.Unlock();
}

void ReadResult(int thread_id) {
  mu.Lock();
  mu.Await(Condition(&ready));
  mu.Unlock();

  // can we read tensor result now?
  auto res = output[0].SubSlice(thread_id).unaligned_flat<Eigen::half>();
  return res(0);
}
```


### Relevant log output

_No response_</details>"
60119,How can I achive the hilbert transform by using tensorflow?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When I discovered that tensorflow did not have a built-in API for implementing the Hilbert transform, I tried to pass the tensor to the scipy.signal.hilbert method in numpy, but keras does not allow the tensor to be processed directly using numpy maths methods, so I used the following method for data[ Tensor/KerasTensor(1,4096,2)] to convert:
1. data_np = data.numpy()
2. data_np = keras.backend.get_val(data)
Neither works. I then decided to build my own hilbert_transform function by referring to the scipy.signal.hilbert method in numpy, at which point another problem arose:
The tf.signal.fft method is inconsistent with the scipy.fft method. scipy.fft can take an axis argument, but tf.signal.fftz cannot.
Therefore, I would like your help in answering:
1. how do I implement the Hilbert transform on data[Tensor/KerasTensor(1,4096,2)]?
or
2. how do I convert data[Tensor/KerasTensor(1,4096,2)] into a numpys array?

Translated with www.DeepL.com/Translator (free version)
```


### Standalone code to reproduce the issue

```shell
from scipy.signal import hilbert
import tensorflow as tf
import numpy as np

def is_complex(x):
    return x.dtype == tf.complex64 or x.dtype == tf.complex128
def highpass_filter(x, N=None, axis=-1):
    x = tf.convert_to_tensor(x)
    if is_complex(x):
        raise ValueError(""x must be real."")
    if N is None:
        N = x.shape[axis]
    if N <= 0:
        raise ValueError(""N must be positive."")

    Xf = fft(x, N, axis=axis)
    h = tf.zeros(N)
    if N % 2 == 0:
        h = tf.tensor_scatter_nd_update(h, [[0], [N // 2]], [1, 1])
        h = tf.tensor_scatter_nd_update(h, tf.range(1, N // 2), tf.ones(N // 2 - 1) * 2)
    else:
        h = tf.tensor_scatter_nd_update(h, [0], [1])
        h = tf.tensor_scatter_nd_update(h, tf.range(1, (N + 1) // 2), tf.ones((N + 1) // 2 - 1) * 2)

    if x.ndim > 1:
        ind = [tf.newaxis] * x.ndim
        ind[axis] = slice(None)
        h = h[tuple(ind)]

    x = tensorflow.signal.ifft(Xf * h, axis=axis)
    return x
x = np.random.rand(1, 4096, 2)
x_h = highpass_filter(x,axis=1)
```


### Relevant log output

_No response_</details>"
60118,Support of protobuf 4.22 for tensorflow-macos,"I am using tensorflow-macos and it is raising an issue with protobuf 4.22 compatibility. I need that more recent version of protobuf for other Python libraries.
"
60116,TensorFlow Lite Object Detection - Plot training and validation losses,"Hello. I am using the following notebook to train my dataset with efficientdet-lite0 model.
https://www.tensorflow.org/lite/models/modify/model_maker/object_detection

I see for each epoch we get the information of training loss ""loss"" and validation loss ""val_loss"". I would like to plot them on the same graph if possible, but as 'ObjectDetector' object has no attribute 'history' I can't do it in a classic way **model.history['loss']**
For TensorFlow Lite the model has been build and fit with **object_detector.create()**

Can you help me to figure out how to plot training and validation losses on the same graph in this case. Thanks in advance!"
60115,Publish Python 3.11 Wheels for `tflite-runtime`,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux x86 & ARM
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: N/A
-   **TensorFlow installed from (source or binary)**: Binary
-   **TensorFlow version (use command below)**: N/A
-   **Python version**: N/A
-   **Bazel version (if compiling from source)**: N/A
-   **GCC/Compiler version (if compiling from source)**: N/A
-   **CUDA/cuDNN version**: N/A
-   **GPU model and memory**: N/A
-   **Exact command to reproduce**: N/A

### Describe the problem

[`tensorflow 2.12.0` has published Python 3.11 wheels to PyPI](https://pypi.org/project/tensorflow/2.12.0/#files). The [latest `tflite-runtime-nightly`](https://pypi.org/project/tflite-runtime-nightly/2.13.0.dev20230324/#files) do not have 3.11 wheels.

I request that 3.11 wheels be built and published to PyPI for `tflite-runtime`.

CC @terryheo 

### Related Issues

- #58032
- #56137"
60114,LSTMBlockCell validation error,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf2.11

### Custom Code

Yes

### OS Platform and Distribution

Linux Debian 10

### Mobile device

N/A

### Python version

N/A

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
In tensorflow/tensorflow/core/kernels/rnn/lstm_ops.cc line 441 the error message prints wci_tensor->dims(), but should print wcf_tensor->dims(). In current version a strange message can appear:
wcf_tensor must be rank 1 but is rank 1.
```


### Standalone code to reproduce the issue

```shell
It is clear from the source code, no need to reproduce.
```


### Relevant log output

_No response_</details>"
60113,categorical_crossentropy's output is non-zero when y_true and y_pred are 1,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When `y_true` and `y_pred` are `1.`, the categorical crossentropy outputted is a non-zero value. But it should be **zero** following the design of categorical crossentropy.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

y_true = tf.constant([[1.]])
y_pred = tf.constant([[1.]])

loss = tf.keras.metrics.categorical_crossentropy(y_true, y_pred)
print(loss)  # 1.1920929e-07 but should be 0
```
```


### Relevant log output

```shell
1.1920929e-07
```
</details>"
60112,Build from source on Windows failed,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installation (pip package or built from source): Built from cource
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.11 and 2.12 both

### 2. Code


bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

Link that i am following. 
https://www.tensorflow.org/install/source_windows

Steps i followed- 
 1- Install Python and add path to enviornment variable , Python version 3.10.10
2- install additional package- 

pip3 install -U six numpy wheel packaging
pip3 install -U keras_preprocessing --no-deps

3- install bazel - version 5.3.0 
path added to enviornment variable. 

4- Install MSYS2 - version - 20230318
added usr/bin path to enviornment variable. 
5- ran below command for aditional package- 
pacman -S git patch unzip
6- Install Visual C++ Build Tools 2019

7- cloned tf library - 

git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow

8- run python ./configure.py with default setting

9- ran bazel 
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

and bazel run failed-

Error Message- 

D:\TFlite\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=211
INFO: Reading rc options for 'build' from d:\tflite\tensorflow\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/Users/Nupur/AppData/Local/Microsoft/WindowsApps/python.exe
INFO: Reading rc options for 'build' from d:\tflite\tensorflow\.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from d:\tflite\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=D:/TFlite/Software/Python/python.exe --action_env PYTHON_LIB_PATH=D:/TFlite/Software/Python/lib/site-packages --python_path=D:/TFlite/Software/Python/python.exe --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true
INFO: Reading rc options for 'build' from d:\tflite\tensorflow\.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file d:\tflite\tensorflow\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file d:\tflite\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:opt in file d:\tflite\tensorflow\.tf_configure.bazelrc: --copt=/arch:AVX --host_copt=/arch:AVX
INFO: Found applicable config definition build:windows in file d:\tflite\tensorflow\.bazelrc: --copt=/W0 --host_copt=/W0 --copt=/Zc:__cplusplus --host_copt=/Zc:__cplusplus --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --features=compiler_param_file --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --cxxopt=/std:c++17 --host_cxxopt=/std:c++17 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/Zc:preprocessor --host_copt=/Zc:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file
INFO: Found applicable config definition build:monolithic in file d:\tflite\tensorflow\.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false
INFO: Repository llvm-project instantiated at:
  D:/tflite/tensorflow/WORKSPACE:15:14: in <toplevel>
  D:/tflite/tensorflow/tensorflow/workspace2.bzl:972:21: in workspace
  D:/tflite/tensorflow/tensorflow/workspace2.bzl:545:15: in _tf_repositories
  D:/tflite/tensorflow/third_party/llvm/setup.bzl:22:19: in llvm_setup
Repository rule llvm_configure defined at:
  C:/users/nupur/_bazel_nupur/cuskol4y/external/llvm-raw/utils/bazel/configure.bzl:169:33: in <toplevel>
ERROR: An error occurred during the fetch of repository 'llvm-project':
   Traceback (most recent call last):
        File ""C:/users/nupur/_bazel_nupur/cuskol4y/external/llvm-raw/utils/bazel/configure.bzl"", line 146, column 25, in _llvm_configure_impl
                _overlay_directories(repository_ctx)
        File ""C:/users/nupur/_bazel_nupur/cuskol4y/external/llvm-raw/utils/bazel/configure.bzl"", line 49, column 13, in _overlay_directories
                fail(""Failed to find python3 binary"")
Error in fail: Failed to find python3 binary
ERROR: D:/tflite/tensorflow/WORKSPACE:15:14: fetching llvm_configure rule //external:llvm-project: Traceback (most recent call last):
        File ""C:/users/nupur/_bazel_nupur/cuskol4y/external/llvm-raw/utils/bazel/configure.bzl"", line 146, column 25, in _llvm_configure_impl
                _overlay_directories(repository_ctx)
        File ""C:/users/nupur/_bazel_nupur/cuskol4y/external/llvm-raw/utils/bazel/configure.bzl"", line 49, column 13, in _overlay_directories
                fail(""Failed to find python3 binary"")
Error in fail: Failed to find python3 binary
INFO: Repository build_bazel_rules_android instantiated at:
  D:/tflite/tensorflow/WORKSPACE:15:14: in <toplevel>
  D:/tflite/tensorflow/tensorflow/workspace2.bzl:972:21: in workspace
  D:/tflite/tensorflow/tensorflow/workspace2.bzl:811:20: in _tf_repositories
  D:/tflite/tensorflow/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  D:/tflite/tensorflow/third_party/repo.bzl:89:35: in <toplevel>
ERROR: D:/tflite/tensorflow/tensorflow/tools/pip_package/BUILD:280:10: //tensorflow/tools/pip_package:build_pip_package depends on //tensorflow/compiler/mlir/tensorflow:gen_mlir_passthrough_op_py in repository @ which failed to fetch. no such package '@llvm-project//mlir': Failed to find python3 binary
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed
INFO: Elapsed time: 4.356s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (52 packages loaded, 12 targets configured)
    currently loading: tensorflow/lite/tools ... (4 packages)
    Fetching @flatbuffers; fetching
    Fetching https://storage.googleapis.com/mirror.tensorflow.org/github.com/bazelbuild/rules_android/archive/v0.1.1.zip

D:\TFlite\tensorflow>


 "
60111,If we have any detail information of each CI in tensorflow Github? ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hello, I have been studying the actions (CI) of various open source projects on Github recently.
I noticed that tensorflowhas a well-established CI, so I would like to further understand its composition and structure. Do you have any relevant materials that I can study and refer to? 
Thank you very much.
```


### Standalone code to reproduce the issue

```shell
none
```


### Relevant log output

_No response_</details>"
60109,"Importing TF 2.12, then torch, hangs, but not the other way around","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.12.0

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04.5

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

_No response_

### Current Behaviour?

```shell
If I import tensorflow and then import torch, the torch import line hangs forever without completing. On the other hand, if I import torch first and then import tensorflow there is no problem.

The hang is so severe that no amount of ctr-c can kill it. You have to kill the python process from a separate terminal to free the hung terminal. 

This issue does not exist in tensorflow 2.11.1 or earlier. It also doesn't happen when using older versions of torch like 1.13.1. Since torch followed by tf works but tf followed by torch doesn't, this seems like an issue tf is causing.
```


### Standalone code to reproduce the issue

```shell
docker pull tensorflow/tensorflow:2.12.0-gpu
docker run -it tensorflow/tensorflow:2.12.0-gpu

pip install torch==2.0.0+cu118 torchvision==0.15.1+cu118 torchaudio==2.0.1+cu118 -f https://download.pytorch.org/whl/cu118/torch_stable.html

python

import tensorflow as tf
import torch
```


### Relevant log output

_No response_</details>"
60106,Problem installing in Colab,"when running

> _!pip install -q git+https://github.com/tensorflow/examples.git_

I get kicked this error

>  error: subprocess-exited-with-error
  
  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─> See above for output.
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  Preparing metadata (setup.py) ... error
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.


Anyone know any fixes for this problem?"
60102,Error converting `None` to a tensor in tensorflow 2.12.0," ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

As of tensorflow 2.12.0, I'm seeing a new error related to converting `None` values to a tensor. I haven't managed to produce a reproducible example (sorry!), but thought I'd post here anyway to see if anyone has any insight? Traceback is:

```python
  iterator = <tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7f536c61c550>
  
      def tf__predict_function(iterator):
          """"""Runs an evaluation execution with a single step.""""""
          with ag__.FunctionScope('predict_function', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:
              do_return = False
              retval_ = ag__.UndefinedReturnValue()
              try:
                  do_return = True
  >               retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
  E               TypeError: in user code:
  E               
  E                   File ""/home/runner/work/cellfinder-core/cellfinder-core/.tox/py310/lib/python3.10/site-packages/keras/engine/training.py"", line 2169, in predict_function  *
  E                       return step_function(self, iterator)
  E                   File ""/home/runner/work/cellfinder-core/cellfinder-core/.tox/py310/lib/python3.10/site-packages/keras/engine/training.py"", line 2155, in step_function  **
  E                       outputs = model.distribute_strategy.run(run_step, args=(data,))
  E                   File ""/home/runner/work/cellfinder-core/cellfinder-core/.tox/py310/lib/python3.10/site-packages/keras/engine/training.py"", line 2143, in run_step  **
  E                       outputs = model.predict_step(data)
  E                   File ""/home/runner/work/cellfinder-core/cellfinder-core/.tox/py310/lib/python3.10/site-packages/keras/engine/training.py"", line 2111, in predict_step
  E                       return self(x, training=False)
  E                   File ""/home/runner/work/cellfinder-core/cellfinder-core/.tox/py310/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 70, in error_handler
  E                       raise e.with_traceback(filtered_tb) from None
  E               
  E                   TypeError: Exception encountered when calling layer 'conv1_bn' (type BatchNormalization).
  E                   
  E                   Failed to convert elements of [1, 1, 1, None, 1] to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.
  E                   
  E                   Call arguments received by layer 'conv1_bn' (type BatchNormalization):
  E                     • inputs=tf.Tensor(shape=(None, None, None, None, 64), dtype=float32)
  E                     • training=False
  E                     • mask=None
  
  /tmp/__autograph_generated_filerbzi9zef.py:15: TypeError
```

### Standalone code to reproduce the issue

```shell
~
```


### Relevant log output

_No response_"
60101,TensorFlow 2.12.0 WSL2 GPU support,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.12.0

### Custom Code

No

### OS Platform and Distribution

Windows 11 + WSL2

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.8/8.4.1.50

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I had a TensorFlow 2.11 environment in WSL that works fine. It is installed using the conda-forge packages. For TensorFlow 2.12 I swapped to using the pip packages for tensorflow, keras, keras-tuner, tensorflow-hub, tensorflow-datasets, and tensorboard.

After installation the conda environment for 2.12 can't find the GPU anymore. Swapping back over to my 2.11 environment, it still finds it fine. Are there new instructions for GPU support in WSL2 as of TensorFlow 2.12.0?
```


### Standalone code to reproduce the issue

```shell
gpus = tf.config.experimental.list_physical_devices('GPU')
print(""GPUS: "", gpus)
```


### Relevant log output

_No response_</details>"
60099,"Build 2.11.1 from source in arm , Error in repository_rule: in call to repository_rule(), parameter 'remotable' is experimental and thus unavailable with the current flags","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11.1

### Custom Code

No

### OS Platform and Distribution

Linux EulerOS(CentOS)

### Mobile device

_No response_

### Python version

3.9

### Bazel version

5.3.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I try to build tensorflow 2.11.1 from source in arm machine
My build cmd is like below :

bazel --host_jvm_args=-Djavax.net.ssl.trustStore='/usr/local/java/jre/lib/security/cacerts' \
                  --host_jvm_args=-Djavax.net.ssl.trustStorePassword='changeit'  build --jobs=16 {{ extra_options }} \
                  --copt=""-mtune=generic"" --copt=""-march=armv8-a"" --copt=""-O3""  \
                 //tensorflow/tools/pip_package:build_pip_package --verbose_failures >> /tmp/workspace/build_tensorflow.log

except the cacerts part,I set 4 build params .

I meet the error 
""Error in repository_rule: in call to repository_rule(), parameter 'remotable' is experimental and thus unavailable with the current flags. It may be enabled by setting --experimental_repo_remote_exec""

I tried:
1. change remotable to False ,result in another error in building
2. add --experimental_repo_remote_exec to build param ,meet new error like :
 Error in check_experimental_cc_shared_library: Pass --experimental_cc_shared_library to use cc_shared_library
```


### Standalone code to reproduce the issue

```shell
I use the build config from the linaro community build pipeline (https://git.linaro.org/ci/job/configs.git)
I pull the configs repo to local machine and adjust a little for network issues

my build cmd:
     
 bazel --host_jvm_args=-Djavax.net.ssl.trustStore='/usr/local/java/jre/lib/security/cacerts' \
                  --host_jvm_args=-Djavax.net.ssl.trustStorePassword='changeit'  build --jobs=16 {{ extra_options }} \
                  --copt=""-mtune=generic"" --copt=""-march=armv8-a"" --copt=""-O3""  \
                 //tensorflow/tools/pip_package:build_pip_package --verbose_failures >> /tmp/workspace/build_tensorflow.log;
```


### Relevant log output

```shell
stderr: |-
    + source /tmp/workspace/venv-cp39-cp39/bin/activate
    ++ deactivate nondestructive
    ++ '[' -n '' ']'
    ++ '[' -n '' ']'
    ++ '[' -n /bin/bash -o -n '' ']'
    ++ hash -r
    ++ '[' -n '' ']'
    ++ unset VIRTUAL_ENV
    ++ '[' '!' nondestructive = nondestructive ']'
    ++ VIRTUAL_ENV=/tmp/workspace/venv-cp39-cp39
    ++ export VIRTUAL_ENV
    ++ _OLD_VIRTUAL_PATH=/opt/rh/devtoolset-10/root/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
    ++ PATH=/tmp/workspace/venv-cp39-cp39/bin:/opt/rh/devtoolset-10/root/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
    ++ export PATH
    ++ '[' -n '' ']'
    ++ '[' -z '' ']'
    ++ _OLD_VIRTUAL_PS1=
    ++ PS1='(venv-cp39-cp39) '
    ++ export PS1
    ++ '[' -n /bin/bash -o -n '' ']'
    ++ hash -r
    + source ./linklibs.sh
    ++ export BAZEL_LINKLIBS=-lstdc++
    ++ BAZEL_LINKLIBS=-lstdc++
    ++ export BAZEL_LINKOPTS=
    ++ BAZEL_LINKOPTS=
    + bazel --host_jvm_args=-Djavax.net.ssl.trustStore=/usr/local/java/jre/lib/security/cacerts --host_jvm_args=-Djavax.net.ssl.trustStorePassword=changeit clean --expunge
    INFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.
    + bazel --host_jvm_args=-Djavax.net.ssl.trustStore=/usr/local/java/jre/lib/security/cacerts --host_jvm_args=-Djavax.net.ssl.trustStorePassword=changeit build --jobs=16 --copt=-mtune=generic --copt=-march=armv8-a --copt=-O3 --cxxopt=-std=c++17 //tensorflow/tools/pip_package:build_pip_package --verbose_failures
    Starting local Bazel server and connecting to it...
    Loading:
    Loading: 0 packages loaded
    ERROR: Traceback (most recent call last):
            File ""/tmp/workspace/tensorflow-2.11.1/third_party/py/python_configure.bzl"", line 288, column 42, in <toplevel>
                    remote_python_configure = repository_rule(
    Error in repository_rule: in call to repository_rule(), parameter 'remotable' is experimental and thus unavailable with the current flags. It may be enabled by setting --experimental_repo_remote_exec
    INFO: Repository tf_runtime instantiated at:
      /tmp/workspace/tensorflow-2.11.1/WORKSPACE:11:14: in <toplevel>
      /tmp/workspace/tensorflow-2.11.1/tensorflow/workspace3.bzl:18:15: in workspace
      /tmp/workspace/tensorflow-2.11.1/third_party/tf_runtime/workspace.bzl:12:20: in repo
      /tmp/workspace/tensorflow-2.11.1/third_party/repo.bzl:136:21: in tf_http_archive
    Repository rule _tf_http_archive defined at:
      /tmp/workspace/tensorflow-2.11.1/third_party/repo.bzl:89:35: in <toplevel>
    INFO: Repository io_bazel_rules_closure instantiated at:
      /tmp/workspace/tensorflow-2.11.1/WORKSPACE:11:14: in <toplevel>
      /tmp/workspace/tensorflow-2.11.1/tensorflow/workspace3.bzl:8:17: in workspace
    Repository rule http_archive defined at:
      /root/.cache/bazel/_bazel_root/6b72de8f0642eb03fd5c3f4db1efede0/external/bazel_tools/tools/build_defs/repo/http.bzl:355:31: in <toplevel>
    INFO: Repository rules_jvm_external instantiated at:
      /tmp/workspace/tensorflow-2.11.1/WORKSPACE:11:14: in <toplevel>
      /tmp/workspace/tensorflow-2.11.1/tensorflow/workspace3.bzl:41:17: in workspace
    Repository rule http_archive defined at:
      /root/.cache/bazel/_bazel_root/6b72de8f0642eb03fd5c3f4db1efede0/external/bazel_tools/tools/build_defs/repo/http.bzl:355:31: in <toplevel>
    ERROR: error loading package '': at /tmp/workspace/tensorflow-2.11.1/tensorflow/workspace2.bzl:10:6: initialization of module 'third_party/py/python_configure.bzl' failed
    INFO: Elapsed time: 4.759s
    INFO: 0 processes.
    FAILED: Build did NOT complete successfully (0 packages loaded)
    FAILED: Build did NOT complete successfully (0 packages loaded)
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>

===============================error after add --experimental_repo_remote_exec=================

  stderr: |-
    + source /tmp/workspace/venv-cp39-cp39/bin/activate
    ++ deactivate nondestructive
    ++ '[' -n '' ']'
    ++ '[' -n '' ']'
    ++ '[' -n /bin/bash -o -n '' ']'
    ++ hash -r
    ++ '[' -n '' ']'
    ++ unset VIRTUAL_ENV
    ++ '[' '!' nondestructive = nondestructive ']'
    ++ VIRTUAL_ENV=/tmp/workspace/venv-cp39-cp39
    ++ export VIRTUAL_ENV
    ++ _OLD_VIRTUAL_PATH=/opt/rh/devtoolset-10/root/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
    ++ PATH=/tmp/workspace/venv-cp39-cp39/bin:/opt/rh/devtoolset-10/root/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
    ++ export PATH
    ++ '[' -n '' ']'
    ++ '[' -z '' ']'
    ++ _OLD_VIRTUAL_PS1=
    ++ PS1='(venv-cp39-cp39) '
    ++ export PS1
    ++ '[' -n /bin/bash -o -n '' ']'
    ++ hash -r
    + source ./linklibs.sh
    ++ export BAZEL_LINKLIBS=-lstdc++
    ++ BAZEL_LINKLIBS=-lstdc++
    ++ export BAZEL_LINKOPTS=
    ++ BAZEL_LINKOPTS=
    + bazel --host_jvm_args=-Djavax.net.ssl.trustStore=/usr/local/java/jre/lib/security/cacerts --host_jvm_args=-Djavax.net.ssl.trustStorePassword=changeit clean --expunge
    INFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.
    + bazel --host_jvm_args=-Djavax.net.ssl.trustStore=/usr/local/java/jre/lib/security/cacerts --host_jvm_args=-Djavax.net.ssl.trustStorePassword=changeit build --jobs=16 --copt=-mtune=generic --copt=-march=armv8-a --copt=-O3 --experimental_repo_remote_exec //tensorflow/tools/pip_package:build_pip_package --verbose_failures
    Starting local Bazel server and connecting to it...
    Loading:
    Loading: 0 packages loaded
    Loading: 0 packages loaded
    Loading: 0 packages loaded
    Loading: 0 packages loaded
    Loading: 0 packages loaded
    Loading: 0 packages loaded
    Loading: 0 packages loaded
    Analyzing: target //tensorflow/tools/pip_package:build_pip_package (1 packages loaded, 0 targets configured)
    Analyzing: target //tensorflow/tools/pip_package:build_pip_package (239 packages loaded, 3979 targets configured)
    Analyzing: target //tensorflow/tools/pip_package:build_pip_package (282 packages loaded, 4077 targets configured)
    Analyzing: target //tensorflow/tools/pip_package:build_pip_package (355 packages loaded, 4790 targets configured)
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/tsl/lib/gtl/BUILD:97:11: in linkstatic attribute of cc_library rule //tensorflow/tsl/lib/gtl:map_util: setting 'linkstatic=1' is recommended if there are no object files
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:1275:11: in linkstatic attribute of cc_library rule //tensorflow/core:lib_headers_for_pybind: setting 'linkstatic=1' is recommended if there are no object files
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/lib/gtl/BUILD:139:11: in linkstatic attribute of cc_library rule //tensorflow/core/lib/gtl:map_util: setting 'linkstatic=1' is recommended if there are no object files
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:472:11: in hdrs attribute of cc_library rule //tensorflow/core:framework_lite: Artifact 'tensorflow/tsl/platform/bfloat16.h' is duplicated (through '//tensorflow/core/platform:framework_lite_hdrs' and '//tensorflow/tsl/platform:framework_lite_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:472:11: in hdrs attribute of cc_library rule //tensorflow/core:framework_lite: Artifact 'tensorflow/tsl/platform/byte_order.h' is duplicated (through '//tensorflow/core/platform:framework_lite_hdrs' and '//tensorflow/tsl/platform:framework_lite_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:472:11: in hdrs attribute of cc_library rule //tensorflow/core:framework_lite: Artifact 'tensorflow/tsl/platform/cpu_info.h' is duplicated (through '//tensorflow/core/platform:framework_lite_hdrs' and '//tensorflow/tsl/platform:framework_lite_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:472:11: in hdrs attribute of cc_library rule //tensorflow/core:framework_lite: Artifact 'tensorflow/tsl/platform/dynamic_annotations.h' is duplicated (through '//tensorflow/core/platform:framework_lite_hdrs' and '//tensorflow/tsl/platform:framework_lite_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:472:11: in hdrs attribute of cc_library rule //tensorflow/core:framework_lite: Artifact 'tensorflow/tsl/platform/macros.h' is duplicated (through '//tensorflow/core/platform:framework_lite_hdrs' and '//tensorflow/tsl/platform:framework_lite_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:472:11: in hdrs attribute of cc_library rule //tensorflow/core:framework_lite: Artifact 'tensorflow/tsl/platform/platform.h' is duplicated (through '//tensorflow/core/platform:framework_lite_hdrs' and '//tensorflow/tsl/platform:framework_lite_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:472:11: in hdrs attribute of cc_library rule //tensorflow/core:framework_lite: Artifact 'tensorflow/tsl/platform/prefetch.h' is duplicated (through '//tensorflow/core/platform:framework_lite_hdrs' and '//tensorflow/tsl/platform:framework_lite_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:472:11: in hdrs attribute of cc_library rule //tensorflow/core:framework_lite: Artifact 'tensorflow/tsl/platform/protobuf.h' is duplicated (through '//tensorflow/core/platform:framework_lite_hdrs' and '//tensorflow/tsl/platform:framework_lite_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:472:11: in hdrs attribute of cc_library rule //tensorflow/core:framework_lite: Artifact 'tensorflow/tsl/platform/thread_annotations.h' is duplicated (through '//tensorflow/core/platform:framework_lite_hdrs' and '//tensorflow/tsl/platform:framework_lite_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:1249:11: in linkstatic attribute of cc_library rule //tensorflow/core:lib_internal: setting 'linkstatic=1' is recommended if there are no object files
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/abi.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/bfloat16.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/casts.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/coding.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/context.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/cpu_info.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/crash_analysis.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/dynamic_annotations.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/env.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/errors.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/file_statistics.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/file_system.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/file_system_helper.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/fingerprint.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/init_main.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/logger.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/mem.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/net.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/notification.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/null_file_system.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/numa.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/path.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/prefetch.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/protobuf.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/ram_file_system.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/random.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/resource.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/stack_frame.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/stacktrace.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/stacktrace_handler.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/statusor.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/str_util.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/stringpiece.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/stringprintf.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/subprocess.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/thread_annotations.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/profile_utils/android_armv7a_cpu_utils_helper.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/profile_utils/clock_cycle_profiler.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/profile_utils/cpu_utils.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:258:11: in hdrs attribute of cc_library rule //tensorflow/core:lib: Artifact 'tensorflow/tsl/platform/profile_utils/i_cpu_utils_helper.h' is duplicated (through '//tensorflow/core/platform:lib_hdrs' and '//tensorflow/tsl/platform:lib_hdrs')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/BUILD:1576:16: in linkstatic attribute of cc_library rule //tensorflow/core:framework_internal: setting 'linkstatic=1' is recommended if there are no object files. Since this rule was created by the macro 'tf_cuda_library', the error might have been caused by the macro implementation
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/common_runtime/BUILD:890:11: in hdrs attribute of cc_library rule //tensorflow/core/common_runtime:graph_constructor: Artifact 'tensorflow/core/common_runtime/eval_const_tensor.h' is duplicated (through '//tensorflow/core/common_runtime:core_cpu_lib_headers' and '//tensorflow/core/common_runtime:eval_const_tensor.h')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/common_runtime/BUILD:890:11: in hdrs attribute of cc_library rule //tensorflow/core/common_runtime:graph_constructor: Artifact 'tensorflow/core/common_runtime/graph_constructor.h' is duplicated (through '//tensorflow/core/common_runtime:core_cpu_lib_headers' and '//tensorflow/core/common_runtime:graph_constructor.h')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/common_runtime/BUILD:890:11: in hdrs attribute of cc_library rule //tensorflow/core/common_runtime:graph_constructor: Artifact 'tensorflow/core/common_runtime/graph_runner.h' is duplicated (through '//tensorflow/core/common_runtime:core_cpu_lib_headers' and '//tensorflow/core/common_runtime:graph_runner.h')
    WARNING: /tmp/workspace/tensorflow-2.11.1/tensorflow/core/common_runtime/BUILD:890:11: in hdrs attribute of cc_library rule //tensorflow/core/common_runtime:graph_constructor: Artifact 'tensorflow/core/common_runtime/shape_refiner.h' is duplicated (through '//tensorflow/core/common_runtime:core_cpu_lib_headers' and '//tensorflow/core/common_runtime:shape_refiner.h')
    Analyzing: target //tensorflow/tools/pip_package:build_pip_package (531 packages loaded, 34453 targets configured)
    ERROR: /tmp/workspace/tensorflow-2.11.1/tensorflow/BUILD:1059:21: in cc_shared_library rule //tensorflow:libtensorflow_framework.so.2.11.1:
    Traceback (most recent call last):
            File ""/virtual_builtins_bzl/common/cc/experimental_cc_shared_library.bzl"", line 404, column 51, in _cc_shared_library_impl
    Error in check_experimental_cc_shared_library: Pass --experimental_cc_shared_library to use cc_shared_library
    ERROR: /tmp/workspace/tensorflow-2.11.1/tensorflow/BUILD:1059:21: Analysis of target '//tensorflow:libtensorflow_framework.so.2.11.1' failed
    ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted:
    INFO: Elapsed time: 76.772s
    INFO: 0 processes.
    FAILED: Build did NOT complete successfully (531 packages loaded, 34537 targets configured)
    FAILED: Build did NOT complete successfully (531 packages loaded, 34537 targets configured)
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>
```
</details>"
60093,tf.raw_ops.Round doesn't have a gradient,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The function tf.raw_ops.Round does not have a gradient, however, it is mentioned in the documentation (https://www.tensorflow.org/api_docs/python/tf/raw_ops) that it does. Despite this inconsistency, the code operates correctly as intended.
```


### Standalone code to reproduce the issue

```shell
import os
import tensorflow as tf
import numpy as np

x = tf.Variable([1.5, -1.5], dtype=tf.dtypes.float64)

def rounding(x):

    round_op = tf.raw_ops.Round(x=x)
    return round_op

t1 = rounding(x)
with tf.GradientTape() as tape:
    tape.watch(x)
    t = rounding(x)

gradient = tape.gradient(t, x)
print(gradient)
```


### Relevant log output

```shell
None
```
</details>"
60092,InvalidArgumentError: Unsupported data type for TPU: string,"**Environment**

```
OS: Kaggle/Colab
TensorFlow: 2.9.1
```

**Background**

I've a training dataset (`x`: image, `y`: caption). For training, I need to transform this caption to text embedding. And I want to do that inside the model, at training time. And not from data-loader (e.x `tf.data` API). 

Now, including the text-encoder for text embedding inside the actual model (image-captioning model), the whole model can be super heavy. To remedy this, I've followed the following approach. I've tried to transform **text to text embedding** inside the model by overriding the `train_step` method. The process works on GPUs. But for TPU devices, no.

```python
class TextToEmbedding(keras.Model):
    def call(self, inputs):
        return self.model(inputs)
    
    def train_step(self, data):
        x, y = data
        y = tfst_model(y)
        return super().train_step((x, y))
    
    def test_step(self, data):
        x, y = data
        y = tfst_model(y)
        return super().test_step((x, y))
```

On TPUs, the error is showed up. It gives: 

> InvalidArgumentError: Unsupported data type for TPU: string, caused by output Pad_1:0 [Op:__inference_train_function_67882]

Though it's stated that it is unsupported for TPU, but I've failed to understand how that can be unspported on such usual cases. Does string input not support on TPU? 


**Reproducible Code**

[Gist.](https://colab.research.google.com/drive/1C0ejXFVav0yCCDmAATisIhiTFPjGxHKM?usp=sharing)

(Also note, I'm running the code on TPU-vm and not on node).

**Similar issues**

- https://github.com/google/uncertainty-baselines/issues/579
- https://github.com/tensorflow/quantum/issues/707
"
60090,[JAX] The use of jax.Array.at fails experimental_from_jax,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Darwin Kernel Version 22.3.0
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.11.0 

### 2. Code


#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```

import tensorflow as tf
import jax
import jax.numpy as jnp
import jax.tree_util as tree

def main3():
    print(f'JAX {jax.__version__}')
    print(f'tf {tf.__version__}')

    @jax.jit
    def _update(x):
        return x.at[0].set(4)


    converter = tf.lite.TFLiteConverter.experimental_from_jax([_update],
                                                              [[('x', jnp.ones(2))]])
    converter.target_spec.supported_ops = [
        tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.
        tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.

    ]

    tflite_update = converter.convert()
    with open('update.tflite', 'wb') as f:
        f.write(tflite_update)

    interpreter = tf.lite.Interpreter(model_content=tflite_update)
    interpreter.allocate_tensors()
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()



    args = jnp.ones(2)

    expected = _update(args)

    # for a, d in zip(args, input_details):
    interpreter.set_tensor(input_details[0]['index'], args)

    interpreter.invoke()
    return

```

full [example](https://github.com/krzysztofrusek/reinforced-lib/blob/lite/examples/lite/lite.py)

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Conversion works
- Interpreter fails at invoke

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```Traceback (most recent call last):
  File ""/Users/krzysiek/Library/Application Support/JetBrains/Toolbox/apps/PyCharm-P/ch-0/223.8836.34/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1496, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""/Users/krzysiek/Library/Application Support/JetBrains/Toolbox/apps/PyCharm-P/ch-0/223.8836.34/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/Users/krzysiek/Documents/ml4wifi/reinforced-lib/examples/lite/lite.py"", line 168, in <module>
    main3()
  File ""/Users/krzysiek/Documents/ml4wifi/reinforced-lib/examples/lite/lite.py"", line 163, in main3
    interpreter.invoke()
  File ""/Users/krzysiek/Documents/ml4wifi/ftmrate_internal/venv/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py"", line 917, in invoke
    self._interpreter.Invoke()
RuntimeError: Updates shape must have rank at least one. Found:[]Node number 1 (TfLiteFlexDelegate) failed to invoke.
```
"
60089,InputCellState is incorrectly populated for TFLite model to MLIR conversion using flatbuffer_translate,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
          Linux Debian 11
- TensorFlow installed from (source or binary):
          Compiled from source
- TensorFlow version (or github SHA if from source):
          744dad26ef526690319042030f776e6f7e62dbc8

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```import tensorflow as tf
import numpy as np

# Define and create the model
model = tf.keras.models.Sequential([
    tf.keras.layers.Input(shape=(3, 5), name='input'),
    tf.keras.layers.LSTM(10, time_major=False, return_sequences=True)
])

model.compile(optimizer='adam',
              loss='mean_squared_error',
              metrics=['accuracy'])

run_model = tf.function(lambda x: model(x))
concrete_func = run_model.get_concrete_function(
    tf.TensorSpec([batchSize, sequenceLength, numFeatures], model.inputs[0].dtype))

print(""hidden_states: "", model.layers[0].states[0])
print(""cell_states: "", model.layers[0].states[1])

# model directory.
MODEL_DIR = ""/tmp/lstmNet""
model.save(MODEL_DIR, save_format=""tf"", signatures=concrete_func)

converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)
tflite_model = converter.convert()
  
# Save the TF Lite model.
with tf.io.gfile.GFile('/tmp/lstmNet.tflite', 'wb') as f:
  f.write(tflite_model)
```

**Any other info / logs**

Using `flatbuffer_translate` to convert the generated `TFLite` model to `MLIR` produces:

```
$ > flatbuffer_translate --tflite-flatbuffer-to-mlir /tmp/lstmNet.tflite

module attributes {tf_saved_model.semantics, tfl.description = ""MLIR Converted."", tfl.schema_version = 3 : i32} {
  func.func @predict(%arg0: tensor<1x5x3xf32> {tf_saved_model.index_path = [""x""]}) -> (tensor<1x5x10xf32> {tf_saved_model.index_path = [""output_0""]}) attributes {tf.entry_function = {inputs = ""serving_default_x:0"", outputs = ""StatefulPartitionedCall:0""}, tf_saved_model.exported_names = [""serving_default""]} {
    %0 = ""tfl.pseudo_const""() {value = dense<[[-0.130185053, -0.0151278675, 0.0130760074], [-0.258772284, 0.299689293, -0.195314541], [0.252850413, -0.259092718, -0.0803229808], [-0.220947981, 0.155216038, 0.108377606], [0.00254765153, 0.111942321, -0.219952658], [0.206842721, -0.193888605, 0.1106188], [0.0955285131, 0.157347143, 0.221373796], [-0.276973069, -0.0735740363, -2.882380e-01], [0.012721926, 0.0903562903, -0.161965311], [-0.119528085, -0.037569046, -0.362928301]]> : tensor<10x3xf32>} : () -> tensor<10x3xf32>
    %1 = ""tfl.pseudo_const""() {value = dense<[[-0.0352886915, 0.21145165, -0.165998831], [0.155003309, 0.144935846, 0.217351139], [-0.351629466, 0.341497242, -0.217549637], [0.0939139425, -0.0606328547, 0.197987914], [0.339334488, -0.0430043638, -0.193897158], [0.188981593, -0.00256928802, 0.357774317], [-0.053791374, -0.159659907, -0.334026635], [-0.313022763, 0.120892107, 0.365564883], [-0.0173099339, 0.0726312696, -0.256803274], [-0.0634435713, 0.320655167, -0.342872471]]> : tensor<10x3xf32>} : () -> tensor<10x3xf32>
    %2 = ""tfl.pseudo_const""() {value = dense<[[-0.0151669681, 0.246311307, -0.0844985544], [0.00762689113, 0.150569379, -0.275011361], [0.0549676716, 0.0834532678, 0.159000754], [0.0447338223, -0.339231104, 0.134988308], [0.160350919, -0.0878992974, 0.0488999486], [0.323455155, 0.345792234, 0.061250925], [0.0837553441, -0.272862256, 0.0991969704], [0.0828025043, -0.364639938, -0.144624218], [0.343984544, -0.183882296, -0.358834654], [-0.133859664, -0.0814070403, 0.36716789]]> : tensor<10x3xf32>} : () -> tensor<10x3xf32>
    %3 = ""tfl.pseudo_const""() {value = dense<[[0.284724414, 0.148204803, -0.349271417], [-0.302493066, 0.158759058, -0.236835971], [0.334069431, -0.00801679491, 0.00450757146], [0.0284658968, 0.0569611788, -0.167743862], [-0.12706618, 7.704550e-02, -0.0319027305], [0.0807663202, -0.0853067636, -0.171359152], [-0.143240675, 0.320195258, -0.107624263], [0.134614825, 0.137890339, 0.220042884], [-0.0685030818, 0.0266759694, -0.279772133], [-0.123277575, -0.130606934, 0.155195773]]> : tensor<10x3xf32>} : () -> tensor<10x3xf32>
    %4 = ""tfl.pseudo_const""() {value = dense<[[0.17501545, 0.386878431, -0.0484701507, -0.0765462443, -0.181067079, -0.0750025958, -0.185812324, 0.0773477107, -0.00309249177, -0.0284322314], [0.134114936, 0.176706672, 0.189387321, 0.149077475, 0.142265841, 0.263391763, 0.0858101398, -0.334216356, -0.186852977, 0.164630443], [-0.00760664651, -0.217253342, 0.201065645, 0.248932779, -0.109697223, 0.0161245521, 0.059841346, -0.260818273, -0.0882004871, -0.274861604], [0.348256409, 5.338460e-02, 0.0468219183, -0.0478721932, -0.165056169, -0.0841045827, 0.321953684, -0.164883882, -0.110893697, 0.123676449], [0.128447369, 0.0639217123, 0.224082395, -0.0548633076, 0.0286352951, 0.224867627, -0.0468161702, 0.208391294, -0.0053243367, -0.0550255962], [-8.189090e-02, 0.0486251637, 0.0460817255, 0.107632667, 0.118330166, -0.0682023465, -0.109725371, 0.125324547, -0.0441931672, 0.148130983], [0.0746245757, 0.124206074, 0.176272869, 0.0834054648, 0.173528254, -0.187932938, -0.215293854, -0.103552267, -0.141145512, 0.0601227432], [0.0378115289, 0.0337749943, 0.0378107131, 0.160787702, -0.216121212, 0.229908451, -0.0723084137, 0.226159409, 0.0131477024, 0.0372102521], [0.0926874801, -0.06026401, -0.0561813228, -0.148479104, 0.287242681, 0.0332023241, -0.220059201, 0.0408726893, 0.191863343, 0.0540938973], [0.133774817, -0.278124928, -0.113644354, -0.0739784315, -0.316670179, -0.11459551, -0.264918804, -0.00448995735, -0.0878850519, -0.133028492]]> : tensor<10x10xf32>} : () -> tensor<10x10xf32>
    %5 = ""tfl.pseudo_const""() {value = dense<[[0.264959276, -0.259054482, -0.110992216, -0.0414756909, 0.0988652482, 0.33331418, -0.348624617, -0.13201724, 0.00749636581, 0.0932318419], [0.210364223, -0.0775818601, -0.0835916772, 0.21802707, 0.0432840511, -0.0722324625, -0.140951559, 0.150197655, 0.0137763629, 0.139982209], [0.0820472389, 0.230024397, -0.156220302, 0.391181529, -0.0579637811, 0.0591350347, 0.0986427441, 0.237236544, 0.187094688, 0.0165050365], [-0.25562951, 0.0437992811, -0.146220565, 0.107471354, 0.00600573421, 0.0497420132, -0.134210557, 0.0306625273, -0.00543002971, -0.0933061838], [-0.0681353137, 0.187344134, -0.0387082808, -0.0682450234, -0.184748515, 0.295936018, 0.0660243928, -0.00609975681, -0.0220854413, -0.0215770602], [0.123715289, -0.149823353, 0.200297117, -0.138535887, 0.0972650945, -0.100543253, 0.0232232288, -0.0455124453, 0.188970223, 0.0623518042], [0.051465977, -0.16720742, -0.192865178, -0.255042017, -0.216406658, 0.00651523052, 0.182573959, -0.127715543, 0.197372794, -0.222561017], [0.126112625, 0.23686114, -0.0976715758, -0.0628687739, 0.00957589969, -0.211140588, -0.284029543, -0.145181119, -0.167763934, -0.224568278], [-0.0698507354, 0.0349835455, 0.0666678548, 0.0400003493, 0.246126533, -0.107588813, 0.103801601, -0.0709155499, -0.131777883, -0.106253646], [-0.207862586, 0.016677089, -0.237795576, -0.033603251, 0.227860093, -0.00418378413, 0.0745725333, -0.1573136, 0.183234155, 0.0573116578]]> : tensor<10x10xf32>} : () -> tensor<10x10xf32>
    %6 = ""tfl.pseudo_const""() {value = dense<[[0.120267294, -0.0315921716, -0.207551152, -0.0469613187, -0.0472462848, 0.136440426, 0.0656398907, 0.0643941164, -0.214579433, 0.25941658], [-0.132091984, 0.142318785, 0.158400849, 0.0916790738, -0.222117975, -0.10034211, -0.134736136, 0.0465843715, 0.0484883301, 0.262922645], [0.0632713437, -0.15643549, -0.410313338, -0.0796892642, 0.0193372741, -0.192711219, 0.084401071, -0.0132020218, 0.0214749519, 0.217391357], [-0.023331102, -0.234836608, 0.283015937, -0.27972725, -0.0461517051, -0.202469319, -0.140018716, 0.266479582, -0.199459061, 0.0869432539], [-0.173759624, -0.146702722, 0.00280229794, 0.128232807, -0.13735646, -0.156450719, -0.197670206, -0.402188092, 0.182303295, 0.127702326], [0.1447341, -0.234402686, 0.017554732, 0.1726809, 0.14695932, 0.111371085, 0.109428726, 0.268820375, -0.0866426378, -0.0679584667], [-0.0736832693, -0.108465984, 0.223148763, 0.00207955297, 0.115413204, -0.0324923247, 0.151184335, -0.0644928366, 0.0295013655, -0.0610723197], [0.104881756, 0.166022196, -0.142411351, 0.142822742, 0.12192411, -1.49629079E-4, -0.335838586, -0.156993166, -0.134479508, -0.183696106], [0.131517142, 0.164839372, 0.0969391465, -0.153943628, 0.251961887, 0.0231811684, -0.0071518924, -0.0611652061, 0.0723072588, -0.00650136918], [0.184471652, 0.0503823385, -0.0937841534, 0.118845418, 0.154655725, -0.47709918, 0.186724663, 0.162837937, -0.161638841, -0.274689823]]> : tensor<10x10xf32>} : () -> tensor<10x10xf32>
    %7 = ""tfl.pseudo_const""() {value = dense<[[0.0217915159, 0.206113726, -0.190372929, -0.422592759, 0.0248149242, -0.0202117879, 7.80003611E-5, 0.0636207908, -0.129249915, 0.195001438], [-0.0265121292, 0.157053322, -0.116082504, -0.0937363803, 0.155826345, 0.10708677, 0.110328041, -0.136170894, 0.143848643, -0.217342377], [-0.182416916, 0.0980726927, 6.768720e-02, -0.134475529, 0.00583284162, -0.0480525941, 0.0232472904, -0.146422684, -0.125577226, 0.231070623], [-0.404781759, -0.0607755706, 0.0381766148, 0.060485743, 0.0692789554, -0.0851613953, -0.151829481, 0.0906099975, -0.0977554768, 0.120315634], [0.147763401, 0.0345406979, 0.121470168, -0.0796541571, -0.0689968764, 0.0199192017, -0.187039837, -0.0605593659, 0.307765067, -0.132991672], [0.128111526, 0.0928409397, 0.315211028, -0.200447187, -0.0918775648, -0.0101282364, 0.0570981055, -0.0672892854, -0.0190094449, 0.0233988091], [-0.00752805918, -0.0974235087, 0.0578115322, -0.154167339, 0.30695793, 0.159499139, -0.102361277, 0.186416253, 0.11203637, -0.187620446], [0.168520495, -0.0998088344, -0.0158915576, 0.101420805, -0.122099787, 0.0111542856, -0.049965702, -0.115302391, -0.121384457, -0.00166527322], [-0.169698805, -0.10477908, -0.141094357, -0.10161002, 0.02446693, 0.249826044, 0.0071637705, -0.052272439, -0.567903876, -0.189442679], [0.225991249, -0.16400744, -0.0723658279, 0.153675273, 0.228954688, -0.0319412872, 0.0650407076, -0.126990899, -0.0396433137, 0.326944739]]> : tensor<10x10xf32>} : () -> tensor<10x10xf32>
    %8 = ""tfl.no_value""() {value} : () -> none
    %9 = ""tfl.pseudo_const""() {value = dense<0.000000e+00> : tensor<10xf32>} : () -> tensor<10xf32>
    %10 = ""tfl.pseudo_const""() {value = dense<1.000000e+00> : tensor<10xf32>} : () -> tensor<10xf32>
    %11 = ""tfl.pseudo_const""() {value = dense<0.000000e+00> : tensor<1x10xf32>} : () -> tensor<1x10xf32>
    %12 = ""tfl.pseudo_const""() {value = dense<1.000000e+00> : tensor<1x10xf32>} : () -> tensor<1x10xf32>
    %13 = ""tfl.unidirectional_sequence_lstm""(%arg0, %0, %1, %2, %3, %4, %5, %6, %7, %8, %8, %8, %9, %10, %9, %9, %8, %8, %11, %12, %8, %8, %8, %8) {asymmetric_quantize_inputs = false, cell_clip = 1.000000e+01 : f32, fused_activation_function = ""TANH"", proj_clip = 0.000000e+00 : f32, time_major = false} : (tensor<1x5x3xf32>, tensor<10x3xf32>, tensor<10x3xf32>, tensor<10x3xf32>, tensor<10x3xf32>, tensor<10x10xf32>, tensor<10x10xf32>, tensor<10x10xf32>, tensor<10x10xf32>, none, none, none, tensor<10xf32>, tensor<10xf32>, tensor<10xf32>, tensor<10xf32>, none, none, tensor<1x10xf32>, tensor<1x10xf32>, none, none, none, none) -> tensor<1x5x10xf32>
    return %13 : tensor<1x5x10xf32>
  }
}
```

Given that the `model.layers[0].states[1]` is `none` the `InputCellState` value for the `tfl.unidirectional_sequence_lstm` op which is `%12` should have been all zeros, but it is all ones. Note that this value is interpreted as all zeros when using the `tf_tfl_translate` command : 

```
$> tf_tfl_translate --savedmodel-signaturedefs-to-mlir /tmp/lstmNet/ --output-mlir

module attributes {tf.versions = {bad_consumers = [], min_consumer = 12 : i32, producer = 1286 : i32}, tf_saved_model.semantics, tfl._legalize_tfl_variables = true} {
  func.func @serving_default(%arg0: tensor<1x5x3xf32> {tf_saved_model.index_path = [""x""]}) -> (tensor<1x5x10xf32> {tf_saved_model.index_path = [""output_0""]}) attributes {tf.entry_function = {control_outputs = """", inputs = ""serving_default_x:0"", outputs = ""StatefulPartitionedCall:0""}, tf_saved_model.exported_names = [""serving_default""]} {
    %cst = arith.constant dense<0.000000e+00> : tensor<1x10xf32>
    %cst_0 = arith.constant dense<[[0.17501545, 0.386878431, -0.0484701507, -0.0765462443, -0.181067079, -0.0750025958, -0.185812324, 0.0773477107, -0.00309249177, -0.0284322314], [0.134114936, 0.176706672, 0.189387321, 0.149077475, 0.142265841, 0.263391763, 0.0858101398, -0.334216356, -0.186852977, 0.164630443], [-0.00760664651, -0.217253342, 0.201065645, 0.248932779, -0.109697223, 0.0161245521, 0.059841346, -0.260818273, -0.0882004871, -0.274861604], [0.348256409, 5.338460e-02, 0.0468219183, -0.0478721932, -0.165056169, -0.0841045827, 0.321953684, -0.164883882, -0.110893697, 0.123676449], [0.128447369, 0.0639217123, 0.224082395, -0.0548633076, 0.0286352951, 0.224867627, -0.0468161702, 0.208391294, -0.0053243367, -0.0550255962], [-8.189090e-02, 0.0486251637, 0.0460817255, 0.107632667, 0.118330166, -0.0682023465, -0.109725371, 0.125324547, -0.0441931672, 0.148130983], [0.0746245757, 0.124206074, 0.176272869, 0.0834054648, 0.173528254, -0.187932938, -0.215293854, -0.103552267, -0.141145512, 0.0601227432], [0.0378115289, 0.0337749943, 0.0378107131, 0.160787702, -0.216121212, 0.229908451, -0.0723084137, 0.226159409, 0.0131477024, 0.0372102521], [0.0926874801, -0.06026401, -0.0561813228, -0.148479104, 0.287242681, 0.0332023241, -0.220059201, 0.0408726893, 0.191863343, 0.0540938973], [0.133774817, -0.278124928, -0.113644354, -0.0739784315, -0.316670179, -0.11459551, -0.264918804, -0.00448995735, -0.0878850519, -0.133028492]]> : tensor<10x10xf32>
    %cst_1 = arith.constant dense<[[0.264959276, -0.259054482, -0.110992216, -0.0414756909, 0.0988652482, 0.33331418, -0.348624617, -0.13201724, 0.00749636581, 0.0932318419], [0.210364223, -0.0775818601, -0.0835916772, 0.21802707, 0.0432840511, -0.0722324625, -0.140951559, 0.150197655, 0.0137763629, 0.139982209], [0.0820472389, 0.230024397, -0.156220302, 0.391181529, -0.0579637811, 0.0591350347, 0.0986427441, 0.237236544, 0.187094688, 0.0165050365], [-0.25562951, 0.0437992811, -0.146220565, 0.107471354, 0.00600573421, 0.0497420132, -0.134210557, 0.0306625273, -0.00543002971, -0.0933061838], [-0.0681353137, 0.187344134, -0.0387082808, -0.0682450234, -0.184748515, 0.295936018, 0.0660243928, -0.00609975681, -0.0220854413, -0.0215770602], [0.123715289, -0.149823353, 0.200297117, -0.138535887, 0.0972650945, -0.100543253, 0.0232232288, -0.0455124453, 0.188970223, 0.0623518042], [0.051465977, -0.16720742, -0.192865178, -0.255042017, -0.216406658, 0.00651523052, 0.182573959, -0.127715543, 0.197372794, -0.222561017], [0.126112625, 0.23686114, -0.0976715758, -0.0628687739, 0.00957589969, -0.211140588, -0.284029543, -0.145181119, -0.167763934, -0.224568278], [-0.0698507354, 0.0349835455, 0.0666678548, 0.0400003493, 0.246126533, -0.107588813, 0.103801601, -0.0709155499, -0.131777883, -0.106253646], [-0.207862586, 0.016677089, -0.237795576, -0.033603251, 0.227860093, -0.00418378413, 0.0745725333, -0.1573136, 0.183234155, 0.0573116578]]> : tensor<10x10xf32>
    %cst_2 = arith.constant dense<[[0.120267294, -0.0315921716, -0.207551152, -0.0469613187, -0.0472462848, 0.136440426, 0.0656398907, 0.0643941164, -0.214579433, 0.25941658], [-0.132091984, 0.142318785, 0.158400849, 0.0916790738, -0.222117975, -0.10034211, -0.134736136, 0.0465843715, 0.0484883301, 0.262922645], [0.0632713437, -0.15643549, -0.410313338, -0.0796892642, 0.0193372741, -0.192711219, 0.084401071, -0.0132020218, 0.0214749519, 0.217391357], [-0.023331102, -0.234836608, 0.283015937, -0.27972725, -0.0461517051, -0.202469319, -0.140018716, 0.266479582, -0.199459061, 0.0869432539], [-0.173759624, -0.146702722, 0.00280229794, 0.128232807, -0.13735646, -0.156450719, -0.197670206, -0.402188092, 0.182303295, 0.127702326], [0.1447341, -0.234402686, 0.017554732, 0.1726809, 0.14695932, 0.111371085, 0.109428726, 0.268820375, -0.0866426378, -0.0679584667], [-0.0736832693, -0.108465984, 0.223148763, 0.00207955297, 0.115413204, -0.0324923247, 0.151184335, -0.0644928366, 0.0295013655, -0.0610723197], [0.104881756, 0.166022196, -0.142411351, 0.142822742, 0.12192411, -1.49629079E-4, -0.335838586, -0.156993166, -0.134479508, -0.183696106], [0.131517142, 0.164839372, 0.0969391465, -0.153943628, 0.251961887, 0.0231811684, -0.0071518924, -0.0611652061, 0.0723072588, -0.00650136918], [0.184471652, 0.0503823385, -0.0937841534, 0.118845418, 0.154655725, -0.47709918, 0.186724663, 0.162837937, -0.161638841, -0.274689823]]> : tensor<10x10xf32>
    %cst_3 = arith.constant dense<[[0.0217915159, 0.206113726, -0.190372929, -0.422592759, 0.0248149242, -0.0202117879, 7.80003611E-5, 0.0636207908, -0.129249915, 0.195001438], [-0.0265121292, 0.157053322, -0.116082504, -0.0937363803, 0.155826345, 0.10708677, 0.110328041, -0.136170894, 0.143848643, -0.217342377], [-0.182416916, 0.0980726927, 6.768720e-02, -0.134475529, 0.00583284162, -0.0480525941, 0.0232472904, -0.146422684, -0.125577226, 0.231070623], [-0.404781759, -0.0607755706, 0.0381766148, 0.060485743, 0.0692789554, -0.0851613953, -0.151829481, 0.0906099975, -0.0977554768, 0.120315634], [0.147763401, 0.0345406979, 0.121470168, -0.0796541571, -0.0689968764, 0.0199192017, -0.187039837, -0.0605593659, 0.307765067, -0.132991672], [0.128111526, 0.0928409397, 0.315211028, -0.200447187, -0.0918775648, -0.0101282364, 0.0570981055, -0.0672892854, -0.0190094449, 0.0233988091], [-0.00752805918, -0.0974235087, 0.0578115322, -0.154167339, 0.30695793, 0.159499139, -0.102361277, 0.186416253, 0.11203637, -0.187620446], [0.168520495, -0.0998088344, -0.0158915576, 0.101420805, -0.122099787, 0.0111542856, -0.049965702, -0.115302391, -0.121384457, -0.00166527322], [-0.169698805, -0.10477908, -0.141094357, -0.10161002, 0.02446693, 0.249826044, 0.0071637705, -0.052272439, -0.567903876, -0.189442679], [0.225991249, -0.16400744, -0.0723658279, 0.153675273, 0.228954688, -0.0319412872, 0.0650407076, -0.126990899, -0.0396433137, 0.326944739]]> : tensor<10x10xf32>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<10xf32>
    %cst_5 = arith.constant dense<1.000000e+00> : tensor<10xf32>
    %cst_6 = arith.constant dense<[[-0.130185053, -0.0151278675, 0.0130760074], [-0.258772284, 0.299689293, -0.195314541], [0.252850413, -0.259092718, -0.0803229808], [-0.220947981, 0.155216038, 0.108377606], [0.00254765153, 0.111942321, -0.219952658], [0.206842721, -0.193888605, 0.1106188], [0.0955285131, 0.157347143, 0.221373796], [-0.276973069, -0.0735740363, -2.882380e-01], [0.012721926, 0.0903562903, -0.161965311], [-0.119528085, -0.037569046, -0.362928301]]> : tensor<10x3xf32>
    %cst_7 = arith.constant dense<[[-0.0352886915, 0.21145165, -0.165998831], [0.155003309, 0.144935846, 0.217351139], [-0.351629466, 0.341497242, -0.217549637], [0.0939139425, -0.0606328547, 0.197987914], [0.339334488, -0.0430043638, -0.193897158], [0.188981593, -0.00256928802, 0.357774317], [-0.053791374, -0.159659907, -0.334026635], [-0.313022763, 0.120892107, 0.365564883], [-0.0173099339, 0.0726312696, -0.256803274], [-0.0634435713, 0.320655167, -0.342872471]]> : tensor<10x3xf32>
    %cst_8 = arith.constant dense<[[-0.0151669681, 0.246311307, -0.0844985544], [0.00762689113, 0.150569379, -0.275011361], [0.0549676716, 0.0834532678, 0.159000754], [0.0447338223, -0.339231104, 0.134988308], [0.160350919, -0.0878992974, 0.0488999486], [0.323455155, 0.345792234, 0.061250925], [0.0837553441, -0.272862256, 0.0991969704], [0.0828025043, -0.364639938, -0.144624218], [0.343984544, -0.183882296, -0.358834654], [-0.133859664, -0.0814070403, 0.36716789]]> : tensor<10x3xf32>
    %cst_9 = arith.constant dense<[[0.284724414, 0.148204803, -0.349271417], [-0.302493066, 0.158759058, -0.236835971], [0.334069431, -0.00801679491, 0.00450757146], [0.0284658968, 0.0569611788, -0.167743862], [-0.12706618, 7.704550e-02, -0.0319027305], [0.0807663202, -0.0853067636, -0.171359152], [-0.143240675, 0.320195258, -0.107624263], [0.134614825, 0.137890339, 0.220042884], [-0.0685030818, 0.0266759694, -0.279772133], [-0.123277575, -0.130606934, 0.155195773]]> : tensor<10x3xf32>
    %0 = ""tfl.no_value""() {value} : () -> none
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<1x10xf32>
    %1 = ""tfl.unidirectional_sequence_lstm""(%arg0, %cst_6, %cst_7, %cst_8, %cst_9, %cst_0, %cst_1, %cst_2, %cst_3, %0, %0, %0, %cst_4, %cst_5, %cst_4, %cst_4, %0, %0, %cst, %cst_10, %0, %0, %0, %0) {cell_clip = 1.000000e+01 : f32, diagonal_recurrent_tensors = false, fused_activation_function = ""TANH"", proj_clip = 0.000000e+00 : f32, time_major = false} : (tensor<1x5x3xf32>, tensor<10x3xf32>, tensor<10x3xf32>, tensor<10x3xf32>, tensor<10x3xf32>, tensor<10x10xf32>, tensor<10x10xf32>, tensor<10x10xf32>, tensor<10x10xf32>, none, none, none, tensor<10xf32>, tensor<10xf32>, tensor<10xf32>, tensor<10xf32>, none, none, tensor<1x10xf32>, tensor<1x10xf32>, none, none, none, none) -> tensor<1x5x10xf32>
    return %1 : tensor<1x5x10xf32>
  }
}
```

Here `%cst_10` which is the `InputCellState` value is all zeros. So the error is coming from the TFLite file to MLIR conversion when invoked via `flatbuffer_translate` command.

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
60088,Selectively build TFLite for iOS failure,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

No

### OS Platform and Distribution

MacOS Ventura 13.1

### Mobile device

N/A

### Python version

3.9.16

### Bazel version

5.3

### GCC/Compiler version

14.0.0

### CUDA/cuDNN version

N/A

### GPU model and memory

N/A

### Current Behaviour?

```shell
This was a subsequence bug from another issue(https://github.com/tensorflow/tensorflow/issues/59853#issuecomment-1481642687) and @yishuangP asked me to file a separate issue for Tensorflow Tools.

I'm trying to build a tf framework from certain tflite model to reduce framework size. However I keep seeing below error. The command I'm using is `bash tensorflow/lite/ios/build_frameworks.sh --input_models=tflite_model_vocab_changes_run_18026681_full_dataset.tflite --target_archs=x86_64,armv7,arm64`


ERROR: /Users/ghe/projects/tensorflow/tensorflow/BUILD:1128:21: declared output 'tensorflow/libtensorflow_framework.2.dylib' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)
ERROR: /Users/ghe/projects/tensorflow/tensorflow/BUILD:1128:21: Executing genrule //tensorflow:libtensorflow_framework.2.dylib_sym [for host] failed: not all outputs were created or valid
realpath: illegal option -- -
usage: realpath [-q] [path ...]
Target //tensorflow/lite/ios/tmp:TensorFlowLiteSelectTfOps_framework failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1046.736s, Critical Path: 138.53s
INFO: 3339 processes: 25 internal, 3314 local.
FAILED: Build did NOT complete successfully
```
```


### Standalone code to reproduce the issue

```shell
There is no code involved. To reproduce:

1. git fetch a new project
2. Copy the toy model I provided into its root directory
3. Run `bash tensorflow/lite/ios/build_frameworks.sh --input_models=tflite_model_vocab_changes_run_18026681_full_dataset.tflite --target_archs=x86_64,armv7,arm64` command

It will compile for a couple minutes, and fail finally.
```


### Relevant log output

```shell
$ bash tensorflow/lite/ios/build_frameworks.sh --input_models=tflite_model_vocab_changes_run_18026681_full_dataset.tflite --target_archs=x86_64,armv7,arm64
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=314
INFO: Reading rc options for 'build' from /Users/ghe/projects/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /Users/ghe/projects/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /Users/ghe/projects/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/Applications/Xcode.app/Contents/Developer/usr/bin/python3 --action_env PYTHON_LIB_PATH= /Library/Python/3.9/site-packages --python_path=/Applications/Xcode.app/Contents/Developer/usr/bin/python3
INFO: Reading rc options for 'build' from /Users/ghe/projects/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /Users/ghe/projects/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /Users/ghe/projects/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:ios in file /Users/ghe/projects/tensorflow/.bazelrc: --apple_platform_type=ios --apple_bitcode=embedded --copt=-fembed-bitcode --copt=-Wno-c++11-narrowing --noenable_platform_specific_config --copt=-w --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --define=with_xla_support=false
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/55dd04f6bcf797b4ff20e74158377bcc912b9870.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
INFO: Analyzed target //tensorflow/lite/ios/tmp:TensorFlowLiteC_framework (144 packages loaded, 7602 targets configured).
INFO: Found 1 target...
Target //tensorflow/lite/ios/tmp:TensorFlowLiteC_framework up-to-date:
  bazel-out/applebin_ios-ios_x86_64-opt-ST-74bbdaf492e7/bin/tensorflow/lite/ios/tmp/TensorFlowLiteC_framework.zip
INFO: Elapsed time: 38.614s, Critical Path: 5.29s
INFO: 13 processes: 2 internal, 11 local.
INFO: Build completed successfully, 13 total actions
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=314
INFO: Reading rc options for 'build' from /Users/ghe/projects/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /Users/ghe/projects/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /Users/ghe/projects/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/Applications/Xcode.app/Contents/Developer/usr/bin/python3 --action_env PYTHON_LIB_PATH= /Library/Python/3.9/site-packages --python_path=/Applications/Xcode.app/Contents/Developer/usr/bin/python3
INFO: Reading rc options for 'build' from /Users/ghe/projects/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /Users/ghe/projects/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /Users/ghe/projects/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:monolithic in file /Users/ghe/projects/tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false
INFO: Found applicable config definition build:macos in file /Users/ghe/projects/tensorflow/.bazelrc: --apple_platform_type=macos --copt=-DGRPC_BAZEL_BUILD --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/55dd04f6bcf797b4ff20e74158377bcc912b9870.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
INFO: Build options --apple_bitcode, --apple_platform_type, --copt, and 1 more have changed, discarding analysis cache.
INFO: Analyzed target //tensorflow/lite/tools:list_flex_ops_no_kernel_main (7 packages loaded, 1231 targets configured).
INFO: Found 1 target...
Target //tensorflow/lite/tools:list_flex_ops_no_kernel_main up-to-date:
  bazel-bin/tensorflow/lite/tools/list_flex_ops_no_kernel_main
INFO: Elapsed time: 0.292s, Critical Path: 0.00s
INFO: 1 process: 1 internal.
INFO: Build completed successfully, 1 total action
~/projects/tensorflow/tensorflow/lite/ios/tmp ~/projects/tensorflow
~/projects/tensorflow
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=314
INFO: Reading rc options for 'build' from /Users/ghe/projects/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /Users/ghe/projects/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /Users/ghe/projects/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/Applications/Xcode.app/Contents/Developer/usr/bin/python3 --action_env PYTHON_LIB_PATH= /Library/Python/3.9/site-packages --python_path=/Applications/Xcode.app/Contents/Developer/usr/bin/python3
INFO: Reading rc options for 'build' from /Users/ghe/projects/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /Users/ghe/projects/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /Users/ghe/projects/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:ios in file /Users/ghe/projects/tensorflow/.bazelrc: --apple_platform_type=ios --apple_bitcode=embedded --copt=-fembed-bitcode --copt=-Wno-c++11-narrowing --noenable_platform_specific_config --copt=-w --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --define=with_xla_support=false
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/55dd04f6bcf797b4ff20e74158377bcc912b9870.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
INFO: Build options --apple_bitcode, --apple_platform_type, --copt, and 2 more have changed, discarding analysis cache.
INFO: Analyzed target //tensorflow/lite/ios/tmp:TensorFlowLiteSelectTfOps_framework (435 packages loaded, 36544 targets configured).
INFO: Found 1 target...
ERROR: /Users/ghe/projects/tensorflow/tensorflow/BUILD:1128:21: declared output 'tensorflow/libtensorflow_framework.2.dylib' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)
ERROR: /Users/ghe/projects/tensorflow/tensorflow/BUILD:1128:21: Executing genrule //tensorflow:libtensorflow_framework.2.dylib_sym [for host] failed: not all outputs were created or valid
realpath: illegal option -- -
usage: realpath [-q] [path ...]
Target //tensorflow/lite/ios/tmp:TensorFlowLiteSelectTfOps_framework failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1046.736s, Critical Path: 138.53s
INFO: 3339 processes: 25 internal, 3314 local.
FAILED: Build did NOT complete successfully
```
</details>"
60087,Plot training and validation losses of object detection model ,"Hello. I am using the following notebook to train my dataset with **efficientdet-lite0** model. 
https://www.tensorflow.org/lite/models/modify/model_maker/object_detection

I can see for each epoch we get the information of training loss ""loss"" and validation loss ""val_loss"". I would like to plot them like:

loss = model.history['loss']
val_loss = model.history['val_loss']
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

And unfortunately getting the following error **AttributeError: 'ObjectDetector' object has no attribute 'history'** 
Can you help me to figure out how to plot training and validation losses on the same graph. Thanks in advance!"
60085,A shape mismatch error occurs when using the atan2 function with broadcasted inputs,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A shape mismatch error can occur when using the atan2 function with broadcasted inputs. This is because the function broadcasts b, to match the shape of a. However, this broadcasting can introduce errors in the gradient computation. Specifically, in this case, the shape of 'b' was broadcasted to [1, 1, 3, 3] according to the output of the atan2 function, but it remains the same shape as the gradient source. To ensure consistent behavior between the forward and backward passes, it would be desirable to avoid such errors.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

a = tf.ones([1, 1, 3, 3])
b = tf.ones([3, 3])

def atan2(a, b):
    a_b = tf.math.atan2(a, b)
    return a_b

value = atan2(a, b)
print(value.shape)
with tf.GradientTape() as tape:
    tape.watch(b)
    value2 = atan2(a, b)

gradient = tape.jacobian(value2, b)
print(gradient)
```


### Relevant log output

```shell
(1, 1, 3, 3)
ValueError: Tensor's shape (1, 1, 3, 3, 1, 1, 3, 3) is not compatible with supplied shape (1, 1, 3, 3, 3, 3).
```
</details>"
60084,tf.py_function throws KeyError when computing Jacobian matrix,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tf.py_function throws KeyError when computing Jacobian matrix. It works well when it is computing gradient.
```


### Standalone code to reproduce the issue

```shell
import os
import tensorflow as tf
import numpy as np

a = tf.Variable([1.0, 2.0])

def square(a):
    return tf.py_function(lambda a: a ** 2, [a], a.dtype)

with tf.GradientTape(persistent=True) as tape:
    tape.watch(a)
    y = square(a)
# gradient = tape.gradient(y, a) # pass
jacobian = tape.jacobian(y, a)
print(jacobian)
```


### Relevant log output

```shell
Node: 'gradient_tape/EagerPyFunc'
KeyError: b'pyfunc_0'


         [[{{node gradient_tape/EagerPyFunc}}]] [Op:__inference_f_173]
```
</details>"
60083,Build from source on Windows failing ,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 
- TensorFlow installation (pip package or built from source): Build from source on Windows
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.12

bazel version - 5.3
python version - 3.9 

i am following below link -

https://www.tensorflow.org/install/source_windows


command that I am  running - 
bazel build //tensorflow/tools/pip_package:build_pip_package




Error message -

Repository rule _tf_http_archive defined at:
  C:/users/anmaheshwari/downloads/tf/tensorflow-2.12.0/tensorflow-2.12.0/third_party/repo.bzl:89:35: in <toplevel>
ERROR: C:/users/anmaheshwari/downloads/tf/tensorflow-2.12.0/tensorflow-2.12.0/tensorflow/core/BUILD:511:11: //tensorflow/core:ops depends on //tensorflow/compiler/mlir/tensorflow:mlir_passthrough_op in repository @ which failed to fetch. no such package '@llvm-project//mlir': Failed to find python3 binary
ERROR: Analysis of target '//tensorflow/lite/delegates/flex:tensorflowlite_flex' failed; build aborted:
INFO: Elapsed time: 297.423s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (87 packages loaded, 331 targets configured)
    currently loading: tensorflow/lite/schema ... (4 packages)
    Fetching @flatbuffers; fetching
    Fetching ...ri/grgp23z7/external/flatbuffers; Extracting C:/users/anmaheshwari/_bazel_anmaheshwari/grgp23z7/external/flatbuffers/temp5290822748239123909/v2.0.6.tar.gz
    Fetching @nsync; fetching
    Fetching https://storage.googleapis.com/mirror.tensorflow.org/github.com/abseil/abseil-cpp/archive/273292d1cfc0a94a65082ee350509af1d113344d.tar.gz

C:\Users\Anmaheshwari\Downloads\TF\tensorflow-2.12.0\tensorflow-2.12.0>"
60081,Segmentation fault when importing tensorflow 2.12 in MacOS,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.12

### Custom Code

No

### OS Platform and Distribution

MacOS 13.1

### Mobile device

_No response_

### Python version

3.8.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Segmentation fault when importing just upgraded TF 2.12
```


### Standalone code to reproduce the issue

```shell
alex@MacBook-Pro tfmiss % python
Python 3.8.6 (default, Jun  7 2022, 10:54:52) 
[Clang 13.1.6 (clang-1316.0.21.2.5)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
```


### Relevant log output

```shell
2023-03-23 08:47:14.945316: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
zsh: segmentation fault  python
```
</details>"
60080,Tensorflow version is 2.11.1 in v2.12.0 tag,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

TF 2.12

### Custom Code

Yes

### OS Platform and Distribution

Linux RHEL 8.6

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

5.3

### GCC/Compiler version

11.2

### CUDA/cuDNN version

11.4/8.3

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Version of TF in v2.12.0 tag is 2.11.1. However, it should have been 2.12.0.
```


### Standalone code to reproduce the issue

```shell
https://github.com/tensorflow/tensorflow/blob/v2.12.0/tensorflow/tools/pip_package/setup.py#L49 shows 2.11.1
```


### Relevant log output

_No response_</details>"
60079,failed when changing GPU delegate compatibility database.,"1. System information

OS Platform and Distribution (e.g., Linux Ubuntu 18.04):
TensorFlow Source Code 2.9


2. Code
When following /home/ts/tensorflow_src/tensorflow/lite/experimental/acceleration/compatibility/README.md to change delegate compatibility database, 

When convertting from json to flatbuffer `flatc -b database.fbs -- gpu_compatibility.json`, it gave that 
Usage: flatc [-b|--binary, -c|--cpp, -n|--csharp, -d|--dart, -g|--go, -j|--java,
-t|--json, --jsonschema, --kotlin, --lobster, -l|--lua, --nim, --php, --proto,
-p|--python, -r|--rust, --swift, -T|--ts, -o, -I, -M, --version, -h|--help,
--strict-json, --allow-non-utf8, --natural-utf8, --defaults-json,
--unknown-json, --no-prefix, --scoped-enums, --no-emit-min-max-enum-values,
--swift-implementation-only, --gen-includes, --no-includes, --gen-mutable,
--gen-onefile, --gen-name-strings, --gen-object-api, --gen-compare,
--gen-nullable, --java-package-prefix, --java-checkerframework, --gen-generated,
--gen-jvmstatic, --gen-all, --gen-json-emit, --cpp-include, --cpp-ptr-type,
--cpp-str-type, --cpp-str-flex-ctor, --cpp-field-case-style, --cpp-std,
--cpp-static-reflection, --object-prefix, --object-suffix, --go-namespace,
--go-import, --go-module-name, --raw-binary, --size-prefixed,
--proto-namespace-suffix, --oneof-union, --keep-proto-id, --proto-id-gap,
--grpc, --schema, --bfbs-filenames, --bfbs-comments, --bfbs-builtins,
--bfbs-gen-embed, --conform, --conform-includes, --filename-suffix,
--filename-ext, --include-prefix, --keep-prefix, --reflect-types,
--reflect-names, --rust-serialize, --rust-module-root-file, --root-type,
--require-explicit-ids, --force-defaults, --force-empty, --force-empty-vectors,
--flexbuffers, --no-warnings, --warnings-as-errors, --cs-global-alias,
--cs-gen-json-serializer, --json-nested-bytes, --ts-flat-files,
--ts-entry-points, --annotate-sparse-vectors, --annotate,
--no-leak-private-annotation]... FILE... [-- BINARY_FILE...]

error:
  current schema has no file_identifier: cannot test if ""gpu_compatibility.json"" matches the schema, use --raw-binary to read this file anyway.

3. and then using `flatc -b database.fbs -- gpu_compatibility.json --raw-binary` to convert. 
4. Found that convertted gpu_compatibility.bin size is 117K, but origin one is only 17K.
5.  and then compile
 bazel build -c opt --config=android_arm64 //tensorflow/lite/java:tensorflowlite_gpu
bazel build -c opt --config=android_arm64 //tensorflow/lite/java:libtensorflowlite_gpu_jni.so

Using libtensorflowlite_gpu_jni.so in example image classification android app, and the app was crashed.


Very thanks."
60078,tf.image.convert_image_dtype behaves incorrectly on Windows (but fine on MacOS),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Windows

### Mobile device

_No response_

### Python version

3.10

### Current Behaviour?


Compare the result of the line 

```python
tf.image.convert_image_dtype(1., tf.uint32)
```
Between MacOS and Windows.  
* On Mac it returns `tf.Tensor(4294967295, shape=(), dtype=uint32)`
* On windows, `tf.Tensor(0, shape=(), dtype=uint32)`.

It seems that Mac has the correct result - you expect the result to saturate to the max uint32 value of `2**32-1==4294967295` when the float-input is >=1 - but it actually jumps back down to zero.  This caused a very sneaky bug in my code which caused the detection system to fail to detect in some cases.


### Standalone code to reproduce the issue

```python
assert int(tf.image.convert_image_dtype(1., tf.uint32))==2**32-1
```


### Relevant log output

_No response_</details>"
60074,`pip install tensorflow-gpu` gives `setuptools.extern.packaging.requirements.InvalidRequirement`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.12.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu 22.10

### Mobile device

_No response_

### Python version

Python 3.10.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

My GPU [AMD Ryzen™ 9 6900HX with Radeon™ Graphics × 16] wasn't detected on the regular `pip install tensorflow`, StackOverflow told others to `pip install tensorflow-gpu` which gave me:

```shell
$ pip install tensorflow-gpu
Collecting tensorflow-gpu
  Using cached tensorflow-gpu-2.12.0.tar.gz (2.6 kB)
  Preparing metadata (setup.py) ... error
  error: subprocess-exited-with-error
  
  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─> [39 lines of output]
      Traceback (most recent call last):
        File ""tfenv3/lib/python3.10/site-packages/setuptools/_vendor/packaging/requirements.py"", line 35, in __init__
          parsed = parse_requirement(requirement_string)
        File ""tfenv3/lib/python3.10/site-packages/setuptools/_vendor/packaging/_parser.py"", line 64, in parse_requirement
          return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))
        File ""tfenv3/lib/python3.10/site-packages/setuptools/_vendor/packaging/_parser.py"", line 82, in _parse_requirement
          url, specifier, marker = _parse_requirement_details(tokenizer)
        File ""tfenv3/lib/python3.10/site-packages/setuptools/_vendor/packaging/_parser.py"", line 126, in _parse_requirement_details
          marker = _parse_requirement_marker(
        File ""tfenv3/lib/python3.10/site-packages/setuptools/_vendor/packaging/_parser.py"", line 147, in _parse_requirement_marker
          tokenizer.raise_syntax_error(
        File ""tfenv3/lib/python3.10/site-packages/setuptools/_vendor/packaging/_tokenizer.py"", line 163, in raise_syntax_error
          raise ParserSyntaxError(
      setuptools.extern.packaging._tokenizer.ParserSyntaxError: Expected end or semicolon (after name and no valid version specifier)
          python_version>""3.7""
                        ^
      
      The above exception was the direct cause of the following exception:
      
      Traceback (most recent call last):
        File ""<string>"", line 2, in <module>
        File ""<pip-setuptools-caller>"", line 34, in <module>
        File ""/tmp/pip-install-1uf6oamu/tensorflow-gpu_f068f7e6c62d42d8b5619a7789641e46/setup.py"", line 40, in <module>
          setuptools.setup()
        File ""tfenv3/lib/python3.10/site-packages/setuptools/__init__.py"", line 107, in setup
          _install_setup_requires(attrs)
        File ""tfenv3/lib/python3.10/site-packages/setuptools/__init__.py"", line 78, in _install_setup_requires
          dist.parse_config_files(ignore_option_errors=True)
        File ""tfenv3/lib/python3.10/site-packages/setuptools/dist.py"", line 887, in parse_config_files
          self._finalize_requires()
        File ""tfenv3/lib/python3.10/site-packages/setuptools/dist.py"", line 594, in _finalize_requires
          self._move_install_requirements_markers()
        File ""tfenv3/lib/python3.10/site-packages/setuptools/dist.py"", line 634, in _move_install_requirements_markers
          inst_reqs = list(_reqs.parse(spec_inst_reqs))
        File ""tfenv3/lib/python3.10/site-packages/setuptools/_vendor/packaging/requirements.py"", line 37, in __init__
          raise InvalidRequirement(str(e)) from e
      setuptools.extern.packaging.requirements.InvalidRequirement: Expected end or semicolon (after name and no valid version specifier)
          python_version>""3.7""
                        ^
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.
```

FYI: Running versions setuptools-67.6.0 wheel-0.40.0 pip-23.0.1


### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

_No response_</details>"
60072,TFLITE mobilenetv2 conversion from tf leads to poor accuracy on imagenet evaluation tool,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 20.04
-   **TensorFlow installed from (source or binary)**: Binary
-   **TensorFlow version (use command below)**: 2.11
-   **Python version**: 3.9

### Describe the problem
Mobilenetv2 converted from TF to TFLITE using default MLIR converter leads to poor accuracy on tflite imagenet evaluation tool. On imagenet evaluation tool I get the below output:

> INFO: Num evaluation runs: 50000
> INFO: Preprocessing latency: avg=3837.7(us), std_dev=0(us)
> INFO: Inference latency: avg=16046.3(us), std_dev=486(us)
> INFO: Top-1 Accuracy: 0.43192
> INFO: Top-2 Accuracy: 0.5436
> INFO: Top-3 Accuracy: 0.60002
> INFO: Top-4 Accuracy: 0.6364
> INFO: Top-5 Accuracy: 0.66124
> INFO: Top-6 Accuracy: 0.68194
> INFO: Top-7 Accuracy: 0.69918
> INFO: Top-8 Accuracy: 0.71268
> INFO: Top-9 Accuracy: 0.72398
> INFO: Top-10 Accuracy: 0.73382

I used imagenet ILSVRC2012_img_val dataset with mobilenet labels downloaded from https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt. I ran the evaluation on all 50000 images. Top1 accuracy came to 43% whereas official mobilenetv2 top1 accuracy is ~72%. I did not quantize the model and used the default float32. 
what can be the cause of this delta in accuracy? If i remove `converter.optimizations = [tf.lite.Optimize.DEFAULT]` from script, 
 i still get 43% Top1 accuracy on imagenet. Shouldn't tflite models have similar accuracy to tf models?

When i quantize the model to use uint8/int8, i get even worse accuracy to the tune of 0.4%.

below is the output log for a uint8 model converted from mobilenetv2:

> INFO: Num evaluation runs: 50000
> INFO: Preprocessing latency: avg=3801.64(us), std_dev=0(us)
> INFO: Inference latency: avg=5958.53(us), std_dev=166(us)
> INFO: Top-1 Accuracy: 0.0044
> INFO: Top-2 Accuracy: 0.00758
> INFO: Top-3 Accuracy: 0.01062
> INFO: Top-4 Accuracy: 0.01338
> INFO: Top-5 Accuracy: 0.01634
> INFO: Top-6 Accuracy: 0.0191
> INFO: Top-7 Accuracy: 0.02216
> INFO: Top-8 Accuracy: 0.02482
> INFO: Top-9 Accuracy: 0.02758
> INFO: Top-10 Accuracy: 0.03016


### Source code / logs
Below code is used for conversion:
1. fp32: 

> import tensorflow as tf
> import tensorflow_hub as hub
> 
> m = tf.keras.Sequential([hub.KerasLayer(""https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/classification/5"")])
> m.build([None, 224, 224, 3])# Batch input shape.
> 
> m.save('model')
> 
> converter = tf.lite.TFLiteConverter.from_saved_model(""/content/model"")
> converter.target_spec.supported_ops = [
>     tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.
>     tf.lite.OpsSet.SELECT_TF_OPS,  # enable TensorFlow ops.
> ]
> converter.optimizations = [tf.lite.Optimize.DEFAULT]
> 
> tflite_file = ""mbnv2_model_test1.tflite""
> with open(tflite_file, 'wb') as f:
>   f.write(converter.convert())

generated model is attached in zip file. 
[mbnv2_model_test1.zip](https://github.com/tensorflow/tensorflow/files/11044725/mbnv2_model_test1.zip)


2. uint8:

> import tensorflow as tf
> import tensorflow_hub as hub
> 
> m = tf.keras.Sequential([hub.KerasLayer(""https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/classification/5"")])
> m.build([None, 224, 224, 3])# Batch input shape.
> 
> m.save('model')
> 
> import numpy as np
> def representative_dataset():
>     for _ in range(100):
>         data = tf.random.normal([1,224,224,3])
>         yield [(tf.cast(data, tf.float32) / 127.5) - 127.5]
> 
> converter = tf.lite.TFLiteConverter.from_saved_model(""/content/model"")
> converter.target_spec.supported_ops = [ tf.lite.OpsSet.TFLITE_BUILTINS_INT8] # enable TensorFlow int8ops.
> converter.optimizations = [tf.lite.Optimize.DEFAULT]
> converter.representative_dataset = representative_dataset
> converter.inference_input_type = tf.uint8  # or tf.int8
> converter.inference_output_type = tf.uint8  # or tf.int8
> 
> tflite_file = ""mbnv2_model_uint8_test1.tflite""
> with open(tflite_file, 'wb') as f:
>   f.write(converter.convert())

generated model is attached [mbnv2_model_uint8_test1.zip](https://github.com/tensorflow/tensorflow/files/11044895/mbnv2_model_uint8_test1.zip)"
60071,tf.experimental.numpy.fabs produces the wrong second derivative in combinations of ForwardAccumulator and tf.GradientTape,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tf.experimental.numpy.fabs produce the wrong second derivative in combinations of ForwardAccumulator and tf.GradientTape. `fabs` is twice differentiable at `x` and it doesn't make sense to output None as the second derivate. The tf.GradientTape alone works well, but combining ForwardAccumulator and tf.GradientTape is memory-efficient for large tensors. It would be appreciated if fabs could support this combination.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import tensorflow.experimental.numpy as np

x = tf.Variable([1.0, 2.0])

with tf.autodiff.ForwardAccumulator(x, tf.constant([1., 0.])) as acc:
  with tf.GradientTape() as tape:
    y = np.fabs(x)
  backward = tape.gradient(y, x)
forward = acc.jvp(backward)

print(backward)
print(forward)

with tf.GradientTape() as t2:
    with tf.GradientTape() as t1:
        y = np.fabs(x)
    dy_dx = t1.gradient(y, x)
d2y_dx2 = t2.gradient(dy_dx, x)
print(dy_dx)
print(d2y_dx2)
```


### Relevant log output

```shell
tf.Tensor([1. 1.], shape=(2,), dtype=float32)
None
tf.Tensor([1. 1.], shape=(2,), dtype=float32)
tf.Tensor([0. 0.], shape=(2,), dtype=float32)
```
</details>"
60069,Issue with tensorflow/Cleverhans,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.11.*

### Custom Code

Yes

### OS Platform and Distribution

windows 10 64b

### Mobile device

Google colab

### Python version

3.9.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

NVIDIA T4 Tensor Core GPU

### Current Behaviour?

```shell
I try to run an adversarial machine learning program but I have an issue with Cleverhans library.
When I'm trying to import the following line :
from cleverhans.utils_tf import model_train , model_eval, batch_eval, model_argmax


 I'm using cleverhans 3.1.0 on google colab.
```


### Standalone code to reproduce the issue

```shell
AttributeError: module 'tensorflow' has no attribute 'GraphKeys'
```


### Relevant log output

```shell
import matplotlib . pyplot as plt

import tensorflow.compat.v1 as tf
from datetime import datetime,  timedelta
from sklearn.preprocessing import LabelEncoder , MinMaxScaler

from keras.models import Sequential
from keras.layers import Dense , Dropout
#from tensorflow.keras.optimizers import Adam 
from keras . optimizers import RMSprop, Adam

from tensorflow.python.platform import flags
from cleverhans.utils_tf import model_train , model_eval, batch_eval, model_argmax


AttributeError: module 'tensorflow' has no attribute 'GraphKeys'
```
</details>"
60067,Calling tf.reduce_mean() along the ragged dimension of a tf.RaggedTensor created using from_uniform_row_length returns a tensor with the wrong shape.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.13.0-dev20230322

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04.1 LTS

### Mobile device

_No response_

### Python version

3.8.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.5

### GPU model and memory

_No response_

### Current Behaviour?
Calling tf.reduce_mean() along the ragged dimension of a tf.RaggedTensor created using from_uniform_row_length returns a tensor with the wrong shape.
```shell
Steps to Recreate:
1. Create a tf.RaggedTensor using from_uniform_row_length()
2. Call tf.reduce_mean() along the ragged dimension
3. Check shape of output
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

values = [[[1, 2, 3], [4, 5]], [[6], [7, 8, 9]]]

rt1 = tf.ragged.constant(values, dtype=tf.int32)
print(""\nTF Ragged Tensor Problem:\n------------------------\n"")
print(""* Ragged Tensor 1 *\n"")
print(f""\nrt1 = {rt1}\n"")
print(f""Shape of rt1: {rt1.shape}\n"")
# TF Shape = (2, None, None)
# Real Shape = (2, 2, ragged)
# [
#     [
#         [1, 2, 3],
#         [4, 5]
#      ],
#      [
#         [6],
#         [7, 8, 9]
#     ]
# ]


rt2 = tf.RaggedTensor.from_uniform_row_length(rt1, 2)
print(""* Ragged Tensor 2 *\n"")
print(f""rt2 = {rt2}\n"")
print(f""Shape of rt2: {rt2.shape}\n"")
# TF Shape = (1, 2, None, None)
# Real Shape = (1, 2, 2, ragged)
# [
#     [
#         [
#             [1, 2, 3],
#             [4, 5]
#         ],
#         [
#             [6],
#             [7, 8, 9]
#         ]
#     ]
# ]

print(""Averaging along ragged dimension (axis = 3)...\n"")

# Reduce mean along ragged axis
print(""* Averaged Tensor: *\n"")
avg_ragged_dim = tf.reduce_mean(rt2, axis=3)
print(f""avg_ragged_dim = {avg_ragged_dim}\n"")
print(f""Shape of avg_ragged_dim: {avg_ragged_dim.shape}\n"")
print(""Notice the 2 is gone instead of the None.\n"")
# TF Shape = (1, None, None) --------> notice 2 is gone instead of None
# Real Shape = (1, 2, 2)
# [
#     [
#         [2.0, 4.5],
#         [6.0, 8.0]
#     ]
# ]
```


### Relevant log output

```shell
TF Ragged Tensor Problem:
------------------------

* Ragged Tensor 1 *


rt1 = <tf.RaggedTensor [[[1, 2, 3], [4, 5]],
 [[6], [7, 8, 9]]]>

Shape of rt1: (2, None, None)

* Ragged Tensor 2 *

rt2 = <tf.RaggedTensor [[[[1, 2, 3], [4, 5]],
  [[6], [7, 8, 9]]]]>

Shape of rt2: (1, 2, None, None)

Averaging along ragged dimension (axis = 3)...

* Averaged Tensor: *

avg_ragged_dim = <tf.RaggedTensor [[[2.0, 4.5],
  [6.0, 8.0]]]>

Shape of avg_ragged_dim: (1, None, None)

Notice the 2 is gone instead of the None.
```
</details>"
60065,How to check for NoneTensorSpec?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.13.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I have a dataset that returns `None` values. I would like to programatically find and replace the `NoneTensorSpec` values of the spec into a custom value (`None`), but I cannot find `NoneTensorSpec` in the public API and am not sure what the best way to filter for it is.
```


### Standalone code to reproduce the issue

```shell
ds = tf.data.Dataset.from_tensors((3, None))
print(ds.element_spec)
```


### Relevant log output

```shell
`(TensorSpec(shape=(), dtype=tf.int32, name=None), NoneTensorSpec())`
```
</details>"
60064,How to save average weights of checkpoints using Tensorflow 2.X?,"HELP NEEDED !!!

Hi, everyone. I found a scrit to load serials of checkpoints for a model and save average weights of them in TensorFlow1.

https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/utils/avg_checkpoints.py

Which is very useful to improve the performance of the model. But in TensorFlow2， the saved checkpoints like this:

![image](https://user-images.githubusercontent.com/42861880/226873472-d5b31ef6-6e59-4136-ae55-8d367418cd7c.png)

How to average the weights of parameters for them? I tried to write a script but the output checkpoint seems not as we expected, anyone can give some helps? Thanks a lot in advance. Here is my script:


`
import os
import numpy as np
import tensorflow as tf	
from absl import app
from absl import flags
from absl import logging


FLAGS = flags.FLAGS

flags.DEFINE_string(""checkpoints"","""",
					""Comma-separated list of checkpoints to average."")
flags.DEFINE_integer(""num_last_chekpoints"", 0,
					""Average the last N saved checkpoints.""
					"" If the checkpoints flag is set, this is ignored."")
flags.DEFINE_string(""prefix"", """",
					""Prefix (e.g., directory) to append to each checkpoint."")
flags.DEFINE_string(""output_path"", ""/tmp/averaged.ckpt"",
					""Path to output the averaged checkpoint to."")

def checkpoint_exists(path):
	return (tf.io.gfile.exists(path) or tf.io.gfile.exists(path + "".index""))

def main(argv):
	if FLAGS.checkpoints:
		# Get the checkpoints list from flags and run some basic checks.
		checkpoints = [c.strip() for c in FLAGS.checkpoints.split("","")]
		checkpoints = [c for c in checkpoints if c]
		if not checkpoints:
			raise ValueError(""No checkpoints provided for averaging."")
		if FLAGS.prefix:
			checkpoints = [FLAGS.prefix + c for c in checkpoints]
	else:
		assert FLAGS.num_last_chekpoints >= 1, ""Must average at least one model""
		assert FLAGS.prefix, (""Prefix must be provided when averaging last""
								"" N checkpoints"")
		# checkpoint_state = tf.train.get_checkpoint_state(
		# 	os.path.dirname(FLAGS.prefix))
		# # Checkpoints are ordered from oldest to newest.
		# checkpoints = checkpoint_state.all_model_checkpoint_paths[
		# 	-FLAGS.num_last_checkpoints:]
		file_list = os.listdir(FLAGS.prefix)
		checkpoints = [os.path.join(FLAGS.prefix, file) for file 
			in file_list if file.endswith("".index"")]
		checkpoints = [checkpoint[:-6] for checkpoint in checkpoints]
		checkpoints.sort(key=lambda checkpoint: int(checkpoint.split('-')[-1]))
		checkpoints = checkpoints[-FLAGS.num_last_checkpoints:]

	checkpoints = [c for c in checkpoints if checkpoint_exists(c)]
	if not checkpoints:
		if FLAGS.checkpoints:
			raise ValueError(
				""None of the provided checkpoints exist. %s"" % FLAGS.checkpoints)
		else:
			raise ValueError(""Could not find checkpoints at %s"" %
				os.path.dirname(FLAGS.prefix))

	# Read variables from all checkpoints and average them.
	logging.info(""Reading variables and averaging checkpoints:"")
	for c in checkpoints:
		logging.info(""%s "", c)

	var_list = tf.train.list_variables(checkpoints[0])
	var_values, var_dtypes = {}, {}
	for (name, shape) in var_list:
		if not name.startswith(""save_counter""):
			var_values[name] = np.zeros(shape)

	for checkpoint in checkpoints:
		reader = tf.train.load_checkpoint(checkpoint)
		for name in var_values:
			tensor = reader.get_tensor(name)
			dtype = reader.get_variable_to_dtype_map()[name]

			if dtype == tf.string:
				var_values[name] = tensor
			else:
				var_values[name] += tensor
			var_dtypes[name] = dtype
		logging.info(""Read from checkpoint %s"", checkpoint)

	for name in var_values:  # Average.
		if var_dtypes[name] != tf.string:
			var_values[name] /= len(checkpoints)

	name = var_list[-1][0]
	assert name.startswith(""save_counter"")
	shape = reader.get_variable_to_shape_map()[name]
	dtype = reader.get_variable_to_dtype_map()[name]
	var_values[name] = np.zeros(shape)
	var_dtypes[name] = dtype

	for name in var_values.keys():
		var_values[name] = tf.Variable(
			var_values[name], dtype=var_dtypes[name])
	save = tf.train.Checkpoint()
	save.mapped = var_values
	# save.listed = []
	# save.mapped = {}
	# for name in var_values.keys():
	# 	save.listed.append(var_values[name])
	# 	save.mapped[name] = var_values[name]
	save_path = save.save(FLAGS.output_path)

	logging.info(""Averaged checkpoints saved in %s"", FLAGS.output_path)


if __name__ == '__main__':
	app.run(main)
`"
60063,CTCModel,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11.1

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
ImportError                               Traceback (most recent call last)
Cell In[1], line 7
      5 from keras.models import Model
      6 import tensorflow_addons as tfa
----> 7 from keras_ctcmodel.CTCModel import CTCModel as CTCModel
      8 from CTCModel import CTCModel
      9 import pickle

File ~\anaconda3\envs\tensorflow\lib\site-packages\keras_ctcmodel\CTCModel.py:7
      5 import os
      6 from keras import Input
----> 7 from keras.engine import Model
      8 from keras.layers import Lambda
      9 from keras.models import model_from_json, Sequential

ImportError: cannot import name 'Model' from 'keras.engine'

I can't find a solution to this problem, please, I need your help !!
```


### Standalone code to reproduce the issue

```shell
from keras.layers import TimeDistributed, Activation, Dense, Input, Bidirectional, LSTM, Masking, GaussianNoise
from keras.layers import Conv2D, MaxPooling2D, Reshape, Flatten, Dense
from tensorflow.keras.layers import BatchNormalization
from keras.optimizers import Adam
from keras.models import Model
import tensorflow_addons as tfa
from keras_ctcmodel.CTCModel import CTCModel as CTCModel
from CTCModel import CTCModel
```


### Relevant log output

_No response_</details>"
60062,[MSVC]Tensorflow failed to error C2678: binary '==': no operator found which takes a left-hand operand of type 'const _Ty',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

master branch, commit: 48246a6

### Custom Code

No

### OS Platform and Distribution

Windows Server 2022

### Mobile device

_No response_

### Python version

3.9

### Bazel version

5.3.0

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Current:
ERROR: F:/gitp/tensorflow/tensorflow/tensorflow/compiler/xla/pjrt/BUILD:598:11: Compiling tensorflow/compiler/xla/pjrt/transpose.cc failed: (Exit 2): cl.exe failed: error executing command 
  cd /d F:/bazeltemp/2powapgf/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC\Tools\MSVC\14.35.32215\include;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC\Tools\MSVC\14.35.32215\ATLMFC\include;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC\Auxiliary\VS\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\ucrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\um;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\shared;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\winrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\cppwinrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um
    SET PATH=C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\VC\VCPackages;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\MSBuild\Current\bin\Roslyn;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Team Tools\Performance Tools\x64;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\HTML Help Workshop;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\CommonExtensions\Microsoft\FSharp\Tools;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\Extensions\Microsoft\CodeCoverage.Console;C:\Program Files (x86)\Windows Kits\10\bin\10.0.22621.0\\x64;C:\Program Files (x86)\Windows Kits\10\bin\\x64;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\\MSBuild\Current\Bin\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\Tools\;;C:\Windows\system32;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja;C:\Program Files\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\VC\Linux\bin\ConnectionManagerExe
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Python39/python.exe
    SET PYTHON_LIB_PATH=C:/Python39/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\CPPTES~1\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TMP=C:\Users\CPPTES~1\AppData\Local\Temp
  C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64\cl.exe @bazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/pjrt/_objs/transpose/transpose.obj.params
# Configuration: 6c527fa88d503266dc3c2010527883ab34835d92feca5051a6b7c0c26b585cd5
# Execution platform: @local_execution_config_platform//:platform
C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC\Tools\MSVC\14.35.32215\include\xstddef(106): error C2678: binary '==': no operator found which takes a left-hand operand of type 'const _Ty' (or there is no acceptable conversion)
        with
        [
            _Ty=xla::TransposePlanCacheKey
        ]

Expect:
Build pass.
```


### Standalone code to reproduce the issue

```shell
git clone https://github.com/tensorflow/tensorflow.git F:\gitP\tensorflow\tensorflow
cd F:\gitP\tensorflow\tensorflow
pip3 install -r tensorflow/tools/ci_build/release/requirements_common.txt 2>&1
set PATH=F:\gitP\tensorflow\tensorflow\..\tools;%path%
set PATH=F:\gitP\tensorflow\tensorflow\..\tools\msys64\usr\bin;%path%
yes """" 2>nul | python ./configure.py 2>&1
set BAZEL_VC=C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC
set BAZEL_VC_FULL_VERSION=14.35.32215
set PATH=F:\gitP\tensorflow\tensorflow\..\tools;%path%
set PATH=F:\gitP\tensorflow\tensorflow\..\tools\msys64\usr\bin;%path%
bazel --output_user_root F:\bazelTemp build --jobs 8 --config=opt --local_ram_resources=2048 --subcommands //tensorflow/tools/pip_package:build_pip_package 2>&1
```


### Relevant log output
[build.zip](https://github.com/tensorflow/tensorflow/files/11038139/build.zip)
[transpose.zip](https://github.com/tensorflow/tensorflow/files/11038153/transpose.zip)
**Or you can follow below steps to reproduce the issue with .i file
Repro Steps:**
1.Download transpose.zip and unzip it
2.Open VS2022 x64 Native Tools command.
3.cl.exe transpose.i /TP /c /EHsc /std:c++17

</details>"
60061,Linking external/org_tensorflow/tensorflow/libtensorflow_cc.so.2.13.0 failed: (Exit 1): gcc failed: error executing command ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

Head

### Custom Code

No

### OS Platform and Distribution

Linux

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

[Dynamic Pywrap PR 58734](https://github.com/tensorflow/tensorflow/pull/58734) seems to introduce a a TensorFlow Serving linking error.


### Standalone code to reproduce the issue

Run
```shell
$ git clone git@github.com:tensorflow/serving.git
$ cd serving
$ sudo ./tools/run_in_docker.sh bazel test --action_env=TF_REVISION=6147c03eb9af1e5d2ae155045b33e909ef96944e tensorflow_serving/... &> /tmp/tfs_bazel_output.log
```
and search `Linking external` in /tmp/tfs_bazel_output.log



### Relevant log output

```shell
/usr/local/google/home/rostam/Workspace/tmp/serving/.cache/_bazel_root/7c4195127656842bcf1efda48d8fd065/external/org_tensorflow/tensorflow/BUILD:1214:21: Linking external/org_tensorflow/tensorflow/libtensorflow_cc.so.2.12.0 failed: (Exit 1): gcc failed: error executing command 
  (cd /usr/local/google/home/rostam/Workspace/tmp/serving/.cache/_bazel_root/7c4195127656842bcf1efda48d8fd065/execroot/tf_serving && \
  exec env - \
    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    TF_REVISION=6147c03eb9af1e5d2ae155045b33e909ef96944e \
  /usr/bin/gcc @bazel-out/k8-opt/bin/external/org_tensorflow/tensorflow/libtensorflow_cc.so.2.12.0-2.params)

```"
60060,tf2.11 tf.profiler.experimental doesn't generate trace.json.gz. Possible to have a python script to get the json.gz from xplane.pb?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf2.11

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
In tf2.10, when use the tf.profiler.experimental.start(logdir) and tf.profiler.experimental.stop(). It will generate many files as bellow in logdir folder.
.
├── events.out.tfevents.XXXX.profile-empty
└── plugins
    └── profile
        └── 2022_11_29_08_25_43
            ├── XXXX.input_pipeline.pb
            ├── XXXX.kernel_stats.pb
            ├── XXXX.memory_profile.json.gz
            ├── XXXX.overview_page.pb
            ├── XXXX.tensorflow_stats.pb
            ├── XXXX.trace.json
            └── XXXX.xplane.pb

But in tf2.11, it only generate these files.
.
├── events.out.tfevents.XXXX.profile-empty
└── plugins
    └── profile
        ├── 2022_11_29_08_32_10
        │   └── XXXX.xplane.pb

Possible to derive json.gz file from xplane.pb with some python scripts?
```


### Standalone code to reproduce the issue

```shell
NA
```


### Relevant log output

_No response_</details>"
60059,QuantizeAndDequantizeV4 throws TypeError in gradient computation in forward mode,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
QuantizeAndDequantizeV4 throws TypeError in gradient computation in forward mode. I encountered a similar issue (https://github.com/tensorflow/tensorflow/issues/59960) where quantize_and_dequantize_v2 is wrongly associated with _QuantizeAndDequantizeV4GradGrad in gradient computation. In this case, the V4 version API doesn't work either. It would be great if they could be fixed in the backend.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

tensor = tf.random.uniform(shape=[1, 1], dtype=tf.float32)

def quantizeAndDequantize(x):

    y = tf.raw_ops.QuantizeAndDequantizeV4(input=x, input_min=0., input_max=5.)
    return y

output = quantizeAndDequantize(tensor) # pass
with tf.autodiff.ForwardAccumulator(tensor, tf.constant([[0.], [1.]])) as acc:
    output = quantizeAndDequantize(tensor)
```


### Relevant log output

```shell
TypeError: _QuantizeAndDequantizeV4GradGrad() takes 2 positional arguments but 4 were given
```
</details>"
60057,Keras clone_model does not work if a concatenate layer with one element to concatenate is used.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.13 (nightly)

### Custom Code

No

### OS Platform and Distribution

Windows

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

Na

### GPU model and memory

_No response_

### Current Behaviour?

```shell
If I build a keras model that contains a concatenate layer which only concatenate one element, then the `clone_model` function will fail on that model.

My assumption is that the reason for this behaviour is that the parameter input of this model is a Tensor instead of a List. If I take a look on concatenate layers with multiple inputs, then the input parameter is a list.

I suppose that the error is that the `self._preserve_input_structure_in_config` is not set to true.
```


### Standalone code to reproduce the issue

```shell
from tensorflow.python.keras.layers import Concatenate, Input
from tensorflow.python.keras.models import clone_model, Model

input1 = Input(shape=(1,2), name=""input"")
concat = Concatenate()([input1])

model = Model(input1, concat)
model.compile()

print(""Model exist, now lets try to clone the model.."")

cloned_model = clone_model(model) # This line fails
```


### Relevant log output

```shell
...\scratch_2.py"", line 12, in <module>
    cloned_model = clone_model(model)

.....

tensorflow\python\keras\layers\merge.py"", line 491, in build
    raise ValueError('A `Concatenate` layer should be called '
ValueError: A `Concatenate` layer should be called on a list of at least 1 input.
```
</details>"
60055,Type INT32 (2) not supported.Node ADD (number 0) failed to invoke with status 1.Node WHILE (number 10) failed to invoke with status 1,"**System information**
- OS Platform : window10
- TensorFlow installed from (source or binary): pip
- TensorFlow version (or github SHA if from source):2.3.3
- board: Arduino Nano 33 ble sense



I tried to run TensorFlow Lite for Microcontrollers with Arduino Nano 33 ble sense, the model is my custom LSTM. And the result : 
Type INT32 (2) not supported.Node ADD (number 0) failed to invoke with status 1.Node WHILE (number 10) failed to invoke with status 1.Invoke failed. But it has output values.

![model1 tflite](https://user-images.githubusercontent.com/127216064/226529585-4acac340-fb0b-4f24-a3ea-b1a588c41648.png)



"
60054,Build Error in Mac OS ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.13

### Custom Code

No

### OS Platform and Distribution

macOS Monterey

### Mobile device

_No response_

### Python version

3.9

### Bazel version

5.3.0

### GCC/Compiler version

Apple clang version 13.1.6 (clang-1316.0.21.2.5)

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Running
`pip install /tmp/tensorflow_pkg/tensorflow-2.13.0-cp38-cp38-macosx_12_0_x86_64.whl`

Leads to 

`tensorflow-2.13.0-cp38-cp38-macosx_12_0_x86_64.whl is not a supported wheel on this platform`

Built the code from source using https://www.tensorflow.org/install/source
```


### Standalone code to reproduce the issue

```shell
No Code
```


### Relevant log output

_No response_</details>"
60053,Efficientnet B7 classification conversion from tf to tflite fails tflite imagenet evaluation test,"
### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 20.04
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: No
-   **TensorFlow installed from (source or binary)**: Binary
-   **TensorFlow version (use command below)**: 2.11
-   **Python version**: 3.9


### Describe the problem
I am trying to convert efficientnet_b7_classification model available in tfhub: https://storage.googleapis.com/tfhub-modules/tensorflow/efficientnet/b7/classification/1.tar.gz to tflite

I use the below code snippet to convert the saved model to tflite:

> import tensorflow as tf
> converter = tf.lite.TFLiteConverter.from_saved_model('saved_model')
> tflite_model = converter.convert()
> with open('model.tflite', 'wb') as f:
>   f.write(tflite_model)

on visualizing the model on netron, i saw that the input to the model is of shape (1,1,1,3). whereas, the input to efficientnet_b7 is 600x600. So the converted model should have the shape (1,600,600,3), but i don't see this.
The output of the model is of the shape (1,1000).

> I try to change the input shape using the below snippet:
> model = tf.saved_model.load('saved_model')
> concrete_func = model.signatures[""serving_default""]
> concrete_func.inputs[0].set_shape([1,600,600,3])
> converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func],model)
> tflite_model = converter.conver

But on visualizing the model, i still see the input shape as (1,1,1,3), and output shape is (1,1000). The model has been trained on imagenet so output shape should be fine with the label file consisting of 1000 labels starting from 1-1000 instead of 0-1000 which includes the dummy as well.

so from my labelfile, i removed the dummy, and it then used the same labelfile to evaluate the tflite model using the imagenet_image_classification run_eval binary. On running it against the converted tflite model, i get the below error log, which seemingly says that the model output shape is wrong. What is the correct way to go about converting and testing efficientnet_b7 model?
The output shape of mobilenets is (1,1001), whereas that of efficientnets is (1,1000) even though both are trained on imagenet dataset. why is that so?

thanks!



### Source code / logs
> $ bazel run -c opt -- //tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification:run_eval --model_file=/home/mtk/Downloads/efficientnet_b7_float_model.tflite --ground_truth_images_path=/home/mtk/Downloads/ILSVRC2012_img_val --ground_truth_labels=/home/mtk/Documents/val.txt --model_output_labels=/home/mtk/Documents/tflite_models/imagenet_classes.txt --output_file_path=tmp/accuracy_output.txt --num_images=0
> INFO: Options provided by the client:
>   Inherited 'common' options: --isatty=1 --terminal_columns=79
> INFO: Reading rc options for 'run' from /home/mtk/Documents/tensorflow/.bazelrc:
>   Inherited 'common' options: --experimental_repo_remote_exec
> INFO: Reading rc options for 'run' from /home/mtk/Documents/tensorflow/.bazelrc:
>   Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
> INFO: Found applicable config definition build:short_logs in file /home/mtk/Documents/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
> INFO: Found applicable config definition build:v2 in file /home/mtk/Documents/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
> INFO: Found applicable config definition build:linux in file /home/mtk/Documents/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
> INFO: Found applicable config definition build:dynamic_kernels in file /home/mtk/Documents/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
> INFO: Analyzed target //tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification:run_eval (0 packages loaded, 0 targets configured).
> INFO: Found 1 target...
> Target //tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification:run_eval up-to-date:
>   bazel-bin/tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification/run_eval
> INFO: Elapsed time: 0.091s, Critical Path: 0.00s
> INFO: 1 process: 1 internal.
> INFO: Build completed successfully, 1 total action
> INFO: Running command line: bazel-bin/tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification/run_eval '--model_file=/home/mtk/Downloads/efficientnet_b7_float_model.tflite' '--ground_truth_images_path=/home/mtk/Downloads/ILSVRC2012_img_val' '--ground_truth_labels=/home/mtk/Documents/val.txt' '--model_output_labels=/home/mtk/Documents/tflite_models/imagenet_classes.txt' '--output_INFO: Build completed successfully, 1 total action
> INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
> INFO: Evaluated: 0%
> 2023-03-20 18:49:35.911460: E tensorflow/lite/tools/evaluation/stages/topk_accuracy_eval_stage.cc:80] model_output_ not set correctly
> ERROR: Could not run the task evaluation!
"
60052,variant_op_registry.h:114 Check failed: existing == nullptr UnaryVariantDeviceCopy for direction: 1 and type_index: tensorflow::Tensor already registered,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

r2.12

### Custom Code

No

### OS Platform and Distribution

Ubuntu 22.04.2 LTS

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

5.3.0

### GCC/Compiler version

11.3.0

### CUDA/cuDNN version

12.0

### GPU model and memory

_No response_

### Current Behaviour?

I am following the [official docs](https://www.tensorflow.org/install/source) on how to build it from source and got this error:

`bazel build --verbose_failures --config=cuda --config=monolithic //tensorflow/tools/pip_package:build_pip_package`

```
2023-03-21 00:37:20.736329: F ./tensorflow/core/framework/variant_op_registry.h:114] Check failed: existing == nullptr (0x606472d59ab8 vs. nullptr)UnaryVariantDeviceCopy for direction: 1 and type_index: tensorflow::Tensor already registered
```

After some debugging in gdb, it look like this issue happens because two ""competing"" libraries get loaded (each of which tries to register same device copy function. Below are (abbreviated) backtraces:

3 (per-direction 3 in total) registrations from libtensorflow_framework.so.2 that looks like this:

```
Breakpoint 1, 0x000073e06f9670a8 in tensorflow::UnaryVariantOpRegistry::RegisterDeviceCopyFn(tensorflow::VariantDeviceCopyDirection, tensorflow::TypeIndex const&, std::function<tsl::Status (tensorflow::Variant const&, tensorflow::Variant*, std::function<tsl::Status (tensorflow::Tensor const&, tensorflow::Tensor*)>)> const&) () from /home/dimanne/.cache/bazel/_bazel_dimanne/33c66632a93eff405a3246128a23107c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/platform/../../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libtensorflow_framework.so.2
#0  0x000073e06f9670a8 in tensorflow::UnaryVariantOpRegistry::RegisterDeviceCopyFn(tensorflow::VariantDeviceCopyDirection, tensorflow::TypeIndex const&, std::function<tsl::Status (tensorflow::Variant const&, tensorflow::Variant*, std::function<tsl::Status (tensorflow::Tensor const&, tensorflow::Tensor*)>)> const&) () from /home/dimanne/.cache/bazel/_bazel_dimanne/33c66632a93eff405a3246128a23107c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/platform/../../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libtensorflow_framework.so.2
#1  0x000073e070509cfc in tensorflow::variant_op_registry_fn_registration::UnaryVariantDeviceCopyRegistration<tensorflow::Tensor>::UnaryVariantDeviceCopyRegistration(tensorflow::VariantDeviceCopyDirection, tensorflow::TypeIndex const&, std::function<tsl::Status (tensorflow::Tensor const&, tensorflow::Tensor*, std::function<tsl::Status (tensorflow::Tensor const&, tensorflow::Tensor*)>)> const&) () from /home/dimanne/.cache/bazel/_bazel_dimanne/33c66632a93eff405a3246128a23107c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/platform/../../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libtensorflow_framework.so.2
#2  0x000073e06f425e78 in _GLOBAL__sub_I_copy_tensor.cc () from /home/dimanne/.cache/bazel/_bazel_dimanne/33c66632a93eff405a3246128a23107c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/platform/../../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libtensorflow_framework.so.2
#3  0x000073e07187547e in call_init (l=<optimized out>, argc=argc@entry=17, argv=argv@entry=0x7ffcb9ade588, env=env@entry=0x7ffcb9ade618) at ./elf/dl-init.c:70
```

And then one attempt to register, presumably same copy function from libtensorflow_cc.so.2:

```
Breakpoint 1, 0x000073e06f9670a8 in tensorflow::UnaryVariantOpRegistry::RegisterDeviceCopyFn(tensorflow::VariantDeviceCopyDirection, tensorflow::TypeIndex const&, std::function<tsl::Status (tensorflow::Variant const&, tensorflow::Variant*, std::function<tsl::Status (tensorflow::Tensor const&, tensorflow::Tensor*)>)> const&) () from /home/dimanne/.cache/bazel/_bazel_dimanne/33c66632a93eff405a3246128a23107c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/platform/../../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libtensorflow_framework.so.2
#0  0x000073e06f9670a8 in tensorflow::UnaryVariantOpRegistry::RegisterDeviceCopyFn(tensorflow::VariantDeviceCopyDirection, tensorflow::TypeIndex const&, std::function<tsl::Status (tensorflow::Variant const&, tensorflow::Variant*, std::function<tsl::Status (tensorflow::Tensor const&, tensorflow::Tensor*)>)> const&) () from /home/dimanne/.cache/bazel/_bazel_dimanne/33c66632a93eff405a3246128a23107c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/platform/../../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libtensorflow_framework.so.2
#1  0x000073e070509cfc in tensorflow::variant_op_registry_fn_registration::UnaryVariantDeviceCopyRegistration<tensorflow::Tensor>::UnaryVariantDeviceCopyRegistration(tensorflow::VariantDeviceCopyDirection, tensorflow::TypeIndex const&, std::function<tsl::Status (tensorflow::Tensor const&, tensorflow::Tensor*, std::function<tsl::Status (tensorflow::Tensor const&, tensorflow::Tensor*)>)> const&) () from /home/dimanne/.cache/bazel/_bazel_dimanne/33c66632a93eff405a3246128a23107c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/platform/../../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libtensorflow_framework.so.2
#2  0x000073e04f15e458 in _GLOBAL__sub_I_copy_tensor.cc () from /home/dimanne/.cache/bazel/_bazel_dimanne/33c66632a93eff405a3246128a23107c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/platform/../../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so_Ucclib___Utensorflow/libtensorflow_cc.so.2
#3  0x000073e07187547e in call_init (l=<optimized out>, argc=argc@entry=17, argv=argv@entry=0x7ffcb9ade588, env=env@entry=0x7ffcb9ade618) at ./elf/dl-init.c:70
```

I found a ~similar issue on [SO](https://stackoverflow.com/questions/53113327/tensorflow-c-error-unary-variantshapefn-for-type-name-int-already-registered)

Does anyone have any ideas how to fix it?


### Relevant log output

_No response_</details>"
60048,TensorArray throws AttributeError in gradient computation ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The [documentation](https://www.tensorflow.org/api_docs/python/tf/TensorArray) claims that TensorArray ""supports gradient back-propagation via special ""flow"" control flow dependencies"". However, it seems that `TensorArray` is an [unsupported type](https://github.com/tensorflow/tensorflow/blob/d5b57ca93e506df258271ea00fc29cf98383a374/tensorflow/python/ops/gradient_checker_v2.py#L261) for gradient computation at the moment. It would be great if there is clarification, in documentation or better error message.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

elems = tf.ones(shape=[2])

def init_tensor_array(elems):

    ta = tf.TensorArray(dtype=tf.float32, size=4, dynamic_size=True)
    ta = ta.write(0, elems)
    return ta

ta = init_tensor_array(elems)
with tf.GradientTape() as tape:
    tape.watch(elems)
    t = init_tensor_array(elems)

gradient = tape.jacobian(t, elems)
```


### Relevant log output

```shell
AttributeError: 'TensorArray' object has no attribute 'shape'
```
</details>"
60043,Accuracy issue during elementwise layer broadcasting in TFlite gpu delegate,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 22.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (or github SHA if from source): r2.8
- TfliteGpuDelegate backend: cl

**Standalone code to reproduce the issue** 

While running the multiplication layer on TfliteGpuDelegate with input dimensions as {input1 - [1, 1600, 1] and input2 - [384]}, I am getting accuracy issues. The accuracy is fine while running on CPU. I am assuming there are limitations or issues in GPU execution during broadcasting. I have run experimentation on the following shapes and these are the observations:

[accuracy when compared the tflite cpu execution.]
model_1 : (1, 10, 3) x (1, 10, 3) => accuracy is fine.
model_2 : (1, 10, 3) x (1, 10, 1) => accuracy is fine.
model_3 : (1, 10, 1) x (1, 10, 3) => accuracy issue occurs. (mean absolute error (mae) = 2.44e-01).
model_4 : (1, 10, 1) x (3) => accuracy issue occurs. (mae = 2.71e-01).

PFA links for the models and detailed accuracy report is below.

models: https://drive.google.com/drive/folders/1DCfV1xPxdliJ7jtYF6LzhDjxTRM0uDSm?usp=share_link
accuracy report : https://drive.google.com/file/d/1wjLa1-KrgeyAzimEvhpq_ifRSfEOYeJr/view?usp=share_link

It would be helpful if the limitations are explained or the issues is debugged regarding this. Also any workaround for fixing (1, 1600, 1) x 384 will be appreciated. 

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
60042,How to build with xnnpack,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.10.0

### Custom Code

Yes

### OS Platform and Distribution

ubuntu 20.04

### Mobile device

Qualcomm

### Python version

3.8

### Bazel version

5.1.1

### GCC/Compiler version

9.3.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I want to build without xnnpack, or with xnnpack-qs8 diabled. How to achieve that?
```


### Standalone code to reproduce the issue

```shell
I tried with ""--define=tflite_with_xnnpack_qu8=false --define=tflite_with_xnnpack_qs8=false"", and ""--define tflite_with_xnnpack=false"". But it seems that they didn't work.
```


### Relevant log output

_No response_</details>"
60041,`zeta` doesn't support gradient computation,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
`zeta` doesn't support gradient computation. The Hurwitz zeta function is differentiable with respect to x.
The partial derivative with respect to x can be expressed as:

dζ(x, q) / dx = - ∑(n=0 to ∞) (n + q)^(-x) * ln(n + q)
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

x = tf.Variable(tf.constant([2], dtype=tf.float32))
y = tf.Variable(tf.constant([2], dtype=tf.float32))

def zeta(x, y):
    t = tf.math.zeta(x, y)
    return t

t = zeta(x, y)
print(t)
with tf.GradientTape() as tape:
    tape.watch(x)
    t = zeta(x, y)

gradient = tape.jacobian(t, x)
print(gradient)
```


### Relevant log output

```shell
tf.Tensor([0.6449342], shape=(1,), dtype=float32)
None
```
</details>"
60040,Missing module,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Preformence bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

code

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hello,
I found a error with missing modul in file async_comp_test.
```


### Standalone code to reproduce the issue

```shell
1. make codespace
2.Find file async_comp_test.py
3. Start debugging.
```


### Relevant log output

```shell
No module named 'tensorflow'
  File ""/workspaces/tensorflow/tensorflow/compiler/tests/async_comp_test.py"", line 20, in <module>
    from tensorflow.core.protobuf import config_pb2
ModuleNotFoundError: No module named 'tensorflow'
```
</details>"
60037,Tensoflow Installation Problem,"I'm using python ```version 3.7.8```
while installing specific version of tensorflow i.e, ```1.14.0``` using pip command ```pip install tensorflow==1.14.0```
it is showing 
```
using cached tensorflow-1.14.0-cp37-cp37m-win_amd64.whl
....
```
and unable to ```import tensorflow``` properly.

It shows error: 
```
File ""C:\Python37\lib\site-packages\tensorflow\__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Python37\lib\site-packages\tensorflow\python\__init__.py"", line 52, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""C:\Python37\lib\site-packages\tensorflow\core\framework\graph_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import node_def_pb2 as tensorflow_dot_core_dot_framework_dot_node__def__pb2
  File ""C:\Python37\lib\site-packages\tensorflow\core\framework\node_def_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2
  File ""C:\Python37\lib\site-packages\tensorflow\core\framework\attr_value_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2
  File ""C:\Python37\lib\site-packages\tensorflow\core\framework\tensor_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2
  File ""C:\Python37\lib\site-packages\tensorflow\core\framework\resource_handle_pb2.py"", line 42, in <module>
    serialized_options=None, file=DESCRIPTOR),
  File ""C:\Python37\lib\site-packages\google\protobuf\descriptor.py"", line 561, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
```"
60036,A CUDA_ERROR_MISALIGNED_ADDRESS/CUDA_ERROR_ILLEGAL_ADDRESS can be triggered in SparseSplit,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf 2.13.0-dev20230317

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.5

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A cuda memory corruption can be triggered by the following code in `tf.raw_ops.SparseSplit`. When the code is executing, a CUDA Memory Error would be reported, the gpu memory would be occupied, and it will never stop.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
for i in range(100):
    with tf.device(""GPU:0""):
        num_split = 30
        split_dim = -1
        indices = tf.saturate_cast(tf.random.uniform([4, 8], minval=-1024, maxval=1024, dtype=tf.int64), dtype=tf.int64)
        values = tf.complex(tf.random.uniform([16], dtype=tf.float32, minval=-1024, maxval=1024),tf.random.uniform([16], dtype=tf.float32, minval=-1024, maxval=1024))
        shape = [108, 28, 108]
        res = tf.raw_ops.SparseSplit(
            num_split=num_split,
            split_dim=split_dim,
            indices=indices,
            values=values,
            shape=shape,
        )
```


### Relevant log output

```shell
2023-03-18 18:36:32.613986: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-03-18 18:36:32.661775: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-18 18:36:33.415445: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-03-18 18:36:35.026862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14577 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:2f:00.0, compute capability: 7.0
2023-03-18 18:36:35.342047: E tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:789] failed to record completion event; therefore, failed to create inter-stream dependency
2023-03-18 18:36:35.342151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1160] failed to enqueue async memcpy from device to host: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered; host dst: 0x7f6d47200000; GPU src: 0x7f69b8000500; size: 8=0x82023-03-18 18:36:35.342182: E tensorflow/compiler/xla/stream_executor/stream.cc:336] Error recording event in stream: Error recording CUDA event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered; not marking stream as bad, as the Event object may be at fault. Monitor for further errors.
2023-03-18 18:36:35.342209: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1033] could not wait stream on event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2023-03-18 18:36:35.342229: E tensorflow/compiler/xla/stream_executor/stream.cc:1120] Error waiting for event in stream: error recording waiting for CUDA event on stream 0x5562acfa15e0; not marking stream as bad, as the Event object may be at fault. Monitor for further errors.
2023-03-18 18:36:35.342248: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:615] unable to add host callback: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2023-03-18 18:36:35.342299: I tensorflow/compiler/xla/stream_executor/stream.cc:1108] [stream=0x556292e3cf30,impl=0x5562acb89830] did not wait for [stream=0x556292e3f420,impl=0x5562acb898d0]
2023-03-18 18:36:35.342323: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1160] failed to enqueue async memcpy from device to host: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered; host dst: 0x7f6d47200100; GPU src: 0x7f69b8000600; size: 8=0x82023-03-18 18:36:35.342345: E tensorflow/compiler/xla/stream_executor/stream.cc:336] Error recording event in stream: Error recording CUDA event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered; not marking stream as bad, as the Event object may be at fault. Monitor for further errors.
2023-03-18 18:36:35.342367: I tensorflow/compiler/xla/stream_executor/stream.cc:1126] [stream=0x5562acfa15e0,impl=0x5562924676a0] did not wait for an event.
2023-03-18 18:36:35.342386: I tensorflow/compiler/xla/stream_executor/stream.cc:2393] [stream=0x5562acfa15e0,impl=0x5562924676a0] was in error state before adding host callback
2023-03-18 18:36:35.342403: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:615] unable to add host callback: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
```
</details>"
60035,A check fail can be triggered in MatrixSquareRoot,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf 2.13.0-dev20230317

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.5

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A crash due to check fail can be triggered by feeding a corner case to `tf.raw_ops.MatrixSquareRoot`. This case is similar to the one I submitted in the last issue, and this kind of bug is likely to be caused by the 0 and large value in input's shape.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
input = tf.random.uniform([10, 15, 0, 4, 5849693008847355793], dtype=tf.float64, minval=-1024, maxval=1024)
res = tf.raw_ops.MatrixSquareRoot(
    input=input,
)
```


### Relevant log output

```shell
2023-03-18 18:27:16.712528: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-03-18 18:27:16.760165: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-18 18:27:17.510832: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-03-18 18:27:19.128040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14577 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:2f:00.0, compute capability: 7.0
2023-03-18 18:27:19.389933: F tensorflow/core/framework/tensor_shape.cc:201] Non-OK-status: InitDims(dim_sizes) status: INVALID_ARGUMENT: Encountered overflow when multiplying 4 with 5849693008847355793, result: -1
Aborted (core dumped)
```
</details>"
60034,A check fail can be triggered in CholeskyGrad,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf 2.13.0-dev20230317

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.5

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A crash due to check fail can be triggered by giving a corner case in `tf.raw_ops.CholeskyGrad`.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
l = tf.random.uniform([8, 0, 2**60, 8], dtype=tf.float64, minval=-2**50, maxval=2**50)
grad = tf.random.uniform([], dtype=tf.float64, minval=-2**50, maxval=2**50)
res = tf.raw_ops.CholeskyGrad(
    l=l,
    grad=grad,
)
```


### Relevant log output

```shell
2023-03-18 18:20:23.847214: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-03-18 18:20:23.894704: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-18 18:20:24.657541: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-03-18 18:20:26.296951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14577 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:2f:00.0, compute capability: 7.0
2023-03-18 18:20:26.563113: F tensorflow/core/framework/tensor_shape.cc:201] Non-OK-status: InitDims(dim_sizes) status: INVALID_ARGUMENT: Encountered overflow when multiplying 1152921504606846976 with 8, result: -9223372036854775808
Aborted (core dumped)
```
</details>"
60032,"failed to expand 'RETURN_IF_ROCTRACER_ERROR', it is invalid to use a preprocessor directive as macro parameter","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!Why is this an issue?
At Codacy we strive to provide great descriptions for our patterns. With good explanations developers can better understand issues and even learn how to fix them.

For this tool we are not yet meeting this standard but you can help us improve the docs. To know more, take a look at our tool documentation guide.

You can also visit the tool's website to find useful tips about the patterns.
```


### Standalone code to reproduce the issue

```shell
}
    RETURN_IF_ROCTRACER_ERROR(static_cast<roctracer_status_t>(
#if TF_ROCM_VERSION >= 50300
        se::wrap::roctracer_next_record(record, &record)
#else
```


### Relevant log output

_No response_</details>"
60031,Build failure: enumeration value ‘kFP8’ not handled in switch [-Werror=switch],"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

Latest master branch (3.17), tf-nightly 2.13

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10.9

### Bazel version

5.3.0

### GCC/Compiler version

11

### CUDA/cuDNN version

CUDA12.1 cuDNN8.8.1 TensorRT8.6.0

### GPU model and memory

RTX 3070 8G

### Current Behaviour?

```shell
Cannot build with bazel

ERROR: /tmp/tensorflow/tensorflow/compiler/tf2tensorrt/BUILD:188:11: Compiling tensorflow/compiler/tf2tensorrt/common/utils.cc failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/compiler/tf2tensorrt/_objs/common_utils/utils.pic.d ... (remaining 176 arguments skipped)
tensorflow/compiler/tf2tensorrt/common/utils.cc: In function ‘std::ostream& nvinfer1::operator<<(std::ostream&, const nvinfer1::DataType&)’:
tensorflow/compiler/tf2tensorrt/common/utils.cc:209:10: error: enumeration value ‘kFP8’ not handled in switch [-Werror=switch]
  209 |   switch (v) {
      |          ^
cc1plus: some warnings being treated as errors
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```


### Standalone code to reproduce the issue

```shell
bazel build --config=cuda --cxxopt=""-march=native"" --cxxopt=""-O3"" //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
/tmp/tensorflow/tensorflow/compiler/tf2tensorrt/BUILD:188:11: Compiling tensorflow/compiler/tf2tensorrt/common/utils.cc failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/compiler/tf2tensorrt/_objs/common_utils/utils.pic.d ... (remaining 176 arguments skipped)
tensorflow/compiler/tf2tensorrt/common/utils.cc: In function ‘std::ostream& nvinfer1::operator<<(std::ostream&, const nvinfer1::DataType&)’:
tensorflow/compiler/tf2tensorrt/common/utils.cc:209:10: error: enumeration value ‘kFP8’ not handled in switch [-Werror=switch]
  209 |   switch (v) {
      |          ^
cc1plus: some warnings being treated as errors
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```
</details>"
60030, About Android calling TensorFlow Lite crash problem,"
### 1. Problem overview
  I recently reported an error when importing the Tensorflow Lite model using Android Studio, but I did not find the problem after searching. I hope you can help me. The following is the detailed information

### 2. System information

    -implementation 'org.tensorflow:tensorflow-lite:2.11.0'
    -implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'
    -implementation 'org.tensorflow:tensorflow-lite-support:0.1.0'
    -implementation 'org.tensorflow:tensorflow-lite-metadata:0.1.0'
    -Android 12.0 x86_64


### 3. Code

      public MyModelRunner(Context context) throws IOException {
              try {
                  interpreter = new Interpreter(loadModelFile(context));
      
              } catch (IOException e) {
                  e.printStackTrace();
              }
          }

    private MappedByteBuffer loadModelFile(Context context) throws IOException {


        AssetFileDescriptor fileDescriptor = context.getAssets().openFd(""new_model.tflite"");

        FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());

        FileChannel fileChannel = inputStream.getChannel();

        long startOffset = fileDescriptor.getStartOffset();
        long declaredLength = fileDescriptor.getDeclaredLength();

        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);
    }

### 4. (optional)  info / logs
2023-03-17 18:02:45.343 22670-22670 libc                    com.example.myproject01              A  Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0xfffffff4 in tid 22670 (ple.myproject01), pid 22670 (ple.myproject01)
2023-03-17 18:02:45.399 22706-22706 DEBUG                   pid-22706                            A  pid: 22670, tid: 22670, name: ple.myproject01  >>> com.example.myproject01 <<<
2023-03-17 18:02:45.637 22706-22706 DEBUG                   pid-22706                            A        #00 pc 00282792  /data/app/~~8c21WzJfTIFEm7pmQFGjow==/com.example.myproject01-zu0b6G7gIfCmfZkzPmHgpg==/lib/x86/libtensorflowlite_flex_jni.so
2023-03-17 18:02:45.637 22706-22706 DEBUG                   pid-22706                            A        #01 pc 020242b8  /data/app/~~8c21WzJfTIFEm7pmQFGjow==/com.example.myproject01-zu0b6G7gIfCmfZkzPmHgpg==/lib/x86/libtensorflowlite_flex_jni.so
2023-03-17 18:02:45.637 22706-22706 DEBUG                   pid-22706                            A        #02 pc 02023ec2  /data/app/~~8c21WzJfTIFEm7pmQFGjow==/com.example.myproject01-zu0b6G7gIfCmfZkzPmHgpg==/lib/x86/libtensorflowlite_flex_jni.so
2023-03-17 18:02:45.637 22706-22706 DEBUG                   pid-22706                            A        #03 pc 0202456b  /data/app/~~8c21WzJfTIFEm7pmQFGjow==/com.example.myproject01-zu0b6G7gIfCmfZkzPmHgpg==/lib/x86/libtensorflowlite_flex_jni.so
2023-03-17 18:02:45.637 22706-22706 DEBUG                   pid-22706                            A        #04 pc 01e6f187  /data/app/~~8c21WzJfTIFEm7pmQFGjow==/com.example.myproject01-zu0b6G7gIfCmfZkzPmHgpg==/lib/x86/libtensorflowlite_flex_jni.so
2023-03-17 18:02:45.637 22706-22706 DEBUG                   pid-22706                            A        #05 pc 0027cb45  /data/app/~~8c21WzJfTIFEm7pmQFGjow==/com.example.myproject01-zu0b6G7gIfCmfZkzPmHgpg==/lib/x86/libtensorflowlite_flex_jni.so
2023-03-17 18:02:45.639 22706-22706 DEBUG                   pid-22706                            A        #37 pc 002e6bdc  [anon:dalvik-classes.dex extracted in memory from /data/app/~~8c21WzJfTIFEm7pmQFGjow==/com.example.myproject01-zu0b6G7gIfCmfZkzPmHgpg==/base.apk] (org.tensorflow.lite.flex.FlexDelegate.<clinit>+4)
2023-03-17 18:02:45.640 22706-22706 DEBUG                   pid-22706                            A        #60 pc 002e47ae  [anon:dalvik-classes.dex extracted in memory from /data/app/~~8c21WzJfTIFEm7pmQFGjow==/com.example.myproject01-zu0b6G7gIfCmfZkzPmHgpg==/base.apk] (org.tensorflow.lite.NativeInterpreterWrapper.maybeCreateFlexDelegate+6)
2023-03-17 18:02:45.641 22706-22706 DEBUG                   pid-22706                            A        #63 pc 002e4be4  [anon:dalvik-classes.dex extracted in memory from /data/app/~~8c21WzJfTIFEm7pmQFGjow==/com.example.myproject01-zu0b6G7gIfCmfZkzPmHgpg==/base.apk] (org.tensorflow.lite.NativeInterpreterWrapper.addDelegates+16)
2023-03-17 18:02:45.641 22706-22706 DEBUG                   pid-22706                            A        #66 pc 002e4e68  [anon:dalvik-classes.dex extracted in memory from /data/app/~~8c21WzJfTIFEm7pmQFGjow==/com.example.myproject01-zu0b6G7gIfCmfZkzPmHgpg==/base.apk] (org.tensorflow.lite.NativeInterpreterWrapper.init+104)
2023-03-17 18:02:45.641 22706-22706 DEBUG                   pid-22706                            A        #69 pc 002e4baa  [anon:dalvik-classes.dex extracted in memory from /data/app/~~8c21WzJfTIFEm7pmQFGjow==/com.example.myproject01-zu0b6G7gIfCmfZkzPmHgpg==/base.apk] (org.tensorflow.lite.NativeInterpreterWrapper.<init>+146)
2023-03-17 18:02:45.641 22706-22706 DEBUG                   pid-22706                            A        #72 pc 002e44d0  [anon:dalvik-classes.dex extracted in memory from /data/app/~~8c21WzJfTIFEm7pmQFGjow==/com.example.myproject01-zu0b6G7gIfCmfZkzPmHgpg==/base.apk] (org.tensorflow.lite.NativeInterpreterWrapperExperimental.<init>)
2023-03-17 18:02:45.641 22706-22706 DEBUG                   pid-22706                            A        #75 pc 002e4304  [anon:dalvik-classes.dex extracted in memory from /data/app/~~8c21WzJfTIFEm7pmQFGjow==/com.example.myproject01-zu0b6G7gIfCmfZkzPmHgpg==/base.apk] (org.tensorflow.lite.Interpreter.<init>+4)
2023-03-17 18:02:45.641 22706-22706 DEBUG                   pid-22706                            A        #78 pc 002e42e6  [anon:dalvik-classes.dex extracted in memory from /data/app/~~8c21WzJfTIFEm7pmQFGjow==/com.example.myproject01-zu0b6G7gIfCmfZkzPmHgpg==/base.apk] (org.tensorflow.lite.Interpreter.<init>+2)
2023-03-17 18:02:45.642 22706-22706 DEBUG                   pid-22706                            A        #81 pc 00000f66  /data/data/com.example.myproject01/code_cache/.overlay/base.apk/classes3.dex (com.example.myproject01.MyModelRunner.<init>+18)
2023-03-17 18:02:45.642 22706-22706 DEBUG                   pid-22706                            A        #84 pc 00000fc8  /data/data/com.example.myproject01/code_cache/.overlay/base.apk/classes3.dex (com.example.myproject01.Recommend_Fragment$1.onClick+20)
---------------------------- PROCESS ENDED (22670) for package com.example.myproject01 ----------------------------
2023-03-17 18:02:45.957   518-597   InputDispatcher         system_process                       E  channel 'c1e13a6 com.example.myproject01/com.example.myproject01.MainActivity (server)' ~ Channel is unrecoverably broken and will be disposed!


"
60029,"pb file, StridedSlice op, attribute begin, Tensor data is empty  ","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.10

### Custom Code

Yes

### OS Platform and Distribution

win10

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
In https://github.com/google/automl/tree/master/efficientdet/tf2, I export a pb model from tf2-ckpt use tf2.10, but converting it to openvino-IR model failed. After I check out hours, I find it is caused by the StridedSlice op in pb file.
When this op slice from the first element, its attribute would be wrong in the pb file.
But it is all right if you slice not from the first element.
```


### Standalone code to reproduce the issue

```shell
Specifically, given a tensor x with shape [m], if I get the first row, y=x[0], the node details in pb file would probably be:

input
name: images

begin
name: strided_slice/stack
category: Const
type: int32[1]
Tensor data is empty.

end
name: strided_slice/stack_1
category: Const
type: int32[1]
[
    1
]

strides
name: strided_slice/stack_2
category: Const
type: int32[1]
[
    1
]
,
The point is the attribute begin, its data is empty rather than 0.
When I use tf 2.9.1, it produces the correct results:

input
name: images

begin
name: strided_slice/stack
category: Const
type: int32[1]
[
    0
]

end
name: strided_slice/stack_1
category: Const
type: int32[1]
[
    1
]

strides
name: strided_slice/stack_2
category: Const
type: int32[1]
[
    1
]
```


### Relevant log output

_No response_</details>"
60028,bug feedback,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.5

### Custom Code

Yes

### OS Platform and Distribution

Windows 10 21H2 9044.2604

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.6

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
Operator MaxPool2D has no output for a long time and doesn't throw any Exception when kernel_size is greater than the height of input_data
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

nums = 1
in_channels = 1
out_channels = 3
height = 1
width = 1
kernel_size = 3
stride = 2
pad_mode = 'valid'
tf_data = np.ones([nums, in_channels, height, width], float)
try:
    print(tf.keras.layers.MaxPool2D(pool_size=(kernel_size, kernel_size), strides=stride, 
          padding=pad_mode,data_format='channels_first')(tf_data))
except BaseException as e:
    print(e)
```


### Relevant log output

```shell
the code is always running without any output or exception
```
</details>"
60027,New,Issue
60024,tf-nightly [2.13.0.dev20230322]: module 'tensorflow' has no attribute 'version',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.13.0.dev20230316

### Custom Code

No

### OS Platform and Distribution

macos 13.0.1 x86

### Mobile device

_No response_

### Python version

3.9.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
AttributeError: module 'tensorflow' has no attribute 'version'

python -c ""import tensorflow as tf; tf.random.set_seed(1)""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
AttributeError: module 'tensorflow' has no attribute 'random'
```


### Standalone code to reproduce the issue

```shell
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
python -c ""import tensorflow as tf; tf.random.set_seed(1)""
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
AttributeError: module 'tensorflow' has no attribute 'version'
```
</details>"
60023,How to find accuracy of a pretrained tflite model,"
### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- Ubuntu 20.04
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: None
-   **TensorFlow installed from (source or binary)**: binary
-   **TensorFlow version (use command below)**: v2.11
-   **Python version**: 3.9

### Describe the problem
I have a pretrained tf mobilenetv2 model downloaded from tfhub which i converted to tflite using tflite interpreter converter. The original model was trained on imagenet dataset, but i want to find out the accuracy of the converted tflite model. How do I do that? As of now, the eval function is specific to models generated from modelmaker. What is the best way to find the model accuracy?

thanks
"
60022,Saving Custom Model Not Working,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Colab Notebook

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I'm trying to save a custom model where the number of channels increases as we progress along the forward pass of the architecture. The forward pass of the model works as expected, and I can train it. However, when trying to save the model, I get an error concerning the number of channels passed to certain convolutional layers. 

Is there a way around this issue?
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1YZd8OQWjgYasVblDit-gRNvA-kFJdRkn?usp=sharing
```


### Relevant log output

```shell
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-7-696ba1b852d4> in <module>
----> 1 wnet.save(""/content/wnet"")

7 frames
/tmp/__autograph_generated_filelz68b5h5.py in tf__call(self, x)
      8                 do_return = False
      9                 retval_ = ag__.UndefinedReturnValue()
---> 10                 x = ag__.converted_call(ag__.ld(self).conv, (ag__.ld(x),), None, fscope)
     11                 x = ag__.converted_call(ag__.ld(self).norm, (ag__.ld(x),), None, fscope)
     12                 x = ag__.converted_call(ag__.ld(self).activation, (ag__.ld(x),), None, fscope)

ValueError: Exception encountered when calling layer 'base_model' (type BaseModel).

in user code:

    File ""<ipython-input-1-355bdcd220c4>"", line 149, in call  *
        x = decoder_block([*previous_skips[str(self.global_depth - 1 - i)],
    File ""/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler  **
        raise e.with_traceback(filtered_tb) from None
    File ""/tmp/__autograph_generated_file30lnbd33.py"", line 12, in tf__call
        x = ag__.converted_call(ag__.ld(self).block, (ag__.ld(x),), None, fscope)
    File ""/tmp/__autograph_generated_filejlndlto_.py"", line 10, in tf__call
        x = ag__.converted_call(ag__.ld(self).conv1, (ag__.ld(x),), None, fscope)
    File ""/tmp/__autograph_generated_filelz68b5h5.py"", line 10, in tf__call
        x = ag__.converted_call(ag__.ld(self).conv, (ag__.ld(x),), None, fscope)

    ValueError: Exception encountered when calling layer 'decoder_block_1' (type DecoderBlock).
    
    in user code:
    
        File ""<ipython-input-1-355bdcd220c4>"", line 96, in call  *
            x = self.block(x)
        File ""/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler  **
            raise e.with_traceback(filtered_tb) from None
        File ""/tmp/__autograph_generated_filejlndlto_.py"", line 10, in tf__call
            x = ag__.converted_call(ag__.ld(self).conv1, (ag__.ld(x),), None, fscope)
        File ""/tmp/__autograph_generated_filelz68b5h5.py"", line 10, in tf__call
            x = ag__.converted_call(ag__.ld(self).conv, (ag__.ld(x),), None, fscope)
    
        ValueError: Exception encountered when calling layer 'u_net_block_7' (type UNetBlock).
        
        in user code:
        
            File ""<ipython-input-2-76991f1be87b>"", line 14, in call  *
                x = self.conv1(x)
            File ""/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler  **
                raise e.with_traceback(filtered_tb) from None
            File ""/tmp/__autograph_generated_filelz68b5h5.py"", line 10, in tf__call
                x = ag__.converted_call(ag__.ld(self).conv, (ag__.ld(x),), None, fscope)
        
            ValueError: Exception encountered when calling layer 'conv_layer_14' (type ConvLayer).
            
            in user code:
            
                File ""<ipython-input-1-355bdcd220c4>"", line 56, in call  *
                    x = self.conv(x)
                File ""/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler  **
                    raise e.with_traceback(filtered_tb) from None
                File ""/usr/local/lib/python3.9/dist-packages/keras/engine/input_spec.py"", line 277, in assert_input_compatibility
                    raise ValueError(
            
                ValueError: Input 0 of layer ""conv3d_14"" is incompatible with the layer: expected axis -1 of input shape to have value 24, but received input with shape (None, 16, 16, 16, 32)
            
            
            Call arguments received by layer 'conv_layer_14' (type ConvLayer):
              • x=tf.Tensor(shape=(None, 16, 16, 16, 32), dtype=float32)
        
        
        Call arguments received by layer 'u_net_block_7' (type UNetBlock):
          • x=tf.Tensor(shape=(None, 16, 16, 16, 32), dtype=float32)
    
    
    Call arguments received by layer 'decoder_block_1' (type DecoderBlock):
      • skip=['tf.Tensor(shape=(None, 16, 16, 16, 8), dtype=float32)', 'tf.Tensor(shape=(None, 16, 16, 16, 8), dtype=float32)', 'tf.Tensor(shape=(None, 16, 16, 16, 8), dtype=float32)']
      • x=tf.Tensor(shape=(None, 8, 8, 8, 8), dtype=float32)


Call arguments received by layer 'base_model' (type BaseModel):
  • x=tf.Tensor(shape=(None, 8, 8, 8, 8), dtype=float32)
  • previous_skips={'0': ListWrapper(['tf.Tensor(shape=(None, 64, 64, 64, 8), dtype=float32)']), '1': ListWrapper(['tf.Tensor(shape=(None, 32, 32, 32, 8), dtype=float32)']), '2': ListWrapper(['tf.Tensor(shape=(None, 16, 16, 16, 8), dtype=float32)', 'tf.Tensor(shape=(None, 16, 16, 16, 8), dtype=float32)'])}
  • previous_peaks={'0': ListWrapper([]), '1': ListWrapper(['tf.Tensor(shape=(None, 32, 32, 32, 8), dtype=float32)']), '2': ListWrapper(['tf.Tensor(shape=(None, 16, 16, 16, 8), dtype=float32)'])}
```
</details>"
60012,Not able to build TFLite GPU Delegate dynamic library with bazel for android arm64,"-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS Monterey(12.6)
-   **TensorFlow installed from (source or binary)**: Source
-   **Python version**: 3.9.13
-   **Bazel version (if compiling from source)**: 5.3.0
-   **GCC/Compiler version (if compiling from source)**: Apple clang version 14.0.0 (clang-1400.0.29.202)
-   **Exact command to reproduce**: bazel build -c opt --config=android_arm64 --cpu=arm64-v8a //tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so

### Describe the problem
Not able to build dynamic tensorflow lite library with gpu delegate for android arm 64. Other tensorflow lite libraries with nnapi, xnnpack delegates are building fine.
### Error
tensorflow/lite/delegates/gpu/delegate.cc:796:7: error: use of undeclared identifier 'AHardwareBuffer_acquire'; did you mean 'AHardwareBuffer_Plane'?
      AHardwareBuffer_acquire(ahwb);
      ^
external/androidndk/ndk/sysroot/usr/include/android/hardware_buffer.h:320:3: note: 'AHardwareBuffer_Plane' declared here
} AHardwareBuffer_Plane;
  ^
tensorflow/lite/delegates/gpu/delegate.cc:804:9: error: use of undeclared identifier 'AHardwareBuffer_release'; did you mean 'AHardwareBuffer_Plane'?
        AHardwareBuffer_release(b);
        ^
external/androidndk/ndk/sysroot/usr/include/android/hardware_buffer.h:320:3: note: 'AHardwareBuffer_Plane' declared here
} AHardwareBuffer_Plane;
  ^
tensorflow/lite/delegates/gpu/delegate.cc:812:7: error: use of undeclared identifier 'AHardwareBuffer_describe'
      AHardwareBuffer_describe(uptr_ahwb.get(), &desc_ahwb);
      ^
tensorflow/lite/delegates/gpu/delegate.cc:1194:18: error: use of undeclared identifier 'AHardwareBuffer_lock'
          return AHardwareBuffer_lock(buffer, this->usage_, -1 /* fence */,
                 ^
tensorflow/lite/delegates/gpu/delegate.cc:1221:24: error: use of undeclared identifier 'AHardwareBuffer_unlock'
                return AHardwareBuffer_unlock(buffer, nullptr /* fence */);
                       ^
5 errors generated.
Target //tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so failed to build
"
60011,Include tf.IsInf on Tensorflow Lite Operators,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 2.11


**Provide the text output from tflite_convert**

```
<unknown>:0: error: failed while converting: 'main': 
Some ops in the model are custom ops, See instructions to implement custom ops: https://www.tensorflow.org/lite/guide/ops_custom 
Custom ops: IsInf
Details:
	tf.IsInf(tensor<?x21x?xf64>) -> (tensor<?x21x?xi1>) : {device = """"}
	tf.IsInf(tensor<?x33x?xf64>) -> (tensor<?x33x?xi1>) : {device = """"}
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
60009,can substitute boringssl with openssl?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.8.4

### Custom Code

No

### OS Platform and Distribution

ubuntu22.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
my project import brpc and tf both, brpc must use openssl, tf use boringssl, so build will fail, can I modify tf use openssl?
```


### Standalone code to reproduce the issue

```shell
build conflict
```


### Relevant log output

_No response_</details>"
60008,tr1,"**System information**
- Android Device information (use `adb shell getprop ro.build.fingerprint`
  if possible):
- TensorFlow Lite in Play Services SDK version (found in `build.gradle`):
- Google Play Services version
  (`Settings` > `Apps` > `Google Play Services` > `App details`):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to or attach code demonstrating
the problem.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
60007,bug feedback,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
The code and results can be seen specifically. When I use model(x, training = fasle) and model.predict(), the results of the first few times are always wrong, and the reason cannot be found.You will find that the code that runs for the first time is always wrong. I did this test because I was using model(img, training = False) and model.predict() to get different results. Later, it was verified that it was the first few times There will be errors, and it will be normal in the future.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.keras.layers import (Activation, BatchNormalization,Dense, Flatten, Conv2D ,Input,
                                     Concatenate, Conv2D, Dense, Dropout,
                                     GlobalAveragePooling2D, Lambda,
                                     MaxPooling2D, add)
from tensorflow.keras.models import Model
img = np.random.random((1,112,112,3))

def create_model():
  model = tf.keras.Sequential([
      tf.keras.layers.Conv2D(32, 3, activation='relu'),
      tf.keras.layers.MaxPooling2D(),
      tf.keras.layers.Conv2D(64, 3, activation='relu'),
      tf.keras.layers.MaxPooling2D(),
      tf.keras.layers.Conv2D(128, 1, activation='relu'),
      tf.keras.layers.MaxPooling2D(),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(64, activation='relu'),
      tf.keras.layers.Dense(10)
    ])

  return model


input_shape     = [112, 112, 3]
inputs = Input(shape=input_shape)
model = create_model()

for i in range(5):
    print(model.predict(img)[0][0:10])
# or run up core ------------------------

@tf.function
def test_step():
    return model(img,training = False)
for i in range(5):
    print(test_step()[0][0:10])
```


### Relevant log output

```shell
[ 0.13942198  0.16932502 -0.00587191 -0.03490476 -0.05865425 -0.10204756
  0.02883459 -0.06782953 -0.17910798  0.10979672]
[ 0.13943174  0.16934936 -0.00590273 -0.03488521 -0.05866342 -0.10207407
  0.02890574 -0.06792954 -0.1791203   0.10971892]
[ 0.13942198  0.16932502 -0.00587191 -0.03490476 -0.05865425 -0.10204756
  0.02883459 -0.06782953 -0.17910798  0.10979672]
[ 0.13942198  0.16932502 -0.00587191 -0.03490476 -0.05865425 -0.10204756
  0.02883459 -0.06782953 -0.17910798  0.10979672]
[ 0.13942198  0.16932502 -0.00587191 -0.03490476 -0.05865425 -0.10204756
  0.02883459 -0.06782953 -0.17910798  0.10979672]

up result------------------------


tf.Tensor(
[ 0.33203632 -0.11037838  0.13731733 -0.01554211  0.212542   -0.13555858
  0.21057142  0.2515831  -0.16747513  0.2777309 ], shape=(10,), dtype=float32)
tf.Tensor(
[ 0.33203542 -0.1103816   0.13731548 -0.01554125  0.21254522 -0.13556004
  0.21057592  0.25158295 -0.16747653  0.27773613], shape=(10,), dtype=float32)
tf.Tensor(
[ 0.3320552  -0.11031806  0.1372948  -0.01558295  0.21247748 -0.13555783
  0.21053793  0.25155216 -0.16746978  0.27769867], shape=(10,), dtype=float32)
tf.Tensor(
[ 0.33203542 -0.1103816   0.13731548 -0.01554125  0.21254522 -0.13556004
  0.21057592  0.25158295 -0.16747653  0.27773613], shape=(10,), dtype=float32)
tf.Tensor(
[ 0.33203542 -0.1103816   0.13731548 -0.01554125  0.21254522 -0.13556004
  0.21057592  0.25158295 -0.16747653  0.27773613], shape=(10,), dtype=float32)
```
</details>"
60004,Cant assign trainable attribute to variables ->AttributeError: can't set attribute,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf.2.10

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When i try to change the trainability of a variable of a model i get the error : AttributeError: can't set attribute
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from keras import layers
import numpy as np


class model(tf.keras.Model):

    def __init__(self):
        super(model, self).__init__()
        self.b = self.add_weight(shape=(1, 5), trainable=True)
        self.a = tf.Variable([1.0], trainable=False)
        self.a.trainable = True

    def call(self, inputs, training=None, mask=None):
        return self.a * self.b * inputs


input = np.random.randn(10, 5)
target = np.random.randn(10, 1)

model = model()
model.b.trainable = True
model.a.trainable = True
model.compile(""adam"", loss=""mse"")

model.fit(input, target, validation_data=(input, target), epochs=3)
```


### Relevant log output

```shell
self.a.trainable = True
AttributeError: can't set attribute

Process finished with exit code 1
```
</details>"
59997,How to benchmark TFLite object detection model,"It seems that only classification models can be benchmarked. There is no explanation for how to run benchmarking on `TFLite` object detection models under https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark. Could anybody point me into the right direction to get the modifications needed to run the `TFLite` benchmark on my custom `TFLite` object detection model?

After running:

```bash
➜  tensorflow git:(master) adb shell am start -S \                                    
  -n org.tensorflow.lite.benchmark/.BenchmarkModelActivity \
  --es args '""--graph=/data/local/tmp/my_custom_object_detection_float32.tflite \
  --num_threads=4""'
```

When I check logcat:

```bash
➜  tensorflow git:(master) adb logcat | grep ""Inference timings in us""
```

I see:

```shell
03-15 14:00:51.294 12836 12836 I tflite  : Inference timings in us: Init: 36618, First inference: 1530248, Warmup (avg): 1.53025e+06, Inference (avg): 1.49712e+06
03-15 14:13:43.409 13393 13393 I tflite  : Inference timings in us: Init: 33412, First inference: 307818, Warmup (avg): 297955, Inference (avg): 333826
```

Basically, my model inference fails and it falls back onto the classification model


<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

### Standalone code to reproduce the issue

```shell
The app falls back onto the classification model once the object detection one fails:

03-15 14:00:51.294 12836 12836 I tflite  : Inference timings in us: Init: 36618, First inference: 1530248, Warmup (avg): 1.53025e+06, Inference (avg): 1.49712e+06
03-15 14:13:43.409 13393 13393 I tflite  : Inference timings in us: Init: 33412, First inference: 307818, Warmup (avg): 297955, Inference (avg): 333826
```


### Relevant log output

_No response_</details>"
59996,Strange Autograph Error,"Hey TF-Team/Community,

I faced a strange error. First of all we whereable to workaround it by adapting the tensorflow.python.autograph.impl.api code. 

Background:
   Packaging our application with pyinstaller, using efficient detection. In development everything works as fine, after packaging wiht pyinstaller somehow the following tf_stack module breaks with the following error:
Unable to cast Python instance to C++ type. (at least this is because it retrieved a None value)

At the end of this post I attached the whole error trace (as json string):).

If you go a little bit up in the errortrace you will find that this error occured at tensorflow.python.autograph.impl.api:436. At the with StacktraceMapper statement. So we had a look at this and found this here:
![Pasted Graphic 4](https://user-images.githubusercontent.com/20928617/225317482-8bdf1e3c-8ecc-4111-ba97-77e99cf72863.png)

When we printed the effective_source_map variable. When the error occured it was just None. When everything worked well it had a value that pointed to a tensorflow py file. (Have no idea what this is all about).


And here things get very strange:
I tried some parameters like multi_strategy and batch_size.
 -> When I used in my tests only 4 images for training it worked with a batch_size >= 2 and single_strategy in my packaged pyinstaller version.

  -> When I used more images I had to increase the batch_size, elsewise the error occured again for example with 12 images I needed at last a batch_size of 3 with 24: 4 with 55 it worked with 10 (haven't tried below for the 55 image case).

  -> If I use multi Strategy and train on 3 Graphic cards everything works also very well. Only on single gpu I had to increase the batch_size.

So our solution here was to modify your code, I wrote a fix for it inside our build script.

We modifyed the following (since it was None):
<img width=""677"" alt=""image"" src=""https://user-images.githubusercontent.com/20928617/225319368-d5ceb662-3f31-495a-a185-4fb11187bd23.png"">

to:
![image](https://user-images.githubusercontent.com/20928617/225320465-c7955865-5cd8-473e-b2d1-d96de8fd0380.png)


and it works very well.

Really I have no clue what this is all about it. It tooks us like 2 weeks to find a solution for this. Maybe you have an idea and can give us an explanation for this strange behaviour.


Here is our fix from our building script:

`
def fix_tf_c_plus_plus_type_cast_error(self):
  code_fix = ""with StackTraceMapper(converted_f):""
  search_str = ""with StackTraceMapper(converted_f), tf_stack.CurrentModuleFilter():""
  tf_autograph_api_file_path = os.path.join(self.site_packages_path, 'tensorflow', 'python', 'autograph', 'impl', 'api.py')
  tf_autograph_api_file = open(tf_autograph_api_file_path, 'r')
  tf_autograph_api_file_code = tf_autograph_api_file.read()
  tf_autograph_api_file_code = tf_autograph_api_file_code.replace(search_str, code_fix)
  tf_autograph_api_file = open(tf_autograph_api_file_path, 'w')
        tf_autograph_api_file.seek(0)
        tf_autograph_api_file.write(tf_autograph_api_file_code)

`




Errortrace:
[{""_id"":""63ff19cfa6631d318bcb12c7"",""msg"":""Unable to cast Python instance to C++ type (compile in debug mode for details)"",""type"":""ERROR"",""source"":""WORKER_NODE: Need a name"",""created_at"":""2023-03-01T10:24:31.323000"",""stacktrace"":[""Traceback (most recent call last):\n"",""  File \""worker\\worker_process.py\"", line 176, in run_job_thread\n"",""  File \""worker\\worker_process.py\"", line 489, in _do_wizard\n"",""  File \""wizards\\base_wizard.py\"", line 110, in start\n"",""  File \""wizards\\implementations\\image_object_detection\\image_object_detection_wizard.py\"", line 112, in run\n"",""  File \""wizards\\implementations\\image_object_detection\\image_object_detection_wizard.py\"", line 313, in train\n"",""  File \""experiment\\experiments\\[train.py](http://train.py/)\"", line 86, in runTrainExperiment\n"",""  File \""experiment\\experiments\\vision\\efficientdet_train_experiment.py\"", line 120, in run\n"",""  File \""experiment\\experiments\\base_train_experiment.py\"", line 266, in custom_run\n"",""  File \""tensorflow\\python\\util\\traceback_utils.py\"", line 153, in error_handler\n"",""  File \""tensorflow\\python\\util\\traceback_utils.py\"", line 150, in error_handler\n"",""  File \""tensorflow\\python\\eager\\def_function.py\"", line 915, in __call__\n"",""  File \""tensorflow\\python\\eager\\def_function.py\"", line 963, in _call\n"",""  File \""tensorflow\\python\\eager\\def_function.py\"", line 785, in _initialize\n"",""  File \""tensorflow\\python\\eager\\[function.py](http://function.py/)\"", line 2480, in _get_concrete_function_internal_garbage_collected\n"",""  File \""tensorflow\\python\\eager\\[function.py](http://function.py/)\"", line 2711, in _maybe_define_function\n"",""  File \""tensorflow\\python\\eager\\[function.py](http://function.py/)\"", line 2627, in _create_graph_function\n"",""  File \""tensorflow\\python\\framework\\func_graph.py\"", line 1141, in func_graph_from_py_func\n"",""  File \""tensorflow\\python\\eager\\def_function.py\"", line 677, in wrapped_fn\n"",""  File \""tensorflow\\python\\framework\\func_graph.py\"", line 1116, in autograph_handler\n"",""  File \""tensorflow\\python\\autograph\\impl\\[api.py](http://api.py/)\"", line 434, in converted_call\n"",""  File \""tensorflow\\python\\autograph\\impl\\[api.py](http://api.py/)\"", line 484, in _fall_back_unconverted\n"",""  File \""tensorflow\\python\\autograph\\impl\\[api.py](http://api.py/)\"", line 458, in _call_unconverted\n"",""  File \""experiment\\experiments\\base_train_experiment.py\"", line 245, in distributed_train_step\n"",""  File \""tensorflow\\python\\distribute\\distribute_lib.py\"", line 1312, in run\n"",""  File \""tensorflow\\python\\distribute\\distribute_lib.py\"", line 2888, in call_for_each_replica\n"",""  File \""tensorflow\\python\\distribute\\mirrored_strategy.py\"", line 676, in _call_for_each_replica\n"",""  File \""tensorflow\\python\\distribute\\mirrored_run.py\"", line 101, in call_for_each_replica\n"",""  File \""tensorflow\\python\\distribute\\mirrored_run.py\"", line 283, in _call_for_each_replica\n"",""  File \""tensorflow\\python\\training\\[coordinator.py](http://coordinator.py/)\"", line 385, in join\n"",""  File \""[six.py](http://six.py/)\"", line 703, in reraise\n"",""  File \""tensorflow\\python\\training\\[coordinator.py](http://coordinator.py/)\"", line 293, in stop_on_exception\n"",""  File \""tensorflow\\python\\distribute\\mirrored_run.py\"", line 386, in run\n"",""  File \""tensorflow\\python\\autograph\\impl\\[api.py](http://api.py/)\"", line 689, in wrapper\n"",""  File \""tensorflow\\python\\autograph\\impl\\[api.py](http://api.py/)\"", line 434, in converted_call\n"",""  File \""tensorflow\\python\\autograph\\impl\\[api.py](http://api.py/)\"", line 484, in _fall_back_unconverted\n"",""  File \""tensorflow\\python\\autograph\\impl\\[api.py](http://api.py/)\"", line 458, in _call_unconverted\n"",""  File \""experiment\\experiments\\vision\\efficientdet_train_experiment.py\"", line 98, in train_step\n"",""  File \""tensorflow_addons\\optimizers\\average_wrapper.py\"", line 56, in apply_gradients\n    return super().apply_gradients(grads_and_vars, name, **kwargs)\n"",""  File \""keras\\optimizers\\optimizer_v2\\optimizer_v2.py\"", line 672, in apply_gradients\n"",""  File \""keras\\optimizers\\optimizer_v2\\optimizer_v2.py\"", line 992, in _prepare\n"",""  File \""tensorflow_addons\\optimizers\\moving_average.py\"", line 112, in _prepare_local\n    apply_state[(var_device, var_dtype)][\""tfa_ma_decay\""] = self._get_decay(\n"",""  File \""tensorflow\\python\\util\\traceback_utils.py\"", line 153, in error_handler\n"",""  File \""tensorflow\\python\\util\\traceback_utils.py\"", line 150, in error_handler\n"",""  File \""tensorflow\\python\\eager\\def_function.py\"", line 915, in __call__\n"",""  File \""tensorflow\\python\\eager\\def_function.py\"", line 963, in _call\n"",""  File \""tensorflow\\python\\eager\\def_function.py\"", line 785, in _initialize\n"",""  File \""tensorflow\\python\\eager\\[function.py](http://function.py/)\"", line 2480, in _get_concrete_function_internal_garbage_collected\n"",""  File \""tensorflow\\python\\eager\\[function.py](http://function.py/)\"", line 2711, in _maybe_define_function\n"",""  File \""tensorflow\\python\\eager\\[function.py](http://function.py/)\"", line 2627, in _create_graph_function\n"",""  File \""tensorflow\\python\\framework\\func_graph.py\"", line 1141, in func_graph_from_py_func\n"",""  File \""tensorflow\\python\\eager\\def_function.py\"", line 677, in wrapped_fn\n"",""  File \""tensorflow\\python\\eager\\[function.py](http://function.py/)\"", line 3251, in bound_method_wrapper\n"",""  File \""tensorflow\\python\\framework\\func_graph.py\"", line 1116, in autograph_handler\n"",""  File \""tensorflow\\python\\autograph\\impl\\[api.py](http://api.py/)\"", line 436, in converted_call\n"",""  File \""tensorflow\\python\\util\\tf_stack.py\"", line 56, in __enter__\n"",""  File \""tensorflow\\python\\util\\tf_stack.py\"", line 99, in update\n"",""RuntimeError: Unable to cast Python instance to C++ type (compile in debug mode for details)\n""],""selected"":true}]

"
59994,LSTM model on arduino nano ble 33 using TFLite,"### 1. System information

- OS Platform and Distribution : Windows 11 + Arduino 1.8.19 
- TensorFlow installation (pip package or built from source): 2.11.0
- TensorFlow library : version 2.0

### 2. Code

  static tflite::MicroErrorReporter micro_error_reporter;
  error_reporter = &micro_error_reporter;
  model = tflite::GetModel(model_tflite_quant);
  if (model->version() != TFLITE_SCHEMA_VERSION) 
  {
    error_reporter->Report(""Model version does not match Schema"");
    while (1);
  }
  else
  {
    Serial.println(""Model schema match!"");
  }

  static tflite::AllOpsResolver resolver;
  
  // Build an interpreter to run the model with.
  static tflite::MicroInterpreter static_interpreter(
    model, resolver, tensor_arena, kTensorArenaSize, error_reporter);
  interpreter = &static_interpreter;

  // Allocate memory for the model's input and output tensors
  TfLiteStatus allocate_status = interpreter->AllocateTensors();

  if (allocate_status != kTfLiteOk) 
  {
    error_reporter->Report(""AllocateTensors() failed"");
    Serial.println(""Allocation failed!"");
    while(1);
  }
  else
  {
    Serial.println(""Allocation done!"");
  }

### 3. Probem

We are running LSTM model on arduino nano ble 33 using TFLite library. but we are getting ""Allocation failed!"". Parameters for LSTM model is 16 and LSTM_units = 1 and giving kTensorArenaSize = 150*1024. can u confirm the what is the reason for ""Allocation failed!""
"
59990,S3_USE_HTTPS 0 is being ignored - TF insists on HTTPS,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

No

### OS Platform and Distribution

cos_containerd

### Mobile device

_No response_

### Python version

3.9.2

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Running the ""Standalone code to reproduce the issue"" on Kubeflow 1.6 trying to use Minio storage via S3 protocol produces the logs attached with the following excerpt showing it is ignoring the S3_USE_HTTPS 0 environment variable: 

2023-03-14 18:07:48.690914: I tensorflow/c/logging.cc:34] Making request to https://10.244.1.171:9000/mlpipeline/tfx-template/data/taxi/data.csv
...
Error message: curlCode: 35, SSL connect error

Note, the following code using boto3 works just fine:

import boto3
session = boto3.session.Session()
s3client = session.client(service_name='s3',
                          aws_access_key_id='minio', 
                          aws_secret_access_key='Wi939w9', 
                          endpoint_url='http://10.244.1.171:9000', 
                          use_ssl='0', 
                          verify='0', 
                          region_name='us-east-1')

{'ResponseMetadata': {'RequestId': '174C5DC82C4E602D',
  'HostId': '',
  'HTTPStatusCode': 200,
  'HTTPHeaders': {'accept-ranges': 'bytes',
   'content-length': '667',
   'content-security-policy': 'block-all-mixed-content',
   'content-type': 'application/xml',
   'server': 'envoy',
   'vary': 'Origin',
   'x-amz-request-id': '174C5DC82C4E602D',
   'x-xss-protection': '1; mode=block',
   'date': 'Tue, 14 Mar 2023 18:51:15 GMT',
   'x-envoy-upstream-service-time': '1'},
  'RetryAttempts': 0},
 'IsTruncated': False,
 'Marker': '',
 'Contents': [{'Key': 'tfx-template/data/taxi/data.csv',
   'LastModified': datetime.datetime(2023, 2, 27, 23, 47, 55, 766000, tzinfo=tzlocal()),
   'ETag': '""43550bbc6946bcabc55328e0adb129b3-1""',
   'Size': 1922812,
   'StorageClass': 'STANDARD',
   'Owner': {'DisplayName': '',
    'ID': '02d6176db174dc93cb1b899f7c6078f08654445fe8cf1b6ce98d8855f66bdbf4'}}],
 'Name': 'mlpipeline',
 'Prefix': 'tfx-template/data/taxi/data.csv',
 'Delimiter': '',
 'MaxKeys': 1000,
 'EncodingType': 'url'}
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import tensorflow_io as tfio

import os
os.environ['AWS_REGION'] = 'us-east-1'
os.environ['S3_ENDPOINT'] = 'http://10.244.1.171:9000'
os.environ[""AWS_ACCESS_KEY_ID""] = 'minio'
os.environ[""AWS_SECRET_ACCESS_KEY""] = 'Wi939w9'
os.environ['S3_USE_HTTPS'] = '0'
os.environ['S3_VERIFY_SSL'] = '0'
os.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""0""
os.environ[""TF_CPP_MIN_VLOG_LEVEL""] = ""5""
os.environ[""AWS_LOG_LEVEL""] = ""trace""

tf.io.gfile.glob('s3://mlpipeline/tfx-template/data/taxi/data.csv')
# exists produces similar logs
#tf.io.gfile.exists('s3://mlpipeline/tfx-template/data/taxi/data.csv')
```


### Relevant log output

```shell
2023-03-14 18:07:48.690794: I tensorflow/c/logging.cc:34] No content body, content-length headers
2023-03-14 18:07:48.690823: I tensorflow/c/logging.cc:34] Found credential in environment with access key id minio
2023-03-14 18:07:48.690830: I tensorflow/c/logging.cc:34] Found secret key
2023-03-14 18:07:48.690836: I tensorflow/c/logging.cc:34] Note: Http payloads are not being signed. signPayloads=0 http scheme=https
2023-03-14 18:07:48.690857: I tensorflow/c/logging.cc:34] Canonical Header String: amz-sdk-invocation-id:26CFFBE4-9F97-48D7-BC0A-FFA9613B9DC0
amz-sdk-request:attempt=1
content-type:application/xml
host:10.244.1.171:9000
x-amz-api-version:2006-03-01
x-amz-content-sha256:UNSIGNED-PAYLOAD
x-amz-date:20230314T180748Z

2023-03-14 18:07:48.690861: I tensorflow/c/logging.cc:34] Signed Headers value:amz-sdk-invocation-id;amz-sdk-request;content-type;host;x-amz-api-version;x-amz-content-sha256;x-amz-date
2023-03-14 18:07:48.690872: I tensorflow/c/logging.cc:34] Canonical Request String: HEAD
/mlpipeline/tfx-template/data/taxi/data.csv

amz-sdk-invocation-id:26CFFBE4-9F97-48D7-BC0A-FFA9613B9DC0
amz-sdk-request:attempt=1
content-type:application/xml
host:10.244.1.171:9000
x-amz-api-version:2006-03-01
x-amz-content-sha256:UNSIGNED-PAYLOAD
x-amz-date:20230314T180748Z

amz-sdk-invocation-id;amz-sdk-request;content-type;host;x-amz-api-version;x-amz-content-sha256;x-amz-date
UNSIGNED-PAYLOAD
2023-03-14 18:07:48.690886: I tensorflow/c/logging.cc:34] Final String to sign: AWS4-HMAC-SHA256
20230314T180748Z
20230314/us-east-1/s3/aws4_request
04da96fd45298dbfd632d7a49f6b1d7de1b62583c81bb099e9b860b2b86628a3
2023-03-14 18:07:48.690891: I tensorflow/c/logging.cc:34] Final computed signing hash: 23e330b317746f6b48081889f4b7bb2756aa9eefe49c098b8c82ca81da3a6c39
2023-03-14 18:07:48.690899: I tensorflow/c/logging.cc:34] Signing request with: AWS4-HMAC-SHA256 Credential=minio/20230314/us-east-1/s3/aws4_request, SignedHeaders=amz-sdk-invocation-id;amz-sdk-request;content-type;host;x-amz-api-version;x-amz-content-sha256;x-amz-date, Signature=23e330b317746f6b48081889f4b7bb2756aa9eefe49c098b8c82ca81da3a6c39
2023-03-14 18:07:48.690905: I tensorflow/c/logging.cc:34] Request Successfully signed
2023-03-14 18:07:48.690914: I tensorflow/c/logging.cc:34] Making request to https://10.244.1.171:9000/mlpipeline/tfx-template/data/taxi/data.csv
2023-03-14 18:07:48.690919: I tensorflow/c/logging.cc:34] Including headers:
2023-03-14 18:07:48.690922: I tensorflow/c/logging.cc:34] amz-sdk-invocation-id: 26CFFBE4-9F97-48D7-BC0A-FFA9613B9DC0
2023-03-14 18:07:48.690926: I tensorflow/c/logging.cc:34] amz-sdk-request: attempt=1
2023-03-14 18:07:48.690929: I tensorflow/c/logging.cc:34] authorization: AWS4-HMAC-SHA256 Credential=minio/20230314/us-east-1/s3/aws4_request, SignedHeaders=amz-sdk-invocation-id;amz-sdk-request;content-type;host;x-amz-api-version;x-amz-content-sha256;x-amz-date, Signature=23e330b317746f6b48081889f4b7bb2756aa9eefe49c098b8c82ca81da3a6c39
2023-03-14 18:07:48.690932: I tensorflow/c/logging.cc:34] content-type: application/xml
2023-03-14 18:07:48.690935: I tensorflow/c/logging.cc:34] host: 10.244.1.171:9000
2023-03-14 18:07:48.690939: I tensorflow/c/logging.cc:34] user-agent: aws-sdk-cpp/1.8.186 Linux/5.15.0-67-generic x86_64 GCC/7.3.1
2023-03-14 18:07:48.690945: I tensorflow/c/logging.cc:34] x-amz-api-version: 2006-03-01
2023-03-14 18:07:48.690949: I tensorflow/c/logging.cc:34] x-amz-content-sha256: UNSIGNED-PAYLOAD
2023-03-14 18:07:48.690954: I tensorflow/c/logging.cc:34] x-amz-date: 20230314T180748Z
2023-03-14 18:07:48.690960: I tensorflow/c/logging.cc:34] Attempting to acquire curl connection.
2023-03-14 18:07:48.690966: I tensorflow/c/logging.cc:34] Connection has been released. Continuing.
2023-03-14 18:07:48.690971: I tensorflow/c/logging.cc:34] Returning connection handle 0x56396c2af710
2023-03-14 18:07:48.690976: I tensorflow/c/logging.cc:34] Obtained connection handle 0x56396c2af710
2023-03-14 18:07:48.695426: E tensorflow/c/logging.cc:40] Curl returned error code 35 - SSL connect error
2023-03-14 18:07:48.695462: I tensorflow/c/logging.cc:34] Destroy curl handle: 0x56396c2af710
2023-03-14 18:07:48.695483: I tensorflow/c/logging.cc:34] Created replacement handle and released to pool: 0x56396c2af710
2023-03-14 18:07:48.695495: I tensorflow/c/logging.cc:34] Request returned error. Attempting to generate appropriate error codes from response
2023-03-14 18:07:48.695504: E tensorflow/c/logging.cc:40] HTTP response code: -1
Resolved remote host IP address: 
Request ID: 
Exception name: 
Error message: curlCode: 35, SSL connect error
0 response headers:
2023-03-14 18:07:48.695516: W tensorflow/c/logging.cc:37] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.
2023-03-14 18:07:48.695521: I tensorflow/c/logging.cc:34] Date header was not found in the response, can't attempt to detect clock skew
2023-03-14 18:07:48.695527: W tensorflow/c/logging.cc:37] Request failed, now waiting 0 ms before attempting again.
2023-03-14 18:07:48.695595: I tensorflow/c/logging.cc:34] No content body, content-length headers
2023-03-14 18:07:48.695603: I tensorflow/c/logging.cc:34] Found credential in environment with access key id minio
2023-03-14 18:07:48.695609: I tensorflow/c/logging.cc:34] Found secret key
2023-03-14 18:07:48.695617: I tensorflow/c/logging.cc:34] Note: Http payloads are not being signed. signPayloads=0 http scheme=https
2023-03-14 18:07:48.695637: I tensorflow/c/logging.cc:34] Canonical Header String: amz-sdk-invocation-id:26CFFBE4-9F97-48D7-BC0A-FFA9613B9DC0
```
</details>"
59985,Using Teachable Machine to make a Audio model for Python,"I have been trying to make an AI model that will listen to user input. Depending on the user's input/command the program will carry out functions etc.

I am trying to use a model I have trained from the teachable machine, but the model is not in a format that can be used in Python. From what I have seen Python requires a .h5. However, TFLite.js is only available to export/download in. When I try to use this in my program I get hit with errors.

What steps can I take to import an Audio model for a Python system?"
59984,v2.11.0 build failure on linux/arm64,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf v2.11.0

### Custom Code

No

### OS Platform and Distribution

Linux arm64

### Mobile device

Raspberry Pi

### Python version

3.9.2

### Bazel version

5.3.0

### GCC/Compiler version

gcc (Debian 10.2.1-6) 10.2.1 20210110

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

```shell
Build from source is failing with following error:

INFO: Found 1 test target...
[0 / 1] [Prepa] BazelWorkspaceStatusAction stable-status.txt
[2,102 / 3,368] checking cached actions
[4,096 / 8,568] [Scann] Compiling tensorflow/compiler/xla/service/compilation_environments.cc
ERROR: /home/pi/tensorflow/tensorflow/cc/BUILD:775:22: Executing genrule //tensorflow/cc:resource_variable_ops_genrule failed: (Exit 127): bash failed: error executing command /bin/bash -c ... (remaining 1 argument skipped)
bazel-out/aarch64-opt/bin/tensorflow/cc/ops/resource_variable_ops_gen_cc: symbol lookup error: bazel-out/aarch64-opt/bin/tensorflow/cc/ops/resource_variable_ops_gen_cc: undefined symbol: _ZN4absl12lts_2022062318container_internal11kEmptyGroupE
Target //tensorflow/tools/lib_package:libtensorflow_test failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 30.190s, Critical Path: 1.80s
INFO: 11 processes: 8 internal, 3 local.
FAILED: Build did NOT complete successfully
//tensorflow/tools/lib_package:libtensorflow_test               FAILED TO BUILD

Executed 0 out of 1 test: 1 fails to build.
FAILED: Build did NOT complete successfully
```


### Standalone code to reproduce the issue

```shell
Run ./configure and accept all defaults
Run bazel test --config opt //tensorflow/tools/lib_package:libtensorflow_test

Note: v2.8.0 builds fine on the same platform using bazel 4.2.1
```


### Relevant log output

```shell
As shown above
```
</details>"
59983,TFlite model has bad performance,"Hi, I reduced .h5 model size using TFLiteConverter, but the tflite model has a bad accuracy and is not usable. [Here](https://colab.research.google.com/drive/14zMCm0XaszVrUoWd9bMvBjSXSdPvSWuw?userstoinvite=tirumaleshk%40google.com&actionButton=1#scrollTo=JWvOm1R5WyPs) is the code I used to reduce the model size. I wanted to know - is there something wrong with reducing the model size code or with the model architecture itself - that it is not possible to reduce the size for this specific model?
I also tested both - .h5 model and .tflite model on 400 images (good and bad quality), but the .tflite model evaluates good quality images the same way as bad quality images.
Here are MAD and RMSE scores:
.tflite vs .h5 (images that are in a good quality)
MAD: 1.8
RMSE: 1.9
tflite vs .h5 (same images but in a bad quality)
MAD: 0.3
RMSE: 0.4
[Here ](https://drive.google.com/drive/folders/1YcCSJgG6R4wRJ5qeU3gUoUd2296GghoL)are some examples. First MOS score in image name is .h5 result and second one is .tflite result.
Thank you in advance!"
59982,`nan` while doing inference in C++ with half precision,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

Debian GNU/Linux 10

### Mobile device

_No response_

### Python version

3.7

### Bazel version

6.1.0

### GCC/Compiler version

LLVM 12

### CUDA/cuDNN version

11.6

### GPU model and memory

Tesla T4

### Current Behaviour?

```shell
I've trained a model in python using mixed precision. However, when loading the model in C++ and trying to perform inference, The output is all `nan`.

I've tested loading the model in python and it works.

Some ideas might be:

- Toolchain is different.
- Tensors in C++ are not zero-initialized.
```


### Standalone code to reproduce the issue

```shell
Python


class Model(tf.keras.Model):
  def __init__(self):
    self.conv = tf.keras.layers.Conv2D(
        128,
        activation='relu')

  def call(self, x):
    return self.conv(x)

x = tf.convert_to_tensor(np.random.random(1, 8, 8))
model = Model()
model(x)
model.save('/tmp/model')
```

C++
```
using namespace ::tensorflow;

const static std::vector<std::string> kInputNames = {
    ""serving_default_args_0:0"",
};

const static std::vector<std::string> kOutputNames = {
    ""StatefulPartitionedCall:0"",
};

int main(int argc, char** argv) {
  SavedModelBundleLite model_;
  LoadSavedModel(SessionOptions(), RunOptions(), ""/tmp/model"",
                 {""serve""}, &model_bundle_);

  Tensor input_features(DataType::DT_FLOAT,
                        {1, 8, 8});
  std::vector<Tensor> cast_output;
  Status status =
      session.Run({ops::Cast(scope, input_features, DataType::DT_HALF),
                   ops::Cast(scope, input_state, DataType::DT_HALF)},
                  &cast_output);
  std::vector<std::pair<std::string, Tensor>>{
      std::make_pair(kInputNames[0], cast_output[0]),
  };

  std::vector<Tensor> output_buf = {
      Tensor(DataType::DT_HALF, {1, 6, 6}),
  };

  // this is nan
  model.GetSession()->Run(inputs, output_names, {}, output_buf);
}
```
```


### Relevant log output

```shell
2023-03-14 05:08:59.641765: I cc/scripts/play_model.cc:94] -nan
2023-03-14 05:08:59.641818: I cc/scripts/play_model.cc:95] -nan
2023-03-14 05:08:59.655568: I cc/scripts/play_model.cc:119] ------- Model Stats -------
2023-03-14 05:08:59.655609: I cc/scripts/play_model.cc:120] Top Move: Loc(0, 0)
2023-03-14 05:08:59.655621: I cc/scripts/play_model.cc:121] Win: -nan Loss: -nan
2023-03-14 05:08:59.655677: I cc/scripts/play_model.cc:123] -----Board-----
0  ○ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅
1  ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅
2  ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅
3  ⋅ ⋅ ⋅ + ⋅ ⋅ ⋅ ⋅ ⋅ + ⋅ ⋅ ⋅ ⋅ ⋅ + ⋅ ⋅ ⋅
4  ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅
5  ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅
6  ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅
7  ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅
8  ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅
9  ⋅ ⋅ ⋅ + ⋅ ⋅ ⋅ ⋅ ⋅ + ⋅ ⋅ ⋅ ⋅ ⋅ + ⋅ ⋅ ⋅
10 ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅
11 ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅
12 ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅
13 ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅
14 ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅
15 ⋅ ⋅ ⋅ + ⋅ ⋅ ⋅ ⋅ ⋅ + ⋅ ⋅ ⋅ ⋅ ⋅ + ⋅ ⋅ ⋅
16 ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅
17 ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅
18 ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅
   A B C D E F G H I J K L M N O P Q R S
```
</details>"
59981,TypeError: VariableMetaclass._variable_v1_call() got an unexpected keyword argument 'experimental_enable_variable_lifting',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

v2.12.0-rc0-46-g0d8efc960d2 2.12.0-rc1

### Custom Code

No

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
self.model = keras.Sequential([
                 ^^^^^^^^^^^^^^^^^^
  File ""C:\Python311\Lib\site-packages\tensorflow\python\trackable\base.py"", line 205, in _method_wrapper
    result = method(self, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Python311\Lib\site-packages\keras\utils\traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""C:\Python311\Lib\site-packages\tensorflow\python\ops\variables.py"", line 285, in __call__
    return cls._variable_v1_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: VariableMetaclass._variable_v1_call() got an unexpected keyword argument 'experimental_enable_variable_lifting'
```


### Standalone code to reproduce the issue

```shell
self.model = keras.Sequential([
    keras.layers.Dense(1, input_dim=self.degree),
    keras.layers.Dense(1)
    ])
self.model.compile(optimizer=optimizer, loss=loss)
self.model.summary()
```


### Relevant log output

_No response_</details>"
59980,"Example android app show ""GPU is not supportted"".","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf.2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Here are some details that will aid in this troubleshooting session:

Using the tensorflow lite sample android app(https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android).
But as the attached GPUNOTSUPPORT.png shown, currently on the 2290, when selecting GPU or NNAPI delegates the sample app shows ""GPU is not supported""

Look into the sample android app code.
1. As GPU.png shown, ""GPU is not supported"" occurred.
2. As the attached code1.png shown, the order call relationship.

The following code is very important:
public CompatibilityList() {
this.compatibilityListHandle = createCompatibilityList();
}
private static native long createCompatibilityList();
guss that createCompatibilityList() make problem.

Look into tensorflow code check createCompatibilityList:

1. In tensorflow/lite/delegates/gpu/java/src/main/native/compatibility_list_jni.cc +69
As the tensorflow.png shown,
75 // Errors in ReadInfo should almost always be failures to construct the OpenGL
76 // environment. Treating that as ""GPU unsupported"" is reasonable, and we can
77 // swallow the error

82 JNIEXPORT jboolean JNICALL
83 Java_org_tensorflow_lite_gpu_CompatibilityList_nativeIsDelegateSupportedOnThisDevice(
84 JNIEnv* env, jclass clazz, jlong compatibility_list_handle) {
85 if (!tflite::jni::CheckJniInitializedOrThrow(env)) return JNI_FALSE;
86
87 CompatibilityListHelper* compatibility_list =
88 reinterpret_cast<CompatibilityListHelper*>(compatibility_list_handle);
89 return compatibility_list->IsDelegateSupportedOnThisDevice() ? JNI_TRUE
90 : JNI_FALSE;
91 }

Thanks you very much.
```


### Standalone code to reproduce the issue

```shell
Using the tensorflow lite sample android app(https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android).
But as the attached GPUNOTSUPPORT.png shown, currently on the 2290, when selecting GPU or NNAPI delegates the sample app shows ""GPU is not supported""
```


### Relevant log output

_No response_</details>"
59979,tf.image.extract_patches fails to compute gradients,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tf.image.extract_patches fails to compute gradients. A similar [issue](https://github.com/tensorflow/tensorflow/issues/7414) says it can only compute gradients with `float32` inputs, but it doesn't work. According to the logs output, the error isn't related to the input `x` types.
```


### Standalone code to reproduce the issue

```shell
import os
import tensorflow as tf
import numpy as np

x = tf.Variable(tf.ones([1, 5, 5, 1]), tf.float32)

def func(x):
    patches = tf.image.extract_patches(x, [1, 3, 3, 1], [1, 1, 1, 1], [1, 1, 1, 1], 'VALID')
    return patches

with tf.GradientTape(persistent=True) as tape:
    tape.watch(x)
    output_rev = func(x)
    print(output_rev)

gradient = tape.jacobian(output_rev, x)
```


### Relevant log output

```shell
ERROR:tensorflow:Got error while pfor was converting op name: ""gradient_tape/Reshape_4""
op: ""Reshape""
input: ""gradient_tape/Reshape""
input: ""gradient_tape/Reshape_4/shape""
attr {
  key: ""T""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""Tshape""
  value {
    type: DT_INT64
  }
}
 with inputs (<tf.Tensor 'gradient_tape/Reshape:0' shape=(1, 3, 3, 9) dtype=float32>, <tf.Tensor 'gradient_tape/Reshape_4/shape:0' shape=(6,) dtype=int64>)
, converted inputs [WrappedTensor(t=<tf.Tensor 'gradient_tape/Reshape/pfor/Reshape:0' shape=(81, 1, 3, 3, 9) dtype=float32>, is_stacked=True, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'gradient_tape/Reshape_4/shape:0' shape=(6,) dtype=int64>, is_stacked=False, is_sparse_stacked=False)]
Here are the pfor conversion stack traces: Tensors in list passed to 'values' of 'ConcatV2' Op have types [int32, int64] that don't all match.
```
</details>"
59973,Tensorflow Lite Build issue - libtensorflow_framework.so.2: cannot open shared object file,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

Build for Android Arm

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
After running the tensroflow lite build tool, the build fails about ~5 hours. This is the command used:  `bash tensorflow/lite/tools/build_aar.sh --input_models=/host_dir/trainable_model.tflite --target_archs=arm64-v8a`
```


### Standalone code to reproduce the issue

```shell
`bash tensorflow/lite/tools/build_aar.sh --input_models=/host_dir/trainable_model.tflite --target_archs=arm64-v8a`

I modified the configs based on this issue to enable experimental mode: https://discuss.tensorflow.org/t/missing-interpreter-class-in-selective-build-of-tflite-android/15325/7
```


### Relevant log output

```shell
[9,650 / 13,797] 8 actions running
ERROR: /tensorflow_src/tensorflow/BUILD:1588:19: Executing genrule //tensorflow:tf_python_api_gen_v2 failed: (Exit 1): bash failed: error executing command /bin/bash -c ... (remaining 1 argument skipped)
2023-03-13 19:18:47.150108: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: libtensorflow_framework.so.2: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 22, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 36, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 77, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: libtensorflow_framework.so.2: cannot open shared object file: No such file or directory
```
</details>"
59970,armeabi-v7a assembler error,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

v2.12.0-rc1

### Custom Code

No

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

N/A

### Python version

N/A

### Bazel version

Using CMake

### GCC/Compiler version

Clang, NDK 25b

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


Building tensorflow lite (v2.12.0-rc1) for Android armeabi-v7a using CMake and NDK 25b, I get the following invalid assembly code error:

```shell
tflite-runtime/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/xnnpack/src/xnnpack/math.h:311:13: error: invalid output constraint \'=t\' in asm\n      : [i] ""=t"" (i)
```
The cause is here (a more recent freeze) https://github.com/google/XNNPACK/blob/test_515720556/src/xnnpack/math.h#L332

Android arm64-v8a builds and runs without error. With an earlier tensorflow lite version (v2.8.0) both armeabi-v7a and arm64-v8a built and ran without error.

As I read it `'=t'` is documented as a valid constraint for ""ARM family"", but the assembler thinks this is not the case.
https://gcc.gnu.org/onlinedocs/gcc/Simple-Constraints.html#Simple-Constraints
https://gcc.gnu.org/onlinedocs/gcc/Machine-Constraints.html#Machine-Constraints

Support at XNNPACK said a compiler flag `-mfpu=vfp` is required to enable the assembly code https://github.com/google/XNNPACK/issues/4348#issuecomment-1445437613 , and that the flag was set. Then suggested without reference that this was a Clang bug, and did not offer a workaround.

Further investigation suggested Clang was not the issue https://github.com/google/XNNPACK/issues/4348#issuecomment-1465060489

The CMake build script covers eight conditions for various arm 32 bit devices. Only two of these (both `-march=armv6`) set the required flag. The `-mfpu=vfp` flag is not set for `-march=armv7-a`, which I suspect is the cause of this issue. https://github.com/google/XNNPACK/blob/master/CMakeLists.txt#L546-L553

XNNPACK support responded, but we did not communicate successfully (as shown by https://github.com/google/XNNPACK/issues/4348#issuecomment-1465103944  and https://github.com/google/XNNPACK/issues/4348#issuecomment-1465259707) ; and we did not get a resolution. Since tflite depends on XNNPACK, I look for resolution here. Thank you.
```


### Standalone code to reproduce the issue

```shell
This is a build issue, no extra code.
```


### Relevant log output

```shell
tflite-runtime/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/xnnpack/src/xnnpack/math.h:311:13: error: invalid output constraint \'=t\' in asm\n      : [i] ""=t"" (i)
```
</details>"
59968,I don't get the label.txt when training the model.txt which causes me to fail to convert to tflite,"D:\Anaconda\lib\site-packages\tensorflow_lite_support\metadata\python\metadata.py:395: UserWarning: File, 'label.txt', does not exist in the metadata. But packing it to tflite model is still allowed.
  warnings.warn(


from tflite_support.metadata_writers import object_detector
from tflite_support.metadata_writers import writer_utils

ObjectDetectorWriter = object_detector.MetadataWriter

_MODEL_PATH = ""D:/android files/8_Car2022_Example_v1.1/app/src/main/assets/test2_mobilenet.tflite""
# Task Library expects label files that are in the same format as the one below.
_LABEL_FILE = ""D:/android files/8_Car2022_Example_v1.1/app/src/main/assets/label.txt""
_SAVE_TO_PATH = ""D:/android files/8_Car2022_Example_v1.1/app/src/main/assets/test_mobilenet_meta.tflite""
# Normalization parameters is required when reprocessing the image. It is
# optional if the image pixel values are in range of [0, 255] and the input
# tensor is quantized to uint8. See the introduction for normalization and
# quantization parameters below for more details.
# https://www.tensorflow.org/lite/models/convert/metadata#normalization_and_quantization_parameters
_INPUT_NORM_MEAN = 127.5
_INPUT_NORM_STD = 127.5

# Create the metadata writer.
writer = ObjectDetectorWriter.create_for_inference(
    writer_utils.load_file(_MODEL_PATH), [_INPUT_NORM_MEAN], [_INPUT_NORM_STD],
    [_LABEL_FILE])

# Verify the metadata generated by metadata writer.
print(writer.get_metadata_json())

# Populate the metadata into the model.
writer_utils.save_file(writer.populate(), _SAVE_TO_PATH)

"
59967,[tensorflow-intel] Missing p39 package for tf-intel 2.10.1 on PyPi,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.1

### Custom Code

No

### OS Platform and Distribution

Windows

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
As identified as part of my poetry package management, the PyPi server is currently lacking a wheel for tensorflow-intel==2.10.1. I also checked ""front-end wise"" on the followng URL: https://pypi.org/project/tensorflow-intel/2.10.1/#files

I would like to use in particular that version for tensorflow, as it is the last one with Windows-native GPU support. It is available for all other Python versions, but not 3.9.
```


### Standalone code to reproduce the issue

```shell
$ conda create -n tf39 python=3.9
$ conda activate tf39
$ pip install tensorflow-intel==2.10.1
```


### Relevant log output

```shell
ERROR: Could not find a version that satisfies the requirement tensorflow-intel==2.10.1 (from versions: 0.0.1, 2.10.0.dev20220728, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.12.0rc0, 2.12.0rc1)
ERROR: No matching distribution found for tensorflow-intel==2.10.1
```
</details>"
59966,Tensorflow  Does Not Seem to Work With Latest cuDNN 8.8 Version,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.2, cuDNN 8.8

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Note: I am using a custom docker container that is based on an older `tensorflow/tensorflow:latest-gpu` image. This image is Ubuntu 20.4 with tf 2.8 and CUDA 11.2. My actual computer running the container is Ubuntu 18.04 with a Quadro RTX 8000 and NVIDIA driver 460.106.0.

I recently upgraded packages in my docker container, and it seems that the newest version of libcudnn8, being cudNN 8.8, conflicts with Tensorflow 2.8 (possibly other versions too, but I can only speak for TF 2.8). I am able to load one of my pre-trained Conv2d-based models just fine, but actually using the model (for both training and prediction) causes trouble. 

As soon as I try to run any data through my model, TF cannot create the cudnn handle and suggests that I have an insufficient driver version. I tried updating the driver version, but that newer version requires CUDA 12.0, which TF is not compatible with. After many hours of trial and error, it seems that if I do NOT upgrade the libcudnn8 library in my docker image, and keep it as the original version of 8.1, then I do not have any problems.

Since someone else reported an issue with cuDNN 8.8 as well on a different matter (https://github.com/tensorflow/tensorflow/issues/59890), it would seem that TF 2 is not compatible with this newer version.
```


### Standalone code to reproduce the issue

```shell
Any piece of code that passes data through a tf.model should result in an error (especially with conv2d layers, since that's what my model uses).
```


### Relevant log output

```shell
2023-03-12 17:40:03.421988: E tensorflow/stream_executor/cuda/cuda_dnn.cc:373] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED
2023-03-12 17:40:03.422120: E tensorflow/stream_executor/cuda/cuda_dnn.cc:382] Possibly insufficient driver version: 460.106.0
2023-03-12 17:40:03.422182: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at conv_ops.cc:1120 : UNIMPLEMENTED: DNN library is not found.
Traceback (most recent call last):
  File ""semantic_segmentation.py"", line 378, in <module>
    SemSeg.train()
  File ""semantic_segmentation.py"", line 231, in train
    loss = self._train_step(image_batch, mask_batch)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 54, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.UnimplementedError: Graph execution error:

Detected at node 'model/conv2d/Conv2D' defined at (most recent call last):
    File ""semantic_segmentation.py"", line 378, in <module>
      SemSeg.train()
    File ""semantic_segmentation.py"", line 231, in train
      loss = self._train_step(image_batch, mask_batch)
    File ""semantic_segmentation.py"", line 213, in _train_step
      predicted_probs = self.model(images, training=True)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1096, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 92, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 451, in call
      return self._run_internal_graph(
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 589, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1096, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 92, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/convolutional.py"", line 248, in call
      outputs = self.convolution_op(inputs, self.kernel)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/convolutional.py"", line 233, in convolution_op
      return tf.nn.convolution(
Node: 'model/conv2d/Conv2D'
DNN library is not found.
         [[{{node model/conv2d/Conv2D}}]] [Op:__inference__train_step_1535]
```
</details>"
59965,Can't convert openimages_v4/ssd/mobilenet_v2,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux 
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.11.0

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

I follow the code to convert a saved model to tflite from tensorflow website here : https://www.tensorflow.org/lite/models/convert/convert_models#convert_a_savedmodel_recommended_

The model I want to convert is this one : 
https://tfhub.dev/google/openimages_v4/ssd/mobilenet_v2/1

I would like to use it in the android example of tensorflow lite: 

https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

The conversion is unsuccessful.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```

2023-03-12 19:16:36.621905: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-12 19:16:36.719912: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2023-03-12 19:16:36.719933: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2023-03-12 19:16:37.381249: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2023-03-12 19:16:37.381323: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2023-03-12 19:16:37.381337: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Traceback (most recent call last):
File ""/home/chauxb/Downloads/tensorflow/convert.py"", line 4, in <module>
    converter = tf.lite.TFLiteConverter.from_saved_model(""/home/chauxb/Downloads/tensorflow/openimages/"") # path to the SavedModel directory
  File ""/home/chauxb/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py"", line 1792, in from_saved_model
    saved_model = _load(saved_model_dir, tags)
  File ""/home/chauxb/.local/lib/python3.10/site-packages/tensorflow/python/saved_model/load.py"", line 828, in load
    result = load_partial(export_dir, None, tags, options)[""root""]
  File ""/home/chauxb/.local/lib/python3.10/site-packages/tensorflow/python/saved_model/load.py"", line 977, in load_partial
    root = load_v1_in_v2.load(export_dir, tags)
  File ""/home/chauxb/.local/lib/python3.10/site-packages/tensorflow/python/saved_model/load_v1_in_v2.py"", line 284, in load
    result = loader.load(tags=tags)
  File ""/home/chauxb/.local/lib/python3.10/site-packages/tensorflow/python/saved_model/load_v1_in_v2.py"", line 209, in load
    meta_graph_def = self.get_meta_graph_def_from_tags(tags)
  File ""/home/chauxb/.local/lib/python3.10/site-packages/tensorflow/python/saved_model/load_v1_in_v2.py"", line 88, in get_meta_graph_def_from_tags
    return super(_EagerSavedModelLoader, self).get_meta_graph_def_from_tags(
  File ""/home/chauxb/.local/lib/python3.10/site-packages/tensorflow/python/saved_model/loader_impl.py"", line 391, in get_meta_graph_def_from_tags
    raise RuntimeError(
RuntimeError: MetaGraphDef associated with tags {'serve'} could not be found in SavedModel, with available tags '[set()]'. To inspect available tag-sets in the SavedModel, please use the SavedModel CLI: `saved_model_cli`.

```

Am I missing something in my conversion?
I am new to this so maybe I am doing something wrong (first time doing a model conversion)?

Thank you for your help in advance "
59963,gpu/cpu time performance,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.10

### Custom Code

No

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

cuDNN = 8.1.0.77

### GPU model and memory

nVidia GTX 1050, 4GB GRAM

### Current Behaviour?

```shell
I have noticed that training on cpu faster than gpu. Note that I have tried same on RTX 3090 TI and got similar results 

GPU performance:

2023-03-12 13:45:41.870315: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-12 13:45:42.381082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2787 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1
WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.
Epoch 1/100
3125/3125 [==============================] - 10s 3ms/step - loss: 0.0495
Epoch 2/100
3125/3125 [==============================] - 10s 3ms/step - loss: 0.0014
Epoch 3/100
3125/3125 [==============================] - 10s 3ms/step - loss: 0.0013
Epoch 4/100
3125/3125 [==============================] - 11s 4ms/step - loss: 0.0013
Epoch 5/100
3125/3125 [==============================] - 11s 4ms/step - loss: 0.0012
Epoch 6/100
3125/3125 [==============================] - 11s 4ms/step - loss: 0.0011
```

CPU performance:
```
2023-03-12 13:48:54.873454: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.
Epoch 1/100
3125/3125 [==============================] - 4s 1ms/step - loss: 0.0511
Epoch 2/100
3125/3125 [==============================] - 4s 1ms/step - loss: 0.0019
Epoch 3/100
3125/3125 [==============================] - 4s 1ms/step - loss: 0.0013
Epoch 4/100
3125/3125 [==============================] - 4s 1ms/step - loss: 0.0013
Epoch 5/100
3125/3125 [==============================] - 4s 1ms/step - loss: 0.0013
Epoch 6/100
3125/3125 [==============================] - 4s 1ms/step - loss: 0.0012
```

### Standalone code to reproduce the issue

```
import numpy as np
from tensorflow.keras.utils import to_categorical
from tensorflow.python.keras.layers import Dense, Input
from tensorflow.python.keras.models import Model, Sequential

def build_std_func_model(n_inputs):
    inp = Input(n_inputs)
    h = Dense(100, activation=""relu"")(inp)
    h = Dense(100, activation=""relu"")(h)
    h = Dense(100, activation=""relu"")(h)
    h = Dense(100, activation=""relu"")(h)
    output = Dense(n_inputs, activation=""linear"")(h)
    return Model(inputs=inp, outputs=output)

def build_std_seq_model(n_inputs):
    model = Sequential()

    model.add(Input(n_inputs))
    model.add(Dense(100, activation=""relu""))
    model.add(Dense(100, activation=""relu""))
    model.add(Dense(100, activation=""relu""))
    model.add(Dense(100, activation=""relu""))
    model.add(Dense(n_inputs, activation=""linear""))
    return model

def make_func_dataset(n_examples=30000, n_linears=15, n_categorical=4):
    linear = np.random.randn(n_examples, n_linears)
    categorical = [np.random.choice([0, 1, 2], size=n_examples) for _ in range(n_categorical)]
    categorical = [to_categorical(item) for item in categorical]
    return [linear, *categorical]

def run_simplest():
    X = make_func_dataset(n_examples=10**5, n_linears=30, n_categorical=0)[0]
    # model = build_std_func_model(30)
    model = build_std_seq_model(30)
    model.compile(optimizer=""adam"", loss=""mean_squared_error"")
    model.fit(X, X, epochs=100)

if __name__ == '__main__':
    # tf.config.set_visible_devices([], 'GPU')
    # run_functional()
    run_simplest()
```


### Relevant log output

_No response_</details>"
59961,Tensorflow optimizer.apply_gradients is very slow.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.13

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I'm using a custom training loop for training a model made using EfficientNetV2 with biFPN. I found that `optimizer.apply_gradients` was running very slow. It took around 1.7 seconds to `apply_gradients` while model was taking only 0.1 seconds. I tried to replicate this using smaller example, so followed tutorial [here](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch). I found a similar issue here also.  `apply_gradients` here also is taking twice the time compared to model+loss. Is it fine to have such behavior? I tried a different version of tensorflow, but found the same behavior.
```


### Standalone code to reproduce the issue

```shell
The code used can be found here:
https://colab.research.google.com/github/tensorflow/docs/blob/snapshot-keras/site/en/guide/keras/writing_a_training_loop_from_scratch.ipynb
```


### Relevant log output

```shell
Model+Loss  0.027350187301635742
Grad  0.027795791625976562
Apply Grad  0.056526899337768555
```
</details>"
59960,quantize_and_dequantize_v2 crashes when computing gradients in forward mode,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
quantize_and_dequantize_v2 crashes when computing gradients in forward mode. According to the error message, `quantize_and_dequantize_v2` seems to link with the wrong version of the operator.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

tensor = tf.random.normal(shape=[1, 1], mean=127, stddev=50, dtype=tf.float32)

def func(tensor):
    quanti = tf.quantization.quantize_and_dequantize_v2(tensor, 
                                                        input_min = 0.0,
                                                        input_max = 255.0,
                                                        num_bits = 8,
                                                        range_given=False)
    
    return quanti

output = func(tensor) # pass
with tf.autodiff.ForwardAccumulator(tensor, tf.constant([[0.], [1.]])) as acc:
    output = func(tensor)
```


### Relevant log output

```shell
TypeError: _QuantizeAndDequantizeV4GradGrad() takes 2 positional arguments but 4 were given
```
</details>"
59959,Tensorflow lite for microcontrollers documentation has broken links to .cc files,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Documentation links to .cc files are broken.
https://www.tensorflow.org/lite/microcontrollers/get_started_low_level

In section 'Run Inference', link to 'hello_world_test.cc' and the other remaining .cc files linked cannot be opened.
```


### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

_No response_</details>"
59958,Build issue by way of third party benchmark from head,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

Head

### Custom Code

No

### OS Platform and Distribution

Linux, Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

The OSS-Fuzz build started failing a few days ago: https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=56922

The bazel log shows:
```shell
Step #3 - ""compile-libfuzzer-address-x86_64"": ERROR: /src/tensorflow/tensorflow/tsl/platform/BUILD:335:11: no such package '@libpfm//': The repository '@libpfm' could not be resolved: Repository '@libpfm' is not defined and referenced by '//tensorflow/tsl/platform:test_benchmark'
```

If I revert commit https://github.com/tensorflow/tensorflow/commit/03b51b52df3a09c39585ac3fe0227606f414651e the build works well again.

### Standalone code to reproduce the issue

```shell
Build by way of OSS-Fuzz [build script](https://github.com/google/oss-fuzz/blob/master/projects/tensorflow/build.sh):


git clone https://github.com/google/oss-fuzz
cd oss-fuzz
python3 infra/helper.py build_fuzzers tensorflow
```
```


### Relevant log output

_No response_</details>"
59957,fvr,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

dewxc

### Custom Code

Yes

### OS Platform and Distribution

sdac

### Mobile device

sx

### Python version

c

### Bazel version

xsxd

### GCC/Compiler version

scdx

### CUDA/cuDNN version

dc

### GPU model and memory

wsc

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
sdacv
```


### Relevant log output

```shell
recfdssdc
```
</details>"
59956,"SavedModel in C++ with Multiple Signatures gives ""You must feed a value for placeholder tensor 'infer_float_board_state' with dtype float and shape [?,19,19,7]""","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

Debian GNU/Linux 10

### Mobile device

n/a

### Python version

3.9

### Bazel version

6.1.0

### GCC/Compiler version

LLVM 12

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

```shell
I have a Keras model that I save with 2 signatures--one for full precision, and one for mixed precision.

After loading in C++ and trying to run, I get this error:


You must feed a value for placeholder tensor 'infer_float_board_state' with dtype float and shape [?,19,19,7]
```

I see a method `GetSignatureDefs` in `SavedModelBundleLite`, but no way to actually use them. How can I call the signature that I actually need?
```


### Standalone code to reproduce the issue

```shell
Python


class Model(tf.keras.Model):
  def __init__(self): super(Model, self).__init__()
  def call(self, x):
    return x + 1.0

  @tf.function(input_signature=[
      tf.TensorSpec([None, 1], tf.float32),
  ])
  def sig_a(self, x):
    return self(x)

  @tf.function(input_signature=[
      tf.TensorSpec([None, 1], tf.float16),
  ])
  def sig_a(self, x):
    return self(x)


model.save(local_path,
                     signatures={
                         'sig_a': model.sig_a,
                         'sig_b': model.sig_b
                     })
```

C++

```
const static std::vector<std::string> kInputNames = {
    ""sig_a_x:0""
};

const static std::vector<std::string> kPlaceHolderNames = {
    ""sig_b_x:0""
};

const static std::vector<std::string> kOutputNames = {
    ""StatefulPartitionedCall:0"",
};

int main(int argc, char** argv) {
  model = LoadSavedModel(session_options_, run_options_, path,
                        {tensorflow::kSavedModelTagServe}, &model_bundle_);
  
  model.GetSession()->Run(<inputs>, <outputs>, <targets>, <output>);
}
```
```


### Relevant log output

```shell
2023-03-10 21:44:24.929526: E cc/scripts/play_model.cc:91] Eval Failed: 3, msg: You must feed a value for placeholder tensor 'infer_float_board_state' with dtype float and shape [?,19,19,7]
	 [[{{node infer_float_board_state}}]]
```
</details>"
59954,"Cannot save model when not giving a name in Layer.add_weight call in ""build""","I have the identical issue to https://github.com/tensorflow/tensorflow/issues/36962, except I'm on 2.11.0.  Was it really resolved?

The issue is that the model will not save and returns: `AttributeError: 'NoneType' object has no attribute 'replace'`

It would have taken a very long time to run this down if I hadn't found the linked issue, but I also have `self.add_weight` in a custom layer with no name."
59950,tf.io.FixedLenSequenceFeature shows error when padding value for unformant serialized features,"I got an error when deserialize the data like the following code.

the sequence data would be like 

```
feature_list {
    key: ""f1""
    value {
      feature {
        int64_list {
          value: 1
          value: 2
        }
      }
      feature {
        int64_list {
          value: 1
          value: 0
          value: 2
        }
      }
    }
```

I want to use ```tf.io.FixedLenSequenceFeature``` to fit sequence data into a dense tensor with padded value -1.

```python
import tensorflow as tf

if __name__ == '__main__':
  sequence_feature_names = ""f1""
  featurelists = {}
  x = [[1,2], [1, 0, 2]]
  feature_list = tf.train.FeatureList(
    feature=[tf.train.Feature(int64_list=tf.train.Int64List(value=x_)) for x_ in x])
  featurelists[sequence_feature_names] = feature_list
  featurelists_features = tf.train.FeatureLists(feature_list=featurelists)
  sequence_example = tf.train.SequenceExample(
    context=None,
    feature_lists=featurelists_features
  )

  print(sequence_example)
  data = sequence_example.SerializeToString()
  des_data = tf.io.parse_single_sequence_example(data,
                                          context_features={},
                                          sequence_features={
                                            # 'sequence_feature_1': tf.io.VarLenFeature(dtype='int64'),
                                            'f1': tf.io.FixedLenSequenceFeature(shape=[3], dtype=tf.int64,
                                                                                allow_missing=True, default_value=-1
                                                                                  )
                                          }, )
  print(des_data)
  print(tf.sparse.to_dense(des_data[1]['sequence_feature_1']))

```

The traceback is like:
```
Traceback (most recent call last):
  File ""D:/cloud-service-ai/dev/fc-dev/honor-feature-columns/tests/parse_seq_example.py"", line 28, in <module>
    des_data = tf.io.parse_single_sequence_example(data,
  File ""D:\Libraries\Anaconda3\envs\tf2py38\lib\site-packages\tensorflow\python\util\traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""D:\Libraries\Anaconda3\envs\tf2py38\lib\site-packages\tensorflow\python\eager\execute.py"", line 54, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__ParseSequenceExampleV2_Ncontext_sparse_0_Nfeature_list_dense_1_Nfeature_list_sparse_0_Tcontext_dense_0_context_ragged_split_types_0_context_ragged_value_types_0_context_sparse_types_0_feature_list_dense_types_1_feature_list_ragged_split_types_0_feature_list_ragged_value_types_0_feature_list_sparse_types_0_device_/job:localhost/replica:0/task:0/device:CPU:0}} Name: <unknown>, Key: sequence_feature_1, Index: 0.  Number of values != expected.  values size: 2 but output shape: [] [Op:ParseSequenceExampleV2]

```

···"
59948,Why is the graph for the 'TFOpLambda' layer's 'call' method not stored in the SavedModel object_graph_def? ,"### Describe the problem

I work on importing TensorFlow models into MATLAB, this includes de-serializing the SavedModel format, identifying the different tf.keras.layers present in the SavedModel and creating an equivalent deep learning network in MATLAB. 

In TensorFlow 2.3.0 and earlier , if we use a TF symbol (like: tf.nn.relu()) between tf.keras.layers instances, it was serialized as a 'TensorFlowOpLayer', that was similar to a layer subclassing tf.keras.layer, i.e., the graph for its 'call' method (call_and_return_conditional_losses) was stored in the SavedModel. Specifically, this 'call_and_return_conditional_losses' function was stored as a child of the node corresponding to the TensorFlowOpLayer in the SavedModel's object_graph_def. 

In TensorFlow 2.6.0 and later, a TF symbol used between tf.keras.layers instances is serialized as a 'TFOpLambda' layer. Saving models containing these TFOpLambda layers into a SavedModel does not serialize the graph for its 'call' method (call_and_return_conditional_losses) anymore. There is no child node of the TFOpLambda node in the SavedModel's object_graph_def that corresponds to the 'call_and_return_conditional_losses' function anymore. 

This creates a problem for me since I rely on decoding the 'call_and_return_conditional_losses' function, in order to import these TensorFlowOpLayer / TFOpLambda into MATLAB. Consider the following model as an example:

```
x = tf.keras.layers.Input(shape=[None, 1])
z = tf.keras.layers.Conv1D(32, kernel_size=2, padding=""causal"")(x)
z = tf.nn.relu(z)  
model = tf.keras.models.Model(inputs=[x], outputs=[z])
model.summary()
model.save('ModelWithTFSymbol')
```

Saving this in TensorFlow 2.3.0 will give you a TensorFlowOpLayer that has its 'call' graph serialized, containing the tf.raw_ops.Relu node that I use to concretely identify this as a ReLU activation operation. 
Model: ""functional_1""
```
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, None, 1)]         0         
                                                                 
 conv1d (Conv1D)             (None, None, 32)          96        
                                                                 
 tf_op_layer_Relu (TensorFlo  (None, None, 32)         0         
 wOpLayer)                                                       
                                                                 
=================================================================
```

Saving the same model in TensorFlow 2.6.0+ will give you a TFOpLambda layer for the 'tf.nn.relu' symbol, that does not have its 'call' graph serialized in the object_graph_def. 
Model: ""model_9""
```
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_13 (InputLayer)       [(None, None, None, 1)]   0         
                                                                 
 conv1d_13 (Conv1D)          (None, None, None, 32)    96        
                                                                 
 tf.nn.relu_2 (TFOpLambda)   (None, None, None, 32)    0         
                                                                 
=================================================================
```

The most information we have in this case is in the TFOpLambda node's metadata (meta_graphs > object_graph_def > TFOpLambda node >user_object > metadata) as follows:

```
{
	""name"": ""tf.nn.relu_13"",
	""trainable"": true,
	""expects_training_arg"": false,
	""dtype"": ""float32"",
	""batch_input_shape"": null,
	""stateful"": false,
	""must_restore_from_config"": true,
	""class_name"": ""TFOpLambda"",
	""config"": {
		""name"": ""tf.nn.relu_13"",
		""trainable"": true,
		""dtype"": ""float32"",
		""function"": ""nn.relu""
	},
	""inbound_nodes"": [
		[
			""conv1d_100"",
			0,
			0,
			{
				""name"": null
			}
		]
	],
	""shared_object_id"": 4
}

```
I have the following questions regarding TFOpLambda layers:

1. Is there a way to always save the 'call' method graph in SavedModels for TFOpLambda layers? Basically a way to keep the older TensorFlowOpLayer behavior in newer versions of TensorFlow?
2. What is the benefit of saving TF Symbols as TFOpLambda layers over the original TensorFlowOpLayers?

"
59946,Fails to build tf-opt with llvm-project repository override,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.9

### Custom Code

Yes

### OS Platform and Distribution

mac(M2)

### Mobile device

_No response_

### Python version

3.9.16

### Bazel version

5.3.0

### GCC/Compiler version

15.0.5 (llvm version)

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I want to build the tf-opt and tf-mlir-translate

I followed the steps given in the [readme](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler/mlir), and roll back the version of llvm-project to [corresponding version](https://github.com/tensorflow/tensorflow/tree/master/third_party/llvm).

I got Error output below:

when build tf-opt:
cannot load '@llvm-project//:vars.bzl': no such file and referenced by '@llvm-project//mlir:Transforms'

when build tf-mlir-translate:
 cannot load '@llvm-project//:vars.bzl': no such file and referenced by '//tensorflow/compiler/mlir:tf-mlir-translate'
```


### Standalone code to reproduce the issue

```shell
LLVM_SRC=/Volumes/back/mlir_project/llvm-project
LLVM_BAZEL_OVERLAY=${LLVM_SRC}/bazel # Note: this can be anywhere
mkdir -p ${LLVM_BAZEL_OVERLAY}
# This will symlink your LLVM sources with the BUILD files to be usable by Bazel.
python ${LLVM_SRC}/utils/bazel/overlay_directories.py \
    --src ${LLVM_SRC} \
    --overlay ${LLVM_SRC}/utils/bazel/llvm-project-overlay/ \
    --target ${LLVM_BAZEL_OVERLAY}
touch ${LLVM_BAZEL_OVERLAY}/BUILD.bazel ${LLVM_BAZEL_OVERLAY}/WORKSPACE
echo 'llvm_targets = [""X86"",""NVPTX"",""AMDGPU"",""AArch64"",""ARM""]' > ${LLVM_BAZEL_OVERLAY}/llvm/targets.bzl
```


### Relevant log output

```shell
ERROR: /private/var/tmp/_bazel_isolateya/af219aaee912b96bcf0876417404d886/external/llvm-project/mlir/BUILD.bazel:5904:11: error loading package '@llvm-project//llvm': at /private/var/tmp/_bazel_isolateya/af219aaee912b96bcf0876417404d886/external/llvm-project/llvm/config.bzl:8:5: cannot load '@llvm-project//:vars.bzl': no such file and referenced by '@llvm-project//mlir:Transforms'
ERROR: Analysis of target '//tensorflow/compiler/mlir:tf-opt' failed; build aborted: 
INFO: Elapsed time: 38.684s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (122 packages loaded, 463 targets \
configured)
    currently loading: @com_google_protobuf// ... (4 packages)
    Fetching https://storage.googleapis.com/.../nsync/archive/1.25.0.tar.gz
    Fetching https://storage.googleapis.com/.../archive/v2.0.6.tar.gz
    Fetching https://storage.googleapis.com/...65082ee350509af1d113344d.tar.gz

```shell
ERROR: /Volumes/back/mlir_project/tensorflow/tensorflow/compiler/mlir/BUILD:219:13: error loading package '@llvm-project//llvm': at /private/var/tmp/_bazel_isolateya/af219aaee912b96bcf0876417404d886/external/llvm-project/llvm/config.bzl:8:5: cannot load '@llvm-project//:vars.bzl': no such file and referenced by '//tensorflow/compiler/mlir:tf-mlir-translate'
ERROR: Analysis of target '//tensorflow/compiler/mlir:tf-mlir-translate' failed; build aborted: Analysis failed
INFO: Elapsed time: 27.395s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (54 packages loaded, 247 targets configured)
    currently loading: @llvm-project//llvm
    Fetching @local_config_cc; Building xcode-locator
    Fetching https://storage.googleapis.com/.../abseil-cpp/archive/273292d1cfc0a94a65082ee350509af1d113344d.tar.gz
```
```
</details>"
59944,Movie recommender model doesn't converge,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.8

### Custom Code

Yes

### OS Platform and Distribution

Windows

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I had started to train my Movie Recommender System (content based filtering) in order to guess user ratings for certain movies. However, my loss is incrediably high(7.3) my accuracy is very low(0.03) and the code barely improves- going from 7.357 loss to 7.355 in 50 epochs or so. I have no idea how to fix this problem and I tried many things including adding dropout layers, more layers, changing output and so. I think my problem might be related to the embedding layer?
note:
num_item_features = 21 , and the movies vector is a list of all the attributes of a certain movie. users vector is used in embedding.
```


### Standalone code to reproduce the issue

```shell
num_outputs = 32 #hyperparameter
K= 32 #hyperparameter, size of embedding vector
tf.random.set_seed(1)
user_NN = tf.keras.models.Sequential([
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(.2, ),
    tf.keras.layers.Dense(num_outputs, activation='linear'),
  
])

item_NN = tf.keras.models.Sequential([
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(.2, ),
    tf.keras.layers.Dense(num_outputs, activation='linear'), 
])

# create the user input and point to the base network
input_user = tf.keras.layers.Input(shape=(1,)) 
u_emb = Embedding(N, K)(input_user) # output is (num_samples, 1, K)
u_emb = Flatten()(u_emb)  # now it's (num_samples, K)

vu = user_NN(u_emb)
vu = tf.linalg.l2_normalize(vu, axis=1)

# create the item input and point to the base network
input_item = tf.keras.layers.Input(shape=(num_item_features))
vm = item_NN(input_item)
vm = tf.linalg.l2_normalize(vm, axis=1) 

# compute the dot product of the two vectors vu and vm
output = tf.keras.layers.Dot(axes=1)([vu, vm])

# specify the inputs and output of the model
model = tf.keras.Model([input_user, input_item], output)

model.summary()


history = model.fit([users, movies], ratings,callbacks=[callback], epochs=1000,batch_size=256,verbose=1,shuffle=True)


num_item_features=movies.shape[1]
users=df3['userId'].to_numpy()
ratings=df3['rating'].to_numpy()
```


### Relevant log output

```shell
Epoch 1/1000
511/511 [==============================] - 8s 12ms/step - loss: 7.5963 - accuracy: 0.0308
Epoch 2/1000
511/511 [==============================] - 8s 16ms/step - loss: 7.5560 - accuracy: 0.0311
Epoch 3/1000
511/511 [==============================] - 6s 12ms/step - loss: 7.5555 - accuracy: 0.0311
Epoch 4/1000
511/511 [==============================] - 8s 15ms/step - loss: 7.5554 - accuracy: 0.0311
Epoch 5/1000
511/511 [==============================] - 6s 13ms/step - loss: 7.5554 - accuracy: 0.0311
Epoch 6/1000
511/511 [==============================] - 7s 14ms/step - loss: 7.5554 - accuracy: 0.0311
Epoch 7/1000
511/511 [==============================] - 7s 13ms/step - loss: 7.5554 - accuracy: 0.0311
Epoch 8/1000
511/511 [==============================] - 7s 13ms/step - loss: 7.5554 - accuracy: 0.0311
Epoch 9/1000
511/511 [==============================] - 8s 15ms/step - loss: 7.5554 - accuracy: 0.0311
Epoch 10/1000
511/511 [==============================] - 6s 12ms/step - loss: 7.5554 - accuracy: 0.0311
Epoch 11/1000
511/511 [==============================] - 8s 16ms/step - loss: 7.5554 - accuracy: 0.0311
Epoch 12/1000
511/511 [==============================] - 6s 12ms/step - loss: 7.5554 - accuracy: 0.0311
Epoch 13/1000
511/511 [==============================] - 8s 16ms/step - loss: 7.5554 - accuracy: 0.0311
Epoch 14/1000
511/511 [==============================] - 9s 17ms/step - loss: 7.5554 - accuracy: 0.0311
Epoch 15/1000
511/511 [==============================] - 8s 15ms/step - loss: 7.5554 - accuracy: 0.0311
Epoch 16/1000
511/511 [==============================] - 6s 12ms/step - loss: 7.5554 - accuracy: 0.0311
```
</details>"
59943,Errors building Tensorflow from source,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

No

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.10.10

### Bazel version

5.3.0

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am trying to build Tensorflow from source, so i can use AVX, AVX2 and so on.

I typed this command, expecting to build Tensorflow with all supported instruction-sets, my pc supports:

bazel build --config=opt --copt=-march=native //tensorflow/tools/pip_package:build_pip_package
```


### Standalone code to reproduce the issue

```shell
I get these errors:

ERROR: An error occurred during the fetch of repository 'llvm-project':
   Traceback (most recent call last):
        File ""C:/tmp/hn4nqluc/external/llvm-raw/utils/bazel/configure.bzl"", line 146, column 25, in _llvm_configure_impl
                _overlay_directories(repository_ctx)
        File ""C:/tmp/hn4nqluc/external/llvm-raw/utils/bazel/configure.bzl"", line 49, column 13, in _overlay_directories
                fail(""Failed to find python3 binary"")
Error in fail: Failed to find python3 binary



ERROR: C:/users/micla/code/python/diceml/tensorflow/WORKSPACE:15:14: fetching llvm_configure rule //external:llvm-project: Traceback (most recent call last):
        File ""C:/tmp/hn4nqluc/external/llvm-raw/utils/bazel/configure.bzl"", line 146, column 25, in _llvm_configure_impl
                _overlay_directories(repository_ctx)
        File ""C:/tmp/hn4nqluc/external/llvm-raw/utils/bazel/configure.bzl"", line 49, column 13, in _overlay_directories
                fail(""Failed to find python3 binary"")
Error in fail: Failed to find python3 binary


ERROR: C:/users/micla/code/python/diceml/tensorflow/tensorflow/tools/pip_package/BUILD:276:10: //tensorflow/tools/pip_package:build_pip_package depends on //tensorflow/compiler/mlir/tensorflow:gen_mlir_passthrough_op_py in repository @ which failed to fetch. no such package '@llvm-project//mlir': Failed to find python3 binary


ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed


FAILED: Build did NOT complete successfully (51 packages loaded, 12 targets config\
ured)
    currently loading: tensorflow/lite/tools ... (3 packages)
    Fetching https://storage.googleapis.com/.../flatbuffers/archive/v2.0.6.tar.gz
```


### Relevant log output

```shell
INFO: Reading 'startup' options from c:\users\micla\code\python\diceml\tensorflow\.bazelrc: --output_user_root=C:/tmp, --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=84
INFO: Reading rc options for 'build' from c:\users\micla\code\python\diceml\tensorflow\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/Users/micla/AppData/Local/Microsoft/WindowsApps/python.exe
INFO: Reading rc options for 'build' from c:\users\micla\code\python\diceml\tensorflow\.bazelrc:
  'build' options: --enable_runfiles --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from c:\users\micla\code\python\diceml\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/micla/AppData/Local/Programs/Python/Python310/python.exe --action_env PYTHON_LIB_PATH=C:/Users/micla/AppData/Local/Programs/Python/Python310/lib/site-packages --python_path=C:/Users/micla/AppData/Local/Programs/Python/Python310/python.exe --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true
INFO: Reading rc options for 'build' from c:\users\micla\code\python\diceml\tensorflow\.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file c:\users\micla\code\python\diceml\tensorflow\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file c:\users\micla\code\python\diceml\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:opt in file c:\users\micla\code\python\diceml\tensorflow\.tf_configure.bazelrc: --copt=/arch:AVX2 --host_copt=/arch:AVX2
INFO: Found applicable config definition build:windows in file c:\users\micla\code\python\diceml\tensorflow\.bazelrc: --copt=/W0 --host_copt=/W0 --copt=/Zc:__cplusplus --host_copt=/Zc:__cplusplus --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --features=compiler_param_file --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --cxxopt=/std:c++17 --host_cxxopt=/std:c++17 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/Zc:preprocessor --host_copt=/Zc:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file c:\users\micla\code\python\diceml\tensorflow\.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/85488e9aae621ec5175b94fc5e31e41b7e5e06ac.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/65f68812d399930d4af587c7a2333e46f367c5a7.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
INFO: Repository llvm-project instantiated at:
  C:/users/micla/code/python/diceml/tensorflow/WORKSPACE:15:14: in <toplevel>
  C:/users/micla/code/python/diceml/tensorflow/tensorflow/workspace2.bzl:975:21: in workspace
  C:/users/micla/code/python/diceml/tensorflow/tensorflow/workspace2.bzl:539:15: in _tf_repositories
  C:/users/micla/code/python/diceml/tensorflow/third_party/llvm/setup.bzl:22:19: in llvm_setup
Repository rule llvm_configure defined at:
  C:/tmp/hn4nqluc/external/llvm-raw/utils/bazel/configure.bzl:169:33: in <toplevel>
ERROR: An error occurred during the fetch of repository 'llvm-project':
   Traceback (most recent call last):
        File ""C:/tmp/hn4nqluc/external/llvm-raw/utils/bazel/configure.bzl"", line 146, column 25, in _llvm_configure_impl
                _overlay_directories(repository_ctx)
        File ""C:/tmp/hn4nqluc/external/llvm-raw/utils/bazel/configure.bzl"", line 49, column 13, in _overlay_directories
                fail(""Failed to find python3 binary"")
Error in fail: Failed to find python3 binary
ERROR: C:/users/micla/code/python/diceml/tensorflow/WORKSPACE:15:14: fetching llvm_configure rule //external:llvm-project: Traceback (most recent call last):
        File ""C:/tmp/hn4nqluc/external/llvm-raw/utils/bazel/configure.bzl"", line 146, column 25, in _llvm_configure_impl
                _overlay_directories(repository_ctx)
        File ""C:/tmp/hn4nqluc/external/llvm-raw/utils/bazel/configure.bzl"", line 49, column 13, in _overlay_directories
                fail(""Failed to find python3 binary"")
Error in fail: Failed to find python3 binary
INFO: Repository build_bazel_rules_android instantiated at:
  C:/users/micla/code/python/diceml/tensorflow/WORKSPACE:15:14: in <toplevel>
  C:/users/micla/code/python/diceml/tensorflow/tensorflow/workspace2.bzl:975:21: in workspace
  C:/users/micla/code/python/diceml/tensorflow/tensorflow/workspace2.bzl:814:20: in _tf_repositories
  C:/users/micla/code/python/diceml/tensorflow/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  C:/users/micla/code/python/diceml/tensorflow/third_party/repo.bzl:89:35: in <toplevel>
INFO: Repository flatbuffers instantiated at:
  C:/users/micla/code/python/diceml/tensorflow/WORKSPACE:15:14: in <toplevel>
  C:/users/micla/code/python/diceml/tensorflow/tensorflow/workspace2.bzl:968:28: in workspace
  C:/users/micla/code/python/diceml/tensorflow/tensorflow/workspace2.bzl:65:16: in _initialize_third_party
  C:/users/micla/code/python/diceml/tensorflow/third_party/flatbuffers/workspace.bzl:6:20: in repo
  C:/users/micla/code/python/diceml/tensorflow/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  C:/users/micla/code/python/diceml/tensorflow/third_party/repo.bzl:89:35: in <toplevel>
ERROR: C:/users/micla/code/python/diceml/tensorflow/tensorflow/tools/pip_package/BUILD:276:10: //tensorflow/tools/pip_package:build_pip_package depends on //tensorflow/compiler/mlir/tensorflow:gen_mlir_passthrough_op_py in repository @ which failed to fetch. no such package '@llvm-project//mlir': Failed to find python3 binary
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed
INFO: Elapsed time: 399.994s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (51 packages loaded, 12 targets config\
ured)
    currently loading: tensorflow/lite/tools ... (3 packages)
    Fetching https://storage.googleapis.com/.../flatbuffers/archive/v2.0.6.tar.gz
```
</details>"
59941,Experimental feature support for TFLite selective build ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11-tflite-android

### Custom Code

No

### Current Behaviour?

I have followed [the tutorial](https://www.tensorflow.org/lite/examples/on_device_training/overview) and made my TextCNN model have the ability of on-device-training. It works well with the prebuilt library for tensorflow-lite.

When trying to [reduce the binary size](https://www.tensorflow.org/lite/guide/reduce_binary_size) 
with the docker environment I found that the Java API such as `Interpreter#runSignature` is a part of experimental feature, which not enabled by the [tensorflow/lite/tools/build_aar.sh](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/build_aar.sh).

I have tried to add `experimental = True` to `tflite_custom_android_library()` function on line 73 of the `build_aar.sh` and rebuild, got the `tensorflow-lite.aar` and `tensorflow-lite-select-tf-ops.aar`. 

It seems that `tensorflow-lite.aar` does have the experimental feature code and works. But `tensorflow-lite-select-tf-ops.aar` failure to run on Android device with error:

```
java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol ""_ZNK6google8protobuf7Message11GetTypeNameEv"" referenced by ""/data/app/*/lib/arm64/libtensorflowlite_flex_jni.so""...
```

I could not find any flag like `experimental = True` for select-tf-ops to enable. Is it possible to enable experimental features for TFLite selective build?


### Relevant log output

_No response_</details>"
59940,Tensorflow2.11  build error   /root/tensorflow/tensorflow/lite/python/BUILD:68:10 Middleman _middlemen/tensorflow_Slite_Spython_Stflite_Uconvert-runfiles failed:,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf2.11

### Custom Code

Yes

### OS Platform and Distribution

centos7.6

### Mobile device

_No response_

### Python version

python3.7.12

### Bazel version

5.0.0

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
# Configuration: 97a3a0fa2bd0b7dd271cfd175d4f2e3372d12545074b079e511143f09e7a2e7c
# Execution platform: @local_execution_config_platform//:platform
2023-03-09 05:22:40.181096: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 22, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 36, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 78, in <module>
    f'{traceback.format_exc()}'
ImportError: Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: /root/tensorflow/tensorflow/lite/python/BUILD:68:10 Middleman _middlemen/tensorflow_Slite_Spython_Stflite_Uconvert-runfiles failed: (Exit 1): bash failed: error executing command
  (cd /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow && \
  exec env - \
```


### Relevant log output

```shell
/bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2 --root_init_template=tensorflow/api_template.__init__.py --apidir=bazel-out/k8-opt/bin/tensorflow_api/v2/ --apiname=tensorflow --apiversion=2  --compat_apiversion=1 --compat_apiversion=2  --compat_init_template=tensorflow/compat_template_v1.__init__.py --compat_init_template=tensorflow/compat_template.__init__.py --packages=tensorflow.python,tensorflow.dtensor.python.accelerator_util,tensorflow.dtensor.python.api,tensorflow.dtensor.python.config,tensorflow.dtensor.python.d_checkpoint,tensorflow.dtensor.python.d_variable,tensorflow.dtensor.python.input_util,tensorflow.dtensor.python.layout,tensorflow.dtensor.python.mesh_util,tensorflow.dtensor.python.tpu_util,tensorflow.dtensor.python.save_restore,tensorflow.lite.python.analyzer,tensorflow.lite.python.lite,tensorflow.lite.python.authoring.authoring,tensorflow.python.modules_with_exports --output_package=tensorflow._api.v2 --use_relative_imports=True --loading=static --loading=default bazel-out/k8-opt/bin/tensorflow/_api/v2/v2.py bazel-out/k8-opt/bin/tensorflow/_api/v2/__internal__/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/__internal__/autograph/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/__internal__/decorator/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/__internal__/dispatch/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/__internal__/distribute/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/__internal__/distribute/combinations/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/__internal__/distribute/interim/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/__internal__/distribute/multi_process_runner/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/__internal__/eager_context/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/__internal__/feature_column/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/__internal__/function/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/__internal__/graph_util/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/__internal__/mixed_precision/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/__internal__/monitoring/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/__internal__/nest/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/__internal__/ops/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/__internal__/smart_cond/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/__internal__/test/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/__internal__/test/combinations/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/__internal__/tf2/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/__internal__/train/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/__internal__/types/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/__internal__/saved_model/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/__internal__/saved_model/load/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/__internal__/tracking/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/__operators__/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/audio/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/autograph/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/autograph/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/autodiff/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/bitwise/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/config/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/config/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/config/optimizer/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/config/threading/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/data/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/data/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/data/experimental/service/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/debugging/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/debugging/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/distribute/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/distribute/cluster_resolver/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/distribute/coordinator/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/distribute/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/distribute/experimental/coordinator/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/distribute/experimental/partitioners/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/distribute/experimental/rpc/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/dtypes/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/errors/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/experimental/dtensor/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/experimental/numpy/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/experimental/numpy/random/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/experimental/tensorrt/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/experimental/dlpack/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/feature_column/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/io/gfile/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/graph_util/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/image/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/io/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/queue/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/linalg/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/linalg/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/lite/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/lite/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/lite/experimental/authoring/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/lite/experimental/microfrontend/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/lite/experimental/microfrontend/python/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/lite/experimental/microfrontend/python/ops/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/lookup/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/lookup/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/math/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/math/special/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/mlir/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/mlir/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/nest/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/nn/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/nn/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/profiler/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/profiler/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/profiler/experimental/client/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/profiler/experimental/server/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/quantization/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/ragged/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/random/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/random/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/raw_ops/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/saved_model/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/saved_model/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/sets/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/signal/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/sparse/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/strings/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/summary/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/summary/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/sysconfig/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/test/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/tpu/experimental/embedding/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/tpu/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/tpu/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/train/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/train/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/types/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/types/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/types/experimental/distributed/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/version/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/xla/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/xla/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/autograph/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/decorator/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/dispatch/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/distribute/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/distribute/combinations/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/distribute/interim/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/distribute/multi_process_runner/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/eager_context/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/feature_column/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/function/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/graph_util/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/mixed_precision/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/monitoring/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/nest/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/ops/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/smart_cond/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/test/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/test/combinations/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/tf2/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/train/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/types/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/saved_model/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/saved_model/load/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/tracking/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/__operators__/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/audio/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/autograph/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/autograph/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/autodiff/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/bitwise/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/compat/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/config/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/config/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/config/optimizer/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/config/threading/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/data/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/data/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/data/experimental/service/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/debugging/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/debugging/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/distribute/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/distribute/cluster_resolver/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/distribute/coordinator/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/distribute/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/distribute/experimental/coordinator/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/distribute/experimental/partitioners/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/distribute/experimental/rpc/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/dtypes/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/errors/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/experimental/dtensor/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/experimental/numpy/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/experimental/numpy/random/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/experimental/tensorrt/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/experimental/dlpack/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/feature_column/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/io/gfile/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/graph_util/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/image/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/io/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/queue/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/linalg/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/linalg/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/lite/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/lite/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/lite/experimental/authoring/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/lite/experimental/microfrontend/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/lite/experimental/microfrontend/python/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/lite/experimental/microfrontend/python/ops/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/lookup/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/lookup/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/math/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/math/special/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/mlir/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/mlir/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/nest/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/nn/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/nn/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/profiler/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/profiler/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/profiler/experimental/client/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/profiler/experimental/server/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/quantization/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/ragged/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/random/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/random/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/raw_ops/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/saved_model/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/saved_model/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/sets/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/signal/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/sparse/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/strings/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/summary/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/summary/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/sysconfig/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/test/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/tpu/experimental/embedding/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/tpu/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/tpu/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/train/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/train/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/types/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/types/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/types/experimental/distributed/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/version/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/xla/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/xla/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/app/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/audio/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/autograph/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/autograph/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/bitwise/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/compat/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/config/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/config/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/config/optimizer/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/config/threading/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/data/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/data/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/data/experimental/service/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/debugging/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/debugging/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/distribute/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/distribute/cluster_resolver/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/distribute/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/distributions/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/dtypes/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/errors/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/feature_column/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/gfile/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/io/gfile/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/graph_util/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/image/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/io/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/queue/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/initializers/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/layers/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/layers/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/linalg/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/linalg/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/lite/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/lite/constants/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/lite/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/lite/experimental/authoring/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/lite/experimental/microfrontend/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/lite/experimental/microfrontend/python/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/lite/experimental/microfrontend/python/ops/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/logging/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/lookup/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/lookup/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/losses/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/manip/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/math/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/math/special/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/metrics/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/mixed_precision/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/mixed_precision/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/mlir/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/mlir/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/nest/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/nn/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/nn/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/nn/rnn_cell/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/profiler/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/python_io/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/quantization/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/ragged/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/random/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/random/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/raw_ops/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/resource_loader/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/strings/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/builder/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/constants/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/loader/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/main_op/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/signature_constants/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/signature_def_utils/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/tag_constants/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/utils/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/sets/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/signal/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/sparse/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/spectral/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/summary/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/sysconfig/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/test/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/tpu/experimental/embedding/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/tpu/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/tpu/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/train/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/train/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/train/queue_runner/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/types/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/types/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/user_ops/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/version/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/xla/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/xla/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/compat/v1/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/compat/v2/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/compat/v1/compat/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v1/compat/v2/compat/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/compat/v1/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/compat/v2/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/compat/v1/compat/__init__.py bazel-out/k8-opt/bin/tensorflow/_api/v2/compat/v2/compat/v2/compat/__init__.py')
# Configuration: 97a3a0fa2bd0b7dd271cfd175d4f2e3372d12545074b079e511143f09e7a2e7c
# Execution platform: @local_execution_config_platform//:platform
2023-03-09 05:22:40.181096: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 22, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 36, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 78, in <module>
    f'{traceback.format_exc()}'
ImportError: Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: /root/tensorflow/tensorflow/lite/python/BUILD:68:10 Middleman _middlemen/tensorflow_Slite_Spython_Stflite_Uconvert-runfiles failed: (Exit 1): bash failed: error executing command
  (cd /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow && \
```
</details>"
59938,"Windows build fails with ""pywrap_tensorflow_import_lib_file: variable '$<' : no input file""","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.10.9

### Bazel version

5.2.0

### GCC/Compiler version

MSVC 2019

### CUDA/cuDNN version

_No response_

### GPU model and memory

CPU build

### Current Behaviour?

```shell
When building `tensorflow 2.11` (and nightly) for Windows (CPU), the bazel build step fails.

`ERROR: C:/tensorflow211/tensorflow/python/BUILD:3776:8: in cmd attribute of genrule rule
//tensorflow/python:pywrap_tensorflow_import_lib_file: variable '$<' : no input file
ERROR: C:/tensorflow211/tensorflow/python/BUILD:3776:8: Analysis of target '//tensorflow/python:pywrap_tensorflow_import_lib_file'
failed
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted:`

The failure occurs at this genrule in python/BUILD:

`tensorflow/python/BUILD:3633`
`genrule(
    name = ""pywrap_tensorflow_import_lib_file"",
    srcs = ["":get_pywrap_tensorflow_import_lib_file""],
    outs = [""_pywrap_tensorflow_internal.lib""],
    cmd = select({
        ""//tensorflow:windows"": ""cp -f $< $@"",
        ""//conditions:default"": ""touch $@"",  # Just a placeholder for Unix platforms
    }),
    visibility = [""//visibility:public""],
)`

My knowledge of bazel is limited, but it seems that ""$<"" is blank due to the empty array for //tensorflow:windows here:

`tensorflow/python/BUILD:366`
`dynamic_deps = ["":_pywrap_tensorflow_internal.so""] + select({
        ""//tensorflow:macos"": [""//tensorflow:libtensorflow_framework.%s.dylib"" % VERSION],
        ""//conditions:default"": [""//tensorflow:libtensorflow_framework.so.%s"" % VERSION],
        ""//tensorflow:windows"": [],
    }),`

Any help getting this to build is appreciated. Thanks!
```


### Standalone code to reproduce the issue

```shell
/c/tf211/tensorflow-base/_h_env/Library/bin/bazel build --config=mkl --define=no_tensorflow_py_deps=true --verbose_failures --config=opt 
--copt=-D_copysign=copysign --host_copt=-D_copysign=copysign --cxxopt=-D_copysign=copysign --host_cxxopt=-D_copysign=copysign 
--python_path=/c/tf211/tensorflow-base/_h_env/python.exe 
--action_env=PYTHON_BIN_PATH=/c/tf211/tensorflow-base/_h_env/python.exe 
--action_env=PYTHON_LIB_PATH=/c/tf211/tensorflow-base/_h_env/Lib/site-packages 
'--linkopt=-LC:\tf211\tensorflow-base\_h_env\Library' --strip=always --experimental_cc_shared_library //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
+ /c/tf211/tensorflow-base/_h_env/Library/bin/bazel build --config=mkl --define=no_tensorflow_py_deps=true --verbose_failures --config=opt 
--copt=-D_copysign=copysign --host_copt=-D_copysign=copysign --cxxopt=-D_copysign=copysign --host_cxxopt=-D_copysign=copysign 
--python_path=/c/tf211/tensorflow-base/_h_env/python.exe 
--action_env=PYTHON_BIN_PATH=/c/tf211/tensorflow-base/_h_env/python.exe 
--action_env=PYTHON_LIB_PATH=/c/tf211/tensorflow-base/_h_env/Lib/site-packages 
'--linkopt=-LC:\tf211\tensorflow-base\_h_env\Library' --strip=always --experimental_cc_shared_library //tensorflow/tools/pip_package:build_pip_package
Starting local Bazel server and connecting to it...
WARNING: Option 'java_toolchain' is deprecated
WARNING: Option 'host_java_toolchain' is deprecated
WARNING: Option 'experimental_strict_action_env' is deprecated: Use --incompatible_strict_action_env instead
INFO: Reading 'startup' options from c:\tf211\tensorflow-base\work\.bazelrc: --output_user_root=C:/tmp
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=0 --terminal_columns=80
INFO: Reading rc options for 'build' from c:\tf211\tensorflow-base\work\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/tf211/tensorflow-base/_h_env/python.exe
INFO: Reading rc options for 'build' from c:\tf211\tensorflow-base\work\.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain 
--host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt 
--announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --config=short_logs --config=v2
INFO: Reading rc options for 'build' from c:\tf211\tensorflow-base\work\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/tf211/tensorflow-base/_h_env/python.exe --action_env 
PYTHON_LIB_PATH=C:/tf211/tensorflow-base/_h_env/Lib/site-packages 
--python_path=C:/tf211/tensorflow-base/_h_env/python.exe --define=with_xla_support=false 
--define=PREFIX=C:/tf211/tensorflow-base/_h_env --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions 
--define=override_eigen_strong_inline=true
INFO: Found applicable config definition build:short_logs in file c:\tf211\tensorflow-base\work\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file c:\tf211\tensorflow-base\work\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:mkl in file c:\tf211\tensorflow-base\work\.bazelrc: --define=build_with_mkl=true 
--define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 --define=build_with_openmp=true -c opt
INFO: Found applicable config definition build:opt in file c:\tf211\tensorflow-base\work\.tf_configure.bazelrc: --copt=/arch:AVX --host_copt=/arch:AVX
INFO: Found applicable config definition build:windows in file c:\tf211\tensorflow-base\work\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES 
--host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++17 --host_cxxopt=/std:c++17 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI 
--host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF 
--linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false --copt=/Zc:__cplusplus
INFO: Found applicable config definition build:monolithic in file c:\tf211\tensorflow-base\work\.bazelrc: --define framework_shared_object=false
Loading: 
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (1 packages loaded, 0 targets configured)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (285 packages loaded, 4132 targets configured)
ERROR: C:/tf211/tensorflow-base/work/tensorflow/python/BUILD:3633:8: in cmd attribute of genrule rule 
//tensorflow/python:pywrap_tensorflow_import_lib_file: variable '$<' : no input file
ERROR: C:/tf211/tensorflow-base/work/tensorflow/python/BUILD:3633:8: Analysis of target '//tensorflow/python:pywrap_tensorflow_import_lib_file' failed
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted:
```
</details>"
59935,third_party/icu/data missing big-endian conversion data files,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.11

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04 s390x

### Mobile device

_No response_

### Python version

3.10

### Bazel version

5.3.0

### GCC/Compiler version

gcc 9

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Two of the unicode unit tests fail on s390x due to the icu_conversion_data.c.gz.* files being in little-endian format. 

Could big-endian formatted versions of the icu_conversion_data.c.gz.* files be added so that they can be selected when building on s390x?
```


### Standalone code to reproduce the issue

```shell
The unit tests:

* //tensorflow/python/kernel_tests/strings_ops:unicode_decode_op_test
* //tensorflow/python/kernel_tests/strings_ops:unicode_transcode_op_test

fail on s390x.
```


### Relevant log output

```shell
======================================================================
ERROR: testDecodeWithDifferentEncodings5 ('SHIFT-JIS', ['Hello', 'こんにちは']) (__main__.UnicodeDecodeTest)
UnicodeDecodeTest.testDecodeWithDifferentEncodings5 ('SHIFT-JIS', ['Hello', 'こんにちは'])
testDecodeWithDifferentEncodings('SHIFT-JIS', ['Hello', 'こんにちは'])
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/jalbrecht/.cache/bazel/_bazel_jalbrecht/d88fb95a52e214eb99836e3d1a65a951/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/kernel_tests/strings_ops/unicode_decode_op_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1378, in _do_call
    return fn(*args)
  File ""/home/jalbrecht/.cache/bazel/_bazel_jalbrecht/d88fb95a52e214eb99836e3d1a65a951/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/kernel_tests/strings_ops/unicode_decode_op_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1361, in _run_fn
    return self._call_tf_sessionrun(options, feed_dict, fetch_list,
  File ""/home/jalbrecht/.cache/bazel/_bazel_jalbrecht/d88fb95a52e214eb99836e3d1a65a951/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/kernel_tests/strings_ops/unicode_decode_op_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun
    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,
tensorflow.python.framework.errors_impl.InvalidArgumentError: Could not create converter for input encoding: SHIFT-JIS
         [[{{node UnicodeDecode/UnicodeDecode}}]]
```
</details>"
59934,Wrong metric calculations for masked out timeseries with sample weights,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf.2.10

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I allready have posted  the bug on tensorflow  forum but since i think it realy is a bug which silently produces wrong result, i should post it here as well:

i got a timeseries dataset where i mask out missing data of shorter sequences with sample_weight_mode=‘temporal’. So far so good. the losses are computed as expected but the metrics seem to ignore the sample-weights completly. is it a bug or am i doiing something wrong.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from keras import layers
import numpy as np

input_layer = layers.Input(shape=(500, 5))
input_lstm = layers.LSTM(30, return_sequences=True)(input_layer)
output1 = layers.Dense(1)(input_lstm)
output2 = layers.Dense(1)(input_lstm)

model = tf.keras.Model(inputs=input_layer, outputs=[output1, output2])

model.compile(optimizer=""adam"", run_eagerly=False, sample_weight_mode='temporal', loss=""mse"", metrics=[[""mae""], [""mae""]])


#dataset
x = np.random.random((2000, 500, 5))

sample_weights = np.ones(x.shape[:-1])
amnt_zeros = np.random.choice(500, 2000)
for idx, zeros in enumerate(amnt_zeros):
    sample_weights[idx, -zeros:] = 0.0

x = x*sample_weights[...,None]
y1 = ((np.sum(x, axis=-1) + 20) * sample_weights)[..., None]
y2 = ((np.sum(x, axis=-1) + 10) * sample_weights)[...,None]


#masked y3 data is increased drasically to show the wrong calculation of the metrics
y2_testsample_weights = np.full_like(y2, -50000) * ((sample_weights - 1)[...,None])
y2 = y2 + y2_testsample_weights

history = model.fit(x=x, y=[y1, y2], sample_weight=sample_weights, epochs=500)
```


### Relevant log output

```shell
46/63 [====================>.........] - ETA: 4s - loss: 297.1746 - dense_loss: 226.9913 - dense_1_loss: 70.1834 - dense_mae: 10.7948 - dense_1_mae: 25511.7363
```
</details>"
59933,error using the Dispatch ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.11

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.9.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
in op_dispatch_handler
    result = api_dispatcher.Dispatch(args, kwargs)
TypeError: Got an unexpected keyword argument 'dim'
```


### Relevant log output

_No response_</details>"
59932,//tensorflow/tools/api/tests:api_compatibility_test fails with protobuf > 3.20.3,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

5.3.0

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

```shell
The test fails
```


### Standalone code to reproduce the issue

```shell
bazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --noremote_accept_cached --test_env=TF_ENABLE_ONEDNN_OPTS=1 --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-requires-gpu --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-requires-gpu --verbose_failures --build_tests_only --jobs=16 -- //tensorflow/tools/api/tests:api_compatibility_test
```


### Relevant log output

```shell
======================================================================
FAIL: testAPIBackwardsCompatibility (__main__.ApiCompatibilityTest)
ApiCompatibilityTest.testAPIBackwardsCompatibility
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/e953b164f58eb4c9598ad736d787ff39/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/tools/api/tests/api_compatibility_test.py"", line 511, in testAPIBackwardsCompatibility
    self._checkBackwardsCompatibility(
  File ""/root/.cache/bazel/_bazel_root/e953b164f58eb4c9598ad736d787ff39/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/tools/api/tests/api_compatibility_test.py"", line 488, in _checkBackwardsCompatibility
    self._AssertProtoDictEquals(
  File ""/root/.cache/bazel/_bazel_root/e953b164f58eb4c9598ad736d787ff39/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/tools/api/tests/api_compatibility_test.py"", line 395, in _AssertProtoDictEquals
    self.fail('%d differences found between API and golden.' % diff_count)
AssertionError: 1 differences found between API and golden.

======================================================================
FAIL: testAPIBackwardsCompatibilityV1 (__main__.ApiCompatibilityTest)
ApiCompatibilityTest.testAPIBackwardsCompatibilityV1
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/e953b164f58eb4c9598ad736d787ff39/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/tools/api/tests/api_compatibility_test.py"", line 528, in testAPIBackwardsCompatibilityV1
    self._checkBackwardsCompatibility(
  File ""/root/.cache/bazel/_bazel_root/e953b164f58eb4c9598ad736d787ff39/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/tools/api/tests/api_compatibility_test.py"", line 488, in _checkBackwardsCompatibility
    self._AssertProtoDictEquals(
  File ""/root/.cache/bazel/_bazel_root/e953b164f58eb4c9598ad736d787ff39/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/tools/api/tests/api_compatibility_test.py"", line 395, in _AssertProtoDictEquals
    self.fail('%d differences found between API and golden.' % diff_count)
AssertionError: 4 differences found between API and golden.

======================================================================
FAIL: testAPIBackwardsCompatibilityV2 (__main__.ApiCompatibilityTest)
ApiCompatibilityTest.testAPIBackwardsCompatibilityV2
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/e953b164f58eb4c9598ad736d787ff39/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/tools/api/tests/api_compatibility_test.py"", line 550, in testAPIBackwardsCompatibilityV2
    self._checkBackwardsCompatibility(
  File ""/root/.cache/bazel/_bazel_root/e953b164f58eb4c9598ad736d787ff39/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/tools/api/tests/api_compatibility_test.py"", line 488, in _checkBackwardsCompatibility
    self._AssertProtoDictEquals(
  File ""/root/.cache/bazel/_bazel_root/e953b164f58eb4c9598ad736d787ff39/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/tools/api/tests/api_compatibility_test.py"", line 395, in _AssertProtoDictEquals
    self.fail('%d differences found between API and golden.' % diff_count)
AssertionError: 1 differences found between API and golden.

----------------------------------------------------------------------
```
</details>"
59931,//tensorflow/python/util/protobuf:protobuf_compare_test fails with protobuf > 3.20.3,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

5.3.0

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

```shell
The test fails
```


### Standalone code to reproduce the issue

```shell
bazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --noremote_accept_cached --test_env=TF_ENABLE_ONEDNN_OPTS=1 --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-requires-gpu --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-requires-gpu --verbose_failures --build_tests_only --jobs=16 -- //tensorflow/python/util/protobuf:protobuf_compare_test
```


### Relevant log output

```shell
ERROR: testCheckInitialized (__main__.AssertTest)
AssertTest.testCheckInitialized
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/e953b164f58eb4c9598ad736d787ff39/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/util/protobuf/protobuf_compare_test.runfiles/org_tensorflow/tensorflow/python/util/protobuf/compare_test.py"", line 324, in testCheckInitialized
    b = copy.deepcopy(a)
  File ""/usr/lib/python3.9/copy.py"", line 153, in deepcopy
    y = copier(memo)
  File ""/usr/local/lib/python3.9/dist-packages/google/protobuf/message.py"", line 83, in __deepcopy__
    clone.MergeFrom(self)
google.protobuf.message.EncodeError: Message compare_test.Labeled is missing required fields: required
```
</details>"
59930,//tensorflow/python/kernel_tests/proto:decode_proto_op_test fails with protobuf > 3.20.3,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

5.3.0

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

```shell
The test fails
```


### Standalone code to reproduce the issue

```shell
bazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --noremote_accept_cached --test_env=TF_ENABLE_ONEDNN_OPTS=1 --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-requires-gpu --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-requires-gpu --verbose_failures --build_tests_only --jobs=16 -- //tensorflow/python/kernel_tests/proto:decode_proto_op_test
```


### Relevant log output

```shell
ERROR: testBinaryDisordereddefaults (__main__.DecodeProtoOpTest)
DecodeProtoOpTest.testBinaryDisordereddefaults
testBinaryDisordereddefaults(values {
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/e953b164f58eb4c9598ad736d787ff39/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/proto/decode_proto_op_test.runfiles/absl_py/absl/testing/parameterized.py"", line 316, in bound_param_test
    return test_method(self, *testcase_params)
  File ""/root/.cache/bazel/_bazel_root/e953b164f58eb4c9598ad736d787ff39/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/proto/decode_proto_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/proto/decode_proto_op_test_base.py"", line 223, in testBinaryDisordered
    self._runDecodeProtoTests(
  File ""/root/.cache/bazel/_bazel_root/e953b164f58eb4c9598ad736d787ff39/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/proto/decode_proto_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/proto/decode_proto_op_test_base.py"", line 206, in _runDecodeProtoTests
    self._compareProtos(batch_shape, sizes, fields, field_dict)
  File ""/root/.cache/bazel/_bazel_root/e953b164f58eb4c9598ad736d787ff39/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/proto/decode_proto_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/proto/decode_proto_op_test_base.py"", line 142, in _compareProtos
    self._compareValues(fd, values.flat,
  File ""/root/.cache/bazel/_bazel_root/e953b164f58eb4c9598ad736d787ff39/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/proto/decode_proto_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/proto/decode_proto_op_test_base.py"", line 57, in _compareValues
    if fd.cpp_type == fd.CPPTYPE_FLOAT:
AttributeError: 'google._upb._message.FieldDescriptor' object has no attribute 'CPPTYPE_FLOAT'
```
</details>"
59929,"java.lang.UnsatisfiedLinkError:Failed to load native TensorFlow Lite methods. Check that the correct native libraries are present, and, if using a custom native library, have been properly loaded via System.loadLibrary():","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

tensorflow-lite:2.8.0

### Custom Code

Yes

### OS Platform and Distribution

macOS Monterey 12.5.1

### Mobile device

Huawei Mate 20 (HMA-AL00)

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hello, this error occurs on some models when my application loads tflite：

java.lang.UnsatisfiedLinkError:Failed to load native TensorFlow Lite methods. Check that the correct native libraries are present, and, if using a custom native library, have been properly loaded via System.loadLibrary():

java.lang.UnsatisfiedLinkError: dalvik.system.PathClassLoader[DexPathList[[zip file ""/data/app/packagename/base.apk""],nativeLibraryDirectories=[/data/app/packagename/lib/arm64, /data/app/packagename/base.apk!/lib/arm64-v8a, /data/user/0/packagename/files/dynamic/arm64-v8a, /system/priv-app/HwCameraKit/HwCameraKit.apk!/lib/arm64-v8a, /system/lib64, /hw_product/lib64, /system/product/lib64, /system/lib64, /hw_product/lib64, /system/product/lib64]]] couldn't find ""libtensorflowlite_jni.so""
org.tensorflow.lite.TensorFlowLite.a(SourceFile:5)
org.tensorflow.lite.nnapi.NnApiDelegate.<init>(SourceFile:2)
```


### Standalone code to reproduce the issue

```shell
dependencies {
    implementation 'org.tensorflow:tensorflow-lite:2.8.0'
}

abiFilters ""armeabi-v7a"", 'arm64-v8a'
```


### Relevant log output

_No response_</details>"
59928,"Crash due to CUDA_ERROR_ILLEGAL_ADDRESS can be triggered in SparseReorder, SparseFillEmptyRowsGrad, SparseSlice and SparseTensorToCSRSparseMatrix","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.13.0-dev20230307

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.5

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Crashes due to ""CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered"" can be triggered in 4 operators, which are SparseReorder, SparseFillEmptyRowsGrad, SparseSlice and SparseTensorToCSRSparseMatrix. The cuase of the crashes might be the same.
```


### Standalone code to reproduce the issue

```shell
# SparseTensorToCSRSparseMatrix
import tensorflow as tf
print(tf.__version__)
for i in range(100):
    with tf.device(""GPU:0""):
        indices = tf.saturate_cast(tf.random.uniform([15, 0], minval=0, maxval=64, dtype=tf.int64), dtype=tf.int64)
        values = tf.random.uniform([3, 6], dtype=tf.float32)
        dense_shape = [33, 73, 91]
        res = tf.raw_ops.SparseTensorToCSRSparseMatrix(
            indices=indices,
            values=values,
            dense_shape=dense_shape,
        )

# SparseReorder
import tensorflow as tf
print(tf.__version__)
with tf.device(""GPU:0""):
    for i in range(100):
        input_indices = tf.saturate_cast(tf.random.uniform([1, 3], minval=0, maxval=64, dtype=tf.int64), dtype=tf.int64)
        input_values = tf.saturate_cast(tf.random.uniform([8], minval=0, maxval=64, dtype=tf.int64), dtype=tf.int8)
        input_shape = []
        res = tf.raw_ops.SparseReorder(
            input_indices=input_indices,
            input_values=input_values,
            input_shape=input_shape,
        )

# SparseFillEmptyRowsGrad
import tensorflow as tf
print(tf.__version__)
for i in range(100):
    with tf.device(""GPU:0""):
        reverse_index_map = tf.saturate_cast(tf.random.uniform([9], minval=0, maxval=64, dtype=tf.int64), dtype=tf.int64)
        grad_values = tf.saturate_cast(tf.random.uniform([0], minval=0, maxval=64, dtype=tf.int64), dtype=tf.uint8)
        res = tf.raw_ops.SparseFillEmptyRowsGrad(
            reverse_index_map=reverse_index_map,
            grad_values=grad_values,
        )

# SparseSlice
import tensorflow as tf
print(tf.__version__)
for i in range(100):
    with tf.device(""GPU:0""):
        indices = tf.saturate_cast(tf.random.uniform([13, 0], minval=0, maxval=64, dtype=tf.int64), dtype=tf.int64)
        values = tf.random.uniform([9], dtype=tf.float64)
        shape = [100, 100]
        start = tf.saturate_cast(tf.random.uniform([2], minval=0, maxval=64, dtype=tf.int64), dtype=tf.int64)
        size = tf.saturate_cast(tf.random.uniform([2], minval=0, maxval=64, dtype=tf.int64), dtype=tf.int64)
        res = tf.raw_ops.SparseSlice(
            indices=indices,
            values=values,
            shape=shape,
            start=start,
            size=size,
        )
```


### Relevant log output

```shell
All the crashes have similar output:

2023-03-08 11:09:51.182519: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-03-08 11:09:51.231134: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-08 11:09:51.979378: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2.13.0-dev20230307
2023-03-08 11:09:53.602923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14581 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:2f:00.0, compute capability: 7.0
2023-03-08 11:09:53.626782: I tensorflow/core/common_runtime/process_function_library_runtime.cc:586] Finished graph optimizations for MultiDevice function ""__wrapped____EagerConst_device_/job:localhost/replica:0/task:0/device:GPU:0"" with target device ""/job:localhost/replica:0/task:0/device:GPU:0"". Took 0 secs.
2023-03-08 11:09:53.871921: I tensorflow/core/common_runtime/process_function_library_runtime.cc:586] Finished graph optimizations for MultiDevice function ""__wrapped____EagerConst_device_/job:localhost/replica:0/task:0/device:GPU:0"" with target device ""/job:localhost/replica:0/task:0/device:GPU:0"". Took 0 secs.
2023-03-08 11:09:53.876076: I tensorflow/core/common_runtime/process_function_library_runtime.cc:586] Finished graph optimizations for MultiDevice function ""__wrapped__RandomUniformInt_device_/job:localhost/replica:0/task:0/device:GPU:0"" with target device ""/job:localhost/replica:0/task:0/device:GPU:0"". Took 0 secs.
2023-03-08 11:09:53.881562: I tensorflow/core/common_runtime/process_function_library_runtime.cc:586] Finished graph optimizations for MultiDevice function ""__wrapped__ClipByValue_device_/job:localhost/replica:0/task:0/device:GPU:0"" with target device ""/job:localhost/replica:0/task:0/device:GPU:0"". Took 0 secs.
2023-03-08 11:09:53.882676: I tensorflow/core/common_runtime/process_function_library_runtime.cc:586] Finished graph optimizations for MultiDevice function ""__wrapped__Cast_device_/job:localhost/replica:0/task:0/device:GPU:0"" with target device ""/job:localhost/replica:0/task:0/device:GPU:0"". Took 0 secs.
2023-03-08 11:09:53.884133: I tensorflow/core/common_runtime/process_function_library_runtime.cc:586] Finished graph optimizations for MultiDevice function ""__wrapped__SparseReorder_device_/job:localhost/replica:0/task:0/device:GPU:0"" with target device ""/job:localhost/replica:0/task:0/device:GPU:0"". Took 0 secs.
2023-03-08 11:09:53.919158: E tensorflow/compiler/xla/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2023-03-08 11:09:53.919227: F tensorflow/core/common_runtime/device/device_event_mgr.cc:223] Unexpected Event status: 1
Aborted (core dumped)
```
</details>"
59927,A crash due to check-fail can be triggered in QuantizedConv2D,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.13.0.dev20230307

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.5

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A crash due to check-fail can be triggered in QuantizedConv2D and its external api `tf.compat.v1.nn.quantized_conv2d`.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
print(tf.__version__)
strides = [1, 128, 128, 1]
padding = ""SAME""
dilations = [1, 1, 1, 1]
input = tf.cast(tf.random.uniform([2, 1, 0, 1], minval=0, maxval=64, dtype=tf.int64), dtype=tf.quint8)
filter = tf.cast(tf.random.uniform([1, 1, 1, 1], minval=0, maxval=64, dtype=tf.int64), dtype=tf.quint8)
min_input = tf.random.uniform([], dtype=tf.float32)
max_input = tf.random.uniform([], dtype=tf.float32)
min_filter = tf.random.uniform([], dtype=tf.float32)
max_filter = tf.random.uniform([], dtype=tf.float32)
# res = tf.raw_ops.QuantizedConv2D(
res = tf.compat.v1.nn.quantized_conv2d(
    strides=strides,
    padding=padding,
    dilations=dilations,
    input=input,
    filter=filter,
    min_input=min_input,
    max_input=max_input,
    min_filter=min_filter,
    max_filter=max_filter,
)
```


### Relevant log output

```shell
2023-03-08 10:44:27.398763: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-03-08 10:44:27.448284: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-08 10:44:28.229207: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2.13.0-dev20230307
2023-03-08 10:44:29.975476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7865 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:2f:00.0, compute capability: 7.0
2023-03-08 10:44:30.000649: I tensorflow/core/common_runtime/process_function_library_runtime.cc:586] Finished graph optimizations for MultiDevice function ""__wrapped____EagerConst_device_/job:localhost/replica:0/task:0/device:GPU:0"" with target device ""/job:localhost/replica:0/task:0/device:GPU:0"". Took 0 secs.
2023-03-08 10:44:30.179857: I tensorflow/core/common_runtime/process_function_library_runtime.cc:586] Finished graph optimizations for MultiDevice function ""__wrapped____EagerConst_device_/job:localhost/replica:0/task:0/device:GPU:0"" with target device ""/job:localhost/replica:0/task:0/device:GPU:0"". Took 0 secs.
2023-03-08 10:44:30.185432: I tensorflow/core/common_runtime/process_function_library_runtime.cc:586] Finished graph optimizations for MultiDevice function ""__wrapped__RandomUniformInt_device_/job:localhost/replica:0/task:0/device:GPU:0"" with target device ""/job:localhost/replica:0/task:0/device:GPU:0"". Took 0 secs.
2023-03-08 10:44:30.193600: I tensorflow/core/common_runtime/process_function_library_runtime.cc:586] Finished graph optimizations for MultiDevice function ""__wrapped__Cast_device_/job:localhost/replica:0/task:0/device:CPU:0"" with target device ""/job:localhost/replica:0/task:0/device:CPU:0"". Took 0 secs.
2023-03-08 10:44:30.195940: I tensorflow/core/common_runtime/process_function_library_runtime.cc:586] Finished graph optimizations for MultiDevice function ""__wrapped__RandomUniform_device_/job:localhost/replica:0/task:0/device:GPU:0"" with target device ""/job:localhost/replica:0/task:0/device:GPU:0"". Took 0 secs.
2023-03-08 10:44:30.201918: I tensorflow/core/common_runtime/process_function_library_runtime.cc:586] Finished graph optimizations for MultiDevice function ""__wrapped__QuantizedConv2D_device_/job:localhost/replica:0/task:0/device:CPU:0"" with target device ""/job:localhost/replica:0/task:0/device:CPU:0"". Took 0 secs.
2023-03-08 10:44:30.202557: F tensorflow/core/kernels/quantized_conv_ops.cc:572] Check failed: out_cols > 0 (0 vs. 0)
Aborted (core dumped)
```
</details>"
59926,`tf.data.Dataset.from_generator(...)` fails when the `output_signature` argument is a dictionary that has a value `tf.RaggedTensorSpec(...)` for some key,"<details><summary>Click to expand!</summary> 
 
### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

v2.11.0-rc2-17-gd5b57ca93e5 2.11.0

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```python
import tensorflow as tf
# With these:
type_value0 = tf.TensorSpec(shape=(), dtype=tf.string)
type_value1 = tf.RaggedTensorSpec(tf.TensorShape([None]), tf.int32, 0, tf.int64)
# These work:
d0 = type_value0.dtype
d1 = type_value1.dtype
# But these do not work
d2 = tf.dtypes.as_dtype(type_value0)
d3 = tf.dtypes.as_dtype(type_value1)
```
... and it appears that it is `tf.dtypes.as_dtype(type_value)` that is used to parse an `output_signature` provided to `tf.data.Dataset.from_generator`.

### Standalone code to reproduce the issue

See above

### Relevant log output
TypeError: Cannot convert the argument `type_value`: TensorSpec(shape=(), dtype=tf.string, name=None) to a TensorFlow DType.
TypeError: Cannot convert the argument `type_value`: RaggedTensorSpec(TensorShape([None]), tf.int32, 0, tf.int64) to a TensorFlow DType.
</details>"
59924,crosstool_wrapper_driver_is_not_gcc error while building TF v2.11,"------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: no
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 20.04 aarch64
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: no
-   **TensorFlow installed from (source or binary)**: source
-   **TensorFlow version (use command below)**: v2.11.0
-   **Python version**: 3.8.10
-   **Bazel version (if compiling from source)**: 5.3.0
-   **GCC/Compiler version (if compiling from source)**: gcc 9.4.0/9.3.0
-   **CUDA/cuDNN version**: 11.8/8.7
-   **GPU model and memory**: RTX A5000
-   **Exact command to reproduce**: follow https://www.tensorflow.org/install/source with cuda enabled
	- docker run -it -rm nvidia/cuda:11.8.0-cudnn8-devel-ubuntu20.04
	- wget https://github.com/bazelbuild/bazel/releases/download/5.3.0/bazel-5.3.0-linux-arm64
	- git clone -b v2.11.0 https://github.com/tensorflow/tensorflow.git
	- ./config with CUDA support? [y/N]: y
	- bazel build --config=cuda --enable_runfiles //tensorflow/tools/pip_package:build_pip_package 

### Describe the problem
TF is configured with gcc and compiles until linking, looks for clang (10.0.0 installed but not used) and fails. Same issue if downgrading to gcc 9.3. Worked on older v2.8 or v2.4. Works on x86_64.

### Source code / logs
ERROR: /home/ubuntu/tensorflow/tensorflow/tensorflow/BUILD:1263:21: Linking tensorflow/libtensorflow_cc.so.2.13.0 failed: (Exit 1): **crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc** @bazel-out/aarch64-opt/bin/tensorflow/libtensorflow_cc.so.2.13.0-2.params
collect2: error: ld returned 1 exit status
...
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 3256.167s, Critical Path: 1408.99s
INFO: 37283 processes: 12379 internal, 24904 local.
FAILED: Build did NOT complete successfully"
59922,Problems with converted 8-bit TFLite models of CycleGAN and running inference (specially allocating tensors),"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No, problem are found on normal TF code.
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: WSL2 Ubuntu 22.04
-   **TensorFlow installed from (source or binary)**: python version is installed using pip, and the benchmark binary is built from TF branch ""v2.11.0""
-   **TensorFlow version (use command below)**: Tested initially on TF2.7 but also on TF2.11
-   **Python version**: 3.9.16 
-   **Bazel version (if compiling from source)**:  5.3.0
-   **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0

------------------------
###  Description of task
- I wanted to to convert CycleGAN model provided by Tensorflow into a TFLite model and run inference.
- I am using this https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb to first define and train the model in TensorFlow
- To convert the models ( 2 Generative and 2 Discriminative) I use the model.save() function
```
generator_g.save('saved_model/cycle_gan_g')
generator_f.save('saved_model/cycle_gan_f')

discriminator_x.save('saved_model/cycle_disc_x')
discriminator_y.save('saved_model/cycle_disc_y')

``` 

- Following this I convert  the model using the TFLiteConverter.

```
def representative_dataset():
    for _ in range(1):
        data = tf.random.normal([1,256,256,3])
        yield [(tf.cast(data, tf.float32) / 127.5) - 1.0]

converter = tf.lite.TFLiteConverter.from_saved_model(mdir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8,tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model = converter.convert()
with open(mdir+'cycle_gan_g.tflite', 'wb') as f:
    f.write(tflite_model)
```

- During conversion I get the following warning which I don't know if it is important for the problems I am facing :
``` 
""fully_quantize: 0, inference_type: 6WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded
, input_inference_type: 0, output_inference_type: 0""
```
------------------------
### Description of problems

Quick note: Problems 2 and 3 occur for the generative TFLite models of Cycle GAN and problem 4 is for the discriminative TFLite  models  of CycleGAN.

**1.** When trying to simply allocate tensors for the converted TFLite models on the python interpreter I get the following error:
**Aborted (core dumped)**
```
interpreter = tf.lite.Interpreter(mdir+'cycle_gan_g.tflite')
interpreter.allocate_tensors()
```

------------------------
**2.** Since the error message was not useful I wanted to run the model using the C++ to understand the problem better. So I built the ""benchmark_model"" tool from https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark using Bazel and tried to run the same tflite model using the benchmark tool with the following cmd calls:
```
.\benchmark_model --enable_op_profiling=true --graph=./models/cycle_gan_f.tflite
```
Running this gave me better idea of the problem: 
* While allocating tensors, node 71 throws an assertion error while dealing with the quantization parameters.
* Link to the specific assert statement https://github.com/tensorflow/tensorflow/blob/be3ea70b27668ed621c8a1f89be899c6ecd0e2b2/tensorflow/lite/kernels/internal/quantization_util.cc#L117
* So it seems for the node 71 (a squared_difference operation) the ""double multiplier"" is greater than 1 which can not be handled by the current code it seems.
* I am not sure if this is a fault of the conversion step or the backend, if the converter is creating bad values for the quantization parameters is there a way to fix this?


------------------------
**3.** Since I was able to access and rebuilt the code I made a small adjustment in the code to artificially reduce the ""double multiplier"" below 1 for this node to see if the model is able to run completely with out any errors. 

* This enabled the allocate_tensor function to complete but I get new error for during inference:
```
ERROR: tensorflow/lite/kernels/concatenation.cc:158 t->dims->data[d] != t0->dims->data[d] (1 != 2)
ERROR: Node number 97 (CONCATENATION) failed to prepare.
```
* It seems that there is a mismatch between dimension of the input tensors and the output tensor but doing a quick check in netron I can see that the dimension are what I believe to be correct:
![image](https://user-images.githubusercontent.com/25208518/223431513-93b50da8-a9cf-4532-b2b8-06019ee283c2.png)
* I have yet to come up with any temporary fix for this issue

------------------------

**4.** When I tried to run inference using the benchmark tool for the discriminative models I get the following error

* This enabled the allocate_tensor function to complete but I get new error for during inference:
```
ERROR: tensorflow/lite/core/subgraph.cc BytesRequired number of elements overflowed.
ERROR: Node number 38 (CONV_2D) failed to prepare.
ERROR: Failed to apply the default TensorFlow Lite delegate indexed at 0.
Failed to allocate tensors!
```
* Again I have yet to come up with any temporary fix for this issue

------------------------
Overall there seems to be problem converting some trivial operations from TF to TFLite. I am not sure if its because of the way Cycle GAN are defined in TF initially or if I am performing the conversion steps wrong. Any help in this matter would be great, I want to convert CycleGAN to int8 TFLite mode and run it.



"
59921,Problems of tflite conversion，about the file of tflite(.h5 to .tflite),"Hi:
### 1. System information

- Linux
- pip:
- TensorFlow 2.10

### 2. Code
###    My Model part ###
...
gru = GRU(64,activation='tanh', recurrent_activation='sigmoid', return_sequences=True,unroll=True)(tmp)

gru_1 = GRU(128,activation='tanh', recurrent_activation='sigmoid', return_sequences=True,unroll=True)(vad_gru)
...
##Converter###

converter = tf.lite.TFLiteConverter.from_keras_model(model)

converter.optimizations = [tf.lite.Optimize.DEFAULT]
def representative_dataset_gen():
    ...

    yield [inp_data]

converter.representative_dataset = representative_dataset_gen

converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8,tf.lite.OpsSet.SELECT_TF_OPS,tf.lite.OpsSet.TFLITE_BUILTINS]

converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8

tflite_model = converter.convert()

with tf.io.gfile.GFile('denoise_quante1.tflite','wb') as f:
    f.write(tflite_model)

### 3. Question

When I convert. h5 to. tflite, if my unroll=False in the GRU layer of the model, the converted tflite (300K) file is much smaller than the. h5 (3M) file, but when I use unroll=True, the tflite (39M) file will become large. Why? Is there any solution？

Thanks！"
59920,Input Layer can't be defined in tensorflow 2.11.0,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Windows

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When trying to define an input layer, an error occured.
I expected it to work just like in the documentation.
Downgrading to Tensorflow 2.10.0 solved the problem
```


### Standalone code to reproduce the issue

```shell
def __init__(self, height, width, channels):
    input_layer = tf.keras.layers.Input(shape=(height, width, channels,))

    ...
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""C:\Users\Noocoon\PycharmProjects\instanceSegmentation\src\center_mask\Unet.py"", line 50, in <module>
    unet = Unet(128, 128, 3)
  File ""C:\Users\Noocoon\PycharmProjects\instanceSegmentation\src\center_mask\Unet.py"", line 28, in __init__
    self.input = tf.keras.layers.Input((height, width, channels, ))
  File ""C:\Users\Noocoon\PycharmProjects\instanceSegmentation\venv\lib\site-packages\keras\utils\traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""C:\Users\Noocoon\PycharmProjects\instanceSegmentation\venv\lib\site-packages\keras\engine\input_layer.py"", line 442, in Input
    outputs = input_layer._inbound_nodes[0].output
AttributeError: 'Node' object has no attribute 'output'
```
</details>"
59918,Please bring back native Windows CUDA support!,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I am very disappointed and sad that native Windows CUDA support was simply dropped. 

The replacement with WSL2 is not sufficient for processes and systems that are not able to use WSL2 because, for example, if they are also Windows-native applications and the port is too expensive. So we can only use version 2.10 (of both the Python and C APIs) and are stuck with it, which is a shame because it prevents us from benefiting from and participating in new developments. We also see a performance loss of about 5% in WSL2, which leads to higher power consumption and thus has a direct impact on our climate, which can make a big difference in our already very compute-intensive business. In addition, the Windows Direct-ML-Plugin interface is not sufficient, since the performance does not yet reach CUDA and optimizations like XLA and others are not supported. Also, you lose all your highly optimized and expensively developed TF CUDA Custom Ops.

It is also clear that the native CUDA feature on Windows is much needed, see here in the following issues other people are looking for exactly the native CUDA feature on Windows:

-  #https://github.com/tensorflow/tensorflow/issues/58629
-  #https://github.com/tensorflow/tensorflow/issues/58933
-  #https://github.com/tensorflow/tensorflow/issues/59905
-  #https://github.com/tensorflow/tensorflow/issues/59119
-  #https://github.com/tensorflow/tensorflow/issues/59016
-  #https://github.com/tensorflow/tensorflow/issues/58985
-  #https://github.com/tensorflow/tensorflow/issues/58729

**All this leads to the simple exclusion and virtual discrimination of a large part of the Tensorflow community that uses CUDA Windows natively.**

Why has support been dropped? You could at least keep support for CUDA Windows Native in custom builds.
I hope and ask that you bring back Windows native CUDA support and let people decide for themselves if they want to use native CUDA or WSL2.

Thank you for the development of Tensorflow! My favourite DL framework! :)


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))
```


### Relevant log output

_No response_</details>"
59913,Optional xnnpack feature relies on weak symbols which are unavailable on Windows," ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Tensorflow Version

2.4

### Current Behaviour?

At the time tflite comes with optional xnnpack feature which relies on TFLITE_ATTRIBUTE_WEAK attribute applied to a delegate function.

As there are no weak symbols on Windows, this feature can not be used universally (i. e. independent from target platform).

I suggest switching this feature on / off in configure time (when build with CMake) or via explicit choosing of option (when build with Bazel). This will force every client to explicitly add a dependency on either xnnpack_enabled or xnnpack_disabled library, just in the same way as with other delegates (i. e. nnapi).

What do you think?"
59912,ValueError: setting an array element with a sequence.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.7.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
ValueError: setting an array element with a sequence.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np


a=np.ones([64])
b=np.ones([64])


class assign:
    def __init__(self):
        self.c=np.ones([64])
    
    
    def assign(self,d):
        for i in range(len(self.c)):
            self.c[i]=d[i]


def sub(a,b,assign_object):
    d=a-b
    assign_object.assign(d)


@tf.function
def f(a,b,assign_object):
    sub(a,b,assign_object)


assign_object=assign()
f(a,b,assign_object)
```


### Relevant log output

_No response_</details>"
59911,Resize OP doesn't work in Metal delegate with dynamic shape input in TFLite,"Hello,
Resize OP doesn't work in Metal delegate with dynamic shape input in TFLite. Here is the small example of network which works on cpu but doesn't work with metal delegate with following error:
```
2023-03-06 15:01:06.317779+0400 TFLiteBenchmark[77147:6040306] Initialized TensorFlow Lite runtime.
INFO: Initialized TensorFlow Lite runtime.
2023-03-06 15:01:06.318869+0400 TFLiteBenchmark[77147:6040306] Created TensorFlow Lite delegate for Metal.
INFO: Created TensorFlow Lite delegate for Metal.
2023-03-06 15:01:06.320045+0400 TFLiteBenchmark[77147:6040306] Metal GPU Frame Capture Enabled
2023-03-06 15:01:06.322306+0400 TFLiteBenchmark[77147:6040306] Metal API Validation Enabled
INFO: GPU delegate created.
ERROR: Failed to apply GPU delegate.
```

Network graph:
<img width=""344"" alt=""image"" src=""https://user-images.githubusercontent.com/2921717/223099424-03870ed2-16fc-4e67-a6d9-17ec2fe9c40f.png"">

[test_net.tflite.zip](https://github.com/tensorflow/tensorflow/files/10897648/test_net.tflite.zip)


### System information

-   **iOS 15.1**:
-   **Iphone 11**:
-   **source**:
-   **v2.11.0**:
"
59908,About fine-tune,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hi:
How to replace several layers in the middle of the model during fine-tune？
Thanks
```


### Standalone code to reproduce the issue

```shell
How to replace several layers in the middle of the model during fine-tune？
```


### Relevant log output

_No response_</details>"
59906,Resource exhausted error during training,"Resource exhausted error during training
I would like to train the deep learning task semantic segmentation.
Specifications: model with 31 million  parameters
image size ( minimum 1024x1024), batch size 256 framework -tensorflow keras 2.7
Total image samples 30,000,
Label Mask samples 30,000
Does colab pro plus allow the training for this specification with out OOM error?

Issue currently facing  is even with 2000 number of images and 512x512 size for batch size 32 results in out of memory error in colab pro subscription.
"
59905,Inexistant CUDA support for last version ?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.11

### Custom Code

No

### OS Platform and Distribution

Windows 10 (Build 19045.2673)

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.0.2

### GPU model and memory

_No response_

### Current Behaviour?

```shell
In the last version of Tensorflow it is saying the support for CUDA on Windows is kinda dropped ?

Is it possible to put it back ? I can't make a code work on a older version of Tensorflow and I want to use GPU processing to loose lowest amount of time possible..
```


### Standalone code to reproduce the issue

```shell
It's on configuration.
```


### Relevant log output

```shell
None
```
</details>"
59902,how to convert tensorflow lite using yolov6 .pt file ?,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
59900,Outdated example,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.6.0

### Custom Code

No

### OS Platform and Distribution

Windows 10 (Build 19045.2673)

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.0.2

### GPU model and memory

_No response_

### Current Behaviour?

[label image example](https://github.com/tensorflow/tensorflow/blob/v2.10.0/tensorflow/examples/label_image/main.cc) is outdated


### Standalone code to reproduce the issue

```shell
Not applicable.
```


### Relevant log output

```shell
Not applicable.
```
</details>"
59899,Getting error when calculating entropy for each images in the batch in the input tensor in a custom layer in tensorflow/Keras.,"I am working on a problem in which I have to create a custom layer in keras, which takes, output of a conv layer of a pre-trained model as an input. This custom layer work is to select K best feature maps based on shannon entropy for each images in that input tensor and then outputs the final tensor with k feature maps or each images. So that this output tensor is passed to other conv layer in the model.

Let input tensor from a conv layer has shape = (None, 224,224, 128) and I want to take 64 best feature maps out of 128 based on shannon entropy . So the output tensor shape should be = (None, 224,224, 64).

----------------------------------
System Informations:
python version = '3.9.12'
tensorflow version = `'2.10.0'`
Platform = Windows 11
Running on CPU
---------------------------------

Below is the code snippet :

`               

              class select_k_fmap(tf.keras.layers.Layer):
                    def __init__(self):
                        super(select_k_fmap, self).__init__()

                    def build(self, input_shape):
                        pass

                    def call(self, inputs):
                        """"""
                        tensor shape = (batch_size, img_height, img_width, no. of filters)
                        """"""    
                        shape = tf.shape(inputs)
                        batch_size = shape[0]
                        print('batch size **** ', batch_size)
                        num_filters = shape[-1]
                        img_height = shape[1]
                        img_width = shape[2]

                        k = 4 # no. of best feature maps to select

                        def k_best_fmap(image):
                            """"""
                            returns k best feature maps of an image having n feature maps
                            """"""    
                            def shannon_entropy(feature_map):
                                """"""
                                returns entropy of a image in a input batch
                                """"""
                                value_ranges = [0.0, 1.0] 
                                nbins = 256 
                                histogram_bin_indexes = tf.histogram_fixed_width_bins(image, value_ranges, nbins)
                                _, _, count = tf.unique_with_counts(histogram_bin_indexes) 
                                prob = count/tf.reduce_sum(count)
                                prob = tf.cast(prob, tf.float32)
                                entropy = (-tf.reduce_sum(prob * tf.math.log(prob)))/(tf.math.log(2.0)) 
                                return entropy

                            final_image = tf.zeros_like(image) #shape = (img_height, img_width, no. of filters)
                            entropy = []
                            num_featuremaps = tf.shape(image)[-1]
                            for j in range(int(num_featuremaps)):
                                image_ith_filter = image[:,:,j]
                                image_ith_filter_entropy = shannon_entropy(image_ith_filter)
                                entropy.append(tf.get_static_value(image_ith_filter_entropy).item()) 


                            entropy_array = tf.argsort(entropy, direction='DESCENDING')
                            k_best_entropy_sort_index = tf.get_static_value(entropy_sort_index[:k]).tolist()

                            for index, element in enumerate(k_best_entropy_sort_index):
                                final_image[:,:,index] = image[:,:,element]


                        output_tensor = tf.map_fn(fn=k_best_fmap, elems=inputs)

                        return output_tensor`

But when I ran this code I got error :

![Screenshot (890)](https://user-images.githubusercontent.com/127013167/222950090-be802ac6-26c4-4a33-8f5d-106d3f800fb4.png)


I have tried many solutions available on the internet. But nothing worked.
I think this error is due to the shape of input tensor having None as first value in its shape (None, 224,224, 128). But I am unable to resolve this error. I want to wok on dynamic batch tensor as input .

It would be a great help for me if anyone of you could assist me. Thanks in advance."
59897,Error to train or load data on RTX 4080,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.13

### Custom Code

Yes

### OS Platform and Distribution

WSL 2

### Mobile device

22.04

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2.0/8.1.0

### GPU model and memory

RTX 4080

### Current Behaviour?

```shell
The error occurs at the time of performing the workouts or loading data with the tf.Dataset API.

At first I tried with my native Windows environment, I followed the installation instructions on the tensorflow page but I got an unexplainable error, the kernel just died.

Finally I tried with the nightly version and the error I have now is supposedly a memory error, which seems strange to me since I tried downloading the batch and nothing. Then I tried running a very small bechmark that I have run with my previous GPU and I get the same error (the benchmark is this one: https://github.com/mrdbourke/m1-machine-learning-test).
```


### Standalone code to reproduce the issue

```shell
GPU RTX 4080
Tensorflow nightly (2.13)
Jupyter notebook to execute: https://github.com/mrdbourke/m1-machine-learning-test/blob/main/00_cifar10_tinyvgg_benchmark.ipynb
```


### Relevant log output

```shell
This output error was running o windows native (tensorflow 2.10): kernel just died


info 16:03:03.770: Got new session 6aead894-0d87-44cf-ba16-f4a714a3ce8f
info 16:03:03.770: Started new restart session
info 16:03:03.777: Dispose Kernel process 14984.
warn 16:03:10.821: StdErr from Kernel Process 2023-03-04 16:03:10.821877: I tensorflow/
warn 16:03:10.821: StdErr from Kernel Process core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.

warn 16:03:11.207: StdErr from Kernel Process 2023-03-04 16:03:11.207450: I tensorflow
warn 16:03:11.207: StdErr from Kernel Process /core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13402 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4080, pci bus id: 0000:08:00.0, compute capability: 8.9

error 16:03:11.949: Disposing session as kernel process died ExitCode: 3221226505, Reason: c:\Users\emman\anaconda3\envs\tf-env\lib\site-packages\traitlets\traitlets.py:2548: FutureWarning: Supporting extra quotes around strings is deprecated in traitlets 5.0. You can use 'hmac-sha256' instead of '""hmac-sha256""' if you require traitlets >=5.
  warn(
c:\Users\emman\anaconda3\envs\tf-env\lib\site-packages\traitlets\traitlets.py:2499: FutureWarning: Supporting extra quotes around Bytes is deprecated in traitlets 5.0. Use '17823775-70bc-4db8-8377-3a19f2263309' instead of 'b""17823775-70bc-4db8-8377-3a19f2263309""'.
  warn(
2023-03-04 16:03:10.821877: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-04 16:03:11.207450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13402 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4080, pci bus id: 0000:08:00.0, compute capability: 8.9

info 16:03:11.950: Dispose Kernel process 20040.
error 16:03:11.951: Raw kernel process exited code: 3221226505
error 16:03:11.954: Error in waiting for cell to complete Error: Canceled future for execute_request message before replies were done
    at t.KernelShellFutureHandler.dispose (c:\Users\emman\.vscode\extensions\ms-toolsai.jupyter-2023.2.1000592019\out\extension.node.js:2:33213)
    at c:\Users\emman\.vscode\extensions\ms-toolsai.jupyter-2023.2.1000592019\out\extension.node.js:2:52265
    at Map.forEach (<anonymous>)
    at y._clearKernelState (c:\Users\emman\.vscode\extensions\ms-toolsai.jupyter-2023.2.1000592019\out\extension.node.js:2:52250)
    at y.dispose (c:\Users\emman\.vscode\extensions\ms-toolsai.jupyter-2023.2.1000592019\out\extension.node.js:2:45732)
    at c:\Users\emman\.vscode\extensions\ms-toolsai.jupyter-2023.2.1000592019\out\extension.node.js:17:127079
    at ee (c:\Users\emman\.vscode\extensions\ms-toolsai.jupyter-2023.2.1000592019\out\extension.node.js:2:1552543)
    at lh.dispose (c:\Users\emman\.vscode\extensions\ms-toolsai.jupyter-2023.2.1000592019\out\extension.node.js:17:127055)
    at ph.dispose (c:\Users\emman\.vscode\extensions\ms-toolsai.jupyter-2023.2.1000592019\out\extension.node.js:17:134354)
    at processTicksAndRejections (node:internal/process/task_queues:96:5)
warn 16:03:11.956: Cell completed with errors {
  message: 'Canceled future for execute_request message before replies were done'
}
info 16:03:11.959: Cancel all remaining cells true || Error || undefined
```


This output error was running on WSL 2 (tensorflow nightly 2.13): ail to allocate memory even when the GPU memory is free (16 GB)
```
2023-03-04 22:55:38.084081: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-04 22:55:38.084850: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-03-04 22:55:38.084914: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-03-04 22:55:38.084958: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-03-04 22:55:38.807671: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-03-04 22:55:38.808089: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-03-04 22:55:38.808116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.
2023-03-04 22:55:38.808171: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-03-04 22:55:38.808390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13344 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4080, pci bus id: 0000:08:00.0, compute capability: 8.9
2023-03-04 22:55:39.164350: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1228800000 exceeds 10% of free system memory.
2023-03-04 22:55:39.904405: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:735] failed to allocate 13.03G (13992198144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-03-04 22:55:40.043405: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:735] failed to allocate 11.73G (12592977920 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-03-04 22:55:40.185360: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:735] failed to allocate 10.55G (11333680128 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-03-04 22:55:40.328259: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:735] failed to allocate 9.50G (10200311808 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-03-04 22:55:40.651989: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1228800000 exceeds 10% of free system memory.
```
</details>"
59890,NCCLAllReduce fails with CUDA 12.0 and CuDNN 8.8.0 when using MirroredStrategy,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

v2.12.0-rc0

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.10.9

### Bazel version

5.3.0

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

12.0/8.8.0

### GPU model and memory

NVIDIA Tesla V100-SXM2 with 16GB

### Current Behaviour?

```
NCCLAllReduce when used in MirrorStrategy causes a crash.
ReductionToOneDevice in MirrorStrategy works fine.
I believe the drivers, CUDA, and CuDNN installations are not an issue because
NCCLAllReduce works fine when using PyTorch.

I have attached relevant code and log from it.

I have built tensorflow from source and there were no errors generated in build process either.

Using TF nightly does not recognize GPUs so I was not able to reproduce the same bug.
```


### Standalone code to reproduce the issue

[Link](https://github.com/tensorflow/docs/blob/9b5b8b055e29d006b94b5dfc906d774ee4094bae/site/en/tutorials/distribute/custom_training.ipynb) from tensorflow's tutorial which can be used for bug replication.


### Relevant log output

```python
2023-03-03 13:12:14.345311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14575 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:04:00.0, compute capability: 7.0
2023-03-03 13:12:14.346201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 14575 MB memory:  -> device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:06:00.0, compute capability: 7.0
2023-03-03 13:12:14.346759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 14575 MB memory:  -> device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:07:00.0, compute capability: 7.0
2023-03-03 13:12:14.347281: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 14575 MB memory:  -> device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:08:00.0, compute capability: 7.0



2023-03-03 13:12:17.150486: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [60000,28,28,1]
	 [[{{node Placeholder/_0}}]]
2023-03-03 13:12:17.150814: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype uint8 and shape [60000]
	 [[{{node Placeholder/_1}}]]
INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2023-03-03 13:12:22.182639: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8800
2023-03-03 13:12:22.192166: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8800
2023-03-03 13:12:22.213909: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8800
2023-03-03 13:12:22.227182: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8800
2023-03-03 13:12:23.789883: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory
2023-03-03 13:12:24.937923: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at nccl_ops.cc:108 : UNKNOWN: Error invoking NCCL: unhandled cuda error
2023-03-03 13:12:24.937999: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at nccl_ops.cc:108 : UNKNOWN: Error invoking NCCL: unhandled cuda error
2023-03-03 13:12:24.938051: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:GPU:1] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): UNKNOWN: Error invoking NCCL: unhandled cuda error
	 [[{{node NcclAllReduce_1}}]]
2023-03-03 13:12:24.938075: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at nccl_ops.cc:108 : UNKNOWN: Error invoking NCCL: unhandled cuda error
2023-03-03 13:12:24.938105: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:GPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): UNKNOWN: Error invoking NCCL: unhandled cuda error
	 [[{{node NcclAllReduce}}]]
2023-03-03 13:12:24.938124: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at nccl_ops.cc:108 : UNKNOWN: Error invoking NCCL: unhandled cuda error
2023-03-03 13:12:24.938175: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:GPU:3] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): UNKNOWN: Error invoking NCCL: unhandled cuda error
	 [[{{node NcclAllReduce_3}}]]
2023-03-03 13:12:24.938202: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): UNKNOWN: Error invoking NCCL: unhandled cuda error
	 [[{{node NcclAllReduce_1}}]]
	 [[Identity_5/_92]]
2023-03-03 13:12:24.938224: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:GPU:2] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): UNKNOWN: Error invoking NCCL: unhandled cuda error
	 [[{{node NcclAllReduce_2}}]]
Output exceeds the size limit. Open the full output data in a text editor
---------------------------------------------------------------------------
UnknownError                              Traceback (most recent call last)
Cell In[14], line 18
     16 num_batches = 0
     17 for x in train_dist_dataset:
---> 18   total_loss += distributed_train_step(x)
     19   num_batches += 1
     20 train_loss = total_loss / num_batches

File /home/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback..error_handler(*args, **kwargs)
    151 except Exception as e:
    152   filtered_tb = _process_traceback_frames(e.__traceback__)
--> 153   raise e.with_traceback(filtered_tb) from None
    154 finally:
    155   del filtered_tb

File /home/anaconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     50 try:
     51   ctx.ensure_initialized()
---> 52   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     53                                       inputs, attrs, num_outputs)
     54 except core._NotOkStatusException as e:
     55   if name is not None:

UnknownError: Graph execution error:

Detected at node 'NcclAllReduce' defined at (most recent call last):
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/runpy.py"", line 86, in _run_code
      exec(code, run_globals)
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/ipykernel_launcher.py"", line 17, in 
      app.launch_new_instance()
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/traitlets/config/application.py"", line 992, in launch_instance
      app.start()
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/ipykernel/kernelapp.py"", line 711, in start
      self.io_loop.start()
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/tornado/platform/asyncio.py"", line 215, in start
      self.asyncio_loop.run_forever()
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever
      self._run_once()
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/asyncio/base_events.py"", line 1906, in _run_once
      handle._run()
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/asyncio/events.py"", line 80, in _run
      self._context.run(self._callback, *self._args)
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 510, in dispatch_queue
      await self.process_one()
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 499, in process_one
      await dispatch(*args)
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 406, in dispatch_shell
      await result
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 729, in execute_request
      reply_content = await reply_content
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/ipykernel/ipkernel.py"", line 411, in do_execute
      res = shell.run_cell(
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/ipykernel/zmqshell.py"", line 531, in run_cell
      return super().run_cell(*args, **kwargs)
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 2961, in run_cell
      result = self._run_cell(
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3016, in _run_cell
      result = runner(coro)
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/IPython/core/async_helpers.py"", line 129, in _pseudo_sync_runner
      coro.send(None)
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3221, in run_cell_async
      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3400, in run_ast_nodes
      if await self.run_code(code, result, async_=asy):
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3460, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File ""/tmp/ipykernel_16086/4166678551.py"", line 18, in 
      total_loss += distributed_train_step(x)
    File ""/tmp/ipykernel_16086/4166678551.py"", line 5, in distributed_train_step
      per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/keras/optimizers/utils.py"", line 175, in _all_reduce_sum_fn
      return distribution.extended.batch_reduce_to(
Node: 'NcclAllReduce'
Detected at node 'NcclAllReduce_3' defined at (most recent call last):
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/runpy.py"", line 86, in _run_code
      exec(code, run_globals)
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/ipykernel_launcher.py"", line 17, in 
      app.launch_new_instance()
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/traitlets/config/application.py"", line 992, in launch_instance
      app.start()
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/ipykernel/kernelapp.py"", line 711, in start
      self.io_loop.start()
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/tornado/platform/asyncio.py"", line 215, in start
      self.asyncio_loop.run_forever()
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever
      self._run_once()
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/asyncio/base_events.py"", line 1906, in _run_once
      handle._run()
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/asyncio/events.py"", line 80, in _run
      self._context.run(self._callback, *self._args)
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 510, in dispatch_queue
      await self.process_one()
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 499, in process_one
      await dispatch(*args)
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 406, in dispatch_shell
      await result
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 729, in execute_request
      reply_content = await reply_content
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/ipykernel/ipkernel.py"", line 411, in do_execute
      res = shell.run_cell(
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/ipykernel/zmqshell.py"", line 531, in run_cell
      return super().run_cell(*args, **kwargs)
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 2961, in run_cell
      result = self._run_cell(
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3016, in _run_cell
      result = runner(coro)
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/IPython/core/async_helpers.py"", line 129, in _pseudo_sync_runner
      coro.send(None)
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3221, in run_cell_async
      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3400, in run_ast_nodes
      if await self.run_code(code, result, async_=asy):
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3460, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File ""/tmp/ipykernel_16086/4166678551.py"", line 18, in 
      total_loss += distributed_train_step(x)
    File ""/tmp/ipykernel_16086/4166678551.py"", line 5, in distributed_train_step
      per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/keras/optimizers/utils.py"", line 175, in _all_reduce_sum_fn
      return distribution.extended.batch_reduce_to(
Node: 'NcclAllReduce_3'
Detected at node 'NcclAllReduce_2' defined at (most recent call last):
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/runpy.py"", line 86, in _run_code
      exec(code, run_globals)
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/ipykernel_launcher.py"", line 17, in 
      app.launch_new_instance()
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/traitlets/config/application.py"", line 992, in launch_instance
      app.start()
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/ipykernel/kernelapp.py"", line 711, in start
      self.io_loop.start()
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/tornado/platform/asyncio.py"", line 215, in start
      self.asyncio_loop.run_forever()
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever
      self._run_once()
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/asyncio/base_events.py"", line 1906, in _run_once
      handle._run()
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/asyncio/events.py"", line 80, in _run
      self._context.run(self._callback, *self._args)
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 510, in dispatch_queue
      await self.process_one()
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 499, in process_one
      await dispatch(*args)
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 406, in dispatch_shell
      await result
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 729, in execute_request
      reply_content = await reply_content
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/ipykernel/ipkernel.py"", line 411, in do_execute
      res = shell.run_cell(
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/ipykernel/zmqshell.py"", line 531, in run_cell
      return super().run_cell(*args, **kwargs)
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 2961, in run_cell
      result = self._run_cell(
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3016, in _run_cell
      result = runner(coro)
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/IPython/core/async_helpers.py"", line 129, in _pseudo_sync_runner
      coro.send(None)
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3221, in run_cell_async
      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3400, in run_ast_nodes
      if await self.run_code(code, result, async_=asy):
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3460, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File ""/tmp/ipykernel_16086/4166678551.py"", line 18, in 
      total_loss += distributed_train_step(x)
    File ""/tmp/ipykernel_16086/4166678551.py"", line 5, in distributed_train_step
      per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/keras/optimizers/utils.py"", line 175, in _all_reduce_sum_fn
      return distribution.extended.batch_reduce_to(
Node: 'NcclAllReduce_2'
Detected at node 'NcclAllReduce_1' defined at (most recent call last):
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/runpy.py"", line 86, in _run_code
      exec(code, run_globals)
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/ipykernel_launcher.py"", line 17, in 
      app.launch_new_instance()
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/traitlets/config/application.py"", line 992, in launch_instance
      app.start()
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/ipykernel/kernelapp.py"", line 711, in start
      self.io_loop.start()
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/site-packages/tornado/platform/asyncio.py"", line 215, in start
      self.asyncio_loop.run_forever()
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever
      self._run_once()
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/asyncio/base_events.py"", line 1906, in _run_once
      handle._run()
    File ""/home/anaconda3/envs/tf_env/lib/python3.10/asyncio/events.py"", line 80, in _run
      self._context.run(self._callback, *self._args)
...
	 [[Identity_5/_92]]
  (4) UNKNOWN:  Error invoking NCCL: unhandled cuda error
	 [[{{node NcclAllReduce_1}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_distributed_train_step_4197]
```
</details>"
59888,GPU:0 is joining a group with incompatible device typeCPU,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf2.9

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tensorflow.python.framework.errors_impl.InternalError: Collective ops is aborted by: Device /job:worker/replica:0/task:0/device:GPU:0 is joining a group with incompatible device typeCPU (group_key=3)
```


### Standalone code to reproduce the issue

```shell
Observed the above errors twice a week. Can you explain what's the root cause and how to fix it?
```


### Relevant log output

_No response_</details>"
59887,compile tensorflow_cc.so with c++17 failed with cuda support,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.8.4

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

gcc (Debian 8.3.0-6) 8.3.0

### CUDA/cuDNN version

11.4

### GPU model and memory

Tesla T4

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
From source file;

tag: v2.8.4
```


### Relevant log output

```shell
build cmd: bazel build --config=cuda --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --cxxopt=-std=c++17 //tensorflow:libtensorflow_cc.so

Report error: Linking tensorflow/libtensorflow_cc.so.2.8.4 failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc @bazel-out/k8-opt/bin/tensorflow/libtensorflow_cc.so.2.8.4-2.params
/usr/bin/ld: bazel-out/k8-opt/bin/tensorflow/core/kernels/libunique_op_gpu.pic.lo(unique_op_gpu_0.cu.pic.o): in function `tensorflow::kernel_factory::OpKernelRegistrar::OpKernelRegistrar(tensorflow::KernelDef const*, absl::lts_20210324::string_view, tensorflow::OpKernel* (*)(tensorflow::OpKernelConstruction*))':
tmpxft_00003846_00000000-6_unique_op_gpu_0.cu.cudafe1.cpp:(.text._ZN10tensorflow14kernel_factory17OpKernelRegistrarC2EPKNS_9KernelDefEN4absl12lts_2021032411string_viewEPFPNS_8OpKernelEPNS_20OpKernelConstructionEE[_ZN10tensorflow14kernel_factory17OpKernelRegistrarC5EPKNS_9KernelDefEN4absl12lts_2021032411string_viewEPFPNS_8OpKernelEPNS_20OpKernelConstructionEE]+0x50): undefined reference to `tensorflow::kernel_factory::OpKernelRegistrar::InitInternal(tensorflow::KernelDef const*, absl::lts_20210324::string_view, std::unique_ptr<tensorflow::kernel_factory::OpKernelFactory, std::default_delete<tensorflow::kernel_factory::OpKernelFactory> >)'
/usr/bin/ld: bazel-out/k8-opt/bin/tensorflow/core/kernels/libunique_op_gpu.pic.lo(unique_op_gpu_0.cu.pic.o): in function `tensorflow::Status tensorflow::errors::Internal<char const*, char const*>(char const*, char const*)':
tmpxft_00003846_00000000-6_unique_op_gpu_0.cu.cudafe1.cpp:(.text._ZN10tensorflow6errors8InternalIJPKcS3_EEENS_6StatusEDpT_[_ZN10tensorflow6errors8InternalIJPKcS3_EEENS_6StatusEDpT_]+0xba): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_20210324::string_view, std::vector<tensorflow::StackFrame, std::allocator<tensorflow::StackFrame> >&&)'
/usr/bin/ld: bazel-out/k8-opt/bin/tensorflow/core/kernels/libunique_op_gpu.pic.lo(unique_op_gpu_0.cu.pic.o): in function `tensorflow::Status tensorflow::errors::Internal<char const*, unsigned long, char const*, char const*>(char const*, unsigned long, char const*, char const*)':
tmpxft_00003846_00000000-6_unique_op_gpu_0.cu.cudafe1.cpp:(.text._ZN10tensorflow6errors8InternalIJPKcmS3_S3_EEENS_6StatusEDpT_[_ZN10tensorflow6errors8InternalIJPKcmS3_S3_EEENS_6StatusEDpT_]+0x427): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_20210324::string_view, std::vector<tensorflow::StackFrame, std::allocator<tensorflow::StackFrame> >&&)'
/usr/bin/ld: bazel-out/k8-opt/bin/tensorflow/core/kernels/libdynamic_partition_op_gpu.pic.lo(dynamic_partition_op_gpu.cu.pic.o): in function `tensorflow::{lambda(tensorflow::KernelDef const*)#1}::operator()(tensorflow::KernelDef const*) const::{lambda(tensorflow::OpKernelConstruction*)#1}::_FUN(tensorflow::OpKernelConstruction)':
```
</details>"
59886,No way to create new react-native app with tfjs-react-native because of dependency issues???,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tfjs-react-native-0.8.0

### Custom Code

Yes

### OS Platform and Distribution

all

### Mobile device

all

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I hope I have just missed something dumb but I don’t think so as I have been struggling to get tfjs-react-native working for the last two days.

version 0.8.0 of this library seems to be unacceptably out of date as far as its dependencies go resulting in it not being possible.....as far as I can tell.....to instantiate a new react-native / expo app and add it as a dependency.  You have to jump through some major hoops to find expo and react SDK versions that might possibly support the versions of react and react-native this library expects and even then it still won't work for me.

There is at least 1 other ticket #7323 mentioning the fact that the libraries dependencies are so out of date that it looks like the library is not maintained at all.

The response to that ticket seems to be 'no it is maintained we will update it sometime'.  Can this please get resolved?
```


### Standalone code to reproduce the issue

```shell
follow this tutorial to create a new expo app https://docs.expo.dev/get-started/create-a-new-app/

then do npm i @tensorflow/tfjs-react-native

It will refuse to install due to conflicting react and react-native dependencies
```


### Relevant log output

_No response_</details>"
59884,LinearOperatorInversion gives weird result on GPU for linear operators,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.13.0-dev20230222

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04.4 LTS

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

8.2.4

### GPU model and memory

_No response_

### Current Behaviour?

```shell
LinearOperatorInversion gives wrong result on GPU for linear operators. In the codes below, `y` should be an identity matrix.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

n = 3
x = np.arange(n*n, dtype=np.float32)
x = np.reshape(x, [n,n])
p = np.dot(x.transpose(), x)

# Linear Operator
op = tf.linalg.LinearOperatorFullMatrix(p)

# Define a function to invert the operator
inv_op = tf.linalg.LinearOperatorInversion(op)

# Get the inverse operator
y = inv_op.matmul(op.to_dense())
print(y)
```


### Relevant log output

```shell
tf.Tensor(
[[ 1.   0.5 -0. ]
 [ 0.   0.   0. ]
 [ 0.   0.5  1. ]], shape=(3, 3), dtype=float32)
```
</details>"
59883,some errors,"I try to generate the ESP32 example project, get the generated tfmicro library and example model, in the Tensorflow directory, run:
make -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_hello_world_esp_project


bash: /c/Users/lenovo/.profile: is a directory

lenovo@LAPTOP-9T3UKCR6 MINGW64 /d/Embedded_deep_learning/ESP32/tensorflow (master)
$ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_hello_world_esp_project
tensorflow/lite/micro/tools/make/downloads/flatbuffers already exists, skipping the download.
tensorflow/lite/micro/tools/make/downloads/kissfft already exists, skipping the download.
tensorflow/lite/micro/tools/make/downloads/pigweed already exists, skipping the download.
process_begin: CreateProcess(D:\NewProgram\Git\Git\usr\bin\sh.exe, D:/NewProgram/Git/Git/usr/bin/sh.exe -c ""python3 tensorflow/lite/micro/tools/make/specialize_files.py --base_files \""tensorflow/lite/micro/flatbuffer_utils.cc tensorflow/lite/micro/micro_log.cc tensorflow/lite/micro/micro_interpreter.cc tensorflow/lite/micro/system_setup.cc tensorflow/lite/micro/all_ops_resolver.cc tensorflow/lite/micro/micro_context.cc tensorflow/lite/micro/mock_micro_graph.cc tensorflow/lite/micro/micro_graph.cc tensorflow/lite/micro/micro_time.cc tensorflow/lite/micro/micro_allocation_info.cc tensorflow/lite/micro/recording_micro_allocator.cc tensorflow/lite/micro/micro_utils.cc tensorflow/lite/micro/micro_profiler.cc tensorflow/lite/micro/micro_string.cc tensorflow/lite/micro/micro_allocator.cc tensorflow/lite/micro/debug_log.cc tensorflow/lite/micro/fake_micro_context.cc tensorflow/lite/micro/micro_resource_variable.cc tensorflow/lite/micro/test_helpers.cc tensorflow/lite/micro/memory_helpers.cc tensorflow/lite/micro/test_helper_custom_ops.cc tensorflow/lite/micro/arena_allocator/recording_single_arena_buffer_allocator.cc tensorflow/lite/micro/arena_allocator/non_persistent_arena_buffer_allocator.cc tensorflow/lite/micro/arena_allocator/single_arena_buffer_allocator.cc tensorflow/lite/micro/arena_allocator/persistent_arena_buffer_allocator.cc tensorflow/lite/micro/memory_planner/non_persistent_buffer_planner_shim.cc tensorflow/lite/micro/memory_planner/greedy_memory_planner.cc tensorflow/lite/micro/memory_planner/linear_memory_planner.cc tensorflow/lite/micro/tflite_bridge/op_resolver_bridge.cc tensorflow/lite/micro/tflite_bridge/flatbuffer_conversions_bridge.cc tensorflow/lite/micro/tflite_bridge/micro_error_reporter.cc tensorflow/lite/allocation.cc tensorflow/lite/allocation_test.cc tensorflow/lite/arena_planner.cc tensorflow/lite/arena_planner_test.cc tensorflow/lite/c/common_internal.cc tensorflow/lite/c/c_api_for_testing.cc tensorflow/lite/c/c_api_opaque.cc tensorflow/lite/c/c_api_opaque_internal.cc tensorflow/lite/c/c_api_signature_runner_test.cc tensorflow/lite/c/c_test.c tensorflow/lite/c/test_util.cc tensorflow/lite/core/api/error_reporter.cc tensorflow/lite/core/api/error_reporter_test.cc tensorflow/lite/core/api/flatbuffer_conversions.cc tensorflow/lite/core/api/flatbuffer_conversions_test.cc tensorflow/lite/core/api/op_resolver.cc tensorflow/lite/core/api/op_resolver_internal_test.cc tensorflow/lite/core/api/op_resolver_test.cc tensorflow/lite/core/api/tensor_utils.cc tensorflow/lite/core/async/async_signature_runner.cc tensorflow/lite/core/async/async_signature_runner_test.cc tensorflow/lite/core/async/async_subgraph.cc tensorflow/lite/core/async/async_subgraph_test.cc tensorflow/lite/core/async/backend_async_kernel_interface.cc tensorflow/lite/core/async/backend_async_kernel_interface_test.cc tensorflow/lite/core/async/c/task.cc tensorflow/lite/core/async/c/task_test.cc tensorflow/lite/core/async/interop/attribute_map_internal.cc tensorflow/lite/core/async/interop/attribute_map_internal_test.cc tensorflow/lite/core/async/interop/c/attribute_map.cc tensorflow/lite/core/async/interop/c/attribute_map_test.cc tensorflow/lite/core/async/interop/c/constants.cc tensorflow/lite/core/async/interop/c/types.cc tensorflow/lite/core/async/interop/c/types_test.cc tensorflow/lite/core/async/interop/reconcile_fns.cc tensorflow/lite/core/async/interop/reconcile_fns_test.cc tensorflow/lite/core/async/interop/variant.cc tensorflow/lite/core/async/interop/variant_test.cc tensorflow/lite/core/async/task_internal.cc tensorflow/lite/core/async/task_internal_test.cc tensorflow/lite/core/async/testing/test_backend.cc tensorflow/lite/core/c/builtin_op_data_test.cc tensorflow/lite/core/c/common.cc tensorflow/lite/core/c/common_test.cc tensorflow/lite/core/c/c_api.cc tensorflow/lite/core/c/c_api_experimental.cc tensorflow/lite/core/c/c_api_experimental_test.cc tensorflow/lite/core/c/c_api_test.cc tensorflow/lite/core/experimental/acceleration/configuration/c/gpu_plugin.cc tensorflow/lite/core/experimental/acceleration/configuration/c/gpu_plugin_test.cc tensorflow/lite/core/experimental/acceleration/configuration/c/nnapi_plugin.cc tensorflow/lite/core/experimental/acceleration/configuration/c/nnapi_plugin_test.cc tensorflow/lite/core/experimental/acceleration/configuration/c/xnnpack_plugin.cc tensorflow/lite/core/experimental/acceleration/configuration/c/xnnpack_plugin_test.cc tensorflow/lite/core/experimental/acceleration/configuration/delegate_registry.cc tensorflow/lite/core/experimental/acceleration/configuration/stable_delegate_registry.cc tensorflow/lite/core/experimental/acceleration/configuration/stable_delegate_registry_test.cc tensorflow/lite/core/interpreter.cc tensorflow/lite/core/interpreter_builder.cc tensorflow/lite/core/interpreter_experimental.cc tensorflow/lite/core/kernels/register.cc tensorflow/lite/core/model_builder.cc tensorflow/lite/core/model_test.cc tensorflow/lite/core/shims/c/shims_test_util.cc tensorflow/lite/core/shims/jni/jni_utils.cc tensorflow/lite/core/subgraph.cc tensorflow/lite/core/subgraph_test.cc tensorflow/lite/core/tools/verifier.cc tensorflow/lite/core/tools/verifier_internal.cc tensorflow/lite/core/tools/verifier_internal_test.cc tensorflow/lite/core/tools/verifier_test.cc tensorflow/lite/create_op_resolver_with_builtin_ops.cc tensorflow/lite/create_op_resolver_with_selected_ops.cc tensorflow/lite/delegates/coreml/builders/activation_layer_builder.cc tensorflow/lite/delegates/coreml/builders/add_op_builder.cc tensorflow/lite/delegates/coreml/builders/concatenation_op_builder.cc tensorflow/lite/delegates/coreml/builders/convolution_op_builder.cc tensorflow/lite/delegates/coreml/builders/dummy_op_builder.cc tensorflow/lite/delegates/coreml/builders/fully_connected_op_builder.cc tensorflow/lite/delegates/coreml/builders/hardswish_op_builder.cc tensorflow/lite/delegates/coreml/builders/mul_op_builder.cc tensorflow/lite/delegates/coreml/builders/op_builder.cc tensorflow/lite/delegates/coreml/builders/pad_op_builder.cc tensorflow/lite/delegates/coreml/builders/pooling_layer_builder.cc tensorflow/lite/delegates/coreml/builders/reshape_op_builder.cc tensorflow/lite/delegates/coreml/builders/resize_bilinear_op_builder.cc tensorflow/lite/delegates/coreml/builders/softmax_op_builder.cc tensorflow/lite/delegates/coreml/builders/threshold_layer_builder.cc tensorflow/lite/delegates/coreml/builders/util.cc tensorflow/lite/delegates/coreml/builders/util_test.cc tensorflow/lite/delegates/delegate_test.cc tensorflow/lite/delegates/delegate_test_util.cc tensorflow/lite/delegates/external/external_delegate.cc tensorflow/lite/delegates/flex/allowlisted_flex_ops.cc tensorflow/lite/delegates/flex/allowlisted_flex_ops_test.cc tensorflow/lite/delegates/flex/buffer_map.cc tensorflow/lite/delegates/flex/buffer_map_test.cc tensorflow/lite/delegates/flex/buffer_map_util.cc tensorflow/lite/delegates/flex/delegate.cc tensorflow/lite/delegates/flex/delegate_data.cc tensorflow/lite/delegates/flex/delegate_data_test.cc tensorflow/lite/delegates/flex/delegate_symbol.cc tensorflow/lite/delegates/flex/delegate_test.cc tensorflow/lite/delegates/flex/java/src/main/native/flex_delegate_jni.cc tensorflow/lite/delegates/flex/kernel.cc tensorflow/lite/delegates/flex/kernel_test.cc tensorflow/lite/delegates/flex/test_util.cc tensorflow/lite/delegates/flex/tflite_subgraph_execute.cc tensorflow/lite/delegates/flex/util.cc tensorflow/lite/delegates/flex/util_test.cc tensorflow/lite/delegates/gpu/api.cc tensorflow/lite/delegates/gpu/cl/api.cc tensorflow/lite/delegates/gpu/cl/buffer.cc tensorflow/lite/delegates/gpu/cl/buffer_test.cc tensorflow/lite/delegates/gpu/cl/cl_arguments.cc tensorflow/lite/delegates/gpu/cl/cl_arguments_test.cc tensorflow/lite/delegates/gpu/cl/cl_command_queue.cc tensorflow/lite/delegates/gpu/cl/cl_context.cc tensorflow/lite/delegates/gpu/cl/cl_device.cc tensorflow/lite/delegates/gpu/cl/cl_device_test.cc tensorflow/lite/delegates/gpu/cl/cl_event.cc tensorflow/lite/delegates/gpu/cl/cl_image_format.cc tensorflow/lite/delegates/gpu/cl/cl_kernel.cc tensorflow/lite/delegates/gpu/cl/cl_memory.cc tensorflow/lite/delegates/gpu/cl/cl_operation.cc tensorflow/lite/delegates/gpu/cl/cl_program.cc tensorflow/lite/delegates/gpu/cl/default/recordable_queue.cc tensorflow/lite/delegates/gpu/cl/default/util.cc tensorflow/lite/delegates/gpu/cl/egl_sync.cc tensorflow/lite/delegates/gpu/cl/environment.cc tensorflow/lite/delegates/gpu/cl/gl_interop.cc tensorflow/lite/delegates/gpu/cl/gpu_api_delegate.cc tensorflow/lite/delegates/gpu/cl/inference_context.cc tensorflow/lite/delegates/gpu/cl/kernels/add_test.cc tensorflow/lite/delegates/gpu/cl/kernels/cast_test.cc tensorflow/lite/delegates/gpu/cl/kernels/cl_test.cc tensorflow/lite/delegates/gpu/cl/kernels/concat_test.cc tensorflow/lite/delegates/gpu/cl/kernels/converter.cc tensorflow/lite/delegates/gpu/cl/kernels/convolution_transposed_3x3_test.cc tensorflow/lite/delegates/gpu/cl/kernels/convolution_transposed_3x3_thin_test.cc tensorflow/lite/delegates/gpu/cl/kernels/convolution_transposed_4x4_test.cc tensorflow/lite/delegates/gpu/cl/kernels/convolution_transposed_test.cc tensorflow/lite/delegates/gpu/cl/kernels/convolution_transposed_thin_test.cc tensorflow/lite/delegates/gpu/cl/kernels/conv_constants_test.cc tensorflow/lite/delegates/gpu/cl/kernels/conv_generic_test.cc tensorflow/lite/delegates/gpu/cl/kernels/conv_weights_converter_test.cc tensorflow/lite/delegates/gpu/cl/kernels/cumsum_test.cc tensorflow/lite/delegates/gpu/cl/kernels/depthwise_conv_3x3_test.cc tensorflow/lite/delegates/gpu/cl/kernels/depthwise_conv_test.cc tensorflow/lite/delegates/gpu/cl/kernels/elementwise_test.cc tensorflow/lite/delegates/gpu/cl/kernels/fully_connected_test.cc tensorflow/lite/delegates/gpu/cl/kernels/gather_test.cc tensorflow/lite/delegates/gpu/cl/kernels/lstm_full_test.cc tensorflow/lite/delegates/gpu/cl/kernels/lstm_test.cc tensorflow/lite/delegates/gpu/cl/kernels/max_unpooling_test.cc tensorflow/lite/delegates/gpu/cl/kernels/mean_stddev_normalization_test.cc tensorflow/lite/delegates/gpu/cl/kernels/one_hot_test.cc tensorflow/lite/delegates/gpu/cl/kernels/padding_test.cc tensorflow/lite/delegates/gpu/cl/kernels/pooling_test.cc tensorflow/lite/delegates/gpu/cl/kernels/prelu_test.cc tensorflow/lite/delegates/gpu/cl/kernels/quantize_and_dequantize_test.cc tensorflow/lite/delegates/gpu/cl/kernels/reduce_test.cc tensorflow/lite/delegates/gpu/cl/kernels/relu_test.cc tensorflow/lite/delegates/gpu/cl/kernels/resampler_test.cc tensorflow/lite/delegates/gpu/cl/kernels/reshapex4_test.cc tensorflow/lite/delegates/gpu/cl/kernels/reshape_test.cc tensorflow/lite/delegates/gpu/cl/kernels/resize_test.cc tensorflow/lite/delegates/gpu/cl/kernels/select_v2_test.cc tensorflow/lite/delegates/gpu/cl/kernels/softmax1x1_test.cc tensorflow/lite/delegates/gpu/cl/kernels/softmax_test.cc tensorflow/lite/delegates/gpu/cl/kernels/space_to_depth_test.cc tensorflow/lite/delegates/gpu/cl/kernels/split_test.cc tensorflow/lite/delegates/gpu/cl/kernels/strided_slice_test.cc tensorflow/lite/delegates/gpu/cl/kernels/tile_test.cc tensorflow/lite/delegates/gpu/cl/kernels/transpose_test.cc tensorflow/lite/delegates/gpu/cl/kernels/winograd_test.cc tensorflow/lite/delegates/gpu/cl/opencl_wrapper.cc tensorflow/lite/delegates/gpu/cl/program_cache.cc tensorflow/lite/delegates/gpu/cl/qcom_thin_filter.cc tensorflow/lite/delegates/gpu/cl/tensor.cc tensorflow/lite/delegates/gpu/cl/tensor_test.cc tensorflow/lite/delegates/gpu/cl/tensor_type_util.cc tensorflow/lite/delegates/gpu/cl/testing/delegate_testing.cc tensorflow/lite/delegates/gpu/cl/testing/gpu_model_test.cc tensorflow/lite/delegates/gpu/cl/testing/internal_api_samples.cc tensorflow/lite/delegates/gpu/cl/testing/memory_sharing_sample.cc tensorflow/lite/delegates/gpu/cl/testing/performance_profiling.cc tensorflow/lite/delegates/gpu/cl/util.cc tensorflow/lite/tools/versioning/op_version_test.cc tensorflow/lite/tools/versioning/runtime_version.cc tensorflow/lite/tools/versioning/runtime_version_test.cc tensorflow/lite/type_to_tflitetype_test.cc tensorflow/lite/util.cc tensorflow/lite/util_test.cc\"" --specialize_directory tensorflow/lite/micro/esp"", ...) failed.
...........
make: tensorflow/lite/micro/tools/make/Makefile:585: pipe: No such file or directory
tensorflow/lite/micro/tools/make/Makefile:594: *** Error with specialize_files.py .  Stop.

lenovo@LAPTOP-9T3UKCR6 MINGW64 /d/Embedded_deep_learning/ESP32/tensorflow (master)
$
I got the following error, I don't know how to solve the source of the error, hope I can get some valuable advice from you

Thank you"
59879,Tensorflow int8 conversion error for segmentation U-Net,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10 
- TensorFlow installation (pip package or built from source):
pip package
- TensorFlow library (version, if pip package or github SHA, if built from source):
- TensorFlow 2.11.0, pip package


### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option B: Paste your code here

import cv2
import numpy as np
from matplotlib import pyplot as plt
import tensorflow as tf
from tensorflow import keras
from mss import mss
from PIL import Image
import os

def U_Net (o_w, o_h, o_c, t_w, t_h, t_c, num_layers, initial_ch, deep_sup):
    global model
    def encoder(e_in, filters, kernel_size=(3, 3), padding=""same"", strides=1):
        e = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=""relu"")(e_in)
        skip_out = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=""relu"")(e)
        e_out = keras.layers.MaxPool2D((2, 2), (2, 2))(skip_out)
        return e_out, skip_out

    def bridge(bridge_in, filters, kernel_size=(3, 3), padding=""same"", strides=1):
        b = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=""relu"")(bridge_in)
        bridge_out = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=""relu"")(b)
        return bridge_out

    def decoder(d_in, skip_in, filters, kernel_size=(3, 3), padding=""same"", strides=1):
        up_sampled = keras.layers.UpSampling2D((2, 2))(d_in)
        up_sampled_reshaped = tf.keras.layers.experimental.preprocessing.Resizing(int((skip_in.shape[1])*t_h/o_h), int((skip_in.shape[2])*t_w/o_w), interpolation=""bilinear"", name=None)(up_sampled)
        skip_in_reshaped = tf.keras.layers.experimental.preprocessing.Resizing(up_sampled_reshaped.shape[1], up_sampled_reshaped.shape[2], interpolation=""bilinear"", name=None)(skip_in)
        concatenated = keras.layers.Concatenate()([up_sampled_reshaped, skip_in_reshaped])
        d = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=""relu"")(concatenated)
        d_out = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=""relu"")(d)
        return d_out

    inputs = keras.layers.Input((o_h, o_w, o_c))
    dict = {}

    num_layers_str = str(num_layers-1)
    num_layers_minus2_str = str(num_layers-3)

    e_out_end = ""e_out""+num_layers_str
    d_out_end = ""d_out""+num_layers_str
    d_out_end_minus2 = ""d_out""+num_layers_minus2_str

    dict['e_out0'] = inputs
    for i in range(1,(num_layers)):
        dict['e_out%s'%i], dict['skip_out%s'%i] = encoder((dict['e_out%s'%(i-1)]), (0.5*initial_ch*(2**i)))


    bridge_out = bridge((dict[e_out_end]), (0.5*initial_ch*(2**num_layers)))

    dict['d_out0'] = bridge_out

    for n in range(1,(num_layers)):
        dict['d_out%s'%n] = decoder((dict['d_out%s'%(n-1)]), (dict['skip_out%s'%(num_layers-n)]), (0.5*initial_ch*(2**(num_layers-n))))

    outputs = keras.layers.Conv2D(t_c, (1, 1), padding=""same"", activation=""sigmoid"")(dict[d_out_end])

    if deep_sup == 1:
        ds = keras.layers.UpSampling2D((4, 4))(dict[d_out_end_minus2])
        ds = keras.layers.Conv2D(t_c, (1, 1), padding=""same"", activation=""sigmoid"")(ds)
        outputsconcat = tf.keras.layers.concatenate([outputs, ds], axis=1)
        model = keras.models.Model(inputs, outputsconcat)

    if deep_sup == 0:
        model = keras.models.Model(inputs, outputs)

    return model


def modelcreator(o_w, o_h, o_c, t_w, t_h, t_c, model_type, num_layers, initial_ch, deep_sup):
    global current_model
    # For U-Net:
    if model_type == ""U-Net"":
        current_model = U_Net (o_w, o_h, o_c, t_w, t_h, t_c, num_layers, initial_ch, deep_sup)
    return current_model

#################
(o_w, o_h, o_c, t_w, t_h, t_c, model_type, num_layers, initial_ch, deep_sup, gan_onoff) = (500,500,3,500,500,3,""U-Net"",5,16,1,0)

current_model = modelcreator(o_w, o_h, o_c, t_w, t_h, t_c, model_type, num_layers, initial_ch, deep_sup, gan_onoff)

current_model.load_weights(Model_directory)

current_model.summary()

############

image_path = 
target_path = 

image_array = []
target_array = []

for i in os.listdir(image_path):
    

    ## Reading Image
    image = cv2.imread(image_path + i)
    image = cv2.resize(image, (o_w, o_h))
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image = image / 255.0
    
    image_array.append(image)
    
for i in os.listdir(target_path):
    
    ## Reading Mask
    target = cv2.imread(target_path + i)
    target = cv2.resize(target, (o_w, o_h))
    target = cv2.cvtColor(target, cv2.COLOR_BGR2RGB)

    ## Normalizaing
    target = target / 255.0
    
    target = np.array(target)
    target2 = np.array(target)
    targetconcat = np.concatenate((target, target2), axis=1)
    target = targetconcat
    
    target_array.append(target)

representative_data = image_array, target_array

################

def representative_data_gen():
    for i in range(len(image_array)):
        # Preprocess a single image
        img = image_array[i]
        img = np.asarray(img)[None, ...]  # Add batch dimension
        img = img.astype(np.float32)  # Convert to float32
        yield [img]  # Yield the preprocessed image as a list

#################

converter = tf.lite.TFLiteConverter.from_keras_model(current_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8  
converter.inference_output_type = tf.int8 
tflite_quant_model = converter.convert()

#################


### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

Resulting model yields bad results and takes more than 5 min to process one image. 


### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.
INFO:tensorflow:Assets written to: C:\Users\David\AppData\Local\Temp\tmpl1ifyrjo\assets
INFO:tensorflow:Assets written to: C:\Users\David\AppData\Local\Temp\tmpl1ifyrjo\assets
C:\Users\David\AppData\Local\miniconda3\lib\site-packages\tensorflow\lite\python\convert.py:765: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.
  warnings.warn(""Statistics for quantized inputs were expected, but not ""
"
59878,ERROR: tensorflow/tensorflow/compiler/xla/service/gpu/runtime/BUILD:146:11: in deps attribute of cc_library rule //tensorflow/compiler/xla/service/gpu/runtime:gemm: target '//tensorflow/compiler/xla/service/gpu:non_atomically_upgradeable_rw_lock' does not exist,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

5.3.0

### GCC/Compiler version

11.3.0

### CUDA/cuDNN version

12.1/8.8.0.121

### GPU model and memory

NVIDIA GeForce 940MX

### Current Behaviour?

```shell
$ bazel build --config=mkl --config=opt //tensorflow/tools/pip_package:build_pip_package
Extracting Bazel installation...
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=211
INFO: Reading rc options for 'build' from tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=~/Documents/dev/programs/anaconda3/envs/tf/bin/python3 --action_env PYTHON_LIB_PATH=~/Documents/dev/programs/anaconda3/envs/tf/lib/python3.10/site-packages --python_path=~/Documents/dev/programs/anaconda3/envs/tf/bin/python3 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-12.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=5.0 --action_env LD_LIBRARY_PATH=/usr/lib/libreoffice/program:/usr/local/cuda/targets/x86_64-linux/lib:/usr/lib/x86_64-linux-gnu --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-11 --config=cuda
INFO: Reading rc options for 'build' from ~/Documents/dev/git/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file ~/Documents/dev/git/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file ~/Documents/dev/git/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda in file ~/Documents/dev/git/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:mkl in file ~/Documents/dev/git/tensorflow/.bazelrc: --define=build_with_mkl=true --define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 --define=build_with_openmp=true -c opt
INFO: Found applicable config definition build:opt in file ~/Documents/dev/git/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare
INFO: Found applicable config definition build:linux in file ~/Documents/dev/git/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file ~/Documents/dev/git/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/91d765cad5599f9710973d3e34d4dc22583e2e79.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/10939d1d580b9d3c9c2f3539c6bdb39f408179c0.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/curl.haxx.se/download/curl-7.88.0.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/boringssl/archive/b9232f9e27e5668bc0414879dcdedb2a59ea75f2.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/XNNPACK/archive/659147817805d17c7be2d60bd7bbca7e780f9c82.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/nvidia/nccl/archive/v2.16.2-1.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
ERROR: ~/Documents/dev/git/tensorflow/tensorflow/compiler/xla/service/gpu/runtime/BUILD:146:11: in deps attribute of cc_library rule //tensorflow/compiler/xla/service/gpu/runtime:gemm: target '//tensorflow/compiler/xla/service/gpu:non_atomically_upgradeable_rw_lock' does not exist
ERROR: ~/Documents/dev/git/tensorflow/tensorflow/compiler/xla/service/gpu/runtime/BUILD:146:11: Analysis of target '//tensorflow/compiler/xla/service/gpu/runtime:gemm' failed
INFO: Repository cudnn_frontend_archive instantiated at:
  ~/Documents/dev/git/tensorflow/WORKSPACE:15:14: in <toplevel>
  ~/Documents/dev/git/tensorflow/tensorflow/workspace2.bzl:967:21: in workspace
  ~/Documents/dev/git/tensorflow/tensorflow/workspace2.bzl:171:20: in _tf_repositories
  ~/Documents/dev/git/tensorflow/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  ~/Documents/dev/git/tensorflow/third_party/repo.bzl:89:35: in <toplevel>
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: 
INFO: Elapsed time: 92.916s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (602 packages loaded, 36603 targets configured)
    Fetching https://github.com/NVIDIA/cudnn-frontend/archive/refs/tags/v0.7.3.zip; 8,793,160B 8s
```


### Standalone code to reproduce the issue

```shell
./configure
bazel build --config=mkl --config=opt //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

_No response_</details>"
59870,BatchToSpaceND and SpaceToBatchND ERROR_GPU_NOT_COMPATIBLE,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 20.04):
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.10.0

### 2. Code
```
converter = tf.lite.TFLiteConverter.from_saved_model(in_keras_path)
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.
    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.
]
converter.target_spec.experimental_supported_backends = [""GPU""] # if empty, GPU is not enabled

converter.experimental_new_converter = True

converter.optimizations = [tf.lite.Optimize.DEFAULT] #8-bit quantization
converter.allow_custom_ops = True

tflite_quant_model = converter.convert()
```

### 3. Failure after conversion
'BatchToSpaceND' ERROR_GPU_NOT_COMPATIBLE
'SpaceToBatchND' ERROR_GPU_NOT_COMPATIBLE
### 5. (optional) Any other info / logs
Hi everyone! I am having issues converting the U-2-Net model from PyTorch to TFLite for running on Android. My conversion path is PyTorch(pth) -> ONNX -> TensorFlow -> TFLite. An important requirement is that the TFLite model should support GPU execution and needs to be quantized. The main conversion path works fine, but I encountered an error during the TFLite conversion. After some investigation, I found out that if I change the layers with Conv2D parameters dilation > 1 and padding > 1 to dilation=1 and padding=1, the conversion works without any issues. However, this reduces the model's quality. Obviously, if GPU support is disabled, the model can be converted. I have tried using QuantizationDebugOptions and QuantizationDebugger, but it did not yield any results, the error remains the same. Can you please suggest any way to perform this conversion without compromising the model's quality?
"
59869,TypeError: this __dict__ descriptor does not support '_DictWrapper' objects,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

v1.12.1-88869-g80170ee25b4 2.12.0-rc0

### Custom Code

No

### OS Platform and Distribution

WIndows 11

### Mobile device

_No response_

### Python version

3.11.2

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am running the keras training module for mnist from tensorflow_datasets and want to save the model.
this is failing with error message below. I looked at the file thats triggering data_structures.py and noticed the super classes don't have the __getattribute__ method. I do see __getattr__.
Is this a version difference. My site-packages identified wrapt as wrapt-1.15.0-dist-info. Is there a version incompatability? If so, which version do I use?
```


### Standalone code to reproduce the issue

```shell
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf

import tensorflow_datasets as tfds
import keras

(ds_train, ds_test), ds_info = tfds.load(
    'mnist',
    split=['train', 'test'],
    shuffle_files=True,
    as_supervised=True,
    with_info=True,
)

def normalize_img(image, label):
  """"""Normalizes images: `uint8` -> `float32`.""""""
  return tf.cast(image, tf.float32) / 255., label

ds_train = ds_train.map(
    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
ds_train = ds_train.cache()
ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)
ds_train = ds_train.batch(128)
ds_train = ds_train.prefetch(tf.data.AUTOTUNE)

ds_test = ds_test.map(
    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
ds_test = ds_test.batch(128)
ds_test = ds_test.cache()
ds_test = ds_test.prefetch(tf.data.AUTOTUNE)


model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(10)
])
model.compile(
    optimizer=tf.keras.optimizers.Adam(0.001),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],
)


model.fit(
    ds_train,
    epochs=6,
    validation_data=ds_test,
)

tf.saved_model.save(
    model,
    'saved/save_files'
)
```


### Relevant log output

```shell
WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.
Traceback (most recent call last):
  File ""C:\Users\pmats\PycharmProjects\pythonProject\minst.py"", line 52, in <module>
    tf.saved_model.save(
  File ""C:\Users\pmats\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow\python\saved_model\save.py"", line 1240, in save
    save_and_return_nodes(obj, export_dir, signatures, options)
  File ""C:\Users\pmats\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow\python\saved_model\save.py"", line 1276, in save_and_return_nodes
    _build_meta_graph(obj, signatures, options, meta_graph_def))
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\pmats\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow\python\saved_model\save.py"", line 1455, in _build_meta_graph
    return _build_meta_graph_impl(obj, signatures, options, meta_graph_def)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\pmats\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow\python\saved_model\save.py"", line 1408, in _build_meta_graph_impl        
    saveable_view = _SaveableView(augmented_graph_view, options)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\pmats\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow\python\saved_model\save.py"", line 281, in __init__
    self._initialize_save_and_restore_functions()
  File ""C:\Users\pmats\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow\python\saved_model\save.py"", line 303, in _initialize_save_and_restore_functions
    save_util_v1.get_checkpoint_factories_and_keys(self.object_names))
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\pmats\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow\python\checkpoint\save_util_v1.py"", line 75, in get_checkpoint_factories_and_keys
    saveable_object_util.saveable_objects_from_trackable(
  File ""C:\Users\pmats\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow\python\training\saving\saveable_object_util.py"", line 614, in saveable_objects_from_trackable
    if trackable_has_serialize_to_tensor(obj):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\pmats\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow\python\training\saving\saveable_object_util.py"", line 745, in trackable_has_serialize_to_tensor
    if ""_serialize_to_tensors"" in obj.__dict__:
                                  ^^^^^^^^^^^^
  File ""C:\Users\pmats\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow\python\trackable\data_structures.py"", line 823, in __getattribute__      
    return super().__getattribute__(name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: this __dict__ descriptor does not support '_DictWrapper' objects
```
</details>"
59868,Freeze zero weights during training,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Is there any way to freeze zero weights during model retraining? (this is different than freezing a layer which is already available in TF)
This is relevant to model pruning. For example, i want to retrain only non-zero weights of a pruned model. Is it possible to use tf.IndexedSlices efficiently for this?
```


### Standalone code to reproduce the issue

```shell
I think we can achieve this if there's any feature in TF to calculate grads only for non-zero weights. I couldn't find any method to achieve this 

@tf.function
def train_step(inputs, targets):
    with tf.GradientTape() as tape:
       predictions = model(inputs, training=True)
       loss_value = loss_fn(targets, predictions)
    grads = tape.gradient(loss_value, model.trainable_weights)
    optimizer.apply_gradients(zip(grads, model.trainable_weights))
    return loss_value
```


### Relevant log output

_No response_</details>"
59865,Cannot place 2 pre-trained models on separate GPUs,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.4.1

### Custom Code

Yes

### OS Platform and Distribution

centos 7

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

2x tesla P40

### Current Behaviour?

```shell
I expected both vgg16 models to run in parallel on 2 GPUs at near 100% capacity, but nvidia-smi shows  one being completely idle. When I used some matmul in place of pretrained vgg16 (function 'model2') I observe both GPUs utilisation.
```


### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf
from tensorflow.python.keras import Input, Model
from tensorflow.python.keras.applications.vgg16 import VGG16
from tensorflow.python.keras.layers import Dense, GlobalAveragePooling2D

tf.debugging.set_log_device_placement(True)


def model(images):
    with tf.device(""/cpu:0""):
        first, second = tf.split(images, 2)
    with tf.device(""/gpu:0""):
        vgg1 = VGG16(False)
        vgg1._name = ""vgg1""  # without this i get error about layers not having unique names         
        vgg1.trainable = False
        vgged1 = vgg1(first)
    with tf.device(""/gpu:1""):
        vgg2 = VGG16(False)  # it feels like this does not create second model but somehow reuses vgg1 on GPU:0
        vgg2._name = ""vgg2""  
        vgg2.trainable = False
        vgged2 = vgg2(second)
    with tf.device(""/cpu:0""):
        gap = GlobalAveragePooling2D()
        concated = tf.concat([gap(vgged1), gap(vgged2)], 1)
        d10 = Dense(10)(concated)
        return d10

def model2(images):
    flat_size=224*224*3

    with tf.device(""/cpu:0""):
        first, second = tf.split(images, 2)
    with tf.device(""/gpu:0""):
        flat1=tf.reshape(first, [-1, flat_size])
        w1 = tf.Variable(tf.random.uniform([flat_size, 10000]))
        res1= tf.matmul(flat1, w1)
    with tf.device(""/gpu:1""):
        flat2 = tf.reshape(second, [-1, flat_size])
        w2 = tf.Variable(tf.random.uniform([flat_size, 10000]))
        res2 = tf.matmul(flat2, w2)
    with tf.device(""/cpu:0""):
        concated = tf.concat([res1, res2], 1)
        w3 = tf.Variable(tf.random.uniform([20000, 10]))
        return tf.matmul(concated,w3)


inp = Input((224, 224, 3))
out = model(inp)  # if I change this to ""out = model2(inp)"" both GPUs work

m = Model(inputs=inp, outputs=out)

x = np.random.random([32, 224, 224, 3])

m.summary()

for
m(x)
```


### Relevant log output

```shell
full output available at: https://snippet.host/hyrgbj

2023-02-28 12:41:01.741524: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-02-28 12:41:03.001461: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-28 12:41:03.002735: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2023-02-28 12:41:03.010097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:08:00.0 name: Tesla P40 computeCapability: 6.1
coreClock: 1.531GHz coreCount: 30 deviceMemorySize: 22.38GiB deviceMemoryBandwidth: 323.21GiB/s
2023-02-28 12:41:03.010449: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties:
pciBusID: 0000:84:00.0 name: Tesla P40 computeCapability: 6.1
coreClock: 1.531GHz coreCount: 30 deviceMemorySize: 22.38GiB deviceMemoryBandwidth: 323.21GiB/s
2023-02-28 12:41:03.010499: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-02-28 12:41:03.017621: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-28 12:41:03.017668: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-02-28 12:41:03.052369: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2023-02-28 12:41:03.053152: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2023-02-28 12:41:03.055905: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2023-02-28 12:41:03.057101: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2023-02-28 12:41:03.057265: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2023-02-28 12:41:03.063035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1
2023-02-28 12:41:03.063516: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-28 12:41:03.069400: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-28 12:41:03.329039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:08:00.0 name: Tesla P40 computeCapability: 6.1
coreClock: 1.531GHz coreCount: 30 deviceMemorySize: 22.38GiB deviceMemoryBandwidth: 323.21GiB/s
2023-02-28 12:41:03.329290: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties:
pciBusID: 0000:84:00.0 name: Tesla P40 computeCapability: 6.1
coreClock: 1.531GHz coreCount: 30 deviceMemorySize: 22.38GiB deviceMemoryBandwidth: 323.21GiB/s
2023-02-28 12:41:03.329353: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-02-28 12:41:03.329389: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-28 12:41:03.329408: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-02-28 12:41:03.329443: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2023-02-28 12:41:03.329473: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2023-02-28 12:41:03.329491: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2023-02-28 12:41:03.329517: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2023-02-28 12:41:03.329537: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2023-02-28 12:41:03.330198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1
2023-02-28 12:41:03.330243: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-02-28 12:41:04.083881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2023-02-28 12:41:04.083958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1
2023-02-28 12:41:04.083972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N N
2023-02-28 12:41:04.083981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   N N
2023-02-28 12:41:04.085081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21298 MB memory) -> physical GPU (device: 0, name: Tesla P40, pci bus id: 0000:08:00.0, compute capability: 6.1)
2023-02-28 12:41:04.085905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 21298 MB memory) -> physical GPU (device: 1, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1)
2023-02-28 12:41:04.106894: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.311401: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.311641: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.311778: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Add in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.312103: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.312365: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.313784: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.313966: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.314080: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.321015: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.321222: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.321295: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.321356: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Add in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.321509: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.321620: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.321957: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.322085: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.322183: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.329542: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.329740: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.329821: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.329882: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Add in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.330035: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.330153: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.330489: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.330650: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.330757: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.336373: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.336577: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.336651: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.336713: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Add in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.336886: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.336996: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.337341: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.337460: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.337556: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.344730: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.344917: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.344990: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.345050: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Add in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.345212: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.345367: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.345719: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.345882: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.345985: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.351300: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.351501: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.351576: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.351637: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Add in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.351790: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.351907: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.352251: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.352372: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.352472: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.357894: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.357971: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.358047: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.358113: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Add in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.358229: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.358333: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.358666: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.358787: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.358885: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.366153: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.366357: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.366447: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.366507: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Add in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.366668: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.366779: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.367122: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.367288: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.367393: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.372946: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.373388: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.373463: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.373542: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Add in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.373696: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.373806: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.374161: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.374283: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.374399: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.379794: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.379872: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.379950: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.380008: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Add in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.380129: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.380234: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.380564: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.380684: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.380780: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.387982: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.388062: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.388136: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.388195: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Add in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.388309: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.388414: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.388756: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.388879: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.388981: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.394514: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.394591: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.394657: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-28 12:41:04.394724: I tensorflow/core/common_runtime/eager/execute.cc:760] Executing op Add in device /job:localhost/replica:0/task:0/device:GPU:0
```
</details>"
59863,Layer input_spec must be an instance of InputSpec!,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.6

### Custom Code

No

### OS Platform and Distribution

windows

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
from tensorflow.python.keras.engine.input_spec import InputSpec as InputSpec1

def input_spec(self, value):
    for v in tf.nest.flatten(value):
      if v is not None and not isinstance(v, InputSpec):
        #if not isinstance(v, InputSpec1):
        raise TypeError('Layer input_spec must be an instance of InputSpec. '
                        'Got: {}'.format(v))
    self._input_spec = value

isinstance(v, InputSpec) return false when the v is InputSpec(shape=(None, 500, 768), ndim=3), after adding if not isinstance(v, InputSpec1),  the assert is fixed. 

Anyway after modifying it, my bert tuning model works fine on GPU on tensflow2.6 on python 3.9.

Thanks for the reply for correct solution.
```


### Standalone code to reproduce the issue

```shell
below is the breakpoint output right before the assert:

v
InputSpec(shape=(None, 500, 768), ndim=3)
type(v)
<class 'tensorflow.python.keras.engine.input_spec.InputSpec'>
InputSpec
<class 'keras.engine.input_spec.InputSpec'>
from tensorflow.python.keras.engine.input_spec import InputSpec as InputSpec1
isinstance(v,InputSpec1)
True
isinstance(v,InputSpec)
False
```


### Relevant log output

_No response_</details>"
59861,disable linkage to nativewindow when API_LEVEL<26,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

latest/nightly

### Custom Code

No

### OS Platform and Distribution

Android

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

5.3.0

### GCC/Compiler version

clang 9

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
in file `tensorflow/lite/delegates/gpu/build_defs.bzl` line 12 it returns a link option of ""-lnativewindow"" by `return [""-lnativewindow""]`, but this option is only available for ndk api_level >=26. 
would be nice if a condition can be set here with something like:

if api_level >=26:
    return [""-lnativewindow""]
else:
    return []
```
```


### Standalone code to reproduce the issue

```shell
set api_level=21, and build tflite_gpu_delegation it will fail with `cannot find -lnativewindow`
```


### Relevant log output

_No response_</details>"
59857,[StableHLO/MHLO] `tf-lower-to-mlprogram-and-hlo`,"### Tensorflow Version

2.13.0-dev20230301 - TF Nightly

### Current Behaviour?

StableHLO / MHLO seems to not be able to convert the `ReadVariableOP` which is a big problem because it prevents to convert any `tf.function` to StableHLO/MHLO which is not first frozen.

You can find a colab here: https://colab.research.google.com/drive/1NrKZ3tihyYR9hj5r9Tfp5cPiirS-nF5u

### Standalone code to reproduce the issue

```python
import tensorflow as tf
from tensorflow.python import pywrap_mlir

USE_FREEZING = False   # Set it to `True` and the problem disappears

def generate_model():
  return tf.keras.models.Sequential([
    tf.keras.layers.Input(shape=(32, 32, 3)),
    tf.keras.layers.Conv2D(2, 3, activation='relu', padding=""same"")
  ])

model = generate_model()
model.trainable = False  # Freeze the outer model

@tf.function()
def infer_func(data):
  return model(data, training=False)

concrete_infer_func = infer_func.get_concrete_function(
    tf.TensorSpec((32, 32, 32, 3), tf.float32)
)

if USE_FREEZING:
    from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2_as_graph
    frozen_func, gdef = convert_variables_to_constants_v2_as_graph(
        func=concrete_infer_func,
        lower_control_flow=True, 
        aggressive_inlining=False
    )
else:
  frozen_func = concrete_infer_func


def convert_to_hlo(concrete_function, use_stablehlo=True):
  
  pipelines = [""tf-lower-to-mlprogram-and-hlo""]
  if not use_stablehlo:
    pipelines.append(""stablehlo-legalize-to-hlo"")

  return pywrap_mlir.import_function(
      concrete_function, 
      pass_pipeline="","".join(pipelines),
      show_debug_info=False
  )

# ---------------------------------------------------------------
# Convert saved model to Stablehlo + upstream MLIR ops
# ---------------------------------------------------------------

WORKFLOW_STEPS = [
  (""mhlo"", False),
  (""stablehlo"", True),
]

for extension, use_stablehlo in WORKFLOW_STEPS:
    with open(f""model.{extension}"", ""w"") as f:
      hlo_str = convert_to_hlo(
        frozen_func, 
        use_stablehlo=use_stablehlo
      )

      f.write(hlo_str)

    import hashlib
    checksum = hashlib.md5(open(f""model.{extension}"",""rb"").read()).hexdigest()
    print(f""File: model.{extension:10s}: {checksum}"")
```


### Relevant log output

As you can see below, they both contain:
- `""tf.ReadVariableOp""`
- `tf.Conv2D`

And these are clearly not part of the StableHLO/MHLO language

```shell
$ cat model.stablehlo

module {
  func.func @__inference_infer_func_49(%arg0: tensor<32x32x32x3xf32>, %arg1: tensor<*x!tf_type.resource>, %arg2: tensor<*x!tf_type.resource>) -> tensor<32x?x?x?xf32> {
    %0 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %1 = ""tf.ReadVariableOp""(%arg2) {device = """"} : (tensor<*x!tf_type.resource>) -> tensor<*xf32>
    %2 = ""tf.ReadVariableOp""(%arg1) {device = """"} : (tensor<*x!tf_type.resource>) -> tensor<*xf32>
    %3 = ""tf.Conv2D""(%arg0, %2) {data_format = ""NHWC"", device = """", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = ""SAME"", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<32x32x32x3xf32>, tensor<*xf32>) -> tensor<32x?x?x?xf32>
    %4 = shape.shape_of %3 : tensor<32x?x?x?xf32> -> tensor<4xindex>
    %5 = stablehlo.dynamic_broadcast_in_dim %1, %4, dims = [3] : (tensor<*xf32>, tensor<4xindex>) -> tensor<32x?x?x?xf32>
    %6 = stablehlo.add %3, %5 : tensor<32x?x?x?xf32>
    %7 = shape.shape_of %6 : tensor<32x?x?x?xf32> -> tensor<4xindex>
    %8 = stablehlo.dynamic_broadcast_in_dim %0, %7, dims = [] : (tensor<f32>, tensor<4xindex>) -> tensor<32x?x?x?xf32>
    %9 = stablehlo.maximum %8, %6 : tensor<32x?x?x?xf32>
    return %9 : tensor<32x?x?x?xf32>
  }
}
```

```shell
$ cat model.mhlo

module {
  func.func @__inference_infer_func_49(%arg0: tensor<32x32x32x3xf32>, %arg1: tensor<*x!tf_type.resource>, %arg2: tensor<*x!tf_type.resource>) -> tensor<32x?x?x?xf32> {
    %0 = mhlo.constant dense<0.000000e+00> : tensor<f32>
    %1 = ""tf.ReadVariableOp""(%arg2) {device = """"} : (tensor<*x!tf_type.resource>) -> tensor<*xf32>
    %2 = ""tf.ReadVariableOp""(%arg1) {device = """"} : (tensor<*x!tf_type.resource>) -> tensor<*xf32>
    %3 = ""tf.Conv2D""(%arg0, %2) {data_format = ""NHWC"", device = """", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = ""SAME"", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<32x32x32x3xf32>, tensor<*xf32>) -> tensor<32x?x?x?xf32>
    %4 = shape.shape_of %3 : tensor<32x?x?x?xf32> -> tensor<4xindex>
    %5 = ""mhlo.dynamic_broadcast_in_dim""(%1, %4) {broadcast_dimensions = dense<3> : tensor<1xi64>} : (tensor<*xf32>, tensor<4xindex>) -> tensor<32x?x?x?xf32>
    %6 = mhlo.add %3, %5 : tensor<32x?x?x?xf32>
    %7 = shape.shape_of %6 : tensor<32x?x?x?xf32> -> tensor<4xindex>
    %8 = ""mhlo.dynamic_broadcast_in_dim""(%0, %7) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>, tensor<4xindex>) -> tensor<32x?x?x?xf32>
    %9 = mhlo.maximum %8, %6 : tensor<32x?x?x?xf32>
    return %9 : tensor<32x?x?x?xf32>
  }
}
```"
59855,Can't install tflite-model-maker from pip or github,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

Windows 10 home 19045

### Mobile device

_No response_

### Python version

3.10.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
pip install tfl-model-maker tries to install over 500 nightly builds, and then fails when it installs the sixth version of sentencepiece

I tried:

git clone https://github.com/tensorflow/examples
cd examples/tensorflow_examples/lite/model_maker/pip_package
pip install -e .

but this fails with: 
ERROR: Could not find a version that satisfies the requirement tflite-support>=0.4.2 (from tflite-model-maker) (from versions: 0.1.0a0.dev3, 0.1.0a0.dev4, 0.1.0a0.dev5, 0.1.0a0, 0.1.0a1)
ERROR: No matching distribution found for tflite-support>=0.4.2

This is all on a clean anaconda environment - the pip command is the first I've run in it.

There's a copy of the pip output online here: https://1drv.ms/t/s!AjAaK-BVbphXlhwIM_9IpsnL0y8z?e=49lK1J
```


### Standalone code to reproduce the issue

```shell
pip install tflite-model-maker
```


### Relevant log output

_No response_</details>"
59853,Selectively build TFLite for IOS failure,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

No

### OS Platform and Distribution

MacOS Monterey 12.6.2

### Mobile device

N/A

### Python version

3.9

### Bazel version

5.3.0

### GCC/Compiler version

14.0.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

N/A

### Current Behaviour?

```shell
I tried building a selective TFL framework following this guide(https://www.tensorflow.org/lite/guide/build_ios#selectively_build_tflite_frameworks). The model I fed into the command only uses builtin and select ops. However this command returns error. 

ERROR: /Users/xx/projects/tensorflow/tensorflow/lite/ios/tmp/BUILD:4:24: Compiling tensorflow/lite/ios/tmp/custom_c_api_registration_registration.cc failed: undeclared inclusion(s) in rule '//tensorflow/lite/ios/tmp:custom_c_api_create_op_resolver':
this rule is missing dependency declarations for the following files included by 'tensorflow/lite/ios/tmp/custom_c_api_registration_registration.cc':
  'tensorflow/lite/core/shims/builtin_ops_list.inc'

I tried adding shims:builtin_ops_list but it is not helping and keep failing.
```


### Standalone code to reproduce the issue

```shell
The original code is not working
```


### Relevant log output

_No response_</details>"
59852,Issue created for Rollback of PR #59820: Cache the result of cuModuleGetFunction,"Merged PR #59820 is rolled back in 43ee0d82fe2939f7f77172085a9f3e6d76e56a32.
    Please follow up with the reviewer and close this issue once its resolved."
59850,tf.scan: add axis argument to scan along a specific axis,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11

### Custom Code

No

### OS Platform and Distribution

all

### Mobile device

-

### Python version

3.9

### Bazel version

-

### GCC/Compiler version

-

### CUDA/cuDNN version

-

### GPU model and memory

-

### Current Behaviour?

```shell
Current behaviour:
 - tf.scan 'unrolls' the argument fn along axis 0. 
Feature request: 
 - tf scan accepts the argument 'axis' (similar to many other tf operations), to specify along which axis the function is applied. The workaround is obviously simple (tf.transpose --> tf.scan --> tf.transpose; swapping the target axis to the front, applying scan, and swapping it back to where it belongs), but makes things awkward, especially in terms of readability. 

There seem to be lots of possible use-cases, but especially for working with time-axes (Batch, Time, Features), where scan should be used over the time axis this seems useful. Lots of examples e.g. in Reinforcement Learning for computing rewards-to-go, MC-state-value samples, GAE, etc.
```


### Standalone code to reproduce the issue

```shell
# e.g. the 'rewards-to-go', here to be computed on a rewards-batch of shape [batch_size, timesteps]
def rewards_to_go(rewards, gamma=0.99):
    discounted_rewards = tf.scan(lambda a, x: x + gamma * a, reverse=True, initializer=0., axis=1)
    return discounted_rewards
```


### Relevant log output

```shell
-
```
</details>"
59849,Inconsistent Log Level in cuda_driver InternalInit(),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The following Warning is logged as ERROR:

W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)

cuda_driver.cc 
InternalInit()
LOG(ERROR) << ""failed call to cuInit: "" << ToString(res); (line 266)

In this case, tensorflow is running CPU inferences. Therefore, the warning is expected, but not al ERROR level
```


### Standalone code to reproduce the issue

```shell
cuda_driver.cc 
InternalInit()
LOG(ERROR) << ""failed call to cuInit: "" << ToString(res); (line 266)

NO GPU
```


### Relevant log output

_No response_</details>"
59848,`Unsupported object type numpy.ndarray` on multi-input `Dataset`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Kaggle kernel

### Mobile device

_No response_

### Python version

3.7.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I'm trying to return tuple to a dataset to be used in a multi-input model, but it's throwing an error saying: `Unsupported object type numpy.ndarray`. Previosuly I was setting shape, with `set_shape`, but right now I simply don't know how to set shape for a different input shape.
```


### Standalone code to reproduce the issue

```shell
def read_image(path, rel):
    # blah blah blah, read somehow
    return image

def read_image1(path1, filter0):
    # blah blah blah, read somehow
    return image

def preprocess(x, y):
    def func(x, y):
        x = json.loads(x)
        x_img1 = read_image(x['path'], x['rel']) # 3D image
        x_img2 = read_image(x['path-fork'], x['filter']) #2D image with different shape
        # image resizing will lose data
        
        y = tf.keras.utils.to_categorical(y, num_classes=len(set(df['label'].values))) # todo: yeah, i'll optimize it never
        
        return (x_img1, x_img2), y

    _x, _y = tf.numpy_function(func, [x, y], [tf.float32, tf.float32])
    # _x.set_shape([256, 256, 3]) <--- previously i used to do this
    _y.set_shape([10])
    
    return _x, _y


# here `x` is an array of string, and those strings are actually json/dictionary
def tf_dataset(x,y, batch=16):
    dataset = tf.data.Dataset.from_tensor_slices((x, y))
    dataset = dataset.shuffle(buffer_size=1000)
    dataset = dataset.map(preprocess)
    dataset = dataset.batch(batch)
    dataset = dataset.prefetch(16)
    return dataset
```


### Relevant log output

```shell
InternalError: Graph execution error:

Unsupported object type numpy.ndarray
     [[{{node PyFunc}}]]
     [[IteratorGetNext]] [Op:__inference_train_function_30745]
```

Here is what I'm trying to do:
![multi input model with tf.data.Dataset](https://user-images.githubusercontent.com/29339330/222134871-4b2d6e7b-32b4-400c-8a91-0fc14a13ae11.png)

</details>"
59846,Encounter segment error when trying to deploy tflite model,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
   Ubuntu 20.04.1
- TensorFlow installation (pip package or built from source):
   Pip package
- TensorFlow library (version, if pip package or github SHA, if built from source):
   2.11.0

### 2. Code

Provide code to help us reproduce your issues using one of the following options:
#### Option B: Paste your code here or provide a link to a custom end-to-end colab

`TF_PATH = ""/data/lxp/mmpose/tf_model"" `
`TFLITE_PATH = ""/data/lxp/mmpose/mmpose.tflite""`
`converter = tf.lite.TFLiteConverter.from_saved_model(TF_PATH)`
`converter.optimizations = [tf.lite.Optimize.DEFAULT]`
`converter.representative_dataset = representative_data_gen`
`converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]`
`converter.target_spec.supported_types = [tf.int8] `
`converter.inference_input_type = tf.int8`
`converter.inference_input_type = tf.int8  # or tf.uint8`
`converter.inference_output_type = tf.int8  # or tf.uint8 `
`tf_lite_model = converter.convert()`
`with open(TFLITE_PATH, 'wb') as f:`
`f.write(tf_lite_model)`


### 3. Failure after conversion
If the conversion is successful, but I encounter segment error when I try to deploy tflite model.
Then, I found that error appeared while ""AllocateTensors()""->""graph_.PrepareSubgraphs()""->""registration->prepare(context_, node)""
There, I printed the node id and registration->builtin_code.
![image](https://user-images.githubusercontent.com/66299911/222093485-4853f014-ba33-4241-9094-45edf074ba75.png)
After using netron, we can know the node 24 correspond to FullyConnected.
![H2T_QR%WE@$0$8OXO9J7~UT](https://user-images.githubusercontent.com/66299911/222094472-74de7d81-4851-49b3-85b8-2d90b16689a7.png)
![image](https://user-images.githubusercontent.com/66299911/222093548-626469a4-4fe7-4ae0-ba82-85cd63ebc616.png)
Also, go into the funtion ""prepare(context_, node)"" , the error is raised by the following code.
`TFLITE_DCHECK(filter->params.zero_point == 0);`
This code restricts the zeropoints of the weight in fully_connected-layer should always be zero.
![image](https://user-images.githubusercontent.com/66299911/222094095-1a8b2f69-2d29-4f91-bfa9-27bac9329559.png)

But mine is -10. What needs mentioning is that  the weight in this fully_connected-layer is poduced by the upper transpose-layer.

I wonder why my zeropoint is not zero while I use tensorflow-Post-training quantization-Full integer quantization to get my quantization_weight and it's hard for me to find the configure to restricts the zeropoints."
59845,"one tflite model padding layer , the data is error(flatbuffers)","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.10 or 2.11

### Custom Code

Yes

### OS Platform and Distribution

android

### Mobile device

android

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
I call tflite::FlatBufferModel::BuildFromFile(model_name.c_str());
then on first padding layer I print the pad data (paddings)，and find the data is 0 0 0 0 8 0 8 0 ，
but using netron open it correct data is  [
    [
        0,
        0
    ],
    [
        8,
        8
    ],
    [
        8,
        8
    ],
    [
        0,
        0
    ]
]
```


### Standalone code to reproduce the issue

```shell
this pad layer   Connect three outputs later,it is one Transpose and two StridedSlice layers.
I guess it is flatbuffer bug
```


### Relevant log output

_No response_</details>"
59844,A check fail can be triggered in tf.ragged.range,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf 2.9 and 2.13.0-dev20230219

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.5

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The following code can trigger a check fail in `tf.ragged.range`. In some cases, it executed and exited normally, but sometimes it failed with a check fail. The large values in `limits` might be the cause of this bug.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
with tf.device(""CPU""):
    Tsplits = tf.int32
    starts = tf.random.uniform([], dtype=tf.float32, minval=-1024, maxval=1024)
    limits = tf.random.uniform([11], dtype=tf.float32, minval=-18446744073709551615, maxval=18446744073709551615)
    deltas = tf.random.uniform([], dtype=tf.float32, minval=-1024, maxval=1024)
    res = tf.ragged.range(
        starts=starts,
        limits=limits,
        deltas=deltas,
        row_splits_dtype=tf.int32,
    )
```


### Relevant log output

```shell
2023-03-01 10:51:07.788154: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-03-01 10:51:07.833699: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-01 10:51:08.584794: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-03-01 10:51:09.415873: I tensorflow/core/common_runtime/local_device.cc:158] LocalDevice using CPU work thread pool: 0x55d03db40440, num_threads=48
2023-03-01 10:51:10.194653: I tensorflow/core/common_runtime/local_device.cc:158] LocalDevice using CPU work thread pool: 0x55d03db40440, num_threads=48
2023-03-01 10:51:10.194691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14061 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:2f:00.0, compute capability: 7.0
2023-03-01 10:51:10.215308: I tensorflow/core/common_runtime/local_device.cc:158] LocalDevice using CPU work thread pool: 0x55d03db40440, num_threads=48
2023-03-01 10:51:10.221055: I tensorflow/core/common_runtime/local_device.cc:158] LocalDevice using CPU work thread pool: 0x55d03db40440, num_threads=48
2023-03-01 10:51:10.223571: I tensorflow/core/common_runtime/local_device.cc:158] LocalDevice using CPU work thread pool: 0x55d03db40440, num_threads=48
2023-03-01 10:51:10.225320: I tensorflow/core/common_runtime/local_device.cc:158] LocalDevice using CPU work thread pool: 0x55d03db40440, num_threads=48
2023-03-01 10:51:10.228201: I tensorflow/core/common_runtime/local_device.cc:158] LocalDevice using CPU work thread pool: 0x55d03db40440, num_threads=48
2023-03-01 10:51:10.230234: F tensorflow/core/framework/tensor_shape.cc:201] Non-OK-status: InitDims(dim_sizes) status: INVALID_ARGUMENT: Expected shape dimensions to be non-negative, got -2147483648
Aborted (core dumped)
```
</details>"
59841,Numpy to tensor,"I apologize for posting a very simple question here as I could not solve the issue myself or find an answer elsewhere.

I was trying to make a prediction using a model and a 3D array with 3 channels as inputs. When this 3D array was saved as an image file, I was able to make a proper prediction using my model:
`image = tf.io.read_file('image.jpg')`
`image = tf.image.decode_image(image, channels=3)`
`image = tf.image.resize(images=image, size=[imsize, imsize])`
`image = tf.keras.applications.resnet50.preprocess_input(image)`
`predictions = model.predict(np.expand_dims(image, axis=0))`

However, when this 3D array was readily available in memory as a numpy array in uint8 format, and I tried to directly use this array to make a prediction, I get a different prediction result. I used `cv2.imread` to simulate the array in memory:
`array = cv2.imread('image.jpg')`
`image = tf.convert_to_tensor(array/255, dtype=tf.float32) `
`image = tf.image.resize(images=image, size=[imsize, imsize])`
`image = tf.keras.applications.resnet50.preprocess_input(image)`
`predictions = model.predict(np.expand_dims(image, axis=0))`

Any help is greatly appreciated! "
59838,mean_absolute_error outputs weird results when input is nan on GPU,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.13.0-dev20230222

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

8.2.4

### GPU model and memory

_No response_

### Current Behaviour?

```shell
mean_absolute_error outputs weird results when input is nan on GPU
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

labels = [np.nan]
predictions = [1]
mae = tf.keras.metrics.mean_absolute_error(labels, predictions)
print(mae)
```


### Relevant log output

```shell
tf.Tensor(1, shape=(), dtype=int32)
```
</details>"
59833,"Centernet TFLite for multiple keypoint tasks crashes, but only on specific images","### 1. System information

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**

  Linux Ubuntu 20.04

- **TensorFlow installation (pip package or built from source):**

  Pip package

- **TensorFlow library (version, if pip package or github SHA, if built from source):**

  2.11.0

### 2. Code

Drive link to the model, including checkpoints, SavedModel and TFLite model: https://drive.google.com/drive/folders/1E0Ris0mfS4s5iWsLcqIoDK03dQO7nJ7T?usp=sharing
Drive link to a working image and a non-working (crashing) image: https://drive.google.com/drive/folders/1U0F-j8ZYYABRtbXm8hyWexP3gna5_ln9?usp=sharing

To convert the model, I first ran the `export_tflite_graph_tf2.py` script ([GitHub link](https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_graph_tf2.py)) with the following command:

```
python3 export_tflite_graph_tf2.py  \
  --pipeline_config_path pipeline.config  \
  --trained_checkpoint_dir checkpoint/  \
  --output_directory .  \
  --keypoint_label_map_path label_map.pbtxt
  --centernet_include_keypoints true
```

Then, I converted the generated `saved_model` to TFLite using the `tflite_convert.py` script ([GitHub link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python/tflite_convert.py)) with the following command:

```
tflite_convert --saved_model_dir saved_model --output_file model.tflite
```

This successfully generates a `model.tflite` file. To run it, I'm using the following reference code (replacing the `model_path` and `image_path` variables with the correct paths):

```
import cv2 as cv
import numpy as np
import tensorflow as tf

model_path = '---'
image_path = '---'

interpreter = tf.lite.Interpreter(model_path)
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
input_shape = input_details[0]['shape']
input_dtype = input_details[0]['dtype']
input_index = input_details[0]['index']

# Load and preprocess image
image = cv.imread(image_path)
image = cv.cvtColor(image, cv.COLOR_BGR2RGB)
image = cv.resize(image, input_shape[2:0:-1])
image = np.expand_dims(image, 0)
image = image.astype(input_dtype)

# Inference
interpreter.set_tensor(input_index, image)
interpreter.invoke()
```

### 3. Failure after conversion

Running the code with the image named `working.jpg` produces the following logs, indicating that there is no error:

```
2023-02-28 11:23:15.406260: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-28 11:23:16.388488: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/biant/anaconda3/envs/test/lib/python3.10/site-packages/cv2/../../lib64::/home/biant/anaconda3/envs/solabml/lib/:/home/biant/anaconda3/envs/test/lib/
2023-02-28 11:23:16.388599: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/biant/anaconda3/envs/test/lib/python3.10/site-packages/cv2/../../lib64::/home/biant/anaconda3/envs/solabml/lib/:/home/biant/anaconda3/envs/test/lib/
2023-02-28 11:23:16.388615: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
```

Running the code with the image named `not_working.jpg` produces the following logs and error:

```
2023-02-28 11:24:18.121298: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-28 11:24:19.110612: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/biant/anaconda3/envs/test/lib/python3.10/site-packages/cv2/../../lib64::/home/biant/anaconda3/envs/solabml/lib/:/home/biant/anaconda3/envs/test/lib/
2023-02-28 11:24:19.110720: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/biant/anaconda3/envs/test/lib/python3.10/site-packages/cv2/../../lib64::/home/biant/anaconda3/envs/solabml/lib/:/home/biant/anaconda3/envs/test/lib/
2023-02-28 11:24:19.110735: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
Traceback (most recent call last):
  File ""/home/biant/inference.py"", line 25, in <module>
    interpreter.invoke()
  File ""/home/biant/anaconda3/envs/test/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py"", line 917, in invoke
    self._interpreter.Invoke()
RuntimeError: tensorflow/lite/kernels/unpack.cc:43 NumElements(input) > 0 was not true.Node number 297 (UNPACK) failed to prepare.
```

### 5. (optional) Any other info / logs

1. There isn't anything special about these two example images, they are from the validation set of Coco-2017 (the training set was used for training the model). The same results can be achieved by trying other random images.

2. On the working image, the detections can be printed and visualized, and the keypoints for both the 'person' and 'face' categories are detected correctly. This suggests that the TFLite model isn't *incapable* of predicting keypoints for two different categories.

3. Converting the checkpoints to a SavedModel using the script `exporter_main_v2.py` ([GitHub link](https://github.com/tensorflow/models/blob/master/research/object_detection/exporter_main_v2.py)) results in a model that doesn't crash with any images. This indicates that the issue is only related to the TFLite model.
3.1 - When observing the images that *don't* crash the TFLite model, it seems like in all cases at least one category 'person' and one category 'face' is predicted (out of the 10 detections that the model outputs).
3.2 - On the images that *do* crash the TFLite model, running them through the SavedModel version reveals that in almost all cases the detections consist of either all 'persons' or all 'faces'. Is it possible that the TFLite model crashes when it only predicts one of the categories?
3.3 - There were a few exceptions to point 3.2, but they were rare enough that the exceptions might just be due to the slight differences between the SavedModel and TFLite model.

**Edit:** As expected, increasing `--max_detections` during model conversion reduces the likelihood that the model will crash. For example, after increasing the number of detections from 10 to 100, only ~5% of the previously offending images are still causing a crash. Could this be due to the fact that when there are more detections outputted, the chance that both classes are detected increases?"
59830,tf.math.reduce_std on ragged tensors doesn't work properly,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Currently, no segment_std operation exists in tensorflow. Thus, I am implementing the function by myself in python using built-in tf methods. std_r=tf.math.reduce_std(...) is a fast and optimized way to achieve that (see the code). Unfortunately, std_r defers from np.std on the segments if the data is in tf.float32 format. The issue does not happen with tf.float64. When I convert the ragged segments segments_r into a tensor, which is only possible if all segments have the same length), reduce_std works fine
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

def _segment_std(data, segment_ids, name=None):
    row_splits = tf.ragged.segment_ids_to_row_splits(segment_ids)
    segments_r = tf.RaggedTensor.from_row_splits(data, row_splits)
    std_r = tf.math.reduce_std(segments_r, axis=1)
    
    segments_t = segments_r.to_tensor()
    std_r_map = tf.map_fn(lambda x: tf.math.reduce_std(x, axis=0), segments_r)
    std_t = tf.math.reduce_std(segments_t, axis=1)
    for b in range(segments_r.shape[0]):
            print(""======="", b, ""======="")
            print(std_r[b])
            print(std_r_map[b])
            print(std_t[b])
    
    print(data.dtype, row_splits.dtype, segments_r.dtype, segments_t.dtype, std_r.dtype, std_r_map.dtype, std_t.dtype)
    return segments_r, segments_t

data_32 = tf.random.uniform((18, 6), dtype=tf.float32)
data_64 = tf.cast(data_32, dtype=tf.float64)
segment_ids = tf.constant([0,0,0,1,1,1,2,2,2,3,3,3,4,4,4,5,5,5])

segments_r_32, segments_t_32 = _segment_std(data_32, segment_ids)
print(""======= numpy ======="")
print(np.std(segments_r_32.numpy(), axis=1))
print(np.std(segments_t_32.numpy(), axis=1))

segments_r_64, segments_t_64 = _segment_std(data_64, segment_ids)
print(""======= numpy ======="")
print(np.std(segments_r_64.numpy(), axis=1))
print(np.std(segments_t_64.numpy(), axis=1))
```


### Relevant log output

```shell
======= 0 =======
tf.Tensor([0.15954268 0.14723612 0.3377797  0.22492617 0.22479114 0.04101199], shape=(6,), dtype=float32)
tf.Tensor([0.15954268 0.14723626 0.3377797  0.2249262  0.22479111 0.04101215], shape=(6,), dtype=float32)
tf.Tensor([0.15954268 0.14723626 0.3377797  0.2249262  0.22479111 0.04101215], shape=(6,), dtype=float32)
======= 1 =======
tf.Tensor([0.2185983  0.41067353 0.14908639 0.40719846 0.21105738 0.18947895], shape=(6,), dtype=float32)
tf.Tensor([0.21859837 0.4106735  0.14908642 0.40719846 0.2110574  0.18947898], shape=(6,), dtype=float32)
tf.Tensor([0.21859837 0.4106735  0.14908642 0.40719846 0.2110574  0.18947898], shape=(6,), dtype=float32)
======= 2 =======
tf.Tensor([0.05812957 0.31665203 0.2071376  0.3885405  0.09823482 0.27517167], shape=(6,), dtype=float32)
tf.Tensor([0.05812923 0.3166521  0.20713761 0.38854057 0.09823488 0.27517167], shape=(6,), dtype=float32)
tf.Tensor([0.05812923 0.3166521  0.20713761 0.38854057 0.09823488 0.27517167], shape=(6,), dtype=float32)
======= 3 =======
tf.Tensor([0.06295965 0.09678382 0.42397946 0.18043025 0.23579012 0.12517032], shape=(6,), dtype=float32)
tf.Tensor([0.06295984 0.09678387 0.42397946 0.18043028 0.23579012 0.12517034], shape=(6,), dtype=float32)
tf.Tensor([0.06295984 0.09678387 0.42397946 0.18043028 0.23579012 0.12517034], shape=(6,), dtype=float32)
======= 4 =======
tf.Tensor([0.01316889 0.16237932 0.28821537 0.11204658 0.17594427 0.34890616], shape=(6,), dtype=float32)
tf.Tensor([0.01316908 0.16237934 0.2882153  0.11204645 0.17594407 0.34890622], shape=(6,), dtype=float32)
tf.Tensor([0.01316908 0.16237934 0.2882153  0.11204645 0.17594407 0.34890622], shape=(6,), dtype=float32)
======= 5 =======
tf.Tensor([0.2691938  0.07409578 0.2120972  0.21070714 0.08790783 0.38431   ], shape=(6,), dtype=float32)
tf.Tensor([0.26919377 0.07409566 0.21209723 0.21070711 0.08790784 0.38431004], shape=(6,), dtype=float32)
tf.Tensor([0.26919377 0.07409566 0.21209723 0.21070711 0.08790784 0.38431004], shape=(6,), dtype=float32)
```
</details>"
59828,Missing Window GPU prebuilt binary for 2.11.0,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf 2.11

### Custom Code

No

### OS Platform and Distribution

Windows

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-windows-x86_64-2.11.0.zip is missing, which is breaking TensorFlow Rust for Windows with GPU: https://github.com/tensorflow/rust/issues/400
```


### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

_No response_</details>"
59826,mixed_float16 in Keras example stops the model to learn,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

TensorFlow version: 2.11.0 as well as 2.13.0-dev20230227

### Custom Code

Yes

### OS Platform and Distribution

Colab & Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

Colab & 12.0

### GPU model and memory

T4 & 3090

### Current Behaviour?

```shell
If I run the Keras vision example forwardforward from:
https://keras.io/examples/vision/forwardforward/ 
in Colab or on a Local Machine with 3090 and turn on mixed_float16 with the code
´´´
gpu_devices = tf.config.list_physical_devices('GPU')
if gpu_devices:
 details = tf.config.experimental.get_device_details(gpu_devices[0])
 compute_capability=details.get('compute_capability')
 print(""Compute capability:"",compute_capability)
 if compute_capability[0]>6:
   print(""Turning on mixed_float16"")
   policy = tf.keras.mixed_precision.Policy('mixed_float16')
   tf.keras.mixed_precision.set_global_policy(policy)
´´´

Then the model no longer trains

When running 
history = model.fit(train_dataset, epochs=epochs)

In mixed_float16 one gets 
1/1 [==============================] - 49s 49s/step - FinalLoss: 0.9510
Epoch 2/250
1/1 [==============================] - 1s 1s/step - FinalLoss: 0.9510
Epoch 3/250
1/1 [==============================] - 1s 1s/step - FinalLoss: 0.9510
Epoch 4/250
1/1 [==============================] - 1s 1s/step - FinalLoss: 0.9510
…
Epoch 250/250
1/1 [==============================] - 1s 1s/step - FinalLoss: 0.9510

Clearly not training!

While without mixed_float16 one gets something like:
1/1 [==============================] - 45s 45s/step - FinalLoss: 0.7278
Epoch 2/250
1/1 [==============================] - 2s 2s/step - FinalLoss: 0.7081
Epoch 3/250
1/1 [==============================] - 2s 2s/step - FinalLoss: 0.7011
Epoch 4/250
…
Epoch 250/250
1/1 [==============================] - 2s 2s/step - FinalLoss: 0.3117


This can be reproduced by turning on the mixed_float16 according to the code above in the colab notebook https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/forwardforward.ipynb
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1U0-NXNjyh6onGsGmiTfoqkX-1rNgoTuq?usp=sharing
```


### Relevant log output

_No response_</details>"
59824,Tensorflow lite build error for Aarch64 ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

Tensorflow lite master branch

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04.6 LTS

### Mobile device

Nvidia Jetson nano

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Trying to build tensorflow lite wheels using the following command 
`
PYTHON=python3 tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh native
`

While trying to build, getting compilation errors of the likes:


In file included from /home/piyush/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/xnnpack/src/amalgam/neonfp16arith.c:8::
/usr/lib/gcc/aarch64-linux-gnu/7/include/arm_neon.h:30805:1: error: inlining failed in call to always_inline ‘vaddq_f16’: target specific option mismatch
 vaddq_f16 (float16x8_t __a, float16x8_t __b)
 ^~~~~~~~~
/home/piyush/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/xnnpack/src/amalgam/neonfp16arith.c:277:27: note: called from here
         const float16x8_t vsum2345 = vaddq_f16(vsum23, vsum45);
```
```


### Standalone code to reproduce the issue

```shell
Build using above command on Aarch64. I am using Nvidia Jetson Nano with Ubuntu 18.04.6 LTS
```


### Relevant log output

```shell
In file included from /home/piyush/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/xnnpack/src/amalgam/neonfp16arith.c:8::
/usr/lib/gcc/aarch64-linux-gnu/7/include/arm_neon.h:30805:1: error: inlining failed in call to always_inline ‘vaddq_f16’: target specific option mismatch
 vaddq_f16 (float16x8_t __a, float16x8_t __b)
 ^~~~~~~~~
/home/piyush/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/xnnpack/src/amalgam/neonfp16arith.c:120:27: note: called from here
         const float16x8_t vsum = vaddq_f16(vsum2345, vsum01678);
```
</details>"
59818,XLA inference fails complaning about branch shape mismatches,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Colab

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

A100 40 GB

### Current Behaviour?

```shell
I have the following function. 

First, I initialize a CLIP-based text encoder:


from keras_cv.models.stable_diffusion.text_encoder import TextEncoder

MAX_PROMPT_LENGTH = 77
text_encoder = TextEncoder(MAX_PROMPT_LENGTH)


Then, I am using the `text_encoder` like so in a function that I use to serialize the `text_encoder` as a `SavedModel`:

```python
from keras_cv.models.stable_diffusion.constants import _UNCONDITIONAL_TOKENS
import tensorflow as tf 

signature_dict = {
    ""tokens"": tf.TensorSpec(shape=[None, 77], dtype=tf.int32, name=""tokens""),
}

def text_encoder_exporter(model: tf.keras.Model):
    BATCH_SIZE = 3
    MAX_PROMPT_LENGTH = 77
    POS_IDS = tf.convert_to_tensor([list(range(MAX_PROMPT_LENGTH))], dtype=tf.int32)
    UNCONDITIONAL_TOKENS = tf.convert_to_tensor([_UNCONDITIONAL_TOKENS], dtype=tf.int32)

    @tf.function(input_signature=[signature_dict])
    def serving_fn(inputs):
        # context
        encoded_text = model([inputs[""tokens""], POS_IDS], training=False)
        encoded_text = tf.squeeze(encoded_text)

        if tf.rank(encoded_text) == 2:
            encoded_text = tf.repeat(
                tf.expand_dims(encoded_text, axis=0), BATCH_SIZE, axis=0
            )

        # unconditional context
        unconditional_context = model([UNCONDITIONAL_TOKENS, POS_IDS], training=False)

        unconditional_context = tf.repeat(unconditional_context, BATCH_SIZE, axis=0)
        return {""context"": encoded_text, ""unconditional_context"": unconditional_context}

    return serving_fn
```

Serialization:

```python
tf.saved_model.save(
    text_encoder,
    ""./text_encoder/1/"",
    signatures={""serving_default"": text_encoder_exporter(text_encoder)},
)
```

Now, while attempting to XLA-compile:

```python
from tensorflow.python.saved_model import tag_constants

batch_size = 3
saved_model_loaded = tf.saved_model.load(
    ""./text_encoder/1/"", tags=[tag_constants.SERVING]
)
text_encoder_predict_fn = saved_model_loaded.signatures[""serving_default""]
# Raises error
xla_text_encoder_predict_fn = tf.function(text_encoder_predict_fn, jit_compile=True)
xla_text_encoder_predict_fn(
    tokens=tf.ones((batch_size, MAX_PROMPT_LENGTH), tf.int32)
).keys()
```
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/gist/sayakpaul/d7dafc252752a6c1ce10e85d8162b8ea/scratchpad.ipynb
```


### Relevant log output

```shell
/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs)
    151     except Exception as e:
    152       filtered_tb = _process_traceback_frames(e.__traceback__)
--> 153       raise e.with_traceback(filtered_tb) from None
    154     finally:
    155       del filtered_tb

/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     50   try:
     51     ctx.ensure_initialized()
---> 52     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     53                                         inputs, attrs, num_outputs)
     54   except core._NotOkStatusException as e:

InvalidArgumentError: left_branch_shape.rank() != right_branch_shape.rank() (4 vs 3)
	 [[{{function_node __inference_serving_fn_55983}}{{node cond}}]] [Op:__inference_function_60000]
```
</details>"
59817,Failing to run hexagon delegates on android,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

QCS6125

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Problem with hexagon delegates, the hexagon libs from hexagon delegates page are set into arm64-v8a folder.

>CPU architecture: 8

>adb shell getprop ro.board.platform
trinket
>adb shell getprop ro.product.device
trinket
```


### Standalone code to reproduce the issue

```shell
implementation 'org.tensorflow:tensorflow-lite:2.10.0'
implementation 'org.tensorflow:tensorflow-lite-hexagon:2.10.0'

Running tensorflow example from Quickstart for Android.
```


### Relevant log output

```shell
/tflite: Failed to fetch Hexagon NN version. This might be because you're using incompatible versions of libhexagon_interface and libhexagon_nn_skel. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide.
```
</details>"
59815,Recognize Flowers with TensorFlow Lite on Android Codelabs - step 4 error,Just following the steps on a fresh installation of Android studio on ubuntu get the following error when trying to run: Could not find compile target android-29 for modules :start
59814,Tensorflow Import error,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.10

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I cannot import the tensorflow package in my jupyter notebook. I have checked the installations in conda and pip, but everything seems to work. So whenever i try to import it i get the following error:
""ImportError: Traceback (most recent call last):
  File ""C:\Users\Rury\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.""
Is it a problem of the environment or a problem with package itself?
I am starting now to study DL so i would appreciate a lot a quick response. Thank you
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
```


### Relevant log output

_No response_</details>"
59811,DEPRECIATION NOTICE,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

Any

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
WARNING! C++ is deprecated, use Rust instead
```


### Standalone code to reproduce the issue

```shell
..
```


### Relevant log output

_No response_</details>"
59810,error with bazel - trying to build windows version of tensorflow for visual studio,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf2.11

### Custom Code

No

### OS Platform and Distribution

windows visual studio 2019

### Mobile device

no

### Python version

3.7

### Bazel version

5.3

### GCC/Compiler version

not sure

### CUDA/cuDNN version

none

### GPU model and memory

no

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
here is the command that i am running

bazel build //tensorflow::install_headers > w:\tmp\bazel_out.txt

any help much appreciated!!
```


### Relevant log output

```shell
In the file that i piped the output to i get this:
tensorflow/core/framework/tensor.cc(744): error C2248: 'tensorflow::TensorShapeBase<tensorflow::TensorShape>::TensorShapeBase': cannot access protected member declared in class 'tensorflow::TensorShapeBase<tensorflow::TensorShape>'
.\tensorflow/core/framework/tensor_shape.h(324): note: see declaration of 'tensorflow::TensorShapeBase<tensorflow::TensorShape>::TensorShapeBase'
.\tensorflow/core/framework/tensor_shape.h(359): note: see declaration of 'tensorflow::TensorShapeBase<tensorflow::TensorShape>'

here is the screen output that i am getting:  

W:\bb\git\tensorflow_old>bazel build //tensorflow:install_headers > w:\tmp\bazel_out.txt
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=80
INFO: Reading rc options for 'build' from w:\bb\git\tensorflow_old\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/Program Files/Python/Python3.7/python.exe
INFO: Reading rc options for 'build' from w:\bb\git\tensorflow_old\.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from w:\bb\git\tensorflow_old\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Program Files/Python/Python3.7/python.exe --action_env PYTHON_LIB_PATH=C:/Program Files/Python/Python3.7/Lib --python_path=C:/Program Files/Python/Python3.7/python.exe --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true
INFO: Reading rc options for 'build' from w:\bb\git\tensorflow_old\.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file w:\bb\git\tensorflow_old\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file w:\bb\git\tensorflow_old\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:windows in file w:\bb\git\tensorflow_old\.bazelrc: --copt=/W0 --host_copt=/W0 --copt=/Zc:__cplusplus --host_copt=/Zc:__cplusplus --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --features=compiler_param_file --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --cxxopt=/std:c++17 --host_cxxopt=/std:c++17 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/Zc:preprocessor --host_copt=/Zc:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file w:\bb\git\tensorflow_old\.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false
INFO: Analyzed target //tensorflow:install_headers (265 packages loaded, 13791 targets configured).
INFO: Found 1 target...
ERROR: W:/bb/git/tensorflow_old/tensorflow/core/framework/BUILD:769:16: Compiling tensorflow/core/framework/tensor.cc failed: (Exit 2): cl.exe failed: error executing command
  cd /d C:/users/scott.ad/_bazel_scott/24njqhju/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.24.28314\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.24.28314\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\cppwinrt
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.24.28314\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\\x64;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\;C:\Program Files (x86)\Windows Kits\10\bin\10.0.18362.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\\MSBuild\Current\Bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\Tools\;;C:\Windows\system32;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Program Files/Python/Python3.7/python.exe
    SET PYTHON_LIB_PATH=C:/Program Files/Python/Python3.7/Lib
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\scott.AD\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TMP=C:\Users\scott.AD\AppData\Local\Temp
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.24.28314\bin\HostX64\x64\cl.exe @bazel-out/x64_windows-opt/bin/tensorflow/core/framework/_objs/tensor/tensor.obj.params
# Configuration: 548e4eb7c3a997dbe854b942bd4ad3607774b15376fbc38c5c147beb568995f0
# Execution platform: @local_execution_config_platform//:platform
cl : Command line warning D9002 : ignoring unknown option '/Zc:preprocessor'
Target //tensorflow:install_headers failed to build
ERROR: W:/bb/git/tensorflow_old/tensorflow/BUILD:1548:8 Executing genrule //tensorflow:install_headers failed: (Exit 2): cl.exe failed: error executing command
  cd /d C:/users/scott.ad/_bazel_scott/24njqhju/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.24.28314\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.24.28314\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\cppwinrt
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.24.28314\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\\x64;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\;C:\Program Files (x86)\Windows Kits\10\bin\10.0.18362.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\\MSBuild\Current\Bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\Tools\;;C:\Windows\system32;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Program Files/Python/Python3.7/python.exe
    SET PYTHON_LIB_PATH=C:/Program Files/Python/Python3.7/Lib
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\scott.AD\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TMP=C:\Users\scott.AD\AppData\Local\Temp
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.24.28314\bin\HostX64\x64\cl.exe @bazel-out/x64_windows-opt/bin/tensorflow/core/framework/_objs/tensor/tensor.obj.params
# Configuration: 548e4eb7c3a997dbe854b942bd4ad3607774b15376fbc38c5c147beb568995f0
# Execution platform: @local_execution_config_platform//:platform
INFO: Elapsed time: 213.717s, Critical Path: 36.87s
INFO: 1962 processes: 484 internal, 1478 local.
FAILED: Build did NOT complete successfully
```
</details>"
59809,libtpu.so 1.5.0 or tpu-vm-tf-2.11.0-pod runtime bug,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

tpu-vm-tf-2.11.0-pod

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
It appears that when using `tf.data.Dataset` with `libtpu.so 1.5.0` and `tpu-vm-tf-2.11.0-pod` runtime, only half the system resources are being able to get utilized. For a `tpu-v4` if a dataset uses more than 200gb system ram, it will get killed (with just a missing context id provided, no detailed error even gets generated). This problem does not occur using `tpu-vm-tf-2.10.1-pod`. I am sorry I am unable to provide a standalone example but please review build flags / runtime image for `tpu-vm-tf-2.11.0-pod`.
```


### Standalone code to reproduce the issue

```shell
Difficult to provide at this point
```


### Relevant log output

_No response_</details>"
59807,unable to use my classifiers.tflite and label.txt. Please help,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
59806,Error when including header files using CMake TF Lite with installable package,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

latest master (commit [1f747f5](https://github.com/tensorflow/tensorflow/commit/1f747f5b52003896c5fe6723ff95f84f00def94a)) 

### Custom Code

No

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

gcc 11.3.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

Nvidia GTX 1650 Mobile

### Current Behaviour?

```shell
After building the TFLite installable package from source using CMake, according to the tutorial from the [documentation](https://www.tensorflow.org/lite/guide/build_cmake), when trying to compile code that has `#include ""utils/neural_network/NeuralNetwork.h""`, the compiler throws an error, since many files required for the inclusion do not exist at `/usr/local/include/tensorflow/lite` (which is where the TFLite files ended up after running `cmake --install .`).

After checking the CMakeLists file at `tensorflow/lite`, it seems that the inclusion of certain directories is not done recursively (e.g. the inclusion of the files in the directory `core/async`, by the command `populate_tflite_source_vars`, leaves out the sub-folders `c`, `interop`, and `testing`), which seems to be the source of the issue, since the header files are not exported during the installation process.

Copying the missing files mentioned by the compiler to their appropriate locations seems to solve the problem, however, I have not completely tested this, since there is a great amount of files that have to be copied over.

Any help would be greatly appreciated.
```


### Standalone code to reproduce the issue

```shell
#include ""utils/neural_network/NeuralNetwork.h""

int main() {
    utils::neural_network::NeuralNetwork nn;
    return 0;
}
```


### Relevant log output

```shell
/usr/local/include/tensorflow/lite/core/interpreter.h:56:10: fatal error: tensorflow/lite/profiling/telemetry/c/telemetry_setting_internal.h: No such file or directory
   56 | #include ""tensorflow/lite/profiling/telemetry/c/telemetry_setting_internal.h""
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
```
</details>"
59803,Support for int32 variables,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?
Tensorflow currently does not support `int32` variables being placed on the GPU.
I've found an old [stackoverflow](https://stackoverflow.com/questions/37439299/no-gpu-kernel-for-an-int32-variable-op) post that argued this was because typically, `int32`  Tensors often represent sizes of shapes, which would be needed in host memory. 

If that is still a concern, maybe automatically placing any int32 Tensors with e.g. less than 1KiB size on the CPU, and larger ones on the GPU, might be the way to go?

For large integer tensors (e.g. indices for sparse operations), the current situation requires a trade-off between two undesireable situations:
* Either use `int64`, which wastes both GPU memory, and also memory bandwidth during kernel calls
* Or use `int32`, but then each step has to invoke a host-to-device data transfer.

Declaring the Variable as `float32` and bitcasting to `int32` before each use also does not allow to work around this issue, as the `bitcast` op also lacks a GPU kernel, and thus requires two memory transfers.

This request is somewhat related to #51728, as supporting int32 as index type for `SparseTensor` would run into the aforementioned data transfer problem if the index Tensor was backed by a Variable. 
Even now, [`SparseTensorDenseMatMul`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/sparse_tensor_dense_matmul_op_gpu.cu.cc#L151) provides a kernel for `int32`, but again it cannot be used efficiently with indices supplies by a Variable.


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.config.set_soft_device_placement(False)

with tf.device(""gpu:0""):
    v = tf.Variable(initial_value=tf.zeros((50, 50), dtype=tf.int32))
```


### Relevant log output

```shell
tensorflow.python.framework.errors_impl.InvalidArgumentError: Could not satisfy device specification '/job:localhost/replica:0/task:0/device:GPU:0'. enable_soft_placement=0. Supported device types [CPU].
```
</details>"
59801,No wheels for tf-nightly Macos arm64,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf-nightly 2.13.0.dev20230224

### Custom Code

No

### OS Platform and Distribution

Macos M1

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I'm not able to install tf-nightly on Macs running arm.
```


### Standalone code to reproduce the issue

```shell
> pip install tf-nightly==2.13.0.dev20230224
ERROR: Could not find a version that satisfies the requirement tf-nightly==2.13.0.dev20230224 (from versions: none)
ERROR: No matching distribution found for tf-nightly==2.13.0.dev20230224
```


### Relevant log output

_No response_</details>"
59799,Running tensorflow distributed on Multiple workers,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.6

### Custom Code

Yes

### OS Platform and Distribution

Linux HPC

### Mobile device

_No response_

### Python version

3.8.3

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.3.1

### GPU model and memory

4 RTX 2080 Ti workers each having 8 GPUs

### Current Behaviour?

```shell
I am trying to run tensorflow distributed training code on multiple worker nodes. I initially tried it using the Mirrored strategy using a single worker with multiple GPUs and the code was working fine and the training process was getting distributed among multiple GPUs. when i have tried it with multiple worker nodes, the training process is actually executing seperately on different workers rather than the load getting distributed among the workers.
My Linux systems has 4 worker nodes with each node having 8 2080Ti GPUs and the GPUs are connected through PCIe system. I also not sure How to configure the TF_config. can anyone help me on this ?
```


### Standalone code to reproduce the issue

```shell
if __name__ == ""__main__"":
    # Get mpi rank
    from getOneHot import getOneHot
    from mpi4py import MPI
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()

    # Load in the parameter files
    from json import load as loadf
    with open(""params.json"", 'r') as inFile:
        params = loadf(inFile)

    # Get data files and prep them for the generator
    import tensorflow
    from tensorflow import distribute as D
    callbacks = []
    devices = getDevices()
    print(devices)
    set_tf_config_mpi()
    strat = D.experimental.MultiWorkerMirroredStrategy(
            communication=D.experimental.CollectiveCommunication.NCCL)

# Create network
    from sys import argv
    resume_training = False
    print(argv)
    if ""resume_latest"" in argv:
        resume_training = True
    with strat.scope():
        # Scheduler
        if isinstance(params[""learning_rate""], str):
            # Get the string for the importable function
            lr = params[""learning_rate""]
            from tensorflow.keras.callbacks import LearningRateScheduler
            # Use a dummy learning rate
            params[""learning_rate""] = 0.1
            # model = create_model(**params)
            # Get the importable function
            lr = lr.split(""."")
            baseImport = __import__(lr[0], globals(), locals(), [lr[1]], 0)
            lr = getattr(baseImport, lr[1])
            # Make a schedule
            lr = LearningRateScheduler(lr)
            callbacks.append(lr)
        # Resume Model?
        model_name = None
        if resume_training:
            initial_epoch, model_name = getInitialEpochsAndModelName(rank)
        if model_name is None:
            initial_epoch=0
            model = create_model(**params)
            resume_training = False
        else:
            from tensorflow.keras.models import load_model
            model = load_model(model_name)
    # Load data from disk
    import numpy
    if ""root"" in params.keys():
        root = params['root']
    else:
        root = ""./""
    if ""filename"" in params.keys():
        filename = params[""filename""]
    else:
        filename = ""150MeV_all_shuffled_normed.csv""

    restricted = [
            'euc1', 'e1', 'x1', 'y1', 'z1',
            'euc2', 'e2', 'x2', 'y2', 'z2',
            'euc3', 'e3', 'x3', 'y3', 'z3',
            ]
   x, y = getOneHot(""{}/{}"".format(root, filename), restricted=restricted, **params)
    # val_filename = ""150MeV_180kMUmin-stdCC_stitched_triples_dtot_trip_only.csv""
    # val_x, val_y = getOneHot(""{}/{}"".format(root, val_filename), restricted=restricted)
    val_x, val_y = None, None
    params[""gbatch_size""] = params['batch_size'] * len(devices)
    print(""x.shape ="", x.shape)
    print(""y.shape ="", y.shape)
    print(""epochs  ="", params['epochs'], type(params['epochs']))
    print(""batch   ="", params['batch_size'], type(params['batch_size']))
    print(""gbatch  ="", params['gbatch_size'], type(params['gbatch_size']))
    # Load data into a distributed dataset
    # Dataset object does nothing in place:
    # https://stackoverflow.com/questions/55645953/shape-of-tensorflow-dataset-data-in-keras-tensorflow-2-0-is-wrong-after-conver
    from tensorflow.data import Dataset
    data = Dataset.from_tensor_slices((x, y))

    # Create validation set
    v = params['validation']
    if val_x is not None:
        vrecord = val_x.shape[0]
        val  = Dataset.from_tensor_slices((val_x, val_y))
        validation = val # data.take(vrecord)
    else:
        vrecord = int(x.shape[0]*v)
        validation = data.take(vrecord)
    validation = validation.batch(params['gbatch_size'])
    validation = validation.repeat(params['epochs'])
    # Validation -- need to do kfold one day
    # This set should NOT be distributed
    vsteps = vrecord // params['gbatch_size']
    if vrecord % params['gbatch_size'] != 0:
        vsteps += 1
    data    = data.batch(params['gbatch_size'])
    data    = data.repeat(params['epochs'])
    records = x.shape[0] # - vrecord
    steps   = records // params['gbatch_size']
    if records % params['gbatch_size']:
        steps += 1
    print(""steps   ="", steps)
    # Note that if we are resuming that the number of _remaining_ epochs has
    # changed!
    # The number of epochs * steps is the numbers of samples to drop
    print(""initial   cardinality = "", data.cardinality())
    print(""initial v cardinality = "", data.cardinality())
    data       = data.skip(initial_epoch*steps)
    validation = validation.skip(initial_epoch*vsteps)
    print(""final     cardinality = "", data.cardinality())
    print(""final v   cardinality = "", data.cardinality())
    # data = strat.experimental_distribute_dataset(data)
    # Split into validation and training
    callbacks  = createCallbacks(params, callbacks, rank, resume_training)
    print(callbacks)
    print(""fitting model"")
    print(data)





    import tensorflow as tf
    options = tf.data.Options()
    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF
    train_data = data.with_options(options)
    val_data = validation.with_options(options)

    history = model.fit(train_data, epochs=params['epochs'],
            batch_size=params['gbatch_size'],
            steps_per_epoch=steps,
            verbose=0,
            initial_epoch=initial_epoch,
            validation_data=val_data,
            validation_steps=vsteps,
            callbacks=callbacks)
    print(""fitting model done"")
    if rank == 0:
        model.save(""model-final"")
    else:
        model.save(""checkpoints/model-tmp"")




############### slurm script :

#!/bin/bash
#SBATCH --job-name=job1 # Job name
#SBATCH --mem=30000 # Job memory request
#SBATCH --gres=gpu:4 # Number of requested GPU(s)
#SBATCH --time=3-23:00:00 # Time limit days-hrs:min:sec
#SBATCH --constraint=rtx_2080 # Specific hardware constraint
#SBATCH --error=slurm.err # Error file name
#SBATCH --output=slurm.out # Output file name
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --array=1-2%1

if [ -d ""model-final"" ]
then
scancel $SLURM_ARRAY_JOB_ID
else
module load Anaconda3/2020.07
module load TensorFlow/2.6.0-foss-2021a-CUDA-11.3.1
mpirun python -u main.py resume_latest
fi
```


### Relevant log output

```shell
I log each and every epoch in a csv file. I see two csv files created and each one has different workers running.
```
</details>"
59797,'_UserObject' object has no attribute 'call_and_return_conditional_losses',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.2.3

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/15WKG30ORtNUcaRDUMnMc-oc2XdjNCoWc?authuser=1#scrollTo=SXlXqqFHwkqa
```


### Relevant log output

100%|██████████| 943/943 [00:00<00:00, 1567.84it/s]
n users- 943, n_items- 1682, user_group- 943, item_group- 1682 
[(<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([  3, 223, 465, ..., 640, 348,  55])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([400, 400, 595, ..., 863, 672,  24])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.6, 1. , 1. , ..., 1. , 0.8, 1. ])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([899, 192,  41, ...,  58, 394, 162])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([  49,   87, 1066, ...,  262,  717,  417])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([1. , 0.4, 0.6, ..., 1. , 0.8, 0.6])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([507, 865, 496, ..., 413,   8, 384])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([1167,  317,  767, ...,   49,  915,  273])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.6, 0.8, 0.8, ..., 0.6, 0.8, 1. ])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([878,  48, 518, ..., 300, 816, 739])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([820,   6, 656, ..., 446, 184, 534])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.8, 0.6, 0.6, ..., 0.6, 0.6, 0.8])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([506, 315, 359, ..., 375, 355, 615])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([652, 220, 251, ..., 657, 578,  25])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.8, 1. , 1. , ..., 0.8, 0.2, 0.4])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([534, 556, 454, ..., 192, 597,  43])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([ 454,   58, 1119, ...,   60,  357,  361])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.6, 0.6, 0.6, ..., 0.6, 1. , 1. ])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([166, 393, 580, ..., 110, 115, 927])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([502, 118, 779, ..., 112,  97, 143])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.8, 1. , 0.8, ..., 0.8, 0.6, 0.8])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([ 79, 119, 908, ..., 658,  47, 100])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([158, 403, 508, ..., 216, 506, 229])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.4, 1. , 0.8, ..., 0.8, 0.8, 0.6])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([492, 589,   2, ..., 313, 364, 158])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([449, 432, 258, ..., 860, 208, 372])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.4, 0.4, 0.8, ..., 0.8, 1. , 0.2])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([883, 314, 226, ...,  30, 469, 180])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([209, 108,  50, ..., 794, 104, 117])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([1. , 0.8, 0.8, ..., 0.6, 1. , 0.8])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([176, 500, 134, ..., 194, 676, 327])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([130, 269, 570, ..., 191, 366, 245])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.8, 0.8, 0.2, ..., 0.8, 0.8, 0.6])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([661, 215, 887, ..., 476, 179,  75])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([541, 419, 513, ..., 546,  33,  38])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.8, 1. , 0.6, ..., 0.6, 0.4, 0.6])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([  1, 145,  25, ..., 154, 416, 778])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([ 416,  189,  434, ...,  672,  289, 1082])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.6, 1. , 1. , ..., 0.8, 1. , 0.4])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([505, 548, 859, ..., 424, 108, 933])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([474, 505, 325, ..., 491, 186, 290])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.4, 1. , 1. , ..., 1. , 0.6, 0.2])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([ 96, 584, 153, ..., 428, 646, 630])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([ 340,  128,  145, ...,   97, 1008,  387])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.8, 1. , 0.8, ..., 0.4, 0.4, 0.6])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([384, 416, 342, ..., 718,  18,  58])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([ 221,  394,  543, ...,  445,  799, 1109])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.6, 0.8, 0.6, ..., 0.8, 0.6, 0.4])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([138, 187, 168, ..., 553, 402, 116])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([ 216,  445,    1, ...,  751, 1491,  732])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.8, 0.6, 1. , ..., 1. , 0.2, 0.8])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([388,  15, 151, ...,  90, 124, 115])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([ 842, 1048,  179, ...,  645,  189,   58])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.8, 0.2, 1. , ..., 0.4, 0.6, 0.8])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([ 75, 434, 325, ..., 554,  58,  18])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([547, 174, 484, ..., 334, 547,  25])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.2, 1. , 0.6, ..., 0.8, 1. , 0.6])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([364, 678, 440, ...,  75, 622,  15])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([ 56,   1, 840, ..., 698, 754, 135])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.6, 1. , 0.8, ..., 0.8, 0.8, 0.8])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([810, 730, 617, ..., 387, 925, 344])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([ 456,  320, 1044, ...,  978,  217,  139])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.8, 0.8, 0.4, ..., 0.6, 1. , 1. ])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([  3, 127, 114, ...,  83, 136,   5])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([302, 409, 435, ..., 920, 474,  94])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.6, 0.8, 0.6, ..., 0.6, 0.8, 0.8])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([426, 100, 714, ..., 143, 456, 446])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([274, 260,  24, ..., 799, 936, 154])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.8, 0.8, 0.6, ..., 0.6, 0.8, 1. ])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([ 10, 534, 908, ..., 596, 221, 768])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([ 108, 1058,    7, ...,  198,   52,  884])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.6, 0.2, 1. , ..., 0.8, 0.8, 0.6])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([ 92, 464, 453, ...,  94, 282, 776])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([253, 887, 288, ..., 112, 221, 552])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.8, 0.6, 1. , ..., 0.8, 0.8, 0.8])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([238, 365, 455, ..., 402, 736,   2])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([ 140,  108,  500, ..., 1333,   66,  622])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.8, 0.8, 0.8, ..., 0.8, 0.8, 1. ])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([803, 542,  94, ..., 440,  51, 117])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([ 504,  408,    6, ...,  365, 1177,   60])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.4, 0.6, 0.6, ..., 0.6, 0.8, 1. ])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([533, 706, 485, ..., 880, 387,  47])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([ 491,  864, 1015, ...,  170,  911,  265])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([1. , 0.8, 0.4, ..., 0.8, 0.6, 0.4])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([661, 606, 654, ..., 413, 283, 110])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([ 64, 391, 260, ...,  58,   6, 329])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.6, 0.8, 0.8, ..., 0.8, 0.6, 0.6])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([927, 180, 581, ..., 138, 251,  47])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([1240,  157,  355, ...,   24,  719,  291])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.6, 0.6, 1. , ..., 0.8, 1. , 0.8])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([339, 437, 516, ..., 617,  69, 465])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([ 665,  142,  217, ...,  542,   95, 1622])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([1. , 0.4, 1. , ..., 0.4, 0.8, 0.4])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([643, 579, 377, ..., 777,  41, 784])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([ 139,  559,  649, ..., 1320, 1180,  159])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.8, 0.6, 0.6, ..., 0.6, 0.8, 0.6])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([199, 131, 630, ...,  70, 220, 478])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([ 117,  408,  416, ...,   88,    0, 1294])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.4, 1. , 0.8, ..., 0.6, 1. , 0.4])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([927,  70, 151, ..., 837, 650, 366])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([1185,  406,  512, ...,  492,   81,  455])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.2, 0.6, 1. , ..., 1. , 0.4, 0.6])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([430, 409, 447, ..., 504,  38, 599])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([100, 488, 460, ..., 781, 403, 333])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.8, 1. , 0.4, ..., 0.6, 1. , 0.8])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([637, 430, 343, ...,  23, 181, 769])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([ 52, 629, 506, ..., 568,  88, 307])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.6, 0.8, 1. , ..., 0.8, 0.4, 0.6])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([325, 544, 379, ...,  90, 332, 450])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([ 474, 1166,  152, ...,  720,  473,  669])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.8, 0.8, 1. , ..., 0.8, 1. , 0.4])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([878, 602, 746, ..., 758, 239,  12])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([ 260,  389,  652, ...,  552,  225, 1023])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([1. , 0.4, 0.6, ..., 0.6, 0.8, 1. ])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([462,  62, 720, ...,  83, 238, 625])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([  60,  809, 1175, ...,  482,   60,  174])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.8, 0.2, 0.2, ..., 0.8, 0.6, 1. ])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([283, 674, 404, ..., 105,  37, 696])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([191, 488,  66, ..., 196, 370, 184])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.6, 0.2, 0.8, ..., 0.4, 0.4, 1. ])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([688, 226,  85, ..., 249, 835, 476])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([ 488,  310,  182, ..., 1034,  629,  361])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.6, 0.4, 0.6, ..., 0.6, 0.4, 0.8])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([437,  72, 483, ..., 405,  83,  90])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([183, 471, 849, ..., 256, 113, 647])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.8, 1. , 0.4, ..., 0.8, 1. , 0.6])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([  8, 650, 538, ..., 767, 309, 601])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([753, 913,  26, ...,  51, 347,  89])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([1. , 0.4, 0.6, ..., 0.8, 1. , 0.8])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([402,  33,  37, ..., 205, 569, 120])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([607, 730, 647, ..., 675, 170,  77])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.6, 0.2, 0.8, ..., 0.8, 1. , 0.6])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([264, 442, 784, ..., 393, 145, 513])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([ 131,  693,  310, ..., 1225,  221,  157])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.6, 0.8, 0.8, ..., 0.2, 0.8, 0.8])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([641, 483, 826, ...,  68, 587,  38])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([ 298,  970,    1, ...,  258, 1024,  208])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.8, 0.6, 0.6, ..., 0.6, 1. , 0.8])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([ 39, 119, 696, ..., 803, 325, 654])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([311, 307, 712, ..., 497, 241,  76])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.8, 0.6, 1. , ..., 0.4, 1. , 0.4])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([633, 859, 523, ..., 246, 128, 727])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([181, 115, 284, ...,  81, 297, 157])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.4, 0.8, 0.8, ..., 0.8, 1. , 0.8])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([143, 890, 752, ..., 635, 415, 551])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([289, 113, 543, ..., 525,  89, 350])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.6, 0.2, 0.6, ..., 0.6, 1. , 0.8])>), (<tf.Tensor: shape=(1600,), dtype=int64, numpy=array([ 11, 173, 404, ..., 421,  23,  14])>, <tf.Tensor: shape=(1600,), dtype=int64, numpy=array([904, 585, 506, ..., 403,  31, 387])>, <tf.Tensor: shape=(1600,), dtype=float64, numpy=array([0.6, 0.8, 0.8, ..., 0.6, 0.6, 0.6])>)]
Training Epoch 0: 100%|██████████| 50/50 [00:04<00:00, 10.76it/s]
Training Epoch 1: 100%|██████████| 50/50 [00:03<00:00, 12.92it/s]
Training Epoch 2: 100%|██████████| 50/50 [00:02<00:00, 20.62it/s]
Training Epoch 3: 100%|██████████| 50/50 [00:02<00:00, 20.51it/s]
Training Epoch 4: 100%|██████████| 50/50 [00:03<00:00, 14.40it/s]
Training Epoch 5: 100%|██████████| 50/50 [00:04<00:00, 11.95it/s]
Training Epoch 6: 100%|██████████| 50/50 [00:03<00:00, 12.75it/s]
Training Epoch 7: 100%|██████████| 50/50 [00:02<00:00, 20.19it/s]
Training Epoch 8: 100%|██████████| 50/50 [00:02<00:00, 20.61it/s]
Training Epoch 9: 100%|██████████| 50/50 [00:02<00:00, 20.55it/s]
Training Epoch 10: 100%|██████████| 50/50 [00:00<00:00, 326.32it/s]
Training Epoch 11: 100%|██████████| 50/50 [00:00<00:00, 322.16it/s]
WARNING:tensorflow:Skipping full serialization of Keras layer <drive.MyDrive.FAiR.Model.Discriminator object at 0x7fc62233cdc0>, because it is not built.
WARNING:tensorflow:Skipping full serialization of Keras layer <drive.MyDrive.FAiR.Model.Discriminator object at 0x7fc620a9ea60>, because it is not built.
WARNING:tensorflow:Skipping full serialization of Keras layer <drive.MyDrive.FAiR.Model.Discriminator object at 0x7fc624006f40>, because it is not built.
WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x7fc62233c3d0>, because it is not built.
WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x7fc620a9e700>, because it is not built.
WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x7fc624006730>, because it is not built.
WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x7fc62233c4f0>, because it is not built.
WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x7fc622da1c70>, because it is not built.
WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x7fc622da11c0>, because it is not built.
WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x7fc622da1760>, because it is not built.
WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x7fc620a9e2b0>, because it is not built.
WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x7fc620a711c0>, because it is not built.
WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x7fc620a71610>, because it is not built.
WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x7fc623727280>, because it is not built.
WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x7fc6237275e0>, because it is not built.
WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x7fc623727eb0>, because it is not built.
WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x7fc6240066d0>, because it is not built.
WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x7fc6241bb850>, because it is not built.
WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x7fc6241bbc40>, because it is not built.
WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x7fc6241bb370>, because it is not built.
WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x7fc624025b80>, because it is not built.
Training Epoch 12: 100%|██████████| 50/50 [00:00<00:00, 92.89it/s]
Training Epoch 13: 100%|██████████| 50/50 [00:00<00:00, 108.54it/s]
Training Epoch 14: 100%|██████████| 50/50 [00:00<00:00, 102.49it/s]
Training Epoch 15: 100%|██████████| 50/50 [00:00<00:00, 59.82it/s]
Training Epoch 16: 100%|██████████| 50/50 [00:00<00:00, 159.39it/s]
Training Epoch 17: 100%|██████████| 50/50 [00:00<00:00, 159.67it/s]
Training Epoch 18: 100%|██████████| 50/50 [00:00<00:00, 146.35it/s]
Training Epoch 19: 100%|██████████| 50/50 [00:00<00:00, 156.57it/s]
Training Epoch 20: 100%|██████████| 50/50 [00:00<00:00, 163.08it/s]
Training Epoch 21: 100%|██████████| 50/50 [00:00<00:00, 154.98it/s]
Training Epoch 22: 100%|██████████| 50/50 [00:00<00:00, 113.12it/s]
Training Epoch 23: 100%|██████████| 50/50 [00:00<00:00, 110.89it/s]
Training Epoch 24: 100%|██████████| 50/50 [00:00<00:00, 104.35it/s]
Training Epoch 25: 100%|██████████| 50/50 [00:00<00:00, 169.43it/s]
Training Epoch 26: 100%|██████████| 50/50 [00:00<00:00, 167.68it/s]
Training Epoch 27: 100%|██████████| 50/50 [00:00<00:00, 164.16it/s]
Training Epoch 28: 100%|██████████| 50/50 [00:00<00:00, 162.16it/s]
Training Epoch 29: 100%|██████████| 50/50 [00:00<00:00, 153.26it/s]
Training Epoch 30: 100%|██████████| 50/50 [00:00<00:00, 164.99it/s]
Training Epoch 31: 100%|██████████| 50/50 [00:00<00:00, 162.19it/s]
Training Epoch 32: 100%|██████████| 50/50 [00:00<00:00, 149.66it/s]
Training Epoch 33: 100%|██████████| 50/50 [00:00<00:00, 132.69it/s]
Training Epoch 34: 100%|██████████| 50/50 [00:00<00:00, 116.23it/s]
Training Epoch 35: 100%|██████████| 50/50 [00:00<00:00, 113.67it/s]
Training Epoch 36: 100%|██████████| 50/50 [00:00<00:00, 103.86it/s]
Training Epoch 37: 100%|██████████| 50/50 [00:00<00:00, 157.86it/s]
Training Epoch 38: 100%|██████████| 50/50 [00:00<00:00, 170.21it/s]
Training Epoch 39: 100%|██████████| 50/50 [00:00<00:00, 167.94it/s]
Training Epoch 40: 100%|██████████| 50/50 [00:00<00:00, 152.25it/s]
Training Epoch 41: 100%|██████████| 50/50 [00:00<00:00, 168.35it/s]
Training Epoch 42: 100%|██████████| 50/50 [00:00<00:00, 159.23it/s]
Training Epoch 43: 100%|██████████| 50/50 [00:00<00:00, 166.22it/s]
Training Epoch 44: 100%|██████████| 50/50 [00:00<00:00, 168.43it/s]
Training Epoch 45: 100%|██████████| 50/50 [00:00<00:00, 112.30it/s]
Training Epoch 46: 100%|██████████| 50/50 [00:00<00:00, 115.24it/s]
Training Epoch 47: 100%|██████████| 50/50 [00:00<00:00, 106.49it/s]
Training Epoch 48: 100%|██████████| 50/50 [00:00<00:00, 103.19it/s]
Training Epoch 49: 100%|██████████| 50/50 [00:00<00:00, 166.10it/s]
Training Epoch 50: 100%|██████████| 50/50 [00:00<00:00, 169.76it/s]
Training Epoch 51: 100%|██████████| 50/50 [00:00<00:00, 170.76it/s]
Training Epoch 52: 100%|██████████| 50/50 [00:00<00:00, 148.27it/s]
Training Epoch 53: 100%|██████████| 50/50 [00:00<00:00, 166.94it/s]
Training Epoch 54: 100%|██████████| 50/50 [00:00<00:00, 167.27it/s]
Training Epoch 55: 100%|██████████| 50/50 [00:00<00:00, 165.81it/s]
Training Epoch 56: 100%|██████████| 50/50 [00:00<00:00, 170.95it/s]
Training Epoch 57: 100%|██████████| 50/50 [00:00<00:00, 101.41it/s]
Training Epoch 58: 100%|██████████| 50/50 [00:00<00:00, 122.40it/s]
Training Epoch 59: 100%|██████████| 50/50 [00:00<00:00, 112.99it/s]
Training Epoch 60: 100%|██████████| 50/50 [00:00<00:00, 103.73it/s]
Training Epoch 61: 100%|██████████| 50/50 [00:00<00:00, 155.80it/s]
Training Epoch 62: 100%|██████████| 50/50 [00:00<00:00, 156.33it/s]
Training Epoch 63: 100%|██████████| 50/50 [00:00<00:00, 158.74it/s]
Training Epoch 64: 100%|██████████| 50/50 [00:00<00:00, 155.79it/s]
Training Epoch 65: 100%|██████████| 50/50 [00:00<00:00, 148.85it/s]
Training Epoch 66: 100%|██████████| 50/50 [00:00<00:00, 170.31it/s]
Training Epoch 67: 100%|██████████| 50/50 [00:00<00:00, 164.71it/s]
Training Epoch 68: 100%|██████████| 50/50 [00:00<00:00, 161.91it/s]
Training Epoch 69: 100%|██████████| 50/50 [00:00<00:00, 117.38it/s]
Training Epoch 70: 100%|██████████| 50/50 [00:00<00:00, 103.59it/s]
Training Epoch 71: 100%|██████████| 50/50 [00:00<00:00, 107.23it/s]
Training Epoch 72: 100%|██████████| 50/50 [00:00<00:00, 106.20it/s]
Training Epoch 73: 100%|██████████| 50/50 [00:00<00:00, 151.79it/s]
Training Epoch 74: 100%|██████████| 50/50 [00:00<00:00, 154.16it/s]
Training Epoch 75: 100%|██████████| 50/50 [00:00<00:00, 168.73it/s]
Training Epoch 76: 100%|██████████| 50/50 [00:00<00:00, 166.91it/s]
Training Epoch 77: 100%|██████████| 50/50 [00:00<00:00, 162.62it/s]
Training Epoch 78: 100%|██████████| 50/50 [00:00<00:00, 167.84it/s]
Training Epoch 79: 100%|██████████| 50/50 [00:00<00:00, 167.66it/s]
Training Epoch 80: 100%|██████████| 50/50 [00:00<00:00, 169.46it/s]
Training Epoch 81: 100%|██████████| 50/50 [00:00<00:00, 114.01it/s]
Training Epoch 82: 100%|██████████| 50/50 [00:00<00:00, 112.97it/s]
Training Epoch 83: 100%|██████████| 50/50 [00:00<00:00, 105.20it/s]
Training Epoch 84: 100%|██████████| 50/50 [00:00<00:00, 135.35it/s]
Training Epoch 85: 100%|██████████| 50/50 [00:00<00:00, 170.15it/s]
Training Epoch 86: 100%|██████████| 50/50 [00:00<00:00, 157.87it/s]
Training Epoch 87: 100%|██████████| 50/50 [00:00<00:00, 150.78it/s]
Training Epoch 88: 100%|██████████| 50/50 [00:00<00:00, 154.19it/s]
Training Epoch 89: 100%|██████████| 50/50 [00:00<00:00, 157.27it/s]
Training Epoch 90: 100%|██████████| 50/50 [00:00<00:00, 167.42it/s]
Training Epoch 91: 100%|██████████| 50/50 [00:00<00:00, 164.26it/s]
Training Epoch 92: 100%|██████████| 50/50 [00:00<00:00, 170.62it/s]
Training Epoch 93: 100%|██████████| 50/50 [00:00<00:00, 117.38it/s]
Training Epoch 94: 100%|██████████| 50/50 [00:00<00:00, 114.73it/s]
Training Epoch 95: 100%|██████████| 50/50 [00:00<00:00, 105.86it/s]
Training Epoch 96: 100%|██████████| 50/50 [00:00<00:00, 129.81it/s]
Training Epoch 97: 100%|██████████| 50/50 [00:00<00:00, 162.66it/s]
Training Epoch 98: 100%|██████████| 50/50 [00:00<00:00, 146.82it/s]
Training Epoch 99: 100%|██████████| 50/50 [00:00<00:00, 152.41it/s]
Training Epoch 100: 100%|██████████| 50/50 [00:00<00:00, 164.92it/s]
Training Epoch 101: 100%|██████████| 50/50 [00:00<00:00, 162.13it/s]
Training Epoch 102: 100%|██████████| 50/50 [00:00<00:00, 158.95it/s]
Training Epoch 103: 100%|██████████| 50/50 [00:00<00:00, 168.56it/s]
Training Epoch 104: 100%|██████████| 50/50 [00:00<00:00, 140.09it/s]
Training Epoch 105: 100%|██████████| 50/50 [00:00<00:00, 116.76it/s]
Training Epoch 106: 100%|██████████| 50/50 [00:00<00:00, 112.11it/s]
Training Epoch 107: 100%|██████████| 50/50 [00:00<00:00, 101.93it/s]
Training Epoch 108: 100%|██████████| 50/50 [00:00<00:00, 168.75it/s]
Training Epoch 109: 100%|██████████| 50/50 [00:00<00:00, 162.03it/s]
Training Epoch 110: 100%|██████████| 50/50 [00:00<00:00, 171.34it/s]
Training Epoch 111: 100%|██████████| 50/50 [00:00<00:00, 158.02it/s]
Training Epoch 112: 100%|██████████| 50/50 [00:00<00:00, 146.59it/s]
Training Epoch 113: 100%|██████████| 50/50 [00:00<00:00, 155.74it/s]
Training Epoch 114: 100%|██████████| 50/50 [00:00<00:00, 157.36it/s]
Training Epoch 115: 100%|██████████| 50/50 [00:00<00:00, 165.48it/s]
Training Epoch 116: 100%|██████████| 50/50 [00:00<00:00, 118.03it/s]
Training Epoch 117: 100%|██████████| 50/50 [00:00<00:00, 121.25it/s]
Training Epoch 118: 100%|██████████| 50/50 [00:00<00:00, 108.64it/s]
Training Epoch 119: 100%|██████████| 50/50 [00:00<00:00, 96.16it/s]
Training Epoch 120: 100%|██████████| 50/50 [00:00<00:00, 167.09it/s]
Training Epoch 121: 100%|██████████| 50/50 [00:00<00:00, 154.64it/s]
Training Epoch 122: 100%|██████████| 50/50 [00:00<00:00, 149.41it/s]
Training Epoch 123: 100%|██████████| 50/50 [00:00<00:00, 150.65it/s]
Training Epoch 124: 100%|██████████| 50/50 [00:00<00:00, 173.42it/s]
Training Epoch 125: 100%|██████████| 50/50 [00:00<00:00, 168.39it/s]
Training Epoch 126: 100%|██████████| 50/50 [00:00<00:00, 165.04it/s]
Training Epoch 127: 100%|██████████| 50/50 [00:00<00:00, 145.72it/s]
Training Epoch 128: 100%|██████████| 50/50 [00:00<00:00, 110.59it/s]
Training Epoch 129: 100%|██████████| 50/50 [00:00<00:00, 100.26it/s]
Training Epoch 130: 100%|██████████| 50/50 [00:00<00:00, 99.18it/s]
Training Epoch 131: 100%|██████████| 50/50 [00:00<00:00, 109.52it/s]
Training Epoch 132: 100%|██████████| 50/50 [00:00<00:00, 168.14it/s]
Training Epoch 133: 100%|██████████| 50/50 [00:00<00:00, 168.50it/s]
Training Epoch 134: 100%|██████████| 50/50 [00:00<00:00, 159.76it/s]
Training Epoch 135: 100%|██████████| 50/50 [00:00<00:00, 164.90it/s]
Training Epoch 136: 100%|██████████| 50/50 [00:00<00:00, 156.67it/s]
Training Epoch 137: 100%|██████████| 50/50 [00:00<00:00, 165.58it/s]
Training Epoch 138: 100%|██████████| 50/50 [00:00<00:00, 154.86it/s]
Training Epoch 139: 100%|██████████| 50/50 [00:00<00:00, 158.34it/s]
Training Epoch 140: 100%|██████████| 50/50 [00:00<00:00, 122.73it/s]
Training Epoch 141: 100%|██████████| 50/50 [00:00<00:00, 111.02it/s]
Training Epoch 142: 100%|██████████| 50/50 [00:00<00:00, 100.51it/s]
Training Epoch 143: 100%|██████████| 50/50 [00:00<00:00, 133.45it/s]
Training Epoch 144: 100%|██████████| 50/50 [00:00<00:00, 158.62it/s]
Training Epoch 145: 100%|██████████| 50/50 [00:00<00:00, 148.27it/s]
Training Epoch 146: 100%|██████████| 50/50 [00:00<00:00, 145.91it/s]
Training Epoch 147: 100%|██████████| 50/50 [00:00<00:00, 155.55it/s]
Training Epoch 148: 100%|██████████| 50/50 [00:00<00:00, 165.33it/s]
Training Epoch 149: 100%|██████████| 50/50 [00:00<00:00, 166.96it/s]
Training Epoch 150: 100%|██████████| 50/50 [00:00<00:00, 165.65it/s]
Training Epoch 151: 100%|██████████| 50/50 [00:00<00:00, 163.49it/s]
Training Epoch 152: 100%|██████████| 50/50 [00:00<00:00, 115.49it/s]
Training Epoch 153: 100%|██████████| 50/50 [00:00<00:00, 118.75it/s]
Training Epoch 154: 100%|██████████| 50/50 [00:00<00:00, 95.54it/s]
Training Epoch 155: 100%|██████████| 50/50 [00:00<00:00, 140.76it/s]
Training Epoch 156: 100%|██████████| 50/50 [00:00<00:00, 167.49it/s]
Training Epoch 157: 100%|██████████| 50/50 [00:00<00:00, 140.92it/s]
Training Epoch 158: 100%|██████████| 50/50 [00:00<00:00, 151.91it/s]
Training Epoch 159: 100%|██████████| 50/50 [00:00<00:00, 152.28it/s]
Training Epoch 160: 100%|██████████| 50/50 [00:00<00:00, 156.73it/s]
Training Epoch 161: 100%|██████████| 50/50 [00:00<00:00, 164.17it/s]
Training Epoch 162: 100%|██████████| 50/50 [00:00<00:00, 162.42it/s]
Training Epoch 163: 100%|██████████| 50/50 [00:00<00:00, 107.74it/s]
Training Epoch 164: 100%|██████████| 50/50 [00:00<00:00, 110.53it/s]
Training Epoch 165: 100%|██████████| 50/50 [00:00<00:00, 107.77it/s]
Training Epoch 166: 100%|██████████| 50/50 [00:00<00:00, 109.52it/s]
Training Epoch 167: 100%|██████████| 50/50 [00:00<00:00, 164.60it/s]
Training Epoch 168: 100%|██████████| 50/50 [00:00<00:00, 154.28it/s]
Training Epoch 169: 100%|██████████| 50/50 [00:00<00:00, 165.34it/s]
Training Epoch 170: 100%|██████████| 50/50 [00:00<00:00, 161.77it/s]
Training Epoch 171: 100%|██████████| 50/50 [00:00<00:00, 150.51it/s]
Training Epoch 172: 100%|██████████| 50/50 [00:00<00:00, 157.45it/s]
Training Epoch 173: 100%|██████████| 50/50 [00:00<00:00, 154.22it/s]
Training Epoch 174: 100%|██████████| 50/50 [00:00<00:00, 154.50it/s]
Training Epoch 175: 100%|██████████| 50/50 [00:00<00:00, 106.84it/s]
Training Epoch 176: 100%|██████████| 50/50 [00:00<00:00, 106.52it/s]
Training Epoch 177: 100%|██████████| 50/50 [00:00<00:00, 97.56it/s]
Training Epoch 178: 100%|██████████| 50/50 [00:00<00:00, 113.32it/s]
Training Epoch 179: 100%|██████████| 50/50 [00:00<00:00, 165.54it/s]
Training Epoch 180: 100%|██████████| 50/50 [00:00<00:00, 164.48it/s]
Training Epoch 181: 100%|██████████| 50/50 [00:00<00:00, 154.58it/s]
Training Epoch 182: 100%|██████████| 50/50 [00:00<00:00, 152.07it/s]
Training Epoch 183: 100%|██████████| 50/50 [00:00<00:00, 157.86it/s]
Training Epoch 184: 100%|██████████| 50/50 [00:00<00:00, 167.20it/s]
Training Epoch 185: 100%|██████████| 50/50 [00:00<00:00, 167.96it/s]
Training Epoch 186: 100%|██████████| 50/50 [00:00<00:00, 168.72it/s]
Training Epoch 187: 100%|██████████| 50/50 [00:00<00:00, 102.82it/s]
Training Epoch 188: 100%|██████████| 50/50 [00:00<00:00, 103.59it/s]
Training Epoch 189: 100%|██████████| 50/50 [00:00<00:00, 97.17it/s]
Training Epoch 190: 100%|██████████| 50/50 [00:00<00:00, 164.43it/s]
Training Epoch 191: 100%|██████████| 50/50 [00:00<00:00, 171.74it/s]
Training Epoch 192: 100%|██████████| 50/50 [00:00<00:00, 156.43it/s]
Training Epoch 193: 100%|██████████| 50/50 [00:00<00:00, 165.72it/s]
Training Epoch 194: 100%|██████████| 50/50 [00:00<00:00, 169.39it/s]
Training Epoch 195: 100%|██████████| 50/50 [00:00<00:00, 172.00it/s]
Training Epoch 196: 100%|██████████| 50/50 [00:00<00:00, 170.08it/s]
Training Epoch 197: 100%|██████████| 50/50 [00:00<00:00, 161.59it/s]
Training Epoch 198: 100%|██████████| 50/50 [00:00<00:00, 152.73it/s]
Training Epoch 199: 100%|██████████| 50/50 [00:00<00:00, 106.74it/s]
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
[<ipython-input-15-9ec0e19149d9>](https://localhost:8080/#) in <module>
    111     parser.add_argument('-f')
    112     flag = parser.parse_args()
--> 113     main(flag)
    114 

9 frames
[/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/saving/saved_model/load.py](https://localhost:8080/#) in _finalize_saved_model_layers(layers)
    673       _set_network_attributes_from_metadata(layer)
    674 
--> 675       call_fn = _get_keras_attr(layer).call_and_return_conditional_losses
    676       if call_fn.input_signature is None:
    677         inputs = infer_inputs_from_restored_call_function(call_fn)

AttributeError: '_UserObject' object has no attribute 'call_and_return_conditional_losses'</details>"
59796,AutoGraph did convert this function: NameError: name 'Tuple' is not defined,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Mac, Linux, Google Colab

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

</details>

### Current Behaviour?

`tf.function` fails when type annotations are used which are locally imported. I get:

```
Cause: name 'Tuple' is not defined
```

Note:

* It seems important that `Tuple` is locally imported. If it is imported globally in the module, there does not seem to be a problem.
* When I use `from __future__ import annotations`, there is also no error. But I assume because this will just not evaluate it directly, but it still lacks the `Tuple` reference, although it's maybe really not relevant then.


### Standalone code to reproduce the issue

```python
import tensorflow as tf

print(""TensorFlow:"", tf.__version__)

tf.compat.v1.disable_eager_execution()

session = tf.compat.v1.Session()


# https://www.tensorflow.org/guide/function


def f(x: tf.Tensor):
  from typing import Tuple

  @tf.function
  def local_func(x: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:
    while tf.reduce_sum(x) > 1:
      tf.print(x)
      x = tf.tanh(x)
    return x, x

  return local_func(x)


session.run(f(tf.random.uniform([5])))
```


Colab link: https://colab.research.google.com/drive/1K69XH_RsU-Ux-RBfUd0B4eR8iJFTXoFM?usp=sharing

### Relevant log output

```
Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 427, in converted_call
    converted_f = _convert_actual(target_entity, program_ctx)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 269, in _convert_actual
    transformed, module, source_map = _TRANSPILER.transform(entity, program_ctx)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/transpiler.py"", line 282, in transform
    return self.transform_function(obj, user_context)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/transpiler.py"", line 490, in transform_function
    transformed_fn = factory.instantiate(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/transpiler.py"", line 213, in instantiate
    new_fn = bound_factory(**self._extra_locals)  # pylint:disable=not-callable
  File ""/tmp/__autograph_generated_filem5lq6_hq.py"", line 6, in inner_factory
    def tf__local_func(x: tf.Tensor) -> Tuple[(tf.Tensor, tf.Tensor)]:
NameError: name 'Tuple' is not defined
WARNING:tensorflow:AutoGraph could not transform <function f.<locals>.local_func at 0x7f15f5ff3af0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: name 'Tuple' is not defined
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
TensorFlow: 2.11.0
Converted call: <function f.<locals>.local_func at 0x7f15f5ff3af0>
    args: (<tf.Tensor 'x:0' shape=(5,) dtype=float32>,)
    kwargs: {}

<function f.<locals>.local_func at 0x7f15f5ff3af0> is not cached for subkey ConversionOptions[{}]
Source code of <function f.<locals>.local_func at 0x7f15f5ff3af0>:

@tf.function
def local_func(x: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:
  while tf.reduce_sum(x) > 1:
    tf.print(x)
    x = tf.tanh(x)
  return x, x


Transformed <function f.<locals>.local_func at 0x7f15f5ff3af0>:

# coding=utf-8
def tf__local_func(x: tf.Tensor) -> Tuple[(tf.Tensor, tf.Tensor)]:
    with ag__.FunctionScope('local_func', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:
        do_return = False
        retval_ = ag__.UndefinedReturnValue()

        def get_state():
            return (x,)

        def set_state(vars_):
            nonlocal x
            (x,) = vars_

        def loop_body():
            nonlocal x
            ag__.converted_call(ag__.ld(tf).print, (ag__.ld(x),), None, fscope)
            x = ag__.converted_call(ag__.ld(tf).tanh, (ag__.ld(x),), None, fscope)

        def loop_test():
            return (ag__.converted_call(ag__.ld(tf).reduce_sum, (ag__.ld(x),), None, fscope) > 1)
        ag__.while_stmt(loop_test, loop_body, get_state, set_state, ('x',), {})
        try:
            do_return = True
            retval_ = (ag__.ld(x), ag__.ld(x))
        except:
            do_return = False
            raise
        return fscope.ret(retval_, do_return)

Error transforming entity <function f.<locals>.local_func at 0x7f15f5ff3af0>
WARNING: AutoGraph could not transform <function f.<locals>.local_func at 0x7f15f5ff3af0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: name 'Tuple' is not defined
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
```
"
59794,Building libtensorflowlite_gpu_delegate.so with bazel raise an error.," ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.11.0

### OS Platform and Distribution

Linux RHEL 8


### Python version

3.10.8

### Bazel version

6.0.0

### GCC/Compiler version

8.5.0

### Android NDK version

android-ndk-r21e

### Current Behaviour?

I tried to build tflite gpu library for android using bazel. There is no problem when building `libtensorflow.so`, but error is occurred when building `libtensorflowlite_gpu_delegate.so`.


### Standalone code to reproduce the issue
- build `libtensorflowlite_gpu_delegate.so`
```shell

bazel build --config android_arm -c opt //tensorflow/lite:libtensorflowlite.so
```
- build `libtensorflowlite_gpu_delegate.so`
```shell

bazel build --config android_arm -c opt //tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so
```


### Relevant log output

```
ERROR: Traceback (most recent call last):
        File ""/home1/irteam/users/yeonghwa.jin/AR.MobileDL/tensorflow_src/tensorflow/lite/delegates/gpu/BUILD"", line 153, column 21, in <toplevel>
                ios_static_framework(
        File ""/home1/irteam/.cache/bazel/_bazel_irteam/326ce39c70196ccdfc072be4c665d440/external/build_bazel_rules_apple/apple/ios.bzl"", line 62, column 11, in ios_static_framework
                native.apple_static_library(
Error: no native function or rule 'apple_static_library'
Available attributes: aar_import, action_listener, alias, android_binary, android_device, android_device_script_fixture, android_host_service_fixture, android_instrumentation_test, android_library, android_local_test, android_sdk, android_tools_defaults_jar, apple_cc_toolchain, available_xcodes, cc_binary, cc_host_toolchain_alias, cc_import, cc_libc_top_alias, cc_library, cc_proto_library, cc_shared_library, cc_shared_library_permissions, cc_test, cc_toolchain, cc_toolchain_alias, cc_toolchain_suite, config_feature_flag, config_setting, constraint_setting, constraint_value, environment, existing_rule, existing_rules, exports_files, extra_action, fdo_prefetch_hints, fdo_profile, filegroup, genquery, genrule, glob, j2objc_library, java_binary, java_import, java_library, java_lite_proto_library, java_package_configuration, java_plugin, java_plugins_flag_alias, java_proto_library, java_runtime, java_test, java_toolchain, label_flag, label_setting, objc_import, objc_library, package, package_group, package_name, platform, propeller_optimize, proto_lang_toolchain, proto_library, py_binary, py_library, py_runtime, py_test, repository_name, sh_binary, sh_library, sh_test, subpackages, test_suite, toolchain, toolchain_type, xcode_config, xcode_config_alias, xcode_version
ERROR: Skipping '//tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so': Error evaluating '//tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so': error loading package 'tensorflow/lite/delegates/gpu': Package 'tensorflow/lite/delegates/gpu' contains errors
WARNING: Target pattern parsing failed.
ERROR: Error evaluating '//tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so': error loading package 'tensorflow/lite/delegates/gpu': Package 'tensorflow/lite/delegates/gpu' contains errors
INFO: Elapsed time: 0.234s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (1 packages loaded)
```"
59793,Test issue no need to take any action.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
59790,Tensorflow grad,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.X

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hi:
If i want to Slimming Network
```


### Standalone code to reproduce the issue

```shell
I want to ask how I can get the gradient value of each iteration in the model training process.
```


### Relevant log output

_No response_</details>"
59789,assertAlmostEqual raise confusing error message when test fails,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_
</details>

### Current Behaviour?

I seem to observe to issues with `assertAlmostEqual` of `tf.test.TestCase`
1. When using `assertAlmostEqual` and when it fails, the error message is confusing. Instead of saying the test fails, it shows the following error message:

```python

File ""..."", line ..., in test_dummy
    self.assertAlmostEqual(a, b)
File "".../python3.9/unittest/case.py"", line 874, in assertAlmostEqual
    if round(diff, places) == 0:
File "".../tensorflow/python/ops/numpy_ops/np_array_ops.py"", line 733, in around
    return a.astype(dtype)
File "".../tensorflow/python/framework/ops.py"", line 440, in __getattr__
    raise AttributeError(

AttributeError: EagerTensor object has no attribute 'astype'.
        If you are looking for numpy-related methods, please run the following:
        from tensorflow.python.ops.numpy_ops import np_config
        np_config.enable_numpy_behavior()
```
2. when I set the `places` to a smaller value that I think should allow the test to pass, the test still fails with the above error message.

When the difference is indeed smaller than the default `places=7`, the test passes.


### Standalone code to reproduce the issue

```python
import tensorflow as tf

class TestDummy(tf.test.TestCase):

    def test_dummy_confusing_error(self):
        # this test should fail. But it raises the above confusing error message.
        a = tf.constant(1.00002, dtype=tf.float32)
        b = tf.constant(1.00001, dtype=tf.float32)
        self.assertAlmostEqual(a, b)
    
    def test_dummy_confusing_bug(self):
        # this test should pass with places=2. But it still raises the above confusing error message.
        a = tf.constant(1.00002, dtype=tf.float32)
        b = tf.constant(1.00001, dtype=tf.float32)
        self.assertAlmostEqual(a, b, places=2)

    def test_dummy_no_error(self):
        # this test passes 
        a = tf.constant(1.000000002, dtype=tf.float32)
        b = tf.constant(1.000000001, dtype=tf.float32)
        self.assertAlmostEqual(a, b)

if __name__ == ""__main__"":
    tf.test.main()
```"
59787,MHLO_SignOp should not be converted to TF_SignOp,"### System information

This issue applies to all systems on all TF versions since 2020-03-28.

### Describe the problem
MHLO_SignOp is defined to [return -0.0](https://github.com/tensorflow/tensorflow/blob/007ef257793b6ea0464361d2fc6a5a5b57403ce5/tensorflow/compiler/xla/mlir_hlo/mhlo/IR/hlo_ops.td#L562) when input is -0.0, but TF_SignOp and [TFL_SignOp](https://github.com/tensorflow/tensorflow/blob/007ef257793b6ea0464361d2fc6a5a5b57403ce5/tensorflow/compiler/mlir/lite/ir/tfl_ops.td#L5036) both return +0.0 on -0.0. For example:

> \>\>\> tf.math.sign([-0.0, 0.0, 1, -1])
> <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.,  0.,  1., -1.], dtype=float32)>

### Source code / logs
https://github.com/tensorflow/tensorflow/blob/6b167daef1d6953fd908c1ced6f347e1a7758138/tensorflow/compiler/mlir/tensorflow/transforms/legalize_hlo_patterns.td#L132
"
59781,"Error When Trying to Get Value for bert_model in ""Classify text with BERT"" tutorial","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.4.4

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am trying to run the ""Classify text with BERT"" tutorial using Jupyter Notebook on a remote server. When I tried to implement 'bert_model = hub.KerasLayer(tfhub_handle_encoder)', it never returned a result and it is still executing with the asterik (*) next to the cell. 

Is this supposed to happen, and is it normal for that line to take a long time to execute?
```


### Standalone code to reproduce the issue

```shell
https://www.tensorflow.org/text/tutorials/classify_text_with_bert

The code is this line:
'bert_model = hub.KerasLayer(tfhub_handle_encoder)'
```


### Relevant log output

_No response_</details>"
59780,TensorFlow crashes with a segfault,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tensorflow-macos 2.11.0

### Custom Code

Yes

### OS Platform and Distribution

MacOS 12, tensorflow-macos and tensorflow-metal

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

</details>

### GPU model and memory

Apple M1

### Current Behaviour?

It crashes on Apple M1 hardware. It should not crash. It does not crash on other hardware.

I think the code should work. It does work fine on other hardware. But even if there is sth wrong with the code, it still should not crash, but throw some exception instead.


### Standalone code to reproduce the issue

On Apple M1 hardware:

* Checkout https://github.com/rwth-i6/returnn. (Maybe commit 3a67da87c2fd8783c5c2469d72cf1319b5b45837 to be sure.)
* Run: `python3 tests/test_TFUtil.py test_get_variable_grad_from_update_ops`

The relevant code:
* https://github.com/rwth-i6/returnn/blob/3a67da87c2fd8783c5c2469d72cf1319b5b45837/tests/test_TFUtil.py#L3507
* https://github.com/rwth-i6/returnn/blob/3a67da87c2fd8783c5c2469d72cf1319b5b45837/returnn/tf/util/basic.py#L6649

Specifically:

```python
def test_get_variable_grad_from_update_ops():
    with tf_compat.v1.variable_scope(""test_get_variable_grad_from_update_ops""):
        var = tf_compat.v1.get_variable(""var"", (), initializer=tf.zeros_initializer())
        loss = (var - 1.0) ** 2
        for opt in [
            tf_compat.v1.train.AdamOptimizer(),
            tf_compat.v1.train.GradientDescentOptimizer(learning_rate=1.0),
            tf_compat.v1.train.MomentumOptimizer(learning_rate=0.1, momentum=0.9),
            tf_compat.v1.train.RMSPropOptimizer(learning_rate=0.1),
        ]:
            print(""Optimizer:"", opt)
            minimize_op = opt.minimize(loss=loss, var_list=[var])
            assert isinstance(minimize_op, tf.Operation)
            update_ops = get_var_update_ops(var, fetches=minimize_op)
            print(""update ops:"", update_ops)
            print(""update op keys:"", get_op_attrib_keys(update_ops[0]))
            print(""update op inputs by name:"", get_op_input_names(update_ops[0]))
            session.run(var.initializer)  # reset
            session.run(tf_compat.v1.global_variables_initializer())  # from Adam or so
            assert_equal(session.run(var), 0.0)
            grad = get_variable_grad_from_update_ops(var, update_ops)
            print(""grad:"", grad)
            _, grad_np = session.run([minimize_op, grad])
            assert_equal(grad_np, -2.0)
```

But there are a few helpers `get_variable_grad_from_update_ops`, `get_var_update_ops` etc.


### Relevant log output

```
Executing: test_get_variable_grad_from_update_ops
Optimizer: <tensorflow.python.training.adam.AdamOptimizer object at 0x16bbe3670>
update ops: [<tf.Operation 'test_get_variable_grad_from_update_ops/Adam/update_test_get_variable_grad_from_update_ops/var/ResourceApplyAdam' type=ResourceApplyAdam>]
update op keys: ['use_nesterov', '_has_manual_control_dependencies', 'use_locking', '_class', 'T']
update op inputs by name: ['var', 'm', 'v', 'beta1_power', 'beta2_power', 'lr', 'beta1', 'beta2', 'epsilon', 'grad']
2023-02-23 10:01:13.694539: W tensorflow/c/c_api.cc:291] Operation '{name:'test_get_variable_grad_from_update_ops/test_get_variable_grad_from_update_ops/var/Adam_1/Assign' id:60 op device:{requested: '', assigned: ''} def:{{{node test_get_variable_grad_from_update_ops/test_get_variable_grad_from_update_ops/var/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](test_get_variable_grad_from_update_ops/test_get_variable_grad_from_update_ops/var/Adam_1, test_get_variable_grad_from_update_ops/test_get_variable_grad_from_update_ops/var/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
grad: Tensor(""test_get_variable_grad_from_update_ops/gradients/test_get_variable_grad_from_update_ops/sub_grad/tuple/control_dependency:0"", shape=(), dtype=float32)
Optimizer: <tensorflow.python.training.gradient_descent.GradientDescentOptimizer object at 0x16bbe36d0>
update ops: [<tf.Operation 'test_get_variable_grad_from_update_ops/GradientDescent/update_test_get_variable_grad_from_update_ops/var/ResourceApplyGradientDescent' type=ResourceApplyGradientDescent>]
update op keys: ['use_locking', '_class', '_has_manual_control_dependencies', 'T']
update op inputs by name: ['var', 'alpha', 'delta']
2023-02-23 10:01:13.768957: W tensorflow/c/c_api.cc:291] Operation '{name:'test_get_variable_grad_from_update_ops/test_get_variable_grad_from_update_ops/var/Adam_1/Assign' id:60 op device:{requested: '', assigned: ''} def:{{{node test_get_variable_grad_from_update_ops/test_get_variable_grad_from_update_ops/var/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](test_get_variable_grad_from_update_ops/test_get_variable_grad_from_update_ops/var/Adam_1, test_get_variable_grad_from_update_ops/test_get_variable_grad_from_update_ops/var/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
grad: Tensor(""test_get_variable_grad_from_update_ops/gradients_1/test_get_variable_grad_from_update_ops/sub_grad/tuple/control_dependency:0"", shape=(), dtype=float32)
Optimizer: <tensorflow.python.training.momentum.MomentumOptimizer object at 0x16bbe35b0>
update ops: [<tf.Operation 'test_get_variable_grad_from_update_ops/Momentum/update_test_get_variable_grad_from_update_ops/var/ResourceApplyMomentum' type=ResourceApplyMomentum>]
update op keys: ['use_locking', 'T', '_has_manual_control_dependencies', '_class', 'use_nesterov']
update op inputs by name: ['var', 'accum', 'lr', 'grad', 'momentum']
2023-02-23 10:01:13.805807: W tensorflow/c/c_api.cc:291] Operation '{name:'test_get_variable_grad_from_update_ops/test_get_variable_grad_from_update_ops/var/Momentum/Assign' id:138 op device:{requested: '', assigned: ''} def:{{{node test_get_variable_grad_from_update_ops/test_get_variable_grad_from_update_ops/var/Momentum/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](test_get_variable_grad_from_update_ops/test_get_variable_grad_from_update_ops/var/Momentum, test_get_variable_grad_from_update_ops/test_get_variable_grad_from_update_ops/var/Momentum/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
grad: Tensor(""test_get_variable_grad_from_update_ops/gradients_2/test_get_variable_grad_from_update_ops/sub_grad/tuple/control_dependency:0"", shape=(), dtype=float32)
Fatal Python error: Segmentation fault

Thread 0x0000000103500580 (most recent call first):
  File ""/Users/az/.local/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 1454 in _call_tf_sessionrun
  File ""/Users/az/.local/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 1361 in _run_fn
  File ""/Users/az/.local/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 1378 in _do_call
  File ""/Users/az/.local/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 1371 in _do_run
  File ""/Users/az/.local/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 1191 in _run
  File ""/Users/az/.local/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 968 in run
  File ""/Users/az/Programmierung/crnn/tests/test_TFUtil.py"", line 3529 in test_get_variable_grad_from_update_ops
  File ""/Users/az/Programmierung/crnn/tests/test_TFUtil.py"", line 4559 in <module>
fish: Job 1, 'python3 tests/test_TFUtil.py te…' terminated by signal SIGSEGV (Address boundary error)
```

Stack trace in LLDB in the crashing thread:
```
* thread #28, queue = 'metal gpu stream', stop reason = EXC_BAD_ACCESS (code=1, address=0xbeaddc3f8010)
  * frame #0: 0x00000001836ea5a0 libobjc.A.dylib`objc_msgSend + 32
    frame #1: 0x000000018df96d38 MPSNDArray`___lldb_unnamed_symbol1550 + 2292
    frame #2: 0x000000018df98bbc MPSNDArray`___lldb_unnamed_symbol1567 + 300
    frame #3: 0x000000018df991e8 MPSNDArray`___lldb_unnamed_symbol1569 + 176
    frame #4: 0x0000000159a7d2b8 libmetal_plugin.dylib`invocation function for block in double dispatchOneKernel<MPSNDArrayIdentity>(MetalStream*, MPSNDArrayIdentity*, NSArray*, MPSNDArray*, char const*, MPSKernelDAGObject*) + 120
    frame #5: 0x00000001836a01b4 libdispatch.dylib`_dispatch_client_callout + 20
    frame #6: 0x00000001836af414 libdispatch.dylib`_dispatch_lane_barrier_sync_invoke_and_complete + 56
    frame #7: 0x0000000159a7d140 libmetal_plugin.dylib`double dispatchOneKernel<MPSNDArrayIdentity>(MetalStream*, MPSNDArrayIdentity*, NSArray*, MPSNDArray*, char const*, MPSKernelDAGObject*) + 120
    frame #8: 0x0000000159a7fffc libmetal_plugin.dylib`metal_plugin::MPSApplyMomentumOp<float>::Compute(metal_plugin::OpKernelContext*) + 2768
    frame #9: 0x0000000159a7f2fc libmetal_plugin.dylib`void metal_plugin::ComputeOpKernel<metal_plugin::MPSApplyMomentumOp<float> >(void*, TF_OpKernelContext*) + 44
    frame #10: 0x000000014cd00028 libtensorflow_framework.2.dylib`tensorflow::PluggableDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) + 148
    frame #11: 0x000000014cc847f0 libtensorflow_framework.2.dylib`tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::Process(tensorflow::SimplePropagatorState::TaggedNode, long long) + 3764
    frame #12: 0x000000028a47eb6c _pywrap_tensorflow_internal.so`Eigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int) + 1496
    frame #13: 0x000000028a47e468 _pywrap_tensorflow_internal.so`tsl::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'()::operator()() const + 80
    frame #14: 0x000000014cb9e878 libtensorflow_framework.2.dylib`tsl::(anonymous namespace)::PThread::ThreadFn(void*) + 120
    frame #15: 0x000000018386426c libsystem_pthread.dylib`_pthread_start + 148
```

As you see from the output, the crash happens in the last `session.run([minimize_op, grad])`.
"
59779,You must feed a value for placeholder tensor ... with dtype int32,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

v1.12.1-89814-gbaac3548e86 2.13.0-dev20230222

### Custom Code

No

### OS Platform and Distribution

Fedora Linux 37

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA Version: 12.0, CUDNN unavailable,  not using gpu anyway

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When training a model defined with the subclass interface, tensorflow throws up many copies of a warning that doesn't appear to have any visible consequences. It does not halt training and model performance seems unaffected, but reporting because somewhat unsightly and may be the source of some insidious error. This error message did not appear when running the same code under TF 2.11, python 3.10. Issue began immediately after upgrade to TF 2.12, python 3.11. Also confirmed present in nightly.
```


### Standalone code to reproduce the issue

```shell
from tensorflow import keras

class build_rnn(keras.Model):
  def __init__(self, layer_size = 128, dim = 5, dropout = 0.2):
    super().__init__(self)
    self.pre = keras.Sequential([keras.layers.Input(shape=(None, dim), ragged=True), keras.layers.GaussianNoise(0.2)])

    self.rnn1 = keras.layers.LSTM(layer_size, return_sequences = True, return_state = True, dropout = dropout, kernel_regularizer=l1(1e-6), recurrent_regularizer=l1(1e-6), bias_regularizer=l1(1e-6))
    self.bn = keras.layers.BatchNormalization()
    self.rnn2 = keras.layers.LSTM(layer_size, return_sequences = True, return_state = True, dropout = dropout, kernel_regularizer=l2(1e-6), recurrent_regularizer=l2(1e-6), bias_regularizer=l2(1e-6))
    self.res = keras.layers.Add()
    self.rnn3 = keras.layers.LSTM(layer_size, return_sequences = False, return_state = True, dropout = dropout, kernel_regularizer=l1_l2(1e-6), recurrent_regularizer=l1_l2(1e-6), bias_regularizer=l1_l2(1e-6))
    
    self.dense1 = keras.layers.Dense(32, kernel_regularizer=l1(1e-5), bias_regularizer=l2(1e-5))
    self.bn2 = keras.layers.BatchNormalization()
    self.dense2 = keras.layers.Dense(16, kernel_regularizer=l2(1e-5), bias_regularizer=l1_l2(1e-5))
    self.hosp_out = keras.layers.Dense(1, activation = ""sigmoid"")

    self.dense4 = keras.layers.Dense(128, kernel_regularizer=l1(1e-5), bias_regularizer=l2(1e-5))
    self.dropout3 = keras.layers.Dropout(0.2)
    self.dense5 = keras.layers.Dense(64, kernel_regularizer=l2(1e-6), bias_regularizer=l1_l2(1e-5))
    self.row_out = keras.layers.Dense(dim, activation = ""linear"")

    
  def call(self, inputs, states=None, return_state=False, training=False):
    x = inputs
    x = self.pre(x, training=training)
    if states is not None: s1, s2, s3 = states
    else: s1 = s2 = s3 = None

    x, h1, s1 = self.rnn1(x, initial_state=s1, training=training)
    xr = self.bn(x, training=training)
    xr, h2, s2 = self.rnn2(xr, initial_state=s2, training=training)
    x = self.res([x, xr])
    x, h3, s3 = self.rnn3(x, initial_state=s3, training=training)

    y = self.dense1(x, training=training)
    y = self.bn2(y, training=training)
    y = self.dense2(y, training=training)
    y = self.hosp_out(y, training=training)

    z = self.dense4(x, training=training)
    z = self.dropout3(z, training=training)
    z = self.dense5(z, training=training)
    z = self.row_out(z, training=training)

    if return_state:
      return y, z, ([h1, s1], [h2, s2], [h3, s3])
      #return x, [h1, s1]
    else:
      return y, z


rmodel = build_rnn(dim = len(columns))

rmodel.compile(loss = [""binary_crossentropy"", 'mse'], optimizer=keras.optimizers.experimental.AdamW())

rmodel.fit(x_train, [y_train, z_train], epochs = 100, callbacks=callbacks.EarlyStopping(monitor=""loss"", patience=7), verbose = 2)
```


### Relevant log output

```shell
2023-02-23 16:48:07.354061: I tensorflow/core/common_runtime/executor.cc:1214] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32
	 [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]
2023-02-23 16:48:07.355191: I tensorflow/core/common_runtime/executor.cc:1214] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32
	 [[{{node gradients/split_grad/concat/split/split_dim}}]]
2023-02-23 16:48:07.356586: I tensorflow/core/common_runtime/executor.cc:1214] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32
	 [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]
2023-02-23 16:48:07.569860: I tensorflow/core/common_runtime/executor.cc:1214] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32
	 [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]
2023-02-23 16:48:07.571111: I tensorflow/core/common_runtime/executor.cc:1214] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32
	 [[{{node gradients/split_grad/concat/split/split_dim}}]]
2023-02-23 16:48:07.572353: I tensorflow/core/common_runtime/executor.cc:1214] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32
	 [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]
2023-02-23 16:48:07.762177: I tensorflow/core/common_runtime/executor.cc:1214] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32
	 [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]
2023-02-23 16:48:07.763419: I tensorflow/core/common_runtime/executor.cc:1214] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32
	 [[{{node gradients/split_grad/concat/split/split_dim}}]]
2023-02-23 16:48:07.764506: I tensorflow/core/common_runtime/executor.cc:1214] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32
	 [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]
2023-02-23 16:48:09.234512: I tensorflow/core/common_runtime/executor.cc:1214] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32
	 [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]
2023-02-23 16:48:09.235990: I tensorflow/core/common_runtime/executor.cc:1214] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32
	 [[{{node gradients/split_grad/concat/split/split_dim}}]]
2023-02-23 16:48:09.237207: I tensorflow/core/common_runtime/executor.cc:1214] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32
	 [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]
2023-02-23 16:48:09.419726: I tensorflow/core/common_runtime/executor.cc:1214] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32
	 [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]
2023-02-23 16:48:09.420944: I tensorflow/core/common_runtime/executor.cc:1214] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32
	 [[{{node gradients/split_grad/concat/split/split_dim}}]]
2023-02-23 16:48:09.422073: I tensorflow/core/common_runtime/executor.cc:1214] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32
	 [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]
2023-02-23 16:48:09.593760: I tensorflow/core/common_runtime/executor.cc:1214] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32
	 [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]
2023-02-23 16:48:09.595009: I tensorflow/core/common_runtime/executor.cc:1214] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32
	 [[{{node gradients/split_grad/concat/split/split_dim}}]]
2023-02-23 16:48:09.596323: I tensorflow/core/common_runtime/executor.cc:1214] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32
	 [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]
```
</details>"
59778,Library not loaded: @rpath/libgpr.20.dylib Rosetta2 conda env,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.7.4

### Custom Code

Yes

### OS Platform and Distribution

macos Ventura 13.2.1 (22D68)

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!

running TF in a rosetta x86_64 conda env with python 3.7 (this is a requirement from a package) I was able to install it using [this answer][https://stackoverflow.com/a/74439006/5318634] but when trying to import it I receive this error.

""""""python
Traceback (most recent call last):
  File ""/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: dlopen(/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 0x0006): Library not loaded: @rpath/libgpr.20.dylib
  Referenced from: <4CC88E4E-9383-347A-B5FD-2C28611F371E> /Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
  Reason: tried: '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../../_solib_darwin_x86_64/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../../../../libgpr.20.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../../_solib_darwin_x86_64/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../../../../libgpr.20.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/Users/azurduy/miniconda/envs/iappkg_x86/bin/../lib/libgpr.20.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/Users/azurduy/miniconda/envs/iappkg_x86/bin/../lib/libgpr.20.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/System/Volumes/Preboot/Cryptexes/OS@rpath/libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../../_solib_darwin_x86_64/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../../../../libgpr.20.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../../_solib_darwin_x86_64/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../../../../libgpr.20.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/Users/azurduy/miniconda/envs/iappkg_x86/bin/../lib/libgpr.20.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/Users/azurduy/miniconda/envs/iappkg_x86/bin/../lib/libgpr.20.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/usr/local/lib/libgpr.20.dylib' (no such file), '/usr/lib/libgpr.20.dylib' (no such file, not in dyld cache)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/__init__.py"", line 40, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 80, in <module>
    f'{traceback.format_exc()}'
ImportError: Traceback (most recent call last):
  File ""/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: dlopen(/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 0x0006): Library not loaded: @rpath/libgpr.20.dylib
  Referenced from: <4CC88E4E-9383-347A-B5FD-2C28611F371E> /Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
  Reason: tried: '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../../_solib_darwin_x86_64/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../../../../libgpr.20.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../../_solib_darwin_x86_64/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../../../../libgpr.20.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/Users/azurduy/miniconda/envs/iappkg_x86/bin/../lib/libgpr.20.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/Users/azurduy/miniconda/envs/iappkg_x86/bin/../lib/libgpr.20.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/System/Volumes/Preboot/Cryptexes/OS@rpath/libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../../_solib_darwin_x86_64/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../../../../libgpr.20.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../../_solib_darwin_x86_64/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../../../../libgpr.20.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/Users/azurduy/miniconda/envs/iappkg_x86/bin/../lib/libgpr.20.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/Users/azurduy/miniconda/envs/iappkg_x86/bin/../lib/libgpr.20.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/usr/local/lib/libgpr.20.dylib' (no such file), '/usr/lib/libgpr.20.dylib' (no such file, not in dyld cache)


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
""""""
```


### Standalone code to reproduce the issue

```shell
import tensorflow
```


### Relevant log output

```shell
ImportError                               Traceback (most recent call last)
<ipython-input-1-d6579f534729> in <module>
----> 1 import tensorflow

~/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/__init__.py in <module>
     39 import sys as _sys
     40
---> 41 from tensorflow.python.tools import module_util as _module_util
     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     43

~/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/__init__.py in <module>
     38 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top
     39
---> 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
     41 from tensorflow.python.eager import context
     42

~/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>
     78 except ImportError:
     79   raise ImportError(
---> 80       f'{traceback.format_exc()}'
     81       f'\n\nFailed to load the native TensorFlow runtime.\n'
     82       f'See https://www.tensorflow.org/install/errors '

ImportError: Traceback (most recent call last):
  File ""/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: dlopen(/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 0x0006): Library not loaded: @rpath/libgpr.20.dylib
  Referenced from: <4CC88E4E-9383-347A-B5FD-2C28611F371E> /Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
  Reason: tried: '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../../_solib_darwin_x86_64/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../../../../libgpr.20.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../../_solib_darwin_x86_64/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../../../../libgpr.20.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/Users/azurduy/miniconda/envs/iappkg_x86/bin/../lib/libgpr.20.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/Users/azurduy/miniconda/envs/iappkg_x86/bin/../lib/libgpr.20.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/System/Volumes/Preboot/Cryptexes/OS@rpath/libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../../_solib_darwin_x86_64/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../../../../libgpr.20.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../../_solib_darwin_x86_64/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../libgpr.20.dylib' (no such file), '/Users/azurduy/miniconda/envs/iappkg_x86/lib/python3.7/site-packages/tensorflow/python/../../../../libgpr.20.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/Users/azurduy/miniconda/envs/iappkg_x86/bin/../lib/libgpr.20.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/Users/azurduy/miniconda/envs/iappkg_x86/bin/../lib/libgpr.20.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/usr/local/lib/libgpr.20.dylib' (no such file), '/usr/lib/libgpr.20.dylib' (no such file, not in dyld cache)


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
```
</details>"
59777,"Tensorflow (pypi) doesn't support protobuf = "">3.20.0""","### System information

-   **Have I written custom code (as opposed to using a stock example script 
provided in TensorFlow)**: no
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 
 
```sw_vers
ProductName:	macOS
ProductVersion:	12.6.3
BuildVersion:	21G419
```

-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: MacBook
-   **TensorFlow installed from (source or binary)**: pip (binary)
-   **TensorFlow version (use command below)**: the most recent one, 
-   **Python version**: 3.8.15
-   **Bazel version (if compiling from source)**: N/A
-   **GCC/Compiler version (if compiling from source)**: N/A
-   **CUDA/cuDNN version**: N/A
-   **GPU model and memory**: N/A
-   **Exact command to reproduce**: see 'Describe the problem' section

### Describe the problem

I have a `pyproject.toml`. I want to upgrade its `onnx`, from 1.12.0 to 1.13.0. When I run `poetry lock`, it tells me

```
Because concrete-ml depends on onnx (1.13.0) which depends on protobuf (>=3.20.2,<4), protobuf is required.
So, because concrete-ml depends on protobuf (3.19.6), version solving failed.
```

so I upgrade `protobuf`, and now, it tells me

```
Because tensorflow (2.11.0) depends on protobuf (>=3.9.2,<3.20)
 and concrete-ml depends on protobuf (>3.20.0), tensorflow is forbidden.
So, because concrete-ml depends on tensorflow (2.11.0), version solving failed.
```

which I understand as the most recent version of tf (2.11.0) currently doesn't support a recent version of protobuf.

This is for me a problem since for now, when I run `pip-audit`, it tells me

```
Found 3 known vulnerabilities in 3 packages
Name   Version ID                  Fix Versions
------ ------- ------------------- ------------
mpmath 1.2.1   PYSEC-2021-427
onnx   1.12.0  GHSA-ffxj-547x-5j7c 1.13.0
py     1.11.0  PYSEC-2022-42969
```

so I can't upgrade `onnx`, and in my understanding, it is related to tf

--

Thanks for your hard work, guys! What you do for the ML community is amazing.
"
59776,save.save_model in training.py directs to wrong file,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.11

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

cudatoolkit=11.2.2 cudnn=8.1.0

### GPU model and memory

_No response_

### Current Behaviour?

Hey,

so I stumbled into this while trying to save a more complex custom model than the one I'll discuss here and getting errors that were reported already in other issues but had remained unsolved. I knew from my dealing with the errors in my other model that saving my model worked fine in TF2.3 but crashed in 2.11 and I had my suspicions as to what caused the problem.
So I created a dummy test that would throw an error (TypeError: 'int' object is not iterable) to be able to check how TF goes through the source code. I executed this once with my old TF2.3 installation and once with my current TF 2.11 installation (did this in 2 different virtual environments).

In both version when calling save on a Keras model that then essentially executes 

```
save.save_model(self, filepath, overwrite, include_optimizer, save_format,
                    signatures, options, save_traces)
```

which is in tensorflow.python.keras.engine.training.py->Model class->def save()

In TF2.3 that then calls the save_model function in tensorflow.python.keras.saving.save.py
In TF2.11 that calls the save function in tensorflow.python.keras.saving.saved_model.save.py

which is odd, because the other function still exits and the new function isn't documented nearly as good as the old one is

In TF2.11 the error disappears when setting save_traces=False.
In TF2.3 the error is thrown no matter what.

I am not sure if this is a bug but it very much seems like one and I think it is connected to some other issues people are having, like this one https://github.com/tensorflow/model-optimization/issues/964 or this one https://github.com/tensorflow/tensorflow/issues/47554 

I don't know how to fix this, because I don't know why save.save_model would not point to tensorflow.python.keras.saving.save.py when the import is still ""from tensorflow.python.keras.saving import save"" in trainings.py. I just want someone to tell me if this is a bug or not.

Have a nice day,
Miriam


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import LSTM, Dense, Dropout

model = Sequential([
    LSTM(128,input_shape=(4,5)),
    Dropout(0,2),
    Dense(1)
])

model.compile(loss='mae', optimizer='adam')
model.summary()

#model.save('testmodel', save_traces=False)
model.save('testmodel', save_traces=True)
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/home/miriam/vscode/UNIQORN/uniqorn/source/text.py"", line 15, in <module>
    model.save('testmodel', save_traces=True)
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 2132, in save
    save.save_model(self, filepath, overwrite, include_optimizer, save_format,
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py"", line 149, in save_model
    saved_model_save.save(model, filepath, overwrite, include_optimizer,
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/save.py"", line 90, in save
    saved_nodes, node_paths = save_lib.save_and_return_nodes(
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py"", line 1267, in save_and_return_nodes
    _build_meta_graph(obj, signatures, options, meta_graph_def))
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py"", line 1440, in _build_meta_graph
    return _build_meta_graph_impl(obj, signatures, options, meta_graph_def)
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py"", line 1383, in _build_meta_graph_impl
    signatures = signature_serialization.find_function_to_export(
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/saved_model/signature_serialization.py"", line 103, in find_function_to_export
    for name, child in children:
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py"", line 177, in list_children
    for name, child in super(_AugmentedGraphView, self).list_children(
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/checkpoint/graph_view.py"", line 75, in list_children
    for name, ref in super(ObjectGraphView,
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/checkpoint/trackable_view.py"", line 84, in children
    for name, ref in obj._trackable_children(save_type, **kwargs).items():
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py"", line 370, in _trackable_children
    super(Functional, self)._trackable_children(save_type, **kwargs))
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 2746, in _trackable_children
    children = super(Model, self)._trackable_children(save_type, **kwargs)
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 3047, in _trackable_children
    children = self._trackable_saved_model_saver.trackable_children(cache)
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/base_serialization.py"", line 55, in trackable_children
    children = self.objects_to_serialize(serialization_cache)
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py"", line 69, in objects_to_serialize
    return (self._get_serialized_attributes(
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py"", line 89, in _get_serialized_attributes
    object_dict, function_dict = self._get_serialized_attributes_internal(
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/model_serialization.py"", line 53, in _get_serialized_attributes_internal
    super(ModelSavedModelSaver, self)._get_serialized_attributes_internal(
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py"", line 99, in _get_serialized_attributes_internal
    functions = save_impl.wrap_layer_functions(self.obj, serialization_cache)
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py"", line 204, in wrap_layer_functions
    fn.get_concrete_function()
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/contextlib.py"", line 120, in __exit__
    next(self.gen)
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py"", line 367, in tracing_scope
    fn.get_concrete_function(*args, **kwargs)
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py"", line 1215, in get_concrete_function
    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py"", line 1206, in _get_concrete_function_garbage_collected
    concrete = self._variable_creation_fn._get_concrete_function_garbage_collected(  # pylint: disable=protected-access
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 192, in _get_concrete_function_garbage_collected
    concrete_function, _ = self._maybe_define_concrete_function(args, kwargs)
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 157, in _maybe_define_concrete_function
    return self._maybe_define_function(args, kwargs)
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 360, in _maybe_define_function
    concrete_function = self._create_concrete_function(args, kwargs)
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 284, in _create_concrete_function
    func_graph_module.func_graph_from_py_func(
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 1283, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py"", line 645, in wrapped_fn
    out = weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py"", line 599, in wrapper
    ret = method(*args, **kwargs)
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/utils.py"", line 165, in wrap_with_training_arg
    return control_flow_util.smart_cond(
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/utils/control_flow_util.py"", line 109, in smart_cond
    return smart_module.smart_cond(
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/framework/smart_cond.py"", line 52, in smart_cond
    return true_fn()
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/utils.py"", line 166, in <lambda>
    training, lambda: replace_training_and_call(True),
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/utils.py"", line 163, in replace_training_and_call
    return wrapped_call(*args, **kwargs)
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py"", line 681, in call
    return call_and_return_conditional_losses(inputs, *args, **kwargs)[0]
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py"", line 639, in __call__
    return self.wrapped_call(*args, **kwargs)
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/miriam/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py"", line 207, in _get_noise_shape
    for i, value in enumerate(self.noise_shape):
TypeError: 'int' object is not iterable
```
</details>"
59774,Incorrect version of protobuf used in docker container,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.13

### Bazel version

5.3.0

### GCC/Compiler version

10.3.0

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

```shell
The version of protobuf specified in the docker container is out of date as it does not match the current requirements.
https://github.com/tensorflow/tensorflow/blob/20e0beaeebc1bd96c8eca40bed0e7b0d065d8e0b/tensorflow/tools/tf_sig_build_dockerfiles/devel.requirements.txt#L18 - this is now wrong
It should match this
https://github.com/tensorflow/tensorflow/blob/20e0beaeebc1bd96c8eca40bed0e7b0d065d8e0b/tensorflow/tools/pip_package/setup.py#L100
```


### Standalone code to reproduce the issue

```shell
Look at the links above
```


### Relevant log output

```shell
n/a
```
</details>"
59773,Unit test failure with TF_ENABLE_ONEDNN_OPTS=1,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.13

### Bazel version

5.3.0

### GCC/Compiler version

10.3.0

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

```shell
//tensorflow/python/kernel_tests/nn_ops:pooling_ops_test_cpu             FAILED in 3 out of 12 in 20.7s
```


### Standalone code to reproduce the issue

```shell
bazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --noremote_accept_cached --test_env=TF_ENABLE_ONEDNN_OPTS=1 --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-requires-gpu --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-requires-gpu --verbose_failures --build_tests_only --jobs=16 -- //tensorflow/python/kernel_tests/nn_ops:pooling_ops_test
```


### Relevant log output

```shell
FAIL: testAvgPoolGradOutputMemoryOutOfBounds (__main__.PoolingTest)
PoolingTest.testAvgPoolGradOutputMemoryOutOfBounds
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/builder/.cache/bazel/_bazel_builder/945690c41481150b9aa58576637dd867/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/nn_ops/pooling_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/nn_ops/pooling_ops_test.py"", line 2348, in testAvgPoolGradOutputMemoryOutOfBounds
    self.evaluate(
AssertionError: InvalidArgumentError not raised
```
</details>"
59772,DepthwiseConv2D is 24 times slower then Conv2D with CUDA 11.8 and jit_compile=True,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

'v1.12.1-89750-g9f16e373ca8', '2.13.0'

### Custom Code

No

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

5.3.0

### GCC/Compiler version

11.3.0

### CUDA/cuDNN version

12.0 / 8.8

### GPU model and memory

RTX 4090 Aorus Master

### Current Behaviour?

```shell
After upgrading to RTX4090 i installed cuda 12.0 + cudnn 8.8 and found that some models become slower (e.g. EfficientNets) and some not (SwinTransformer).

I localized the issue to combination of `DepthwiseConv2D + model.compile(jit_compile=True)`. 
In my case this combination is 24 times slower then `Conv2D + model.compile(jit_compile=True)` and 8 times slower then `DepthwiseConv2D + model.compile(jit_compile=False)`.
Here are some numbers:

Colab + TF 2.11.0 with fp32
dw=True,  jit_compile=True,  ETA after 200 steps: 1:18:06
dw=False, jit_compile=True,  ETA after 200 steps: 1:18:20
dw=True,  jit_compile=False, ETA after 200 steps: 2:29:57


Colab + TF nightly with fp32
dw=True,  jit_compile=True,  ETA after 200 steps: 1:21:25
dw=False, jit_compile=True,  ETA after 200 steps: 1:18:23
dw=True,  jit_compile=False, ETA after 200 steps: 2:29:40


RTX4090 + Cuda 12.0 + Cudnn 8.8 + TF nightly with fp32
dw=True,  jit_compile=True,  ETA after 200 steps: 3:43:26
dw=False, jit_compile=True,  ETA after 200 steps:    9:13
dw=True,  jit_compile=False, ETA after 200 steps:   26:18


RTX4090 + Cuda 12.0 + Cudnn 8.8 + TF nightly with mixed_fp16
dw=True,  jit_compile=True,  ETA after 200 steps: 1:17:36
dw=False, jit_compile=True,  ETA after 200 steps:    4:33
dw=True,  jit_compile=False, ETA after 200 steps:   21:57
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1KTMAVhpQLQhRphRfRjHNo8faR9NBOlK6#scrollTo=IEGDQp7G6aUq
```


### Relevant log output

_No response_</details>"
59771,Amount of RAM used ,"Hi,
I have built a custom model using tensorflow/keras.
While looking at the amount of RAM (nvidia-smi) used to process a single input it gives me 8 gb of RAM (it is expected, as the model is purpose really heavy, and the input resolution also).

Nevertheless, after converting the model to tflite (even without any optimization), and running it on my iPadProM1 with metal delegate support for GPU acceleration, the peak of memory I observed is around 2-3 GB of RAM.
The model even succeeds to run for a reasonable amount of time on an iPhone 12.

I was wondering what I am missing. For my specific project, it is really important for us to keep the performance of the model as close as possible to the original one, even if time/RAM will be more expensive.


Why is tflite lowering the RAM of my model while running on an edge device? 


Here is the parameter used to convert the model:

converter = tf.lite.TFLiteConverter.from_keras_model(my_model)
converter.optimizations = []
tflite_model = converter.convert()
with open('lunetnoopt.tflite', 'wb') as f:
    f.write(tflite_model)



"
59770,ideal development mode discussion,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tag: v2.11.0

### Custom Code

No

### OS Platform and Distribution

MacOS Ventura13

### Mobile device

_No response_

### Python version

3.8

### Bazel version

5.3.0

### GCC/Compiler version

clang 14.0

### CUDA/cuDNN version

none

### GPU model and memory

none

### Current Behaviour?

```shell
Currently, we just have Mac as our daily work laptop. But we want to deep dive into tensorflow source code and maybe doing some customized changes, then compile/build and deploy into test/production environment for model training.  Meanwhile, as we knew, tensorflow is a typical cross-platform end2end tool, how can we make sure the whole develop/delivery pipeline runs smoothly?   Do we have any best practice?  Thank you.
```


### Standalone code to reproduce the issue

```shell
None regarding this topic.
```


### Relevant log output

_No response_</details>"
59769,tensorflow build failed due to '*.so' file dlopen.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tag: v2.11.0

### Custom Code

No

### OS Platform and Distribution

MacOS Ventura13

### Mobile device

_No response_

### Python version

3.8

### Bazel version

5.3.0

### GCC/Compiler version

clang 14.0

### CUDA/cuDNN version

none

### GPU model and memory

none

### Current Behaviour?

```shell
ImportError: dlopen(/private/var/tmp/_bazel_username/b326506f60d8e1a7b80e7181271420e8/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/platform/_pywrap_cpu_feature_guard.so, 0x0002): tried: '/private/var/tmp/_bazel_ysong2/b326506f60d8e1a7b80e7181271420e8/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/platform/_pywrap_cpu_feature_guard.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64')
```


### Standalone code to reproduce the issue

```shell
bazel build //tensorflow/tools/pip_package:build_pip_package



is there any conifg to control the way of ""so"" file generation?
```


### Relevant log output

_No response_</details>"
59767,can't load saved Convnext model ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

Nvidia A100

### Current Behaviour?

```shell
Can't load tensorflow saved model (.h5) for Convnext model ..
```


### Standalone code to reproduce the issue

```shell
model download link - https://easyupload.io/mn1i8h
```


### Relevant log output

```shell
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[13], line 47
     43 models=[""EfficientNetv2L_avg.h5"",
     44         ""convnext_tiny_avg.h5"",""convnext_Large_avg.h5""]
     46 for model in models:
---> 47     _f1_score=evaluate_f1_for_tf_model(model_path=model,validation_dataset=sub_sample_validation_dataset,y_true=y_true)
     48     print(f""f1 score of the {model} is {_f1_score}"")

Cell In[13], line 23, in evaluate_f1_for_tf_model(model_path, validation_dataset, y_true, show_confusion_report)
     21 # Wrap the loaded model inside the strategy scope to distribute it across the GPUs
     22 with strategy.scope():
---> 23     model = tf.keras.models.load_model(model_path)
     25 #show model arch
     26 print(model.summary())

File /gpfslocalsup/pub/anaconda-py3/2022.05/envs/tensorflow-gpu-2.11.0+py3.10.8/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File /gpfslocalsup/pub/anaconda-py3/2022.05/envs/tensorflow-gpu-2.11.0+py3.10.8/lib/python3.10/site-packages/keras/saving/legacy/serialization.py:385, in class_and_config_for_serialized_keras_object(config, module_objects, custom_objects, printable_module_name)
    381 cls = object_registration.get_registered_object(
    382     class_name, custom_objects, module_objects
    383 )
    384 if cls is None:
--> 385     raise ValueError(
    386         f""Unknown {printable_module_name}: '{class_name}'. ""
    387         ""Please ensure you are using a `keras.utils.custom_object_scope` ""
    388         ""and that this object is included in the scope. See ""
    389         ""https://www.tensorflow.org/guide/keras/save_and_serialize""
    390         ""#registering_the_custom_object for details.""
    391     )
    393 cls_config = config[""config""]
    394 # Check if `cls_config` is a list. If it is a list, return the class and the
    395 # associated class configs for recursively deserialization. This case will
    396 # happen on the old version of sequential model (e.g. `keras_version` ==
    397 # ""2.0.6""), which is serialized in a different structure, for example
    398 # ""{'class_name': 'Sequential',
    399 #   'config': [{'class_name': 'Embedding', 'config': ...}, {}, ...]}"".

ValueError: Unknown layer: 'LayerScale'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.
```
</details>"
59765,444 unit test failures on AARCH64 build of 2.12.0-rc0,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12.0-rc0

### Custom Code

No

### OS Platform and Distribution

CentOS 7

### Mobile device

n/a

### Python version

3.8.10

### Bazel version

5.3.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

```shell
Executed 3688 out of 3688 tests: 3244 tests pass and 444 fail locally.
```


### Standalone code to reproduce the issue

```shell
bazel test --test_timeout=300,500,-1,-1 \
              --flaky_test_attempts=3 \
              --test_output=all \
              --cache_test_results=no \
              --config=nonccl \
              --config=mkl_aarch64_threadpool \
              --copt=""-mtune=generic"" \
              --copt=""-march=armv8-a"" \
              --copt=""-O3"" \
              --verbose_failures \
              --test_env=TF_ENABLE_ONEDNN_OPTS=1 \
              --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-no_oss_py38,-no_oss_py39,-no_oss_py310 \
              --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-no_oss_py38,-no_oss_py39,-no_oss_py310 \
              --build_tests_only \
              -- ${DEFAULT_BAZEL_TARGETS} \
              -//tensorflow/lite/... -//tensorflow/compiler/mlir/lite/tests:const-fold.mlir.test -//tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu -//tensorflow/core/platform:ram_file_system_test -//tensorflow/python/client:session_list_devices_test -//tensorflow/python/compiler/xla:xla_test_cpu -//tensorflow/python/data/experimental/kernel_tests:checkpoint_input_pipeline_hook_test -//tensorflow/python/data/kernel_tests:iterator_test_cpu -//tensorflow/python/distribute:parameter_server_strategy_test_cpu -//tensorflow/python/distribute:parameter_server_strategy_test_2gpu -//tensorflow/python/kernel_tests/linalg:matrix_triangular_solve_op_test -//tensorflow/python/training:server_lib_test -//tensorflow/tsl/framework/convolution:spatial_convolutions_test \
              -//tensorflow/compiler/mlir/tfr/examples/mnist:mnist_ops_test -//tensorflow/core/grappler/optimizers:auto_mixed_precision_test_cpu -//tensorflow/core/grappler/optimizers:remapper_test_cpu -//tensorflow/python/kernel_tests/nn_ops:atrous_conv2d_test_cpu -//tensorflow/python/kernel_tests/nn_ops:conv_ops_test_cpu
```


### Relevant log output

```shell
No useful log output, tests immediately segfault.
```
</details>"
59763,Tensorflow 2.11 error: AttributeError: module 'tensorflow._api.v2.compat.v2.__internal__' has no attribute 'register_load_context_function',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

Windows

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I had to update Tensorflow to the currently latest version 2.11. when importing i get ""AttributeError: module 'tensorflow._api.v2.compat.v2.internal' has no attribute 'register_load_context_function'"". I have also completely reinstalled a full anaconda environment and downgraded Python to the version compatible with the latest of Tensorflow and then ""pip3 install Tensorflow==2.11"". Got the same error. I have no other ideas.

The full error log is the following


import tensorflow as tf

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
~\AppData\Local\Temp\ipykernel_432\3752927832.py in <module>
----> 1 import tensorflow as tf

~\AppData\Roaming\Python\Python310\site-packages\tensorflow\__init__.py in <module>
    467 if hasattr(_current_module, ""keras""):
    468   try:
--> 469     _keras._load()
    470   except ImportError:
    471     pass

~\AppData\Roaming\Python\Python310\site-packages\tensorflow\python\util\lazy_loader.py in _load(self)
     39     """"""Load the module and insert it into the parent's globals.""""""
     40     # Import the target module and insert it into the parent's namespace
---> 41     module = importlib.import_module(self.__name__)
     42     self._parent_module_globals[self._local_name] = module
     43 

~\anaconda3\envs\mltrade2\lib\importlib\__init__.py in import_module(name, package)
    124                 break
    125             level += 1
--> 126     return _bootstrap._gcd_import(name[level:], package, level)
    127 
    128 

~\anaconda3\envs\mltrade2\lib\site-packages\keras\__init__.py in <module>
     19 """"""
     20 from keras import distribute
---> 21 from keras import models
     22 from keras.engine.input_layer import Input
     23 from keras.engine.sequential import Sequential

~\anaconda3\envs\mltrade2\lib\site-packages\keras\models\__init__.py in <module>
     16 
     17 
---> 18 from keras.engine.functional import Functional
     19 from keras.engine.sequential import Sequential
     20 from keras.engine.training import Model

~\anaconda3\envs\mltrade2\lib\site-packages\keras\engine\functional.py in <module>
     32 from keras.engine import input_spec
     33 from keras.engine import node as node_module
---> 34 from keras.engine import training as training_lib
     35 from keras.engine import training_utils
     36 from keras.saving.legacy import serialization

~\anaconda3\envs\mltrade2\lib\site-packages\keras\engine\training.py in <module>
     43 from keras.saving.experimental import saving_lib
     44 from keras.saving.legacy import hdf5_format
---> 45 from keras.saving.legacy import save
     46 from keras.saving.legacy import saving_utils
     47 from keras.saving.legacy import serialization

~\anaconda3\envs\mltrade2\lib\site-packages\keras\saving\legacy\save.py in <module>
     22 from keras.saving.legacy import serialization
     23 from keras.saving.legacy.saved_model import load as saved_model_load
---> 24 from keras.saving.legacy.saved_model import load_context
     25 from keras.saving.legacy.saved_model import save as saved_model_save
     26 from keras.utils import traceback_utils

~\anaconda3\envs\mltrade2\lib\site-packages\keras\saving\legacy\saved_model\load_context.py in <module>
     66 
     67 
---> 68 tf.__internal__.register_load_context_function(in_load_context)

AttributeError: module 'tensorflow._api.v2.compat.v2.__internal__' has no attribute 'register_load_context_function'
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
```


### Relevant log output

_No response_</details>"
59762,tensorflow static library of windows: missing few files,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Its tensorflow provided code as given in https://www.tensorflow.org/install/lang_c
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: windows 10
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: Laptop
-   **TensorFlow installed from (source or binary)**: static library from https://www.tensorflow.org/install/lang_c
-   **TensorFlow version (use command below)**: 2.11
-   **Python version**: NA
-   **Bazel version (if compiling from source)**: NA
-   **GCC/Compiler version (if compiling from source)**: 12.2
-   **CUDA/cuDNN version**:NA
-   **GPU model and memory**: NA
-   **Exact command to reproduce**: Took static library from tensorflow ebsite from link below : https://www.tensorflow.org/install/lang_c
Have taken windows version of https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-2.11.0.zip (CPU version)
unzipped and created hello_tf.c as mentioned in same webpage.
Compiled the hello_tf.c as below : 
gcc hello_tf.c -I D:\tensorflow_lib\include -lD:\tensorflow_lib\lib\tensorflow -o hello_tf

Got error as 
_In file included from hello_tf.c:2:
D:\tensorflow_lib\include/tensorflow/c/c_api.h:23:10: fatal error: tensorflow/c/tf_buffer.h: No such file or directory
   23 | #include ""tensorflow/c/tf_buffer.h""
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated._

Then took tf_buffer.h from github repo , to see if the error is only dependent on tf_buffer.h, but then got another error : 

_D:\tensorflow_lib>gcc hello_tf.c -I D:\tensorflow_lib\include -lD:\tensorflow_lib\lib\tensorflow -o hello_tf
In file included from D:\tensorflow_lib\include/tensorflow/c/tf_tstring.h:19,
                 from D:\tensorflow_lib\include/tensorflow/c/c_api.h:27,
                 from hello_tf.c:2:
D:\tensorflow_lib\include/tensorflow/core/platform/ctstring.h:19:10: fatal error: tensorflow/tsl/platform/ctstring.h: No such file or directory
   19 | #include ""tensorflow/tsl/platform/ctstring.h""
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated._

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Took static library from tensorflow ebsite from link below : https://www.tensorflow.org/install/lang_c
Have taken windows version of https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-2.11.0.zip (CPU version)
unzipped and created hello_tf.c as mentioned in same webpage.
Compiled the hello_tf.c as below : 
gcc hello_tf.c -I D:\tensorflow_lib\include -lD:\tensorflow_lib\lib\tensorflow -o hello_tf

Got error as 
_In file included from hello_tf.c:2:
D:\tensorflow_lib\include/tensorflow/c/c_api.h:23:10: fatal error: tensorflow/c/tf_buffer.h: No such file or directory
   23 | #include ""tensorflow/c/tf_buffer.h""
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated._

Then took tf_buffer.h from github repo , to see if the error is only dependent on tf_buffer.h, but then got another error : 

_D:\tensorflow_lib>gcc hello_tf.c -I D:\tensorflow_lib\include -lD:\tensorflow_lib\lib\tensorflow -o hello_tf
In file included from D:\tensorflow_lib\include/tensorflow/c/tf_tstring.h:19,
                 from D:\tensorflow_lib\include/tensorflow/c/c_api.h:27,
                 from hello_tf.c:2:
D:\tensorflow_lib\include/tensorflow/core/platform/ctstring.h:19:10: fatal error: tensorflow/tsl/platform/ctstring.h: No such file or directory
   19 | #include ""tensorflow/tsl/platform/ctstring.h""
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated._


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
59761,`tensorflow.experimental.numpy.kron` not working with multidimensional Arrays,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Linux ubuntu 18.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I tested the function against numpy and it throws error when the `ndim` of the input tensors is greater than 2.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
try:
    a = tf.constant(np.arange(100).reshape(2, 5, 2, 5))
    b = tf.constant(np.arange(24).reshape(2, 3, 4))
    print(a.ndim) # 4
    print(b.ndim) # 3

    y = tf.experimental.numpy.kron(a, b)

    print(y.shape)
except:
    print(""Can't use tf.experimental.numpy.kron on multi-dimensional arrays"")

x = np.arange(100).reshape(2, 5, 2, 5)
y = np.arange(24).reshape(2, 3, 4)

print(x.ndim) # 4
print(y.ndim) # 3

z = np.kron(x, y)

print(z.shape) # (2, 10, 6, 20)
```


### Relevant log output

```shell
When I remove the try block I get the error saying:
`UnimplementedError: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Broadcast between [2,1,5,1,2,1,5,1] and [1,1,1,2,1,3,1,4] is not supported yet. [Op:Mul]`
```
</details>"
59757,SimpleRNN doesn't appear to use its recurrent machinery,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.7.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

Python 3.9.15

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
code generating the issue is reported here:
https://stackoverflow.com/questions/75496612/keras-simplernncell-appears-to-fail-to-distribute-learning-among-all-its-weights.

basically I expect that all weights in my model change when changing the epochs number, while what I observe is that the state weight (in my simple code the state matrix reduces to only one weight) stays stuck to its initialization.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow_addons.layers import ESN
from tensorflow_addons.rnn import ESNCell
from tensorflow.keras.layers import RNN
from tensorflow.keras.layers import SimpleRNN, SimpleRNNCell
from sklearn.preprocessing import MinMaxScaler
from tensorflow import random as rnd


# Fix the seed
rnd.set_seed(0)


# The data can be downloaded from https://mantas.info/wp/wp-content/uploads/simple_esn/MackeyGlass_t17.txt
data = np.loadtxt('MackeyGlass_t17.txt')

# Normalize
scaler = MinMaxScaler(feature_range=(0, 1))
scaled = scaler.fit_transform(data.reshape(-1, 1))

# Split Dataset in Train and Test
train, test = scaled[0:-100], scaled[-100:]

# Split into input and output 
train_X, train_y = train[:-1], train[1:]
test_X, test_y = test[:-1], test[1:] 

# Reshaping 
train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))
test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))

# Batch and epochs
batch_size = 20
epochs = 3


# Design and run the model
model = Sequential()
model.add(RNN(SimpleRNNCell(1)))
#model.add(ESN(units = 12, spectral_radius = spectral_radius, leaky=0.75, connectivity = 0.9)) # this line works exactly like the next one
#model.add(RNN(ESNCell(12, spectral_radius = spectral_radius, leaky=0.75, connectivity = 0.9)))
model.add(Dense(train_y.shape[1]))
model.compile(loss='huber', optimizer='adam')

model.fit(train_X, train_y, epochs=epochs, batch_size=batch_size, validation_data=(test_X, test_y), verbose=0, shuffle=False)

# Print the weights of the dense layer
#print(model.layers[1].get_weights())
#for layer in model.layers: print(layer.get_config(), layer.get_weights())
for layer in model.layers: print(layer.get_weights())
```


### Relevant log output

```shell
f I run this code with 2 epochs I receive the following output:

    [array([[-0.8942287]], dtype=float32), array([[1.]], dtype=float32), array([0.05435111], dtype=float32)] [array([[-1.272426]], dtype=float32), array([0.04711587], dtype=float32)]

If I run this code with 3 epochs I receive the following output:

    [array([[-0.89395165]], dtype=float32), array([[1.]], dtype=float32), array([0.06734365], dtype=float32)] [array([[-1.2927996]], dtype=float32), array([0.05247825], dtype=float32)]

so the state weight (array([[1.]]) is the only one not changing.
```
</details>"
59754,not gpu registered kernels for complex numbers ( tf.complex64 and tf.complex128) for the gather_nd method of tf.Variable,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.11

### Custom Code

Yes

### OS Platform and Distribution

Linux ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
not gpu registered kernels for complex number ( tf.complex64 and tf.complex128) for the gather_nd method of tf.Variable. However, it works without problems with CPU
```


### Standalone code to reproduce the issue

```shell
v = tf.Variable(np.array([1, 2, 3], np.complex64))
print(v.gather_nd(indices = [0]))
```


### Relevant log output

```shell
Colocation members, user-requested devices, and framework assigned devices, if any:
  resource (_Arg)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0
  ResourceGatherNd (ResourceGatherNd) /job:localhost/replica:0/task:0/device:GPU:0

Op: ResourceGatherNd
Node attrs: dtype=DT_COMPLEX64, Tindices=DT_INT64
Registered kernels:
  device='GPU'; dtype in [DT_INT8]; Tindices in [DT_INT64]
  device='GPU'; dtype in [DT_INT8]; Tindices in [DT_INT32]
  device='GPU'; dtype in [DT_UINT8]; Tindices in [DT_INT64]
  device='GPU'; dtype in [DT_UINT8]; Tindices in [DT_INT32]
  device='GPU'; dtype in [DT_INT16]; Tindices in [DT_INT64]
  device='GPU'; dtype in [DT_INT16]; Tindices in [DT_INT32]
  device='GPU'; dtype in [DT_UINT16]; Tindices in [DT_INT64]
  device='GPU'; dtype in [DT_UINT16]; Tindices in [DT_INT32]
  device='GPU'; dtype in [DT_UINT32]; Tindices in [DT_INT64]
  device='GPU'; dtype in [DT_UINT32]; Tindices in [DT_INT32]
  device='GPU'; dtype in [DT_INT64]; Tindices in [DT_INT64]
  device='GPU'; dtype in [DT_INT64]; Tindices in [DT_INT32]
  device='GPU'; dtype in [DT_UINT64]; Tindices in [DT_INT64]
  device='GPU'; dtype in [DT_UINT64]; Tindices in [DT_INT32]
  device='GPU'; dtype in [DT_DOUBLE]; Tindices in [DT_INT64]
  device='GPU'; dtype in [DT_DOUBLE]; Tindices in [DT_INT32]
  device='GPU'; dtype in [DT_FLOAT]; Tindices in [DT_INT64]
  device='GPU'; dtype in [DT_FLOAT]; Tindices in [DT_INT32]
  device='GPU'; dtype in [DT_HALF]; Tindices in [DT_INT64]
  device='GPU'; dtype in [DT_HALF]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_VARIANT]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_VARIANT]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_RESOURCE]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_RESOURCE]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_STRING]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_STRING]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_BOOL]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_BOOL]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_COMPLEX128]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_COMPLEX128]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_COMPLEX64]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_COMPLEX64]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_DOUBLE]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_DOUBLE]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_FLOAT]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_FLOAT]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_BFLOAT16]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_BFLOAT16]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_HALF]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_HALF]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_INT32]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_INT32]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_INT8]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_INT8]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_UINT8]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_UINT8]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_INT16]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_INT16]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_UINT16]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_UINT16]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_UINT32]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_UINT32]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_INT64]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_INT64]; Tindices in [DT_INT32]
  device='CPU'; dtype in [DT_UINT64]; Tindices in [DT_INT64]
  device='CPU'; dtype in [DT_UINT64]; Tindices in [DT_INT32]
```
</details>"
59752,tf.nn.learned_unigram_candidate_sampler function causes silent breakdown,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf2.7,tf2.11.0,tf-nightly

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu18.04

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.6/8.6

### GPU model and memory

GPU

### Current Behaviour?

```shell
# code trigger bug
import tensorflow as tf
tensor= tf.random.uniform([2, 2], minval=-256, maxval=257, dtype=tf.int64)
output = tf.nn.learned_unigram_candidate_sampler(tensor,2,10,True,19)
print(""Anything"")
print(output)
```


### Standalone code to reproduce the issue

```shell
In tf2.7.0 the execution will directly lead to segment fault without any error information.
In tf2.11.0 and the latest nightly version, the execution will silently break down at line 4, which means that the ""Anything"" will not be output, and neither does the next line ""print(output)"".
```


### Relevant log output

```shell
Segment fault in tf2.7.0.
no output but silently shutdown in tf2.11.0 and nightly version.
```
</details>"
59751,inconsistent selection of optimizers,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11

### Custom Code

No

### OS Platform and Distribution

Ubuntu

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am upgrading to tensorflow 2.11 from 2.6. it seems keras now has multiple classes of optimizers - two of them being `optimzier_v2` and `optimizer_experimental`. It is not clear in the tf version 2.11, which optimizer class is meant to be used as default.

When trying to instantiate optimizers directly in python,  we get `optimizer_experimental`.

However, when initiated from the config, it returns the optimzier_v2.

We are trying to log the current learning rate as the training progresses, and the different interface of the two optimizer classes causes things to break. 

We can check the class types when getting the learning rate from the optimizer object as a workaround, but it will be nice to have consistent default behavior.
```


### Standalone code to reproduce the issue

```shell
import tensorflow.keras.optimizers as optimizers
type(optimizers.Adam())
```
keras.optimizers.optimizer_experimental.adam.Adam


```
import tensorflow.keras.optimizers as optimizers
cfg = """"""
class_name: adam
config:
    beta_1: 0.9
    beta_2: 0.99
    epsilon: 1.0e-05
    learning_rate:
      class_name: ExponentialDecay
      config:
        decay_rate: 1.0
        decay_steps: 10000
        initial_learning_rate: 0.01
""""""

import yaml
cfg = yaml.safe_load(cfg)
opt = optimizers.get(cfg)
type(opt)
```
keras.optimizers.optimizer_v2.adam.Adam
```


### Relevant log output

_No response_</details>"
59750,"why my code can run in tensorflow-gpu,can`t run in tensorflow-cpu?","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.6

### Custom Code

Yes

### OS Platform and Distribution

windows10

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
my code can run in tensorflow-gpu==2.6.0,but can`t run in tensorflow-cpu==2.6.0. they use same code and DS .why?
error is :
Traceback (most recent call last):
  File ""C:\Users\DH\PycharmProjects\four\model_test.py"", line 53, in <module>
    movie_combine_layer_flat_val = movie_layer_model([np.reshape(item.take(0), [1, 1]), titles, actors, categories])
  File ""C:\Users\DH\anaconda3\envs\test\lib\site-packages\keras\utils\traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""C:\Users\DH\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\framework\ops.py"", line 7215, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling layer ""movie_actor_embed_layer"" ""                 f""(type Embedding).

{{function_node __wrapped__ResourceGather_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[0,3] = 2010 is not in [0, 2006) [Op:ResourceGather]

Call arguments received by layer ""movie_actor_embed_layer"" ""                 f""(type Embedding):
  • inputs=tf.Tensor(shape=(1, 30), dtype=int32)
it look like matrix boundary problem, but it can`t happen in tensorflow-gpu-2.6.0
```


### Standalone code to reproduce the issue

```shell
for item in movies.values:
    titles = np.zeros([1, title_count])
    titles[0] = item.take(1)

    actors = np.zeros([1, 30])
    actors[0] = item.take(2)

    categories = np.zeros([1, 10])
    categories[0] = item.take(3)

    movie_combine_layer_flat_val = movie_layer_model([np.reshape(item.take(0), [1, 1]), titles, actors, categories])
    movie_matrics.append(movie_combine_layer_flat_val)
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""C:\Users\DH\PycharmProjects\four\model_test.py"", line 53, in <module>
    movie_combine_layer_flat_val = movie_layer_model([np.reshape(item.take(0), [1, 1]), titles, actors, categories])
  File ""C:\Users\DH\anaconda3\envs\test\lib\site-packages\keras\utils\traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""C:\Users\DH\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\framework\ops.py"", line 7215, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling layer ""movie_actor_embed_layer"" ""                 f""(type Embedding).

{{function_node __wrapped__ResourceGather_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[0,3] = 2010 is not in [0, 2006) [Op:ResourceGather]

Call arguments received by layer ""movie_actor_embed_layer"" ""                 f""(type Embedding):
  • inputs=tf.Tensor(shape=(1, 30), dtype=int32)
```
</details>"
59749,make -f error,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 22.04.2 LTS
- TensorFlow installation (pip package or built from source): built from source
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.11.0

### 2. Code
make -f tensorflow/lite/micro/tools/make/Makefile test_hello_world_test 

### 3. Failure after conversion
make: tensorflow/lite/micro/tools/make/flatbuffers_download.sh: Permission denied                                       tensorflow/lite/micro/tools/make/Makefile:491: *** Something went wrong with the flatbuffers download: .  Stop.
"
59748,Found Minor Error in Comment ,"https://github.com/tensorflow/tensorflow/blob/b308c65397964d74b46a4015d15b6911ac710616/tensorflow/compiler/xla/tests/constants_test.cc#L85

F8 might need to be F16 in this comment"
59747,can't install throught `pip install -q git+https://github.com/tensorflow/docs`,"seen as below 
![error](https://user-images.githubusercontent.com/76671016/220236110-8b97769f-f846-447a-ae6e-bddf90a6bb05.JPG)
i got this command from here:<u>https://keras.io/examples/vision/video_classification/</u>


tensorflow version : 2.11.0 
"
59746,Order of tensorflow.Dataset.concatenate operations,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Feature Request

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9.7

### Bazel version

_No response_

### GCC/Compiler version

9.4.9

### CUDA/cuDNN version

_No response_

### GPU model and memory

NVIDIA RTX A4000 15.74GiB

### Current Behaviour?

If I have a long list of datasets that I wish to concatenate, it matters whether I concatenate them pairwise with heavy left-hand sides or heavy right-hand sides.  I would like to see this in the documentation so that others don't have to learn this on their own.  (Additionally, a static tf.Dataset.concatenate_from_list function that does the right thing for the user would be nice.)  Specifically, for my large list of datasets that contain large amounts of image data that are read from disk via py_functions, batched, and prefetched, the following code snippets give different performances.  `concatenate_by_appending` causes the process to be killed, likely due to an out-of-memory condition.  In contrast, `concatenate_by_prepending` and `concatenate_by_splitting` work well.  I suspect that if I used an aggressive form of `shuffle` then only `concatenate_by_splitting` would work well.

I should clarify it is not the creation of the `concatenate_by_appending` dataset that causes the process to be killed.  The process is killed only once I attempt to `predict` using the combined dataset.  The pre-pending approach is right-heavy for each concatenation operation, and the left-most dataset can be found immediately, as the left branch of the top-most concatenation.  In contrast, the appending approach is left-heavy for each concatenation operation, requires descending through all concatenations to find the left-most dataset, and causes the process to be killed, likely due to resource exhaustion, specifically likely an out-of-memory condition.  At least I think that is what is happening.
### Standalone code to reproduce the issue

```python
def concatenate_by_prepending(dataset_list):
    response = None
    # Note: list traversal is reversed
    for dataset in reversed(dataset_list):
        if response is None:
            response = dataset
        else:
            # Note: Right-heavy concatenation
            response = dataset.concatenate(response)
    return response

def concatenate_by_appending(dataset_list):
    response = None
    # Note: list traversal is forward
    for dataset in dataset_list:
        if response is None:
            response = dataset
        else:
            # Note: Left-heavy concatenation
            response = response.concatenate(dataset)
    return response

def concatenate_by_splitting(dataset_list):
    # Note: recursive, rather than list traversing
    length = len(dataset_list)
    if length == 1:
        return dataset_list[0]
    if length > 1:
        # Note: Left and right are of roughly equal size
        return concatenate_by_splitting(dataset_list[0 : length // 2]).concatenate(
            concatenate_by_splitting(dataset_list[length // 2 : length])
        )
    return None
```


### Relevant log output

_No response_</details>"
59744,tf.data.Dataset is much slower than Python generator producing the same data,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

MacOS 12.6.3

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Iterating tf.data.Dataset with 10k records (consisting of 10 short strings each) is 78 times slower than iterating a Python generator producing the same data
```


### Standalone code to reproduce the issue

```shell
from time import perf_counter
import tensorflow as tf

def gen():
    for _ in range(10000):
        yield {str(k): 'string value' for k in range(10)}

start = perf_counter()
for _ in gen():
    pass
print('Python generator', perf_counter() - start)

ds = tf.data.Dataset.from_generator(
    gen,
    output_signature={str(k): tf.TensorSpec(shape=(), dtype=tf.string) for k in range(10)}
)

ds.save('/tmp/ds')
ds = tf.data.Dataset.load('/tmp/ds')

start = perf_counter()
for _ in ds:
    pass
print('tf.data.Dataset', perf_counter() - start)
```


### Relevant log output

```shell
Python generator 0.018086554016917944
tf.data.Dataset 1.4203055879333988
```
</details>"
59743,"Bfloat16 on GPU for  sigmoid/swish activations, dropout and LSTM layers","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

TensorFlow version 2.13.0-dev20230215

### Custom Code

Yes

### OS Platform and Distribution

ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.8/8.6

### GPU model and memory

single A100 80G

### Current Behaviour?

```shell
current behaviour:
when using mixed_bfloat16 policy, sigmoid type activation and dropout layer run on CPU, and LSTM layer does not support bfloat16 input.

expected behavior:
sigmoid/swish activation, dropout and LSTM layers run on GPU with mixed_bfloat16 policy
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras import mixed_precision
from tensorflow.keras.layers import LSTM, Embedding

tf.debugging.set_log_device_placement(True)
#from tensorflow.python.framework.ops import disable_eager_execution
#disable_eager_execution()


policy = mixed_precision.Policy('mixed_bfloat16')
# float32
print(policy.name, policy.variable_dtype, policy.compute_dtype)

mixed_precision.set_global_policy(policy)

input_shape = (4, 28, 28, 3)
x = tf.random.normal((4, 28, 28, 3))
#layer = tf.keras.layers.Conv2D(2, 3, activation='relu', input_shape=(28, 28, 3))
layer1 = tf.keras.layers.Activation(activation=tf.keras.activations.swish)
layer2 = tf.keras.layers.Activation(activation=tf.keras.activations.sigmoid)
#layer3 = tf.keras.layers.Activation(activation=tf.keras.activations.relu)
layer4 = tf.keras.layers.Dropout(0.1, name=""dropout"")

x1 = layer1(x)
x2 = layer2(x1)
y = layer4(x2, training=True)
print(' ====== output ======== ', y.dtype, layer1.dtype, layer2.dtype, layer4.dtype)

vocab_size = 50
input_shape = (4, vocab_size, 1)
x = tf.random.normal(input_shape)

def customer_model():
    model = Sequential()
    model.add(LSTM(128))
    return model

model = customer_model()
model.build(input_shape=input_shape)
model.summary()
y = model(x)
```


### Relevant log output

```shell
2023-02-20 15:15:13.745147: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-20 15:15:14.363645: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
mixed_bfloat16 float32 bfloat16
2023-02-20 15:15:15.799744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78915 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0001:00:00.0, compute capability: 8.0
input: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:15.809731: I tensorflow/core/common_runtime/placer.cc:114] input: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
_EagerConst: (_EagerConst): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:15.809763: I tensorflow/core/common_runtime/placer.cc:114] _EagerConst: (_EagerConst): /job:localhost/replica:0/task:0/device:GPU:0
output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:15.809775: I tensorflow/core/common_runtime/placer.cc:114] output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:15.813051: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
input: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.219908: I tensorflow/core/common_runtime/placer.cc:114] input: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
_EagerConst: (_EagerConst): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.219944: I tensorflow/core/common_runtime/placer.cc:114] _EagerConst: (_EagerConst): /job:localhost/replica:0/task:0/device:GPU:0
output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.219952: I tensorflow/core/common_runtime/placer.cc:114] output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.221005: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.221972: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
shape: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.222322: I tensorflow/core/common_runtime/placer.cc:114] shape: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
RandomStandardNormal: (RandomStandardNormal): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.222337: I tensorflow/core/common_runtime/placer.cc:114] RandomStandardNormal: (RandomStandardNormal): /job:localhost/replica:0/task:0/device:GPU:0
output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.222345: I tensorflow/core/common_runtime/placer.cc:114] output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.222865: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:GPU:0
x: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.223723: I tensorflow/core/common_runtime/placer.cc:114] x: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
y: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.223736: I tensorflow/core/common_runtime/placer.cc:114] y: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
Mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.223750: I tensorflow/core/common_runtime/placer.cc:114] Mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
z_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.223761: I tensorflow/core/common_runtime/placer.cc:114] z_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.224202: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0
x: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.224907: I tensorflow/core/common_runtime/placer.cc:114] x: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
y: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.224922: I tensorflow/core/common_runtime/placer.cc:114] y: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
AddV2: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.224936: I tensorflow/core/common_runtime/placer.cc:114] AddV2: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0
z_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.224944: I tensorflow/core/common_runtime/placer.cc:114] z_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.225386: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0
x: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.229609: I tensorflow/core/common_runtime/placer.cc:114] x: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
Cast: (Cast): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.229623: I tensorflow/core/common_runtime/placer.cc:114] Cast: (Cast): /job:localhost/replica:0/task:0/device:GPU:0
y_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.229630: I tensorflow/core/common_runtime/placer.cc:114] y_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.230004: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Cast in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.230386: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.230496: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Cast in device /job:localhost/replica:0/task:0/device:GPU:0
x: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.230820: I tensorflow/core/common_runtime/placer.cc:114] x: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
y: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.230831: I tensorflow/core/common_runtime/placer.cc:114] y: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
Mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.230843: I tensorflow/core/common_runtime/placer.cc:114] Mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
z_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.230850: I tensorflow/core/common_runtime/placer.cc:114] z_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.231215: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0
x: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.231575: I tensorflow/core/common_runtime/placer.cc:114] x: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
Sigmoid: (Sigmoid): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.231596: I tensorflow/core/common_runtime/placer.cc:114] Sigmoid: (Sigmoid): /job:localhost/replica:0/task:0/device:CPU:0
y_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.231608: I tensorflow/core/common_runtime/placer.cc:114] y_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.232198: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Sigmoid in device /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.233223: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0
input: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.233727: I tensorflow/core/common_runtime/placer.cc:114] input: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
Identity: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.233747: I tensorflow/core/common_runtime/placer.cc:114] Identity: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.233759: I tensorflow/core/common_runtime/placer.cc:114] output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.234163: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.234923: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Sigmoid in device /job:localhost/replica:0/task:0/device:CPU:0
x: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.235519: I tensorflow/core/common_runtime/placer.cc:114] x: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
Cast: (Cast): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.235536: I tensorflow/core/common_runtime/placer.cc:114] Cast: (Cast): /job:localhost/replica:0/task:0/device:CPU:0
y_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.235546: I tensorflow/core/common_runtime/placer.cc:114] y_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.235889: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Cast in device /job:localhost/replica:0/task:0/device:CPU:0
input: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.236126: I tensorflow/core/common_runtime/placer.cc:114] input: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
_EagerConst: (_EagerConst): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.236140: I tensorflow/core/common_runtime/placer.cc:114] _EagerConst: (_EagerConst): /job:localhost/replica:0/task:0/device:GPU:0
output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.236147: I tensorflow/core/common_runtime/placer.cc:114] output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.236694: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.237002: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0
input: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.237338: I tensorflow/core/common_runtime/placer.cc:114] input: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.237358: I tensorflow/core/common_runtime/placer.cc:114] Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.237369: I tensorflow/core/common_runtime/placer.cc:114] output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.237779: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Shape in device /job:localhost/replica:0/task:0/device:GPU:0
shape: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.238193: I tensorflow/core/common_runtime/placer.cc:114] shape: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
RandomUniform: (RandomUniform): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.238212: I tensorflow/core/common_runtime/placer.cc:114] RandomUniform: (RandomUniform): /job:localhost/replica:0/task:0/device:GPU:0
output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.238223: I tensorflow/core/common_runtime/placer.cc:114] output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.238690: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.238925: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Cast in device /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.239130: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
x: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.239387: I tensorflow/core/common_runtime/placer.cc:114] x: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
y: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.239404: I tensorflow/core/common_runtime/placer.cc:114] y: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
GreaterEqual: (GreaterEqual): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.239434: I tensorflow/core/common_runtime/placer.cc:114] GreaterEqual: (GreaterEqual): /job:localhost/replica:0/task:0/device:CPU:0
z_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.239442: I tensorflow/core/common_runtime/placer.cc:114] z_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.240101: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op GreaterEqual in device /job:localhost/replica:0/task:0/device:CPU:0
x: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.240458: I tensorflow/core/common_runtime/placer.cc:114] x: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
Cast: (Cast): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.240477: I tensorflow/core/common_runtime/placer.cc:114] Cast: (Cast): /job:localhost/replica:0/task:0/device:CPU:0
y_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.240489: I tensorflow/core/common_runtime/placer.cc:114] y_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.240879: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Cast in device /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.241141: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
condition: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.241482: I tensorflow/core/common_runtime/placer.cc:114] condition: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
t: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.241500: I tensorflow/core/common_runtime/placer.cc:114] t: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
e: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.241508: I tensorflow/core/common_runtime/placer.cc:114] e: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
SelectV2: (SelectV2): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.241516: I tensorflow/core/common_runtime/placer.cc:114] SelectV2: (SelectV2): /job:localhost/replica:0/task:0/device:CPU:0
output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.241527: I tensorflow/core/common_runtime/placer.cc:114] output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.242187: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op SelectV2 in device /job:localhost/replica:0/task:0/device:CPU:0
 ====== output ========  <dtype: 'bfloat16'> float32 float32 float32
2023-02-20 15:15:16.242688: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.242816: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.242920: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.243029: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0
input: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.244373: I tensorflow/core/common_runtime/placer.cc:114] input: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
_EagerConst: (_EagerConst): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.244402: I tensorflow/core/common_runtime/placer.cc:114] _EagerConst: (_EagerConst): /job:localhost/replica:0/task:0/device:GPU:0
output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.244422: I tensorflow/core/common_runtime/placer.cc:114] output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.245327: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
resource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.245736: I tensorflow/core/common_runtime/placer.cc:114] resource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
VarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.245756: I tensorflow/core/common_runtime/placer.cc:114] VarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.246125: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
resource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.253589: I tensorflow/core/common_runtime/placer.cc:114] resource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
value: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.253619: I tensorflow/core/common_runtime/placer.cc:114] value: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.253635: I tensorflow/core/common_runtime/placer.cc:114] AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.254140: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.259081: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.259243: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.259523: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.259624: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.266083: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.266318: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.266583: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.266830: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
seed: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.267103: I tensorflow/core/common_runtime/placer.cc:114] seed: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
StatelessRandomGetKeyCounter: (StatelessRandomGetKeyCounter): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.267120: I tensorflow/core/common_runtime/placer.cc:114] StatelessRandomGetKeyCounter: (StatelessRandomGetKeyCounter): /job:localhost/replica:0/task:0/device:GPU:0
key_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.267134: I tensorflow/core/common_runtime/placer.cc:114] key_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
counter_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.267143: I tensorflow/core/common_runtime/placer.cc:114] counter_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.267699: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op StatelessRandomGetKeyCounter in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.286569: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
shape: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.286927: I tensorflow/core/common_runtime/placer.cc:114] shape: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
key: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.286943: I tensorflow/core/common_runtime/placer.cc:114] key: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
counter: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.286957: I tensorflow/core/common_runtime/placer.cc:114] counter: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
alg: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.286966: I tensorflow/core/common_runtime/placer.cc:114] alg: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
StatelessRandomUniformV2: (StatelessRandomUniformV2): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.286977: I tensorflow/core/common_runtime/placer.cc:114] StatelessRandomUniformV2: (StatelessRandomUniformV2): /job:localhost/replica:0/task:0/device:GPU:0
output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.286994: I tensorflow/core/common_runtime/placer.cc:114] output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.287706: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op StatelessRandomUniformV2 in device /job:localhost/replica:0/task:0/device:GPU:0
x: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.291713: I tensorflow/core/common_runtime/placer.cc:114] x: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
y: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.291736: I tensorflow/core/common_runtime/placer.cc:114] y: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
Sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.291752: I tensorflow/core/common_runtime/placer.cc:114] Sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0
z_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.291766: I tensorflow/core/common_runtime/placer.cc:114] z_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.292313: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.300187: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.300343: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0
resource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.300704: I tensorflow/core/common_runtime/placer.cc:114] resource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
VarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.300720: I tensorflow/core/common_runtime/placer.cc:114] VarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.301056: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
resource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.301395: I tensorflow/core/common_runtime/placer.cc:114] resource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
value: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.301410: I tensorflow/core/common_runtime/placer.cc:114] value: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.301424: I tensorflow/core/common_runtime/placer.cc:114] AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.301772: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.302407: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.302584: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.302674: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op StatelessRandomGetKeyCounter in device /job:localhost/replica:0/task:0/device:GPU:0
shape: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.303396: I tensorflow/core/common_runtime/placer.cc:114] shape: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
key: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.303409: I tensorflow/core/common_runtime/placer.cc:114] key: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
counter: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.303444: I tensorflow/core/common_runtime/placer.cc:114] counter: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
alg: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.303454: I tensorflow/core/common_runtime/placer.cc:114] alg: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
StatelessRandomNormalV2: (StatelessRandomNormalV2): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.303464: I tensorflow/core/common_runtime/placer.cc:114] StatelessRandomNormalV2: (StatelessRandomNormalV2): /job:localhost/replica:0/task:0/device:GPU:0
output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.303474: I tensorflow/core/common_runtime/placer.cc:114] output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.304084: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op StatelessRandomNormalV2 in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.304383: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.304498: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0
input: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.304807: I tensorflow/core/common_runtime/placer.cc:114] input: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
Qr: (Qr): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.304826: I tensorflow/core/common_runtime/placer.cc:114] Qr: (Qr): /job:localhost/replica:0/task:0/device:GPU:0
q_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.304838: I tensorflow/core/common_runtime/placer.cc:114] q_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
r_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.304845: I tensorflow/core/common_runtime/placer.cc:114] r_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.305406: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Qr in device /job:localhost/replica:0/task:0/device:GPU:0
input: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.321530: I tensorflow/core/common_runtime/placer.cc:114] input: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
DiagPart: (DiagPart): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.321554: I tensorflow/core/common_runtime/placer.cc:114] DiagPart: (DiagPart): /job:localhost/replica:0/task:0/device:GPU:0
diagonal_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.321568: I tensorflow/core/common_runtime/placer.cc:114] diagonal_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.322028: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op DiagPart in device /job:localhost/replica:0/task:0/device:GPU:0
x: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.322425: I tensorflow/core/common_runtime/placer.cc:114] x: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
Sign: (Sign): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.322448: I tensorflow/core/common_runtime/placer.cc:114] Sign: (Sign): /job:localhost/replica:0/task:0/device:GPU:0
y_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.322462: I tensorflow/core/common_runtime/placer.cc:114] y_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.322849: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Sign in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.333823: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.334332: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
x: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.334700: I tensorflow/core/common_runtime/placer.cc:114] x: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
perm: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.334718: I tensorflow/core/common_runtime/placer.cc:114] perm: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
Transpose: (Transpose): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.334731: I tensorflow/core/common_runtime/placer.cc:114] Transpose: (Transpose): /job:localhost/replica:0/task:0/device:GPU:0
y_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.334742: I tensorflow/core/common_runtime/placer.cc:114] y_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.335267: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Transpose in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.342633: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
tensor: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.343070: I tensorflow/core/common_runtime/placer.cc:114] tensor: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
shape: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.343097: I tensorflow/core/common_runtime/placer.cc:114] shape: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.343117: I tensorflow/core/common_runtime/placer.cc:114] Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.343132: I tensorflow/core/common_runtime/placer.cc:114] output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.343815: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.347316: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0
resource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.348034: I tensorflow/core/common_runtime/placer.cc:114] resource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
VarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.348058: I tensorflow/core/common_runtime/placer.cc:114] VarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.348550: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
resource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.348896: I tensorflow/core/common_runtime/placer.cc:114] resource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
value: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.348912: I tensorflow/core/common_runtime/placer.cc:114] value: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.348922: I tensorflow/core/common_runtime/placer.cc:114] AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.349273: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.349657: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.349863: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
dims: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.350162: I tensorflow/core/common_runtime/placer.cc:114] dims: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
value: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.350183: I tensorflow/core/common_runtime/placer.cc:114] value: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
Fill: (Fill): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.350203: I tensorflow/core/common_runtime/placer.cc:114] Fill: (Fill): /job:localhost/replica:0/task:0/device:GPU:0
output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.350213: I tensorflow/core/common_runtime/placer.cc:114] output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.350732: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.354235: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.354513: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.354638: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.354780: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.354859: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.354995: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
values_0: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.355260: I tensorflow/core/common_runtime/placer.cc:114] values_0: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
values_1: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.355270: I tensorflow/core/common_runtime/placer.cc:114] values_1: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
values_2: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.355276: I tensorflow/core/common_runtime/placer.cc:114] values_2: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
axis: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-20 15:15:16.355285: I tensorflow/core/common_runtime/placer.cc:114] axis: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
ConcatV2: (ConcatV2): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.355299: I tensorflow/core/common_runtime/placer.cc:114] ConcatV2: (ConcatV2): /job:localhost/replica:0/task:0/device:GPU:0
output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.355305: I tensorflow/core/common_runtime/placer.cc:114] output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.355806: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op ConcatV2 in device /job:localhost/replica:0/task:0/device:GPU:0
resource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.359677: I tensorflow/core/common_runtime/placer.cc:114] resource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
VarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.359707: I tensorflow/core/common_runtime/placer.cc:114] VarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.360132: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
resource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.360527: I tensorflow/core/common_runtime/placer.cc:114] resource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
value: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.360544: I tensorflow/core/common_runtime/placer.cc:114] value: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.360554: I tensorflow/core/common_runtime/placer.cc:114] AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-20 15:15:16.360967: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
Traceback (most recent call last):
  File ""/home/azureuser/env_tf_nightly2/lib/python3.8/site-packages/keras/engine/training.py"", line 510, in build
    self.call(x, **kwargs)
  File ""/home/azureuser/env_tf_nightly2/lib/python3.8/site-packages/keras/engine/sequential.py"", line 427, in call
    outputs = layer(inputs, **kwargs)
  File ""/home/azureuser/env_tf_nightly2/lib/python3.8/site-packages/keras/layers/rnn/base_rnn.py"", line 556, in __call__
    return super().__call__(inputs, **kwargs)
  File ""/home/azureuser/env_tf_nightly2/lib/python3.8/site-packages/keras/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/azureuser/env_tf_nightly2/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py"", line 56, in _SatisfiesTypeConstraint
    raise TypeError(
TypeError: Exception encountered when calling layer 'lstm' (type LSTM).

Value passed to parameter 'input' has DataType bfloat16 not in list of allowed values: float16, float32, float64

Call arguments received by layer 'lstm' (type LSTM):
  • inputs=tf.Tensor(shape=(4, 50, 1), dtype=bfloat16)
  • mask=None
  • training=None
  • initial_state=None

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""test_bfloat16.py"", line 72, in <module>
    model.build(input_shape=input_shape)
  File ""/home/azureuser/env_tf_nightly2/lib/python3.8/site-packages/keras/engine/sequential.py"", line 383, in build
    super().build(input_shape)
  File ""/home/azureuser/env_tf_nightly2/lib/python3.8/site-packages/keras/engine/training.py"", line 512, in build
    raise ValueError(
ValueError: You cannot build your model by calling `build` if your layers do not support float type inputs. Instead, in order to instantiate and build your model, call your model on real tensor data (of the correct dtype).

The actual error from `call` is: Exception encountered when calling layer 'lstm' (type LSTM).

Value passed to parameter 'input' has DataType bfloat16 not in list of allowed values: float16, float32, float64

Call arguments received by layer 'lstm' (type LSTM):
  • inputs=tf.Tensor(shape=(4, 50, 1), dtype=bfloat16)
  • mask=None
  • training=None
  • initial_state=None.
```
</details>"
59741,ld  linker error when handling python lib link,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tag: v2.11.0

### Custom Code

No

### OS Platform and Distribution

MacOS Ventura13

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

5.3.0

### GCC/Compiler version

clang 14.0

### CUDA/cuDNN version

none

### GPU model and memory

none

### Current Behaviour?

```shell
ERROR: /Users/ysong2/Downloads/tensorflow/tensorflow/python/BUILD:358:27: Linking tensorflow/python/_pywrap_py_exception_registry.so failed: (Aborted): cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh @bazel-out/darwin-opt/bin/tensorflow/python/_pywrap_py_exception_registry.so-2.params
ld: malformed trie, node past end file 'bazel-out/darwin-opt/bin/_solib_darwin_x86_64/libtensorflow_Spython_S_Upywrap_Utensorflow_Uinternal.so'
clang: error: linker command failed with exit code 1 (use -v to see invocation)
Error in child process '/usr/bin/xcrun'. 1
external/local_config_cc/cc_wrapper.sh: line 69: 23910 Abort trap: 6           ""$(/usr/bin/dirname ""$0"")""/wrapped_clang ""$@""
Target //tensorflow/tools/pip_package:build_pip_package failed to build.

I want to compile and build fully success.
```


### Standalone code to reproduce the issue

```shell
execting: ""bazel build //tensorflow/tools/pip_package:build_pip_package"" under tensorflow root directory then this erorr occurs.
```


### Relevant log output

_No response_</details>"
59740,What is the final training result of asynchronous synchronous parallel distributed training？,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When distributed training adopts asynchronous synchronous parallel, the parameters updated by each worker are inconsistent. Is the final result of distributed training the slowest parameter updated by workers?
```


### Standalone code to reproduce the issue

```shell
When distributed training adopts asynchronous synchronous parallel, the parameters updated by each worker are inconsistent. Is the final result of distributed training the slowest parameter updated by workers?
```


### Relevant log output

_No response_</details>"
59739,Fails to build with llvm-project repository override,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.9

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

5.3.0

### GCC/Compiler version

11.3

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Following instructions from tensorflow/compiler/mlir/README.md with [""X86"", ""AArch64"", ""ARM""] in targets.bzl results in a build failure with:

cannot load '@llvm-project//:vars.bzl': no such file and referenced by '//tensorflow/compiler/mlir/tensorflow:tensorflow_test_passes'
```


### Standalone code to reproduce the issue

```shell
LLVM_SRC=/home/ubuntu/src/llvm-project
LLVM_BAZEL_OVERLAY=/home/ubuntu/src/llvm-overlay
mkdir -p ${LLVM_BAZEL_OVERLAY}
python3 ${LLVM_SRC}/utils/bazel/overlay_directories.py \
    --src ${LLVM_SRC} \
    --overlay ${LLVM_SRC}/utils/bazel/llvm-project-overlay/ \
    --target ${LLVM_BAZEL_OVERLAY}
touch ${LLVM_BAZEL_OVERLAY}/BUILD.bazel ${LLVM_BAZEL_OVERLAY}/WORKSPACE
echo 'llvm_targets = [""X86"", ""AArch64"", ""ARM""]' > ${LLVM_BAZEL_OVERLAY}/llvm/targets.bzl
```


### Relevant log output

```shell
INFO: Repository stablehlo instantiated at:
  /home/ubuntu/src/tensorflow/WORKSPACE:15:14: in <toplevel>
  /home/ubuntu/src/tensorflow/tensorflow/workspace2.bzl:960:28: in workspace
  /home/ubuntu/src/tensorflow/tensorflow/workspace2.bzl:81:14: in _initialize_third_party
  /home/ubuntu/src/tensorflow/third_party/stablehlo/workspace.bzl:11:20: in repo
  /home/ubuntu/src/tensorflow/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /home/ubuntu/src/tensorflow/third_party/repo.bzl:89:35: in <toplevel>
INFO: Repository nsync instantiated at:
  /home/ubuntu/src/tensorflow/WORKSPACE:15:14: in <toplevel>
  /home/ubuntu/src/tensorflow/tensorflow/workspace2.bzl:967:21: in workspace
  /home/ubuntu/src/tensorflow/tensorflow/workspace2.bzl:470:20: in _tf_repositories
  /home/ubuntu/src/tensorflow/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /home/ubuntu/src/tensorflow/third_party/repo.bzl:89:35: in <toplevel>
ERROR: /home/ubuntu/src/tensorflow/tensorflow/compiler/mlir/tensorflow/BUILD:1455:11: error loading package '@llvm-project//llvm': at /home/ubuntu/.cache/bazel/_bazel_ubuntu/b813f517b143a3c1665dd902035fe00f/external/llvm-project/llvm/config.bzl:8:5: cannot load '@llvm-project//:vars.bzl': no such file and referenced by '//tensorflow/compiler/mlir/tensorflow:tensorflow_test_passes'
ERROR: Analysis of target '//tensorflow/compiler/mlir:tf-opt' failed; build aborted: 
INFO: Elapsed time: 59.508s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (84 packages loaded, 255 targets configured)
    currently loading: @llvm-project//mlir ... (3 packages)
    Fetching https://storage.googleapis.com/.../github.com/openxla/stablehlo/archive/51f005f0a8ff6e28f535adfec4de936cb4097aa4.zip
    Fetching https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/nsync/archive/1.25.0.tar.gz
```
</details>"
59736,Add API to convert variable to constant in Keras saved model to improve inference performance,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf 2.11

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


Keras save_model keeps variable status instead of const. It will block some inference opportunities, such as Conv + Batchnormalization folding.

We expect Keras save format can provide similar APIs like [convert_variables_to_constants](https://www.tensorflow.org/api_docs/python/tf/compat/v1/graph_util/convert_variables_to_constants) in TF1 pb format.

With variables replaced by constant in inference, more optimization can be done.



### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
# tf.keras.utils.set_random_seed(34)
np.random.seed(34)
tf.random.set_seed(34)

training_data = np.random.normal(0, 1, size=(32, 5, 5, 3)).astype(""float32"")
test_data = np.random.normal(0, 1, size=(16, 5, 5, 3)).astype(""float32"")
label_data = np.random.normal(0, 1, size=(32, 5, 5, 3)).astype(""float32"")

model = tf.keras.models.Sequential()
model.add(tf.keras.Input(shape=(5, 5, 3)))
model.add(tf.keras.layers.Conv2D(filters=3, kernel_size=3, strides=(1, 1), padding=""same"", data_format=""channels_last""))
model.add(tf.keras.layers.BatchNormalization())

model.summary()

# Compile the model
model.compile(optimizer=keras.optimizers.Adam(1e-3), loss=tf.keras.losses.MeanSquaredError())

# Train the model for 1 epoch from Numpy data
batch_size = 1
print(""Fit on NumPy data"")
history = model.fit(training_data, label_data, batch_size=batch_size, epochs=3)

print(history.history)

predictions = model.predict(training_data, batch_size=2)
print(predictions)
print(predictions.shape)
```


### Relevant log output

```shell
We expect the Batchnormalization op is folded in above code.
```
</details>"
59735,Cropping1D cann't support negatitve integer parameter,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.11

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tf.keras.layers.Cropping1D((-20,0))(out) report error, The 'cropping' argument must be a tuple of 2 interges. The really reson is keras.utils.conv_utils.normalize_tuple() update. 
And tf <= 2.6, the error is not exist. Is there a way to fix or keep away from this error?
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
input_shape = (2, 3, 2)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
y = tf.keras.layers.Cropping1D(cropping=(-20,0))(x)
```
```


### Relevant log output

_No response_</details>"
59732,TF 2.9 on Ubuntu 22 utilizes less memory than TF 2.3 on Ubuntu 16,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

No

### OS Platform and Distribution

Ubuntu 22

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2

### GPU model and memory

_No response_

### Current Behaviour?


We recently switched our cluster system from Ubuntu 16.04, TF2.3, CUDA 10.1 to Ubuntu 22.04, TF2.9 and CUDA 11.2. After some trainings crashed with an out-of-memory error we found that Tensorflow is now utilizing less memory than before (ca. 600mb on an RTX 2080 Ti).

The nvidia-driver is 510.85.02 for Ubuntu 22 and 510.54 for Ubuntu 16.

I understand many factors can play a role here (OS version, TF version, Cuda version). I will update the information once I am able to also run TF2.3/Cuda10.1 on Ubuntu 22.



### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.device(""gpu"")
```


### Relevant log output


Before (Ubuntu 16.04, TF 2.3, CUDA 10.1):

```
Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10078 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:e1:00.0, compute capability: 7.5)
```

relevant `nvidia-smi` output:
```
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |
| 45%   73C    P2   215W / 250W |  10838MiB / 11264MiB |     90%      Default |
|                               |                      |                  N/A |
```

After (Ubuntu 22.04, TF2.9, CUDA 11.2)

```
Created device /device:GPU:0 with 9650 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:20:00.0, compute capability: 7.5
```

relevant `nvidia-smi` output:
```
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:1D:00.0 Off |                  N/A |
| 78%   72C    P2   191W / 250W |  10266MiB / 11264MiB |     79%      Default |
|                               |                      |                  N/A |
```
```
</details>"
59731,tensorflow-cpu-aws 2.12.0rc0 only has the Python 3.11 wheel,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.12.0rc0

### Custom Code

No

### OS Platform and Distribution

aarch64

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
See https://pypi.org/project/tensorflow-cpu-aws/2.12.0rc0/#files. There is only one file.
```


### Standalone code to reproduce the issue

```shell
Just go to https://pypi.org/project/tensorflow-cpu-aws/2.12.0rc0/#files.
Or reproduce using `pip` on aarch64 and Python 3.10:


pip install tensorflow==2.12.0rc0
```


### Relevant log output

```shell
Collecting tensorflow==2.12.0rc0
      Downloading tensorflow-2.12.0rc0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (2.0 kB)
    ERROR: Could not find a version that satisfies the requirement tensorflow-cpu-aws==2.12.0-rc0; platform_system == ""Linux"" and (platform_machine == ""arm64"" or platform_machine == ""aarch64"") (from tensorflow) (from versions: 2.9.1, 2.10.0rc0, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0)
    ERROR: No matching distribution found for tensorflow-cpu-aws==2.12.0-rc0; platform_system == ""Linux"" and (platform_machine == ""arm64"" or platform_machine == ""aarch64"")
```
</details>"
59730,Unable to run quantized Bert model,"### 1. System information

- OS Platform and Distribution: Ubuntu 22.04, aarch64
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): tensorflow-cpu-aws==2.11.0
- Model library: transformers==4.26.1

### 2. Code

```
import tensorflow as tf
from transformers import BertConfig, TFBertModel

BATCH_SIZE = 1
SEQUENCE_LEGNTH = 10
VOCAB_SIZE = 150000

tf.random.set_seed(0)
tf.config.threading.set_intra_op_parallelism_threads(4)

bert_config = BertConfig(vocab_size=VOCAB_SIZE,
                         hidden_size=768,
                         num_hidden_layers=4,
                         num_attention_heads=12,
                         intermediate_size=1200,
                         hidden_act='gelu',
                         hidden_dropout_prob=0.1,
                         attention_probs_dropout_prob=0.1,
                         max_position_embeddings=512,
                         type_vocab_size=2,
                         initializer_range=0.02,
                         layer_norm_eps=1e-12,
                         position_embedding_type='absolute',
                         return_dict=False)

model = TFBertModel(bert_config)
input = [
    tf.random.uniform(shape=(BATCH_SIZE, SEQUENCE_LEGNTH), minval=0, maxval=VOCAB_SIZE, dtype=tf.int32),
    tf.ones(shape=(BATCH_SIZE, SEQUENCE_LEGNTH), dtype=tf.int32)
]

# Run original model
model(input)
print(""1"")

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
model2 = converter.convert()

interpreter = tf.lite.Interpreter(model_content=model2)
input0_index = interpreter.get_input_details()[0][""index""]
input1_index = interpreter.get_input_details()[1][""index""]
output0_index = interpreter.get_output_details()[0][""index""]
output1_index = interpreter.get_output_details()[1][""index""]

interpreter.resize_tensor_input(input0_index, input[0].shape, strict=True)
interpreter.resize_tensor_input(input1_index, input[1].shape, strict=True)
interpreter.resize_tensor_input(output0_index, [BATCH_SIZE, SEQUENCE_LEGNTH, 768], strict=True)
interpreter.resize_tensor_input(output1_index, [BATCH_SIZE, 768], strict=True)

interpreter.allocate_tensors()
interpreter.set_tensor(input0_index, input[0])
interpreter.set_tensor(input1_index, input[1])

# Run converted model
interpreter.invoke() # Seg Fault
print(""2"")
```

### 3. Failure after conversion

The conversion is successful, but interpreter's invoke() fails with segmentation fault.

### 4. (optional) RNN conversion support
N/ A

### 5. (optional) Any other info / logs
I was mainly following the example code [here](https://www.tensorflow.org/lite/performance/post_training_quant#test_the_model_on_one_image), and tried to convert Bert model to a quantized one. The interpreter failed with the following error:

```
/usr/local/lib/python3.10/dist-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/usr/local/lib/python3.10/dist-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']
caused by: [""[Errno 2] The file to load file system plugin from does not exist.: '/usr/local/lib/python3.10/dist-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so'""]
  warnings.warn(f""unable to load libtensorflow_io_plugins.so: {e}"")
/usr/local/lib/python3.10/dist-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/usr/local/lib/python3.10/dist-packages/tensorflow_io/python/ops/libtensorflow_io.so']
caused by: ['/usr/local/lib/python3.10/dist-packages/tensorflow_io/python/ops/libtensorflow_io.so: cannot open shared object file: No such file or directory']
  warnings.warn(f""file system plugins are not loaded: {e}"")
1
2023-02-17 18:55:23.466676: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)
2023-02-17 18:55:23.466855: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session
2023-02-17 18:55:29.150726: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.
2023-02-17 18:55:29.150768: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
Segmentation fault (core dumped)
```
"
59729,Modifying the original tensor can change the value of copied tensor,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.13.0.dev20230204

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Modifying the original tensor can change the value of the copied tensor. This buggy behavior is unstable so we run it 100 times to reproduce. The bug only happens on the CPU.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

for _ in range(100):
    with tf.device(""cpu""):
        x = np.arange(10)
        x_copy = tf.experimental.numpy.copy(x)

        x[3] = 42
        assert tf.experimental.numpy.all((x_copy == tf.range(10, dtype=tf.int64))), x_copy
```


### Relevant log output

```shell
AssertionError: tf.Tensor([ 0  1  2 42  4  5  6  7  8  9], shape=(10,), dtype=int64)
```
</details>"
59728,Cast int32 to bfloat16 does not run on A100 GPU,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

TensorFlow version 2.13.0-dev20230215

### Custom Code

Yes

### OS Platform and Distribution

ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.8/8.6

### GPU model and memory

single A100 80G

### Current Behaviour?

```shell
when using tf.cast to cast tf.int32 tensor to tf.bfloat16 tensor, op run on GPU.

when i convert int32->bfloat16, it run on CPU.
when i convert int32->float32->bfloat16, it run on GPU. is it expected ?
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.keras import mixed_precision

tf.debugging.set_log_device_placement(True)
policy = mixed_precision.Policy('mixed_bfloat16')
print(policy.name)
mixed_precision.set_global_policy(policy)

class toy_layer(tf.keras.layers.Layer):
  def build(self, input_shape):
    self.kernel = self.add_weight('kernel', (input_shape[-1], 10))
  def call(self, inputs):
    out = tf.linalg.matmul(inputs, self.kernel)
    out2 = tf.ones((10, 10), dtype=tf.int32)
    #out2 = tf.cast(out2, tf.float32, name=""cast_out2_1"")
    out2 = tf.cast(out2, out.dtype, name=""cast_out2_2"")
    out3 = out * out2
    return out3

layer = toy_layer()
y = layer(tf.ones((10, 10)))
```


### Relevant log output

```shell
2023-02-17 16:26:34.748024: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-17 16:26:35.370677: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
mixed_bfloat16
2023-02-17 16:26:36.876507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78915 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0001:00:00.0, compute capability: 8.0
input: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-17 16:26:36.886233: I tensorflow/core/common_runtime/placer.cc:114] input: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
_EagerConst: (_EagerConst): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:36.886254: I tensorflow/core/common_runtime/placer.cc:114] _EagerConst: (_EagerConst): /job:localhost/replica:0/task:0/device:GPU:0
output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:36.886267: I tensorflow/core/common_runtime/placer.cc:114] output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:36.889118: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
input: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.293473: I tensorflow/core/common_runtime/placer.cc:114] input: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
_EagerConst: (_EagerConst): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.293513: I tensorflow/core/common_runtime/placer.cc:114] _EagerConst: (_EagerConst): /job:localhost/replica:0/task:0/device:GPU:0
output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.293521: I tensorflow/core/common_runtime/placer.cc:114] output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.294777: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
dims: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-17 16:26:37.295227: I tensorflow/core/common_runtime/placer.cc:114] dims: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
value: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.295238: I tensorflow/core/common_runtime/placer.cc:114] value: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
Fill: (Fill): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.295250: I tensorflow/core/common_runtime/placer.cc:114] Fill: (Fill): /job:localhost/replica:0/task:0/device:GPU:0
output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.295257: I tensorflow/core/common_runtime/placer.cc:114] output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.295729: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.297303: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.297667: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.297928: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.298227: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
seed: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-17 16:26:37.298533: I tensorflow/core/common_runtime/placer.cc:114] seed: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
StatelessRandomGetKeyCounter: (StatelessRandomGetKeyCounter): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.298547: I tensorflow/core/common_runtime/placer.cc:114] StatelessRandomGetKeyCounter: (StatelessRandomGetKeyCounter): /job:localhost/replica:0/task:0/device:GPU:0
key_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.298561: I tensorflow/core/common_runtime/placer.cc:114] key_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
counter_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.298573: I tensorflow/core/common_runtime/placer.cc:114] counter_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.299212: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op StatelessRandomGetKeyCounter in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.300704: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
shape: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-17 16:26:37.300963: I tensorflow/core/common_runtime/placer.cc:114] shape: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
key: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.300974: I tensorflow/core/common_runtime/placer.cc:114] key: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
counter: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.300979: I tensorflow/core/common_runtime/placer.cc:114] counter: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
alg: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-17 16:26:37.300993: I tensorflow/core/common_runtime/placer.cc:114] alg: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
StatelessRandomUniformV2: (StatelessRandomUniformV2): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.300999: I tensorflow/core/common_runtime/placer.cc:114] StatelessRandomUniformV2: (StatelessRandomUniformV2): /job:localhost/replica:0/task:0/device:GPU:0
output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.301008: I tensorflow/core/common_runtime/placer.cc:114] output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.301526: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op StatelessRandomUniformV2 in device /job:localhost/replica:0/task:0/device:GPU:0
x: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.302304: I tensorflow/core/common_runtime/placer.cc:114] x: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
y: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.302316: I tensorflow/core/common_runtime/placer.cc:114] y: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
Sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.302332: I tensorflow/core/common_runtime/placer.cc:114] Sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0
z_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.302338: I tensorflow/core/common_runtime/placer.cc:114] z_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.302706: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0
x: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.303361: I tensorflow/core/common_runtime/placer.cc:114] x: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
y: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.303372: I tensorflow/core/common_runtime/placer.cc:114] y: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
Mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.303379: I tensorflow/core/common_runtime/placer.cc:114] Mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
z_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.303391: I tensorflow/core/common_runtime/placer.cc:114] z_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.303748: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0
x: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.304205: I tensorflow/core/common_runtime/placer.cc:114] x: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
y: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.304216: I tensorflow/core/common_runtime/placer.cc:114] y: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
AddV2: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.304223: I tensorflow/core/common_runtime/placer.cc:114] AddV2: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0
z_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.304229: I tensorflow/core/common_runtime/placer.cc:114] z_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.304663: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0
resource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.305249: I tensorflow/core/common_runtime/placer.cc:114] resource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
VarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.305263: I tensorflow/core/common_runtime/placer.cc:114] VarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.305637: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
resource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.306104: I tensorflow/core/common_runtime/placer.cc:114] resource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
value: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.306122: I tensorflow/core/common_runtime/placer.cc:114] value: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.306139: I tensorflow/core/common_runtime/placer.cc:114] AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.306554: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
x: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.307498: I tensorflow/core/common_runtime/placer.cc:114] x: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
Cast: (Cast): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.307523: I tensorflow/core/common_runtime/placer.cc:114] Cast: (Cast): /job:localhost/replica:0/task:0/device:GPU:0
y_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.307532: I tensorflow/core/common_runtime/placer.cc:114] y_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.307943: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Cast in device /job:localhost/replica:0/task:0/device:GPU:0
resource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.308714: I tensorflow/core/common_runtime/placer.cc:114] resource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.308734: I tensorflow/core/common_runtime/placer.cc:114] ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:0
value_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.308743: I tensorflow/core/common_runtime/placer.cc:114] value_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.309154: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.309366: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Cast in device /job:localhost/replica:0/task:0/device:GPU:0
a: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.309693: I tensorflow/core/common_runtime/placer.cc:114] a: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
b: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.309710: I tensorflow/core/common_runtime/placer.cc:114] b: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.309727: I tensorflow/core/common_runtime/placer.cc:114] MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
product_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.309738: I tensorflow/core/common_runtime/placer.cc:114] product_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.310165: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.897703: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.897841: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
dims: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-17 16:26:37.898261: I tensorflow/core/common_runtime/placer.cc:114] dims: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
value: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-17 16:26:37.898272: I tensorflow/core/common_runtime/placer.cc:114] value: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
Fill: (Fill): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.898280: I tensorflow/core/common_runtime/placer.cc:114] Fill: (Fill): /job:localhost/replica:0/task:0/device:GPU:0
output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.898285: I tensorflow/core/common_runtime/placer.cc:114] output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.898934: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0
x: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-17 16:26:37.899509: I tensorflow/core/common_runtime/placer.cc:114] x: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
Cast: (Cast): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-17 16:26:37.899523: I tensorflow/core/common_runtime/placer.cc:114] Cast: (Cast): /job:localhost/replica:0/task:0/device:CPU:0
y_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-17 16:26:37.899530: I tensorflow/core/common_runtime/placer.cc:114] y_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-02-17 16:26:37.899935: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Cast in device /job:localhost/replica:0/task:0/device:CPU:0
x: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.900335: I tensorflow/core/common_runtime/placer.cc:114] x: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
y: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.900348: I tensorflow/core/common_runtime/placer.cc:114] y: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
Mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.900361: I tensorflow/core/common_runtime/placer.cc:114] Mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
z_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.900369: I tensorflow/core/common_runtime/placer.cc:114] z_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-02-17 16:26:37.901006: I tensorflow/core/common_runtime/eager/execute.cc:1514] Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0
```
</details>"
59727,RaggedTensor slice gradient computation error,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.10

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Attempting to compute the gradient, both as part of an autographed keras layer, or in eager mode, fails. This seems to happen when concatenating different slices of a ragged tensor.
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1kteIaQeDouRH-DEEYYeGE9jiMtgcXUZm#scrollTo=MOhPQNf4JDBB


import tensorflow as tf

values = tf.constant([0, 1,2,3,4,5,6,7,8,9], tf.float32)
values = tf.reshape(values, [-1, 1])

r = tf.RaggedTensor.from_row_lengths(values, [0, 2, 2, 1, 0, 2, 3, 0, 0])
r = tf.RaggedTensor.from_uniform_row_length(r, 3)
r = tf.RaggedTensor.from_uniform_row_length(r, 3)

def crop(raggedImage: tf.RaggedTensor, top, bottom, left, right) -> tf.RaggedTensor:
    '''
    Crops a ragged tensor, removing 'pixels' from the boundary.
    The input is interpreted as being b x h x w x s x c layout.
    Only the sample dimension may be ragged.
    '''
    if bottom > 0:
        bottom = -bottom
    else:
        bottom = None
    if right > 0:
        right = -right
    else:
        right = None
    cropped = raggedImage[:, top : bottom, left : right, :, :]
    return cropped

diameter = 3

with tf.GradientTape() as tape:
	tape.watch(r)
	l = []
	for u in range(diameter):
		for v in range(diameter):
			l.append(crop(r, u, diameter-1-u, v, diameter-1-v))
	s = tf.concat(l, axis=2)

tf.print(tape.gradient(s, [r]))
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""venv-2.10/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py"", line 663, in _num_elements
    shape_tuple = grad.values._shape_tuple()  # pylint: disable=protected-access
AttributeError: 'IndexedSlices' object has no attribute '_shape_tuple'
```
</details>"
59726,"""unknown rank error"" in tensorflow.keras.layers.Layer with tensorflow.py_function output","Issue Type -> Bug
Have you reproduced the bug with TF nightly? -> No
Tensorflow Version -> v2.11.0-rc2-17-gd5b57ca93e5,  2.11.0
Custom Code -> Yes
OS Platform and Distribution -> Google colab 

#### Standalone code to reproduce the issue
```python3
import numpy as np,tensorflow as tf 
@tf.function
def sample(inputs:tf.Tensor):                                                   # some sample function 

  def numpy_method(inputs:np.ndarray):
    # some complex method 
    return np.sqrt(inputs)# example code 

  inputs=tf.numpy_function(numpy_method,[inputs],inputs.dtype,name='outputs')   # make apply numpy method 
  inputs=tf.keras.layers.AvgPool2D()(inputs)                                    # ERROR !!!!!

  return inputs

sample(tf.random.uniform((1,32,32,3)))
```

#### Relevant log output

```shell

.....

ValueError: in user code:

    File ""<ipython-input-189-8001acaa2b43>"", line 10, in sample  *
        inputs=tf.keras.layers.AvgPool2D()(inputs)                                    # ERROR !!!!!
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler  **
        raise e.with_traceback(filtered_tb) from None

    ValueError: Exception encountered when calling layer 'average_pooling2d' (type AveragePooling2D).
    
    Cannot take the length of shape with unknown rank.
    
    Call arguments received by layer 'average_pooling2d' (type AveragePooling2D):
      • inputs=tf.Tensor(shape=<unknown>, dtype=float32)
```

#### But works fine if we reverse the order (i.e. tf.numpy_function cannot properly cast back outputs to tensors)
```python3
import numpy as np,tensorflow as tf 
@tf.function
def sample(inputs:tf.Tensor):                                                   # some sample function 

  def numpy_method(inputs:np.ndarray):
    # some complex method 
    return np.sqrt(inputs)# example code 

  inputs=tf.keras.layers.AvgPool2D()(inputs)                                    # works fine 
  inputs=tf.numpy_function(numpy_method,[inputs],inputs.dtype,name='outputs')   # make apply numpy method 

  return inputs

sample(tf.random.uniform((1,32,32,3)))
```
#### Output 
```shell
<tf.Tensor: shape=(1, 16, 16, 3), dtype=float32, numpy=
array([[[[0.7783269 , 0.6260437 , 0.81261575] ......
```
This issue was also mention here -> #37193 [""Cannot take the length of shape with unknown rank"" error](https://github.com/tensorflow/tensorflow/issues/37193#top) in but not resolved. "
59725,locals() not working in tensorflow.function decorator ,"Issue Type -> Bug
Have you reproduced the bug with TF nightly? -> No
Tensorflow Version -> v2.11.0-rc2-17-gd5b57ca93e5,  2.11.0
Custom Code -> Yes
OS Platform and Distribution -> Google colab 

#### Standalone code to reproduce the issue

```python3
import tensorflow as tf
@tf.function 
def sample_function(inputs:tf.Tensor,application_order:list=['method_3','method_1','method_2']):

  def method_1(inputs:tf.Tensor): return inputs*2                               # some complex code  
  def method_2(inputs:tf.Tensor): return inputs/2                               # some complex code
  def method_3(inputs:tf.Tensor): return inputs*3                               # some complex code

  print(locals().keys())                                                        # check all methods ! work fine all are here as dictionary

  for application in application_order:
    inputs=locals()[application](inputs)                                        # ERROR with tensorflow.function decorator 
  
  return inputs

sample_function(tf.random.normal((1,3,3,1)))                                    # call function # ERROR
```

#### Relevant log output
```shell
dict_keys(['application_order', 'do_return', 'retval_', 'method_1', 'method_2', 'method_3', 'fscope', 'ag__', 'inputs'])

..... 

KeyError: in user code:

    File ""<ipython-input-128-82f7957c05a2>"", line 12, in sample_function  *
        inputs=locals()[application](inputs)                                        # ERROR with tensorflow.function decorator

    KeyError: 'method_3'
```

#### Work fine without tensorflow.function
```python3
import tensorflow as tf
#@tf.function 
def sample_function(inputs:tf.Tensor,application_order:list=['method_3','method_1','method_2']):

  def method_1(inputs:tf.Tensor): return inputs*2                               # some complex code  
  def method_2(inputs:tf.Tensor): return inputs/2                               # some complex code
  def method_3(inputs:tf.Tensor): return inputs*3                               # some complex code

  print(locals().keys())                                                        # check all methods ! work fine all are here as dictionary

  for application in application_order:
    inputs=locals()[application](inputs)                                        # working fine as normal function !!!!!!!!! 
  
  return inputs

sample_function(tf.random.normal((1,3,3,1)))                                    # call function # works fine
```
#### Relevant log output
```shell
dict_keys(['inputs', 'application_order', 'method_1', 'method_2', 'method_3'])
<tf.Tensor: shape=(1, 3, 3, 1), dtype=float32, numpy=
....

```
"
59724,Feature request: tf.gather which returns 0 on invalid indices,"This is about `tf.gather`. The wanted behavior could be added directly, or maybe via a new option, or maybe via a separate op.

Actually, on GPU (at least where I tested it), the behavior of `tf.gather` is already as I want it:

- It returns 0 for invalid indices.
- I think (not really verified): gradients for those 0s back to the `param` would go nowhere. (This is important though.)

On CPU, the current behavior is:

- An exception is raised.

I basically want the same behavior on CPU as we currently have it on GPU.

My current workaround is sth like:
```python
# need to extend param such that we can lead gradients to nowhere
zeros = tf.zeros([tf.shape(param)[i] if i != axis else 1 for i in range(param.ndim)], dtype=param.dtype)
param = tf.concat([param, zeros], axis=axis)
indices = tf.where(indices >= 0 and indices < tf.shape(param)[axis], indices, tf.shape(param)[axis] - 1)
gather = tf.gather(param, indices, axis=axis)
```
"
59723,Building for mac catalyst,"Hi I created a stackoverlfow post already: https://stackoverflow.com/questions/75473220/build-tensorflow-lite-for-mac-catalyst/75480973#75480973
But I thought I should have asked here first.

### System information
 
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac mini M1 Ventura 13.1
-   **TensorFlow installed from (source or binary)**: source
-   **TensorFlow version (use command below)**: 2.10.0
-   **Python version**: 3.10.8
-   **Bazel version (if compiling from source)**: 5.3.0
-   **GCC/Compiler version (if compiling from source)**: clang 14.0.0
-   **Exact command to reproduce**: bazel build --config=catalyst //tensorflow/lite:tensorflowlite.framework

### Describe the problem
So I'm trying to build tensorflow lite from source (github) with bazel for mac catalyst and I wanted to know if it is featured and if not if there is anyway I could build it.

### Source code / logs
For now without modification of any sort I have this error: `ERROR: Config value 'catalyst' is not defined in any .rc file` 
"
59722,TensorFlow hangs in session.run,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

tensorflow-macos 2.11.0

### Custom Code

Yes

### OS Platform and Distribution

MacOS 12, tensorflow-macos and tensorflow-metal

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

M1

</details>

### Current Behaviour?

This is a new neural model I implemented, and I want to do training. It's modified based on an existing attention-based encoder-decoder model, where everything works fine.

In the new model, it just hangs in `session.run` and does not do anything. I also cannot interrupt it. It hangs inside the TensorFlow C++ code.

This seems to be specific for Mac M1 hardware. I cannot reproduce the problem on other hardware or environments.


### Standalone code to reproduce the issue

So far I don't have a minimal example, and this will be quite a big effort to generate one, as this is some very complex model. But here some relevant details:

* This is based on [RETURNN](https://github.com/rwth-i6/returnn).
* We still use graph-mode.
* I tested both with control flow v1 (calling `disable_control_flow_v2`) and control flow v2. It hangs in both cases.
* I tested using tfdbg or `enable_dump_debug_info`. It crashes then with a segfault.
* I get a number of other warnings, which are maybe related. See below.

To reproduce:

* Code: https://github.com/rwth-i6/i6_experiments/blob/81bcef39b5829aa43b84bcab4b4fa03f82fc3bc5/users/zeyer/experiments/exp2023_02_16_chunked_attention/demo_returnn_config.py
* Checkout the [i6_experiments repo](https://github.com/rwth-i6/i6_experiments). (Maybe use commit 81bcef39b5829aa43b84bcab4b4fa03f82fc3bc5 to be sure.)
* Checkout [RETURNN](https://github.com/rwth-i6/returnn). (Maybe commit 2ed598443f22de42599a0fee9bc43fbb5e0abec2 to be sure.)
* Run: `python3 returnn/rnn.py i6_experiments/users/zeyer/experiments/exp2023_02_16_chunked_attention/demo_returnn_config.py`


### Relevant log output

With control flow v2:

```
2023-02-17 10:02:03.997491: W tensorflow/core/common_runtime/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:
type_id: TFT_OPTIONAL
args {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_TENSOR
    args {
      type_id: TFT_INT32
    }
  }
}
 is neither a subtype nor a supertype of the combined inputs preceding it:
type_id: TFT_OPTIONAL
args {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_TENSOR
    args {
      type_id: TFT_FLOAT
    }
  }
}

	while inferring type of node 'output/rec/while/body/_38/output/rec/prev_target_embed_moved_input/cond/output/_1608'

2023-02-17 10:34:46.595736: W tensorflow/c/c_api.cc:291] Operation '{name:'global_step' id:1961 op device:{requested: '/device:CPU:0', assigned: ''} def:{{{node global_step}} = VarHandleOp[_class=[""loc:@global_step""], _has_manual_control_dependencies=true, allowed_devices=[], container="""", dtype=DT_INT64, shape=[], shared_name=""global_step"", _device=""/device:CPU:0""]()}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2023-02-17 10:35:56.799620+0100 python3[5197:2744697] Execution of the command buffer was aborted due to an error during execution. Caused GPU Timeout Error (00000002:kIOGPUCommandBufferCallbackErrorTimeout)
...
2023-02-17 10:36:01.801307+0100 python3[5197:2744697] Execution of the command buffer was aborted due to an error during execution. Ignored (for causing prior/excessive GPU errors) (00000004:kIOGPUCommandBufferCallbackErrorSubmissionsIgnored)
...
```
(Related: https://github.com/tensorflow/tensorflow/issues/57052)

With control flow v1:
```
2023-02-17 10:10:01.733679: W tensorflow/c/c_api.cc:291] Operation '{name:'global_step' id:1528 op device:{requested: '/device:CPU:0', assigned: ''} def:{{{node global_step}} = VarHandleOp[_class=[""loc:@global_step""], _has_manual_control_dependencies=true, allowed_devices=[], container="""", dtype=DT_INT64, shape=[], shared_name=""global_step"", _device=""/device:CPU:0""]()}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2023-02-17 10:10:14.257716+0100 python3[3727:2732395] Execution of the command buffer was aborted due to an error during execution. Caused GPU Timeout Error (00000002:kIOGPUCommandBufferCallbackErrorTimeout)
2023-02-17 10:10:14.257754+0100 python3[3727:2732582] Execution of the command buffer was aborted due to an error during execution. Caused GPU Timeout Error (00000002:kIOGPUCommandBufferCallbackErrorTimeout)
2023-02-17 10:10:14.258366+0100 python3[3727:2732395] Execution of the command buffer was aborted due to an error during execution. Caused GPU Timeout Error (00000002:kIOGPUCommandBufferCallbackErrorTimeout)
2023-02-17 10:10:14.258504+0100 python3[3727:2732582] Execution of the command buffer was aborted due to an error during execution. Caused GPU Timeout Error (00000002:kIOGPUCommandBufferCallbackErrorTimeout)
2023-02-17 10:10:14.258541+0100 python3[3727:2732395] Execution of the command buffer was aborted due to an error during execution. Caused GPU Timeout Error (00000002:kIOGPUCommandBufferCallbackErrorTimeout)
2023-02-17 10:10:14.258587+0100 python3[3727:2732395] Execution of the command buffer was aborted due to an error during execution. Caused GPU Timeout Error (00000002:kIOGPUCommandBufferCallbackErrorTimeout)
2023-02-17 10:10:19.258726+0100 python3[3727:2732395] Execution of the command buffer was aborted due to an error during execution. Caused GPU Timeout Error (00000002:kIOGPUCommandBufferCallbackErrorTimeout)
2023-02-17 10:10:19.258784+0100 python3[3727:2732395] Execution of the command buffer was aborted due to an error during execution. Caused GPU Timeout Error (00000002:kIOGPUCommandBufferCallbackErrorTimeout)
```
"
59721,Bad CRC while building TensorFlowLiteSelectTfOps_framework,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

No

### OS Platform and Distribution

Macbook Air M1, Ventura 13.1

### Mobile device

_No response_

### Python version

3.10.10

### Bazel version

5.3.0

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Executing the following command:
```shell
bazel build -c opt --config=ios --ios_multi_cpus=arm64,x86_64 \
  //tensorflow/lite/ios:TensorFlowLiteSelectTfOps_framework
```
results in the following error:
```
ERROR: /Users/vishnu/work/ente/tensorflow/tensorflow/lite/ios/BUILD:146:21: Processing and signing TensorFlowLiteSelectTfOps_framework failed: (Exit 2): process-and-sign--932494739.sh failed: error executing command bazel-out/applebin_ios-ios_arm64-opt-ST-82c2c37ad712/bin/tensorflow/lite/ios/TensorFlowLiteSelectTfOps_framework-intermediates/process-and-sign--932494739.sh should_compress
bazel-out/applebin_ios-ios_arm64-opt-ST-82c2c37ad712/bin/tensorflow/lite/ios/TensorFlowLiteSelectTfOps_framework_archive-root/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps  bad CRC a72ee873  (should be a131c8e2)
```


### Standalone code to reproduce the issue
Executing the following command on the current `main` (0c955358f75cc3d2f0e21e60cb9b3a2bd3faa24b) as well as `v2.11.0` reproduces the error:

```shell
bazel build -c opt --config=ios --ios_multi_cpus=arm64,x86_64 \
  //tensorflow/lite/ios:TensorFlowLiteSelectTfOps_framework
```


### Relevant log output

```shell
bazel build -c opt --config=ios --ios_multi_cpus=arm64,x86_64 \
  //tensorflow/lite/ios:TensorFlowLiteSelectTfOps_framework

INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=101
INFO: Reading rc options for 'build' from /Users/vishnu/work/ente/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /Users/vishnu/work/ente/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false
INFO: Reading rc options for 'build' from /Users/vishnu/work/ente/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/opt/homebrew/opt/python@3.10/bin/python3.10 --action_env PYTHON_LIB_PATH=/opt/homebrew/opt/python@3.10/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages --python_path=/opt/homebrew/opt/python@3.10/bin/python3.10
INFO: Reading rc options for 'build' from /Users/vishnu/work/ente/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /Users/vishnu/work/ente/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /Users/vishnu/work/ente/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:ios in file /Users/vishnu/work/ente/tensorflow/.bazelrc: --apple_platform_type=ios --apple_bitcode=embedded --copt=-fembed-bitcode --copt=-Wno-c++11-narrowing --noenable_platform_specific_config --copt=-w --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --define=with_xla_support=false
INFO: Analyzed target //tensorflow/lite/ios:TensorFlowLiteSelectTfOps_framework (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: /Users/vishnu/work/ente/tensorflow/tensorflow/lite/ios/BUILD:146:21: Processing and signing TensorFlowLiteSelectTfOps_framework failed: (Exit 2): process-and-sign--932494739.sh failed: error executing command bazel-out/applebin_ios-ios_arm64-opt-ST-82c2c37ad712/bin/tensorflow/lite/ios/TensorFlowLiteSelectTfOps_framework-intermediates/process-and-sign--932494739.sh should_compress
bazel-out/applebin_ios-ios_arm64-opt-ST-82c2c37ad712/bin/tensorflow/lite/ios/TensorFlowLiteSelectTfOps_framework_archive-root/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps  bad CRC a72ee873  (should be a131c8e2)
Target //tensorflow/lite/ios:TensorFlowLiteSelectTfOps_framework failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 38.891s, Critical Path: 38.60s
INFO: 4 processes: 3 internal, 1 local.
FAILED: Build did NOT complete successfully
```
</details>"
59720,Tensorflow 2.12.0rc0 pre-release does not provide GPU package,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.12.0rc0

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
PyPi lists the tensorflow==2.12.0rc0 package, but not the equivalent GPU package tensorflow-gpu==2.12.0rc0:
https://pypi.org/project/tensorflow/#history
https://pypi.org/project/tensorflow-gpu/#history

Is the GPU pre-release package going to be published ad PyPi as well?
```


### Standalone code to reproduce the issue

```shell
pip install tensorflow-gpu==2.12.0rc0
```


### Relevant log output

_No response_</details>"
59719,Inconsistent Runtime of XLA Compiled Model Inference,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8.13

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.6

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hi, I am trying to measure the runtime of a jit compiled model inference using time.time(). Code and output is provided below.

From my understanding, I would expect the first iteration to take longer since the function needs to be traced and compiled. However, I’m confused why the runtime from iteration 36 and onwards increases significantly. Is there an issue with the way I am measuring the runtime? If not, what would explain the increase in runtime?

I am running this on a P40 GPU by the way on tf 2.11. I had tried to run the same code on the nightly version but got the error ""*** TypeError: expected bytes, tensorflow.python.client._pywrap_tf_session.StringBuffer found"" when calling the compiled function. Thanks in advance.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import time

model = tf.keras.Sequential()
layers = 15
for _ in range(layers):
    model.add(tf.keras.layers.Conv2D(3, 3, data_format=""channels_first""))
model_input = tf.random.uniform(shape=(12, 3, 256, 256))
xla_fn = tf.function(model, jit_compile=True)
iterations = 50
for i in range(iterations):
    start_time = time.time()
    xla_fn(model_input)
    end_time = time.time()
    print(f""Iteration {i} time: {1000 * (end_time - start_time)}"")
```


### Relevant log output

```shell
Iteration 0 time: 6884.105682373047
Iteration 1 time: 1.5869140625
Iteration 2 time: 1.2054443359375
Iteration 3 time: 1.1410713195800781
Iteration 4 time: 1.1148452758789062
Iteration 5 time: 1.1096000671386719
Iteration 6 time: 1.1081695556640625
Iteration 7 time: 1.1057853698730469
Iteration 8 time: 1.1026859283447266
Iteration 9 time: 1.0957717895507812
Iteration 10 time: 1.115560531616211
Iteration 11 time: 1.100301742553711
Iteration 12 time: 1.1072158813476562
Iteration 13 time: 1.0993480682373047
Iteration 14 time: 1.102447509765625
Iteration 15 time: 1.100778579711914
Iteration 16 time: 1.0983943939208984
Iteration 17 time: 1.111745834350586
Iteration 18 time: 1.0929107666015625
Iteration 19 time: 1.0929107666015625
Iteration 20 time: 1.10626220703125
Iteration 21 time: 1.1260509490966797
Iteration 22 time: 1.1012554168701172
Iteration 23 time: 1.0957717895507812
Iteration 24 time: 1.1103153228759766
Iteration 25 time: 1.0924339294433594
Iteration 26 time: 1.0981559753417969
Iteration 27 time: 1.0950565338134766
Iteration 28 time: 1.1153221130371094
Iteration 29 time: 1.0952949523925781
Iteration 30 time: 1.100301742553711
Iteration 31 time: 1.1081695556640625
Iteration 32 time: 1.0983943939208984
Iteration 33 time: 1.0955333709716797
Iteration 34 time: 1.0974407196044922
Iteration 35 time: 1.1048316955566406
Iteration 36 time: 13.811588287353516
Iteration 37 time: 18.66292953491211
Iteration 38 time: 18.66936683654785
Iteration 39 time: 18.68462562561035
Iteration 40 time: 18.68152618408203
Iteration 41 time: 19.069910049438477
Iteration 42 time: 17.874717712402344
Iteration 43 time: 18.11075210571289
Iteration 44 time: 18.109560012817383
Iteration 45 time: 18.10741424560547
Iteration 46 time: 18.124818801879883
Iteration 47 time: 18.130064010620117
Iteration 48 time: 17.90904998779297
Iteration 49 time: 17.884492874145508
```
</details>"
59716,"The Whisper Hybrid encoder model with dynamic quantization functions properly, but it fails when using a full int8 model with post-training quantization.","### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Refer the below colab to reproduce the issue ..
https://colab.research.google.com/drive/1S_3bVlwRZkMaYvvKwtPfWlyXQLS0Bvxa?usp=sharing





```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
59715,[Tensorflow Nightly] Type Error in `graph_def_versions`,"### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Tensorflow Version

2.13.0-dev20230216


### Standalone code to reproduce the issue

```python
import tensorflow as tf

def generate_model():
  return tf.keras.models.Sequential([
    tf.keras.layers.Input(shape=(256, 256, 32)),
    tf.keras.layers.Conv2D(32, (3, 3), padding='same'),
    tf.keras.layers.Activation('relu'),
    tf.keras.layers.Conv2D(32, (3, 3)),
    tf.keras.layers.Activation('relu'),
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
    tf.keras.layers.Dropout(0.25),
    tf.keras.layers.Conv2D(64, (3, 3), padding='same'),
    tf.keras.layers.Activation('relu'),
    tf.keras.layers.Conv2D(64, (3, 3)),
    tf.keras.layers.Activation('relu'),
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
    tf.keras.layers.Dropout(0.25),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512),
    tf.keras.layers.Activation('relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(10),
    tf.keras.layers.Activation('softmax')
  ])

model = generate_model()
```


### Relevant log output

```python
Which produces:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<stdin>"", line 2, in generate_model
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/trackable/base.py"", line 205, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3243, in graph_def_versions
    return versions_pb2.VersionDef.FromString(self._version_def)
TypeError: expected bytes, tensorflow.python.client._pywrap_tf_session.StringBuffer found
```
"
59713,XLA compilation slow on new calls although no recompiling is performed,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

v2.7.0-rc1-69-gc256c071bb2 2.7.0

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
(ON CPU) After compilation I expect speed up in speed using @tf.function(jit_compile=True). This is the case, however when used in a loop some of the iterations happen almost instantly whilst others take quite a long time. I feels like the code is recompiling, however using print statements I see that this is not the case. At first I thought there could be easy and difficult input samples to the function, however for further debugging I added an additional loop that just repeats every function evaluation multiple times. Here I saw that it is just one time a very slow one (feels like recompiling) and afterwards on the same input and exact same function it's really quick. No additional print statement is executed, hinting to the fact that the function is not compiled again. 
I was wondering if this is a known issue and if so how to fix it.

The last function below is called in a main script and makes use of a tf.keras.layers.Layer object and its call function. The call function is also given below and makes use of the jit_compiled function RK_multistep.
```


### Standalone code to reproduce the issue

```shell
@tf.function(jit_compile=True)
def RK_multistep(hybMod, x_k, u_k, dt):
    print(""compiling RK"")
    M = 5
    DT = dt/M
    ...

def call(self, model, x_k, u_k):
        print(""----- START RK repeats -----"")
        t1 = time.time()
        for k in range(3):
            t3 = time.time()
            x_kk = self.method(model, x_k, u_k, self.dt)
            t4 = time.time()
            print(f""RK iteration {k} took {t4 - t3}"")
        t2 = time.time()
        print(f""All RK iterations Took {t2 - t1}"")
        print(""----- END RK repeats -----"")
        x_kk = self.RK_multistep(model, x_k, u_k, self.dt)
    return x_kk

print(""----- START model in batches -----"")
t1 = time.time()
for k in range(20):
    t3 = time.time()
    Xbatch = np.expand_dims(X_train[k, :], 0)       # add batch dimension: shape (1, state_dims)
    Ubatch = np.expand_dims(Utrain[1][k, :, :], 0)  # add batch dimension: shape (1, prediction_length, control_dims)
    model_input = [Xbatch, Ubatch]
    Modelout = model(model_input)                   # here the model is called that used the call as defined above
    t4 = time.time()
    print(f""Outer layer iteration {k} took {t4 - t3}"")
t2 = time.time()
print(f""Took {t2-t1}"")
```


### Relevant log output

```shell
----- START model in batches -----
compiling g
compiling g_inner
compiling PI_big
compiling flow
compiling flow
----- START RK repeats -----
compiling RK
compiling f
compiling forcebalance
compiling pressure_change
RK iteration 0 took 16.773661613464355
RK iteration 1 took 0.0
RK iteration 2 took 0.0
All RK iterations Took 16.773661613464355
----- END RK repeats -----
Outer layer iteration 0 took 17.035797357559204
----- START RK repeats -----
RK iteration 0 took 0.0
RK iteration 1 took 0.0
RK iteration 2 took 0.0
All RK iterations Took 0.0
----- END RK repeats -----
Outer layer iteration 1 took 0.015622138977050781
----- START RK repeats -----
RK iteration 0 took 0.0
RK iteration 1 took 0.0
RK iteration 2 took 0.0
All RK iterations Took 0.0
----- END RK repeats -----
Outer layer iteration 2 took 0.0
----- START RK repeats -----
RK iteration 0 took 0.0
RK iteration 1 took 0.0
RK iteration 2 took 0.0
All RK iterations Took 0.0
----- END RK repeats -----
Outer layer iteration 3 took 0.0
----- START RK repeats -----
RK iteration 0 took 0.0
RK iteration 1 took 0.0
RK iteration 2 took 0.0
All RK iterations Took 0.0
----- END RK repeats -----
Outer layer iteration 4 took 0.015624761581420898
----- START RK repeats -----
RK iteration 0 took 15.472918510437012
RK iteration 1 took 0.01561427116394043
RK iteration 2 took 0.0
All RK iterations Took 15.488532781600952
----- END RK repeats -----
Outer layer iteration 5 took 15.488532781600952
----- START RK repeats -----
RK iteration 0 took 14.715417385101318
RK iteration 1 took 0.0
RK iteration 2 took 0.0
All RK iterations Took 14.715417385101318
----- END RK repeats -----
Outer layer iteration 6 took 14.715417385101318
----- START RK repeats -----
RK iteration 0 took 13.904020547866821
RK iteration 1 took 0.0
RK iteration 2 took 0.0
All RK iterations Took 13.904020547866821
----- END RK repeats -----
Outer layer iteration 7 took 13.921808958053589
----- START RK repeats -----
RK iteration 0 took 0.0
RK iteration 1 took 0.0
RK iteration 2 took 0.0
All RK iterations Took 0.0
----- END RK repeats -----
Outer layer iteration 8 took 0.0
...
```
</details>"
59712,tensor.ndim not working in tensorflow.function decorator,"Tensorflow Version -> v2.11.0-rc2-17-gd5b57ca93e5,  2.11.0
Custom Code -> Yes
OS Platform and Distribution -> Google colab 

### Standalone code to reproduce the issue

```python3
import tensorflow as tf

tensor=tf.random.normal((32,128,128,3))                                         # some images 
print('Work here->',tensor.ndim)

@tf.function
def some_function(images):
  #assert images.ndim==4                                                                # not working here also 
  # some complex code 
  print('NOT working here->',images.ndim)

some_function(tensor)
```

### Relevant log output

```shell
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
[<ipython-input-247-f1adf673db7a>](https://localhost:8080/#) in <module>
     10   print('NOT working here->',images.ndim)
     11 
---> 12 some_function(tensor)

1 frames
[/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py](https://localhost:8080/#) in error_handler(*args, **kwargs)
    151     except Exception as e:
    152       filtered_tb = _process_traceback_frames(e.__traceback__)
--> 153       raise e.with_traceback(filtered_tb) from None
    154     finally:
    155       del filtered_tb

[/tmp/__autograph_generated_filesflyrml3.py](https://localhost:8080/#) in tf__some_function(images)
      6         def tf__some_function(images):
      7             with ag__.FunctionScope('some_function', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:
----> 8                 ag__.ld(print)('NOT working here->', ag__.ld(images).ndim)
      9         return tf__some_function
     10     return inner_factory

AttributeError: in user code:

    File ""<ipython-input-246-0cf28125d885>"", line 10, in some_function  *
        print('NOT working here->',images.ndim)

    AttributeError: 'Tensor' object has no attribute 'ndim'
```"
59711,Cannot convert explicit Q/DQ nodes using TF-TRT,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

TF 2.10

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04.5 LTS

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.8

### GPU model and memory

_No response_

### Current Behaviour?

I am trying to convert a quantized TF model with TF-TRT, however, the following issues prevent me from doing so. I have attempted a temporary workaround to fix issue#1 but am stuck with no possible solution to the next problem. According to PR #52248, explicit Q/DQ models should be supported with Tensorflow when using Tensor-RT 8. 

*Problem 1*
The non-deprecated way to add quantize-dequantize nodes in Tensorflow models is through 
 `tf.quantization.quantize_and_dequantize_v2`. However, this adds 

https://github.com/tensorflow/tensorflow/blob/b6517cce24a06e07535c4b047a5991a290ef9368/tensorflow/python/ops/array_ops.py#L6323-L6326

nodes with tag `QuantizeAndDequantizeV4`

https://github.com/tensorflow/tensorflow/blob/6285a27e1bc55c3adb10be723b16d43c444dbdcb/tensorflow/core/ops/array_ops.cc#L2916

which is clearly not supported in the list of ops for explicit precision mode.

https://github.com/tensorflow/tensorflow/blob/7103c2ca32786fc7a2809a84e4c923196b208416/tensorflow/compiler/tf2tensorrt/convert/ops/quantization_ops.h#L35-L39

A possible workaround is to use the deprecated API `tf.quantization.quantize_and_dequantize` which will add a `QuantizeAndDequantizeV2` node which is still supported.

*Problem 2*
After performing the hack above, I encounter a second error (possibly because of incorrect usage). To convert the explicitly quantized TF saved using TensorRT I am following the examples provided in the [Nvidia TF-TRT](https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#quickstart-guide) documentation.  

However, I get failed tensorRT engine conversion issues and some warnings which I have attached in the log outputs.

**Expected Behavior**
The explicitly quantized Tensorflow saved models should be convertible with TF-TRT.


### Standalone code to reproduce the issue

```shell
**Custom quantized keras layer to build an example model**

import tensorflow as tf
from tensorflow import keras

class CustomConv2D(keras.layers.Layer):
    def __init__(self, filters, kernel_size, name=""CustomConv2d""):
        super(CustomConv2D, self).__init__()
        self.w = self.add_weight(
            shape=(kernel_size, kernel_size, filters, filters), 
            initializer=""random_normal"", 
            dtype=""float32"", 
            name=self.name+""_weights"", 
            trainable=True
        )
    
    def call(self, inputs):
        # Using the deprecated quantize_and_dequantize here since quantize_and_dequantize_v2 is listed as unsupported-ops by TF-TRT
        q_i = tf.quantization.quantize_and_dequantize(inputs, 0, 1, name=self.name+""_q_i"", narrow_range=True)
        q_w = tf.quantization.quantize_and_dequantize(self.w, -1, 1, name=self.name+""q_w"",narrow_range=True)
        return tf.nn.conv2d(q_i, q_w, 2, ""SAME"")
    

l = CustomConv2D(64, 3)
t = tf.random.normal((1, 224, 224, 64), dtype=""float32"")

model = tf.keras.Sequential()
model.add(tf.keras.layers.InputLayer(input_shape=(224, 224, 64)))
for i in range(5):
    model.add(CustomConv2D(64, 3, name=f'custom_conv2d_{i}'))

model.save('./saved_model_qat/')


**Code used for converting saved quantized TF model using TF-TRT** 
```python
from tensorflow.python.compiler.tensorrt import trt_convert as trt
converter = trt.TrtGraphConverterV2(
   input_saved_model_dir='saved_model_qat',
   precision_mode=trt.TrtPrecisionMode.INT8,
   use_calibration=False
)
trt_func = converter.convert()
converter.summary()

x_test = tf.ones((2, 224, 224, 64))


MAX_BATCH_SIZE=2
def input_fn():
   batch_size = MAX_BATCH_SIZE
   x = x_test[0:batch_size, :]
   yield [x]

converter.build(input_fn=input_fn)
```
```


### Relevant log output

```shell
Logs generated 

1. when using `quantize_and_dequantize_v2` instead of `quantize_and_dequantize` at the `trt_func = converter.convert()` step

INFO:tensorflow:Clearing prior device assignments in loaded saved model
INFO:tensorflow:Automatic mixed precision will be used on the whole TensorFlow Graph. This behavior can be deactivated using the environment variable: TF_TRT_EXPERIMENTAL_FEATURES=deactivate_mixed_precision.
More information can be found on: https://www.tensorflow.org/guide/mixed_precision.
2023-02-16 12:12:02.652333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-02-16 12:12:02.653263: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1
2023-02-16 12:12:02.653394: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session
2023-02-16 12:12:02.653696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-02-16 12:12:02.654338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-02-16 12:12:02.655044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-02-16 12:12:02.655721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-02-16 12:12:02.656447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-02-16 12:12:02.657032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1637] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20554 MB memory:  -> device: 0, name: NVIDIA A10, pci bus id: 0000:04:00.0, compute capability: 8.6
2023-02-16 12:12:02.668533: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:2359] Running auto_mixed_precision graph optimizer
2023-02-16 12:12:02.675439: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1195] Automatic Mixed Precision Grappler Pass Summary:

Total processable nodes: 46
Recognized nodes available for conversion: 11
Total nodes converted: 6
Total FP16 Cast ops used (excluding Const and Variable casts): 10
Allowlisted nodes converted: 5
Denylisted nodes blocking conversion: 0
Nodes blocked from conversion by denylisted nodes: 0

For more information regarding mixed precision training, including how to make automatic mixed precision aware of a custom op type, please see the documentation available here:
https://docs.nvidia.com/deeplearning/frameworks/tensorflow-user-guide/index.html#tfamp


2023-02-16 12:12:02.682088: W tensorflow/compiler/tf2tensorrt/segment/segment.cc:952] 

################################################################################
TensorRT unsupported/non-converted OP Report:
        - QuantizeAndDequantizeV4 -> 10x
        - Conv2D -> 5x
        - NoOp -> 2x
        - Identity -> 1x
        - Placeholder -> 1x
--------------------------------------------------------------------------------
        - Total nonconverted OPs: 19
        - Total nonconverted OP Types: 5
For more information see https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#supported-ops.
################################################################################

2023-02-16 12:12:02.682177: W tensorflow/compiler/tf2tensorrt/segment/segment.cc:1280] The environment variable TF_TRT_MAX_ALLOWED_ENGINES=20 has no effect since there are only 0 TRT Engines with  at least minimum_segment_size=3 nodes.
2023-02-16 12:12:02.682195: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:799] Number of TensorRT candidate segments: 0


2. when using `quantize_and_dequantize` at the `trt_func = converter.convert()` step
```shell
INFO:tensorflow:Clearing prior device assignments in loaded saved model
INFO:tensorflow:Automatic mixed precision will be used on the whole TensorFlow Graph. This behavior can be deactivated using the environment variable: TF_TRT_EXPERIMENTAL_FEATURES=deactivate_mixed_precision.
More information can be found on: https://www.tensorflow.org/guide/mixed_precision.
2023-02-16 12:14:27.966166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-02-16 12:14:27.966825: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1
2023-02-16 12:14:27.966960: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session
2023-02-16 12:14:27.967241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-02-16 12:14:27.967877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-02-16 12:14:27.968493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-02-16 12:14:27.969167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-02-16 12:14:27.969781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-02-16 12:14:27.970355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1637] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20554 MB memory:  -> device: 0, name: NVIDIA A10, pci bus id: 0000:04:00.0, compute capability: 8.6
2023-02-16 12:14:27.981918: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:2359] Running auto_mixed_precision graph optimizer
2023-02-16 12:14:27.988304: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1195] Automatic Mixed Precision Grappler Pass Summary:

Total processable nodes: 46
Recognized nodes available for conversion: 11
Total nodes converted: 6
Total FP16 Cast ops used (excluding Const and Variable casts): 10
Allowlisted nodes converted: 5
Denylisted nodes blocking conversion: 0
Nodes blocked from conversion by denylisted nodes: 0

For more information regarding mixed precision training, including how to make automatic mixed precision aware of a custom op type, please see the documentation available here:
https://docs.nvidia.com/deeplearning/frameworks/tensorflow-user-guide/index.html#tfamp


2023-02-16 12:14:27.993404: I tensorflow/compiler/tf2tensorrt/convert/trt_optimization_pass.cc:206] [TF-TRT] Using explicit QDQ mode
2023-02-16 12:14:27.994965: W tensorflow/compiler/tf2tensorrt/convert/ops/quantization_ops.cc:146] QuantizeAndDequantizeV2: StatefulPartitionedCall/sequential/custom_conv2d_1/custom_conv2d_1q1_w has narrow_range=true, but for TensorRT conversion, narrow_range=false is recommended.
2023-02-16 12:14:27.995142: W tensorflow/compiler/tf2tensorrt/convert/ops/quantization_ops.cc:146] QuantizeAndDequantizeV2: StatefulPartitionedCall/sequential/custom_conv2d_2/custom_conv2d_2q1_w has narrow_range=true, but for TensorRT conversion, narrow_range=false is recommended.
2023-02-16 12:14:27.995296: W tensorflow/compiler/tf2tensorrt/convert/ops/quantization_ops.cc:146] QuantizeAndDequantizeV2: StatefulPartitionedCall/sequential/custom_conv2d_3/custom_conv2d_3q1_w has narrow_range=true, but for TensorRT conversion, narrow_range=false is recommended.
2023-02-16 12:14:27.995449: W tensorflow/compiler/tf2tensorrt/convert/ops/quantization_ops.cc:146] QuantizeAndDequantizeV2: StatefulPartitionedCall/sequential/custom_conv2d_4/custom_conv2d_4q1_w has narrow_range=true, but for TensorRT conversion, narrow_range=false is recommended.
2023-02-16 12:14:27.995603: W tensorflow/compiler/tf2tensorrt/convert/ops/quantization_ops.cc:146] QuantizeAndDequantizeV2: StatefulPartitionedCall/sequential/custom_conv2d_5/custom_conv2d_5q1_w has narrow_range=true, but for TensorRT conversion, narrow_range=false is recommended.
2023-02-16 12:14:27.995626: W tensorflow/compiler/tf2tensorrt/convert/ops/quantization_ops.cc:146] QuantizeAndDequantizeV2: StatefulPartitionedCall/sequential/custom_conv2d_1/custom_conv2d_1_q1_i has narrow_range=true, but for TensorRT conversion, narrow_range=false is recommended.
2023-02-16 12:14:27.995681: W tensorflow/compiler/tf2tensorrt/convert/ops/quantization_ops.cc:146] QuantizeAndDequantizeV2: StatefulPartitionedCall/sequential/custom_conv2d_2/custom_conv2d_2_q1_i has narrow_range=true, but for TensorRT conversion, narrow_range=false is recommended.
2023-02-16 12:14:27.995710: W tensorflow/compiler/tf2tensorrt/convert/ops/quantization_ops.cc:146] QuantizeAndDequantizeV2: StatefulPartitionedCall/sequential/custom_conv2d_3/custom_conv2d_3_q1_i has narrow_range=true, but for TensorRT conversion, narrow_range=false is recommended.
2023-02-16 12:14:27.995737: W tensorflow/compiler/tf2tensorrt/convert/ops/quantization_ops.cc:146] QuantizeAndDequantizeV2: StatefulPartitionedCall/sequential/custom_conv2d_4/custom_conv2d_4_q1_i has narrow_range=true, but for TensorRT conversion, narrow_range=false is recommended.
2023-02-16 12:14:27.995764: W tensorflow/compiler/tf2tensorrt/convert/ops/quantization_ops.cc:146] QuantizeAndDequantizeV2: StatefulPartitionedCall/sequential/custom_conv2d_5/custom_conv2d_5_q1_i has narrow_range=true, but for TensorRT conversion, narrow_range=false is recommended.
2023-02-16 12:14:27.995803: W tensorflow/compiler/tf2tensorrt/segment/segment.cc:952] 

################################################################################
TensorRT unsupported/non-converted OP Report:
        - Conv2D -> 5x
        - NoOp -> 2x
        - Identity -> 1x
        - Placeholder -> 1x
--------------------------------------------------------------------------------
        - Total nonconverted OPs: 9
        - Total nonconverted OP Types: 4
For more information see https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#supported-ops.
################################################################################

2023-02-16 12:14:27.995933: W tensorflow/compiler/tf2tensorrt/segment/segment.cc:1280] The environment variable TF_TRT_MAX_ALLOWED_ENGINES=20 has no effect since there are only 5 TRT Engines with  at least minimum_segment_size=3 nodes.
2023-02-16 12:14:27.995954: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:799] Number of TensorRT candidate segments: 5
2023-02-16 12:14:27.997440: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:916] Replaced segment 0 consisting of 3 nodes by TRTEngineOp_000_000.
2023-02-16 12:14:27.997478: W tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:919] TF-TRT Warning: Cannot replace segment 1 consisting of 16 nodes by TRTEngineOp_000_001 reason: Segment has no inputs (possible constfold failure) (keeping original segment).
2023-02-16 12:14:27.997532: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:916] Replaced segment 2 consisting of 3 nodes by TRTEngineOp_000_002.
2023-02-16 12:14:27.997588: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:916] Replaced segment 3 consisting of 3 nodes by TRTEngineOp_000_003.
2023-02-16 12:14:27.997641: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:916] Replaced segment 4 consisting of 3 nodes by TRTEngineOp_000_004.
2023-02-16 12:14:28.000505: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1533] No allowlist ops found, nothing to do
2023-02-16 12:14:28.002105: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1533] No allowlist ops found, nothing to do
2023-02-16 12:14:28.003662: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1533] No allowlist ops found, nothing to do
2023-02-16 12:14:28.005172: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1533] No allowlist ops found, nothing to do
```

As can be seen, the `Conv2D` nodes are strangely left unconverted by TF-TRT.

3. The next step `converter.build(input_fn=input_fn)` throws more errors
```shell
2023-02-16 12:16:44.399623: I tensorflow/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8700

2023-02-16 12:16:44.493304: I tensorflow/compiler/tf2tensorrt/common/utils.cc:104] Linked TensorRT version: 8.5.1
2023-02-16 12:16:44.493380: I tensorflow/compiler/tf2tensorrt/common/utils.cc:106] Loaded TensorRT version: 8.5.1
2023-02-16 12:16:46.885047: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:83] TF-TRT Warning: DefaultLogger The NetworkDefinitionCreationFlag::kEXPLICIT_PRECISION flag has been deprecated and has no effect. Please do not use this flag when creating the network.
2023-02-16 12:16:46.886238: W tensorflow/compiler/tf2tensorrt/convert/ops/quantization_ops.cc:146] QuantizeAndDequantizeV2: StatefulPartitionedCall/sequential/custom_conv2d_2/custom_conv2d_2_q1_i has narrow_range=true, but for TensorRT conversion, narrow_range=false is recommended.
2023-02-16 12:16:46.985131: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:1103] TF-TRT Warning: Engine creation for TRTEngineOp_000_000 failed. The native segment will be used instead. Reason: INTERNAL: tensorflow/compiler/tf2tensorrt/convert/ops/quantization_ops.cc:217 TRT_ENSURE_OK failure:
  INTERNAL: ./tensorflow/compiler/tf2tensorrt/convert/ops/layer_utils.h:610 TRT_ENSURE failure
2023-02-16 12:16:46.985391: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:936] TF-TRT Warning: Engine retrieval for input shapes: [[2,112,112,64]] failed. Running native segment for TRTEngineOp_000_000
2023-02-16 12:16:49.308912: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:83] TF-TRT Warning: DefaultLogger The NetworkDefinitionCreationFlag::kEXPLICIT_PRECISION flag has been deprecated and has no effect. Please do not use this flag when creating the network.
2023-02-16 12:16:49.310048: W tensorflow/compiler/tf2tensorrt/convert/ops/quantization_ops.cc:146] QuantizeAndDequantizeV2: StatefulPartitionedCall/sequential/custom_conv2d_3/custom_conv2d_3_q1_i has narrow_range=true, but for TensorRT conversion, narrow_range=false is recommended.
2023-02-16 12:16:49.406425: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:1103] TF-TRT Warning: Engine creation for TRTEngineOp_000_002 failed. The native segment will be used instead. Reason: INTERNAL: tensorflow/compiler/tf2tensorrt/convert/ops/quantization_ops.cc:217 TRT_ENSURE_OK failure:
  INTERNAL: ./tensorflow/compiler/tf2tensorrt/convert/ops/layer_utils.h:610 TRT_ENSURE failure
2023-02-16 12:16:49.406586: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:936] TF-TRT Warning: Engine retrieval for input shapes: [[2,56,56,64]] failed. Running native segment for TRTEngineOp_000_002
2023-02-16 12:16:51.735398: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:83] TF-TRT Warning: DefaultLogger The NetworkDefinitionCreationFlag::kEXPLICIT_PRECISION flag has been deprecated and has no effect. Please do not use this flag when creating the network.
2023-02-16 12:16:51.736541: W tensorflow/compiler/tf2tensorrt/convert/ops/quantization_ops.cc:146] QuantizeAndDequantizeV2: StatefulPartitionedCall/sequential/custom_conv2d_4/custom_conv2d_4_q1_i has narrow_range=true, but for TensorRT conversion, narrow_range=false is recommended.
2023-02-16 12:16:51.835118: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:1103] TF-TRT Warning: Engine creation for TRTEngineOp_000_003 failed. The native segment will be used instead. Reason: INTERNAL: tensorflow/compiler/tf2tensorrt/convert/ops/quantization_ops.cc:217 TRT_ENSURE_OK failure:
  INTERNAL: ./tensorflow/compiler/tf2tensorrt/convert/ops/layer_utils.h:610 TRT_ENSURE failure
2023-02-16 12:16:51.835262: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:936] TF-TRT Warning: Engine retrieval for input shapes: [[2,28,28,64]] failed. Running native segment for TRTEngineOp_000_003
2023-02-16 12:16:54.067165: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:83] TF-TRT Warning: DefaultLogger The NetworkDefinitionCreationFlag::kEXPLICIT_PRECISION flag has been deprecated and has no effect. Please do not use this flag when creating the network.
2023-02-16 12:16:54.068481: W tensorflow/compiler/tf2tensorrt/convert/ops/quantization_ops.cc:146] QuantizeAndDequantizeV2: StatefulPartitionedCall/sequential/custom_conv2d_5/custom_conv2d_5_q1_i has narrow_range=true, but for TensorRT conversion, narrow_range=false is recommended.
2023-02-16 12:16:54.175458: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:1103] TF-TRT Warning: Engine creation for TRTEngineOp_000_004 failed. The native segment will be used instead. Reason: INTERNAL: tensorflow/compiler/tf2tensorrt/convert/ops/quantization_ops.cc:217 TRT_ENSURE_OK failure:
  INTERNAL: ./tensorflow/compiler/tf2tensorrt/convert/ops/layer_utils.h:610 TRT_ENSURE failure
2023-02-16 12:16:54.175760: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:936] TF-TRT Warning: Engine retrieval for input shapes: [[2,14,14,64]] failed. Running native segment for TRTEngineOp_000_004
2023-02-16 12:16:54.201923: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:936] TF-TRT Warning: Engine retrieval for input shapes: [[2,112,112,64]] failed. Running native segment for TRTEngineOp_000_000
```
```
</details>"
59710,"TF Distributed looks for libcublasLt.so.10, when installed CUDA version is 11.6","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.8, 2.11

### Custom Code

Yes

### OS Platform and Distribution

RHEL 7.9

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.6/8.4

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When using tf multi worker, the chief node looks for libcublasLt from CUDA 10.2, while the loaded CUDA version is 11.6.

The expected behaviour is that TensorFlow should be using libcublasLt from the currently available CUDA version rather than 10.2

The worker node exits with a communication error - Connection reset by peer
```


### Standalone code to reproduce the issue

```shell
https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#train_the_model
```


### Relevant log output

```shell
2023-02-16 09:26:37.626920: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-16 09:26:38.332105: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11641 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:3b:00.0, compute capability: 7.0
2023-02-16 09:26:38.334955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:worker/replica:0/task:0/device:GPU:0 with 11641 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:3b:00.0, compute capability: 7.0
2023-02-16 09:26:38.341938: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> 10.149.255.254:12345, 1 -> 10.149.0.4:23456}
2023-02-16 09:26:38.342153: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:437] Started server with target: grpc://10.149.255.254:12345
2023-02-16 09:26:44.265969: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: ""TensorSliceDataset/_2""
op: ""TensorSliceDataset""
input: ""Placeholder/_0""
input: ""Placeholder/_1""
attr {
  key: ""Toutput_types""
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: ""_cardinality""
  value {
    i: 60000
  }
}
attr {
  key: ""is_files""
  value {
    b: false
  }
}
attr {
  key: ""metadata""
  value {
    s: ""\n\024TensorSliceDataset:0""
  }
}
attr {
  key: ""output_shapes""
  value {
    list {
      shape {
        dim {
          size: 28
        }
        dim {
          size: 28
        }
      }
      shape {
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

Epoch 1/3
Could not load library libcublasLt.so.10. Error: libcublasLt.so.10: cannot open shared object file: No such file or directory
```
</details>"
59709,"Node: 'sequential/embedding/embedding_lookup' indices[0,57] = 1544453 is not in [0, 1498136) 	 [[{{node sequential/embedding/embedding_lookup}}]] [Op:__inference_train_function_8141]","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tensorflow.compat.v2.version

### Custom Code

Yes

### OS Platform and Distribution

google colab

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
InvalidArgumentError: Graph execution error:

Detected at node 'sequential/embedding/embedding_lookup' defined at (most recent call last):
    File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code
      exec(code, run_globals)
    File ""/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py"", line 16, in <module>
      app.launch_new_instance()
    File ""/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py"", line 992, in launch_instance
      app.start()
    File ""/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py"", line 612, in start
      self.io_loop.start()
    File ""/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py"", line 149, in start
      self.asyncio_loop.run_forever()
    File ""/usr/lib/python3.8/asyncio/base_events.py"", line 570, in run_forever
      self._run_once()
    File ""/usr/lib/python3.8/asyncio/base_events.py"", line 1859, in _run_once
      handle._run()
    File ""/usr/lib/python3.8/asyncio/events.py"", line 81, in _run
      self._context.run(self._callback, *self._args)
    File ""/usr/local/lib/python3.8/dist-packages/tornado/ioloop.py"", line 690, in <lambda>
      lambda f: self._run_callback(functools.partial(callback, future))
    File ""/usr/local/lib/python3.8/dist-packages/tornado/ioloop.py"", line 743, in _run_callback
      ret = callback()
    File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 787, in inner
      self.run()
    File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 748, in run
      yielded = self.gen.send(value)
    File ""/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py"", line 365, in process_one
      yield gen.maybe_future(dispatch(*args))
    File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 209, in wrapper
      yielded = next(result)
    File ""/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py"", line 268, in dispatch_shell
      yield gen.maybe_future(handler(stream, idents, msg))
    File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 209, in wrapper
      yielded = next(result)
    File ""/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py"", line 543, in execute_request
      self.do_execute(
    File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 209, in wrapper
      yielded = next(result)
    File ""/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py"", line 306, in do_execute
      res = shell.run_cell(code, store_history=store_history, silent=silent)
    File ""/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py"", line 536, in run_cell
      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py"", line 2854, in run_cell
      result = self._run_cell(
    File ""/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py"", line 2881, in _run_cell
      return runner(coro)
    File ""/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py"", line 68, in _pseudo_sync_runner
      coro.send(None)
    File ""/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py"", line 3057, in run_cell_async
      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
    File ""/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py"", line 3249, in run_ast_nodes
      if (await self.run_code(code, result,  async_=asy)):
    File ""/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py"", line 3326, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File ""<ipython-input-32-2dfe1a9e3af4>"", line 1, in <module>
      model.fit(padded_sequences_x, padded_sequences_y, batch_size=64, epochs=15)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1650, in fit
      tmp_logs = self.train_function(iterator)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1249, in train_function
      return step_function(self, iterator)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1233, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1222, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1023, in train_step
      y_pred = self(x, training=True)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 561, in __call__
      return super().__call__(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1132, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/sequential.py"", line 413, in call
      return super().call(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 511, in call
      return self._run_internal_graph(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 668, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1132, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/core/embedding.py"", line 208, in call
      out = tf.nn.embedding_lookup(self.embeddings, inputs)
Node: 'sequential/embedding/embedding_lookup'
indices[0,57] = 1544453 is not in [0, 1498136)
	 [[{{node sequential/embedding/embedding_lookup}}]] [Op:__inference_train_function_8141]

```


### Standalone code to reproduce the issue

```shell
link to code and dataset:
colab file: https://colab.research.google.com/drive/13JZX5HQQGJrf6t9CcCA0kM3JHegBtnTz?usp=sharing   dataset: 
 https://drive.google.com/file/d/1pKAJ-PF42nRB2kw0ZdMAkg1i2fUSvF-y/view?usp=sharing



i expect the code to fit in the model:
model:
# Define  model
model = Sequential()
model.add(Embedding(len(word_index)+1 ,
                    embedding_weights.shape[1],
                    weights=[embedding_weights],
                    input_length=padded_sequences_x.shape[1],
                    trainable=False))
model.add(Bidirectional(LSTM(32, return_sequences=True)))
model.add(attention(return_sequences=False)) # receive 3D and output 3D
model.add(RepeatVector(len(padded_sequences_y[1]))) # repeat vector
model.add(LSTM(32, return_sequences=True))  #decoder layer
model.add(Dropout(0.2))
model.add(TimeDistributed(Dense(1, activation='softmax')))
#model.add(Dense(1, activation='softmax'))


summery:
Model: ""sequential""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding (Embedding)       (None, 79, 100)           149813600 
                                                                 
 bidirectional (Bidirectiona  (None, 79, 64)           34048     
 l)                                                              
                                                                 
 attention (attention)       (None, 64)                143       
                                                                 
 repeat_vector (RepeatVector  (None, 38, 64)           0         
 )                                                               
                                                                 
 lstm_1 (LSTM)               (None, 38, 32)            12416     
                                                                 
 dropout (Dropout)           (None, 38, 32)            0         
                                                                 
 time_distributed (TimeDistr  (None, 38, 1)            33        
 ibuted)
```


### Relevant log output

_No response_</details>"
59707,tflite_runtime wheel file import error in beaglebone black wireless,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf

### Custom Code

No

### OS Platform and Distribution

Debian GNU/Linux 10 (buster)

### Mobile device

_No response_

### Python version

3.7.3

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
i want to install tflite_runtime on beaglebone black wireless, and i build the wheel file follow the https://www.tensorflow.org/lite/guide/build_cmake_pip and i change the -mfpu=neon-vpfv4 to -mfpu=neon becauuse the beaglebone black wireless is not support the vpfv4, after build the wheel file and install in beaglebone black,
in python3 code: ""from tflite_runtime.interpreter import Interpreter"", it output 
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/debian/.local/lib/python3.7/site-packages/tflite_runtime/interpreter.py"", line 33, in <module>
    from tflite_runtime import _pywrap_tensorflow_interpreter_wrapper as _interpreter_wrapper
ImportError: /home/debian/.local/lib/python3.7/site-packages/tflite_runtime/_pywrap_tensorflow_interpreter_wrapper.so: undefined symbol: _ZN6tflite9telemetry20TelemetryReportEventEP13TfLiteContextPKc12TfLiteStatus
>>>
```


### Standalone code to reproduce the issue

```shell
from tflite_runtime.interpreter import Interpreter
```


### Relevant log output

_No response_</details>"
59705,tf.experimental.numpy.power behaves differently when inputs are large numbers,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.13.0.dev20230204

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04.4 LTS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tf.experimental.numpy.power behaves differently when inputs are large numbers
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

with tf.device(""cpu""):
    x_t = tf.experimental.numpy.power(2, 64)
    print(x_t)

with tf.device(""gpu""):
    x_t = tf.experimental.numpy.power(2, 64)
    print(x_t)
```


### Relevant log output

```shell
tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor(1, shape=(), dtype=int64)
```
</details>"
59704,tf.image.convert_image_dtype behaves differently between CPU and GPU when inputs are negative numbers ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.13.0.dev20230204

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04.4 LTS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tf.image.convert_image_dtype behaves differently between CPU and GPU when inputs are negative numbers. CPU produces weird outputs but doesn't raise warnings or exceptions.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

with tf.device(""gpu""):

    x = tf.ones((10,10), dtype=tf.float32)*(-0.3)
    y = tf.image.convert_image_dtype(x, dtype=tf.uint8)
    print(y)

with tf.device(""cpu""):

    x = tf.ones((10,10), dtype=tf.float32)*(-0.3)
    y = tf.image.convert_image_dtype(x, dtype=tf.uint8)
    print(y)
```


### Relevant log output

```shell
tf.Tensor(
[[0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]], shape=(10, 10), dtype=uint8)
tf.Tensor(
[[180 180 180 180 180 180 180 180 180 180]
 [180 180 180 180 180 180 180 180 180 180]
 [180 180 180 180 180 180 180 180 180 180]
 [180 180 180 180 180 180 180 180 180 180]
 [180 180 180 180 180 180 180 180 180 180]
 [180 180 180 180 180 180 180 180 180 180]
 [180 180 180 180 180 180 180 180 180 180]
 [180 180 180 180 180 180 180 180 180 180]
 [180 180 180 180 180 180 180 180 180 180]
 [180 180 180 180 180 180 180 180 180 180]], shape=(10, 10), dtype=uint8)
```
</details>"
59702,tf.cast behaves differently in CPU and GPU when casting negative values,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.13.0.dev20230204

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04.4 LTS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tf.cast behaves differently in CPU and GPU when casting negative values
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

with tf.device('cpu'):
    data = np.array([[-1.0]]).astype(np.float64)
    x = tf.dtypes.cast(data, tf.uint8)
    print(x)
    
with tf.device('gpu'):
    data = np.array([[-1.0]]).astype(np.float64)
    x = tf.dtypes.cast(data, tf.uint8)
    print(x)
```


### Relevant log output

```shell
tf.Tensor([[255]], shape=(1, 1), dtype=uint8)
tf.Tensor([[0]], shape=(1, 1), dtype=uint8)
```
</details>"
59701,Video CNN Tutorial,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.11.0

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I do not know if this belongs here. 

I was trying to work through your ""Video classification with a 3D convolutional neural network"" tutorial. Going through the ""Load Video Data"" portion. I keep coming across an error that I cannot resolve.

"" line 102, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)

ValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.
""
this is the code step:
ucf_sample_video = frames_from_video_file(next(subset_paths['train'].glob('*/*.avi')), 50)

The isolated video_path argument in the tutorial works, it just seems to be this assignment and I am not sure how to fix it.

I have tried to work through with little success and end up getting the same error farther down in the tutorial with:

for frames, labels in train_ds.take(10):
  print(labels)

I apologize if this is not where I should be posting stuff about the tutorial.
```


### Standalone code to reproduce the issue

```shell
https://www.tensorflow.org/tutorials/load_data/video

I have copied and pasted everything that you see in your tutorial.
```


### Relevant log output

_No response_</details>"
59699,Test Issue,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
Sample issue
```


### Relevant log output

_No response_</details>"
59698,LSTM prediction() - too slow,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### Specs
Memory: 32GB
Ubuntu 22.04 LTS
Processor: 12th Gen Intel-Core i9-12900KSx24
OS-type; 64-bit



### Current Behaviour?

```
I have trained an LSTM Autoencoder (see code for the structure). 
With the trained model I attempt to make predictions. For that, I have re-formated the input data to fit the input shape needed for LSTM. 
The issue is, that the speed of prediction is too slow. I am using the model to predict anomalies in sensor data in real time. My system runs in 125HZ -> 0.008 sec, so predictions of 0.03 slow down my system, which results in inability to execute the tasks properly.

I have previously seen a similar issue discussed: https://github.com/tensorflow/tensorflow/issues/40261, I was unable to draw conclusions that can help in my case.

Does anyone have any clue how can I speed up the prediction time? - any help would be much appreciated

Using `model.predict(test_seq)`: avg: 0.03 sec
Using `model(test_seq)`: abg: 0.03 sec
```


### Standalone code to reproduce the issue

```shell
def to_sequence(data, timesteps=1):
    n_features=data.shape[2]
    seq = []
    for i in range(len(data)-timesteps):
        # takes a window of data of specified timesteps
        temp = data[i:(i+timesteps)]
        temp = temp.reshape(timesteps, n_features)
        seq.append(temp)
        
    return np.array(seq)


def LSTM_autoencoder(data):
    
    
    n_timesteps = data.shape[1]
    n_features = data.shape[2]
    
    keras.backend.clear_session()
    
    
    model = keras.Sequential()
    model.add(keras.layers.Input(shape=(n_timesteps, n_features)))
    model.add(keras.layers.Conv1D(filters=32, kernel_size=15, padding='same', 
                            data_format='channels_last',dilation_rate=1, activation=""linear""))
    model.add(keras.layers.LSTM(units=50, activation='relu', name='LSTM_1', return_sequences=False))
    model.add(keras.layers.Dropout(0.2))
    # to connect encoder with decoder RepeatVector repeats the provided 2D input multiple times to create 3D output
    model.add(keras.layers.RepeatVector(n=n_timesteps))
    # decoder expects the 3D input
    model.add(keras.layers.LSTM(units=50, activation='relu', name='LSTM_2', return_sequences=True))
    model.add(keras.layers.Conv1D(filters=32, kernel_size=15, padding='same', 
                            data_format='channels_last',dilation_rate=1, activation=""linear""))
    model.add(keras.layers.Dropout(0.2))
    # allows the same output layer to be reused for each element in sequence
    model.add(keras.layers.TimeDistributed(keras.layers.Dense(units=n_features)))

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=""mse"")
    
    return model

model = keras.models.load_model('Lstm3')

test=np.ones(255).reshape(51,5) # (51,5)
test_expanded = np.expand_dims(test,axis=1) # (51,1,5)
test_seq = to_sequence(test_expanded , 50) # (1,50,5)

t=0
while t<100:
  t_start = time.time()
  model.predict(test_seq)
  print(time.time()-t_start)
  t+=1
```


### Relevant log output

_No response_</details>"
59696,Model performs differently on identical training and validation sets,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Training a `tf.keras.models.Model` on a single, constant `(X, y)` does not behave as expected. The training loss converges to 0 as the model appears to learn the single training data element, but the validation loss remains high, despite the training and validation data being identical.

I am using a ResNet-18 model architecture (see https://arxiv.org/pdf/1711.09577.pdf or the attached gist) which uses several stacked convolutional layers to form a deep and robust model.


### Standalone code to reproduce the issue

```python
import numpy as np
import tensorflow as tf

# Build model
model = build_my_model() # see attached gist below for specific model build

# Generate training/validation data
input_data = np.random.random((batch_size,*imshape))
dummy_target = np.random.random((batch_size,3))

# Convert to tf.data.Dataset
ds1 = tf.data.Dataset.from_tensors(input_data)
ds2 = tf.data.Dataset.from_tensors(dummy_target)
dataset = tf.data.Dataset.zip((ds1,ds2)).repeat()
for img,target in dataset.take(1):
    print(img.shape, target)

# Fit model -- note training loss goes to 0, while validation loss does not
model.fit(
    dataset,
    epochs=10,
    steps_per_epoch=30,
    validation_data = dataset,
    validation_steps=3,
)

# Predict on the input data to show that the loss is still much larger than it should be
ypred = model.predict(input_data, verbose=0)
print(dummy_target, ypred, model.loss(ypred, dummy_target))
```

Reproducible gist in Google Colab
```shell
https://colab.research.google.com/gist/taylora-mitre/9ccdc0275f8d2119e33e311092c5334e/train-and-validation.ipynb
```


### Relevant log output

```
Epoch 1/10
30/30 [==============================] - 7s 224ms/step - loss: 5.9760e-05 - val_loss: 5.9585
Epoch 2/10
30/30 [==============================] - 7s 225ms/step - loss: 3.0297e-06 - val_loss: 2.6298
Epoch 3/10
30/30 [==============================] - 7s 228ms/step - loss: 1.2876e-07 - val_loss: 1.4914
Epoch 4/10
30/30 [==============================] - 7s 223ms/step - loss: 5.6785e-09 - val_loss: 1.0507
Epoch 5/10
30/30 [==============================] - 7s 229ms/step - loss: 2.6075e-10 - val_loss: 0.8550
Epoch 6/10
30/30 [==============================] - 7s 220ms/step - loss: 7.5849e-12 - val_loss: 0.7406
Epoch 7/10
30/30 [==============================] - 7s 218ms/step - loss: 5.1980e-13 - val_loss: 0.6433
Epoch 8/10
30/30 [==============================] - 7s 222ms/step - loss: 1.7708e-13 - val_loss: 0.5429
Epoch 9/10
30/30 [==============================] - 7s 221ms/step - loss: 3.3924e-13 - val_loss: 0.4408
Epoch 10/10
30/30 [==============================] - 7s 221ms/step - loss: 3.0570e-13 - val_loss: 0.3440
<keras.callbacks.History at 0x7f6960343eb0>
```

</details>"
59694,"WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04.1 LTS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The warning appears:


WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.
```

Can you please, explain, what is meant by this warning and how to fix it? I have the following accompanying questions to it:

* Function traces are basically graph representations of functions or something else?

* If these jit traces are not found in a loaded model, will they be generated again? Is model compilation needed for regeneration?

* Are these functions needed only for training or also for prediction/evaluation?

I have found in Keras docs (https://keras.io/api/models/model_saving_apis/) that I can disable saving the traces with the following note:
""Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a get_config() method.""

* Why would the `get_config()` method be only needed when I disable saving the traces?

Thanks in advance for any explanations given!

```shell
### Reproducer

It's not my code but the warnings appear e.g. here: https://jovian.com/blog/tutorials/%20parameter-logging-wandb-ai
```

```
### Relevant log output

WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.
```
</details>"
59693,Update curl to 7.88.0,Vulnerabilities are reported in curl 7.87. Can someone please update curl to 7.88 ?
59692,TFLite undefined symbol when cross-compiling for android on linux ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12

### Custom Code

Yes

### OS Platform and Distribution

Debian 11 

### Mobile device

Android NDK 25

### Python version

3.9.2

### Bazel version

Tried on 7.0.0 and 6.0.0

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


I'm trying to cross compile TFLite on my linux machine to be compatible with android machines so I can use it on my c++ code that runs on android devices.

I've tried 3 different bazel builds with 3 different issues.

Also tried the 'build with cmake for arm' section with errors.

More details can be found here:

https://stackoverflow.com/questions/75457043/build-tflite-c-lib-for-android-with-ndk-25-with-bazel

https://stackoverflow.com/questions/75435665/errno-8-exec-format-error-on-tflite-cmake-build-for-arm64-v8a
 
How can I fix this or are there any pre-compiled so files?


### Standalone code to reproduce the issue

First I ran `./configure` and skipped over the `configure ./WORKSPACE` part since I'm using ndk version 25 and can't give the path.

First command tried: 
`bazel build -c opt --config=elinux_aarch64 //tensorflow/lite:libtensorflowlite.so --fat_apk_cpu=arm64-v8a`

Second command:
`bazel build -c opt --config=android_arm64 //tensorflow/lite:libtensorflowlite.so`

Third command (not right but I've tried it anyways):
`bazel build -c opt --config=elinux_aarch64 //tensorflow/lite:libtensorflowlite.so`

I've also tried running the `cross compilation with cmake for arm` by doing:
 - Clone to tensorflow_src
 - Create tensorflow_build in the same place as src
 - Run `curl -LO https://storage.googleapis.com/mirror.tensorflow.org/developer.arm.com/media/Files/downloads/gnu-a/8.3-2019.03/binrel/gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu.tar.xz`
 - Run `mkdir -p ${HOME}/toolchains`
 - Run `tar xvf gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu.tar.xz -C ${HOME}/toolchains`
 - go to tensorflow_build dir
 - Run `ARMCC_PREFIX=${HOME}/toolchains/gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu/bin/aarch64-linux-gnu-` && `ARMCC_FLAGS=""-funsafe-math-optimizations""`
 - And finally `cmake -DCMAKE_C_COMPILER=${ARMCC_PREFIX}gcc   -DCMAKE_CXX_COMPILER=${ARMCC_PREFIX}g++   -DCMAKE_C_FLAGS=""${ARMCC_FLAGS}""   -DCMAKE_CXX_FLAGS=""${ARMCC_FLAGS}""   -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON   -DCMAKE_SYSTEM_NAME=Linux   -DCMAKE_SYSTEM_PROCESSOR=aarch64   ../tensorflow_src/tensorflow/lite/`
 My cmake looks like this:
```cmake
set(TENSORFLOW_SOURCE_DIR """" CACHE PATH
        ""~/Desktop/tensorflow/tensorflow_build"" )
if(NOT TENSORFLOW_SOURCE_DIR)
        get_filename_component(TENSORFLOW_SOURCE_DIR
                ""~/Desktop/tensorflow/tensorflow_src"" ABSOLUTE)
endif()

add_subdirectory(
        ""${TENSORFLOW_SOURCE_DIR}/tensorflow/lite""
        ""${CMAKE_CURRENT_BINARY_DIR}/tensorflow-lite"" EXCLUDE_FROM_ALL)
...
target_link_directories(
...
tensorflow-lite
)
```


### Relevant log output
Output of the first command:
I get the .so, get the relevant header files from r1.11 branch
`C/C++: ld: error: undefined symbol: tflite::impl::Interpreter::Invoke()`

Second one I get:
When I try to compile with bazel I get this:

```shell
...
INFO: Found applicable config definition build:android_arm64 in file /home/tb/Desktop/tensorflow/tensorflow_src/.bazelrc: --config=android --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a
INFO: Found applicable config definition build:android in file /home/tb/Desktop/tensorflow/tensorflow_src/.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --noenable_platform_specific_config --copt=-w --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --define=with_xla_support=false
INFO: Build options --copt, --cpu, --crosstool_top, and 2 more have changed, discarding analysis cache.
ERROR: /home/tb/.cache/bazel/_bazel_tb/f8bd8a53c316ccb93548129bbd0ade7a/external/local_config_cc/BUILD:47:19: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'arm64-v8a'
ERROR: /home/tb/.cache/bazel/_bazel_tb/f8bd8a53c316ccb93548129bbd0ade7a/external/local_config_cc/BUILD:47:19: Analysis of target '@local_config_cc//:toolchain' failed
ERROR: Analysis of target '//tensorflow/lite:libtensorflowlite.so' failed; build aborted: 
INFO: Elapsed time: 0.238s
INFO: 0 processes.
ERROR: Build did NOT complete successfully
```

Third one, as expected:
`C/C++: ld: error: /home/tb/Desktop/proj/cmake/android/../../third-party/libs/tflite/arm64-v8a/libtensorflowlite.so is incompatible with aarch64linux`

The result I got when I run with the cmake attempt:
When compiling:
```
In file included from /home/tb/Desktop/tensorflow/tensorflow_src/tensorflow/lite/python/interpreter_wrapper/python_utils.cc:16:
/home/tb/Desktop/tensorflow/tensorflow_src/tensorflow/lite/python/interpreter_wrapper/python_utils.h:19:10: fatal error: 'Python.h' file not found
```

When I give the path for python manually cmake it gives a bunch of more errors related to python, it's a rabbit hole.

Note: My project REQUIRES  ndk versions above 25 so I have to find a way to build it like that and it's the reason why I can't say yes to `configure ./WORKSPACE` when running `./configure
`"
59685,experimental_convert_saved_model_to_mlir strips public functions with expected names from exported MLIR,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

v1.12.1-89390-gf1ef1162316 2.13.0-dev20230214

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Using `experimental_convert_saved_model_to_mlir` to generate MLIR for a model strips nice names (e.g. ""train"") and leaves behind funky ones (e.g. ""__inference_train_250"").

See https://github.com/iree-org/iree/issues/10849#issuecomment-1430031039 for more background.

### Standalone code to reproduce the issue

```python
import tensorflow as tf
from tensorflow.python import pywrap_mlir
from pathlib import Path


class MyModel(tf.Module):
  def __init__(self, **kwargs):
    super().__init__(**kwargs)
    self.w = tf.Variable(5.0, name='weight')
    self.b = tf.Variable(0.0, name='bias')

  @tf.function
  def train(self, x):
    self.w.assign_add(x)
    self.b.assign_add(x)
    return self.w


m = MyModel()
m.train(tf.constant(3.0))
m.train(tf.constant(4.0))
tf.saved_model.save(m, '/tmp/simple-model')


def convert_to_hlo(model_path: str):
  result = pywrap_mlir.experimental_convert_saved_model_to_mlir(
      model_path, """", show_debug_info=False)
  pipeline = [""tf-lower-to-mlprogram-and-hlo""]
  result = pywrap_mlir.experimental_run_pass_pipeline(
      result, "","".join(pipeline), show_debug_info=False)
  return result

Path(""/tmp/simple-model.mlir"").write_text(
  convert_to_hlo(""/tmp/simple-model""))
```

This generates the following MLIR:

```mlir
module {
  ml_program.global public mutable @vars.__sm_node1__w(dense<1.200000e+01> : tensor<f32>) : tensor<f32>
  ml_program.global public mutable @vars.__sm_node2__b(dense<7.000000e+00> : tensor<f32>) : tensor<f32>
  func.func @__inference_train_250(%arg0: tensor<f32>) -> tensor<f32> {
    %0 = ml_program.global_load @vars.__sm_node1__w : tensor<f32>
    %1 = stablehlo.add %0, %arg0 : tensor<f32>
    ml_program.global_store @vars.__sm_node1__w = %1 : tensor<f32>
    %2 = ml_program.global_load @vars.__sm_node1__w : tensor<f32>
    %3 = ml_program.global_load @vars.__sm_node2__b : tensor<f32>
    %4 = stablehlo.add %3, %arg0 : tensor<f32>
    ml_program.global_store @vars.__sm_node2__b = %4 : tensor<f32>
    return %2 : tensor<f32>
  }
}
```

Note the name `__inference_train_250` rather than the expected `train`.


### Relevant log output

_No response_</details>"
