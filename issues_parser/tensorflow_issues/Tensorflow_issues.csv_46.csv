Issue Number,Issue Title,Issue Body
24725,tensorflow cannot launch operator: NormalizeWithDimensions,"2019-01-06 03:31:37.602215: E tensorflow/stream_executor/cuda/cuda_dnn.cc:4063] failed to run cudnnLRNCrossChannelForward
Traceback (most recent call last):
  File ""main_segnet.py"", line 124, in <module>
    main()
  File ""main_segnet.py"", line 119, in main
    segnet_solver.train()
  File ""/home/jingwei/RA_XJTLU__10_12_2018_to_2_28_2019/KTF/scripts/Segmentation_Nets/Seg_net/solver_segnet.py"", line 188, in train
    _, loss, cur_lr = sess.run([self.train_op, self.net.Loss, self.lr_node], feed_dict=feed_dict)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
    run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: NormalizeWithDimensions launch failed
	 [[Node: norm1 = LRN[T=DT_FLOAT, alpha=0.0001, beta=0.75, bias=1, depth_radius=5, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](_arg_x_0_3/_87)]]

Caused by op u'norm1', defined at:
  File ""main_segnet.py"", line 124, in <module>
    main()
  File ""main_segnet.py"", line 91, in main
    is_bw_loss=False)
  File ""/home/jingwei/RA_XJTLU__10_12_2018_to_2_28_2019/KTF/scripts/Segmentation_Nets/Seg_net/model_segnet.py"", line 56, in __init__
    phase_train=self.phase_train)
  File ""/home/jingwei/RA_XJTLU__10_12_2018_to_2_28_2019/KTF/scripts/Segmentation_Nets/Seg_net/layers_segnet.py"", line 207, in inference
    norm1 = tf.nn.lrn(images, depth_radius=5, bias=1.0, alpha=0.0001, beta=0.75, name='norm1')
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py"", line 4380, in lrn
    beta=beta, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3414, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InternalError (see above for traceback): NormalizeWithDimensions launch failed
	 [[Node: norm1 = LRN[T=DT_FLOAT, alpha=0.0001, beta=0.75, bias=1, depth_radius=5, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](_arg_x_0_3/_87)]]


This bug happens in GTX2080ti only. Tensorflow works fine in GTX1080ti.
I try to upgrade the version of tensorflow from 1.5.0 to 1.9.0 but it does not help.
My operating system is Ubuntu 18.04."
24722,Error building master branch from source,"**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: master branch
- Python version: 3.6
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 18 and 19
- GCC/Compiler version (if compiling from source): 5.4
- CUDA/cuDNN version: 9.0/7.2 and 10.0/7.4
- GPU model and memory: GT 755M



**Describe the problem**

The problem occurred for both gpu and cpu build. I was able to replicate the error on two different machines with set up listed above (i.e. one with cuda 9.0 and cuDNN 7.2 and another with cuda 10.0 and cuDNN 7.4). On both machines, I was able to build tensorflow 1.12 without any issue.

The exact error message is follows:

```bash
ERROR: missing input file '@pasta//:LICENSE'
ERROR: /home/shiki/tensorflow/tensorflow/tools/pip_package/BUILD:235:1: //tensorflow/tools/pip_package:build_pip_package: missing input file '@pasta//:LICENSE'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /home/shiki/tensorflow/tensorflow/tools/pip_package/BUILD:235:1 1 input file(s) do not exist
```

It seems that the issue is related to dependency with respect to `pasta` which unfortunately I am not familiar with. "
24721,Problems with building tensorflow,"Hello,

I'm having problems building tensorflow from source using bazel. I am currently using the guide here: (https://medium.com/@amsokol.com/update-2-how-to-build-and-install-tensorflow-gpu-cpu-for-windows-from-source-code-using-bazel-61c26553f7e8)

I have verified that everything is installed correctly, and keep on running into problems when it comes time to build. I have tried multiple versions of bazel, making sure to clean between builds.

**System information**
OS =  windows 7
CPU = i7-4790k
GPU = RTX2070, GTX970

I have decided to stick with bazel 0.19.0 as this is the only version that actually progresses. Others seems to get hung up in one place or another.

bazel info release results in: 
WARNING: The following rc files are no longer being read, please transfer their
contents or import their path into one of the standard rc files:
c:\users\scorpion\development\tensorflow-build\tensorflow/.bazelrc
c:\users\scorpion\development\tensorflow-build\tensorflow/tools/bazel.rc
WARNING: Option 'experimental_shortened_obj_file_path' is deprecated
release 0.19.0

--Though I have been able to find individuals reporting similar issues, I have not been able to find a solution to the problem.

When it comes time to build, here is the result:

(tensorflow-v1.12) C:\Users\SCORPION\Development\tensorflow-build\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
WARNING: The following rc files are no longer being read, please transfer their
contents or import their path into one of the standard rc files:
c:\users\scorpion\development\tensorflow-build\tensorflow/.bazelrc
c:\users\scorpion\development\tensorflow-build\tensorflow/tools/bazel.rc
Starting local Bazel server and connecting to it...
WARNING: Option 'experimental_shortened_obj_file_path' is deprecated
Loading:
Loading: 0 packages loaded
Loading: 0 packages loaded
    currently loading: tensorflow/tools/pip_package
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (1 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (2 packages loaded, 0 targets configured)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (5 packages loaded, 0 targets configured)
ERROR: While resolving toolchains for target //tensorflow/tools/pip_package:simple_console_for_windows: invalid registered toolchain '@local_config_sh//:local_sh_toolchain': no such package '@local_config_sh//': type 'path' has no method replace(string, string)
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: invalid registered toolchain '@local_config_sh//:local_sh_toolchain': no such package '@local_config_sh//': type 'path' has no method replace(string, string)
INFO: Elapsed time: 12.142s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (10 packages loaded, 29 targets configured)
FAILED: Build did NOT complete successfully (10 packages loaded, 29 targets configured)


Any help would be greatly appreciated

Edit:
After playing around with everything, changing bazel to 0.20.0, and reconfiguring, I am now receiving this error:

C:\tensorflow>bazel build --config=opt --cpu=x64_windows --compiler=msvc-cl --incompatible_remove_native_http_archive=false --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package

WARNING: The following rc files are no longer being read, please transfer their
contents or import their path into one of the standard rc files:
c:\tensorflow/.bazelrc
c:\tensorflow/tools/bazel.rc
WARNING: The following configs were expanded more than once: [cuda]. For repeata
ble flags, repeats are counted twice and may lead to unexpected behavior.
WARNING: Option 'experimental_shortened_obj_file_path' is deprecated
INFO: Invocation ID: 3fb463b2-516b-4fc8-a22f-f30ba60ff8f3
Loading:
Loading: 0 packages loaded
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (0 packages l
oaded, 0 targets configured)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (2 packages l
oaded, 0 targets configured)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (55 packages
loaded, 62 targets configured)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (119 packages
 loaded, 2958 targets configured)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (190 packages
 loaded, 3607 targets configured)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (224 packages
 loaded, 4739 targets configured)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (232 packages
 loaded, 4776 targets configured)
ERROR: C:/tensorflow/third_party/eigen3/BUILD:34:1: no such package '@eigen_arch
ive//': Traceback (most recent call last):
        File ""C:/tensorflow/third_party/repo.bzl"", line 106
                _apply_patch(ctx, ctx.attr.patch_file)
        File ""C:/tensorflow/third_party/repo.bzl"", line 69, in _apply_patch
                _wrap_bash_cmd(ctx, [""patch"", ""-p1"", ""-d"", ctx.pa...)])
        File ""C:/tensorflow/third_party/repo.bzl"", line 28, in _wrap_bash_cmd
                fail(""BAZEL_SH environment variable i..."")
BAZEL_SH environment variable is not set and referenced by '//third_party/eigen3
:eigen3'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' fai
led; build aborted: no such package '@eigen_archive//': Traceback (most recent c
all last):
        File ""C:/tensorflow/third_party/repo.bzl"", line 106
                _apply_patch(ctx, ctx.attr.patch_file)
        File ""C:/tensorflow/third_party/repo.bzl"", line 69, in _apply_patch
                _wrap_bash_cmd(ctx, [""patch"", ""-p1"", ""-d"", ctx.pa...)])
        File ""C:/tensorflow/third_party/repo.bzl"", line 28, in _wrap_bash_cmd
                fail(""BAZEL_SH environment variable i..."")
BAZEL_SH environment variable is not set
INFO: Elapsed time: 8.092s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (233 packages loaded, 4776 targets c
onfigured)
FAILED: Build did NOT complete successfully (233 packages loaded, 4776 targets c
onfigured)

All of the bazel environment variables are definitely set though...

Edit 2:
OK! So I finally got it to build correctly (I think). It seems like I got every error that this thing could throw at me, but it was the weekend and I had time to actually work through the logs and errors.

One of the errors it kept snagging on was swig not being present in the /_bazel_scorpion/ folder even though swig was set in environment variables. I decided to copy the swig files over that I had to that folder and that seemed to fix that problem.

Adding the patches, mentioned in the post at the beginning of this comment, helped bypass issues with bazel.rc

I also had to change a const value in a header file to progress past another problem (I apologize, but I have totally forgotten the error thrown and the specific header file. If I find the issue again I will reference it here).

Another was it kept throwing this 'x64_windows' error. Fixed with the '--cpu=x64_windows --compiler=msvc-cl' build option.

Another issue was it kept having problems downloading resources, or at least that's what I was able to gather from the errors it was throwing. Using pip install h5py fixed this issue, and pip install enum34 fixed some other issue I can't remember right now.

I don't remember exactly what this other error was, but I fixed that by having to uninstall .NET 4.7, installing .NET 4, installing whatever package I was trying to install, and then upgrading to .NET 4.7 again. I think it was an SDK referenced somewhere.

Anyways, I was finally able to get it to work by entering:
bazel build --cpu=x64_windows --compiler=msvc-cl --incompatible_remove_native_http_archive=false --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package


I initially tried to build from source because it kept on hanging on 'adding available GPU device' for 3-4min when I installed tensorflow-gpu using pip. (On a side note, I had to specifically enter '5.2,7.5' in 'python ./configure' to get this issue to stop happening. This seems like... a really big issue for a lot of people though. Will these issues be fixed in the future?
Best of luck to everyone trying to do this and running into problems. I hope this helps some of you!"
24720,ExtractImagePatches request,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 1.12


**Provide the text output from tflite_convert**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, CONV_2D, MAXIMUM, MAX_POOL_2D, MUL, PAD. Here is a list of operators for which you will need custom implementations: ExtractImagePatches.

```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24717,Deadlock probably with RandomShuffleQueue and FIFOQueue,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.5 LTS (GNU/Linux 4.17.0-041700-generic x86_64)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tensorflow-gpu (1.12.0)
- Python version: Python 3.5.2
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: 1080Ti 12G


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

I used seq2seq to train an RNN model, and during training the process randomly hangs. It seems to be associated with RandomShuffleQueue or FIFOQueue because they seem to create threads.

```
    for dataset in datasets:
      _, line = parallel_reader.parallel_read( # This creates tf.FIFOQueue
        dataset.data_sources,
        reader_class=dataset.reader,
        num_epochs=num_epochs, # This should be None
        num_readers=1,
        shuffle=False,
        capacity=common_queue_capacity,
        min_after_dequeue=common_queue_min,
        seed=seed
      )
      lines.append(line)

    if shuffle:
      shuffle_queue = tf.RandomShuffleQueue(
        dtypes=[tf.string for _ in datasets],
        capacity=common_queue_capacity,
        min_after_dequeue=common_queue_min,
        seed=seed
      )
      enqueue_ops = []
      enqueue_ops.append(shuffle_queue.enqueue(lines))
      tf.train.add_queue_runner(
          tf.train.QueueRunner(shuffle_queue, enqueue_ops))
```
To debug, I start another thread to print the current stacks. When it does not hang, it prints:
```
--------------------------------------------------------------------
threads: 2019-01-06 02:09:15.317473
Thread 0x00007f39da7fc700 (most recent call first):
  File ""/usr/lib/python3.5/threading.py"", line 293 in wait
  File ""/usr/lib/python3.5/threading.py"", line 549 in wait
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py"", line 311 in wait_for_stop
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 293 in _close_on_stop
  File ""/usr/lib/python3.5/threading.py"", line 862 in run
  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner
  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap

Thread 0x00007f39daffd700 (most recent call first):
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1407 in _call_tf_sessionrun
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1267 in _single_tensor_run
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 257 in _run
  File ""/usr/lib/python3.5/threading.py"", line 862 in run
  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner
  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap

Thread 0x00007f39db7fe700 (most recent call first):
  File ""/usr/lib/python3.5/threading.py"", line 293 in wait
  File ""/usr/lib/python3.5/threading.py"", line 549 in wait
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py"", line 311 in wait_for_stop
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 293 in _close_on_stop
  File ""/usr/lib/python3.5/threading.py"", line 862 in run
  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner
  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap

Thread 0x00007f39dbfff700 (most recent call first):
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1407 in _call_tf_sessionrun
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1257 in _single_operation_run
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 257 in _run
  File ""/usr/lib/python3.5/threading.py"", line 862 in run
  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner
  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap

Thread 0x00007f39f0ff9700 (most recent call first):
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1407 in _call_tf_sessionrun
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1257 in _single_operation_run
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 257 in _run
  File ""/usr/lib/python3.5/threading.py"", line 862 in run
  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner
  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap

Thread 0x00007f39f17fa700 (most recent call first):
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1407 in _call_tf_sessionrun
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1257 in _single_operation_run
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 257 in _run
  File ""/usr/lib/python3.5/threading.py"", line 862 in run
  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner
  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap

Thread 0x00007f39f1ffb700 (most recent call first):
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1407 in _call_tf_sessionrun
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1257 in _single_operation_run
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 257 in _run
  File ""/usr/lib/python3.5/threading.py"", line 862 in run
  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner
  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap

Thread 0x00007f39f27fc700 (most recent call first):
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1407 in _call_tf_sessionrun
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1257 in _single_operation_run
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 257 in _run
  File ""/usr/lib/python3.5/threading.py"", line 862 in run
  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner
  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap

Thread 0x00007f39f2ffd700 (most recent call first):
  File ""/usr/lib/python3.5/threading.py"", line 293 in wait
  File ""/usr/lib/python3.5/threading.py"", line 549 in wait
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py"", line 311 in wait_for_stop
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 293 in _close_on_stop
  File ""/usr/lib/python3.5/threading.py"", line 862 in run
  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner
  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap

Thread 0x00007f39f37fe700 (most recent call first):
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1407 in _call_tf_sessionrun
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1257 in _single_operation_run
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 257 in _run
  File ""/usr/lib/python3.5/threading.py"", line 862 in run
  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner
  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap

Thread 0x00007f39f3fff700 (most recent call first):
  File ""/usr/lib/python3.5/threading.py"", line 293 in wait
  File ""/usr/lib/python3.5/threading.py"", line 549 in wait
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py"", line 311 in wait_for_stop
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 293 in _close_on_stop
  File ""/usr/lib/python3.5/threading.py"", line 862 in run
  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner
  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap

Thread 0x00007f39f8e17700 (most recent call first):
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1407 in _call_tf_sessionrun
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1257 in _single_operation_run
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 257 in _run
  File ""/usr/lib/python3.5/threading.py"", line 862 in run
  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner
  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap

Thread 0x00007f39fc7f8700 (most recent call first):
  File ""/usr/lib/python3.5/threading.py"", line 293 in wait
  File ""/usr/lib/python3.5/threading.py"", line 549 in wait
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py"", line 311 in wait_for_stop
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 293 in _close_on_stop
  File ""/usr/lib/python3.5/threading.py"", line 862 in run
  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner
  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap

Thread 0x00007f39fcff9700 (most recent call first):
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1407 in _call_tf_sessionrun
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1257 in _single_operation_run
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 257 in _run
  File ""/usr/lib/python3.5/threading.py"", line 862 in run
  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner
  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap

Thread 0x00007f39fd7fa700 (most recent call first):
  File ""/usr/lib/python3.5/threading.py"", line 293 in wait
  File ""/usr/lib/python3.5/threading.py"", line 549 in wait
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py"", line 311 in wait_for_stop
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 293 in _close_on_stop
  File ""/usr/lib/python3.5/threading.py"", line 862 in run
  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner
  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap

Thread 0x00007f39fdffb700 (most recent call first):
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1407 in _call_tf_sessionrun
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1257 in _single_operation_run
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 257 in _run
  File ""/usr/lib/python3.5/threading.py"", line 862 in run
  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner
  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap

Thread 0x00007f39fe7fc700 (most recent call first):
  File ""/usr/lib/python3.5/threading.py"", line 293 in wait
  File ""/usr/lib/python3.5/threading.py"", line 549 in wait
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py"", line 311 in wait_for_stop
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 293 in _close_on_stop
  File ""/usr/lib/python3.5/threading.py"", line 862 in run
  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner
  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap

Thread 0x00007f39feffd700 (most recent call first):
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1407 in _call_tf_sessionrun
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1257 in _single_operation_run
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 257 in _run
  File ""/usr/lib/python3.5/threading.py"", line 862 in run
  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner
  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap

Thread 0x00007f3a03ab1700 (most recent call first):
  File ""/usr/lib/python3.5/threading.py"", line 293 in wait
  File ""/usr/lib/python3.5/threading.py"", line 549 in wait
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py"", line 311 in wait_for_stop
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 293 in _close_on_stop
  File ""/usr/lib/python3.5/threading.py"", line 862 in run
  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner
  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap

Thread 0x00007f3cb8ff9700 (most recent call first):
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1407 in _call_tf_sessionrun
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1257 in _single_operation_run
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 257 in _run
  File ""/usr/lib/python3.5/threading.py"", line 862 in run
  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner
  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap

Thread 0x00007f3cb97fa700 (most recent call first):
  File ""/usr/lib/python3.5/threading.py"", line 293 in wait
  File ""/usr/lib/python3.5/threading.py"", line 549 in wait
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py"", line 311 in wait_for_stop
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 293 in _close_on_stop
  File ""/usr/lib/python3.5/threading.py"", line 862 in run
  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner
  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap

Thread 0x00007f3cb9ffb700 (most recent call first):
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1407 in _call_tf_sessionrun
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1257 in _single_operation_run
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 257 in _run
  File ""/usr/lib/python3.5/threading.py"", line 862 in run
  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner
  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap

Thread 0x00007f3a032b0700 (most recent call first):
  File ""/usr/lib/python3.5/threading.py"", line 293 in wait
  File ""/usr/lib/python3.5/threading.py"", line 549 in wait
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py"", line 311 in wait_for_stop
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 293 in _close_on_stop
  File ""/usr/lib/python3.5/threading.py"", line 862 in run
  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner
  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap

Thread 0x00007f3a02aaf700 (most recent call first):
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1407 in _call_tf_sessionrun
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1257 in _single_operation_run
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 257 in _run
  File ""/usr/lib/python3.5/threading.py"", line 862 in run
  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner
  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap

Thread 0x00007f39fffff700 (most recent call first):
  File ""/usr/lib/python3.5/threading.py"", line 293 in wait
  File ""/usr/lib/python3.5/threading.py"", line 549 in wait
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py"", line 311 in wait_for_stop
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 293 in _close_on_stop
  File ""/usr/lib/python3.5/threading.py"", line 862 in run
  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner
  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap

Thread 0x00007f39ff7fe700 (most recent call first):
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1407 in _call_tf_sessionrun
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1257 in _single_operation_run
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 257 in _run
  File ""/usr/lib/python3.5/threading.py"", line 862 in run
  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner
  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap

Thread 0x00007f3d9ffff700 (most recent call first):
  File ""/usr/lib/python3.5/threading.py"", line 293 in wait
  File ""/usr/lib/python3.5/queue.py"", line 164 in get
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/summary/writer/event_file_writer.py"", line 159 in run
  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner
  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap

Current thread 0x00007f3f50b37700 (most recent call first):
  File ""/home/yjf/s2s_download/seq2seq/bin/thread_monitor.py"", line 25 in log_threads
  File ""/usr/lib/python3.5/threading.py"", line 862 in run
  File ""/usr/lib/python3.5/threading.py"", line 914 in _bootstrap_inner
  File ""/usr/lib/python3.5/threading.py"", line 882 in _bootstrap

Thread 0x00007f4075bb4700 (most recent call first):
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1407 in _call_tf_sessionrun
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1319 in _run_fn
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1334 in _do_call
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1328 in _do_run
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1152 in _run
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 929 in run
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 1076 in run
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 1312 in run
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 1240 in run
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 1156 in run
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 671 in run
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1095 in _train_model
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 525 in fit
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py"", line 488 in new_func
  File ""/home/yjf/s2s_download/seq2seq/seq2seq/contrib/experiment.py"", line 104 in continuous_train_and_eval
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py"", line 52 in _execute_schedule
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py"", line 225 in run
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py"", line 306 in new_func
  File ""/home/yjf/s2s_download/seq2seq/bin/train.py"", line 273 in main
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 125 in run
  File ""/home/yjf/s2s_download/seq2seq/bin/train.py"", line 280 in <module>
  File ""/usr/lib/python3.5/runpy.py"", line 85 in _run_code
  File ""/usr/lib/python3.5/runpy.py"", line 184 in _run_module_as_main
--------------------------------------------------------------------
```

When the process is hang, seemingly in deadlock, the only change is the main thread's stack,
with the top of the stack being `join`.

```
Thread 0x00007f4075bb4700 (most recent call first):
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py"", line 373 in join  # <--------------------- HERE
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 1229 in close
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 1069 in close
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 821 in _close_internal
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 783 in __exit__
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1095 in _train_model
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 525 in fit
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py"", line 488 in new_func
  File ""/home/yjf/s2s_download/seq2seq/seq2seq/contrib/experiment.py"", line 104 in continuous_train_and_eval
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py"", line 52 in _execute_schedule
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py"", line 225 in run
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py"", line 306 in new_func
  File ""/home/yjf/s2s_download/seq2seq/bin/train.py"", line 273 in main
  File ""/home/yjf/.local/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 125 in run
  File ""/home/yjf/s2s_download/seq2seq/bin/train.py"", line 280 in <module>
  File ""/usr/lib/python3.5/runpy.py"", line 85 in _run_code
  File ""/usr/lib/python3.5/runpy.py"", line 184 in _run_module_as_main
```
I am not quite familiar with python multi-thread. Perhaps due to the GIL, this info was not printed when the process hanged, but when I interrupted the process with ctrl+c. If you have any ideas on these kind of debugging, I will be very grateful to hear.

**Describe the expected behavior**

It should not hang.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

I am not quite sure which parts lead to the hanging.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

None."
24716,Building Pip package keeps failing,"**System information**
- OS Platform and Distribution: Windows 10
- Mobile device: Lenovo Z50 Laptop
- TensorFlow installed from (source or binary): source
- TensorFlow version: i'm not sure
- Python version: 3.6
- Installed using virtualenv? pip? conda?: i don't have it installed i cloned the github repository though i ran through issues and been told to not have tensorflow installed before building pip packages
- Bazel version (if compiling from source): 0.18.0
- GCC/Compiler version: i don't know where to find that
- CUDA/cuDNN version: 9.0/7.4.1
- GPU model and memory: NVIDIA GEFORCE 850M



**Describe the problem**
  So After i cloned the tensorflow repository i typed python ./configure.py but then following the steps it didn't show ""configuration finished"" at the end and neither any errors, so i tried to build the pip packages with gpu support since my gpu supports cuda and that stuff but then it abored due to the issues below but before that it keeps spamming a certain line, and also where am i failing it's been 5 days of purely tensorflow installation i just wanna make things learn!!

```
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)
    Fetching https://mirror.bazel.build/.../archive/7f634429a04abc48e2eb041c81c5235816c96514.tar.gz; 360,763b 13s
FAILED: Build did NOT complete successfully (305 packages loaded)
```"
24715,Mobilenet transfer learning has a higher final test accuracy than Inception V3 and Inception ResNet v2,"I tried and did some experimenting on running the [retrain.py](https://github.com/tensorflow/hub/blob/master/examples/image_retraining/retrain.py) script for transfer learning. I did the transfer learning of images with 512px x 384px, of the same learning rate (used 0.01) and training steps (used 10000) using [Inception V3](https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1), [Mobilenet_v2_140_224](https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/2), and [Inception ResNet V2](https://tfhub.dev/google/imagenet/inception_resnet_v2/feature_vector/1). I found out that while and after retraining, Mobilenet_v2_140_224 has a higher accuracy compared to Inception V2 and Inception ResNet V3. Below are the final test accuracy of the retraining:

- Mobilenet_v2_140_224: **0.97790337 (97.9%)**
- Inception V3: **0.93268245 (93.3%)**
- Inception ResNet V2: **0.9116136 (91.2%)**

What could possibly be the cause of this? Thanks a lot."
24714,Tensor conversion requested dtype float32_ref for Tensor with dtype float32?,"Hi:
my program have bug in these code line:
     
               word_embeddings = tf.scatter_nd_update(var_output, error_word_f, sum_all)
                word_embeddings_2 = tf.nn.dropout(word_embeddings, self.dropout_pl)

The error hint as follows:
ValueError: Tensor conversion requested dtype float32_ref for Tensor with dtype float32: 'Tensor(""dropout:0"", shape=(), dtype=float32)
  ========================================
   it looks  like word_embeddings 's dtype is float32_ref but actual the function tf.nn.dropout  need word_embeddings dtype float32  ,how can i convert word_embeddings's dtype float32_ref to float32 before run  tf.nn.dropout(word_embeddings, self.dropout_pl)?
   
"
24712,"tf.nn.embedding_lookup return NaN, i think it is not correct!","here is my test case code:
-----------------------------------start
import tensorflow as tf
import numpy as np

tf.enable_eager_execution()
a = np.zeros([2,4,2],dtype=float)
out = tf.nn.embedding_lookup(a,[0],'div',None,-1)
-------------------------------------end
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.12.0
- Python version:3.5
- Bazel version (if compiling from source):NA
- GCC/Compiler version (if compiling from source):NA
- CUDA/cuDNN version:NA
- GPU model and memory:NA


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
it returns :
[[[nan nan]
   [nan nan]
   [nan nan]
   [nan nan]]]
**Describe the expected behavior**
when i study the source code in 
tensorflow/python/ops/embedding_ops.py
tensorflow/tensorflow/python/ops/clip_ops.py:line148-152
i think it should returns 
[[[0. 0.]
   [0. 0.]
   [0. 0.]
   [0. 0.]]]
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24709,TFLite c++ code with TFLite binary gives error using BuildFromBuffer(),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac High Sierra 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pixel 2
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): r.12
- Python version: 3.5
- Bazel version (if compiling from source): 0.21
- GCC/Compiler version (if compiling from source): Apple LLVM version 9.1.0 (clang-902.0.39.2)
- CUDA/cuDNN version: 10.1
- GPU model and memory: NVIDIA 2080TI

**Describe the current behavior**
High-Level: I'm trying to built a TFLite Interpreter in C++ running on a Pixel 2. I have successfully loaded my model.tflite binary using the BuildFromFile() API. However, I now want to read the model from the assets/ directory. To do this I believe I am supposed to use the  BuildFromBuffer() API.

I'm using the BuildFromBuffer() API in model.h using a mmapped buffer and not getting a valid model back. Am I supposed to supply some other kind of buffer? See my code below. I get an error about a bad model identifier.

**Describe the expected behavior**
I expected a valid FlatBufferModel to be returned.

**Code to reproduce the issue**
`
            Code that errors:

            AAsset *asset = AAssetManager_open(mgr, tflite_path.c_str(), O_RDONLY);
            off_t start = 0;
            off_t length = AAsset_getLength(asset);
            int mmap_fd_ = AAsset_openFileDescriptor(asset, &start, &length);
            if (mmap_fd_ < 0) {
                LOGE(""Failed to open file: %s"", tflite_path.c_str());
                AAsset_close(asset);
                return false;
            }
            // MMAP TFLite Binary
            struct stat sb;
            fstat(mmap_fd_, &sb);
            char *mmapped_buffer_ = (char *) mmap(nullptr, sb.st_size, PROT_READ, MAP_SHARED, mmap_fd_,
                                                  0);
            if (mmapped_buffer_ == MAP_FAILED) {
                LOGE(""Mmap failed on file: %s"", tflite_path.c_str());
                AAsset_close(asset);
                return false;
            }

            // Build the Model
            using namespace tflite;
            std::unique_ptr<FlatBufferModel> model = FlatBufferModel::BuildFromBuffer(mmapped_buffer_,
                                                                                      strlen(mmapped_buffer_));
            if (!model) {
                LOGE(""Failed to create FlatBufferModel with file: %s"", tflite_path.c_str()); // My code errors here.
                AAsset_close(asset);
                return false;
            }`


`
            Code that runs successfully: 

            const char *external_path = env->GetStringUTFChars(jExternalPath, JNI_FALSE);
            const char *relative_path = ""/Download/tflite/1052model.ckpt-303187.tflite"";
            char tflite_path[strlen(external_path) + strlen(relative_path)];
            strcpy(tflite_path, external_path);
            strcat (tflite_path, relative_path);

            std::unique_ptr<tflite::FlatBufferModel> model(
                    tflite::FlatBufferModel::BuildFromFile(tflite_path));

            if(!model){
                printf(""Failed to mmap model\n"");
                exit(0);
            }

            tflite::ops::builtin::BuiltinOpResolver resolver;
            std::unique_ptr<tflite::Interpreter> interpreter;
            if (tflite::InterpreterBuilder(*model, resolver)(&interpreter) == kTfLiteOk) {
                interpreter->Invoke();
                LOGD(""TFLiteOk!"");
            } else {
                LOGD(""TFLite -- NOT -- Ok!"");
            }`
**Other info / logs**
Error logs:
19-01-04 12:38:14.953 25181-25181/com.google.sample.oboe.hearsight E/tflite: Model provided has model identifier '
2019-01-04 12:38:14.953 25181-25181/com.google.sample.oboe.hearsight E/AUDIO-APP: Failed to create FlatBufferModel with file: 1052model.ckpt-303187.tflite"
24705,TensorFlow 2.0 installation issue on Windows (CPU),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 (Home Edition)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version: tf-nightly-2.0-preview
- Python version: 3.6.x
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: NA
- GPU model and memory: NA

On a Windows 10 machine, anytime I try to install the TF 2.0 preview by:

```pip install tf-nightly-2.0-preview```

I obtain the following error message:

**Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'C:\\Users\\xxx\\AppData\\Local\\Temp\\pip-install-3rfhl_nx\\tf-nightly-2.0-preview\\tf_nightly_2.0_preview-1.13.0.dev20181226.data/purelib/tensorflow/include/tensorflow/include/external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorSyclConvertToDeviceExpression.h'**

I tried on three different machines with the same OS, fresh install of Anaconda, fresh environment with all possible 3.6.x versions of Python. I also tried to install directly by the whl file after downloading it from PyPy. Any ideas what I am doing wrong?"
24704,Can't load JNI library and Python bindings in same process,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 1.11.0
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.20.0
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the problem**

I'm trying to load both the JNI library (libtensorflow_jni.so) and the Python bindings into the same process. I've built both from source via the //tensorflow/tools/pip_package:build_pip_package and //tensorflow/java:libtensorflow_jni targets. When I load the JNI library after importing tensorflow in a Python process, I get the following error:
```python
>>> import tensorflow as tf
>>> import ctypes
>>> ctypes.CDLL('tensorflow/java/libtensorflow_jni.so')
2019-01-04 16:07:57.226191: F tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/cc/saved_model/load_attempt_count
Aborted
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
- Built TensorFlow JNI library and pip package from source
- Installed whl file via pip
- Ran the steps above

My guess is one or both of _pywrap_tensorflow.so and libtensorflow_jni.so are statically linking that initialization code. Are they intended to be usable in the same process? If not, is there a way to build both objects to have them both dynamically link against the offending section?"
24703,Tensorflow Feature Column V2 test cases failing due to Out of Memory for CUDA,"I am trying to run feature_column_v2_test script using  the bazel test command :
bazel test tensorflow/python/feature_column:feature_column_v2_test
I get an error of i.e out of memory for CUDA
---------------------------------------------------------------------------------------------------
```
The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.
E.2019-01-04 11:57:48.246499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1464] Ignoring visible gpu device 0 whose executor is in invalid state: Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 6373572608
2019-01-04 11:57:48.248421: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 6373572608
```
-----------------------------------------------------------------------------------------------
**My System information**
- Build Tensorflow via Bazel no change from the original repository:
- OS is Ubuntu 18.04
-  Dell G7 15 GTX 1060 Nvidia Cuda 9.0:
- TensorFlow installed from (source or binary):
- 1.18 Tensorflow version:
- Python version is 3.6.7:
- Bazel version  0.19.2
- CUDA/cuDNN version: 9/7
- GPU model and memory: GTX 1060 6GB

Pls help

Regards
Amit"
24702,"Errors of the form ""In rule 'core_test', size 'medium' is not a valid size."" ","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 1.12
- Python version: 3.6.7
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): MSVC - Visual Studio 2015 Update 3
- CUDA/cuDNN version: Cuda Toolkit 9.0/cuDNN 7.4.2
- GPU model and memory: Geforce 1070 GTX, 8 GB

I am trying to build Tensorflow in Windows 10, from the source code. In order to that; I followed the instructions at https://www.tensorflow.org/install/source_windows . After I give the `bazel build` command, the building ends with errors of the type:

`ERROR: D:/tensorflowinstallation/tensorflow_build/tensorflow/tensorflow/contrib/labeled_tensor/BUILD:66:1: In rule 'core_test', size 'medium' is not a valid size.
ERROR: D:/tensorflowinstallation/tensorflow_build/tensorflow/tensorflow/contrib/labeled_tensor/BUILD:66:1: In rule 'core_test', timeout 'illegal' is not a valid timeout.
ERROR: D:/tensorflowinstallation/tensorflow_build/tensorflow/tensorflow/contrib/labeled_tensor/BUILD:168:1: In rule 'ops_test', size 'medium' is not a valid size.
ERROR: D:/tensorflowinstallation/tensorflow_build/tensorflow/tensorflow/contrib/labeled_tensor/BUILD:168:1: In rule 'ops_test', timeout 'illegal' is not a valid timeout.
ERROR: D:/tensorflowinstallation/tensorflow_build/tensorflow/tensorflow/python/debug/BUILD:620:1: In rule 'framework_test', size 'medium' is not a valid size.
ERROR: D:/tensorflowinstallation/tensorflow_build/tensorflow/tensorflow/contrib/eager/python/BUILD:37:1: In rule 'tfe_test', size 'medium' is not a valid size.`


Here is a screenshot of the command prompt:
![image](https://user-images.githubusercontent.com/17207330/50683013-b0f01500-1021-11e9-8bba-eed36813df21.png)


I searched for this type of errors but I found no mention about them at all. I think I may be missing something very fundamental during the build preparation phase but I followed every step as mentioned in the above link. So, I am clueless what these errors are about and turned here for help.

"
24701,Cannot reproduce tf.contrib.autograph's RNN Colorbot example,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No (Used script provided in Tensorflow)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): r1.12
- Python version: 3.6

**Describe the current behavior**

I tried to test some autograph's features with RNN.
At first, I tried tensorflow's official [example](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/autograph/examples/notebooks/rnn_keras_estimator.ipynb) with Python 3.6 Colab Notebook with tensorflow version as r1.12.0.
However, it emits type error as
```
TypeError                                 Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    454                 preferred_dtype=default_dtype,
--> 455                 as_ref=input_arg.is_ref)
    456             if input_arg.number_attr and len(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in internal_convert_n_to_tensor(values, dtype, name, as_ref, preferred_dtype, ctx)
   1210             preferred_dtype=preferred_dtype,
-> 1211             ctx=ctx))
   1212   return ret

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)
   1145     if ret is None:
-> 1146       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1147 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
    228   _ = as_ref
--> 229   return constant(v, dtype=dtype, name=name)
    230 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name, verify_shape)
    207       tensor_util.make_tensor_proto(
--> 208           value, dtype=dtype, shape=shape, verify_shape=verify_shape))
    209   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape)
    441     else:
--> 442       _AssertCompatible(values, dtype)
    443       nparray = np.array(values, dtype=np_dt)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py in _AssertCompatible(values, dtype)
    352       raise TypeError(""Expected %s, got %s of type '%s' instead."" %
--> 353                       (dtype.name, repr(mismatch), type(mismatch).__name__))
    354 

TypeError: Expected int64, got range(0, 64) of type 'range' instead.

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
<ipython-input-8-e180019d193a> in <module>()
     14 regressor.train(
     15     input_fn=lambda: input_fn(data_dir, train_url, params),
---> 16     steps=100)
     17 eval_results = regressor.evaluate(
     18     input_fn=lambda: input_fn(data_dir, test_url, params, training=False),

/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)
    352 
    353       saving_listeners = _check_listeners_type(saving_listeners)
--> 354       loss = self._train_model(input_fn, hooks, saving_listeners)
    355       logging.info('Loss for final step: %s.', loss)
    356       return self

/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)
   1205       return self._train_model_distributed(input_fn, hooks, saving_listeners)
   1206     else:
-> 1207       return self._train_model_default(input_fn, hooks, saving_listeners)
   1208 
   1209   def _train_model_default(self, input_fn, hooks, saving_listeners):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in _train_model_default(self, input_fn, hooks, saving_listeners)
   1235       worker_hooks.extend(input_hooks)
   1236       estimator_spec = self._call_model_fn(
-> 1237           features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
   1238       global_step_tensor = training_util.get_global_step(g)
   1239       return self._train_with_estimator_spec(estimator_spec, worker_hooks,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)
   1193 
   1194     logging.info('Calling model_fn.')
-> 1195     model_fn_results = self._model_fn(features=features, **kwargs)
   1196     logging.info('Done calling model_fn.')
   1197 

<ipython-input-6-eed6cb165519> in model_fn(features, labels, mode, params)
     10 
     11   if mode == tf.estimator.ModeKeys.TRAIN:
---> 12     predictions = colorbot(inputs, training=True)
     13     loss = loss_fn(labels, predictions)
     14 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    755       if not in_deferred_mode:
    756         self._in_call = True
--> 757         outputs = self.call(inputs, *args, **kwargs)
    758         self._in_call = False
    759         if outputs is None:

/tmp/tmpwq7vt5r_.py in call(self, inputs, training)
     83       seq = self._rnn_layer(seq, self.upper_cell, batch_size, training)
     84       indices = length - 1, ag__.range_(batch_size)
---> 85       indices = tf.stack(indices, 1)
     86       sequence_ends = tf.gather_nd(seq, indices)
     87       return self.relu_layer(sequence_ends)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in stack(values, axis, name)
    872                                                       expanded_num_dims))
    873 
--> 874   return gen_array_ops.pack(values, axis=axis, name=name)
    875 
    876 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py in pack(values, axis, name)
   4873     axis = _execute.make_int(axis, ""axis"")
   4874     _, _, _op = _op_def_lib._apply_op_helper(
-> 4875         ""Pack"", values=values, axis=axis, name=name)
   4876     _result = _op.outputs[:]
   4877     _inputs_flat = _op.inputs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    481                                 (prefix, dtype.name))
    482               else:
--> 483                 raise TypeError(""%s that don't all match."" % prefix)
    484             else:
    485               raise TypeError(""%s that are invalid."" % prefix)

TypeError: Tensors in list passed to 'values' of 'Pack' Op have types [int64, <NOT CONVERTIBLE TO TENSOR>] that don't all match.
```

Moreover, if I try to run the original colab notebook example, which run tensorflow 1.13 dev in python2.7, it raises graph construction error as

```
GraphConstructionErrorTraceback (most recent call last)
<ipython-input-15-e180019d193a> in <module>()
     14 regressor.train(
     15     input_fn=lambda: input_fn(data_dir, train_url, params),
---> 16     steps=100)
     17 eval_results = regressor.evaluate(
     18     input_fn=lambda: input_fn(data_dir, test_url, params, training=False),

/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.pyc in train(self, input_fn, hooks, steps, max_steps, saving_listeners)
    356 
    357       saving_listeners = _check_listeners_type(saving_listeners)
--> 358       loss = self._train_model(input_fn, hooks, saving_listeners)
    359       logging.info('Loss for final step: %s.', loss)
    360       return self

/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.pyc in _train_model(self, input_fn, hooks, saving_listeners)
   1122       return self._train_model_distributed(input_fn, hooks, saving_listeners)
   1123     else:
-> 1124       return self._train_model_default(input_fn, hooks, saving_listeners)
   1125 
   1126   def _train_model_default(self, input_fn, hooks, saving_listeners):

/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.pyc in _train_model_default(self, input_fn, hooks, saving_listeners)
   1152       worker_hooks.extend(input_hooks)
   1153       estimator_spec = self._call_model_fn(
-> 1154           features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
   1155       global_step_tensor = training_util.get_global_step(g)
   1156       return self._train_with_estimator_spec(estimator_spec, worker_hooks,

/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.pyc in _call_model_fn(self, features, labels, mode, config)
   1110 
   1111     logging.info('Calling model_fn.')
-> 1112     model_fn_results = self._model_fn(features=features, **kwargs)
   1113     logging.info('Done calling model_fn.')
   1114 

<ipython-input-13-eed6cb165519> in model_fn(features, labels, mode, params)
     10 
     11   if mode == tf.estimator.ModeKeys.TRAIN:
---> 12     predictions = colorbot(inputs, training=True)
     13     loss = loss_fn(labels, predictions)
     14 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.pyc in __call__(self, inputs, *args, **kwargs)
    554           if not self.dynamic:
    555             try:
--> 556               outputs = self.call(inputs, *args, **kwargs)
    557             except TypeError as e:
    558               messages = ['`tf.Tensor` as a Python `bool` is not allowed',

/tmp/tmpxLZSMS.py in call(self, inputs, training)
    101         return ag__.converted_call('relu_layer', self, autograph.ConversionOptions(recursive=False, verbose=0, strip_decorators=(defun, autograph.convert, autograph.do_not_convert, autograph.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), sequence_ends)
    102     except:
--> 103       ag__.rewrite_graph_construction_error(ag_source_map__)
    104 
    105 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/autograph/core/errors.pyc in rewrite_graph_construction_error(source_map)
    135     raise original_error
    136   else:
--> 137     raise new_error
    138   finally:
    139     # Addresses warning https://docs.python.org/2/library/sys.html#sys.exc_info.

GraphConstructionError: Traceback (most recent call last):
  File ""<ipython-input-12-87e5f229a162>"", line 59, in call
    seq = self._rnn_layer(seq, self.lower_cell, batch_size, training)
  File ""<ipython-input-12-87e5f229a162>"", line 27, in _rnn_layer
    cell_output, (state, output) = cell.call(ch, (state, output))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/autograph/impl/api.py"", line 200, in converted_call
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/lstm_ops.py"", line 426, in call
    wci = wcf = wco = array_ops.zeros([self._num_units], dtype=self.dtype)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 1793, in zeros
    dtype = dtypes.as_dtype(dtype).base_dtype
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/dtypes.py"", line 719, in as_dtype
    raise TypeError(""Cannot convert value %r to a TensorFlow DType."" % type_value)

Cannot convert value None to a TensorFlow DType.
```

It says that it cannot convert value **None** to a Tensorflow DType, but I cannot find any None value if I print values related to erroneous call stack.

**Describe the expected behavior**

In the original notebook example, it prints out evaluation loss without error

**Code to reproduce the issue**
[link](https://colab.research.google.com/drive/1fqr0wiNctLm0M07xss-Ra_iGC7DCojjs) to my python3 colab notebook"
24698,"C++ include files when linking against headers with wrong type, no long int in code base, erroring about long int type.","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes I have written code which takes an image from a OpenFX framework and passes it as input to a Tensorflow session in C++, runs the feed forward and returns the result to a pixel buffer

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux CentOS 6.10, using gcc/4.8.5 and bazel 0.11.0 with CUDA 9.1 and cuDNN 7.1.2

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
from source v1.6.0 zip here:
https://github.com/tensorflow/tensorflow/archive/v1.6.0.zip
- TensorFlow version (use command below):
NA

- Python version:
2.7.13 NA
- Bazel version (if compiling from source):
0.11.0
- GCC/Compiler version (if compiling from source):
gcc 4.8.5
- CUDA/cuDNN version:
CUDA 9.1 with 3 updates, cuDNN 7.1.3
- GPU model and memory:
GTX 1060 6Gb

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
the code will no longer compile when I moved to a machine with a GPU 
```

  CXX      Linux-64-debug/rotobot.o
rotobot.cpp: In function void drawMasks(OpenImageIO::v1_6::ImageBuf&, std::unique_ptr<tensorflow::Session>&, OpenImageIO::v1_6::ImageBuf&, std::string&, bool, const bool*, const bool*, const bool*, double, double, double, double, double, double, bool):
rotobot.cpp:435:68: error: inputTensor2 was not declared in this scope
  tensorflow::Status run_status = tfSession->Run({ { ""input_image"", inputTensor2 },{ ""input_image_meta"", inputMetadataTensor } },
                                                                    ^
rotobot.cpp:439:11: error: no matching function for call to tensorflow::Session::Run(<brace-enclosed initializer list>, <brace-enclosed initializer list>, <brace-enclosed initializer list>, std::vector<tensorflow::Tensor>*)
   &outputs);
           ^
rotobot.cpp:439:11: note: candidates are:
In file included from rotobot.cpp:32:0:
/home/sam/dev/tensorflow-1.6.0/tensorflow/core/public/session.h:121:18: note: virtual tensorflow::Status tensorflow::Session::Run(const std::vector<std::pair<std::basic_string<char>, tensorflow::Tensor> >&, const std::vector<std::basic_string<char> >&, const std::vector<std::basic_string<char> >&, std::vector<tensorflow::Tensor>*)
   virtual Status Run(const std::vector<std::pair<string, Tensor> >& inputs,
                  ^
/home/sam/dev/tensorflow-1.6.0/tensorflow/core/public/session.h:121:18: note:   no known conversion for argument 1 from <brace-enclosed initializer list> to const std::vector<std::pair<std::basic_string<char>, tensorflow::Tensor> >&
/home/sam/dev/tensorflow-1.6.0/tensorflow/core/public/session.h:150:18: note: virtual tensorflow::Status tensorflow::Session::Run(const tensorflow::RunOptions&, const std::vector<std::pair<std::basic_string<char>, tensorflow::Tensor> >&, const std::vector<std::basic_string<char> >&, const std::vector<std::basic_string<char> >&, std::vector<tensorflow::Tensor>*, tensorflow::RunMetadata*)
   virtual Status Run(const RunOptions& run_options,
                  ^
/home/sam/dev/tensorflow-1.6.0/tensorflow/core/public/session.h:150:18: note:   candidate expects 6 arguments, 4 provided
In file included from /home/sam/dev/tensorflow-1.6.0/tensorflow/core/framework/tensor.h:23:0,
                 from /home/sam/dev/tensorflow-1.6.0/tensorflow/core/public/session.h:24,
                 from rotobot.cpp:32:
/home/sam/dev/tensorflow-1.6.0/tensorflow/core/framework/types.h: In instantiation of struct tensorflow::DataTypeToEnum<long int>:
/home/sam/dev/tensorflow-1.6.0/tensorflow/core/framework/tensor.h:566:46:   required from typename tensorflow::TTypes<T, NDIMS>::Tensor tensorflow::Tensor::tensor() [with T = long int; long unsigned int NDIMS = 3ul; typename tensorflow::TTypes<T, NDIMS>::Tensor = Eigen::TensorMap<Eigen::Tensor<long int, 3, 1, long int>, 16, Eigen::MakePointer>]
rotobot.cpp:1743:53:   required from here
/home/sam/dev/tensorflow-1.6.0/tensorflow/core/framework/types.h:356:3: error: static assertion failed: Specified Data Type not supported
   static_assert(IsValidDataType<T>::value, ""Specified Data Type not supported"");
   ^
In file included from /home/sam/dev/tensorflow-1.6.0/tensorflow/core/public/session.h:24:0,
                 from rotobot.cpp:32:
/home/sam/dev/tensorflow-1.6.0/tensorflow/core/framework/tensor.h: In instantiation of typename tensorflow::TTypes<T, NDIMS>::Tensor tensorflow::Tensor::tensor() [with T = long int; long unsigned int NDIMS = 3ul; typename tensorflow::TTypes<T, NDIMS>::Tensor = Eigen::TensorMap<Eigen::Tensor<long int, 3, 1, long int>, 16, Eigen::MakePointer>]:
rotobot.cpp:1743:53:   required from here
/home/sam/dev/tensorflow-1.6.0/tensorflow/core/framework/tensor.h:566:46: error: v is not a member of tensorflow::DataTypeToEnum<long int>

```
**Describe the expected behavior**
The code was compiling previously

The previous build environment was v1.6.0.2-gcbc6580

But was built without CUDA options.

I am linking against 
`
-ltensorflow -ltensorflow_cc and -ltensorflow_framework -lnsync`

and a few more

but I am not compiling yet.

lines 31,32,33 respectively are:

#include <tensorflow/core/platform/init_main.h>
#include <tensorflow/core/public/session.h>
#include <tensorflow/core/framework/tensor_shape.h>


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

lines 31,32,33 respectively are:

#include <tensorflow/core/platform/init_main.h>
#include <tensorflow/core/public/session.h>
#include <tensorflow/core/framework/tensor_shape.h>

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

see:
https://stackoverflow.com/questions/54033100/tensorflow-1-6-0-with-cuda-support-on-centos-6-10-c-linking-against-libtensorf"
24695,Incompatible checkpoint being restored on estimator with keras model,"**System information**
- Have I written custom code: yes
- OS Platform and Distribution: Archlinux
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12.0
- Python version: 3.7.1
- CUDA/cuDNN version: 10.0.130-2/7.4.1.5-2
- GPU model and memory: 11GB GTX 1080 Ti

**Describe the current behavior**

I am running a Keras Model within a custom estimator and giving the size of a hidden layer as a parameter.
After each run with the estimator, a checkpoint is stored, saving the weights of that run. 

However, if the architecture of the model is changed between runs, instead of receiving an error due to incompatible checkpoints, tensorflow is loading the old checkpoint into the new model even though new dimension is being considered.

**Describe the expected behavior**

According to the [guide](https://www.tensorflow.org/guide/checkpoints#avoiding_a_bad_restoration), ""restoring a model's state from a checkpoint only works if the model and checkpoint are compatible"" and it is given an example of how the described situation (changes in the model architecture) should generate an `InvalidArgumentError`.

However no error is raised, being unclear the behaviour - are the new weights (in case of increased size) initialised and trained? Are they simply ignored?

**Code to reproduce the issue**

Below is a minimum working example to reproduce the problem:

```python
import tensorflow as tf
import numpy as np
from tensorflow.keras import layers


def main(argv):
    steps = 1000
    # Generate random data
    train_x = np.random.randn(100, 4).astype('float32')
    train_y = np.random.randint(0, 3, 100).astype('int32')

    # First estimator, with *10* hidden units
    classifier = tf.estimator.Estimator(
        model_fn=model_fn,
        model_dir=""tmp"",
        params={'hidden': 10}
    )

    # train first model and store checkpoints on ""tmp""
    classifier.train(
        input_fn=lambda: tf.data.Dataset.from_tensors((train_x, train_y)),
        steps=steps)

    # second estimator, with *20* hidden units and same model_dir output
    # BUG: should break as model_dir is the same as previous estimator!!!
    classifier = tf.estimator.Estimator(
        model_fn=model_fn,
        model_dir=""tmp"",
        params={'hidden': 20}
    )

    classifier.train(
        input_fn=lambda: tf.data.Dataset.from_tensors((train_x, train_y)),
        steps=steps)


def model_fn(features, labels, mode, params):

    # create model with params[hidden] units on hidden layer
    model = tf.keras.Sequential([layers.Dense(params[""hidden""], activation='relu'),
                                 layers.Dense(3)])
    logits = model(features)
    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)
    optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)
    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())

    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)


if __name__ == '__main__':
    tf.logging.set_verbosity(tf.logging.INFO)
    tf.app.run(main)
```



**Other info / logs**
Log output (abbreviated)

~~~
# RUN 1
...
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Saving checkpoints for 0 into tmp/model.ckpt.
INFO:tensorflow:loss = 1.1404053, step = 0
INFO:tensorflow:Saving checkpoints for 1 into tmp/model.ckpt.
INFO:tensorflow:Loss for final step: 1.1404053.
...
# RUN 2
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.

INFO:tensorflow:Restoring parameters from tmp/model.ckpt-1
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Saving checkpoints for 1 into tmp/model.ckpt.
INFO:tensorflow:loss = 1.112431, step = 1
INFO:tensorflow:Saving checkpoints for 2 into tmp/model.ckpt.
INFO:tensorflow:Loss for final step: 1.112431.
~~~
"
24694,Tensorflow-gpu installation issue ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version: idk
- Python version: Python 3.6.2
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: CUDA v.10.0/cuDNN 7.4.0
- GPU model and memory: GeForce 940MX


I first installed tensorflow-gpu using the command ""pip install tensorflow-gpu""
however when I tested it using the commands:
>>>import tensorflow as tf
 >>>print(tf.__version__)
""Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: module 'tensorflow' has no attribute '__version__'""

So, I checked 
>>> dir(tf)
['__doc__', '__loader__', '__name__', '__package__', '__path__', '__spec__']

Prior to my attempt to install and use tensorflow-gpu, I was using tensorflow supported using cpu and it was okay. 

I tried uninstalling it using the command ""pip uninstall tensorflow-gpu"" but I cannot do so due to error:
FileNotFoundError: [Errno 2] No such file or directory: 'C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-uninstall-ba9tnm4x\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\tensor\\tensorsyclconverttodeviceexpression.h' 

Result of uninstallation is:
Uninstalling tensorflow-gpu-1.12.0:
  Would remove:
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\src\tensor\tensorsyclconverttodeviceexpression.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\src\tensor\tensorsyclexprconstructor.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\src\tensor\tensorsyclextractaccessor.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\src\tensor\tensorsyclextractfunctors.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\src\tensor\tensorsyclfunctors.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\src\tensor\tensorsyclleafcount.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\src\tensor\tensorsyclplaceholderexpr.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\src\tensor\tensorsyclrun.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\src\tensor\tensorsycltuple.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\src\tensor\tensortrace.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\src\tensor\tensortraits.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\src\tensor\tensoruint128.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\src\tensor\tensorvolumepatch.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\src\tensorsymmetry\dynamicsymmetry.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\src\tensorsymmetry\staticsymmetry.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\src\tensorsymmetry\symmetry.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\src\tensorsymmetry\util\templategrouptheory.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\src\threadpool\eventcount.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\src\threadpool\nonblockingthreadpool.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\src\threadpool\runqueue.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\src\threadpool\simplethreadpool.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\src\threadpool\threadcancel.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\src\threadpool\threadenvironment.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\src\threadpool\threadlocal.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\src\threadpool\threadpoolinterface.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\src\threadpool\threadyield.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\src\util\cxx11meta.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\src\util\cxx11workarounds.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\src\util\emulatearray.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\src\util\emulatecxx11meta.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\src\util\maxsizevector.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\tensor
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\tensorsymmetry
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\cxx11\threadpool
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\fft
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\kroneckerproduct
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\matrixfunctions
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\specialfunctions
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\src\fft\ei_fftw_impl.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\src\fft\ei_kissfft_impl.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\src\kroneckerproduct\kroneckertensorproduct.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\src\matrixfunctions\matrixexponential.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\src\matrixfunctions\matrixfunction.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\src\matrixfunctions\matrixlogarithm.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\src\matrixfunctions\matrixpower.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\src\matrixfunctions\matrixsquareroot.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\src\matrixfunctions\stemfunction.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\src\specialfunctions\arch\cuda\cudaspecialfunctions.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\src\specialfunctions\specialfunctionsarrayapi.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\src\specialfunctions\specialfunctionsfunctors.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\src\specialfunctions\specialfunctionshalf.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\src\specialfunctions\specialfunctionsimpl.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\include\external\eigen_archive\unsupported\eigen\src\specialfunctions\specialfunctionspacketmath.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\blas.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\cuda\cuda_activation.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\cuda\cuda_blas.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\cuda\cuda_diagnostics.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\cuda\cuda_dnn.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\cuda\cuda_driver.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\cuda\cuda_event.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\cuda\cuda_fft.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\cuda\cuda_gpu_executor.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\cuda\cuda_helpers.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\cuda\cuda_kernel.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\cuda\cuda_platform.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\cuda\cuda_platform_id.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\cuda\cuda_rng.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\cuda\cuda_stream.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\cuda\cuda_timer.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\cuda\cudnn_version.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\device_description.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\device_memory.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\device_options.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\dnn.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\dso_loader.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\event.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\executor_cache.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\fft.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\gpu_launch_dim.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\host\host_gpu_executor.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\host\host_platform.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\host\host_platform_id.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\host\host_stream.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\host\host_timer.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\host_buffer.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\host_or_device_scalar.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\kernel.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\kernel_cache_config.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\kernel_spec.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\launch_dim.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\lib\array_slice.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\lib\casts.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\lib\demangle.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\lib\env.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\lib\error.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\lib\human_readable.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\lib\initialize.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\lib\inlined_vector.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\lib\mathutil.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\lib\notification.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\lib\numbers.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\lib\path.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\lib\process_state.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\lib\ptr_util.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\lib\stacktrace.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\lib\static_threadlocal.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\lib\status.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\lib\status_macros.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\lib\statusor.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\lib\statusor_internals.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\lib\str_util.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\lib\strcat.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\lib\stringpiece.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\lib\stringprintf.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\lib\thread_options.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\lib\threadpool.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\module_spec.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\multi_platform_manager.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\platform.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\platform\default\mutex.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\platform\logging.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\platform\mutex.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\platform\port.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\platform\thread_annotations.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\plugin.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\plugin_registry.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\rng.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\scratch_allocator.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\shared_memory_config.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\stream.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\stream_executor.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\stream_executor_internal.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\stream_executor_pimpl.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\temporary_device_memory.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\temporary_memory_manager.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\timer.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\tensorflow\stream_executor\trace_listener.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\third_party\eigen3\eigen\cholesky
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\third_party\eigen3\eigen\core
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\third_party\eigen3\eigen\eigenvalues
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\third_party\eigen3\eigen\lu
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\third_party\eigen3\eigen\qr
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\third_party\eigen3\eigen\svd
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\third_party\eigen3\license
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\third_party\eigen3\unsupported\eigen\cxx11\fixedpoint
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\third_party\eigen3\unsupported\eigen\cxx11\src\fixedpoint\fixedpointtypes.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\third_party\eigen3\unsupported\eigen\cxx11\src\fixedpoint\matmatproduct.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\third_party\eigen3\unsupported\eigen\cxx11\src\fixedpoint\matmatproductavx2.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\third_party\eigen3\unsupported\eigen\cxx11\src\fixedpoint\matmatproductneon.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\third_party\eigen3\unsupported\eigen\cxx11\src\fixedpoint\matvecproduct.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\third_party\eigen3\unsupported\eigen\cxx11\src\fixedpoint\packetmathavx2.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\third_party\eigen3\unsupported\eigen\cxx11\src\fixedpoint\packetmathavx512.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\third_party\eigen3\unsupported\eigen\cxx11\src\fixedpoint\typecastingavx2.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\third_party\eigen3\unsupported\eigen\cxx11\src\fixedpoint\typecastingavx512.h
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\third_party\eigen3\unsupported\eigen\cxx11\tensor
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\third_party\eigen3\unsupported\eigen\cxx11\threadpool
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\third_party\eigen3\unsupported\eigen\matrixfunctions
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\include\third_party\eigen3\unsupported\eigen\specialfunctions
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\*
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\tools\*
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow_gpu-1.12.0.dist-info\*
    c:\users\lenovo\appdata\local\programs\python\python36\scripts\freeze_graph.exe
    c:\users\lenovo\appdata\local\programs\python\python36\scripts\saved_model_cli.exe
    c:\users\lenovo\appdata\local\programs\python\python36\scripts\tensorboard.exe
    c:\users\lenovo\appdata\local\programs\python\python36\scripts\tflite_convert.exe
    c:\users\lenovo\appdata\local\programs\python\python36\scripts\toco.exe
    c:\users\lenovo\appdata\local\programs\python\python36\scripts\toco_from_protos.exe
  Would not remove (might be manually added):
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\client\notebook.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\client\session_benchmark.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\debug\examples\__init__.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\debug\examples\debug_errors.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\debug\examples\debug_fibonacci.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\debug\examples\debug_mnist.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\debug\examples\debug_tflearn_iris.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\debug\ops\gen_debug_ops.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\eager\graph_callable.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\estimator\canned\dnn_testing_utils.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\estimator\canned\linear_testing_utils.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\grappler\controller.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\grappler\cost_analyzer.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\grappler\cost_analyzer_tool.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\grappler\graph_placer.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\grappler\hierarchical_controller.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\grappler\model_analyzer.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\kernel_tests\__init__.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\kernel_tests\boosted_trees\__init__.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\kernel_tests\distributions\__init__.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\kernel_tests\linalg\__init__.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\kernel_tests\random\__init__.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\kernel_tests\reduction_ops_test_big.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\kernel_tests\testdata\__init__.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\lib\core\__init__.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\ops\accumulate_n_benchmark.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\ops\batch_norm_benchmark.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\ops\concat_benchmark.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\ops\conv2d_benchmark.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\ops\matmul_benchmark.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\ops\split_benchmark.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\ops\transpose_benchmark.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\profiler\internal\model_analyzer_testlib.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\profiler\pprof_profiler.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.lib
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\training\checkpointable\data_structures_base.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\util\protobuf\compare_test_pb2.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\tools\api\__init__.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\tools\api\generator\__init__.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\tools\api\generator\create_python_api.py
    c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\tensorflow\tools\api\generator\doc_srcs.py
Proceed (y/n)? y
Exception:
Traceback (most recent call last):
  File ""c:\users\lenovo\appdata\local\programs\python\python36\lib\shutil.py"", line 544, in move
    os.rename(src, real_dst)
FileNotFoundError: [WinError 3] The system cannot find the path specified: 'c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\tensor\\tensorsyclconverttodeviceexpression.h' -> 'C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-uninstall-ba9tnm4x\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\tensor\\tensorsyclconverttodeviceexpression.h'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\pip\_internal\basecommand.py"", line 228, in main
    status = self.run(options, args)
  File ""c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\pip\_internal\commands\uninstall.py"", line 68, in run
    auto_confirm=options.yes, verbose=self.verbosity > 0,
  File ""c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\pip\_internal\req\req_install.py"", line 661, in uninstall
    uninstalled_pathset.remove(auto_confirm, verbose)
  File ""c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\pip\_internal\req\req_uninstall.py"", line 219, in remove
    renames(path, new_path)
  File ""c:\users\lenovo\appdata\local\programs\python\python36\lib\site-packages\pip\_internal\utils\misc.py"", line 273, in renames
    shutil.move(old, new)
  File ""c:\users\lenovo\appdata\local\programs\python\python36\lib\shutil.py"", line 558, in move
    copy_function(src, real_dst)
  File ""c:\users\lenovo\appdata\local\programs\python\python36\lib\shutil.py"", line 257, in copy2
    copyfile(src, dst, follow_symlinks=follow_symlinks)
  File ""c:\users\lenovo\appdata\local\programs\python\python36\lib\shutil.py"", line 121, in copyfile
    with open(dst, 'wb') as fdst:
FileNotFoundError: [Errno 2] No such file or directory: 'C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-uninstall-ba9tnm4x\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\tensor\\tensorsyclconverttodeviceexpression.h'  
"
24690,tf.keras.backend.batch_normalization documentation equation does not match implementation,"**System information**
- TensorFlow version: r1.12
- Doc Link: https://www.tensorflow.org/api_docs/python/tf/keras/backend/batch_normalization

**Describe the documentation issue**
The documentation describes the op as computing 

~~~
output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta
~~~

the op actually computes

~~~
output = (x - mean) / (sqrt(var + epsilon)) * gamma + beta
~~~

with `epsilon` inside the sqrt

Practically, it doesn't really matter where the epsilon is added in actual use.  (I only noticed because I was comparing the output of each layer when translating something from caffe to tensorflow)

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**

Maybe eventually, would take me awhile to get around to it"
24687,I am getting the following error when exporting keras model to tensorflow,"from tensorflow.python.saved_model import builder as saved_model_builder
from tensorflow.python.saved_model import utils
from tensorflow.python.saved_model import tag_constants, signature_constants
from tensorflow.python.saved_model.signature_def_utils_impl import build_signature_def, predict_signature_def
from tensorflow.contrib.session_bundle import exporter

export_path = 'plantmodelproduction'
builder = saved_model_builder.SavedModelBuilder(export_path)

signature = predict_signature_def(inputs={'images': new_model.input},
                                  outputs={'scores': new_model.output})

with K.get_session() as sess:
    builder.add_meta_graph_and_variables(sess=sess,
                                         tags=[tag_constants.SERVING],
                                         signature_def_map={'predict': signature})
    builder.save()



INFO:tensorflow:No assets to save.
INFO:tensorflow:No assets to write.
ERROR:tensorflow:==================================
Object was never used (type <class 'tensorflow.python.framework.ops.Tensor'>):
<tf.Tensor 'IsVariableInitialized_3579:0' shape=() dtype=bool>
If you want to mark it as used call its ""mark_used()"" method.
It was originally created here:
  File ""C:\Users\isheunesu\Anaconda\lib\site-packages\IPython\core\interactiveshell.py"", line 3287, in run_code
    return outflag  File ""<ipython-input-96-dc248f1de322>"", line 2, in <module>
    model=load_model('plantdiseasemodel.h5')  File ""C:\Users\isheunesu\Anaconda\lib\site-packages\keras\engine\saving.py"", line 422, in load_model
    h5dict.close()  File ""C:\Users\isheunesu\Anaconda\lib\site-packages\keras\engine\saving.py"", line 288, in _deserialize_model
    K.batch_set_value(weight_value_tuples)  File ""C:\Users\isheunesu\Anaconda\lib\site-packages\keras\backend\tensorflow_backend.py"", line 2581, in batch_set_value
    get_session().run(assign_ops, feed_dict=feed_dict)  File ""C:\Users\isheunesu\Anaconda\lib\site-packages\keras\backend\tensorflow_backend.py"", line 206, in get_session
    session.run(tf.variables_initializer(uninitialized_vars))  File ""C:\Users\isheunesu\Anaconda\lib\site-packages\keras\backend\tensorflow_backend.py"", line 199, in <listcomp>
    [tf.is_variable_initialized(v) for v in candidate_vars])  File ""C:\Users\isheunesu\Anaconda\lib\site-packages\tensorflow\python\util\tf_should_use.py"", line 189, in wrapped
    return _add_should_use_warning(fn(*args, **kwargs))
==================================
ERROR:tensorflow:==================================
Object was never used (type <class 'tensorflow.python.framework.ops.Tensor'>):
<tf.Tensor 'IsVariableInitialized_3578:0' shape=() dtype=bool>
If you want to mark it as used call its ""mark_used()"" method.
It was originally created here:
  File ""C:\Users\isheunesu\Anaconda\lib\site-packages\IPython\core\interactiveshell.py"", line 3287, in run_code
    return outflag  File ""<ipython-input-96-dc248f1de322>"", line 2, in <module>
    model=load_model('plantdiseasemodel.h5')  File ""C:\Users\isheunesu\Anaconda\lib\site-packages\keras\engine\saving.py"", line 422, in load_model
    h5dict.close()  File ""C:\Users\isheunesu\Anaconda\lib\site-packages\keras\engine\saving.py"", line 288, in _deserialize_model
    K.batch_set_value(weight_value_tuples)  File ""C:\Users\isheunesu\Anaconda\lib\site-packages\keras\backend\tensorflow_backend.py"", line 2581, in batch_set_value
    get_session().run(assign_ops, feed_dict=feed_dict)  File ""C:\Users\isheunesu\Anaconda\lib\site-packages\keras\backend\tensorflow_backend.py"", line 206, in get_session
    session.run(tf.variables_initializer(uninitialized_vars))  File ""C:\Users\isheunesu\Anaconda\lib\site-packages\keras\backend\tensorflow_backend.py"", line 199, in <listcomp>
    [tf.is_variable_initialized(v) for v in candidate_vars])  File ""C:\Users\isheunesu\Anaconda\lib\site-packages\tensorflow\python\util\tf_should_use.py"", line 189, in wrapped
    return _add_should_use_warning(fn(*args, **kwargs))
=================================="
24686,tf.print  can't print log in jupyter notebook. ,"When I tried to use tf.print() or get the log_device_placement information in google colabratory. I just can't get the print output. I tried in my own jupyter, and I find the print log is in the cmd.exe windows. This behavior is very strange and I think it should be fixed.  The code is like below.


```python
import tensorflow as tf
print(tf.GIT_VERSION, tf.VERSION)
a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
c = tf.matmul(a, b)
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
print(sess.run(c))
```

the print output is just like this:

> v1.12.0-0-ga6d8ffae09 1.12.0
[[22. 28.]
 [49. 64.]]

but it should be with the log_device_placement information.

"
24685,Image Style Transfer Model not generating fake nodes,"HI,
I am trying to quantize the following model
https://github.com/hwalsuklee/tensorflow-fast-style-transfer

 I have trained the model with following 
tf.contrib.quantize.create_training_graph(quant_delay=0)
and for Inference I am using the following 
 tf.contrib.quantize.create_eval_graph()

I am freezing the graph with following command
python -m tensorflow.python.tools.freeze_graph \
--input_graph=FST_out.pb \
--input_checkpoint=q_out_trt1/final.ckpt-14000 \
--input_binary=true \
--output_graph=q_out_trt1/frozen_FST.pb \
--output_node_names=Tanh

What I have noticed after freezing the graph is that there is no Fake Quant Nodes .We have also noticed that most of the models which are generating fake nodes are slim based.
Please let us know the reason for not generating the fake nodes and if the quantization aware training works only for tf.contrib.slim based model.

Regards
Shailendra"
24684,Large Python call overhead in eager mode,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX Mojave
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6
- GCC/Compiler version (if compiling from source): 4.2.1
v1.12.0-rc2-3-ga6d8ffae09 1.12.0

We are currently working with Tensorflow Probability as part of PyMC4. The model logp graph is static and rather cheap but the samplers are in python. Thus, for every logp evaluation, we have to evaluate the TF graph. Compared to Theano, this is rather slow (about 20x slower). I suspect that the Python call overhead is just very high, especially in graph mode (see https://github.com/tensorflow/tensorflow/issues/120). 

Eager mode makes things better but it's still about 10x slower, even when using defun. I guess I'm asking how much additional work needs to be done other than giving a call hook directly into compiled code and whether that part is optimized at all.

Here is a notebook with some very basic speed comparisons to Theano: https://gist.github.com/twiecki/43d6b78455ef5812bb90b5522fe7686c

 The difference grows more dramatic in a real-world scenario but the notebook is more involved: https://github.com/aseyboldt/pymc4/blob/london/pymc4_experiment.ipynb"
24679,Out of bounds error while iterating on the rows of a RaggedTensor,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac OS X 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
VERSION=""1.13.0-dev20181226"" (this is the TF 2.0-preview)
GIT_VERSION=""b'v1.12.0-5133-gc343196842'""
- Python version:
3.6.6
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
Iterating over the elements of a `RaggedTensor` results in an `tf.errors.InvalidArgumentError` after the last element.

**Describe the expected behavior**
I expect to be able to iterate over the rows of a `RaggedTensor` without any error.

**Code to reproduce the issue**

```python
import tensorflow as tf
r = tf.ragged.constant([[1., 2.], [3., 4., 5.], [6.]])
for elem in r:
    print(elem)
```

**Other info / logs**
Here is the output of the program above:

```
tf.Tensor([1. 2.], shape=(2,), dtype=float32)
tf.Tensor([3. 4. 5.], shape=(3,), dtype=float32)
tf.Tensor([6.], shape=(1,), dtype=float32)
2019-01-03 16:27:36.084103: W tensorflow/core/framework/op_kernel.cc:1408] OP_REQUIRES failed at strided_slice_op.cc:106 : Invalid argument: slice index 3 of dimension 0 out of bounds.
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/ragged/ragged_getitem.py"", line 104, in ragged_tensor_getitem
    return _ragged_getitem(self, key)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/ragged/ragged_getitem.py"", line 153, in _ragged_getitem
    row = rt_input.values[starts[row_key]:limits[row_key]]
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 654, in _slice_helper
    name=name)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 820, in strided_slice
    shrink_axis_mask=shrink_axis_mask)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 9334, in strided_slice
    _six.raise_from(_core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: slice index 3 of dimension 0 out of bounds. [Op:StridedSlice] name: RaggedGetItem/strided_slice/
```"
24677,GPU peer-to-peer access,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): NVIDIA DOCKER Image
- TensorFlow version (use command below): v1.11.0-0-gc19e29306c 1.11.0
- Python version: 3.5.2
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: NVIDIA DOCKER Image
- GPU model and memory: RTX 2080Ti (4 pcs.)


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
Hello, I'm trying to utilize 4 RTX2080Ti GPUs at once. My mainboard is ASUS X99-E WS and my CPU is i7-6850K. However, tensorflow does not show peer-to-peer accessibility. I checked `nvidia-smi topo -m` and it seems every GPUs is able to use peer-to-peer access. Is there any opthons I have to modify before using tensorflow?


```Python
# tensorflow session output
2019-01-03 07:07:29.683339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0, 1, 2, 3
2019-01-03 07:07:31.374026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-01-03 07:07:31.374063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 1 2 3 
2019-01-03 07:07:31.374070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N N N N 
2019-01-03 07:07:31.374075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 1:   N N N N 
2019-01-03 07:07:31.374079: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 2:   N N N N 
2019-01-03 07:07:31.374084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 3:   N N N N 
2019-01-03 07:07:31.374707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9854 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:05:00.0, compute capability: 7.5)
2019-01-03 07:07:31.468037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10165 MB memory) -> physical GPU (device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:06:00.0, compute capability: 7.5)
2019-01-03 07:07:31.564214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10165 MB memory) -> physical GPU (device: 2, name: GeForce RTX 2080 Ti, pci bus id: 0000:09:00.0, compute capability: 7.5)
2019-01-03 07:07:31.660232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 10165 MB memory) -> physical GPU (device: 3, name: GeForce RTX 2080 Ti, pci bus id: 0000:0a:00.0, compute capability: 7.5)

# nvidia-smi
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.78       Driver Version: 410.78       CUDA Version: 10.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce RTX 208...  Off  | 00000000:05:00.0  On |                  N/A |
| 30%   34C    P8     1W / 250W |    500MiB / 10988MiB |      1%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce RTX 208...  Off  | 00000000:06:00.0 Off |                  N/A |
| 30%   34C    P8    21W / 250W |      0MiB / 10989MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  GeForce RTX 208...  Off  | 00000000:09:00.0 Off |                  N/A |
| 30%   29C    P8    22W / 250W |      0MiB / 10989MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  GeForce RTX 208...  Off  | 00000000:0A:00.0 Off |                  N/A |
| 29%   27C    P8    12W / 250W |      0MiB / 10989MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+


# nvidia-smi topo -m output
	GPU0	GPU1	GPU2	GPU3	CPU Affinity
GPU0	 X 	PIX	PHB	PHB	0-11
GPU1	PIX	 X 	PHB	PHB	0-11
GPU2	PHB	PHB	 X 	PIX	0-11
GPU3	PHB	PHB	PIX	 X 	0-11

Legend:

  X    = Self
  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node
  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
  PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)
  PIX  = Connection traversing a single PCIe switch
  NV#  = Connection traversing a bonded set of # NVLinks


# lspci -t output
-+-[0000:ff]-+-0b.0
 |           +-0b.1
 |           +-0b.2
 |           +-0b.3
 |           +-0c.0
 |           +-0c.1
 |           +-0c.2
 |           +-0c.3
 |           +-0c.4
 |           +-0c.5
 |           +-0f.0
 |           +-0f.1
 |           +-0f.4
 |           +-0f.5
 |           +-0f.6
 |           +-10.0
 |           +-10.1
 |           +-10.5
 |           +-10.6
 |           +-10.7
 |           +-12.0
 |           +-12.1
 |           +-13.0
 |           +-13.1
 |           +-13.2
 |           +-13.3
 |           +-13.4
 |           +-13.5
 |           +-13.6
 |           +-13.7
 |           +-14.0
 |           +-14.1
 |           +-14.2
 |           +-14.3
 |           +-14.4
 |           +-14.5
 |           +-14.6
 |           +-14.7
 |           +-15.0
 |           +-15.1
 |           +-15.2
 |           +-15.3
 |           +-16.0
 |           +-16.6
 |           +-16.7
 |           +-17.0
 |           +-17.4
 |           +-17.5
 |           +-17.6
 |           +-17.7
 |           +-1e.0
 |           +-1e.1
 |           +-1e.2
 |           +-1e.3
 |           +-1e.4
 |           +-1f.0
 |           \-1f.2
 \-[0000:00]-+-00.0
             +-01.0-[01]--
             +-01.1-[02]--
             +-02.0-[07-0a]----00.0-[08-0a]--+-08.0-[0a]--+-00.0     // GPU 0
             |                               |            +-00.1
             |                               |            +-00.2
             |                               |            \-00.3
             |                               \-10.0-[09]--+-00.0     // GPU 1
             |                                            +-00.1
             |                                            +-00.2
             |                                            \-00.3
             +-03.0-[03-06]----00.0-[04-06]--+-08.0-[06]--+-00.0     // GPU 2
             |                               |            +-00.1
             |                               |            +-00.2
             |                               |            \-00.3
             |                               \-10.0-[05]--+-00.0     // GPU 3
             |                                            +-00.1
             |                                            +-00.2
             |                                            \-00.3
             +-05.0
             +-05.1
             +-05.2
             +-05.4
             +-11.0
             +-14.0
             +-16.0
             +-19.0
             +-1a.0
             +-1b.0
             +-1c.0-[0b]--
             +-1c.3-[0c-0f]----00.0-[0d-0f]--+-01.0-[0e]----00.0
             |                               \-05.0-[0f]----00.0
             +-1c.4-[10]----00.0
             +-1c.6-[11]----00.0
             +-1d.0
             +-1f.0
             +-1f.2
             \-1f.3

```
"
24676,NNAPI doesn't support tensors with rank 0 - LeakyRelu?,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04 + Android 8.1
- TensorFlow installed from (source or binary): Anaconda  
- TensorFlow version (or github SHA if from source): tensorflow 1.12.0 + keras 2.2.4 for converting model using tflite_convert and tensorflow-lite:1.12.0 in Android gradle build config

**Provide the text output from tflite_convert**

```
(update_36) m@m-hp:~/repos/keras_ocr_preproc$ tflite_convert --output_file=android/some_name.tflite --keras_model_file=android/some_name.hdf5
2019-01-03 01:45:35.173445: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-03 01:45:35.178794: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.
```
..which sounds OK.


**Any other info / logs**

Log from Android 8.1, tensorflow-lite 1.12.0 device: Alcatel 1 dual sim:
```
E/tflite: NNAPI doesn't support tensors with rank 0 (index 16 name class_2/LeakyRelu/alpha)
    Returning error since TFLite returned failure nnapi_delegate.cc:742.
    Failed to build graph for NNAPI
E/AndroidRuntime: FATAL EXCEPTION: DefaultFallbackThread
    Process: eu.yesse.readerdemo.release, PID: 27460
    java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: @
```

It crashes only when setUseNNAPI is set to true (I know - usage is deprecated in tflite:1.12.0). 
When setUseNNAPI is commented, it seems that fallback to CPU happens and everything works, but slow. 
Also for other Androids <=8.0, it works extremaly slow comparing to evaluation of the same model on iOS devices using CoreML. 
"
24672,Incorrect relative include paths in include/tensorflow/core/framework/op_def.pb.h when building external c++ files using Windows,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: Windows 10
- TensorFlow installed from: binary
- TensorFlow version: 1.12.0
- Python version: 3.6.6
- Installed using: pip
- CUDA/cuDNN version: 9.2/7
- GPU model and memory: GeForce GTX 1050 Ti


**Describe the problem**
NOTE this error is not in building Tensorflow, but in building an external library that needs to include Tensorflow, and we believe the problem may be internal to Tensorflow. Apologies if this is in the wrong place.

Hello, I am no expert in Tensorflow or C++ but will try to describe the problem as best as I can. When trying to build an external package that contains .cpp files that need to include Tensorflow the C++ compiler (Visual Studio cl.exe) throws an error as the file Anaconda3\lib\site-packages\tensorflow\include\tensorflow/core/framework/op_def.pb.h cannot find the relevant files in the include directory. It is looking for:
- google/protobuf/stubs/common.h
- unsupported/Eigen/CXX11/Tensor
- absl/strings/string_view.h

The reason I believe this is is that the relative paths have changed on tensorflow 1.12, these files exist but are now stored in different paths. I have tried adding more include paths but then I get further errors and this fix doesn't feel like the correct way to go. 

On older versions of tensorflow (tested 1.4) these include files are at the correct relative paths (as stated above) to tensorflow's include directory

**Provide the exact sequence of commands / steps that you executed before running into the problem**
>> D:\programs_hdd\visual_studio\VC\Tools\MSVC\14.13.26128\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Users\peter\Anaconda3\lib\site-packages\numpy\core\include -IC:\Users\peter\Anaconda3\lib\site-packages\tensorflow\include -ID:\documents\work\bibliotecas_terceiro\tensorflow_qrnn\src -ID:\documents\work\bibliotecas_terceiro\tensorflow_qrnn\src\third_party ""-IC:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.2\include"" ""-IC:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.2\include\cuda"" -IC:\Users\peter\Anaconda3\Lib\site-packages\tensorflow\include\third_party\eigen3 -IC:\Users\peter\Anaconda3\Lib\site-packages\tensorflow\include\external\com_google_absl -IC:\Users\peter\Anaconda3\include -IC:\Users\peter\Anaconda3\include -ID:\programs_hdd\visual_studio\VC\Tools\MSVC\14.13.26128\ATLMFC\include -ID:\programs_hdd\visual_studio\VC\Tools\MSVC\14.13.26128\include ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.16299.0\ucrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.16299.0\shared"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.16299.0\um"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.16299.0\winrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.16299.0\cppwinrt"" /EHsc /TpD:\documents\work\bibliotecas_terceiro\tensorflow_qrnn\src\fo_pool_op_cpu.cpp /Fobuild\temp.win-amd64-3.6\Release\documents\work\bibliotecas_terceiro\tensorflow_qrnn\src\fo_pool_op_cpu.obj

>> fo_pool_op_cpu.cpp
>> C:\Users\peter\Anaconda3\lib\site-packages\tensorflow\include\tensorflow/core/framework/op_def.pb.h(9): fatal error C1083: Cannot open include file: 'google/protobuf/stubs/common.h': No such file or directory
error: command 
>> 'D:\\programs_hdd\\visual_studio\\VC\\Tools\\MSVC\\14.13.26128\\bin\\HostX86\\x64\\cl.exe' failed with exit status 2

Thank you
"
24671,alternative for tf.contrib.losses.metric_learning.triplet_semihard_loss,"Hi, 
I read that tf contrib is deprecated soon, is there an alternative for triplet loss that will remain in the api?"
24670,AddSymbolicGradients segfaults internally C++ API,"**System information**
- Have I written custom code:
- Linux Ubuntu 16.04
- TensorFlow installed from source:
- TensorFlow version: r1.13 #24493 
- CPU only
- Bazel version: Build label: 0.21.0
- GCC/Compiler version: gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
- memory: 6GB

**Describe the current behavior**
AddSymbolicGradients call results in a segmentation fault.

**Describe the expected behavior**
AddSymbolicGradients should return grads or fail safely within TF_CHECK_OK

**Code to reproduce the issue**
```
auto input_slices = Split(scope, 1, x, window_size);

auto initial_state = Fill(scope, {batch_size, state_size}, 0);

vector<Output> states;
states.reserve(window_size+1);
states.push_back(initial_state);

for (int i=0; i!=window_size; i++) {
    auto concat = Concat(scope, InputList(initializer_list<Input>{input_slices[i], states[i]}), 1);
    auto new_state = Tanh(scope, Add(scope, MatMul(scope, concat, w_rnn), b_rnn));
    states.push_back(new_state);
}

// dense output
auto out = Tanh(scope, Add(scope, MatMul(scope, states[window_size], w_dense), b_dense));

// loss function
auto loss = ReduceMean(scope, Square(scope, Sub(scope, out, y)), {0, 1});

vector<Output> grad_outputs;
TF_CHECK_OK(AddSymbolicGradients(scope, {loss}, {w_rnn, w_dense, b_rnn, b_dense}, &grad_outputs));
```

**Other info / logs**
gdb where command output:

> (gdb) where
> #0  0x00005555559b45aa in tensorflow::(anonymous namespace)::SymbolicGradientBuilder::Initialize() ()
> #1  0x00005555559b6f1c in tensorflow::AddSymbolicGradients(tensorflow::Scope const&, std::vector<tensorflow::Output, std::allocator<tensorflow::O
> utput> > const&, std::vector<tensorflow::Output, std::allocator<tensorflow::Output> > const&, std::vector<tensorflow::Output, std::allocator<tens
> orflow::Output> > const&, std::vector<tensorflow::Output, std::allocator<tensorflow::Output> >*) ()
> #2  0x00005555559b9bb2 in tensorflow::AddSymbolicGradients(tensorflow::Scope const&, std::vector<tensorflow::Output, std::allocator<tensorflow::O
> utput> > const&, std::vector<tensorflow::Output, std::allocator<tensorflow::Output> > const&, std::vector<tensorflow::Output, std::allocator<tens
> orflow::Output> >*) ()
> #3  0x000055555597ce6c in Rnn::train(Dataset, int, int, int) ()
> #4  0x00005555558410d6 in main ()

Variables scope nor vectors should be the issue here. i've tried to unfold the loop without luck. Any hint? "
24669,no such package '@png_archive//,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): 'Build from source on Windows' TensorFlow Official Website
- TensorFlow version: 
- Python version: 3.6.5
- Installed using virtualenv? pip? conda?: using cmd.exe
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
I was trying to build TensorFlow from source in order to solve ""Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2"" problem.
Although I installed Bazel properly and added necessary environment variables, everytime I tried to build pip packages, which is ""bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package"", an error called ""Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@png_archive//'"" always occurs.
I also referred to some issues on the same problem, but none of them solved my problem. Is there anything I need to do? Or should I just install it through pip and ignore ""Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2"" this message?

**Provide the exact sequence of commands / steps that you executed before running into the problem**
C:\tensorflow>python ./configure.py
WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
nul
WARNING: Running Bazel server needs to be killed, because the startup options are different.
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
INFO: Invocation ID: 2c23affd-49a0-4cde-94b2-236aa0a37d76
You have bazel 0.21.0 installed.
Please specify the location of python. [Default is C:\Users\wltjd\AppData\Local\Programs\Python\Python36\python.exe]:


Found possible Python library paths:
  C:\Users\wltjd\AppData\Local\Programs\Python\Python36\lib\site-packages
Please input the desired Python library path to use.  Default is [C:\Users\wltjd\AppData\Local\Programs\Python\Python36\lib\site-packages]

Do you wish to build TensorFlow with XLA JIT support? [y/N]: y
XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: N
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: N
No CUDA support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:
Eigen strong inline overridden.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=gdr            # Build with GDR support.
        --config=verbs          # Build with libverbs support.
        --config=ngraph         # Build with Intel nGraph support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=noignite       # Disable Apacha Ignite support.
        --config=nokafka        # Disable Apache Kafka support.
        --config=nonccl         # Disable NVIDIA NCCL support.

C:\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
c:\tensorflow/.bazelrc
Starting local Bazel server and connecting to it...
WARNING: Option 'experimental_shortened_obj_file_path' is deprecated
INFO: Invocation ID: 61281b1e-9316-4ac5-9241-238712f7de45
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@png_archive//': Traceback (most recent call last):
        File ""C:/tensorflow/third_party/repo.bzl"", line 106
                _apply_patch(ctx, ctx.attr.patch_file)
        File ""C:/tensorflow/third_party/repo.bzl"", line 73, in _apply_patch
                _execute_and_check_ret_code(ctx, cmd)
        File ""C:/tensorflow/third_party/repo.bzl"", line 52, in _execute_and_check_ret_code
                fail(""Non-zero return code({1}) when ...))
Non-zero return code(256) when executing '?C:\msys64_2\usr\bin\bash.exe -l -c ""patch"" ""-p1"" ""-d"" ""C:/users/wltjd/_bazel_wltjd/xv6zejqw/external/png_archive"" ""-i"" ""C:/tensorflow/third_party/png_fix_rpi.patch""':
Stdout:
Stderr: java.io.IOException: ERROR: src/main/native/windows/processes-jni.cc(383): CreateProcessW(""C:\users\wltjd\_bazel_wltjd\xv6zejqw\external\png_archive\?C:\msys64_2\usr\bin\bash.exe"" -l -c ""\""patch\"" \""-p1\"" \""-d\"" \""C:/users/wltjd/_bazel_wltjd/xv6zejqw/external/png_archive\"" \""-i\"" \""C:(...)): ??? ??? ?? ? ????.
INFO: Elapsed time: 14.881s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (319 packages loaded, 8559 targets configured)
    Fetching @swig; fetching 8s
    Fetching @grpc; fetching 7s
    Fetching @cython; fetching 7s
    Fetching @eigen_archive; fetching 6s
    Fetching @llvm; fetching 5s
    Fetching @icu; fetching 4s
    Fetching @boringssl; fetching 4s
    Fetching @png_archive; fetching

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24668,Quantized tflite model has large sized apk than unquantized,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

 I have trained inceptionV3 classification model with custom dataset and got retrained.pb and retrained_labels.txt. I want to use the model in mobile phone and I converted the model using the following code
```
import tensorflow as tf
graph_def_file = ""retrained_graph.pb""
input_arrays = [""Mul""]
output_arrays = [""final_result""]
converter = tf.lite.TFLiteConverter.from_frozen_graph(
  graph_def_file,input_arrays,output_arrays)
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)
```
**Describe the current behavior**
I got the tflite model which is of size 85 MB and built an apk file which comes with size 113 MB and I quantized the model with following command

```
tflite_convert \
  --output_file=foo.tflite \
  --graph_def_file=retrained_graph.pb \
  --inference_type=QUANTIZED_UINT8 \
  --input_arrays=Mul \
  --output_arrays=final_result \
  --mean_values=128 \
  --std_dev_values=127 \
  --default_ranges_min=0 \
  --default_ranges_max=6 
```
It gives me a tflite model of size 21 MB and when i use this quantized model to build an apk it generates an apk file of size 135 MB
**Describe the expected behavior**
1. Why the size of the apk file is increased when i use the quantized model ?

2. How to reduce the apk size built with tflite model ?

"
24667,InvalidArgumentError when use Keras loss function categorical_crossentropy,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.12.0
- Python version: python3.6
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): 0.17.2
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.0
- GPU model and memory: Tesla K20c



**Describe the problem**
When I build model with tensorflow.keras loss function **sparse_categorical_crossentropy**, the model run very well. But I try use **categorical_crossentropy**, I get error **InvalidArgumentError: Incompatible shapes: [2] vs. [0]**

**My code:**

```
BATCH_SIZE = 64
IMG_SIZE = 98 
N_CLASSES = 179   
N_EPOCHS = 5

model = Sequential([
        Conv2D(filters=32, kernel_size=3, strides=1, activation=tf.nn.relu, input_shape=(IMG_SIZE, IMG_SIZE, 1), padding=""same""),
	AveragePooling2D(pool_size=(2, 2), strides=2, padding=""same""),
	Conv2D(filters=64, kernel_size=3, strides=1, activation=tf.nn.relu, padding=""same""),
	AveragePooling2D(pool_size=(2, 2), padding=""same""),
	Conv2D(filters=64, kernel_size=3, strides=1, activation=tf.nn.relu, padding=""same""),
	AveragePooling2D(pool_size=(2, 2), strides=2, padding=""same""),
	Flatten(),
	Dense(units=1024, activation=tf.nn.relu),
	Dropout(rate=0.8),
	Dense(units=N_CLASSES, activation=tf.nn.softmax)
])

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

print(model.summary())

train_data = train_data.reshape(-1, IMG_SIZE, IMG_SIZE, 1)
val_data = val_data.reshape(-1, IMG_SIZE, IMG_SIZE, 1)
test_data = test_data.reshape(-1, IMG_SIZE, IMG_SIZE, 1)

train_label = to_categorical(train_label, N_CLASSES)
val_label = to_categorical(val_label, N_CLASSES)
test_label = to_categorical(test_label, N_CLASSES)
model.fit(train_data, train_label, batch_size=BATCH_SIZE, epochs=N_EPOCHS, validation_data=(val_data, val_label))
```
**Any other info / logs**
```
Train on 803 samples, validate on 98 samples
Epoch 1/100

---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-18-30f90ff839f0> in <module>
----> 1 model.fit(train_data, train_label, batch_size=BATCH_SIZE, epochs=N_EPOCHS, validation_data=(val_data, val_label))

c:\users\ktmt\tensor-gpu\lib\site-packages\tensorflow\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)
   1637           initial_epoch=initial_epoch,
   1638           steps_per_epoch=steps_per_epoch,
-> 1639           validation_steps=validation_steps)
   1640 
   1641   def evaluate(self,

c:\users\ktmt\tensor-gpu\lib\site-packages\tensorflow\python\keras\engine\training_arrays.py in fit_loop(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)
    213           ins_batch[i] = ins_batch[i].toarray()
    214 
--> 215         outs = f(ins_batch)
    216         if not isinstance(outs, list):
    217           outs = [outs]

c:\users\ktmt\tensor-gpu\lib\site-packages\tensorflow\python\keras\backend.py in __call__(self, inputs)
   2984 
   2985     fetched = self._callable_fn(*array_vals,
-> 2986                                 run_metadata=self.run_metadata)
   2987     self._call_fetch_callbacks(fetched[-len(self._fetches):])
   2988     return fetched[:len(self.outputs)]

c:\users\ktmt\tensor-gpu\lib\site-packages\tensorflow\python\client\session.py in __call__(self, *args, **kwargs)
   1437           ret = tf_session.TF_SessionRunCallable(
   1438               self._session._session, self._handle, args, status,
-> 1439               run_metadata_ptr)
   1440         if run_metadata:
   1441           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

c:\users\ktmt\tensor-gpu\lib\site-packages\tensorflow\python\framework\errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)
    526             None, None,
    527             compat.as_text(c_api.TF_Message(self.status.status)),
--> 528             c_api.TF_GetCode(self.status.status))
    529     # Delete the underlying status object from memory otherwise it stays alive
    530     # as there is a reference to status from this from the traceback due to

InvalidArgumentError: Incompatible shapes: [2] vs. [0]
	 [[{{node training/Adam/gradients/loss/dense_1_loss/Sum_grad/floordiv}} = FloorDiv[T=DT_INT32, _class=[""loc:@training/Adam/gradients/loss/dense_1_loss/Sum_grad/Tile""], _device=""/job:localhost/replica:0/task:0/device:GPU:0""](training/Adam/gradients/loss/dense_1_loss/Sum_grad/Shape, training/Adam/gradients/loss/dense_1_loss/Sum_grad/Maximum)]]
	 [[{{node loss/dense_1_loss/broadcast_weights/assert_broadcastable/AssertGuard/Assert/Switch_2/_127}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_343_l...t/Switch_2"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
```
Thanks!
"
24665,why invoke() take so much time? use pos_training_quantized=True to convert pb into a tflite model.,"**MY QUESTION**

Training a model with FLOAT-32 and converts into a saved model. Then I use **'pos_training_quantized=True '** to generate .tflite model (int8). 
according to official website: https://www.tensorflow.org/lite/performance/post_training_quantization
```
At inference, weights are converted from 8-bits of precision to floating-point and computed using floating point kernels. This conversion is done once and cached to reduce latency.
```
However, at inference, tflite model costs more than pb, and almost costs in **invoke()** op. now ,the speed is much slow than saved model. Why this happens, the latency is not reduced like the official description.

Can anyone explain? or pointing out my coding faults and any procedure faults about using tflite to inference.
**Appreciate much!!**

**System information**
- Linux Ubuntu 16.04
- TensorFlow installed from: source
- TensorFlow version: 1.12
- GPU: Tesla P4 (server)
- CPU: Intel Core i5-5300U CPU @ 2.30GHz  4 (local)

**Provide the text output from tflite_convert**

```from __future__ import print_function
from __future__ import absolute_import
from __future__ import division

import tensorflow as tf
import numpy as np
import os
import cv2
import time
from scipy import misc


# convert saved model to tflite
saved_model_path = '/home/tom/PycharmProjects/transwarp/wxn/tmp/1'
tflite_model_path = '/home/tom/PycharmProjects/transwarp/wxn/post_quantized_model.tflite'
images_root_path = '/home/tom/PycharmProjects/transwarp/wxn/combine_train'
val_file = '/home/tom/PycharmProjects/transwarp/wxn/combine_train/val1.txt'
tfrecord_dir = '/home/tom/PycharmProjects/transwarp/wxn/combine_train/combine_train_tfrecord'


def post_training_convert(tag_set, signature_key, tflite_model_path):
    converter = tf.contrib.lite.TocoConverter.from_saved_model(saved_model_path, tag_set = tag_set, signature_key = signature_key)
    converter.post_training_quantize = True
    tflite_post_quantized_model = converter.convert()
    open(tflite_model_path, 'wb').write(tflite_post_quantized_model)
    print('tflite model transformation done!')
    print('format: ', converter.inference_type, 'post_training_quantize: ', str(converter.post_training_quantize))

post_training_convert(['SERVING'],'predict_signature',tflite_model_path)

def image_preprocess(image, resized_height, resized_width):
    image = tf.image.resize_images(image, (resized_height, resized_width))
    image = tf.image.per_image_standardization(image)
    image = tf.cast(image, tf.float32)
    image = tf.expand_dims(image, axis = 0)
    return image

def main():
    """"""
    use ./lite model to predict
    """"""
    with tf.Graph().as_default() as g:
        with tf.Session() as sess:
            interpreter = tf.contrib.lite.Interpreter(model_path = tflite_model_path)
            interpreter.allocate_tensors()

            input_details = interpreter.get_input_details()
            output_details = interpreter.get_output_details()
            print('intput_details: ', input_details)
            print('output_details: ', output_details)

            input_shape = input_details[0]['shape']
            print('input_shape: ', input_shape)
            val_file_list = open(val_file, 'r').readlines()

            count = 0
            time_count = []
            f = open('/home/tom/PycharmProjects/transwarp/wxn/result.txt', 'a')

            for file in val_file_list:
                file_name, file_label = file.strip('\n').split()
                image = cv2.imread(os.path.join(images_root_path, file_name))
                if image is None:
                    print('Warning, fail to read image: ', file_name)
                    continue

                image = image_preprocess(image, 224, 224)

                image_array = sess.run(image)
                start_time = time.time()
                start_time1 = time.time()
                interpreter.set_tensor(input_details[0]['index'], image_array)
                print('set time: ', time.time()-start_time1)
                start_time2 = time.time()
                interpreter.invoke()
                print('invoke time: ', time.time()-start_time2)
                start_time3 = time.time()
                output_data = interpreter.get_tensor(output_details[0]['index'])
                print('out time: ', time.time()-start_time3)
                print(output_data)
                end_time = time.time()
                duration = end_time - start_time
                count += 1
                time_count.append(duration)
                f.write(file_name + ' ' +
                        file_label +' ' +
                        str(output_data[0][0])+' ' +
                        str(round(duration, 4)) + '\n')
            print('total processing images: ', count)
            print('average processing time: ', np.mean(time_count))
    f.close()
main()



```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**
      **inference 25 images, just list some results.**
```
set time: cost of 'interpreter.set_tensor() '
invoke time: cost of 'interpreter.invoke()'
out_time : cost of 'interpreter.get_tensor()'
```
#In GPU, the inference time
set time:  0.000224828720093
invoke time:  7.69320392609
out time:  2.00271606445e-05
[[4]]
set time:  0.000519990921021
invoke time:  7.68318390846
out time:  2.88486480713e-05
[[4]]
set time:  0.000233888626099
invoke time:  7.56485295296
out time:  1.90734863281e-05
[[4]]
set time:  0.000152111053467
invoke time:  7.75410199165
out time:  2.71797180176e-05
[[4]]
total processing images:  25
average processing time:  7.540134401321411
********************************
#In CPU, the inference time
set time:  7.772445678710938e-05
invoke time:  7.645012378692627
out time:  1.52587890625e-05
[[4]]
set time:  9.489059448242188e-05
invoke time:  7.672791957855225
out time:  1.9788742065429688e-05
[[4]]
set time:  0.00010085105895996094
invoke time:  7.442874908447266
out time:  1.52587890625e-05
[[4]]
set time:  7.891654968261719e-05
invoke time:  7.478809833526611
out time:  1.7881393432617188e-05
[[4]]
total processing images:  25
average processing time:  7.6469355392456055

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24663,TFLite App error with SSD_MobileNet V2 Quantized 300*300 CoCo,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Nexus 6 and Moto G5sPlus
- TensorFlow installed from (source or binary): binary (pip)
- TensorFlow version (use command below): 1.12.0
- Python version: 3.5.5 (anaconda)
- Bazel version (if compiling from source):N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version:N/A
- GPU model and memory: 8GB Tesla K80 on VM


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
TFLite app crashes with log shown below.

**Describe the expected behavior**
Runs and detects objects smoothly
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
[Tensorflow lite convertion tutorial](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md) and used android source code from [Tensorflow/lite/Java/Demo](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo), Changed classifier to ```ImageClassifierQuantizedMobileNet``` for Quantized model and ```ImageClassifierFloatMobileNet``` for Float model.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Used steps as on [Tensorflow lite convertion tutorial](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md), I generated both UNIT8 and FLOAT type tflite models. I've used [SSD_MobileNet_V2_COCO](https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/ssd_mobilenet_v2_quantized_300x300_coco.config) to train my dataset for object detection.
Following are the errors I get while trying to build for android:

**Using Quantized (UINT8)**

```
01-02 14:41:39.721 5697-5697/? E/SchedPolicy: open of /dev/cpuctl/bg_non_interactive/tasks failed: No such file or directory
01-02 14:41:39.727 5697-5697/? I/zygote: Late-enabling -Xcheck:jni
01-02 14:41:39.789 5697-5707/? E/zygote: Failed sending reply to debugger: Broken pipe
01-02 14:41:39.790 5697-5707/? I/zygote: Debugger is no longer active
01-02 14:41:40.050 5697-5719/? D/TfLiteCameraDemo: Created a Tensorflow Lite Image Classifier.
01-02 14:41:40.069 5697-5725/? D/OpenGLRenderer: HWUI GL Pipeline
01-02 14:41:40.163 5697-5725/? I/Adreno: QUALCOMM build                   : 908a5ce, I77d3059488
    Build Date                       : 06/07/18
    OpenGL ES Shader Compiler Version: EV031.22.00.01_06
    Local Branch                     : 
    Remote Branch                    : refs/tags/AU_LINUX_ANDROID_LA.UM.6.5.R1.08.01.00.312.086
    Remote Branch                    : NONE
    Reconstruct Branch               : NOTHING
01-02 14:41:40.163 5697-5725/? D/vndksupport: Loading /vendor/lib/hw/gralloc.msm8953.so from current namespace instead of sphal namespace.
01-02 14:41:40.168 5697-5725/? I/Adreno: PFP: 0x005ff087, ME: 0x005ff063
01-02 14:41:40.173 5697-5725/? I/zygote: android::hardware::configstore::V1_0::ISurfaceFlingerConfigs::hasWideColorDisplay retrieved: 0
01-02 14:41:40.174 5697-5725/? I/OpenGLRenderer: Initialized EGL, version 1.4
01-02 14:41:40.174 5697-5725/? D/OpenGLRenderer: Swap behavior 2
01-02 14:41:40.258 5697-5705/? I/zygote: Do partial code cache collection, code=28KB, data=30KB
    After code cache collection, code=28KB, data=30KB
    Increasing code cache capacity to 128KB
01-02 14:41:40.395 5697-5705/? I/zygote: Do partial code cache collection, code=62KB, data=58KB
01-02 14:41:40.396 5697-5705/? I/zygote: After code cache collection, code=62KB, data=58KB
    Increasing code cache capacity to 256KB
01-02 14:41:40.523 5697-5697/? I/CameraManagerGlobal: Connecting to camera service
01-02 14:41:40.682 5697-5725/? D/vndksupport: Loading /vendor/lib/hw/android.hardware.graphics.mapper@2.0-impl.so from current namespace instead of sphal namespace.
01-02 14:41:40.683 5697-5725/? D/vndksupport: Loading /vendor/lib/hw/gralloc.msm8953.so from current namespace instead of sphal namespace.
01-02 14:41:40.746 5697-5724/? D/TfLiteCameraDemo: Timecost to put values into ByteBuffer: 67
01-02 14:41:41.101 5697-5724/android.example.com.tflitecamerademo E/AndroidRuntime: FATAL EXCEPTION: CameraBackground
    Process: android.example.com.tflitecamerademo, PID: 5697
    java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite tensor with type FLOAT32 and a Java object of type [[B (which is compatible with the TensorFlowLite type UINT8).
        at org.tensorflow.lite.Tensor.throwExceptionIfTypeIsIncompatible(Tensor.java:261)
        at org.tensorflow.lite.Tensor.copyTo(Tensor.java:141)
        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:161)
        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:275)
        at org.tensorflow.lite.Interpreter.run(Interpreter.java:249)
        at com.example.android.tflitecamerademo.ImageClassifierQuantizedMobileNet.runInference(ImageClassifierQuantizedMobileNet.java:95)
        at com.example.android.tflitecamerademo.ImageClassifier.classifyFrame(ImageClassifier.java:126)
        at com.example.android.tflitecamerademo.Camera2BasicFragment.classifyFrame(Camera2BasicFragment.java:815)
        at com.example.android.tflitecamerademo.Camera2BasicFragment.-wrap0(Unknown Source:0)
        at com.example.android.tflitecamerademo.Camera2BasicFragment$4.run(Camera2BasicFragment.java:710)
        at android.os.Handler.handleCallback(Handler.java:790)
        at android.os.Handler.dispatchMessage(Handler.java:99)
        at android.os.Looper.loop(Looper.java:164)
        at android.os.HandlerThread.run(HandlerThread.java:65)
01-02 14:41:41.806 5697-5697/? I/Choreographer: Skipped 73 frames!  The application may be doing too much work on its main thread.
01-02 14:41:41.809 5697-5707/? I/zygote: Ignoring second debugger -- accepting and dropping
```


**Using Float type**
```
01-02 14:39:44.097 5364-5364/? E/SchedPolicy: open of /dev/cpuctl/bg_non_interactive/tasks failed: No such file or directory
01-02 14:39:44.101 5364-5364/? I/zygote: Late-enabling -Xcheck:jni
01-02 14:39:44.164 5364-5371/? E/zygote: Failed sending reply to debugger: Broken pipe
01-02 14:39:44.165 5364-5371/? I/zygote: Debugger is no longer active
01-02 14:39:44.408 5364-5387/? D/TfLiteCameraDemo: Created a Tensorflow Lite Image Classifier.
01-02 14:39:44.427 5364-5395/? D/OpenGLRenderer: HWUI GL Pipeline
01-02 14:39:44.520 5364-5395/? I/Adreno: QUALCOMM build                   : 908a5ce, I77d3059488
    Build Date                       : 06/07/18
    OpenGL ES Shader Compiler Version: EV031.22.00.01_06
    Local Branch                     : 
    Remote Branch                    : refs/tags/AU_LINUX_ANDROID_LA.UM.6.5.R1.08.01.00.312.086
    Remote Branch                    : NONE
    Reconstruct Branch               : NOTHING
01-02 14:39:44.521 5364-5395/? D/vndksupport: Loading /vendor/lib/hw/gralloc.msm8953.so from current namespace instead of sphal namespace.
01-02 14:39:44.526 5364-5395/? I/Adreno: PFP: 0x005ff087, ME: 0x005ff063
01-02 14:39:44.532 5364-5395/? I/zygote: android::hardware::configstore::V1_0::ISurfaceFlingerConfigs::hasWideColorDisplay retrieved: 0
01-02 14:39:44.532 5364-5395/? I/OpenGLRenderer: Initialized EGL, version 1.4
01-02 14:39:44.533 5364-5395/? D/OpenGLRenderer: Swap behavior 2
01-02 14:39:44.615 5364-5369/? I/zygote: Do partial code cache collection, code=28KB, data=30KB
01-02 14:39:44.616 5364-5369/? I/zygote: After code cache collection, code=28KB, data=30KB
    Increasing code cache capacity to 128KB
01-02 14:39:44.752 5364-5369/? I/zygote: Do partial code cache collection, code=62KB, data=58KB
    After code cache collection, code=62KB, data=58KB
    Increasing code cache capacity to 256KB
01-02 14:39:44.865 5364-5364/? I/CameraManagerGlobal: Connecting to camera service
01-02 14:39:44.988 5364-5395/? D/vndksupport: Loading /vendor/lib/hw/android.hardware.graphics.mapper@2.0-impl.so from current namespace instead of sphal namespace.
01-02 14:39:44.989 5364-5395/? D/vndksupport: Loading /vendor/lib/hw/gralloc.msm8953.so from current namespace instead of sphal namespace.
01-02 14:39:45.072 5364-5392/? D/TfLiteCameraDemo: Timecost to put values into ByteBuffer: 87
01-02 14:39:45.900 5364-5392/android.example.com.tflitecamerademo E/AndroidRuntime: FATAL EXCEPTION: CameraBackground
    Process: android.example.com.tflitecamerademo, PID: 5364
    java.lang.IllegalArgumentException: Cannot copy between a TensorFlowLite tensor with shape [1, 10, 4] and a Java object with shape [1, 1].
        at org.tensorflow.lite.Tensor.throwExceptionIfTypeIsIncompatible(Tensor.java:270)
        at org.tensorflow.lite.Tensor.copyTo(Tensor.java:141)
        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:161)
        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:275)
        at org.tensorflow.lite.Interpreter.run(Interpreter.java:249)
        at com.example.android.tflitecamerademo.ImageClassifierFloatMobileNet.runInference(ImageClassifierFloatMobileNet.java:92)
        at com.example.android.tflitecamerademo.ImageClassifier.classifyFrame(ImageClassifier.java:126)
        at com.example.android.tflitecamerademo.Camera2BasicFragment.classifyFrame(Camera2BasicFragment.java:815)
        at com.example.android.tflitecamerademo.Camera2BasicFragment.-wrap0(Unknown Source:0)
        at com.example.android.tflitecamerademo.Camera2BasicFragment$4.run(Camera2BasicFragment.java:710)
        at android.os.Handler.handleCallback(Handler.java:790)
        at android.os.Handler.dispatchMessage(Handler.java:99)
        at android.os.Looper.loop(Looper.java:164)
        at android.os.HandlerThread.run(HandlerThread.java:65)
01-02 14:39:45.931 5364-5364/android.example.com.tflitecamerademo I/Choreographer: Skipped 60 frames!  The application may be doing too much work on its main thread.
```

Kindly let me know where I am wrong."
24661,tensorflow.lite.python.convert.ConverterError: TOCO failed.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04
- TensorFlow installed from (source or binary):pip install
- TensorFlow version (or github SHA if from source):1.9

**Command Used**
 tflite_convert   --output_file=detect.tflite   --graph_def_file=frozen_inference_graph.pb    --input_arrays=image_tensor   --input_shape=1,300,300,3   --output_arrays='detection_boxes','detection_scores','num_detections','detection_classes'

**Provide the text output from tflite_convert**

```
2019-01-02 06:50:15.359432: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Traceback (most recent call last):
  File ""/home/ubuntu/.local/bin/tflite_convert"", line 11, in <module>
    sys.exit(main())
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 320, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 316, in run_main
    _convert_model(tflite_flags)
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 121, in _convert_model
    output_data = converter.convert()
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/lite.py"", line 309, in convert
    allow_custom_ops=self.allow_custom_ops)
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py"", line 225, in toco_convert
    input_data.SerializeToString())
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py"", line 107, in toco_convert_protos
    (stdout, stderr))
RuntimeError: TOCO failed see console for info.
2019-01-02 06:50:19.643688: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3
2019-01-02 06:50:19.643862: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2019-01-02 06:50:19.643913: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3
2019-01-02 06:50:19.643951: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2019-01-02 06:50:19.643990: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2019-01-02 06:50:19.644022: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3
2019-01-02 06:50:19.644052: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2019-01-02 06:50:19.644085: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2019-01-02 06:50:19.644118: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2019-01-02 06:50:19.644185: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayScatterV3
2019-01-02 06:50:19.644216: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2019-01-02 06:50:19.644248: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2019-01-02 06:50:19.644282: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: LoopCond
2019-01-02 06:50:19.644317: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Exit
2019-01-02 06:50:19.644338: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArraySizeV3
2019-01-02 06:50:19.644370: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Exit
2019-01-02 06:50:19.644391: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArraySizeV3
2019-01-02 06:50:19.644420: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayReadV3
2019-01-02 06:50:19.644471: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayWriteV3
2019-01-02 06:50:19.644517: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayWriteV3
2019-01-02 06:50:19.644572: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayGatherV3
2019-01-02 06:50:19.644621: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayGatherV3
2019-01-02 06:50:19.644652: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.644674: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.644700: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.665993: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.666097: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.666417: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3
2019-01-02 06:50:19.666452: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2019-01-02 06:50:19.666479: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3
2019-01-02 06:50:19.666507: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2019-01-02 06:50:19.666531: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3
2019-01-02 06:50:19.666554: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2019-01-02 06:50:19.666588: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3
2019-01-02 06:50:19.666616: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2019-01-02 06:50:19.666640: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3
2019-01-02 06:50:19.666663: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2019-01-02 06:50:19.666690: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2019-01-02 06:50:19.666720: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3
2019-01-02 06:50:19.666744: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2019-01-02 06:50:19.666771: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2019-01-02 06:50:19.666794: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3
2019-01-02 06:50:19.666817: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2019-01-02 06:50:19.666845: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2019-01-02 06:50:19.666870: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3
2019-01-02 06:50:19.666892: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2019-01-02 06:50:19.666918: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2019-01-02 06:50:19.666940: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2019-01-02 06:50:19.667007: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayScatterV3
2019-01-02 06:50:19.667034: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2019-01-02 06:50:19.667092: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayScatterV3
2019-01-02 06:50:19.667118: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2019-01-02 06:50:19.667175: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayScatterV3
2019-01-02 06:50:19.667200: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2019-01-02 06:50:19.667261: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayScatterV3
2019-01-02 06:50:19.667285: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2019-01-02 06:50:19.667312: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2019-01-02 06:50:19.667342: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: LoopCond
2019-01-02 06:50:19.667371: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Exit
2019-01-02 06:50:19.667391: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArraySizeV3
2019-01-02 06:50:19.667418: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Exit
2019-01-02 06:50:19.667441: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArraySizeV3
2019-01-02 06:50:19.667468: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Exit
2019-01-02 06:50:19.667488: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArraySizeV3
2019-01-02 06:50:19.667513: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Exit
2019-01-02 06:50:19.667533: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArraySizeV3
2019-01-02 06:50:19.667560: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayReadV3
2019-01-02 06:50:19.667582: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayReadV3
2019-01-02 06:50:19.667606: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayReadV3
2019-01-02 06:50:19.667627: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayReadV3
2019-01-02 06:50:19.667648: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.667670: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.667691: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.667712: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.667732: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.667753: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.667774: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.667797: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.667817: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.667839: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.667859: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.667880: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.667900: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.667921: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.667941: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.667962: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.667982: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668018: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668039: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668060: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668080: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668101: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668121: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668142: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668162: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668183: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668204: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668224: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668247: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668268: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668290: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668310: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668330: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668351: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668376: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668398: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668419: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668441: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668461: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668482: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668502: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668526: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668547: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668568: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668589: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668609: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668630: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668650: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668670: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668691: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668711: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668732: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668752: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668772: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668793: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668827: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668848: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668869: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668889: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668909: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668929: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668949: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668970: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.668991: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.669011: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.669031: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.669052: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.669072: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.669093: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.669113: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.669134: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.669172: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.669194: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.669214: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.669235: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.669255: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.669276: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.669296: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.669316: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.669336: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.669357: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.669378: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.669398: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.669418: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.669439: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.669459: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.669480: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.669501: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.669543: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.669565: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.669627: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack
2019-01-02 06:50:19.669766: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Equal
2019-01-02 06:50:19.672088: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.672220: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.672312: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.672346: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.672383: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.672495: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.672586: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.672620: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.672656: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.672774: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.672863: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.672898: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.672935: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.673048: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.673134: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.673167: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.673232: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.673352: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.673439: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.673472: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.673510: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.673631: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.673719: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.673752: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.673789: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.673902: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.673997: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.674031: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.674067: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.674187: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.674290: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.674328: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.674367: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.674488: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.674579: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.674614: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.674652: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.674783: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.674894: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.674929: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.674967: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.675082: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.675206: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.675243: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.675281: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.675407: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.675496: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.675530: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.675568: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.675687: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.675772: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.675805: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.675843: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.675962: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.676048: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.676082: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.676120: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.676240: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.676329: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.676363: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.676401: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.676521: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.676609: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.676669: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.676714: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.676834: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.676919: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.676954: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.676991: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.677112: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.677203: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.677237: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.677276: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.677398: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.677485: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.677519: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.677556: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.677676: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.677763: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.677795: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.677833: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.677953: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.678036: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.678070: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.678113: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.678249: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.678343: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.678378: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.678416: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.678536: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.678624: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.678664: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.678702: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.678827: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.678916: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.678949: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.678985: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.679103: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.679186: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.679220: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.679255: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.679367: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.679449: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.679486: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.679523: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.679633: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.679719: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.679752: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.679788: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.679897: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.679980: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.680013: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.680049: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.680173: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.680257: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.680291: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.680326: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.680482: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.680571: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.680605: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.680642: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.680759: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.680876: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.680912: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.680950: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.681068: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.681156: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.681189: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.681225: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.681342: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.681427: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.681461: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.681499: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.681615: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.681704: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.681737: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.681774: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.681898: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.681985: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.682020: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.682058: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.682175: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.682279: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.682317: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.682356: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.682478: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.682566: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.682600: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.682637: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.682760: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.682857: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.682892: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.682930: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.683054: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.683141: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.683175: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.683212: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.683337: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.683426: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.683459: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.683497: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.683626: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.683713: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.683746: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.683785: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.683904: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.683991: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.684025: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.684062: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.684178: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.684267: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.684302: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.684338: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.684459: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.684546: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.684579: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.684617: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.684733: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.684821: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.684853: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.684891: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.685012: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.685100: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.685134: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.685172: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.685290: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.685376: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.685409: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.685446: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.685566: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.685651: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.685684: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.685721: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.685834: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.685922: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.685955: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.685992: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.686118: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.686221: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.686258: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.686298: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.686424: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.686515: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.686548: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.686586: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.686702: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.686787: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.686826: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.686868: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.686987: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.687070: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.687102: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.687138: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.687249: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.687334: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.687367: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.687404: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.687517: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.687601: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.687634: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.687672: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.687795: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.687880: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.687913: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.687951: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.688067: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.688156: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.688189: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.688228: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.688349: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.688437: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.688471: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.688507: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.688624: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.688711: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.688744: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.688782: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.688904: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.688991: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.689025: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.689062: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.689223: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.689313: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.689347: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.689383: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.689503: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.689590: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.689625: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.689662: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.689781: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.689871: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.689904: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.689941: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.690052: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.690135: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.690171: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.690220: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.690341: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.690432: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.690466: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.690505: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.690627: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.690715: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.690749: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.690786: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.690914: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.691005: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.691039: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.691076: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.691193: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.691280: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.691314: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.691351: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.691462: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.691553: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.691587: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.691626: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.691742: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.691830: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.691864: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.691902: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.692025: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.692112: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.692145: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.692182: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.692297: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.692385: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.692419: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.692456: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.692575: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.692663: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.692702: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.692740: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.692852: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.692934: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.692967: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.693004: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.693119: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.693207: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.693241: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.693280: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.693402: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.693493: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.693528: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.693567: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.693683: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.693770: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.693804: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.693841: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.693960: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.694048: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.694082: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.694121: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.694255: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.694347: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.694381: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.694419: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.694532: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.694618: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.694652: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.694693: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.694812: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.694901: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.694940: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.694982: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.695196: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.695284: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.695318: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.695357: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.695474: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.695561: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.695595: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.695633: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.695752: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.695840: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.695873: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.695909: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.696021: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.696111: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.696145: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.696182: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.696294: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.696380: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.696414: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.696451: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.696566: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.696652: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.696686: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.696727: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.696863: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.696952: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.696985: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.697022: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.697138: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.697221: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.697253: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.697289: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.697402: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where
2019-01-02 06:50:19.697489: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2
2019-01-02 06:50:19.697523: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike
2019-01-02 06:50:19.697659: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Size
2019-01-02 06:50:19.697708: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Equal
2019-01-02 06:50:19.698007: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayWriteV3
2019-01-02 06:50:19.698241: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayWriteV3
2019-01-02 06:50:19.698460: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayWriteV3
2019-01-02 06:50:19.698516: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayWriteV3
2019-01-02 06:50:19.698569: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayGatherV3
2019-01-02 06:50:19.698610: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayGatherV3
2019-01-02 06:50:19.698649: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayGatherV3
2019-01-02 06:50:19.698684: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayGatherV3
2019-01-02 06:50:20.238716: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 4322 operators, 7127 arrays (0 quantized)
2019-01-02 06:50:20.644583: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 4296 operators, 7080 arrays (0 quantized)
2019-01-02 06:50:21.296853: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 4296 operators, 7080 arrays (0 quantized)
2019-01-02 06:50:21.959362: F tensorflow/contrib/lite/toco/graph_transformations/resolve_tensorflow_switch.cc:95] Check failed: other_op->type == OperatorType::kTensorFlowMerge
Aborted (core dumped)

None

```

Also, please include a link to a GraphDef or the model if possible.
**Graph Used**
I used the frozen  graph [ssd_inception_v2_coco](http://download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2018_01_28.tar.gz)

**Any other info / logs**
I have tried with ssdlite_mobilenet_v2_coco_2018_05_09 also.Please suggest a object detection model which can be converted to tflite.


Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24660,cuDNN launch failure when implementing custom kernel_regularizer function within [tf.layers] module,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO
- TensorFlow installed from (source or binary): SOURCE (pip)
- TensorFlow version (use command below): tensorflow-gpu API r1.12 
- Python version: 2.7.12
- Bazel version (if compiling from source): not compiled with bazel
- GCC/Compiler version (if compiling from source): gcc 5.4.0 20160609
- CUDA/cuDNN version: CUDA 9.0.176 (wth patch 9.0.176.1 and 9.0.176.2), cuDNN version 7.21
- GPU model and memory: two NVIDIA GeForce GTX 1080ti s, 11171mb memory each


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

I implemented a spectral normalization regularizer that can be passed as the [kernel_regularizer] argument in the [tf.layers] module. This implementation causes three problems which are not expected.

1. Slows down the first epoch of the training process by around ten-fold.
2. Training process gets slower after every epoch by around 1.5 times. (No new ops are being added in the graph and this was made sure by using finalize() method of the graph within the session.)
3. Eventually crashes after a few epochs and throws the following InternalError

Caused by op u'GRAN_model/encoder_models/encoder_real/encoder/encoder_conv_3/Conv2D', defined at:
  File ""main.py"", line 70, in <module>
    gran.initialize()
  File ""main.py"", line 33, in initialize
    config=self.config)
  File ""/home/bispl/github/generative_recon/network.py"", line 32, in build_network
    self.graph.build_model(model=self.model)
  File ""/home/bispl/github/generative_recon/graph.py"", line 45, in build_model
    self.model.build_model(image_input=self.image_input, bold_input=self.bold_input, unpaired_image_input=self.unpaired_image_input, training=self.training)
  File ""/home/bispl/github/generative_recon/model.py"", line 38, in build_model
    self.encoder_real_model_output = self.encoder.build_model(image_input=self.image_input, bold_input=self.bold_input, model_scope='encoder_real', reuse=False)
  File ""/home/bispl/github/generative_recon/model_components.py"", line 37, in build_model
    _x = tf.layers.conv2d(inputs=_x, filters=256, kernel_size=[7,7], kernel_initializer=self.initializer, kernel_regularizer=self.regularizer, kernel_constraint=self.co
nstraint, strides=1, padding='SAME', name=self.scope+'_conv_3')
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/convolutional.py"", line 417, in conv2d
    return layer.apply(inputs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 817, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py"", line 374, in __call__
    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 757, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/layers/convolutional.py"", line 194, in call
    outputs = self._convolution_op(inputs, self.kernel)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py"", line 868, in __call__
    return self.conv_op(inp, filter)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py"", line 520, in __call__
    return self.call(inp, filter)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py"", line 204, in __call__
    name=self.name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py"", line 957, in conv2d
    data_format=data_format, dilations=dilations, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3274, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1770, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): cuDNN launch failure : input shape([16,128,24,32]) filter shape([7,7,128,256])
         [[node GRAN_model/encoder_models/encoder_real/encoder/encoder_conv_3/Conv2D (defined at /home/bispl/github/generative_recon/model_components.py:37)  = Conv2D[T
=DT_FLOAT, data_format=""NCHW"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:
0""](GRAN_model/encoder_models/encoder_real/encoder/encoder_maxpool_0/MaxPool, encoder/encoder_conv_3/kernel/read)]]
         [[{{node GRAN_graph_metrics/generator_real_loss/update_op/_215}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", s
end_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_23669_GRAN_graph_metrics/generator_real_loss/update_op"", tensor_
type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

**Describe the expected behavior**
The expected behavior are
1. Training process gets slower by at most two-fold using this implementation
2. Does not further slow down after every epoch
3. No error thrown.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

1. Implementation of the spectral normalization function to be passed to kernel_regularizer argument of the tf.layers module (e.g. tf.layers.conv2d)


```
def spectral_normalization(kernel):
    with tf.variable_scope(""spectral_normalization"", reuse=tf.AUTO_REUSE):
        print (kernel)
        w = kernel
        _w = tf.reshape(w, [-1, w.shape[-1]], name=""reshape_weight_to_2d"")

        u_tilde = tf.get_local_variable(name=""u_tilde"", shape=[1, w.shape[-1].value], initializer=tf.initializers.truncated_normal)
        _u_tilde = tf.identity(u_tilde, name=""u_tilde_update"")
        for i in range(1):
            _v_tilde = tf.nn.l2_normalize(tf.matmul(_u_tilde, _w, transpose_b=True), name=""v_tilde_update_{}"".format(i))
            _u_tilde = tf.nn.l2_normalize(tf.matmul(_v_tilde, _w), name=""u_tilde_update_{}"".format(i))

        update_u_tilde = u_tilde.assign(_u_tilde)
        sigma_w = tf.squeeze(tf.matmul(tf.matmul(_v_tilde, _w), _u_tilde, transpose_b=True), name='sigma_w')
        _w_sn = _w / sigma_w
        w_sn = tf.reshape(_w_sn, w.shape, name=""reshape_weight_to_original"")
        kernel_spectral_normalized = tf.identity(w_sn, name=""kernel_spectral_normalized"")
        with tf.control_dependencies([update_u_tilde]):
            apply_regularization = kernel.assign(kernel_spectral_normalized, name=""apply_regularization"")
            tf.add_to_collection(name=""SPECTRAL_NORMALIZATION"", value=apply_regularization)

```

2. the function is passed to a layer in [tf.layers] module

`conv_layer = tf.layers.conv2d(inputs=input,  filters=128, kernel_regularizer=spectral_normalization)
`

3. run the regularizer after each update


```
weight_regularization_op = tf.get_collection(""SPECTRAL_NORMALIZATION"")
.
.
.
with tf.Session() as sess:
    for _ in range(num_epoch):
        sess.run(train_op)
        sess.run(weight_regularization_op)

```


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

2018-12-31 17:47:41.987355: E tensorflow/stream_executor/cuda/cuda_dnn.cc:82] CUDNN_STATUS_EXECUTION_FAILED
in tensorflow/stream_executor/cuda/cuda_dnn.cc(2531): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd.handle(), input_data.opaque(), filter.handle(), filter_d
ata.opaque(), conv.handle(), ToConvForwardAlgo(algo_desc), scratch.opaque(), scratch.size(), beta, output_nd.handle(), output_data->opaque())'
Traceback (most recent call last):
  File ""main.py"", line 72, in <module>
    gran.train()
  File ""main.py"", line 43, in train
    save_epoch=self.base_option['save_epoch'])
  File ""/home/bispl/github/generative_recon/network.py"", line 51, in train
    self.session.train_graph(savedir=savedir, save_epoch=save_epoch)
  File ""/home/bispl/github/generative_recon/session.py"", line 112, in train_graph
    feed_dict={self.graph.image_input: train_image_input, self.graph.bold_input: train_bold_input, self.graph.unpaired_image_input: train_unpaired_image_input, self.gra
ph.training: True, self.graph.generator_learning_rate: self.generator_learning_rate})
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: cuDNN launch failure : input shape([16,128,24,32]) filter shape([7,7,128,256])
         [[node GRAN_model/encoder_models/encoder_real/encoder/encoder_conv_3/Conv2D (defined at /home/bispl/github/generative_recon/model_components.py:37)  = Conv2D[T
=DT_FLOAT, data_format=""NCHW"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:
0""](GRAN_model/encoder_models/encoder_real/encoder/encoder_maxpool_0/MaxPool, encoder/encoder_conv_3/kernel/read)]]
         [[{{node GRAN_graph_metrics/generator_real_loss/update_op/_215}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", s
end_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_23669_GRAN_graph_metrics/generator_real_loss/update_op"", tensor_
type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

Caused by op u'GRAN_model/encoder_models/encoder_real/encoder/encoder_conv_3/Conv2D', defined at:
  File ""main.py"", line 70, in <module>
    gran.initialize()
  File ""main.py"", line 33, in initialize
    config=self.config)
  File ""/home/bispl/github/generative_recon/network.py"", line 32, in build_network
    self.graph.build_model(model=self.model)
  File ""/home/bispl/github/generative_recon/graph.py"", line 45, in build_model
    self.model.build_model(image_input=self.image_input, bold_input=self.bold_input, unpaired_image_input=self.unpaired_image_input, training=self.training)
  File ""/home/bispl/github/generative_recon/model.py"", line 38, in build_model
    self.encoder_real_model_output = self.encoder.build_model(image_input=self.image_input, bold_input=self.bold_input, model_scope='encoder_real', reuse=False)
  File ""/home/bispl/github/generative_recon/model_components.py"", line 37, in build_model
    _x = tf.layers.conv2d(inputs=_x, filters=256, kernel_size=[7,7], kernel_initializer=self.initializer, kernel_regularizer=self.regularizer, kernel_constraint=self.co
nstraint, strides=1, padding='SAME', name=self.scope+'_conv_3')
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/convolutional.py"", line 417, in conv2d
    return layer.apply(inputs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 817, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py"", line 374, in __call__
    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 757, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/layers/convolutional.py"", line 194, in call
    outputs = self._convolution_op(inputs, self.kernel)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py"", line 868, in __call__
    return self.conv_op(inp, filter)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py"", line 520, in __call__
    return self.call(inp, filter)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py"", line 204, in __call__
    name=self.name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py"", line 957, in conv2d
    data_format=data_format, dilations=dilations, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3274, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1770, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): cuDNN launch failure : input shape([16,128,24,32]) filter shape([7,7,128,256])
         [[node GRAN_model/encoder_models/encoder_real/encoder/encoder_conv_3/Conv2D (defined at /home/bispl/github/generative_recon/model_components.py:37)  = Conv2D[T
=DT_FLOAT, data_format=""NCHW"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:
0""](GRAN_model/encoder_models/encoder_real/encoder/encoder_maxpool_0/MaxPool, encoder/encoder_conv_3/kernel/read)]]
         [[{{node GRAN_graph_metrics/generator_real_loss/update_op/_215}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", s
end_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_23669_GRAN_graph_metrics/generator_real_loss/update_op"", tensor_
type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]"
24659,TensorArrays in tf.map_fn are being placed on CPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.5 LTS
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12
- Python version:3.5.2
- CUDA/cuDNN version:
- GPU model and memory: Compute capability 7, ~30GB

**Describe the current behavior**

```
def get_index(code_book, tensor):
    def index(t):
        a = tf.where(tf.equal(code_book, tf.fill(code_book.shape, t)))[:,-1]
        return a
    tensor_flat = tf.reshape(tensor, [-1])
    return tf.reshape(tf.map_fn(index, tensor_flat, dtype=tf.int64, back_prop=False, parallel_iterations=20), tensor.shape)
```    
I am using the above function to find the indexes of elements in `tensor`, in `code_book`. `tensor` is a rank 1 float16 tensor.           
When running,          
I noticed that after sometime, GPU is sitting idle and CPU is being used.        
I checked the device placements of the ops and found these,      
```
map_52/TensorArray_1: (TensorArrayV3): /job:localhost/replica:0/task:0/device:CPU:0
map_52/while/strided_slice: (StridedSlice): /job:localhost/replica:0/task:0/device:CPU:0
map_52/while/Shape: (Shape): /job:localhost/replica:0/task:0/device:CPU:0
map_52/while/TensorArrayWrite/TensorArrayWriteV3/Enter: (Enter): /job:localhost/replica:0/task:0/device:CPU:0
map_52/while/TensorArrayWrite/TensorArrayWriteV3: (TensorArrayWriteV3): /job:localhost/replica:0/task:0/device:CPU:0
map_52/TensorArrayStack/TensorArraySizeV3: (TensorArraySizeV3): /job:localhost/replica:0/task:0/device:CPU:0
map_52/TensorArrayStack/range: (Range): /job:localhost/replica:0/task:0/device:CPU:0
map_52/TensorArrayStack/TensorArrayGatherV3: (TensorArrayGatherV3): /job:localhost/replica:0/task:0/device:CPU:0
```

When forcing 'tf.map_fn' to be placed on GPU using `tf.device`, I get these errors,          
```Cannot assign a device for operation map/TensorArray_1: Could not satisfy explicit device specification '' because the node no[11/1932$
nsorArray_1 (defined at weights_changer.py:63) having device No device assignments were active during op 'map/TensorArray_1' creation. 
 was colocated with a group of nodes that required incompatible device '/device:GPU:0'
Colocation Debug Info:
Colocation group had the following types and devices: 
TensorArrayGatherV3: CPU XLA_CPU XLA_GPU 
Range: GPU CPU XLA_CPU XLA_GPU 
Const: GPU CPU XLA_CPU XLA_GPU 
TensorArraySizeV3: CPU XLA_CPU XLA_GPU 
TensorArrayWriteV3: CPU XLA_CPU XLA_GPU 
Enter: CPU XLA_CPU XLA_GPU 
StridedSlice: GPU CPU XLA_CPU XLA_GPU 
TensorArrayV3: GPU CPU XLA_CPU XLA_GPU 

Colocation members and user-requested devices:
  map/TensorArray_1 (TensorArrayV3) 
  map/while/strided_slice (StridedSlice) /device:GPU:0
  map/while/TensorArrayWrite/TensorArrayWriteV3/Enter (Enter) /device:GPU:0
  map/while/TensorArrayWrite/TensorArrayWriteV3 (TensorArrayWriteV3) /device:GPU:0
  map/TensorArrayStack/TensorArraySizeV3 (TensorArraySizeV3) 
  map/TensorArrayStack/range/start (Const) 
  map/TensorArrayStack/range/delta (Const) 
  map/TensorArrayStack/range (Range) 
  map/TensorArrayStack/TensorArrayGatherV3 (TensorArrayGatherV3) 
[[node map/TensorArray_1 (defined at weights_changer.py:63)  = TensorArrayV3[clear_after_read=true, dtype=DT_INT64, dynamic_size=false, element_shape=<unknown>, identical_element_shapes=true, tensor_array_name=""""](map/TensorArray_1/size)]]

No node-device colocations were active during op 'map/TensorArray_1' creation.
No device assignments were active during op 'map/TensorArray_1' creation.
```
Is this the expected behavior?"
24658,[BUG] r1.12.0-rc2  :StringPiece has no member named 'starts_with',"**System information**
- target-cpu  arm-v8
- tensorflow  r1.12.0-rc2
- local gcc 5.4.0

This is the entry point to the tensorflow utest program
```C
GTEST_API_ int main(int argc, char** argv) {
  std::cout << ""Running main() from test_main.cc\n"";

  tensorflow::testing::InstallStacktraceHandler();
  testing::InitGoogleTest(&argc, argv);
  for (int i = 1; i < argc; i++) {
    if (tensorflow::StringPiece(argv[i]).starts_with(""--benchmarks="")) {
      const char* pattern = argv[i] + strlen(""--benchmarks="");
      tensorflow::testing::Benchmark::Run(pattern);
      return 0;
    }
  }
  return RUN_ALL_TESTS();
}
```
If I try to compile any test files, I get the following error:
```
test_main.cc: In function 'int main(int, char**)':
test_main.cc:40:42: error: 'using StringPiece = class absl::string_view {aka class absl::string_view}' has no member named 'starts_with'
     if (tensorflow::StringPiece(argv[i]).starts_with(""--benchmarks="")) {

```

"
24653,tf.log() is missing although it is still used in many code examples,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac OS X 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
VERSION=""1.13.0-dev20181226"" (this is the TF 2.0-preview)
GIT_VERSION=""b'v1.12.0-5133-gc343196842'""
- Python version:
3.6.6
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
`tf.log()` is missing.  It is only available in `tf.math.log()`.

**Describe the expected behavior**
The log function is fundamental and used all the time, it should be available in `tf.log()` (and also in `tf.math.log()`. Especially considering that `tf.exp()` is available. Note that many code examples still show `tf.log()` (try searching for `'tf.log('` in the code base, you will find 12 matches across 8 files).

**Code to reproduce the issue**

```python
import tensorflow as tf
tf.log(42.)
```

**Other info / logs**
I also checked the changes listed in the API names RFC, and I noticed a few discrepancies (I can file a separate issue if necessary):

```
Not deleted: tf.Event
Not deleted: tf.losses
Not deleted: tf.space_to_batch

Not moved: tf.floormod => tf.math.floormod
Not moved: tf.realdiv => tf.math.realdiv
Not moved: tf.SummaryMetadata => tf.summary.SummaryMetadata
Not moved: tf.truncatediv => tf.math.truncatediv
Not moved: tf.truncatemod => math.truncatemod

Not added: tf.batch_to_space_nd
Not added: tf.debugging.is_finite
Not added: tf.debugging.is_inf
Not added: tf.debugging.is_non_decreasing
Not added: tf.debugging.is_strictly_increasing
Not added: tf.debugging.Print
Not added: tf.debugging.verify_tensor_all_finite
Not added: tf.dtypes.bitcast
Not added: tf.initializers.glorot_normal
Not added: tf.initializers.orthogonal_initializer
Not added: tf.initializers.tables_initializer
Not added: tf.initializers.uniform_unit_scaling
Not added: tf.initializers.variance_scaling
Not added: tf.io.PaddingFIFOQueue
Not added: tf.io.PriorityQueue
Not added: tf.io.QueueBase
Not added: tf.io.RandomShuffleQueue
Not added: tf.io.tf_record_iterator
Not added: tf.linalg.matrix_band_part
Not added: tf.linalg.matrix_inverse
Not added: tf.linalg.matrix_solve_ls
Not added: tf.linalg.self_adjoint_eig
Not added: tf.linalg.self_adjoint_eigvals
Not added: tf.math.mod
Not added: tf.math.reduce_join
Not added: tf.quantization.quantize_v2
Not added: tf.random.get_seed
Not added: tf.random.multinomial
Not added: tf.random.random_gamma
Not added: tf.random.random_poisson
Not added: tf.random.set_random_seed
Not added: tf.saved_model.build_tensor_info
Not added: tf.saved_model.get_tensor_from_tensor_info
Not added: tf.saved_model.LEGACY_INIT_OP_KEY
Not added: tf.saved_model.load
Not added: tf.saved_model.main_op
Not added: tf.saved_model.MAIN_OP_KEY
Not added: tf.saved_model.main_op_with_restore
Not added: tf.saved_model.maybe_saved_model_directory
Not added: tf.saved_model.SavedModelBuilder
Not added: tf.saved_model.TRAINING
Not added: tf.sparse.matmul
Not added: tf.sparse.merge
Not added: tf.sparse.placeholder
Not added: tf.sparse.reduce_max_sparse
Not added: tf.sparse.reduce_sum_sparse
Not added: tf.sparse.SparseTensorValue
Not added: tf.summary.HistogramProto
Not added: tf.train.confusion_matrix
```

And `tf.spectral` is called `tf.signal`."
24650,MKL convolution throws exception for simple code,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below):b'v1.12.0-5324-g3ae375aa92' 1.12.0
- Python version:3.6
- Bazel version (if compiling from source):19.2
- GCC/Compiler version (if compiling from source):5.3.0
- CUDA/cuDNN version:n/a
- GPU model and memory:n/a

**Describe the current behavior**
```python
import numpy as np
import tensorflow as tf

x = tf.placeholder(tf.float32, shape=[1, 3, None, None])
xpad = tf.pad(x, [[0, 0], [0, 0], [2, 3], [2, 3]])
W = tf.random_normal([7, 7, 3, 64])
out = tf.nn.conv2d(xpad, W, [1, 1, 2, 2], padding=""VALID"", data_format='NCHW')

sess = tf.Session()
with sess.as_default():
    sess.run(out, feed_dict=
            {x: np.random.rand(1, 3, 600, 800)})
```
This simple code snippet is legal and runs well on a GPU. However, when run on a CPU with MKL build it throws:
```
2019-01-01 04:17:00.402605: W tensorflow/core/framework/op_kernel.cc:1412] OP_REQUIRES failed at mkl_conv_ops.cc:1128 : Aborted: Operation received an exception:Status: 3, message: could not create a dilated convolution forward descriptor, in file tensorflow/core/kernels/mkl_conv_ops.cc:1125
Traceback (most recent call last):
  File ""/HOME/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    return fn(*args)
  File ""/HOME/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/HOME/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.AbortedError: Operation received an exception:Status: 3, message: could not create a dilated convolution forward descriptor, in file tensorflow/core/kernels/mkl_conv_ops.cc:1125              
         [[{{node Conv2D}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""a.py"", line 15, in <module>
    {x: np.random.rand(1, 3, 600, 800)})
  File ""/HOME/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/HOME/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/HOME/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/HOME/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.AbortedError: Operation received an exception:Status: 3, message: could not create a dilated convolution forward descriptor, in file tensorflow/core/kernels/mkl_conv_ops.cc:1125              
         [[node Conv2D (defined at a.py:10) ]]

Errors may have originated from an input operation.
Input Source operations connected to node Conv2D:
 random_normal/stddev (defined at a.py:9)
 Placeholder (defined at a.py:7)
 Pad/paddings (defined at a.py:8)

Original stack trace for 'Conv2D':
  File ""a.py"", line 10, in <module>
    out = tf.nn.conv2d(xpad, W, [1, 1, 2, 2], padding=""VALID"", data_format='NCHW')
  File ""/HOME/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 1026, in conv2d
    data_format=data_format, dilations=dilations, name=name)
  File ""/HOME/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/HOME/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 501, in new_func
    return func(*args, **kwargs)
  File ""/HOME/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3300, in create_op
    op_def=op_def)
  File ""/HOME/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1801, in __init__
    self._traceback = tf_stack.extract_stack()
```

The code will not throw if the padding is [2, 2] instead of [2, 3]. "
24649,Why tf.matmul doesn't get zero?,"<em>I got a problem when using TensorFlow eager mode that tf.matmul result of two matrixes should obvious be zeros but now different small number as results.</em>

**System information**
- OS Platform and Distribution:Linux Ubuntu 16.04, also on macOS
- TensorFlow installed from:binary
- TensorFlow version :1.12
- Python version:3.6
- GPU model and memory: no GPU, 8GB memory

**Describe the current behavior**
It should be all zeros of the output, but it shows random small values

**Code to reproduce the issue**
` index=np.random.randint(10, size=20)`
 ` grad = tf.one_hot(index, 10, on_value=-0.045, off_value=0.005)`
`  w=tf.constant(0.01, shape=[100, 10])`
`  print(tf.matmul(grad, tf.transpose(w)))`


Is this a bug or something else?
"
24646,TensorFlow Lite conversion fails,"**System information**
Windows 7/64bit
- TensorFlow installed from binary:
1.12 from pip
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

Traceback (most recent call last):
  File ""C:\Users\io\AppData\Roaming\Python\Python36\site-packages\tensorflow\contrib\lite\python\lite.py"", line 453, in
convert
    **converter_kwargs)
  File ""C:\Users\io\AppData\Roaming\Python\Python36\site-packages\tensorflow\contrib\lite\python\convert.py"", line 342,
in toco_convert_impl
    input_data.SerializeToString())
  File ""C:\Users\io\AppData\Roaming\Python\Python36\site-packages\tensorflow\contrib\lite\python\convert.py"", line 135,
in toco_convert_protos
    (stdout, stderr))
RuntimeError: TOCO failed see console for info.
b'Traceback (most recent call last):\r\n  File ""C:\\Users\\io\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensor
flow\\contrib\\lite\\toco\\python\\tensorflow_wrap_toco.py"", line 18, in swig_import_helper\r\n    fp, pathname, descrip
tion = imp.find_module(\'_tensorflow_wrap_toco\', [dirname(__file__)])\r\n  File ""c:\\users\\io\\anaconda3\\lib\\imp.py""
, line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named \'
_tensorflow_wrap_toco\'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (mos
t recent call last):\r\n  File ""c:\\users\\io\\anaconda3\\lib\\runpy.py"", line 193, in _run_module_as_main\r\n    ""__mai
n__"", mod_spec)\r\n  File ""c:\\users\\io\\anaconda3\\lib\\runpy.py"", line 85, in _run_code\r\n    exec(code, run_globals
)\r\n  File ""C:\\Users\\io\\Anaconda3\\Scripts\\toco_from_protos.exe\\__main__.py"", line 5, in <module>\r\n  File ""C:\\U
sers\\io\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\contrib\\lite\\toco\\python\\toco_from_protos.p
y"", line 22, in <module>\r\n    from tensorflow.contrib.lite.toco.python import tensorflow_wrap_toco\r\n  File ""C:\\User
s\\io\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\contrib\\lite\\toco\\python\\tensorflow_wrap_toco.
py"", line 28, in <module>\r\n    _tensorflow_wrap_toco = swig_import_helper()\r\n  File ""C:\\Users\\io\\AppData\\Roaming
\\Python\\Python36\\site-packages\\tensorflow\\contrib\\lite\\toco\\python\\tensorflow_wrap_toco.py"", line 20, in swig_i
mport_helper\r\n    import _tensorflow_wrap_toco\r\nModuleNotFoundError: No module named \'_tensorflow_wrap_toco\'\r\n'
None


Github's Continuous build status says: 
Win GPU nightly failing, 
so I did not try retraining and reconverting.



"
24644,I replaced keras to tensorflow.keras in my project and got OOM with same batch size.,"I would like to completely remove keras dependency from my project and replace it to tensorflow.keras, but it is significantly degrades performance.  
Out Of Memory occurs when I just replace keras to tensorflow.keras with same batch size.
Also epoch train time increased.
Looks like tensorflow.keras unoptimized.
Will tensorflow.keras be optimized in future?"
24643,Tensorboard histogram_freq is not using GPU: slows down training,"**System information**
- No custom code
- Ubuntu 18.04
- TensorFlow version 1.11.0
- tf.COMPILER_VERSION = v1.11.0-0-gc19e29306c
- Python version 3.6
- CUDA/cuDNN version 9.0
- GPU model and memory: GTX 1060 6GB

When I run Tensorboard with `histogram_freq = x` where `x != 0`, it uses the CPU at the end of each epoch when Tensorboard writes to logs. 

Usually the GPU is used for the entirety of training and writing to logs and this is also true when `histogram_freq = 0`. 

This [issue] (https://github.com/keras-team/keras/issues/3358) has been mentioned in previous questions but it is dissimilar in that their issue occurs when validation data is passed via a data generator and that histograms are not created - no reference to CPU or GPU. 

I do not use a data generator for validation and histograms appear to be created. This is an issue as writing to logs using CPU seems to take an eternity making the training process up to five times longer.

Notes: 

Training carried out by Keras `fit_generator`.

Training dataset passed by `ImageDataGenerator`.

Validation dataset is passed without data generator.

    tensorboard = TensorBoard(log_dir='../logs/vv2', histogram_freq=1, write_graph=True, write_images=True)
    model.fit_generator(train_generator, 
                                  steps_per_epoch=batch_size, 
                                  epochs=epochs, 
                                  validation_data=(x_test,y_test),
                                  callbacks=[tensorboard])
"
24642,Failed to load the native TensorFlow runtime in Mac OSX,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Mac OSX 10.11.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed with pip
- TensorFlow version:  1.12.0
- Python version: 3.6.6

**Describe the problem**

Everything was ok with an older version, but with the current version install pass ok but when try to import, many errors are produced.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

`import tensorflow
`

**Any other info / logs**
```
Traceback (most recent call last):
  File ""/Users/mikko/dev/wip/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/Users/mikko/dev/wip/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/Users/mikko/dev/wip/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/Users/mikko/dev/wip/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/Users/mikko/dev/wip/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/Users/mikko/dev/wip/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _clock_gettime
  Referenced from: /Users/mikko/dev/wip/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so (which was built for Mac OS X 10.12)
  Expected in: /usr/lib/libSystem.B.dylib
 in /Users/mikko/dev/wip/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so

During handling of the above exception, another exception occurred:


Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/mikko/dev/wip/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/Users/mikko/dev/wip/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/Users/mikko/dev/wip/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/Users/mikko/dev/wip/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/Users/mikko/dev/wip/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/Users/mikko/dev/wip/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/Users/mikko/dev/wip/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/Users/mikko/dev/wip/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/Users/mikko/dev/wip/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _clock_gettime
  Referenced from: /Users/mikko/dev/wip/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so (which was built for Mac OS X 10.12)
  Expected in: /usr/lib/libSystem.B.dylib
 in /Users/mikko/dev/wip/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

```"
24641,tf.contrib.quantize.create_training_graph with tf.keras.activations.relu quantizes before the relu cliping op,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.12
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

The use of the param 'max_value' in https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu produces the a quantized graph with the FakeQuant op just after the relu op and not after the automatically inserted clip op.

![Generated graph](https://c2.staticflickr.com/8/7889/31598991447_a5d540f672_b.jpg)

**Describe the expected behavior**

The FakeQuant op should be added after the clip op

**Code to reproduce the issue**
```
...
net = tf.layers.dense(inputs=net,
                              units=units,
                              activation=None,
                              name='dense_' + str(ly_idx),
                              )
net = tf.keras.activations.relu(net,max_value=params['clip_activations'])
...
```



Thanks,
"
24639,model_to_dot() missing from tf.keras although it is documented on keras.io,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac OS X 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
VERSION=""1.13.0-dev20181226"" (this is the TF 2.0-preview)
GIT_VERSION=""b'v1.12.0-5133-gc343196842'""
- Python version:
3.6.6
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
The official Keras API is documented at keras.io, and it shows an example using `model_to_dot()` ([here](https://keras.io/visualization/#model-visualization)). The example uses `keras.utils.vis_utils.model_to_dot()`. However, this example does not work when using `tf.keras`, since there is no `tf.keras.utils.vis_utils` package.
I can work around this issue by using: `from tensorflow.python.keras.utils.vis_utils import model_to_dot`, but I believe importing from `tensorflow.python` is frowned upon and may break in the future.

**Describe the expected behavior**
I expect `model_to_dot()` to be available in `tf.keras.utils.vis_utils`, or at least in `tf.keras.utils`.

**Code to reproduce the issue**

```python
from tensorflow import keras
keras.utils.vis_utils.model_to_dot
```

**Other info / logs**
On a related note, `keras.utils.print_summary` is part of the Keras API ([here](https://keras.io/utils/#print_summary)), but absent from `tf.keras.utils`.

`keras.utils.np_utils.to_categorical` is documented [here](https://keras.io/losses/) but absent from `tf.keras.utils.np_utils` (it's in `tf.keras.utils`). This might be an issue with the Keras API rather than with tf.keras.
```"
24636,Tensorflow can find right cudnn in one python file but fail in another,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution :  `windows 10`
- TensorFlow installed from : `binary`
- TensorFlow version: `1.12.0`
- Python version: `3.5`
- CUDA/cuDNN version: `cuda 9.0 ` & `cudnn 7.4.2`
- GPU model and memory: `Geforce 1060`

**Describe the current behavior**
I am trying to use tensorflow gpu version to train and test my deep learning model. But here comes the problem. When I train my model in one python file things go on well. Tensorflow-gpu can be used properly. Then I save my model as a pretrained on as `grapg.pb` format and try to reuse it in another python file. 

Then I got the following error messages.

**Describe the expected behavior**
Run tensorflow well in both files.

**Code to reproduce the issue**
```
 E tensorflow/stream_executor/cuda/cuda_dnn.cc:363] Loaded runtime CuDNN 
    library: 7.1.4 but source was compiled with: 7.2.1.  CuDNN library major 
    and minor version needs to match or have higher minor version in case of 
    CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN 
    library.  If building from sources, make sure the library loaded at runtime 
    is compatible with the version specified during compile configuration.
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24634,Unsupported data type in placeholder op,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac OS Mojave
- TensorFlow installed from (source or binary):
pip install tensorflow
- TensorFlow version (or github SHA if from source):
1.12

```
RuntimeError: TOCO failed see console for info.
b""2018-12-30 15:40:54.449737: I tensorflow/contrib/lite/toco/import_tensorflow.cc:189] Unsupported data type in placeholder op: 2\n2018-12-30 15:40:54.450020: F tensorflow/contrib/lite/toco/import_tensorflow.cc:2137] Check failed: status.ok() Unexpected value forattribute 'T'. Expected 'DT_FLOAT'\n""
```

To save the model, I have:
```
// model is an Estimator instance
def export(model):
  model.export_saved_model(""tmp/export"", serving_input_receiver_fn)
```

and:
```
def serving_input_receiver_fn():
  features = { 'x': tf.placeholder(shape=[1, 100, 100, 1], dtype=tf.as_dtype(np.int32)) }
  return tf.estimator.export.ServingInputReceiver(features, features)
```

As you can see, dtype is np.int32, so I attempt to cast that to a tf type. 

I can attach the full model def on request. 

Thanks."
24632,Interrupting tf.keras training while using the TensorBoard callback wreaks havoc,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac OS X 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
VERSION=""1.13.0-dev20181226"" (this is the TF 2.0-preview)
GIT_VERSION=""b'v1.12.0-5133-gc343196842'""
- Python version:
3.6.6
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
Using tf.keras in Jupyter (or a Python shell) with the `TensorBoard` callback, some problems occur if I interrupt training. These problems did not occur in TF 1.12:

1. I get an exception if I call `fit()` again on the same model:

```
tensorflow.python.framework.errors_impl.NotFoundError: Resource
localhost/logdir:logs/run1/N10tensorflow22SummaryWriterInterfaceE does not exist.
[Op:WriteScalarSummary] name: epoch_loss/
```
I can workaround this problem by recompiling the model.

2. I also get an exception if I interrupt training, then I delete the logs directory, then I try to use the `TensorBoard` callback on the same logs directory again:
```
tensorflow.python.framework.errors_impl.UnknownError: The events file logs/run1/events.out.tfevents.1546185456.macmix.local.v2 has disappeared.
	Failed to flush 1 events to logs/run1/events.out.tfevents.1546185456.macmix.local.v2
	Could not flush events file. [Op:FlushSummaryWriter]
```
This one is more severe: sometimes it recovers by itself after a while. Sometimes is doesn't and I cannot find any way to manually recover from this error, other than restarting the Jupyter kernel (or the Python shell).

**Describe the expected behavior**
I expect the `TensorBoard` callback to gracefully handle these issues, perhaps display a warning, but do not force a recompile or a kernel restart.

**Code to reproduce the issue**

```python
import shutil
import numpy as np
import tensorflow as tf
from tensorflow import keras

X_train = np.random.rand(1000, 10)
y_train = np.random.rand(1000)
model = keras.models.Sequential([keras.layers.Dense(1)])
model.compile(loss=""mse"", optimizer=""sgd"")
tensorboard_cb = keras.callbacks.TensorBoard(""logs/run1"")
model.fit(X_train, y_train, epochs=1000, callbacks=[tensorboard_cb])
# NOTE: you must interrupt training (Ctrl-C) before it finishes

# For issue #1, try this:
model.fit(X_train, y_train, epochs=1000, callbacks=[tensorboard_cb])

# For issue #2, try this (you may need to interrupt and retry a few times):
shutil.rmtree(""logs"")
model = keras.models.Sequential([keras.layers.Dense(1)])
model.compile(loss=""mse"", optimizer=""sgd"")
tensorboard_cb = keras.callbacks.TensorBoard(""logs/run1"")
model.fit(X_train, y_train, epochs=1000, callbacks=[tensorboard_cb])
```

**Other info / logs**
Here is a gist with the full session output:
https://gist.github.com/ageron/1d430d4a7716c7a2bae44ee82c321f01"
24627,"Keras model summary does not display ""Connected to"" correctly","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac OS X 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
VERSION=""1.13.0-dev20181226"" (this is the TF 2.0-preview)
GIT_VERSION=""b'v1.12.0-5133-gc343196842'""
- Python version:
3.6.6
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
The `model.summary()` method does not display the **Connect to** column correctly when using the functional API (see the full output below). It looks like `<tensorflow.python.keras.engine.i` or `<tensorflow.python.keras.layers.c` or `<tensorflow.python.keras.layers.m`, which are probably truncated object names. Perhaps these objects are missing a `__str__()` method?

**Describe the expected behavior**
I expect the same nice output as in previous versions, such as `input_1[0][0]`, `dense_3[0][0]` or `concatenate[0][0]`.

**Code to reproduce the issue**

```python
import tensorflow as tf
from tensorflow import keras

input_data = keras.layers.Input(shape=[8])
hidden1 = keras.layers.Dense(30, activation=""relu"")(input_data)
hidden2 = keras.layers.Dense(30, activation=""relu"")(hidden1)
concat = keras.layers.concatenate([input_data, hidden2])
output = keras.layers.Dense(1)(concat)
model = keras.models.Model(inputs=[input_data], outputs=[output])
model.compile(loss=""mean_squared_error"", optimizer=""sgd"")

model.summary()
```

**Other info / logs**
Here is the output:

```
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            [(None, 8)]          0
__________________________________________________________________________________________________
dense (Dense)                   (None, 30)           270         <tensorflow.python.keras.engine.i
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 30)           930         <tensorflow.python.keras.layers.c
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 38)           0           <tensorflow.python.keras.engine.i
                                                                 <tensorflow.python.keras.layers.c
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            39          <tensorflow.python.keras.layers.m
==================================================================================================
Total params: 1,239
Trainable params: 1,239
Non-trainable params: 0
__________________________________________________________________________________________________
```

And here is the output in TF 1.12:

```
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 8)            0
__________________________________________________________________________________________________
dense (Dense)                   (None, 30)           270         input_1[0][0]
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 30)           930         dense[0][0]
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 38)           0           input_1[0][0]
                                                                 dense_1[0][0]
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            39          concatenate[0][0]
==================================================================================================
Total params: 1,239
Trainable params: 1,239
Non-trainable params: 0
__________________________________________________________________________________________________
```"
24626,Master build crash with GPU support testing with OpenSeq2Seq ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9.6 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not Mobile
- TensorFlow installed from (source or binary): Clone from github, master branch
- TensorFlow version (use command below): 1.12
- Python version: 3.5
- Bazel version (if compiling from source): 0.21
- GCC/Compiler version (if compiling from source):  gcc version 6.3.0 20170516 (Debian 6.3.0-18+deb9u1)
- CUDA/cuDNN version: cuDNN 7.4.1 for CUDA 10.0
- GPU model and memory: GTX2070, 8G

To reproduce:

- Install latest CUDA and cuDNN and make sure CUDA samples and cuDNN samples run successfully. Make sure CUDA things are OK.
- Checkout the latest tensorflow code, configure with cuda, bazel build with --config=opt --config=cuda
- Waiting then create python package and install
- ldd /usr/local/lib/python3.5/site-packages/tensorflow/libtensorflow_framework.so everything is linked correct.

- Download OpenSeq2Seq git clone https://github.com/NVIDIA/OpenSeq2Seq.git
- Run toy test
cd OpenSeq2Seq; python3 run.py --config_file=example_files/speech2text/ds2_toy_config.py --mode=train

**CRASH and generate core dump in libtensorflow_framework.so.**


"
24624,Load weights from TensorFlow checkpoint to Keras model,"I have trained a TensorFlow with Keras model and using `keras.callbacks.ModelCheckpoint` I've saved the weights as follows:

    cp_callback = keras.callbacks.ModelCheckpoint(checkpoint_path,
                                              save_weights_only=True,
                                              verbose=1)
    model.fit(X_train, Y_train, callbacks=[cp_callback], epochs=50, batch_size=256)

But while trying to load the saved weights there is nothing changed to my model, after building the model architecture and compile it I load weights as follows:

    model.load_weights('./checkpoints/cp.ckpt')

But nothing happens, the testing accuracy is as random guessing, while my real testing accuracy is 80.49%

The model consists of keras dence layers with l2 kernel_regularizer and glorot kernel_initializer, also I'm using TensorFlow version 1.12.0, any ideas?"
24623,tf.keras.Model.load_weights failed,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Both on Ubuntu 16.04 / Windows 10 1803
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12.0
- Python version: Python 3.6.7 |Anaconda, Inc.| (default, Oct 23 2018, 19:16:44) 
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: CUDA 9.0 & cuDNN 7.1.4
- GPU model and memory: NVIDIA GeForce GTX 1080Ti 12Gb

**Describe the current behavior**

When load a trained keras model (`tf.keras.Model`):
```python3
model.load_weights(""./test/trained.h5"")
```
It will raise the bug
`ValueError: You are trying to load a weight file containing 1 layers into a model with 0 layers.`

It's discussed in keras repo: https://github.com/keras-team/keras/issues/10417 and  https://github.com/keras-team/keras/issues/11683 and has not been fixed so far.

It still exists in latest keras version but it only happens on `keras >= 2.2.0` (reported by https://github.com/keras-team/keras/issues/10417#issuecomment-402696659). However, it's also exists in `2.1.6-tf` with tensorflow 1.12.

**Describe the expected behavior**
Load the model from the `*.h5` file.

**Code to reproduce the issue**
To reproduce the issue, run the demo:
```python3
import numpy as np
from tensorflow.python.keras.callbacks import ModelCheckpoint
from tensorflow.python.keras.layers import Dense

from files_functions import *

tf.enable_eager_execution()

def measure(y_true, y_pred):
    return tf.reduce_mean(y_pred - y_true)


class DemoNet(tf.keras.Model):
    def __init__(self):
        super(DemoNet, self).__init__()
        self.encoder_layer = Dense(128, input_shape=(128,), activation='tanh')

    def call(self, inputs, training=None, mask=None):
        """"""Run the model.""""""
        encoded = self.encoder_layer(inputs)

        return encoded


model = DemoNet()
model.compile(optimizer=tf.train.AdamOptimizer(),
              loss='mse',
              metrics=[measure])

training_input = np.ones((30000, 128)).astype(np.float32)
current_path = ""./test/""
save_path = current_path
checkpoint = ModelCheckpoint(save_path + '/trained.h5', monitor='val_loss', verbose=0, save_best_only=True,
                             mode='min',
                             save_weights_only=True)
model.fit(training_input, training_input, epochs=1, batch_size=512, verbose=2, \
          callbacks=[checkpoint], validation_split=0.2)

del model

model = DemoNet()
model.compile(optimizer=tf.train.AdamOptimizer(),
              loss='mse',
              metrics=[measure])

testdata = np.ones((3000, 128))

model.load_weights(""./test/trained.h5"")
eval = model.evaluate(testdata, testdata, batch_size=512)
```
The demo simpily build a network with one fully connected layer. It will raise the error:
`ValueError: You are trying to load a weight file containing 1 layers into a model with 0 layers.`

The environment of python can be installed by `requrement.txt`:
```
Package             Version   
------------------- ----------
absl-py             0.6.1     
astor               0.7.1     
certifi             2018.11.29
channelnorm-cuda    0.0.0     
chardet             3.0.4     
correlation-cuda    0.0.0     
cycler              0.10.0    
gast                0.2.0     
grpcio              1.16.1    
h5json              1.1.3     
h5py                2.8.0     
idna                2.8       
Keras               2.1.0     
Keras-Applications  1.0.6     
Keras-Preprocessing 1.0.5     
kiwisolver          1.0.1     
lxml                4.2.5     
Markdown            3.0.1     
matplotlib          3.0.2     
numpy               1.15.4    
Pillow              5.2.0     
pip                 18.1      
protobuf            3.6.0     
pyparsing           2.3.0     
python-dateutil     2.7.5     
PyYAML              3.13      
requests            2.21.0    
resample2d-cuda     0.0.0     
scipy               1.1.0     
setuptools          40.6.2    
six                 1.11.0    
tensorboard         1.12.0    
tensorflow-gpu      1.12.0    
termcolor           1.1.0     
urllib3             1.24.1    
Werkzeug            0.14.1    
wheel               0.32.3
```

**Other info / logs**
Trackback of the error:
```
Traceback (most recent call last):
  File ""/***/***/test.py"", line 51, in <module>
    model.load_weights(""./test/trained.h5"")
  File ""/home/***/anaconda3/envs/transmitter/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 1544, in load_weights
    saving.load_weights_from_hdf5_group(f, self.layers)
  File ""/home/***/anaconda3/envs/transmitter/lib/python3.6/site-packages/tensorflow/python/keras/engine/saving.py"", line 784, in load_weights_from_hdf5_group
    ' layers.')
ValueError: You are trying to load a weight file containing 1 layers into a model with 0 layers.
```"
24622,tf.keras.utils.plot_model() raises TypeError: 'InputLayer' object is not iterable,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac OS X 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
VERSION=""1.13.0-dev20181226"" (this is the TF 2.0-preview)
GIT_VERSION=""b'v1.12.0-5133-gc343196842'""
- Python version:
3.6.6
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
tf.keras.utils.plot_model() raises `TypeError: 'InputLayer' object is not iterable`

**Describe the expected behavior**
I expect it to save a png image of the model.

**Code to reproduce the issue**

```python
import tensorflow as tf
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(3, input_shape=[2]))
model.add(tf.keras.layers.Dense(1))
tf.keras.utils.plot_model(model, to_file='my_model.png')
```

**Other info / logs**
I ran the code in TF 1.12, no problem.

Here is the traceback in TF 2.0-preview:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/utils/vis_utils.py"", line 148, in plot_model
    dot = model_to_dot(model, show_shapes, show_layer_names, rankdir)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/utils/vis_utils.py"", line 123, in model_to_dot
    for inbound_layer in node.inbound_layers:
TypeError: 'InputLayer' object is not iterable
```"
24620, Does tf.contrib.signal.stft  do the right audio framing,"Hello
When using  TensorFlow for audio analysis, I found an unreasonable results in **tf.contrib.signal.stft**.
My code is as follows:
```
sample_rate = 16000 #16kHz
segment_size = 2000 #ms
window_size_ms = 40
window_stride_ms = 20
segment_size_samples = int(sample_rate * segment_size / 1000)
window_size_samples = int(sample_rate * window_size_ms / 1000)
window_stride_samples = int(sample_rate * window_stride_ms / 1000)
audio_sequence = tf.placeholder(tf.float32,
                                [1,segment_size_samples],
                                name = 'audio_sequence')
stfts = tf.contrib.signal.stft(audio_sequence, 
                               frame_length=window_size_samples,
                               frame_step=window_stride_samples,
                               fft_length=window_size_samples,
                               name = 'stfts')
stfts.get_shape()
```
The result is **TensorShape([Dimension(1), Dimension(99), Dimension(321)])**
As far as I know, the right should be **(1,100,321)**.
So, what is the underlying reason behind this incident?
Thx~"
24619,TensorFlow official devel docker cannot build TensorFlow,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a
- TensorFlow installed from (source or binary):n/a
- TensorFlow version:n/a
- Python version:3
- Installed using virtualenv? pip? conda?:n.a
- Bazel version (if compiling from source):n/a
- GCC/Compiler version (if compiling from source):n/a
- CUDA/cuDNN version:10.0
- GPU model and memory:n/a

I'm trying to build tensorflow from latest source locally but failed. Then I found that I cannot even build it with the official docker. I ran the following command:

```bash
nvidia-docker run -it --rm tensorflow/tensorflow:devel-gpu-py3 bash
# inside the docker:
cd tensorflow_src    # it contains latest source code as of Dec 28
./configure   # choose the default options for all questions
bazel build //tensorflow/tools/pip_package:build_pip_package --verbose_failures
```

Errors:
```
ERROR: /tensorflow_src/tensorflow/contrib/seq2seq/BUILD:84:1: Linking of rule '//tensorflow/contrib/seq2seq:gen_beam_search_ops_py_wrappers_cc' fa$
led (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64 \
    PATH=/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/host/bin/tensorflow/contrib/seq2seq/gen_beam_sea$
ch_ops_py_wrappers_cc '-Wl,-rpath,$ORIGIN/../../../_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Ute$
sorflow' '-Wl,-rpath,$ORIGIN/../../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' -Lbaz$
l-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow -Lbazel-out/host/bin/_solib$
local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Wl,-ldl '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..,-rp$
th,$ORIGIN/../..' -Wl,-rpath,../local_config_cuda/cuda/lib64 -Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64 -pthread -Wl,-S -Wl,-no-as-ne$
ded -pie -Wl,-z,relro,-z,now '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -no-canonical-prefixes -fno-canonical-system-headers -B/usr/bin -Wl,--gc-$
ections -Wl,@bazel-out/host/bin/tensorflow/contrib/seq2seq/gen_beam_search_ops_py_wrappers_cc-2.params)
/usr/bin/ld: warning: libcuda.so.1, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrap$
ers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuMemFree_v2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuMemsetD32Async'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuEventCreate'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuStreamAddCallback'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuModuleLoadFatBinary'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuCtxEnablePeerAccess'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuMemGetInfo_v2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuLaunchKernel'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuStreamSynchronize'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuEventQuery'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuEventElapsedTime'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuDeviceCanAccessPeer'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuCtxSynchronize'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuDeviceGetAttribute'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuFuncGetAttribute'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuMemcpyDtoH_v2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuStreamQuery'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuDevicePrimaryCtxGetState'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuCtxSetCurrent'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuStreamWaitEvent'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuEventSynchronize'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuModuleUnload'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuDeviceGet'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuMemsetD32_v2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuMemAllocManaged'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuCtxGetSharedMemConfig'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuMemFreeHost'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuFuncSetCacheConfig'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuStreamCreate'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuMemGetAddressRange_v2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuCtxGetDevice'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuDeviceGetProperties'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuMemcpyDtoHAsync_v2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuGetErrorName'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuDeviceGetCount'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuModuleGetFunction'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuMemHostRegister_v2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuDevicePrimaryCtxRelease'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuMemcpyHtoDAsync_v2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuMemcpyDtoD_v2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuOccupancyMaxPotentialBlockSize'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuModuleLoadDataEx'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuDevicePrimaryCtxRetain'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuMemHostAlloc'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuInit'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuGetErrorString'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuDriverGetVersion'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuDeviceGetPCIBusId'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuEventRecord'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuPointerGetAttribute'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuDeviceTotalMem_v2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuMemsetD8_v2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuDeviceComputeCapability'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuMemAlloc_v2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuDeviceGetName'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuDevicePrimaryCtxSetFlags'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuMemHostUnregister'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.
so: undefined reference to `cuModuleGetGlobal_v2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuMemcpyDtoDAsync_v2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuEventDestroy_v2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuOccupancyMaxActiveBlocksPerMultiprocessor'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuCtxGetCurrent'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuStreamDestroy_v2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuCtxSetSharedMemConfig'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuMemsetD8Async'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuMemcpyHtoD_v2'
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 575.519s, Critical Path: 282.39s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]
INFO: 7411 processes: 7411 local.
FAILED: Build did NOT complete successfully
```"
24618,tensor flow requires ffmpeg ... it was replaced by avconv,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24616,Tensorflow 1.10,"Hi . I have unstatement import issue while using Pycharm community edition. Im using this command 
import tensorflow as tf 
Python : 3.6.0
Tensorflow : 1.10.1
"
24615,Building from source on docker very slow (9 hours),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version:  docker: latest-devel-gpu-py3 (image id: d6c139d2fdbf)
- Python version: py3
- Installed using virtualenv? pip? conda?: docker
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
- CUDA/cuDNN version: Cuda compilation tools, release 9.0, V9.0.176
- GPU model and memory: GTX650Ti, 1024MB
- CPU: Intel(R) Core(TM)2 Duo CPU     E7500  @ 2.93GHz
- Storage: SSD
- RAM: 8GB


**Describe the problem**
I'm trying to compile GPU TF, but it seems to take forever. I'm now on 9 hours and counting. I'm expecting the compilation time to be much quicker.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

following https://www.tensorflow.org/install/source#gpu_support_2 using
docker: latest-devel-gpu-py3 (image id: d6c139d2fdbf).

Compilation finally ends after writing this at roughly 9 hours and 10 minutes

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

final output logs after completion:
```
[17,524 / 17,527] Compiling tensorflow/contrib/tensor_forest/hybrid/core/ops/k_f|
[17,524 / 17,527] 2 actions running                                             |
    Compiling tensorflow/contrib/tensor_forest/hybrid/core/ops/k_feature_gradien|
[17,525 / 17,528] 2 actions, 1 running                                          |
    Compiling tensorflow/contrib/tensor_forest/hybrid/core/ops/unpack_path_op.cc|
[17,526 / 17,529] 2 actions running                                             |
    Compiling tensorflow/contrib/tensor_forest/hybrid/core/ops/unpack_path_op.cc|
[17,526 / 17,529] 2 actions running                                             |
    Compiling tensorflow/contrib/tensor_forest/hybrid/core/ops/unpack_path_op.cc|
[17,526 / 17,529] 2 actions running                                             |
    Compiling tensorflow/contrib/tensor_forest/hybrid/core/ops/unpack_path_op.cc|
[17,526 / 17,529] 2 actions running                                             |
    Compiling tensorflow/contrib/tensor_forest/hybrid/core/ops/unpack_path_op.cc|
[17,533 / 17,536] 2 actions, 1 running                                          |
    Compiling tensorflow/contrib/tensor_forest/hybrid/core/ops/routing_gradient_|
[17,533 / 17,536] 2 actions, 1 running                                          |
    Compiling tensorflow/contrib/tensor_forest/hybrid/core/ops/routing_gradient_|
[17,537 / 17,538] [-----] Linking tensorflow/contrib/tensor_forest/hybrid/python|
[17,537 / 17,538] Linking tensorflow/contrib/tensor_forest/hybrid/python/ops/_tr|
Target //tensorflow/tools/pip_package:build_pip_package up-to-date:             |
  bazel-bin/tensorflow/tools/pip_package/build_pip_package                      |
INFO: Elapsed time: 75209.586s, Critical Path: 1426.82s                         |
INFO: 14080 processes: 14080 local.                                             |
INFO: Build completed successfully, 17538 total actions                         |
root@61c2126bd1ff:/tensorflow#              
```                       "
24614,Tensorflow docker image doesn't set correct `LD_LIBRARY_PATH` variable for CUDA libraries,"I ran the tensorflow docker with GPU support with the following command:
```
docker run --runtime=nvidia -it --rm tensorflow/tensorflow:latest-devel-gpu-py3 python -c ""import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))""
```

I got the following error message:
```
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

It complains that the library `libcublas.so.9.0` cannot be found.

I then started a bash session with the following command:
```
docker run --runtime=nvidia -it --rm tensorflow/tensorflow:latest-devel-gpu-py3 bash
```
and checked the variable `LD_LIBRARY_PATH`:
```
$ echo $LD_LIBRARY_PATH
/usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64

$ ls /usr/local/
bin  cuda  cuda-9.0  etc  games  include  lib  man  sbin  share  src
```

As we can see, there is no `/usr/local/nvidia` directory in the docker image. The `libcublas.so.9.0` library can be found at `/usr/local/cuda/lib64`.

To fix this problem, the `LD_LIBRARY_PATH` variable should be set to `/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib:/usr/local/cuda/lib64`.
"
24612,weird behavior in distributed training using partitioner,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS 7.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): unknown, probably binary
- TensorFlow version (use command below): 1.10.0
- Python version: 2.7.5
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: NA
- GPU model and memory: NA


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

I wrote a custom estimator to implement FM algorithm and used train_and_evaluate to perform distributed training (4 ps and 100 workers). When I set partitioner to None, the training process quickly converged to an expected test AUC 0.87. However, when I either used fixed_size_partitioner or min_max_variable_partitioner with modified min_slice_size, the test AUC stayed below 0.64.  Different learning rates and batch sizes didn't help. Am I missing anything in distributed TF?

**Describe the expected behavior**

With partitioner dividing embedding matrix to each PS, the training process should also converge quickly.

**Code to reproduce the issue**
```
class FMEstimator:
	def __init__(self, model_dir, config=None, params=None, optimizer=None, partitioner=None):
		self.model_dir = model_dir
		self.config = config
		self.params = params
		self.optimizer = optimizer
		self.partitioner = partitioner

	def __embedding_lookup_square_sparse(self, params, sp_ids, partition_strategy=""mod"", name=None, max_norm=None):
		if isinstance(params, variables.PartitionedVariable):
			params = list(params)  # Iterate to get the underlying Variables.
		if not isinstance(params, list):
			params = [params]

		with ops.name_scope(name, ""embedding_lookup_square_sparse"",
		                    params + [sp_ids]) as name:
			segment_ids = sp_ids.indices[:, 0]
			if segment_ids.dtype != dtypes.int32:
				segment_ids = math_ops.cast(segment_ids, dtypes.int32)

			ids = sp_ids.values
			ids, idx = array_ops.unique(ids)

			embeddings = tf.nn.embedding_lookup(params, ids, partition_strategy=partition_strategy, max_norm=max_norm)

			embeddings = tf.square(embeddings)

			assert idx is not None

			embeddings = math_ops.sparse_segment_sum(embeddings, idx, segment_ids, name=name)

			return embeddings

	def get_model_fn(self):
		def custom_model_fn(features, labels, mode, params):
			linear_bias = tf.get_variable(name='linear_bias',
			                              shape=[1],
			                              dtype=tf.float32,
			                              initializer=tf.random_normal_initializer(stddev=0.0001))

			linear_w = tf.get_variable(name='linear_w',
			                           shape=[params['feature_size'], 1],
			                           dtype=tf.float32,
			                           initializer=tf.random_normal_initializer(stddev=0.0001),
			                           partitioner=self.partitioner)

			# wx
			# size: [batch_size, 1]
			logits_wide = tf.nn.embedding_lookup_sparse(params=linear_w,
			                                            sp_ids=features['featureID'],
			                                            sp_weights=None,
			                                            combiner='sum')
			# wx + b
			logits_linear = linear_bias + logits_wide

			cross_emb_w = tf.get_variable(name='cross_emb_w',
			                              shape=[params['feature_size'], params['cross_emb_size']],
			                              dtype=tf.float32,
                                                      partitioner=self.partitioner,
			                              initializer=tf.random_normal_initializer(stddev=0.0001))

			# (a1,b1,c1) (a2,b2,c2) -> (a1+a2, b1+b2, c1+c2)
			# size: [batch_size, cross_emb_size]
			summed_cross_emb = tf.nn.embedding_lookup_sparse(params=cross_emb_w,
			                                                 sp_ids=features['featureID'],
			                                                 sp_weights=None,
			                                                 combiner='sum')
			# ((a1+a2)^2, (b1+b2)^2, (c1+c2)^2)
			# size: [batch_size, cross_emb_size]
			squared_summed_cross_emb = tf.square(summed_cross_emb)

			# (a1^2, b1^2, c1^2) (a2^2, b2^2, c2^2) -> (a1^2+a2^2, b1^2+b2^2, c1^2+c2^2)
			# size: [batch_size, cross_emb_size]
			summed_squared_cross_emb = self.__embedding_lookup_square_sparse(params=cross_emb_w,
			                                                                 sp_ids=features['featureID'])

			# a1a2+b1b2+c1c2
			# size: [batch_size, 1]
			logits_cross = tf.reduce_sum(0.5 * tf.subtract(squared_summed_cross_emb, summed_squared_cross_emb), 1, keepdims=True)

			# logits = tf.add(logits_linear, logits_cross)
			logits = logits_linear
			logits_adjusted = logits + tf.math.log(params['negative_sampling_rate'])

			if mode == tf.estimator.ModeKeys.PREDICT:
				predictions = {
					'probabilities': tf.nn.sigmoid(logits_adjusted),
					'logits': logits,
					'logits_adjusted': logits_adjusted
				}

				return tf.estimator.EstimatorSpec(mode, predictions=predictions)

			else:
				loss = tf.reduce_mean(
					tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(labels, dtype=tf.float32),
					                                        logits=logits))

				if mode == tf.estimator.ModeKeys.EVAL:
					auc = tf.metrics.auc(
						labels=labels,
						predictions=1 / (1 + tf.math.exp(-logits_adjusted)),
						num_thresholds=400,
						curve='ROC',
						summation_method='careful_interpolation')
					logloss = tf.metrics.mean(tf.nn.sigmoid_cross_entropy_with_logits(
						labels=tf.cast(labels, dtype=tf.float32),
						logits=logits_adjusted))
					tf.summary.scalar('True_AUC', auc)
					tf.summary.scalar('True_Logloss', logloss)
					metrics = {
						'True_AUC': auc,
						'True_Logloss': logloss
					}

					predictions = {
						'probabilities': tf.nn.sigmoid(logits_adjusted),
						'logits': logits,
						'logits_adjusted': logits_adjusted
					}

					return tf.estimator.EstimatorSpec(mode, loss=loss, predictions=predictions,
					                                  eval_metric_ops=metrics)

				elif mode == tf.estimator.ModeKeys.TRAIN:
					train_op = self.optimizer.minimize(loss, global_step=tf.train.get_global_step())

					return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)

		return custom_model_fn

	def get_estimator(self):
		return tf.estimator.Estimator(model_fn=self.get_model_fn(),
		                              model_dir=self.model_dir,
		                              config=self.config,
		                              params=self.params)


```
**Other info / logs**
The input data is in sparse tensor of tf.int64. Feature size is around 480,000. FM cross_emb_size is 4.
I have checked the graph from tensorboard and timeline from hooks. Both of them seemed correct. The embedding matrix was divided and placed on each ps with corresponding update ops. I used both the estimator.evaluate function and hand-made prediction calculation with exported embedding matrix, and got the same low AUC value. I also ask a question in stackoverflow ([stackoverflow_link](https://stackoverflow.com/questions/53870450/distributed-tf-multi-ps-version-lr-doesnt-converge)), but go no response by now.
"
24611,performance issue in CollectiveAllReduceStrategy,"Tried both `CollectiveAllReduceStrategy` and `MirroredStrategy` on a same 8 GPU machine. And `CollectiveAllReduceStrategy` is constantly 30% slower than MirroredStrategy no matter how many cards I am using.

The tensorflow version I am running is 1.12.0.

I am using estimator API and tried many different models including resnet50, resnet34, vgg etc. 

Is this expected? Or I need some configuration to make CollectiveAllReduceStrategy faster? Thanks

"
24609,unable to convert the .pb to .tflite as tflite_convert is giving me a problem,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- I am working on tensorflow-for-poets-2 and trying to convert the trained model to tflite so that i can port my model to mobile device. which i was not able to do and i am following the hands on from the 
https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#0

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"" >> 1.12.0

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
my log when executing the code:

(tensorflow) C:\Users\H156759>python
Python 3.6.7 |Anaconda, Inc.| (default, Dec 10 2018, 20:35:02) [MSC v.1915 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> tf.__version__
'1.12.0'
>>> exit()

(tensorflow) C:\Users\H156759>tflite_convert --help
Traceback (most recent call last):
  File ""C:\Pavans\Anaconda3\envs\tensorflow\Scripts\tflite_convert-script.py"", line 6, in <module>
    from tensorflow.contrib.lite.python.tflite_convert import main
ModuleNotFoundError: No module named 'tensorflow.contrib.lite.python.tflite_convert'"
24608,Build did NOT complete successfully while  Fetching @com_google_protobuf,"**System information**
- OS Platform and Distribution: Mac OS Mojave 10.14.2 (18C54)
- TensorFlow version: 1.12 (cloned from master)
- Python version: 3.6.6
- Icompiling in conda virtual env
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): /MacOSX10.14.sdk/usr/include/c++/4.2.1

exception when executing `bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/lite/...`

Internal error thrown during build. Printing stack trace: java.lang.RuntimeException: Unrecoverable error while evaluating node 'REPOSITORY_DIRECTORY:@remotejdk_macos' (requested by nodes 'REPOSITORY:@remotejdk_macos')
	at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:499)
	at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:368)
	at java.base/java.util.concurrent.ForkJoinTask$AdaptedRunnableAction.exec(Unknown Source)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.localPopAndExec(Unknown Source)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)
Caused by: java.lang.IllegalArgumentException: Invalid EvalException:
java.lang.ExceptionInInitializerError
	at java.base/javax.crypto.JceSecurityManager.<clinit>(Unknown Source)
	at java.base/javax.crypto.Cipher.getConfiguredPermission(Unknown Source)
	at java.base/javax.crypto.Cipher.getMaxAllowedKeyLength(Unknown Source)
	at java.base/sun.security.ssl.CipherSuite$BulkCipher.isUnlimited(Unknown Source)
	at java.base/sun.security.ssl.CipherSuite$BulkCipher.<init>(Unknown Source)
	at java.base/sun.security.ssl.CipherSuite$BulkCipher.<clinit>(Unknown Source)
	at java.base/sun.security.ssl.CipherSuite.<clinit>(Unknown Source)
	at java.base/sun.security.ssl.SSLContextImpl.getApplicableSupportedCipherSuiteList(Unknown Source)
	at java.base/sun.security.ssl.SSLContextImpl.access$100(Unknown Source)
	at java.base/sun.security.ssl.SSLContextImpl$AbstractTLSContext.<clinit>(Unknown Source)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Unknown Source)
	at java.base/java.security.Provider$Service.getImplClass(Unknown Source)
	at java.base/java.security.Provider$Service.newInstance(Unknown Source)
	at java.base/sun.security.jca.GetInstance.getInstance(Unknown Source)
	at java.base/sun.security.jca.GetInstance.getInstance(Unknown Source)
	at java.base/javax.net.ssl.SSLContext.getInstance(Unknown Source)
	at java.base/javax.net.ssl.SSLContext.getDefault(Unknown Source)
	at java.base/javax.net.ssl.SSLSocketFactory.getDefault(Unknown Source)
	at java.base/javax.net.ssl.HttpsURLConnection.getDefaultSSLSocketFactory(Unknown Source)
	at java.base/javax.net.ssl.HttpsURLConnection.<init>(Unknown Source)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.<init>(Unknown Source)
	at java.base/sun.net.www.protocol.https.Handler.openConnection(Unknown Source)
	at java.base/java.net.URL.openConnection(Unknown Source)
	at com.google.devtools.build.lib.bazel.repository.downloader.HttpConnector.connect(HttpConnector.java:93)
	at com.google.devtools.build.lib.bazel.repository.downloader.HttpConnectorMultiplexer.establishConnection(HttpConnectorMultiplexer.java:300)
	at com.google.devtools.build.lib.bazel.repository.downloader.HttpConnectorMultiplexer.connect(HttpConnectorMultiplexer.java:126)
	at com.google.devtools.build.lib.bazel.repository.downloader.HttpDownloader.download(HttpDownloader.java:246)
	at com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryContext.downloadAndExtract(SkylarkRepositoryContext.java:433)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.base/java.lang.reflect.Method.invoke(Unknown Source)
	at com.google.devtools.build.lib.syntax.MethodDescriptor.call(MethodDescriptor.java:130)
	at com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:919)
	at com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:887)
	at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:69)
	at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:200)
	at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:182)
	at com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:91)
	at com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:475)
	at com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:437)
	at com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryFunction.fetch(SkylarkRepositoryFunction.java:125)
	at com.google.devtools.build.lib.rules.repository.RepositoryDelegatorFunction.compute(RepositoryDelegatorFunction.java:217)
	at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:422)
	at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:368)
	at java.base/java.util.concurrent.ForkJoinTask$AdaptedRunnableAction.exec(Unknown Source)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.localPopAndExec(Unknown Source)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)
Caused by: java.lang.SecurityException: Can not initialize cryptographic mechanism
	at java.base/javax.crypto.JceSecurity.<clinit>(Unknown Source)
	... 51 more
Caused by: java.lang.SecurityException: Couldn't parse jurisdiction policy files in: unlimited
	at java.base/javax.crypto.JceSecurity.setupJurisdictionPolicies(Unknown Source)
	at java.base/javax.crypto.JceSecurity.access$000(Unknown Source)
	at java.base/javax.crypto.JceSecurity$1.run(Unknown Source)
	at java.base/javax.crypto.JceSecurity$1.run(Unknown Source)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	... 52 more

	at com.google.devtools.build.lib.syntax.EvalException.<init>(EvalException.java:112)
	at com.google.devtools.build.lib.syntax.EvalException$EvalExceptionWithJavaCause.<init>(EvalException.java:209)
	at com.google.devtools.build.lib.syntax.EvalException$EvalExceptionWithJavaCause.<init>(EvalException.java:217)
	at com.google.devtools.build.lib.syntax.MethodDescriptor.call(MethodDescriptor.java:163)
	at com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:919)
	at com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:887)
	at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:69)
	at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:200)
	at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:182)
	at com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:91)
	at com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:475)
	at com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:437)
	at com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryFunction.fetch(SkylarkRepositoryFunction.java:125)
	at com.google.devtools.build.lib.rules.repository.RepositoryDelegatorFunction.compute(RepositoryDelegatorFunction.java:217)
	at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:422)
	... 6 more

INFO: Elapsed time: 12,581s
"
24607,TFLite not support Dynamic input size ,"**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**

Try running tflite_convert again, with --input_shapes=1,none,none,3,  but these conversion cannot be supported.
For some networks requiring dynamic input, the static input of tflite is too limited, such as Superresolution Neural Network.

"
24606,Can't run tensorflow on PyCharm at Win10,"Hi, can anyone help me with this errors? I've tried varios Python installations an have installed Cuda 10.
I tried to run TensorFlow-GPU...
```

""C:\Users\Christoph Richter\PycharmProjects\NNV3\Scripts\python.exe"" ""C:/Users/Christoph Richter/PycharmProjects/NNV3/MNISTTest.py""
Using TensorFlow backend.
Traceback (most recent call last):
  File ""C:\Users\Christoph Richter\PycharmProjects\NNV3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Christoph Richter\PycharmProjects\NNV3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Christoph Richter\PycharmProjects\NNV3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\Python35\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\Python35\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Users/Christoph Richter/PycharmProjects/NNV3/MNISTTest.py"", line 1, in <module>
    import keras
  File ""C:\Users\Christoph Richter\PycharmProjects\NNV3\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils
  File ""C:\Users\Christoph Richter\PycharmProjects\NNV3\lib\site-packages\keras\utils\__init__.py"", line 6, in <module>
    from . import conv_utils
  File ""C:\Users\Christoph Richter\PycharmProjects\NNV3\lib\site-packages\keras\utils\conv_utils.py"", line 9, in <module>
    from .. import backend as K
  File ""C:\Users\Christoph Richter\PycharmProjects\NNV3\lib\site-packages\keras\backend\__init__.py"", line 89, in <module>
    from .tensorflow_backend import *
  File ""C:\Users\Christoph Richter\PycharmProjects\NNV3\lib\site-packages\keras\backend\tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf
  File ""C:\Users\Christoph Richter\PycharmProjects\NNV3\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Christoph Richter\PycharmProjects\NNV3\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Christoph Richter\PycharmProjects\NNV3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Christoph Richter\PycharmProjects\NNV3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Christoph Richter\PycharmProjects\NNV3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Christoph Richter\PycharmProjects\NNV3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\Python35\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\Python35\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

Process finished with exit code 1
```
"
24605,[Feature Request] Indexing an array of functions,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.12.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
Currently using a tensor to index a list of functions is inconsistent. In order to have the same functionality while training I have to chain together a series of tf.cond statements to accomplish the same task. Currently, this can be done in eager execution but not in outside of it.

Example:

```
import tensorflow as tf

tf.enable_eager_execution()

def mul(a):
    return tf.multiply(a, 10)

def add(a):
    return tf.add(a, 10)

def sub(a):
    return tf.subtract(a, 10)

def div(a):
    return tf.divide(a, 10)

functions = [mul, add, sub, div]
a = tf.ones(shape=(), dtype=tf.float32) * 10

for i in range(10):
    random_op = tf.random_uniform(shape=(), minval=0, maxval=len(functions), dtype=tf.int64)
    result = functions[random_op](a)
    print(random_op, result)
```

Output:
```
tf.Tensor(1, shape=(), dtype=int64) tf.Tensor(20.0, shape=(), dtype=float32)
tf.Tensor(1, shape=(), dtype=int64) tf.Tensor(20.0, shape=(), dtype=float32)
tf.Tensor(1, shape=(), dtype=int64) tf.Tensor(20.0, shape=(), dtype=float32)
tf.Tensor(2, shape=(), dtype=int64) tf.Tensor(0.0, shape=(), dtype=float32)
tf.Tensor(0, shape=(), dtype=int64) tf.Tensor(100.0, shape=(), dtype=float32)
tf.Tensor(0, shape=(), dtype=int64) tf.Tensor(100.0, shape=(), dtype=float32)
tf.Tensor(1, shape=(), dtype=int64) tf.Tensor(20.0, shape=(), dtype=float32)
tf.Tensor(1, shape=(), dtype=int64) tf.Tensor(20.0, shape=(), dtype=float32)
tf.Tensor(2, shape=(), dtype=int64) tf.Tensor(0.0, shape=(), dtype=float32)
tf.Tensor(0, shape=(), dtype=int64) tf.Tensor(100.0, shape=(), dtype=float32)
```

**Will this change the current api? How?**
I suspect this will require the api to detect the output of the indexing the list to be a function as opposed to a new tensor.
**Who will benefit with this feature?**
This should come as a large benefit to anyone who is performing online data augmentation.
**Any Other info.**
"
24604,How to create an tensor as vector C++ ?,"Hey, there.

I am trying to create an tensor as vector with 16 positions. However i cannot. Can you help me please?
My code:

```
Tensor targetsValues(DT_FLOAT, TensorShape({16}));
targetsValues.scalar<float>()() = { 62, 62, 62, 62, 43, 55,  3, 78, 27, 78,  4, 78,  9, 78, 13, 78 };
```

The error:

![image](https://user-images.githubusercontent.com/18532570/50494950-08302d00-0a05-11e9-8eb3-1170f1c0c720.png)
"
24603,Retval[0] has already been set ,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Modified Code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Fedora 28
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary pip
- TensorFlow version (use command below):1.12
- Python version:3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
 I have a working SavedModel (ie. a saved model that works when restored in python) **that fails when run on tensorflow serving.**
The error message on the server is:
OP_REQUIRES failed at function_ops.cc:68 : Internal: Retval[0] has already been set.
Error: W **external/org_tensorflow/tensorflow/core/framework/op_kernel.cc:1401]** OP_REQUIRES failed at function_ops.cc:68 : Internal: Retval[0] has already been set.

 
### Exact Steps to Reproduce
(https://drive.google.com/file/d/1at1CQ9iHgcPHCn-MkvSGcgtbVM2lrKJn/view) link to saved model. it can be restored and run in python successfully but will error if run on a model server. (Takes an image as input  sess.run(fetches=[""loop/Exit_1:0""],feed_dict={""image_bytes:0"": image})


### Source code / logs
Relevant source code(I hope):
(contains a while loop with a concat in the body)
`
val, idx =tf.nn.top_k(softmax ,name=""topk"")
    sentence = tf.Variable([vocab.start_id],False,name=""sentence"",)
    sentence = tf.concat([sentence, idx[0]], 0)#

    def cond(sentence,state):
        return tf.math.not_equal( sentence[-1],tf.constant(vocab.end_id))

    def body(sentence,state):
        input_seqs = tf.expand_dims([sentence[-1]], 1)

        seq_embeddings = tf.nn.embedding_lookup(self.embedding_map, input_seqs)
        embed = seq_embeddings

        # In inference mode, use concatenated states for convenient feeding and
        # fetching.
        state_feed = tf.concat(axis=1, values=state, name=""state"")

        # Placeholder for feeding a batch of concatenated states.
        # state_feed = tf.placeholder(dtype=tf.float32,
        #                             shape=[None, sum(lstm_cell.state_size)],
        #                             name=""state_feed"")
        state_tuple = tf.split(value=state_feed, num_or_size_splits=2, axis=1)

        # Run a single LSTM step.
        lstm_outputs, new_state_tuple = lstm_cell(
            inputs=tf.squeeze(embed, axis=[1]),
            state=state_tuple)

        # Concatentate the resulting state.
        state = tf.concat(axis=1, values=new_state_tuple, name=""state"")

        # Stack batches vertically.

        lstm_outputs = tf.reshape(lstm_outputs, [-1, lstm_cell.output_size])

        with tf.variable_scope(""logits"") as logits_scope:
            logits = tf.contrib.layers.fully_connected(
            inputs=lstm_outputs,
            num_outputs=self.config.vocab_size,
            activation_fn=None,
            weights_initializer=self.initializer,
            scope=logits_scope, reuse = True
            )

        softmax = tf.nn.softmax(logits, name=""softmax"")
        self.softmax = softmax
        val, idx = tf.nn.top_k(softmax, name=""topk"")

        sentence = tf.concat([sentence,idx[0]],0)
        self.output = sentence
        return [sentence, state]
    out = tf.while_loop(cond, body, [sentence, state],parallel_iterations=1,maximum_iterations=20,name=""loop"",shape_invariants=[tf.TensorShape([None]),tf.TensorShape([None,None])])

    return out
`
fails with error:
`
 W external/org_tensorflow/tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at function_ops.cc:68 : Internal: Retval[0] has already been set.

`"
24602,Install documentation for pip install on PI not right / fails,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: unknown.
- Doc Link: https://www.tensorflow.org/install/pip


**Describe the documentation issue**

for // Linux rasputin 4.9.35-v7+ #1014 SMP Fri Jun 30 14:47:43 BST 2017 armv7l GNU/Linux
the directions for installing tensor flow fail after setting up the v environment.

(venv) ebcdic@rasputin:~ $ pip install --upgrade tensorflow
Collecting tensorflow
  Could not find a version that satisfies the requirement tensorflow (from versions: )
No matching distribution found for tensor flow

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
24601,possible bug in examples/speech_commands/test_streaming_accuracy.cc,"in tensorflow/tensorflow/examples/speech_commands/test_streaming_accuracy.cc line 247 the following code is not correct:

`const int64 audio_data_end = (sample_count - clip_duration_ms);`
it should be corrected as follows:
`const int64 audio_data_end = (sample_count - clip_duration_samples);`

it is wrong because the dimensions do not match. sample_count is in units [sample] but clip_duration_ms is in [ms] units.
Additionally it gives the following error:
`Segmentation fault (core dumped)`

but if audio_data_end is set correctly as i mentioned above, this error would not occur.
"
24600,cudnn gru has bug,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version: 2.7.12
- Bazel version (if compiling from source):0.19
- GCC/Compiler version (if compiling from source):5.4.0
- CUDA/cuDNN version:10.0
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I write a unit test to compare tf.CudnnGRU and native gru but they got different result, i think it's cudnn's problem....becase i don't find any problem in my code....

**Describe the expected behavior**
should be same
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

import numpy as np
import tensorflow as tf
from tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops

S_MIN=-40
S_MAX=13
EXP_MAX=40

def native_gru(input, w):

        seq_len, batch_size, hidden_size=input.shape

        offset = 0
        wu = w[offset:offset + hidden_size*hidden_size].reshape(
                (hidden_size,hidden_size)).transpose()
        offset += hidden_size*hidden_size
        wr = w[offset:offset + hidden_size*hidden_size].reshape(
                (hidden_size,hidden_size)).transpose()
        offset += hidden_size*hidden_size
        wc = w[offset:offset + hidden_size*hidden_size].reshape(
                (hidden_size,hidden_size)).transpose()
        offset += hidden_size*hidden_size


        ru = w[offset:offset + hidden_size*hidden_size].reshape(
                (hidden_size,hidden_size)).transpose()
        offset += hidden_size*hidden_size
        rr = w[offset:offset + hidden_size*hidden_size].reshape(
                (hidden_size,hidden_size)).transpose()
        offset += hidden_size*hidden_size
        rc = w[offset:offset + hidden_size*hidden_size].reshape(
                (hidden_size,hidden_size)).transpose()
        offset += hidden_size*hidden_size



        bx_u = w[offset:offset + hidden_size]
        offset += hidden_size
        bx_r = w[offset:offset + hidden_size]
        offset += hidden_size
        bx_c = w[offset:offset + hidden_size]
        offset += hidden_size


        bh_u = w[offset:offset + hidden_size]
        offset += hidden_size
        bh_r = w[offset:offset + hidden_size]
        offset += hidden_size
        bh_c = w[offset:offset + hidden_size]
        offset += hidden_size

        def sigmod(x):
                y = np.copy(x)
                y[x < S_MIN] = S_MIN
                y[x>S_MAX] = S_MAX
                return 1./(1. + np.exp(-y))

        def tanh(x):
                y=-2.*x
                y[y>EXP_MAX] = EXP_MAX
                return (2./(1.+np.exp(y))) - 1

        output = []
        pre_h = np.zeros((batch_size, hidden_size), dtype=input.dtype)

        for i in range(seq_len):
                emb_1 = input[i]
                update_gate = sigmod(np.matmul(emb_1, wu) + np.matmul(pre_h, ru)+bx_u+bh_u)
                reset_gate = sigmod(np.matmul(emb_1, wr)+np.matmul(pre_h, rr)+bx_r+bh_r)
                h_t_temp = tanh(np.matmul(emb_1, wc)+reset_gate*(np.matmul(pre_h,rc)+bh_c)+bx_c)
                new_h = update_gate*pre_h + (1-update_gate)*h_t_temp
                pre_h = new_h
                output.append(new_h)

        output = np.concatenate(output, -1)
        output = output.reshape((batch_size, -1, hidden_size))
        output = output.transpose((1,0,2))
        return output

def TestGRU():
        num_steps = 2
        batch_size = 1
        hidden_size = 1
        input_w_size = 6*hidden_size*hidden_size + 6*hidden_size
        x = np.random.uniform(low=-0.1, high=0.1, size=(num_steps, batch_size, hidden_size)).astype(np.float32)
        #x = np.ones((num_steps, batch_size, hidden_size)).astype(np.float32)


        flat_w = np.random.uniform(low=-0.1, high=0.1, size=(input_w_size)).astype(np.float32)
        #flat_w = np.ones((input_w_size)).astype(np.float32)
        out = native_gru(x, flat_w)

        n_layer = 1
        rnn_cudnn = cudnn_rnn_ops.CudnnGRU(n_layer, hidden_size, hidden_size, 'linear_input')
        #param_cudnn = tf.Variable(tf.ones([rnn_cudnn.params_size()]), validate_shape=False)
        param_cudnn = tf.Variable(flat_w)

        y_cudnn, state_cudnn = rnn_cudnn(x,#tf.transpose(x, [1,0,2]),
        tf.zeros([n_layer, batch_size, hidden_size]), param_cudnn)
        print(""native gru:""+str(out))
        with tf.Session() as sess:
                sess.run(tf.global_variables_initializer())
                print(""cudnn gru:"" +str(sess.run([y_cudnn])))

if __name__ == '__main__':
        TestGRU()

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

"
24599,Documentation: User-friendly Bazel installation,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>

**System information**
- TensorFlow version: GPU 1.14.0
- Doc Link: https://www.tensorflow.org/install/source_windows


**Describe the documentation issue**

The documentation for installing Bazel on Windows is separated into two different pages with no indication that the second page is required after following the first.  This can lead to install issues and build errors that make absolutely no sense, but do after figuring out that you're missing 5 dependencies, mentioned only on the second page under ""Building C++ Projects""

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**

I have written a massive walkthrough for installing tensorflow <a href=""http://aaronjencks.net/blog/8/#Bazel"">here</a>, but in reality, just a mention of the second page as an indication that there's more to set up than just the ""Install Bazel"" <a href=""https://docs.bazel.build/versions/master/install-windows.html"">link</a> on the site, that the user must also install the dependencies for building C++ projects with Bazel, found <a href=""https://docs.bazel.build/versions/master/windows.html"">here</a>, would solve the issue."
24598,Potential tf.boolean_mask bug when the mask array is empty,"**System information**
- OS Platform and Distribution : Windows 10
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.11.0
- Python version: 3.6.6
- CUDA/cuDNN version: V8.0.60
- GPU model and memory: Geforce GTX 1070 8GB

**Describe the current behavior**
I have actually experiencing almost the similar problem like in thread:  https://github.com/tensorflow/tensorflow/issues/24585
Again, I want to partition a minibatch into different parts, process them in parallel using different computation units and then stitch them back together. However this time I used `tf.boolean_mask` instead of `tf.dynamic_partition` for the partition operation, since the latter runs into problems when one of the partitions is empty. This code is below (it is copy&paste reproducible):

```
import tensorflow as tf
import numpy as np


def build_conv_layer(input, filter_size, num_of_input_channels, num_of_output_channels, name_suffix=""""):
    # OK
    conv_weights = tf.Variable(
        tf.truncated_normal([filter_size, filter_size, num_of_input_channels, num_of_output_channels],
                            stddev=0.1, dtype=tf.float32))
    # OK
    conv_biases = tf.Variable(
        tf.constant(0.1, shape=[num_of_output_channels], dtype=tf.float32))
    conv = tf.nn.conv2d(input, conv_weights, strides=[1, 1, 1, 1], padding='SAME')
    relu = tf.nn.relu(tf.nn.bias_add(conv, conv_biases))
    pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
    return pool


batch_size = 250
child_count = 3
channel_count = 32

dataTensor = tf.placeholder(tf.float32, shape=(None, 28, 28, 1), name=""dataTensor"")
indices_tensor = tf.placeholder(name=""indices_tensor"", dtype=tf.int32)
batch_size_tensor = tf.placeholder(name=""batch_size_tensor"", dtype=tf.int32)

condition_indices_list = []
partition_list = []
mask_list = []
for child_index in range(child_count):
    mask_indices = tf.reshape(indices_tensor[:, child_index], [-1])
    condition_indices = tf.boolean_mask(tf.range(batch_size_tensor), mask_indices)
    partition = tf.boolean_mask(dataTensor, mask_indices)
    mask_list.append(mask_indices)
    condition_indices_list.append(condition_indices)
    partition_list.append(partition)

transformed_list = [build_conv_layer(input=part, filter_size=5, num_of_input_channels=1, num_of_output_channels=32)
                    for part in partition_list]
squared_list = [tf.square(part) for part in partition_list]
stitched_conv_transform = tf.dynamic_stitch(indices=condition_indices_list, data=transformed_list)
stitched_square_transform = tf.dynamic_stitch(indices=condition_indices_list, data=squared_list)
sum = tf.reduce_sum(stitched_square_transform)
grads = tf.gradients(sum, dataTensor)

sess = tf.Session()
samples = np.random.uniform(size=(batch_size, 28, 28, 1))
indices_arr = np.zeros(shape=(batch_size, child_count), dtype=np.int32)
indices_arr[:, 0] = 1
indices_arr[-2] = np.array([0, 1, 0])
indices_arr[-1] = np.array([0, 1, 0])

feed_dict = {dataTensor: samples,
             batch_size_tensor: batch_size,
             # indices_tensor: np.argmax(np.random.uniform(size=(GlobalConstants.EVAL_BATCH_SIZE, child_count)), axis=1)}
             indices_tensor: indices_arr}
outputs = []
outputs.extend(mask_list)
outputs.extend(transformed_list)
outputs.extend(squared_list)
outputs.append(stitched_conv_transform)
outputs.append(stitched_square_transform)
outputs.append(sum)
outputs.append(grads)

init = tf.global_variables_initializer()
sess.run(init)
for i in range(10000):
    results = sess.run(outputs, feed_dict=feed_dict)
    assert np.allclose(results[-1][0], 2.0*samples)
    print(""{0} runned."".format(i))
```

To my disappointment, `tf.boolean_mask` runs into a similar problem, when `indices_arr ` contains no references to at least one partition and it produces an empty array for that partition as the result. The for loop in the end runs correctly a few times but then the program crashes with the following error:

> InternalError (see above for traceback): WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true / nonzero indices.  temp_storage_bytes: 1, status: invalid configuration argument
	 [[{{node boolean_mask/Where}} = Where[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](boolean_mask/Reshape_1/_9)]]
	 [[{{node DynamicStitch/_49}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_259_DynamicStitch"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

I think this is the same error underlying the problem in https://github.com/tensorflow/tensorflow/issues/24585 where it crashes when `tf.dynamic_partition` receives an empty index array since they could be using the same mechanism in the cub library (or whatever cub is). The `tf.dynamic_partition` error also occurs after a few succesfull iterations like this one. What could be the reason here?"
24596,Failded to build libtensorflow_inference.so,"<em>Please Help,I try to build libtensorflow_inference.so ,but i get an error.  I guess the problem is my sytem ,I use window10, I don't know</em>

``bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain  --cpu=armeabi-v7a
``

this is my error information:

``
INFO: Analysed target //tensorflow/contrib/android:libtensorflow_inference.so (0 packages loaded, 2701 targets configured).
``
``
INFO: Found 1 target...
``
``
ERROR: C:/users/cuiwanxin/_bazel_cuiwanxin/obpmm2rb/external/com_google_absl/absl/numeric/BUILD.bazel:25:1: C++ compilation of rule '@com_google_absl//absl/numeric:int128' failed (Exit -1). Note: Remote connection/protocol failed with: execution failed: false failed: error executing command
``
``
  cd C:/users/cuiwanxin/_bazel_cuiwanxin/obpmm2rb/execroot/org_tensorflow
``
``
  SET PATH=D:\mysy64\usr\bin\bash.exe;D:\mysy64\usr\share
``
``
    SET PWD=/proc/self/cwd
``
``
  /bin/false -MD -MF bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/numeric/_objs/int128/int128.pic.d -frandom-seed=bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/numeric/_objs/int128/int128.pic.o -fPIC -iquote external/com_google_absl -iquote bazel-out/armeabi-v7a-opt/genfiles/external/com_google_absl -iquote bazel-out/armeabi-v7a-opt/bin/external/com_google_absl -Wall -Wextra -Wcast-qual -Wconversion-null -Wmissing-declarations -Woverlength-strings -Wpointer-arith -Wunused-local-typedefs -Wunused-result -Wvarargs -Wvla -Wwrite-strings -Wno-sign-compare -c external/com_google_absl/absl/numeric/int128.cc -o bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/numeric/_objs/int128/int128.pic.o
``
``
Execution platform: @bazel_tools//platforms:host_platform
``
``
Action failed to execute: java.io.IOException: ERROR: src/main/native/windows/processes-jni.cc(383): CreateProcessW(""C:\users\cuiwanxin\_bazel_cuiwanxin\obpmm2rb\execroot\org_tensorflow\bin\false"" -MD -MF bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/numeric/_objs/int128/int128.pic.d -frandom-see(...)):
``
``
Target //tensorflow/contrib/android:libtensorflow_inference.so failed to build
``
``
INFO: Elapsed time: 11.720s, Critical Path: 0.07s
``
``
INFO: 0 processes.
``
``
FAILED: Build did NOT complete successfully
``
"
24595,Multi-tower support on each GPU in MirroredStrategy and estimator,"
**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**
*Feature*:
Add an argument like `num_towers_per_gpu`  in `tf.contrib.distribute.MirroredStrategy` to enable multi-tower computation on each GPU.
*Current state*:
Current `tf.contrib.distribute.MirroredStrategy`'s design is one GPU one tower. It does not support mult-tower on one GPU. It has args `devices`, `num_gpus`, `num_gpus_per_worker`, `cross_tower_ops`, `prefetch_on_device`, `auto_shard_dataset`. We can only set the number of GPUs, and the mirrored strategy will assign one tower to each GPU.

**Will this change the current api? How?**
Yes. It will add a new argument like `num_towers_per_gpu` to `tf.contrib.distribute.MirroredStrategy`.

**Who will benefit with this feature?**
This feature can make it available to assign multiple towers on each GPU. It can maximize GPU utilization.

**Any Other info.**
"
24594,tensorflow parameter server start with no worker_hosts specific in  cluster def,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (1.8.0):
- Are you willing to contribute it (Yes):



**Describe the feature and the current behavior/state.**
I'm setting up a distributed tensorflow with ps and worker. Currently tf.train.Server must accept a ClusterDef to define the total cluster before program start.

If I only use ps to store and update variables, use worker to train. Why PS must need to know the worker hosts addrs before it starts.

Suppose I want to setup 1 ps and 2 worker, the cluster_def shows below

For PS:

```
  cluster = tf.train.ClusterSpec({'ps': {0: 'ps0:2222'}})
```
For worker0:

```
  cluster = tf.train.ClusterSpec({'ps': {0: 'ps0:2222'}, 'worker': {0: 'localhost:0'}})
```
For worker1:

```
 cluster = tf.train.ClusterSpec({'ps': {0: 'ps0:2222'}, 'worker': {1: 'localhost:0'}})
```
I don't need to communicate between worker1 and worker0, If i config this, PS and worker can start, but when call sess.run with train_op ,the following fault will cause:

```
tensorflow.python.framework.errors_impl.InternalError: No worker known as 
/job:worker/replica:0/task:0
     [[Node: gradients/LinearRegression/xw_plus_b_grad/BiasAddGrad_S29 = _Recv[client_terminated=false, recv_device=""/job:ps/replica:0/task:0/device:CPU:0"", send_device=""/job:worker/replica:0/task:0/device:CPU:0"", send_device_incarnation=-801089107413104296, tensor_name=""edge_78_gradients/LinearRegression/xw_plus_b_grad/BiasAddGrad"", tensor_type=DT_FLOAT, _device=""/job:ps/replica:0/task:0/device:CPU:0""]()]]
```

Seem when GrpcServer::Init, PS did not know any worker info, its channel_cache is empty, RpcRemoteRendezvous::RecvFromRemoteAsync can not find Worker channel and throw errors. The interesting is worker can connect to ps to fetch variable for forward computing.

I don't know if we have any plan to support this usage. I don't know whether the error here is a bug or by-design?

**Will this change the current api? How?**
No
**Who will benefit with this feature?**
We want to when any worker host crash, we can re-launch another worker to continue train, but the new worker may change ip and port. In this case, If we support this feature, we can do worker FO simply
**Any Other info.**
"
24593,Keras model evaluate() progress bar randomly stops before 100%,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac OS X 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
VERSION=1.13.0-dev20181225 (note: this is the 2.0-preview)
GIT_VERSION=b'v1.12.0-5131-gc6f3c5dc48'
- Python version:
3.6.6
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
When evaluating a Keras model, the progress bar randomly stops before 100% (however, the loss and metrics returned by the function are correct). Also, it does not end with a newline.

**Describe the expected behavior**
I expect the progress bar to go up to 100% and display a newline.

**Code to reproduce the issue**

```python
import tensorflow as tf
import numpy as np

np.random.seed(42)
tf.random.set_seed(42)

(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()
model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=[28, 28]),
    tf.keras.layers.Dense(10, activation=""softmax""),
])
model.compile(loss=""sparse_categorical_crossentropy"",
              optimizer=""sgd"", metrics=[""accuracy""])

model.fit(X_train, y_train, epochs=2)
print(model.evaluate(X_test, y_test))
```

**Other info / logs**
Here is the output of this program:

```
Epoch 1/2
60000/60000 [==============================] - 2s 28us/sample - loss: 32.8388 - acc: 0.8403
Epoch 2/2
60000/60000 [==============================] - 2s 27us/sample - loss: 26.3895 - acc: 0.8683
 9792/10000 [============================>.] - ETA: 0s - loss: 33.9531 - acc: 0.8363[33.969303797870886, 0.8358]
```

Notice that the evaluation progress bar (last line) does not go up to 100% (it stops at 9792/10000). Moreover, there is no newline at the end, so the function's returned values (`[33.969303797870886, 0.8358]`) are printed on the same line.

Moreover, when I run the same code again, I get a different output (only the last line differs). This time the progress bar stopped at 9088/10000, but notice that the function's results are the same as above:

```
Epoch 1/2
60000/60000 [==============================] - 2s 29us/sample - loss: 32.8388 - acc: 0.8403
Epoch 2/2
60000/60000 [==============================] - 2s 29us/sample - loss: 26.3895 - acc: 0.8683
 9088/10000 [==========================>...] - ETA: 0s - loss: 34.8416 - acc: 0.8327[33.969303797870886, 0.8358]
```
"
24592,Estimators + tf.data iterators incompatible with eager execution enabled,"> Have I written custom code (as opposed to using a stock example script provided in TensorFlow):

Yes

> OS Platform and Distribution (e.g., Linux Ubuntu 16.04):

MacOS Mojave version 10.14.2

> Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:

N/A

> TensorFlow installed from (source or binary):

binary

> TensorFlow version (use command below):

v1.12.0-rc2-3-ga6d8ffae09 1.12.0

> Python version:

Python 3.6.0

> Bazel version (if compiling from source):

N/A

> GCC/Compiler version (if compiling from source):

N/A

> CUDA/cuDNN version:

N/A

> GPU model and memory:

N/A

**Describe the current behavior**

Say you train an estimator using [tf.estimator.train_and_evaluate](https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate). The `input_fn` to your estimator returns a [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset). you may want to write a small program to make predictions over a subset of data to sanity-check model training and gain more insight into how your estimator makes predictions, without having to go through the trouble of deploying your model. You may want to, for simplicity, use eager execution to do this. Something like:

```python
import tensorflow as tf; tf.enable_eager_execution()

estimator = tf.estimator.Estimator(
        model.model_fn,
        warm_start_from=tf.train.latest_checkpoint(args.job_dir))
it = model_input_fn().make_one_shot_iterator()

batch = it.get_next() # Returns <features>, <label>
predictions = estimator.predict(lambda: tf.data.Dataset.from_tensor_slices(batch))
print('Prediction:', next(predictions))
print('Label:', batch[1])
```

This currently throws the following error:

```
RuntimeError: Attempting to capture an EagerTensor without building a function.
```

**Describe the expected behavior**

Ideally this would just work? Unless I'm missing something  

**Code to reproduce the issue**

1. Train a toy estimator using `tf.estimator.train_and_evaluate`. Have its input function return a dataset.
1. Write a script similar to the one written above, which basically creates an estimator warm-started from the latest checkpoint in your training job, calls the input function to get the actual labels, then tries to compare that to prediction output from an estimator.

**Other info / logs**

The problem _appears_ to be something that `from_tensor_slices` is doing. Relevant part of my traceback is below

```
 File ""WORKDIR/model_evaluation.py"", line 61, in <lambda>
    predictions = estimator.predict(lambda: tf.data.Dataset.from_tensor_slices(batch))
  File ""PYTHONDIR/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 289, in from_tensor_slices
    return TensorSliceDataset(tensors)
  File ""PYTHONDIR/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1565, in __init__
    for i, t in enumerate(nest.flatten(tensors))
  File ""PYTHONDIR/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1565, in <listcomp>
    for i, t in enumerate(nest.flatten(tensors))
  File ""PYTHONDIR/site-packages/tensorflow/python/framework/ops.py"", line 1050, in convert_to_tensor
    as_ref=False)
  File ""PYTHONDIR/site-packages/tensorflow/python/framework/ops.py"", line 1106, in internal_convert_to_tensor
    raise RuntimeError(""Attempting to capture an EagerTensor without ""
RuntimeError: Attempting to capture an EagerTensor without building a function.
```"
24591,list index out of range in freeze_graph.py,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):**NO**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**Linux Ubuntu 16.04.5**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:**No**
- TensorFlow installed from (source or binary):**binary**
- TensorFlow version (use command below):**1.12.0**
- Python version:**3.5.2**
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:**9.0/7.0**
- GPU model and memory:**GTX 1080 11GB**

**Describe the current behavior**
I try to freeze graph generated by tf.esitimator. esitimator will generate *.pbtxt and ckpt file automatically, so I use following command:
`python freeze_graph.py --input_graph=path/to/.pbtxt --input_checkpoint=path/to/model.ckpt-0 --output_graph=path/to/saved/.pb --output_node_name=OutPutOp`

**Other info / logs**
`Traceback (most recent call last):
  File ""model_freeze.py"", line 494, in <module>
    run_main()
  File ""model_freeze.py"", line 490, in run_main
    app.run(main=my_main, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""model_freeze.py"", line 489, in <lambda>
    my_main = lambda unused_args: main(unused_args, flags)
  File ""model_freeze.py"", line 382, in main
    flags.saved_model_tags, checkpoint_version)
  File ""model_freeze.py"", line 364, in freeze_graph
    checkpoint_version=checkpoint_version)
  File ""model_freeze.py"", line 191, in freeze_graph_with_def_protos
    var_list=var_list, write_version=checkpoint_version)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py"", line 1102, in __init__
    self.build()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py"", line 1114, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py"", line 1151, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py"", line 773, in _build_internal
    saveables = self._ValidateAndSliceInputs(names_to_saveables)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py"", line 680, in _ValidateAndSliceInputs
    for converted_saveable_object in self.SaveableObjectsForOp(op, name):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py"", line 654, in SaveableObjectsForOp
    variable, """", name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py"", line 128, in __init__
    self.handle_op = var.op.inputs[0]
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 2128, in __getitem__
    return self._inputs[i]
IndexError: list index out of range
`
"
24590,"  java.lang.IllegalArgumentException: ConcatOp : Dimensions of inputs should match: shape[0] = [1,1,1,256] vs. shape[13] = [0,1,1,256]","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.12
- Python version:3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:9.0/7.0
- GPU model and memory:ZR390


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
   I use yolov2 to train my model,and transplant it to mobile phone,in office demo https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android
it crash,output this bug:
 2018-12-27 09:41:33.780 17423-17443/org.tensorflow.demo E/TensorFlowInferenceInterface: Failed to run TensorFlow inference with inputs:[input], outputs:[output]
2018-12-27 09:41:33.783 17423-17443/org.tensorflow.demo E/AndroidRuntime: FATAL EXCEPTION: inference
    Process: org.tensorflow.demo, PID: 17423
    java.lang.IllegalArgumentException: ConcatOp : Dimensions of inputs should match: shape[0] = [1,1,1,256] vs. shape[13] = [0,1,1,256]
    	 [[{{node concat_9}} = ConcatV2[N=19, T=DT_FLOAT, Tidx=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Reshape_152, Reshape_153, Reshape_154, Reshape_155, Reshape_156, Reshape_157, Reshape_158, Reshape_159, Reshape_160, Reshape_161, Reshape_162, Reshape_163, Reshape_164, Reshape_165, Reshape_166, Reshape_167, Reshape_168, Reshape_169, Reshape_170, concat_9/axis)]]
        at org.tensorflow.Session.run(Native Method) 
It confused me  long time,can anyone help me ,thanks a lot 
**Describe the expected behavior**
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24589,"Why can't Python 2.7 install tensorFlow with windows operating system ?  Does TensorFlow not support Python 27 now? Is there a kind person who can tell me, I really urgent!","This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
24587,Controlling Scheduling of Unrelated Ops,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.11
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
[tf.control_dependencies](https://www.tensorflow.org/api_docs/python/tf/control_dependencies) can be used to order the operations on same variables in TensorFlow. What's the alternative of control_dependencies if I want to force relative scheduling of completely unrelated operations i.e. operations that don't share input or output variables? A similar discussion:
https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/XFnL2WlnWd8

**Will this change the current api? How?**
Not Sure

**Who will benefit with this feature?**
Anyone who wants to schedule operations to optimize performance when multiple GPUs are being used.
**Any Other info.**
Specifically, if you look at [this gist](https://gist.github.com/xilenteyex/066c5218802c16f4b987c7d086f6f4a5), there are two matrix multiplications, mul1 and mul2, is there a way to force TensorFlow to always compute mul1 before computing mul2 if they are placed on the same device ? In case there is no way, how can I go about adding this functionality ? Any hints ?"
24586,Using MirroredStrategy for Multi-GPU Training Fails with DNNLinearCombinedClassifier using default FTRL optimizer,"Many similar issues using MirroredStrategy with other optimizers have been filed. Looks like FTRL optimizer needs to be updated as well.

**Environment**
== cat /etc/issue ===============================================
Linux 1226-234618-hanks421-172-20-170-14 4.4.0-1072-aws #82-Ubuntu SMP Fri Nov 2 15:00:21 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.5 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux 1226-234618-hanks421-172-20-170-14 4.4.0-1072-aws #82-Ubuntu SMP Fri Nov 2 15:00:21 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy                              1.14.3                   
protobuf                           3.6.1                    
tensorflow                         1.12.0                   

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.12.0
tf.GIT_VERSION = unknown
tf.COMPILER_VERSION = unknown
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /databricks/python/lib:
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Thu Dec 27 00:32:16 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 396.44                 Driver Version: 396.44                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  Off  | 00000000:00:1B.0 Off |                    0 |
| N/A   43C    P0    67W / 300W |    503MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-SXM2...  Off  | 00000000:00:1C.0 Off |                    0 |
| N/A   42C    P0    54W / 300W |    503MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-SXM2...  Off  | 00000000:00:1D.0 Off |                    0 |
| N/A   42C    P0    56W / 300W |    503MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-SXM2...  Off  | 00000000:00:1E.0 Off |                    0 |
| N/A   42C    P0    57W / 300W |    503MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart.so.9.2.148
/usr/local/cuda-9.2/doc/man/man7/libcudart.7
/usr/local/cuda-9.2/doc/man/man7/libcudart.so.7

**Describe the current behavior**
Using MirroredStrategy for Multi-GPU Training fails with DNNLinearCombinedClassifier using default FTRL optimizer

**Other info / logs**
```
Traceback (most recent call last):
  File ""/databricks/python/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/databricks/python/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 177, in _call_for_each_tower
    **merge_kwargs)
  File ""/databricks/python/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py"", line 665, in _distributed_apply
    self._create_slots(var_list)
  File ""/databricks/python/lib/python2.7/site-packages/tensorflow/python/training/ftrl.py"", line 125, in _create_slots
    with ops.colocate_with(v):
  File ""/databricks/python/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/databricks/python/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 4094, in _colocate_with_for_gradient
    with self.colocate_with(op, ignore_existing):
  File ""/databricks/python/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/databricks/python/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 4146, in colocate_with
    op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op
  File ""/databricks/python/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1307, in internal_convert_to_tensor_or_indexed_slices
    value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/databricks/python/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1146, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/databricks/python/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/values.py"", line 439, in _tensor_conversion_mirrored
    assert not as_ref
AssertionError
```
"
24585,tf.dynamic_stitch does not work when one of the partitions has zero elements.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS Platform and Distribution : Windows 10
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6.6
- CUDA/cuDNN version: V8.0.60
- GPU model and memory: Geforce GTX 1070 8GB


**Describe the current behavior**
I am building a model which partitions a given minibatch into different parts, processes each part with different computation units in parallel and then stitch them back together. To implement this, I decided to use `tf.dynamic_partition` and `tf.dynamic_stitch` methods. I have the following code for testing purposes:

```
import tensorflow as tf
import numpy as np


def build_conv_layer(input, filter_size, num_of_input_channels, num_of_output_channels, name_suffix=""""):
    # OK
    conv_weights = tf.Variable(
        tf.truncated_normal([filter_size, filter_size, num_of_input_channels, num_of_output_channels],
                            stddev=0.1, dtype=tf.float32))
    # OK
    conv_biases = tf.Variable(
        tf.constant(0.1, shape=[num_of_output_channels], dtype=tf.float32))
    conv = tf.nn.conv2d(input, conv_weights, strides=[1, 1, 1, 1], padding='SAME')
    relu = tf.nn.relu(tf.nn.bias_add(conv, conv_biases))
    pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
    return pool

batch_size = 250
child_count = 3
channel_count = 32

dataTensor = tf.placeholder(tf.float32, shape=(None, 28, 28, 1), name=""dataTensor"")
indices_tensor = tf.placeholder(name=""indices_tensor"", dtype=tf.int32)
batch_size_tensor = tf.placeholder(name=""batch_size_tensor"", dtype=tf.int32)

condition_indices = tf.dynamic_partition(data=tf.range(batch_size_tensor), partitions=indices_tensor,
                                         num_partitions=child_count)
partition_list = tf.dynamic_partition(data=dataTensor, partitions=indices_tensor, num_partitions=child_count)
transformed_list = [build_conv_layer(input=part, filter_size=5, num_of_input_channels=1,
                                     num_of_output_channels=channel_count)
                    for part in partition_list]
stitched = tf.dynamic_stitch(indices=condition_indices, data=transformed_list)

sess = tf.Session()
samples = np.random.uniform(size=(batch_size, 28, 28, 1))
indices_arr = np.zeros(shape=(batch_size, ), dtype=np.int32)
indices_arr[-1] = 1
indices_arr[-2] = 0
feed_dict = {dataTensor: samples,
             batch_size_tensor: batch_size,
             # indices_tensor: np.argmax(np.random.uniform(size=(GlobalConstants.EVAL_BATCH_SIZE, child_count)), axis=1)}
             indices_tensor: indices_arr}
outputs = []
outputs.extend(transformed_list)
outputs.append(stitched)

init = tf.global_variables_initializer()
sess.run(init)
for i in range(10000):
    results = sess.run(outputs, feed_dict=feed_dict)
    print(""{0} runned."".format(i))


```

So, I divide the `dataTensor` into 3 parts with `tf.dynamic_partition` and each part goes through a convolutional layer. After that, they are stitched back together into a single minibatch again, with `tf.dynamic_stitch`, using individual samples' original location information from `condition_indices`. This works without any visible problems when all three partitions have assigned at least one sample. But when at least one partition does have zero samples, like in code above (which is controlled with `indices_arr`), Tensorflow crashes at the `tf.dynamic_stitch` line with the following error:

> InvalidArgumentError (see above for traceback): data[0].shape = [0,14,14,32] does not start with indices[0].shape = [249]

It seems like `tf.dynamic_partition` and  `tf.dynamic_stitch` stops working when one of the partitions is empty (receiving no samples) and **all partitions receive zero samples as the result**. The code above is reproducible with simple copy and paste. Am I missing something or doing something wrong or is this a bug in `tf.dynamic_stitch` implementation? Doesn't it support empty partitions which can occur in practice, if you determine partitions as the result of an algorithm ? 
"
24582,TF Identity - SparseTensor - Not Working,"`tf.identity` seems to be not working with SparseTensor. This is a problem, because it makes quite impossible to use them with `tf.control_dependencies`.

Below a very simple example:

```python
import tensorflow as tf

a = tf.SparseTensor([[0,0]], [0], [800, 256])

print_op = tf.print(""Hello world"")

with tf.control_dependencies([print_op]):
    a = tf.identity(a)
```

```python
TypeError: Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor. Contents: SparseTensor(indices=Tensor(""SparseTensor_5/indices:0"", shape=(1, 2), dtype=int64), values=Tensor(""SparseTensor_5/values:0"", shape=(1,), dtype=int32), dense_shape=Tensor(""SparseTensor_5/dense_shape:0"", shape=(2,), dtype=int64)). Consider casting elements to a supported type.
```
"
24581,Failed to Build Tensorflow due to extension_dict.cc on Mac OSX,"I am using the latest version of Mac 10.14.1, Python 3.7 (through Anaconda, I then later installed Python 3.6 in an environment to see if it would work, but the same error appears), and Bazel 0.15.0. 
When configuring the install, I put N for everything and used default for all others. 

I then executed the following to build a CPU-only version since I am on a Mac:

bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

After a long time of waiting for the build process, I've hit the following error that caused my build to fail:

```
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (1 packages loaded).
INFO: Found 1 target...
ERROR: /private/var/tmp/_bazel_maisora/ceb5b62c0dc483a80db02f088b1ce574/external/protobuf_archive/BUILD:659:1: C++ compilation of rule '@protobuf_archive//:python/google/protobuf/pyext/_message.so' failed (Exit 1)
external/protobuf_archive/python/google/protobuf/pyext/extension_dict.cc:184:7: error: assigning to 'char *' from incompatible type 'const char *'
  if (PyString_AsStringAndSize(arg, &name, &name_size) < 0) {
      ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/protobuf_archive/python/google/protobuf/pyext/extension_dict.cc:56:22: note: expanded from macro 'PyString_AsStringAndSize'
       ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \
                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
1 error generated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 9.701s, Critical Path: 2.59s
INFO: 17 processes: 17 local.
FAILED: Build did NOT complete successfully
```

How do I get around this to successfully build tensorflow?"
24580,Tf cast function not changing data type of the source variable,"On using Tf.cast(source_variable,dtype=tf.int64), the datatype of the source variable remains unchanged. If I assign the result of tf.cast to a new_variable, the new_variable's datatype is equal to the ""dtype"" but source_variable.dtype remains the same.

Example:
source_variable data-type is float32
result = tf.cast(source_variable,tf.int32)
The Data type of ""result"" is int32, but the datatype of ""source_variable"" is float32.

How do I change the datatype of the source_variable to int32?"
24579,"Unimplemented ops BatchMatMul, Erf, SquaredDifference.","- OS Platform and Distribution : Linux Ubuntu 16.04
- TensorFlow installed from : binary
- TensorFlow version : 1.12.0

**Text output from tflite_convert**
```
tflite_convert 
--output_file=/tmp/lite_model.tflite 
--graph_def_file=frozen_graph.pb 
--input_shapes=1,100 
--input_arrays=input_data 
--output_arrays=output_data

RuntimeError: TOCO failed see console for info.
b'2018-12-24 16:31:57.805610: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\n2018-12-24 16:31:57.807588: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\n2018-12-24 16:31:57.811170: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Erf\n2018-12-24 16:31:57.814732: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\n2018-12-24 16:31:57.816627: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\n2018-12-24 16:31:57.820222: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Erf\n2018-12-24 16:31:57.823765: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\n2018-12-24 16:31:57.825625: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\n2018-12-24 16:31:57.829252: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Erf\n2018-12-24 16:31:57.832835: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\n2018-12-24 16:31:57.834676: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\n2018-12-24 16:31:57.838239: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Erf\n2018-12-24 16:31:57.841768: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\n2018-12-24 16:31:57.843573: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\n2018-12-24 16:31:57.847116: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Erf\n2018-12-24 16:31:57.850639: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\n2018-12-24 16:31:57.852443: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\n2018-12-24 16:31:57.856015: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Erf\n2018-12-24 16:31:57.859547: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\n2018-12-24 16:31:57.861375: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\n2018-12-24 16:31:57.865034: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Erf\n2018-12-24 16:31:57.868582: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\n2018-12-24 16:31:57.870416: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\n2018-12-24 16:31:57.873967: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Erf\n2018-12-24 16:31:57.877478: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\n2018-12-24 16:31:57.879385: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\n2018-12-24 16:31:57.882895: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Erf\n2018-12-24 16:31:57.886449: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\n2018-12-24 16:31:57.888249: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\n2018-12-24 16:31:57.891826: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Erf\n2018-12-24 16:31:57.895390: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\n2018-12-24 16:31:57.897229: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\n2018-12-24 16:31:57.900830: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Erf\n2018-12-24 16:31:57.904400: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\n2018-12-24 16:31:57.906183: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\n2018-12-24 16:31:57.909826: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Erf\n2018-12-24 16:31:57.913387: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SquaredDifference\n2018-12-24 16:31:57.942135: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1110 operators, 1758 arrays (0 quantized)\n2018-12-24 16:31:57.970531: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 1107 operators, 1749 arrays (0 quantized)\n2018-12-24 16:31:58.007597: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1107 operators, 1749 arrays (0 quantized)\n2018-12-24 16:31:58.302044:
 I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 702 operators, 1261 arrays (0 quantized)\n2018-12-24 16:31:58.342334:
 I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 690 operators, 1250 arrays (0 quantized)\n2018-12-24 16:31:58.366749:
 I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 677 operators, 1224 arrays (0 quantized)\n2018-12-24 16:31:58.390442:
 I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 677 operators, 1224 arrays (0 quantized)\n2018-12-24 16:31:58.420840:
 I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 922432 bytes, theoretical optimal value: 921600 bytes.\n2018-12-24 16:31:58.425368: F tensorflow/contrib/lite/toco/tflite/export.cc:386] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime.
 If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.contrib.lite.TFLiteConverter(). Here is a list of operators for which  you will need custom implementations: BatchMatMul, Erf, SquaredDifference.\nAborted (core dumped)\n'
None
```

**Any other info / logs**
Why is the op SquaredDifference listed in the error ?
The https://www.tensorflow.org/lite/tf_ops_compatibility says tf.squared_difference is compatible.

Thanks in advance.!"
24578,tf.version.GIT_VERSION looks like repr(GIT_VERSION),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac OS X 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
VERSION=1.13.0-dev20181225
GIT_VERSION=b'v1.12.0-5131-gc6f3c5dc48'
- Python version:
3.6.6
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
The current GIT_VERSION is `b'v1.12.0-5131-gc6f3c5dc48'`.

It starts with `b'` and ends with `'`, this seems wrong. It may break scripts that rely on the `GIT_VERSION`. Note that the `GIT_VERSION` is fine in TF 1.12 (`v1.12.0-rc2-3-ga6d8ffae09`).

**Describe the expected behavior**
I expect `v1.12.0-5131-gc6f3c5dc48` rather than `b'v1.12.0-5131-gc6f3c5dc48'`

**Code to reproduce the issue**

```python
import tensorflow as tf
print(tf.version.GIT_VERSION)
```

**Other info / logs**
I checked the other versions in `tf.version`, they look fine.

```python
>>> print(tf.version.COMPILER_VERSION)
4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.42)
>>> print(tf.version.GRAPH_DEF_VERSION)
27
>>> print(tf.version.GRAPH_DEF_VERSION_MIN_CONSUMER)
0
>>> print(tf.version.GRAPH_DEF_VERSION_MIN_PRODUCER)
0
>>> print(tf.version.VERSION)
1.13.0-dev20181225
```"
24577,total_loss attribute not found when fitting a Sequential model using a validation dataset,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac OS X 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
VERSION=1.13.0-dev20181225
GIT_VERSION=b'v1.12.0-5131-gc6f3c5dc48'
- Python version:
3.6.6
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
I try to fit a `Sequential` model with both a training dataset and a validation dataset. Works fine in TF 1.12, but the same code now fails in 1.13.0-dev20181225 (`total_loss` attribute not found).

**Describe the expected behavior**
Training should work fine.

**Code to reproduce the issue**

```python
import tensorflow as tf
from tensorflow import keras
import numpy as np

X_train = np.random.rand(1000, 2).astype(np.float32)
y_train = np.random.rand(1000).astype(np.float32)
X_valid = np.random.rand(200, 2).astype(np.float32)
y_valid = np.random.rand(200).astype(np.float32)
batch_size = 32
learning_rate = 0.01

train_set = tf.data.Dataset.from_tensor_slices((X_train, y_train)).repeat().batch(batch_size)
valid_set = tf.data.Dataset.from_tensor_slices((X_valid, y_valid)).repeat().batch(batch_size)

model = keras.models.Sequential([keras.layers.Dense(1)])
model.compile(loss=""mse"", optimizer=keras.optimizers.SGD(learning_rate))

model.fit(train_set, epochs=5,
          steps_per_epoch=len(X_train) // batch_size,
          validation_data=valid_set,
          validation_steps=len(X_valid) // batch_size)
```

**Other info / logs**
Here is the stacktrace:

```python
>>> model.fit(train_set, epochs=5,
...           steps_per_epoch=len(X_train) // batch_size,
...           validation_data=valid_set,
...           validation_steps=len(X_valid) // batch_size)
WARNING:tensorflow:From /Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py:1732: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.
WARNING:tensorflow:From /Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
Traceback (most recent call last):
  File ""<stdin>"", line 4, in <module>
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 829, in fit
    initial_epoch=initial_epoch)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1492, in fit_generator
    initial_epoch=initial_epoch)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py"", line 149, in model_iteration
    mode=mode)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py"", line 132, in configure_callbacks
    callback_model._make_eval_function()
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 2063, in _make_eval_function
    '_eval_function', [self.total_loss] + metrics_tensors)
AttributeError: 'Sequential' object has no attribute 'total_loss'
```
"
24576,Cannot import tensorflow.spectral,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac OS X 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
VERSION=1.13.0-dev20181225
GIT_VERSION=b'v1.12.0-5131-gc6f3c5dc48'
- Python version:
3.6.6
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
I cannot import `tensorflow.spectral`.

**Describe the expected behavior**
It should be possible.

**Code to reproduce the issue**

```python
from tensorflow import spectral
```

**Other info / logs**
Here is the stacktrace:

```python
>>> from tensorflow import spectral
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: cannot import name 'spectral'
```
"
24575,[Performance] Unnecessary 'memcpy' in Gather Op,"**Describe the current behavior**
In distributed mode, user define a Graph, 'distribute_runtime' partition graph, register graph, and then run graph, For example as below.
![image](https://user-images.githubusercontent.com/7778833/50445822-3140b700-094c-11e9-8d5e-78418403836f.png)
ps: Gather Op generates a new Tensor (allocation and **memcpy**), Send Op put the Tensor to local Rendezvous's table.
worker: Recv Op triggers GRPC RecvTensorResponse method via remote Rendezvous, calls EncodeTensorToByteBuffer(https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/rpc/grpc_tensor_coding.cc), if DataTypeCanUseMemcpy==true, encodes tensor with wire format instead of TensorProto serialization. in this case, TensorEncoding doesn't depend on a continuous memory(Gather Op output Tensor)

**Describe the expected behavior**
When Gather Op -> Send Op and Tensor's dtype is simple type, Gather Op's output is a 'IndexedTensor' without memcpy. The 'IndexedTensor' contains some offsets of IDs and a RefCounted TensorBuffer. In GRPC ZeroCopyStream, send the list<pair<pointer, length>> by grpc::Slice.
Performance: reduce a memcpy
Same optimization can apply to other Op, such as: Concat Op, Slice Op ...

**Other info**

"
24574,Redundant summary in KMeansClustering,"The estimator produces two summaries: `loss` and `loss/raw`. The former is due to the Estimator API, and the latter is due to [this line](https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/contrib/factorization/python/ops/kmeans.py#L209). The summaries are identical, and Im wondering what the reason for keeping the latter is. Is it that the Estimator API can, in some cases, augment the loss passed to it, or is it a leftover from before the API was introduced? Thanks!"
24573,Calling a Dense layer fails when it is created with kernel_initializer=tf.keras.initializers.Zeros(),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac OS X 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
VERSION=1.13.0-dev20181225
GIT_VERSION=b'v1.12.0-5131-gc6f3c5dc48'
- Python version:
3.6.6
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
I get a `TypeError` exception when I call a `Dense` layer which was created with `kernel_initializer=tf.keras.initializers.Zeros()`.

**Describe the expected behavior**
I expect no error, and the kernel should be initialized to zeros, just like when I set `kernel_initializer=""zeros""`.

**Code to reproduce the issue**

```python
import tensorflow as tf
from tensorflow import keras

inputs = tf.constant([[1., 2.], [3., 4.]])
layer_init0 = keras.layers.Dense(units=4, kernel_initializer=tf.keras.initializers.Zeros())
print(layer_init0(inputs))
print(layer_init0.get_weights())
```

If you do not set the `kernel_initializer`, or if you set it to `""zeros""`, everything works fine.

**Other info / logs**
Here is the stacktrace:

```python
>>> print(layer_init0(inputs))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 541, in __call__
    self._maybe_build(inputs)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1572, in _maybe_build
    self.build(input_shapes)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py"", line 949, in build
    trainable=True)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 355, in add_weight
    aggregation=aggregation)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/base.py"", line 612, in _add_variable_with_custom_getter
    **kwargs_for_getter)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_utils.py"", line 145, in make_variable
    aggregation=aggregation)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 213, in __call__
    return cls._variable_v1_call(*args, **kwargs)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 176, in _variable_v1_call
    aggregation=aggregation)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 155, in <lambda>
    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 2489, in default_variable_creator
    import_scope=import_scope, distribute_strategy=distribute_strategy)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 217, in __call__
    return super(VariableMetaclass, cls).__call__(*args, **kwargs)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 298, in __init__
    constraint=constraint)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 410, in _init_from_args
    initial_value() if init_from_fn else initial_value,
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_utils.py"", line 127, in <lambda>
    shape, dtype=dtype, partition_info=partition_info)
TypeError: __call__() got an unexpected keyword argument 'partition_info'
```

"
24572,The performance with profiler seems much slower than normal,"
    I found something uninterpretable when profiling with tensorflow profiler.
    The time measured with profiler seems much slower than without it. 
            eg: I translate a sentence with the model Transformer, it takes 70ms normally, but it takes 220ms with profiler. code as follows:
             with tf.contrib.tfprof.ProfileContext('/home/work/tmp/profile') as pctx
        Does profiler take too much overhead? Which part costs so much time?? Does the overhead take place in every op or between ops??
"
24571,Request for function argument for tf.device in eager mode,"

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No custom code.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: Using pip install.
- **TensorFlow version (use command below)**: 1.12
- **Python version**: 2.7.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A


### Describe the problem
In graph mode, `tf.device()` accepts either a device specifier or a function that specifies ops that should run on a specific device. However in eager mode, there's no ability to pass in a function.

It's marked as a todo in the code base, so might be in the pipeline but I'd love to know if there's a rough time estimate/anything in the works?
### Source code / logs
Here's the place in the codebase where its marked as a TODO:
https://github.com/tensorflow/tensorflow/blob/a6d8ffae097d0132989ae4688d224121ec6d8f35/tensorflow/python/framework/ops.py#L4928"
24570,Unexpected warning during GeneratorDataset iterator finalization,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 14.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): `('v1.12.0-0-ga6d8ffae09', '1.12.0')`
- Python version: 2.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory:
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.73       Driver Version: 410.73       CUDA Version: 10.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Quadro M2000M       Off  | 00000000:01:00.0  On |                  N/A |
| N/A   57C    P0    N/A /  N/A |    832MiB /  4043MiB |      4%      Default |
+-------------------------------+----------------------+----------------------+
```

**Describe the current behavior**
When using `from_generator` to create a `tf.data.Dataset` instance, reading from a one_shot_iterator and not explicitly closing the session object, I observe a warning: 
```
 W tensorflow/core/kernels/data/generator_dataset_op.cc:78] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
```

**Describe the expected behavior**
Would expect the program to finish without a warning.

**Code to reproduce the issue**
```
import itertools

import tensorflow as tf

def gen():
    for i in itertools.count(1):
        yield (i, [1] * i)

ds = tf.data.Dataset.from_generator(
    gen, (tf.int64, tf.int64), (tf.TensorShape([]), tf.TensorShape([None])))

iterator = ds.make_one_shot_iterator()

sess = tf.Session()
iterator = iterator.get_next()

sess.run(iterator)
# sess.close()   <-- no warning is shown if closing the session explicitly
```

**Other info / logs**
```
2018-12-25 23:43:21.645869: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-12-25 23:43:21.684610: W tensorflow/core/kernels/data/generator_dataset_op.cc:78] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}} = PyFunc[Tin=[DT_INT64], Tout=[DT_INT64], token=""pyfunc_2""](arg0)]]
```
"
24568,TF failing to build on Mac OSX because of missing ares.h even after installing c-ares ,Delete.
24567,docs:generate can't find numpy headers,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.12.0
- Python version: 3.6.7
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 0.18.1
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the problem**

arrayobject.h and other numpy headers not found when building api docs from source (*bazel run //tensorflow/tools/docs:generate -- --output_dir=/tmp/master_out* per https://www.tensorflow.org/community/documentation)

Numpy and most of other libraries are installed using *pip3 install --user*, so they are not in the */usr/lib/python3/dist-packages*, but rather in *~/.local/lib/python3.6/site-packages*.

The goal of this exercise is to build a local set of api docs for offline use (think of ""[HTML+zip]"" docs downloadable from https://docs.scipy.org/doc/ or *make html* for scikit-learn docs).

**Provide the exact sequence of commands / steps that you executed before running into the problem**

*PATH=/home/username/tmp/bazel/bin:$PATH ./configure*

*/usr/bin/python3* 

*/usr/lib/python3/dist-packages*
OR 
*/home/username/.local/lib/python3.6/site-packages/numpy/core/include/numpy*
with the same result

*PATH=/home/username/tmp/bazel/bin:$PATH bazel run //tensorflow/tools/docs:generate -- --output_dir=/tmp/master_out*

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

output of the *docs:generate* above:

> ...
> ERROR: /home/username/tmp/tensorflow-1.12.0/tensorflow/python/eager/BUILD:10:1: undeclared inclusion(s) in rule '//tensorflow/python/eager:pywrap_tfe_lib':
> this rule is missing dependency declarations for the following files included by 'tensorflow/python/eager/pywrap_tensor.cc':
>   'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/arrayobject.h'
> ...
> 

*cat .tf_configure.bazelrc*

> build --action_env PYTHON_BIN_PATH=""/usr/bin/python3""
> build --action_env PYTHON_LIB_PATH=""/home/username/.local/lib/python3.6/site-packages/numpy/core/include/numpy""
> build --python_path=""/usr/bin/python3""
> build:ignite --define with_ignite_support=true
> build:xla --define with_xla_support=true
> build --action_env TF_NEED_OPENCL_SYCL=""0""
> build --action_env TF_NEED_ROCM=""0""
> build --action_env TF_NEED_CUDA=""0""
> build --action_env TF_DOWNLOAD_CLANG=""0""
> build:opt --copt=-march=native
> build:opt --host_copt=-march=native
> build:opt --define with_default_optimizations=true
> build:v2 --define=tf_api_version=2

Attempts to list multiple include paths like *PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages:/home/username/.local/lib/python3.6/site-packages/numpy/core/include/numpy""* result in the following error:

> ERROR: /home/username/tmp/tensorflow-1.12.0/third_party/python_runtime/BUILD:5:1: no such package '@local_config_python//': Traceback (most recent call last):
> 	File ""/home/username/tmp/tensorflow-1.12.0/third_party/py/python_configure.bzl"", line 308
> 		_create_local_python_repository(repository_ctx)
> 	File ""/home/username/tmp/tensorflow-1.12.0/third_party/py/python_configure.bzl"", line 270, in _create_local_python_repository
> 		_check_python_lib(repository_ctx, python_lib)
> 	File ""/home/username/tmp/tensorflow-1.12.0/third_party/py/python_configure.bzl"", line 213, in _check_python_lib
> 		_fail((""Invalid python library path: %...))
> 	File ""/home/username/tmp/tensorflow-1.12.0/third_party/py/python_configure.bzl"", line 28, in _fail
> 		fail((""%sPython Configuration Error:%...)))
> Python Configuration Error: Invalid python library path: /usr/lib/python3/dist-packages:/home/username/.local/lib/python3.6/site-packages/numpy/core/include/numpy
>  and referenced by '//third_party/python_runtime:headers'
> ERROR: Analysis of target '//tensorflow/tools/docs:generate' failed; build aborted: Analysis failed


"
24566,leaky_relu - buggy when alpha > 1 or alpha < 0,"tf.nn.leaky_relu() is implemented as math_ops.maximum(alpha * features, features, name=name). This won't work if alpha > 1 or alpha < 0. Although this is not expected, there is nothing that prevents users from setting alpha parameter as such.
A better implementation could be nn.relu(features) - alpha*nn.relu(-features) which won't suffer from the above issue."
24565,build_pip_package failed to build,"**System information**
-macos mojave 10.14.1 :
-build source code
- 1.12
- python3.6
- Bazel 0.19 and 0.20, 0.21
**logs**
`executing command 
  (cd /private/var/tmp/_bazel_Ryan/4520a5bb283b0ab95531a31cb29b1a66/execroot/org_tensorflow && \
  exec env - \
    APPLE_SDK_PLATFORM='' \
    APPLE_SDK_VERSION_OVERRIDE='' \
    PATH=/Users/Ryan/.yarn/bin:/Users/Ryan/Library/Android/sdk/platform-tools/:/usr/local/Cellar/gcc/8.2.0/bin:/usr/local/Cellar/python/3.6.5_1/bin:/usr/local/Cellar/python@2/2.7.15_1/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/go/bin:/usr/local/mysql/bin \
    PYTHON_BIN_PATH=/usr/local/Cellar/python/3.6.5_1/bin/python3.6 \
    PYTHON_LIB_PATH=/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages \
    TF_DOWNLOAD_CLANG=0 \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL_SYCL=0 \
    TF_NEED_ROCM=0 \
    XCODE_VERSION_OVERRIDE=10.1.0 \
  external/local_config_cc/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -iquote external/com_google_absl -iquote bazel-out/darwin-opt/genfiles/external/com_google_absl -iquote bazel-out/darwin-opt/bin/external/com_google_absl -iquote external/bazel_tools -iquote bazel-out/darwin-opt/genfiles/external/bazel_tools -iquote bazel-out/darwin-opt/bin/external/bazel_tools -MD -MF bazel-out/darwin-opt/bin/external/com_google_absl/absl/base/_objs/dynamic_annotations/dynamic_annotations.d '-frandom-seed=bazel-out/darwin-opt/bin/external/com_google_absl/absl/base/_objs/dynamic_annotations/dynamic_annotations.o' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -isysroot __BAZEL_XCODE_SDKROOT__ n -Wall -Wextra -Wcast-qual -Wconversion-null -Wmissing-declarations -Woverlength-strings -Wpointer-arith -Wunused-local-typedefs -Wunused-result -Wvarargs -Wvla -Wwrite-strings -Wno-sign-compare -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/com_google_absl/absl/base/dynamic_annotations.cc -o bazel-out/darwin-opt/bin/external/com_google_absl/absl/base/_objs/dynamic_annotations/dynamic_annotations.o)
clang: error: no such file or directory: 'n'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 1.994s, Critical Path: 1.17s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]
INFO: 1 process: 1 local.
FAILED: Build did NOT complete successfully`
"
24564,"tf_python_api  build failed, ImportError: No module named '_pywrap_tensorflow_internal'","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
-  windows 10
- TensorFlow installed from: source 
- TensorFlow version: r1.8
- Python version: 3.5
- win10 + cmake +vs2015 source build, Debug X64
-  installed conda tensorflow 1.8
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.0 / 7.1
- GPU model and memory: GTX 1070

**Describe the problem**
I build 275 projects with Gpu-option, pywrap_tensorflow_internal.dll and tensorflow.dll is build out;  
All successful  except tf_python_api .  build error info :  ImportError: DLL load failed: ImportError: No module named '_pywrap_tensorflow_internal'.

I  try to fix it by copying pywrap_tensorflow_internal.dll and pywrap_tensorflow_internal.pyd into system32 dir, but it don't take effect.

Detail info:
259>------ : : tf_python_api, : Debug x64 ------
259>  Generating __init__.py files for Python API.
259>  Traceback (most recent call last):
259>    File ""E:\TensorBuilds\tensorflow\tensorflow\contrib\cmake\build\tf_python\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
259>      return importlib.import_module(mname)
259>    File ""D:\AppProgram\Anaconda2\envs\py3\lib\importlib\__init__.py"", line 126, in import_module
259>      return _bootstrap._gcd_import(name[level:], package, level)
259>    File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
259>    File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
259>    File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
259>    File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
259>    File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
259>    File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
259>    File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
259>  ImportError: DLL load failed: 
259>
259>  During handling of the above exception, another exception occurred:
259>
259>  Traceback (most recent call last):
259>    File ""E:\TensorBuilds\tensorflow\tensorflow\contrib\cmake\build\tf_python\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
259>      from tensorflow.python.pywrap_tensorflow_internal import *
259>    File ""E:\TensorBuilds\tensorflow\tensorflow\contrib\cmake\build\tf_python\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
259>      _pywrap_tensorflow_internal = swig_import_helper()
259>    File ""E:\TensorBuilds\tensorflow\tensorflow\contrib\cmake\build\tf_python\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
259>      return importlib.import_module('_pywrap_tensorflow_internal')
259>    File ""D:\AppProgram\Anaconda2\envs\py3\lib\importlib\__init__.py"", line 126, in import_module
259>      return _bootstrap._gcd_import(name[level:], package, level)
259>  ImportError: No module named '_pywrap_tensorflow_internal'
259>
259>  During handling of the above exception, another exception occurred:
259>
259>  Traceback (most recent call last):
259>    File ""E:/TensorBuilds/tensorflow/tensorflow/contrib/cmake/build/tf_python/tensorflow/tools/api/generator/create_python_api.py"", line 26, in <module>
259>      from tensorflow.python.util import tf_decorator
259>    File ""E:\TensorBuilds\tensorflow\tensorflow\contrib\cmake\build\tf_python\tensorflow\python\__init__.py"", line 49, in <module>
259>      from tensorflow.python import pywrap_tensorflow
259>    File ""E:\TensorBuilds\tensorflow\tensorflow\contrib\cmake\build\tf_python\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
259>      raise ImportError(msg)
259>  ImportError: Traceback (most recent call last):
259>    File ""E:\TensorBuilds\tensorflow\tensorflow\contrib\cmake\build\tf_python\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
259>      return importlib.import_module(mname)
259>    File ""D:\AppProgram\Anaconda2\envs\py3\lib\importlib\__init__.py"", line 126, in import_module
259>      return _bootstrap._gcd_import(name[level:], package, level)
259>    File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
259>    File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
259>    File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
259>    File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
259>    File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
259>    File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
259>    File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
259>  ImportError: DLL load failed: 
259>
259>  During handling of the above exception, another exception occurred:
259>
259>  Traceback (most recent call last):
259>    File ""E:\TensorBuilds\tensorflow\tensorflow\contrib\cmake\build\tf_python\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
259>      from tensorflow.python.pywrap_tensorflow_internal import *
259>    File ""E:\TensorBuilds\tensorflow\tensorflow\contrib\cmake\build\tf_python\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
259>      _pywrap_tensorflow_internal = swig_import_helper()
259>    File ""E:\TensorBuilds\tensorflow\tensorflow\contrib\cmake\build\tf_python\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
259>      return importlib.import_module('_pywrap_tensorflow_internal')
259>    File ""D:\AppProgram\Anaconda2\envs\py3\lib\importlib\__init__.py"", line 126, in import_module
259>      return _bootstrap._gcd_import(name[level:], package, level)
259>  ImportError: No module named '_pywrap_tensorflow_internal'
259>
259>
259>  Failed to load the native TensorFlow runtime.
259>
259>  See https://www.tensorflow.org/install/install_sources#common_installation_problems
259>
259>  for some common reasons and solutions.  Include the entire stack trace
259>  above this error message when asking for help.
259>C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(171,5): error MSB6006: cmd.exe 1
========== :  0  1  0  258  ==========



"
24563,bazel build failed,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 1809 Build 17763.195
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): compile from source
- TensorFlow version: master
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: n/a
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): Microsoft Build Tools 2015 Update 3
- CUDA/cuDNN version: 10.0
- GPU model and memory: Nvidia RTX 2080 TI, 16GB RAM



**Describe the problem**

Built as in: https://www.tensorflow.org/install/source_windows
-->console output for configuration and build process see below
Build process failed with ""C++ compilation of rule '//tensorflow/compiler/xla/service/gpu:fusion_merger' failed (Exit 2): python.exe failed: error executing command""

**Any other info / logs**
console output:
[buildconsole.txt](https://github.com/tensorflow/tensorflow/files/2708791/buildconsole.txt)

"
24562,Failed to load the native TensorFlow runtime.,"I am new to tensorflow.I have installed tensorflow without GPU as my machine won't support gpu version.
I have installed it in portable winpython.

![image](https://user-images.githubusercontent.com/46132760/50423282-63351900-0879-11e9-9d74-4753a482e0c0.png)

Here is my stack trace


Traceback (most recent call last):
  File ""C:\Program Files (x86)\JetBrains\PyCharm 5.0.6\helpers\pydev\pydev_monkey_qt.py"", line 71, in patched_import
    return original_import(name, *args, **kwargs)
  File ""D:\Life_In_IEM\Project\Predicting_Riot\Mine_Data\WPy-3670\python-3.6.7.amd64\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\Life_In_IEM\Project\Predicting_Riot\Mine_Data\WPy-3670\python-3.6.7.amd64\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""D:\Life_In_IEM\Project\Predicting_Riot\Mine_Data\WPy-3670\python-3.6.7.amd64\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""D:\Life_In_IEM\Project\Predicting_Riot\Mine_Data\WPy-3670\python-3.6.7.amd64\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed with error code -1073741795


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
24561, Tensorflow c++ compile error on linux collect2: error: ld returned 1 exit status,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04
- TensorFlow installed from (source or binary):
source
- TensorFlow version:
r1.8
- Installed using :
bazel 0.16.1
- GCC/Compiler version (if compiling from source):
5.4.0
- CUDA/cuDNN version:
9.0 / 7.4.2
- GPU model and memory:
GPU
**Describe the problem**

compile TensorFlow C++ 
bazel build --config=monolithic tensorflow:libtensorflow_cc.so
use 
g++ -std=c++11 -o tf_example \
-I/usr/local/include/tf \
-I/usr/local/include/eigen3 \
-g -Wall -D_DEBUG -Wshadow -Wno-sign-compare -w  \
-L/usr/local/lib/libtensorflow_cc \
`pkg-config --cflags --libs protobuf` -ltensorflow_cc test.cpp

to run test.cpp 
have error 
/tmp/ccPa2mDH.o: In function `main':
/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:9: undefined reference to `tensorflow::Scope::NewRootScope()'
/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:11: undefined reference to `tensorflow::Input::Initializer::Initializer(std::initializer_list<tensorflow::Input::Initializer> const&)'
/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:11: undefined reference to `tensorflow::ops::Const(tensorflow::Scope const&, tensorflow::Input::Initializer const&)'
/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:13: undefined reference to `tensorflow::Input::Initializer::Initializer(std::initializer_list<tensorflow::Input::Initializer> const&)'
/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:13: undefined reference to `tensorflow::ops::Const(tensorflow::Scope const&, tensorflow::Input::Initializer const&)'
/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:15: undefined reference to `tensorflow::Scope::WithOpName(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const'
/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:15: undefined reference to `tensorflow::ops::MatMul::MatMul(tensorflow::Scope const&, tensorflow::Input, tensorflow::Input, tensorflow::ops::MatMul::Attrs const&)'
/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:15: undefined reference to `tensorflow::Scope::~Scope()'
/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:17: undefined reference to `tensorflow::ClientSession::ClientSession(tensorflow::Scope const&)'
/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:19: undefined reference to `tensorflow::ClientSession::Run(std::vector<tensorflow::Output, std::allocator<tensorflow::Output> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) const'
/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:19: undefined reference to `tensorflow::internal::LogMessageFatal::LogMessageFatal(char const*, int)'
/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:19: undefined reference to `tensorflow::internal::LogMessageFatal::~LogMessageFatal()'
/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:21: undefined reference to `tensorflow::internal::LogMessage::LogMessage(char const*, int, int)'
/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:21: undefined reference to `tensorflow::internal::LogMessage::~LogMessage()'
/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:17: undefined reference to `tensorflow::ClientSession::~ClientSession()'
/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:9: undefined reference to `tensorflow::Scope::~Scope()'
/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:13: undefined reference to `tensorflow::Scope::~Scope()'
/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:19: undefined reference to `tensorflow::internal::LogMessageFatal::~LogMessageFatal()'
/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:21: undefined reference to `tensorflow::internal::LogMessage::~LogMessage()'
/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:17: undefined reference to `tensorflow::ClientSession::~ClientSession()'
/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:9: undefined reference to `tensorflow::Scope::~Scope()'
/tmp/ccPa2mDH.o: In function `tensorflow::TfCheckOpHelper[abi:cxx11](tensorflow::Status, char const*)':
/usr/local/include/tf/tensorflow/core/lib/core/status.h:127: undefined reference to `tensorflow::TfCheckOpHelperOutOfLine[abi:cxx11](tensorflow::Status const&, char const*)'
/tmp/ccPa2mDH.o: In function `tensorflow::TensorShapeRep::~TensorShapeRep()':
/usr/local/include/tf/tensorflow/core/framework/tensor_shape.h:510: undefined reference to `tensorflow::TensorShapeRep::DestructorOutOfLine()'
/tmp/ccPa2mDH.o: In function `tensorflow::core::RefCounted::~RefCounted()':
/usr/local/include/tf/tensorflow/core/lib/core/refcount.h:79: undefined reference to `tensorflow::internal::LogMessageFatal::LogMessageFatal(char const*, int)'
/usr/local/include/tf/tensorflow/core/lib/core/refcount.h:79: undefined reference to `tensorflow::internal::LogMessageFatal::~LogMessageFatal()'
/tmp/ccPa2mDH.o: In function `tensorflow::Tensor::operator=(tensorflow::Tensor const&)':
/usr/local/include/tf/tensorflow/core/framework/tensor.h:170: undefined reference to `tensorflow::Tensor::CopyFromInternal(tensorflow::Tensor const&, tensorflow::TensorShape const&)'
/tmp/ccPa2mDH.o: In function `tensorflow::Input::Input(tensorflow::Output const&)':
/usr/local/include/tf/tensorflow/cc/framework/ops.h:204: undefined reference to `tensorflow::Tensor::Tensor()'
/usr/local/include/tf/tensorflow/cc/framework/ops.h:204: undefined reference to `tensorflow::Tensor::~Tensor()'
/tmp/ccPa2mDH.o: In function `tensorflow::Input::Initializer::~Initializer()':
/usr/local/include/tf/tensorflow/cc/framework/ops.h:105: undefined reference to `tensorflow::Tensor::~Tensor()'
/tmp/ccPa2mDH.o: In function `tensorflow::Input::~Input()':
/usr/local/include/tf/tensorflow/cc/framework/ops.h:97: undefined reference to `tensorflow::Tensor::~Tensor()'
/tmp/ccPa2mDH.o: In function `tensorflow::TensorShape::TensorShape(std::initializer_list<long long>)':
/usr/local/include/tf/tensorflow/core/framework/tensor_shape.h:291: undefined reference to `tensorflow::TensorShapeBase<tensorflow::TensorShape>::TensorShapeBase(std::initializer_list<long long>)'
/tmp/ccPa2mDH.o: In function `tensorflow::Input::Initializer::Initializer<float, void>(std::initializer_list<float> const&)':
/usr/local/include/tf/tensorflow/cc/framework/ops.h:138: undefined reference to `tensorflow::Tensor::Tensor()'
/usr/local/include/tf/tensorflow/cc/framework/ops.h:141: undefined reference to `tensorflow::Tensor::Tensor(tensorflow::DataType, tensorflow::TensorShape const&)'
/usr/local/include/tf/tensorflow/cc/framework/ops.h:141: undefined reference to `tensorflow::Tensor::~Tensor()'
/usr/local/include/tf/tensorflow/cc/framework/ops.h:141: undefined reference to `tensorflow::Tensor::~Tensor()'
/usr/local/include/tf/tensorflow/cc/framework/ops.h:138: undefined reference to `tensorflow::Tensor::~Tensor()'
/tmp/ccPa2mDH.o: In function `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >* tensorflow::internal::MakeCheckOpString<unsigned long, unsigned long>(unsigned long const&, unsigned long const&, char const*)':
/usr/local/include/tf/tensorflow/core/platform/default/logging.h:184: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::CheckOpMessageBuilder(char const*)'
/usr/local/include/tf/tensorflow/core/platform/default/logging.h:186: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::ForVar2()'
/usr/local/include/tf/tensorflow/core/platform/default/logging.h:187: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString[abi:cxx11]()'
/usr/local/include/tf/tensorflow/core/platform/default/logging.h:184: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder()'
/usr/local/include/tf/tensorflow/core/platform/default/logging.h:184: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder()'
/tmp/ccPa2mDH.o: In function `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >* tensorflow::internal::MakeCheckOpString<long, int>(long const&, int const&, char const*)':
/usr/local/include/tf/tensorflow/core/platform/default/logging.h:184: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::CheckOpMessageBuilder(char const*)'
/usr/local/include/tf/tensorflow/core/platform/default/logging.h:186: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::ForVar2()'
/usr/local/include/tf/tensorflow/core/platform/default/logging.h:187: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString[abi:cxx11]()'
/usr/local/include/tf/tensorflow/core/platform/default/logging.h:184: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder()'
/usr/local/include/tf/tensorflow/core/platform/default/logging.h:184: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder()'
/tmp/ccPa2mDH.o: In function `tensorflow::TTypes<float, 2ul, long>::Tensor tensorflow::Tensor::tensor<float, 2ul>()':
/usr/local/include/tf/tensorflow/core/framework/tensor.h:569: undefined reference to `tensorflow::Tensor::CheckTypeAndIsAligned(tensorflow::DataType) const'
/tmp/ccPa2mDH.o: In function `tensorflow::TTypes<float, 1ul, long>::Tensor tensorflow::Tensor::shaped<float, 1ul>(tensorflow::gtl::ArraySlice<long long>)':
/usr/local/include/tf/tensorflow/core/framework/tensor.h:666: undefined reference to `tensorflow::Tensor::CheckTypeAndIsAligned(tensorflow::DataType) const'
/tmp/ccPa2mDH.o: In function `Eigen::DSizes<long, 2> tensorflow::TensorShape::AsEigenDSizes<2>() const':
/usr/local/include/tf/tensorflow/core/framework/tensor_shape.h:463: undefined reference to `tensorflow::TensorShape::CheckDimsEqual(int) const'
/tmp/ccPa2mDH.o: In function `void tensorflow::Tensor::FillDimsAndValidateCompatibleShape<1ul>(tensorflow::gtl::ArraySlice<long long>, std::array<long, 1ul>*) const':
/usr/local/include/tf/tensorflow/core/framework/tensor.h:630: undefined reference to `tensorflow::internal::LogMessageFatal::LogMessageFatal(char const*, int)'
/usr/local/include/tf/tensorflow/core/framework/tensor.h:630: undefined reference to `tensorflow::internal::LogMessageFatal::~LogMessageFatal()'
/usr/local/include/tf/tensorflow/core/framework/tensor.h:636: undefined reference to `tensorflow::internal::LogMessageFatal::LogMessageFatal(char const*, int)'
/usr/local/include/tf/tensorflow/core/framework/tensor.h:636: undefined reference to `tensorflow::internal::LogMessageFatal::~LogMessageFatal()'
/usr/local/include/tf/tensorflow/core/framework/tensor.h:630: undefined reference to `tensorflow::internal::LogMessageFatal::~LogMessageFatal()'
/usr/local/include/tf/tensorflow/core/framework/tensor.h:636: undefined reference to `tensorflow::internal::LogMessageFatal::~LogMessageFatal()'
/tmp/ccPa2mDH.o: In function `Eigen::DSizes<long, 2> tensorflow::TensorShape::AsEigenDSizesWithPadding<2>() const':
/usr/local/include/tf/tensorflow/core/framework/tensor_shape.h:470: undefined reference to `tensorflow::TensorShape::CheckDimsAtLeast(int) const'
/usr/local/include/tf/tensorflow/core/framework/tensor_shape.h:473: undefined reference to `tensorflow::TensorShapeBase<tensorflow::TensorShape>::dims() const'
/usr/local/include/tf/tensorflow/core/framework/tensor_shape.h:474: undefined reference to `tensorflow::TensorShapeBase<tensorflow::TensorShape>::dim_size(int) const'
/usr/local/include/tf/tensorflow/core/framework/tensor_shape.h:476: undefined reference to `tensorflow::TensorShapeBase<tensorflow::TensorShape>::dims() const'
/tmp/ccPa2mDH.o: In function `void std::_Destroy<tensorflow::Tensor>(tensorflow::Tensor*)':
/usr/include/c++/5/bits/stl_construct.h:93: undefined reference to `tensorflow::Tensor::~Tensor()'
/tmp/ccPa2mDH.o: In function `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >* tensorflow::internal::MakeCheckOpString<long long, long long>(long long const&, long long const&, char const*)':
/usr/local/include/tf/tensorflow/core/platform/default/logging.h:184: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::CheckOpMessageBuilder(char const*)'
/usr/local/include/tf/tensorflow/core/platform/default/logging.h:186: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::ForVar2()'
/usr/local/include/tf/tensorflow/core/platform/default/logging.h:187: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString[abi:cxx11]()'
/usr/local/include/tf/tensorflow/core/platform/default/logging.h:184: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder()'
/usr/local/include/tf/tensorflow/core/platform/default/logging.h:184: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder()'
collect2: error: ld returned 1 exit status
"
24559,[Perfomance]Dilated/Atrous Conv implementation with cudnn,"**System information**
- TensorFlow version (TF 1.12.0):
- Are you willing to contribute it (Yes):


**Describe the feature and the current behavior/state.**
The [doc](https://www.tensorflow.org/api_docs/python/tf/nn/atrous_conv2d) and the [code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_ops.py#L947) shows that dilated conv is implemented with traival ops(space_to_batch->regular conv->batch_to_space in python level. And the python code does not pass dilations parameters to lower level code. However both the registered [dilated conv ops](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/nn_ops.cc#L265), [gpu code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L2901) and [cpu code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/eigen_spatial_convolutions.h#L1568) are ready to handle with dilatied conv. It is easy to implement dialted conv with cudnn by modifying the python level code.

**Will this change the current api? How?**
NO.

**Who will benefit with this feature?**
who deploy the model which has dilated conv

**Any Other info.**
time of my model reduces from 350ms to 216ms on 1080ti.
"
24558,"Unable to build tensorflow from source on windows,cuda 10,cudnn 7.3.1,python 3.6,compute 6.1","
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 1.11
- Python version: 3.6
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): 0.17.2
- GCC/Compiler version (if compiling from source): MSVC 2015
- CUDA/cuDNN version: cuda 10.0,cudnn 7.3.1,compute 6.1
- GPU model and memory: GTX 1060 6GB GDDR6

**Describe the problem**

My GPU is only recognized by CUDA 10 that is the reason i am trying to build it from source but every time i get the same error.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. python ./configure.py
2. Do you wish to build TensorFlow with nGraph support? [y/N]: n
3. Do you wish to build TensorFlow with CUDA support? [y/N]: y
4. Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: 10.0
5. Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 7.3.1
6. Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5, 7.0]: 6.1
7. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:
8. Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: Y

Didn't added the steps related to path of python and cuda.

bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**

I have also followed the steps from the below article but in my case whenever i add the patch file it says 

no such package @eigen_archive//: Traceback (most recent call last):
 File C:/tensorflow-build/tensorflow/third_party/repo.bzl, line 106
 _apply_patch(ctx, ctx.attr.patch_file)

Below is the link of the article that i have referred.

https://medium.com/@amsokol.com/update-1-how-to-build-and-install-tensorflow-gpu-cpu-for-windows-from-source-code-using-bazel-and-c2e86fec9ef2

ERROR: C:/tensorflow-build/tensorflow/tensorflow/core/kernels/BUILD:4567:1: C++ compilation of rule '//tensorflow/core/kernels:multinomial_op_gpu' failed (Exit 1): msvc_wrapper_for_nvcc.bat failed: error executing command
  cd C:/users/biswa/_bazel_biswa/j7bi4x5j/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt;
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\LIB\amd64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\Tools;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Windows Kits\10\bin\x86;;C:\Windows\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/tensorflow-v1.1/Scripts/python.exe
    SET PYTHON_LIB_PATH=C:/tensorflow-v1.1/Lib/site-packages
    SET TEMP=C:\Users\biswa\AppData\Local\Temp
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1
    SET TF_CUDA_VERSION=10.0
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TMP=C:\Users\biswa\AppData\Local\Temp
  external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.bat /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/genfiles/external/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive /Ibazel-out/x64_windows-opt/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-opt/genfiles/external/jpeg /Ibazel-out/x64_windows-opt/bin/external/jpeg /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/genfiles/external/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/genfiles/external/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/genfiles/external/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/genfiles/external/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/gif_archive/lib /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/lib /Ibazel-out/x64_windows-opt/bin/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/windows /Ibazel-out/x64_windows-opt/bin/external/gif_archive/windows /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include/crt /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX -x cuda -DGOOGLE_CUDA=1 -nvcc_options=relaxed-constexpr -nvcc_options=ftz=true -DGOOGLE_CUDA=1 -DTENSORFLOW_MONOLITHIC_BUILD /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COMPILE_LIBRARY /Fobazel-out/x64_windows-opt/bin/tensorflow/core/kernels/_objs/multinomial_op_gpu/multinomial_op_gpu.cu.o /c tensorflow/core/kernels/multinomial_op_gpu.cu.cc
c:\users\biswa\_bazel_biswa\j7bi4x5j\execroot\org_tensorflow\external\eigen_archive\eigen\src/Core/util/Memory.h(164): warning: calling a __host__ function from a __host__ __device__ function is not allowed

c:\users\biswa\_bazel_biswa\j7bi4x5j\execroot\org_tensorflow\external\eigen_archive\eigen\src/Core/util/Memory.h(179): warning: calling a __host__ function from a __host__ __device__ function is not allowed

c:\users\biswa\_bazel_biswa\j7bi4x5j\execroot\org_tensorflow\external\eigen_archive\eigen\src/Core/arch/CUDA/Half.h(212): error: more than one instance of overloaded function ""__hadd"" matches the argument list:
            function ""__hadd(int, int)""
            function ""__hadd(__half, __half)""
            argument types are: (const Eigen::half, const Eigen::half)

c:\users\biswa\_bazel_biswa\j7bi4x5j\execroot\org_tensorflow\external\eigen_archive\eigen\src/Core/products/Parallelizer.h(20): warning: variable ""m_maxThreads"" was set but never used

c:\users\biswa\_bazel_biswa\j7bi4x5j\execroot\org_tensorflow\external\eigen_archive\eigen\src/Core/ArrayWrapper.h(94): warning: __declspec attributes ignored

external/com_google_absl\absl/strings/string_view.h(501): warning: expression has no effect

external/protobuf_archive/src\google/protobuf/arena_impl.h(55): warning: integer conversion resulted in a change of sign
external/protobuf_archive/src\google/protobuf/arena_impl.h(309): warning: integer conversion resulted in a change of sign

external/protobuf_archive/src\google/protobuf/arena_impl.h(310): warning: integer conversion resulted in a change of sign

external/protobuf_archive/src\google/protobuf/map.h(1025): warning: invalid friend declaration

.\tensorflow/core/lib/gtl/flatmap.h(157): warning: invalid friend declaration

c:\users\biswa\_bazel_biswa\j7bi4x5j\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function ""Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::VALUE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::VALUE]""
(855): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::VALUE]""
(2096): here
            instantiation of ""Eigen::internal::igamma_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma(const Scalar &, const Scalar &) [with Scalar=float]""
c:\users\biswa\_bazel_biswa\j7bi4x5j\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsHalf.h(34): here

c:\users\biswa\_bazel_biswa\j7bi4x5j\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function ""Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::VALUE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::VALUE]""
(863): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::VALUE]""
(2096): here
            instantiation of ""Eigen::internal::igamma_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma(const Scalar &, const Scalar &) [with Scalar=float]""
c:\users\biswa\_bazel_biswa\j7bi4x5j\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsHalf.h(34): here

c:\users\biswa\_bazel_biswa\j7bi4x5j\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function ""Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::DERIVATIVE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::DERIVATIVE]""
(855): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::DERIVATIVE]""
(2102): here
            instantiation of ""Eigen::internal::igamma_der_a_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma_der_a(const Scalar &, const Scalar &) [with Scalar=float]""
c:\users\biswa\_bazel_biswa\j7bi4x5j\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsHalf.h(38): here

c:\users\biswa\_bazel_biswa\j7bi4x5j\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function ""Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::DERIVATIVE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::DERIVATIVE]""
(863): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::DERIVATIVE]""
(2102): here
            instantiation of ""Eigen::internal::igamma_der_a_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma_der_a(const Scalar &, const Scalar &) [with Scalar=float]""
c:\users\biswa\_bazel_biswa\j7bi4x5j\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsHalf.h(38): here

c:\users\biswa\_bazel_biswa\j7bi4x5j\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function ""Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
(855): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
(2108): here
            instantiation of ""Eigen::internal::gamma_sample_der_alpha_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::gamma_sample_der_alpha(const Scalar &, const Scalar &) [with Scalar=float]""
c:\users\biswa\_bazel_biswa\j7bi4x5j\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsHalf.h(42): here

c:\users\biswa\_bazel_biswa\j7bi4x5j\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function ""Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
(863): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
(2108): here
            instantiation of ""Eigen::internal::gamma_sample_der_alpha_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::gamma_sample_der_alpha(const Scalar &, const Scalar &) [with Scalar=float]""
c:\users\biswa\_bazel_biswa\j7bi4x5j\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsHalf.h(42): here

c:\users\biswa\_bazel_biswa\j7bi4x5j\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function ""Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::VALUE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::VALUE]""
(855): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::VALUE]""
(2096): here
            instantiation of ""Eigen::internal::igamma_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma(const Scalar &, const Scalar &) [with Scalar=double]""
c:\users\biswa\_bazel_biswa\j7bi4x5j\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(120): here

c:\users\biswa\_bazel_biswa\j7bi4x5j\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function ""Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::VALUE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::VALUE]""
(863): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::VALUE]""
(2096): here
            instantiation of ""Eigen::internal::igamma_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma(const Scalar &, const Scalar &) [with Scalar=double]""
c:\users\biswa\_bazel_biswa\j7bi4x5j\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(120): here

c:\users\biswa\_bazel_biswa\j7bi4x5j\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function ""Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::DERIVATIVE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::DERIVATIVE]""
(855): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::DERIVATIVE]""
(2102): here
            instantiation of ""Eigen::internal::igamma_der_a_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma_der_a(const Scalar &, const Scalar &) [with Scalar=double]""
c:\users\biswa\_bazel_biswa\j7bi4x5j\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(135): here

c:\users\biswa\_bazel_biswa\j7bi4x5j\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function ""Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::DERIVATIVE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::DERIVATIVE]""
(863): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::DERIVATIVE]""
(2102): here
            instantiation of ""Eigen::internal::igamma_der_a_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma_der_a(const Scalar &, const Scalar &) [with Scalar=double]""
c:\users\biswa\_bazel_biswa\j7bi4x5j\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(135): here

c:\users\biswa\_bazel_biswa\j7bi4x5j\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function ""Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
(855): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
(2108): here
            instantiation of ""Eigen::internal::gamma_sample_der_alpha_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::gamma_sample_der_alpha(const Scalar &, const Scalar &) [with Scalar=double]""
c:\users\biswa\_bazel_biswa\j7bi4x5j\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(154): here

c:\users\biswa\_bazel_biswa\j7bi4x5j\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function ""Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
          detected during:
            instantiation of ""Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
(863): here
            instantiation of ""Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]""
(2108): here
            instantiation of ""Eigen::internal::gamma_sample_der_alpha_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::gamma_sample_der_alpha(const Scalar &, const Scalar &) [with Scalar=double]""
c:\users\biswa\_bazel_biswa\j7bi4x5j\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(154): here

1 error detected in the compilation of ""C:/Users/biswa/AppData/Local/Temp/nvcc_inter_files_tmp_dir/multinomial_op_gpu.cu.cpp1.ii"".
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 452.097s, Critical Path: 198.16s
INFO: 1906 processes: 1906 local.
FAILED: Build did NOT complete successfully
"
24557,Tensorflow autograph problem in creating AdamOptimizer,"## env
win7 x64
tf.version >= 1.12, cpu only
python 3.6.6
## code
```python
import tensorflow as tf
from tensorflow.python import autograph


@autograph.convert()
def prepare_for_cost(ys, ps, limit=tf.constant(.9)):
    l = len(ys)
    nys = []
    nps = []
    for i in range(l):
        y = ys[i]
        p = ps[i]
        if p[1] > limit:
            nys.append(y)
            nps.append(p)
        else:
            nys.append([0.0, 1.0])
            nps.append([1.0, 0.0])
    nys_list = autograph.stack(nys, tf.float32)
    nps_list = autograph.stack(nps, tf.float32)
    return nys_list, nps_list


with tf.Graph().as_default() as g, tf.Session(graph=g) as sess:
    xs = tf.placeholder(tf.float32, [None, 2])
    w = tf.Variable(tf.random_normal([2, 2]))
    ps = tf.matmul(xs, w)
    ys = tf.placeholder(tf.float32, [None, 2])

    nys, nps = prepare_for_cost(ys, ps)
    nys.set_shape(tf.TensorShape((None, 2)))
    nps.set_shape(tf.TensorShape((None, 2)))

    cost = tf.reduce_mean(
        tf.nn.softmax_cross_entropy_with_logits_v2(labels=nys, logits=nps))
    train_op = tf.train.AdamOptimizer(0.01).minimize(cost)
    sess.run(tf.global_variables_initializer())
    sess.run([train_op], feed_dict={ys: [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0]], xs: [[.2, .8], [.7, .3], [.1, .92]]})
```

error stacks below.  what's more, there's a little bug when using autograph.tensor_list to create a list, it's about none check like ```if tensor: ...```

the key tensor with variant type is:
Tensor(""gradients/prepare_for_cost/while/Switch_2_grad/b_switch:0"", shape=(), dtype=variant)

```
File ""D:/project/myshpy/test/myshtest/common/CommonPy.py"", line 36, in <module>
        train_op = tf.train.AdamOptimizer(0.01).minimize(cost)
      File ""D:\soft\python36\lib\site-packages\tensorflow\python\training\optimizer.py"", line 400, in minimize
        grad_loss=grad_loss)
      File ""D:\soft\python36\lib\site-packages\tensorflow\python\training\optimizer.py"", line 519, in compute_gradients
        colocate_gradients_with_ops=colocate_gradients_with_ops)
      File ""D:\soft\python36\lib\site-packages\tensorflow\python\ops\gradients_impl.py"", line 630, in gradients
        gate_gradients, aggregation_method, stop_gradients)
      File ""D:\soft\python36\lib\site-packages\tensorflow\python\ops\gradients_impl.py"", line 858, in _GradientsHelper
        loop_state.PostProcessing()
      File ""D:\soft\python36\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 1404, in PostProcessing
        grad_val = constant_op.constant(0, dtype=dtype, shape=shape)
      File ""D:\soft\python36\lib\site-packages\tensorflow\python\framework\constant_op.py"", line 208, in constant
        value, dtype=dtype, shape=shape, verify_shape=verify_shape))
      File ""D:\soft\python36\lib\site-packages\tensorflow\python\framework\tensor_util.py"", line 477, in make_tensor_proto
        (dtype, nparray.dtype, values))
    TypeError: Incompatible types: <dtype: 'variant'> vs. int32. Value is 0
```"
24556,AlibabaCloud OSS support,AlibabaCloud Object Storage Service(OSS) is one of the most widely used cloud stroage services in the world. Could I ask for OSS support with tensorflow?
24555,Error loading Keras model with custom layer in JSON format,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX Mojave
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12.0
- Python version: 2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I was following the Keras tutorial on TF website and I was trying out my custom layer, which was basically the same as in the tutorial.
```
class MyLayer(layers.Layer):

  def __init__(self, output_dim, **kwargs):
    self.output_dim = output_dim
    super(MyLayer, self).__init__(**kwargs)

  def build(self, input_shape):
    shape = tf.TensorShape((input_shape[1], self.output_dim))
    # Create a trainable weight variable for this layer.
    self.kernel = self.add_weight(name='kernel',
                                  shape=shape,
                                  initializer='uniform',
                                  trainable=True)
    # Be sure to call this at the end
    super(MyLayer, self).build(input_shape)

  def call(self, inputs):
    return tf.matmul(inputs, self.kernel)

  def compute_output_shape(self, input_shape):
    shape = tf.TensorShape(input_shape).as_list()
    shape[-1] = self.output_dim
    return tf.TensorShape(shape)

  def get_config(self):
    base_config = super(MyLayer, self).get_config()
    base_config['output_dim'] = self.output_dim
    return base_config

  @classmethod
  def from_config(cls, config):
    return cls(**config)


model = tf.keras.Sequential([
    MyLayer(10),
    layers.Activation('softmax')])

# The compile step specifies the training configuration
model.compile(optimizer=tf.train.RMSPropOptimizer(0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

dataset = get_dataset(*get_train_data()) # custom functions

val_dataset = get_dataset(*get_val_data())

test_dataset = get_dataset(*get_test_data())

# Trains for 5 epochs.
model.fit(dataset, steps_per_epoch=30, epochs=5, callbacks=callbacks, validation_data=val_dataset)
model.evaluate(test_dataset, steps=30)

model.save_weights('./weights/model1')
json_string = model.to_json()

pprint(json.loads(json_string))

fresh_model = tf.keras.models.model_from_json(json_string, custom_objects={'MyLayer':MyLayer})
fresh_model.load_weights('./weights/model1')

fresh_model.compile(optimizer=tf.train.RMSPropOptimizer(0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

fresh_model.evaluate(test_dataset, steps=30)
```
However, I got the following error when I tried to run it and I could not find any related issues online.
```
Traceback (most recent call last):
  File ""/Users/ray/miniconda3/envs/ml/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2878, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-2-67ef4d13fb1e>"", line 1, in <module>
    runfile('/Users/ray/WorkSpace/machinelearning/tensorflow-learn/custom_layer.py', wdir='/Users/ray/WorkSpace/machinelearning/tensorflow-learn')
  File ""/Applications/PyCharm.app/Contents/helpers/pydev/_pydev_bundle/pydev_umd.py"", line 197, in runfile
    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script
  File ""/Users/ray/WorkSpace/machinelearning/tensorflow-learn/custom_layer.py"", line 86, in <module>
    metrics=['accuracy'])
  File ""/Users/ray/miniconda3/envs/ml/lib/python2.7/site-packages/tensorflow/python/training/checkpointable/base.py"", line 474, in _method_wrapper
    method(self, *args, **kwargs)
  File ""/Users/ray/miniconda3/envs/ml/lib/python2.7/site-packages/tensorflow/python/keras/engine/training.py"", line 600, in compile
    skip_target_weighing_indices)
  File ""/Users/ray/miniconda3/envs/ml/lib/python2.7/site-packages/tensorflow/python/keras/engine/training.py"", line 134, in _set_sample_weight_attributes
    self.output_names, sample_weight_mode, skip_target_weighing_indices)
AttributeError: 'Sequential' object has no attribute 'output_names'
```

**Describe the expected behavior**
If the tutorial was correct, the code should run without any bugs and the evaluation results should be the same after loading.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached."
24554,ImportError: DLL load failed: The specified module could not be found.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home 64-bit version: 1809
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.12.0 (gpu)
- Python version: 3.6.8 64-bit
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: CUDA 10.0.130/cuDNN 7.4.2.24
- GPU model and memory: Nvidia GeForce 940MX 4GB
- GPU Drivers: 417.35



**Describe the problem**
When importing tensorflow in python interpreter ImportError is encountered.
`CUDA toolkit 10.0` is installed & tested on visual studio using sample project builds.
`cuDNN 7.4.2.24` is installed & extracted in `C:\tools\` with `C:\tools\cuda\bin` containing `cudnn64_7.dll`
```cmd
SET PATH=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\bin;%PATH%
SET PATH=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\extras\CUPTI\libx64;%PATH%
SET PATH=C:\tools\cuda\bin;%PATH%
```
I haven't installed the following as they were required for installation from source rather than binary, although the newer versions of both (2017) are installed
```
Microsoft Visual C++ 2015 Redistributable Update 3
Microsoft Build Tools 2015 Update 3
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```py
import tensorflow as tf
```

**Any other info / logs**
```
Traceback (most recent call last):
  File ""C:\Users\Danis\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Danis\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Danis\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Danis\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Danis\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Danis\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Danis\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Danis\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Danis\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Danis\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Danis\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Danis\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Danis\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```"
24549,Building from source with bazel: no such package '@local_config_cuda//cuda',"**System information**
- OS Platform and Distribution: Windows 10
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.13 
- Python version: 3.6.5
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.21.0
- CUDA/cuDNN version: 10.0/7.4.2
- GPU model and memory: GTX 960M


**Describe the problem**
I followed the instructions for building Tensorflow from source on the website. But the command 'bazel build...' gives me these errors:

> C:\tensorflow>bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package
WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
c:\tensorflow/.bazelrc
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
WARNING: Option 'experimental_shortened_obj_file_path' is deprecated
DEBUG: C:/users/maxim/_bazel_maxim/xv6zejqw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest Visual C++ installed.
DEBUG: C:/users/maxim/_bazel_maxim/xv6zejqw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variables, eg. VS140COMNTOOLS
DEBUG: C:/users/maxim/_bazel_maxim/xv6zejqw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: Looking for Visual C++ through registry
DEBUG: C:/users/maxim/_bazel_maxim/xv6zejqw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: Looking for default Visual C++ installation directory
DEBUG: C:/users/maxim/_bazel_maxim/xv6zejqw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: 'PROGRAMFILES(X86)' environment variable is not set, using 'C:\Program Files (x86)' as default
DEBUG: C:/users/maxim/_bazel_maxim/xv6zejqw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: Visual C++ build tools found at C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC
ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1556
                _create_local_cuda_repository(repository_ctx)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1395, in _create_local_cuda_repository
                find_cc(repository_ctx)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 239, in find_cc
                _get_msvc_compiler(repository_ctx)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 156, in _get_msvc_compiler
                find_msvc_tool(repository_ctx, vc_path, ""cl.exe"")
        File ""C:/users/maxim/_bazel_maxim/xv6zejqw/external/bazel_tools/tools/cpp/windows_cc_configure.bzl"", line 220, in find_msvc_tool
                repository_ctx.path((vc_path + ""\\Tools\\MSVC"")).readdir()
C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC (No such file or directory)
WARNING: Target pattern parsing failed.
ERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1556
                _create_local_cuda_repository(repository_ctx)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1395, in _create_local_cuda_repository
                find_cc(repository_ctx)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 239, in find_cc
                _get_msvc_compiler(repository_ctx)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 156, in _get_msvc_compiler
                find_msvc_tool(repository_ctx, vc_path, ""cl.exe"")
        File ""C:/users/maxim/_bazel_maxim/xv6zejqw/external/bazel_tools/tools/cpp/windows_cc_configure.bzl"", line 220, in find_msvc_tool
                repository_ctx.path((vc_path + ""\\Tools\\MSVC"")).readdir()
C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC (No such file or directory)
INFO: Elapsed time: 1,004s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
    currently loading: tensorflow/tools/pip_package

I'm new to Tf so I'm kind of lost.
Is there anything I can do ? 

Thanks.
"
24548,"The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24547,ValueError while using nce_loss with weights and biases of dtype tf.float64,"ValueError while I'm trying to use nce_loss with weights and biases of dtype tf.float64

my pseudo code
```
tf.nn.nce_loss(
    weights = nce_weights, (dtype=tf.float64)
     biases = nce_biases, (dtype=tf.float64)
      ......
```
the error:
```
  ...(ignored)...
  File ""##\Python36\lib\site-packages\tensorflow\python\ops\nn_impl.py"", line 1242, in nce_loss
    name=name)
  File ""##\Python36\lib\site-packages\tensorflow\python\ops\nn_impl.py"", line 1124, in _compute_sampled_logits
    true_logits -= math_ops.log(true_expected_count)
  File ""##\Python36\lib\site-packages\tensorflow\python\ops\math_ops.py"", line 970, in binary_op_wrapper
    y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=""y"")
  File ""##\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 1014, in convert_to_tensor
    as_ref=False)
  File ""##\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 1104, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""##\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 947, in _TensorTensorConversionFunction
    (dtype.name, t.dtype.name, str(t)))
ValueError: Tensor conversion requested dtype float64 for Tensor with dtype float32: 'Tensor(""nce_loss/Log:0"", shape=(?, 1), dtype=float32)'
```
After some tracing back, I located the problem at the function ""_compute_sampled_logits"" in the file ""nn_impl.py"".  The Problem came from these codes:
```
    ...
    if subtract_log_q:
      # Subtract log of Q(l), prior probability that l appears in sampled.
      true_logits -= math_ops.log(true_expected_count)
      sampled_logits -= math_ops.log(sampled_expected_count)
    ...
```
Where the `true_expected_count` was generated automatically with default dtype float32. But the `true_logits` has the same dtype float64 as my input. It came into crashes when trying the subtraction. Finally, I fixed this by adding somthing like these:
```
    if subtract_log_q:
      # Subtract log of Q(l), prior probability that l appears in sampled.
      if true_logits.dtype == dtypes.float64:
        true_expected_count = math_ops.cast(true_expected_count, dtypes.float64)
      true_logits -= math_ops.log(true_expected_count)
      if sampled_logits.dtype == dtypes.float64:
        sampled_expected_count = math_ops.cast(sampled_expected_count, dtypes.float64)
      sampled_logits -= math_ops.log(sampled_expected_count)
```
I'm not very familiar with the TensorFlows mechanism. But I noticed that the same logic is still exist in higher versions of TensorFlow. So I believe this bug still exists.

**Environments**
- tf version 1.8.0
- python 3.6.0
"
24546,"tf.contrib.layers.batch_norm(g3, epsilon=1e-5, scope='g_b3') with bach size 1 error","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 18.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):anaconda3
- TensorFlow version (use command below):1.9
- Python version:3.6.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):5.2
- CUDA/cuDNN version: 
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
InternalError: cuDNN launch failure : input shape ([1,1,56,56])
**Describe the expected behavior**

**Code to reproduce the issue**
InternalError: cuDNN launch failure : input shape ([1,1,56,56])
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
def generator(z, batch_size, z_dim):
    g_w1 = tf.get_variable('g_w1', [z_dim, 3136], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))
    g_b1 = tf.get_variable('g_b1', [3136], initializer=tf.truncated_normal_initializer(stddev=0.02))
    g1 = tf.matmul(z, g_w1) + g_b1
    g1 = tf.reshape(g1, [-1, 56, 56, 1])
#     g1 = tf.contrib.layers.batch_norm(g1, epsilon=1e-5, scope='g_b1')
    g1 = tf.nn.relu(g1)

    # Generate 50 features
    g_w2 = tf.get_variable('g_w2', [3, 3, 1, z_dim/2], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))
    g_b2 = tf.get_variable('g_b2', [z_dim/2], initializer=tf.truncated_normal_initializer(stddev=0.02))
    g2 = tf.nn.conv2d(g1, g_w2, strides=[1, 2, 2, 1], padding='SAME')
    g2 = g2 + g_b2
#     g2 = tf.contrib.layers.batch_norm(g2, epsilon=1e-5, scope='g_b2')
    g2 = tf.nn.relu(g2)
    g2 = tf.image.resize_images(g2, [56, 56])

    # Generate 25 features
    g_w3 = tf.get_variable('g_w3', [3, 3, z_dim/2, z_dim/4], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))
    g_b3 = tf.get_variable('g_b3', [z_dim/4], initializer=tf.truncated_normal_initializer(stddev=0.02))
    g3 = tf.nn.conv2d(g2, g_w3, strides=[1, 2, 2, 1], padding='SAME')
    g3 = g3 + g_b3
#     g3 = tf.contrib.layers.batch_norm(g3, epsilon=1e-5, scope='g_b3')
    g3 = tf.nn.relu(g3)
    g3 = tf.image.resize_images(g3, [56, 56])

    # Final convolution with one output channel
    g_w4 = tf.get_variable('g_w4', [1, 1, z_dim/4, 1], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))
    g_b4 = tf.get_variable('g_b4', [1], initializer=tf.truncated_normal_initializer(stddev=0.02))
    g4 = tf.nn.conv2d(g3, g_w4, strides=[1, 2, 2, 1], padding='SAME')
    g4 = g4 + g_b4
    g4 = tf.sigmoid(g4)
    
    # Dimensions of g4: batch_size x 28 x 28 x 1
    return g4
z_dimensions = 100
z_placeholder = tf.placeholder(tf.float32, [None, z_dimensions])
generated_image_output = generator(z_placeholder, 1, z_dimensions)

z_batch = np.random.normal(0, 1, [1, z_dimensions])
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    generated_image = sess.run(generated_image_output,
                                feed_dict={z_placeholder: z_batch})
    generated_image = generated_image.reshape([28, 28])
    plt.imshow(generated_image, cmap='Greys')
**Other info / logs**
InternalError: cuDNN launch failure : input shape ([1,1,56,56])
         [[Node: g_b1_1/FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, data_format=""NCHW"", epsilon=1.001e-05, is_training=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](g_b1_1/FusedBatchNorm-0-TransposeNHWCToNCHW-LayoutOptimizer, g_b1_1/Const, g_b1/beta/read, g_b1_1/Const_1, g_b1_1/Const_1)]]

Caused by op 'g_b1_1/FusedBatchNorm', defined at:
  File ""/home/humaolin/anaconda3/bin/ipython"", line 11, in <module>
    sys.exit(start_ipython())
  File ""/home/humaolin/anaconda3/lib/python3.6/site-packages/IPython/__init__.py"", line 125, in start_ipython
    return launch_new_instance(argv=argv, **kwargs)
  File ""/home/humaolin/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/home/humaolin/anaconda3/lib/python3.6/site-packages/IPython/terminal/ipapp.py"", line 356, in start
    self.shell.mainloop()
  File ""/home/humaolin/anaconda3/lib/python3.6/site-packages/IPython/terminal/interactiveshell.py"", line 485, in mainloop
    self.interact()
  File ""/home/humaolin/anaconda3/lib/python3.6/site-packages/IPython/terminal/interactiveshell.py"", line 476, in interact
    self.run_cell(code, store_history=True)
  File ""/home/humaolin/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2662, in run_cell
    raw_cell, store_history, silent, shell_futures)
  File ""/home/humaolin/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2785, in _run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/home/humaolin/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2903, in run_ast_nodes
    if self.run_code(code, result):
  File ""/home/humaolin/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2963, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-3-a711fc10a844>"", line 39, in <module>
    generated_image_output = generator(z_placeholder, 1, z_dimensions)
  File ""<ipython-input-3-a711fc10a844>"", line 6, in generator
    g1 = tf.contrib.layers.batch_norm(g1, epsilon=1e-5, scope='g_b1')
  File ""/home/humaolin/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 183, in func_with_args
    return func(*args, **current_args)
  File ""/home/humaolin/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 650, in batch_norm
    outputs = layer.apply(inputs, training=is_training)
  File ""/home/humaolin/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 774, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""/home/humaolin/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 329, in __call__
    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""/home/humaolin/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 703, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/home/humaolin/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/normalization.py"", line 158, in call
    return super(BatchNormalization, self).call(inputs, training=training)
  File ""/home/humaolin/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/layers/normalization.py"", line 511, in call
    outputs = self._fused_batch_norm(inputs, training=training)
  File ""/home/humaolin/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/layers/normalization.py"", line 398, in _fused_batch_norm
    training, _fused_batch_norm_training, _fused_batch_norm_inference)
  File ""/home/humaolin/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/utils/tf_utils.py"", line 51, in smart_cond
    pred, true_fn=true_fn, false_fn=false_fn, name=name)
  File ""/home/humaolin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/smart_cond.py"", line 54, in smart_cond
    return true_fn()
  File ""/home/humaolin/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/layers/normalization.py"", line 384, in _fused_batch_norm_training
    data_format=self._data_format)
  File ""/home/humaolin/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py"", line 906, in fused_batch_norm
    name=name)
  File ""/home/humaolin/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 3465, in _fused_batch_norm
    is_training=is_training, name=name)
  File ""/home/humaolin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/humaolin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3414, in create_op
    op_def=op_def)
  File ""/home/humaolin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InternalError (see above for traceback): cuDNN launch failure : input shape ([1,1,56,56])
         [[Node: g_b1_1/FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, data_format=""NCHW"", epsilon=1.001e-05, is_training=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](g_b1_1/FusedBatchNorm-0-TransposeNHWCToNCHW-LayoutOptimizer, g_b1_1/Const, g_b1/beta/read, g_b1_1/Const_1, g_b1_1/Const_1)]]

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24545,Saver restore OOM with sharded=True,"My model is saved with `tf.train.Saver(sharded=True)`. In predicting process, it has OOM problem when loading it in chief worker using `tf.train.Saver(sharded=True)`. However, it works fine using `tf.train.Saver(sharded=False)`.
tf version: 1.12"
24544,Error ValueError: Unknown field group_norm,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0
- Python version: 3.5.2
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version: 9.0
- GPU model and memory: GTX 1060

**Describe the current behavior**
I have a Docker instance of Cuda and Tensorflow which I run with a nvidia container.

This container is being started with the following command: sudo docker run --runtime=nvidia -it -v $(pwd):/code --rm -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY=$DISPLAY -e QT_X11_NO_MITSHM=1 ce2d9c04faa2 bash

When executing the following command in the container : python3 model_main.py --pipeline_config_path ./train_results/rcnn_inception3/faster_rcnn_inception_v2_coco.config --model_dir ./train_results/rcnn_inception3/ --logstderr

I get the exception: 

Traceback (most recent call last):
  File ""model_main.py"", line 109, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""model_main.py"", line 105, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py"", line 471, in train_and_evaluate
    return executor.run()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py"", line 610, in run
    return self.run_local()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py"", line 711, in run_local
    saving_listeners=saving_listeners)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 1207, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 1234, in _train_model_default
    input_fn, model_fn_lib.ModeKeys.TRAIN))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 1075, in _get_features_and_labels_from_input_fn
    self._call_input_fn(input_fn, mode))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 1162, in _call_input_fn
    return input_fn(**kwargs)
  File ""/code/image_retrieval/tensorflow/models/research/object_detection/inputs.py"", line 488, in _train_input_fn
    batch_size=params['batch_size'] if params else train_config.batch_size)
  File ""/code/image_retrieval/tensorflow/models/research/object_detection/builders/dataset_builder.py"", line 145, in build
    num_parallel_calls=num_parallel_calls)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1040, in map
    return ParallelMapDataset(self, map_func, num_parallel_calls)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/dataset_ops.py"", line 2649, in __init__
    use_inter_op_parallelism)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/dataset_ops.py"", line 2611, in __init__
    map_func, ""Dataset.map()"", input_dataset)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1860, in __init__
    self._function.add_to_graph(ops.get_default_graph())
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/function.py"", line 479, in add_to_graph
    self._create_definition_if_needed()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/function.py"", line 335, in _create_definition_if_needed
    self._create_definition_if_needed_impl()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/function.py"", line 344, in _create_definition_if_needed_impl
    self._capture_by_value, self._caller_device)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/function.py"", line 864, in func_graph_from_py_func
    outputs = func(*func_graph.inputs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1794, in tf_data_structured_function_wrapper
    ret = func(*nested_args)
  File ""/code/image_retrieval/tensorflow/models/research/object_detection/builders/dataset_builder.py"", line 129, in process_fn
    processed_tensors = transform_input_data_fn(processed_tensors)
  File ""/code/image_retrieval/tensorflow/models/research/object_detection/inputs.py"", line 465, in transform_and_pad_input_data_fn
    model = model_builder.build(model_config, is_training=True)
  File ""/code/image_retrieval/tensorflow/models/research/object_detection/builders/model_builder.py"", line 122, in build
    add_summaries)
  File ""/code/image_retrieval/tensorflow/models/research/object_detection/builders/model_builder.py"", line 389, in _build_faster_rcnn_model
    frcnn_config.first_stage_box_predictor_conv_hyperparams, is_training)
  File ""/code/image_retrieval/tensorflow/models/research/object_detection/builders/hyperparams_builder.py"", line 218, in build
    if hyperparams_config.HasField('group_norm'):
ValueError: Unknown field group_norm.

My config file is the default one, with changed pathes to fit the environment

**Describe the expected behavior**

Tensorflow training session being started

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24543,g,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24542,Step to Build for Centos 7,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): Latest
- Are you willing to contribute it (Yes/No): No



Can anyone give the steps for building this for Centos 7. "
24541,How to resize segmentation map to the size of input image obtained from Deeplab?,"Currently, Deeplab v3 return resized (small) image and its corresponding mask. But, I need segmentation mask for the original input image (large)? How can I achieve this?
"
24540,How to use the Indexdslice for sparse update ,"Hi~everyone :)
Im try to use  sparsed gradients to the apply_gradient func and I have turn my gradients to IndexdSlice:
`grads[i] = tf.IndexedSlices(values = tf.gather_nd(grads[i], idx), indices = idx, dense_shape = tf.identity(grads[i].get_shape()))`
and with vars_list,
`vars_list = [v for _, v in grads_vars]
....
grads_vars[i]  = (grads[i], vars_list[i])`
the I put the grads_vars into the apply_gradient func , but it didn't work, this is the error msg:
`Traceback (most recent call last):
  File ""12-13_linner.py"", line 147, in <module>
    residual, new_grads_vars, grads, residual_mask, new_grads = grads_sparse(grads_vars, global_steps, residual_accum, sess)
  File ""12-13_linner.py"", line 63, in grads_sparse
    vars_list[i].scatter_sub(delta, use_locking=False)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py"", line 700, in scatter_sub
    use_locking=use_locking)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_state_ops.py"", line 1141, in scatter_sub
    use_locking=use_locking, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py"", line 454, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3155, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1731, in __init__
    control_input_ops)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1579, in _create_c_op
    raise ValueError(str(e))
ValueError: Shapes must be equal rank, but are 1 and 3 for 'ScatterSub' (op: 'ScatterSub') with input shapes: [1,10], [?,2], [?].
`  
then I print the grads , 
`[IndexedSlicesValue(values=array([ 0.15668039,  0.14776216, -0.15483621, -0.15541458,  0.14010738,
       -0.2145288 , -0.19674896], dtype=float32), indices=array([[0, 1],
       [0, 2],
       [0, 3],
       [0, 5],
       [0, 6],
       [0, 8],
       [0, 9]]), dense_shape=array([ 1, 10], dtype=int32)), 
IndexedSlicesValue(values=array([ 0.10107333, -0.17999038,  0.24159592, -0.18415265, -0.16449636,
       -0.15763052,  0.24781035, -0.11305401], dtype=float32), indices=array([[0, 0],
       [0, 1],
       [0, 2],
       [0, 3],
       [0, 5],
       [0, 6],
       [0, 8],
       [0, 9]]), dense_shape=array([ 1, 10], dtype=int32)), 
IndexedSlicesValue(values=array([-0.2975006 , -0.12903467, -0.1116452 , -0.15254416, -0.15824951,
       -0.33808792, -0.10385308], dtype=float32), indices=array([[0, 0],
       [2, 0],
       [3, 0],
       [6, 0],
       [7, 0],
       [8, 0],
       [9, 0]]), dense_shape=array([10,  1], dtype=int32)), 
IndexedSlicesValue(values=array([-0.53333503], dtype=float32), indices=array([[0, 0]]), dense_shape=array([1, 1], dtype=int32))]`
and var_list:
`[array([[ 0.62123364,  0.8572454 ,  0.64919454,  1.4851073 ,  1.0997279 ,
        -1.718374  ,  0.59419113, -1.4921925 ,  0.0046594 ,  1.0894274 ]],
      dtype=float32), 
 array([[0.1021007 , 0.03610747, 0.08513964, 0.08579567, 0.08281428,
        0.05231547, 0.04030512, 0.11618505, 0.14989284, 0.06900915]],
      dtype=float32), 
 array([[-0.57928437],
       [-2.0731926 ],
       [-1.3741436 ],
       [-0.32964215],
       [ 0.69279206],
       [ 0.22279441],
       [ 1.3773512 ],
       [-0.14758699],
       [-0.3795929 ],
       [ 1.3115551 ]], dtype=float32), 
 array([[-0.03738503]], dtype=float32)]`
it's seems that the have the same shape, can anyone help me how to fix it ? or tell me the right way to use the apply_gradients with the IndexdSlice, pls.

"
24539,[feature request] make MutableHashTableOfScalars and MutableHashTableOfTensors trainable,"**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): Yes


**Describe the feature and the current behavior/state.**

This issue is related to issue #19324, in which we need some technique to do embedding on a large sparse hash table, so that we can reduce memory usage and the probability of hash collision.

As far as I can see, ops like MutableHashTableOfScalars and MutableHashTableOfTensors in [lookup table ops](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/lookup_ops.cc) can be used to meet this demand. The main problem is that they are not trainable. 

In other words, what we need may be a feature like this:
```
# define a Hash Table
table = tf.contrib.lookup.MutableHashTable(
    key_dtype=tf.string,
    value_dtype=tf.float64,
    default_value=-1)
params = table.lookup([""w1"", ""w2"", ""b""])
# define other ops with params
...
# optimize the table
train_op = optimizer.minimize(loss, var_list=[table], global_step=global_step)
```

**Will this change the current api? How?**

Yes, the current API would be changed. 

In order to implement a feature like this, we may need to:
* do modification on the lookup table ops to make it trainable. This involves several parts of fix like:
   1. some fix in python layer as only variables are trainable now.
   2. several new ops may be add in lookup table so that we can update the gradients. 
* modify the api of MutableHashTable, so that we can pass more args, like: 
   1. shard it to different parameter servers.
   2. when value for a key doesn't exist, we can initialize it with different initial value.
   3. we can make the KV pairs have TTL.
* add a new API like embedding_lookup_sparse, or do modification on it.
* make sure the checkpoint for hash tables can work properly

**Who will benefit with this feature?**

As a matter of fact, sparse models in recommend system may benefit with this feature a lot.
"
24537,tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[30003] and type half on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cuda_host_bfc,"I am working on Sparse autoencoder model which have 15 convolution layers and 21 transpose convolution layers. I am running my code in a multi GPU system. This code is running well in the small dataset, but I am getting OMM resource exhausted error  when running on a huge dataset. I changed the batch size to 8 but still facing same error. Any help will be appreciated.

**Traceback:**
[[[[[Node: tower_1/DecodeRaw/_193 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:1"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_15_tower_1/DecodeRaw"", tensor_type=DT_HALF, _device=""/job:localhost/replica:0/task:0/device:GPU:1""]()]]](url)
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

Exception in thread QueueRunnerThread-tower_1/shuffle_batch/random_shuffle_queue-tower_1/shuffle_batch/random_shuffle_queue_enqueue:
Traceback (most recent call last):
  File ""/opt/rh/rh-python36/root/usr/lib64/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/opt/rh/rh-python36/root/usr/lib64/python3.6/threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 252, in _run
    enqueue_callable()
  File ""/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1205, in _single_operation_run
    self._call_tf_sessionrun(None, {}, [], target_list, None)
  File ""/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1350, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[30003] and type half on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cuda_host_bfc
         [[Node: tower_1/DecodeRaw = DecodeRaw[little_endian=true, out_type=DT_HALF, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](tower_1/ReaderReadV2:1)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

         [[Node: tower_1/DecodeRaw/_193 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:1"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_15_tower_1/DecodeRaw"", tensor_type=DT_HALF, _device=""/job:localhost/replica:0/task:0/device:GPU:1""]()]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.](url)
](url)

**System information**
    Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
    OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux (Centos 7)
    TensorFlow installed from (source or binary): Binary
    TensorFlow version (use command below): 1.10
    Python version: 3.6.5_1
    Bazel version (if compiling from source): N/A
    GCC/Compiler version (if compiling from source): N/A
    CUDA/cuDNN version: Cuda compilation tools, release 9.0, V9.0.176
    GPU model and memory: NVIDIA TITAN V (4 GPUs)
    Exact command to reproduce: (see above)"
24536,Windows:  //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: Windows 10
- TensorFlow installed from (source or binary): Source
- TensorFlow version: Master
- Python version: 3.6
- Installed using virtualenv? pip? conda?: VirtualEnv and Pip
- Bazel version (if compiling from source): 0.18 (Have tried 0.21)
- GCC/Compiler version (if compiling from source): Visual Studio 2015
- CUDA/cuDNN version: CUDA = 9, cuDNN = 7
- GPU model and memory: GeForce GTX 950M 4GB



**Describe the problem**
The build process seems like it completes, but when it gets up to generating the python pip library - it fails at ""ImportError: DLL load failed: The specified module could not be found.""

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Here is the full dump
```

(venv) E:\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
ERROR: Failed to query DisplayName of HKCU\Software\Microsoft\Windows\CurrentVersion\Uninstall\Google Chrome
WARNING: Option 'experimental_shortened_obj_file_path' is deprecated
WARNING: E:/tensorflow/tensorflow/python/BUILD:3028:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: E:/tensorflow/tensorflow/contrib/metrics/BUILD:16:1: in py_library rule //tensorflow/contrib/metrics:metrics_py: target '//tensorflow/contrib/metrics:metrics_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: E:/tensorflow/tensorflow/python/BUILD:98:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: E:/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: E:/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
WARNING: E:/tensorflow/tensorflow/contrib/gan/BUILD:140:1: in py_library rule //tensorflow/contrib/gan:losses_impl: target '//tensorflow/contrib/gan:losses_impl' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: E:/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:356:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: E:/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:76:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: E:/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:233:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: E:/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: E:/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: E:/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: E:/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded).
INFO: Found 1 target...
ERROR: E:/tensorflow/tensorflow/python/keras/api/BUILD:45:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Exit 1): bash.exe failed: error executing command
  cd C:/users/benitch/_bazel_benitch/4ejpfwyr/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=E:/CUDA
    SET CUDNN_INSTALL_PATH=E:/CUDA/cudNN
    SET PATH=E:\msys64\usr\bin;E:\msys64\bin;E:\venv\Scripts;E:\CUDA\bin;E:\CUDA\libnvvp;C:\ProgramData\Oracle\Java\javapath;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files\Microsoft SQL Server\130\Tools\Binn\;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\Program Files\nodejs\;C:\Program Files (x86)\WinMerge;C:\Users\Benitch\.dnx\bin;C:\Program Files\Microsoft DNX\Dnvm\;C:\Program Files\Intel\WiFi\bin\;C:\Program Files\Common Files\Intel\WirelessCommon\;C:\Program Files\Git LFS;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\Microsoft VS Code\bin;C:\Program Files\dotnet\;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;E:\bazel;E:\msys64\usr\bin;C:\Users\Benitch\.dnx\bin;C:\Program Files\Microsoft DNX\Dnvm\;C:\Users\Benitch\AppData\Local\Programs\Python\Python36\Scripts\;C:\Users\Benitch\AppData\Local\Programs\Python\Python36\;C:\Users\Benitch\AppData\Local\Microsoft\WindowsApps;C:\Users\Benitch\AppData\Roaming\npm
    SET PYTHON_BIN_PATH=E:/venv/Scripts/python.exe
    SET PYTHON_LIB_PATH=E:/venv/Lib/site-packages
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=5.0
    SET TF_CUDA_VERSION=9.0
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
  E:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.exe  --apidir=bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api_v2/ --apiname=keras --apiversion=2  --package=tensorflow.python,tensorflow.python.keras --output_package=tensorflow.python.keras.api._v2 bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/activations/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/applications/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/applications/densenet/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/applications/inception_resnet_v2/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/applications/inception_v3/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/applications/mobilenet/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/applications/mobilenet_v2/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/applications/nasnet/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/applications/resnet50/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/applications/vgg16/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/applications/vgg19/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/applications/xception/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/backend/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/callbacks/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/constraints/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/datasets/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/datasets/boston_housing/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/datasets/cifar10/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/datasets/cifar100/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/datasets/fashion_mnist/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/datasets/imdb/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/datasets/mnist/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/datasets/reuters/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/estimator/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/experimental/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/initializers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/layers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/losses/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/metrics/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/models/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/optimizers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/preprocessing/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/preprocessing/image/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/preprocessing/sequence/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/preprocessing/text/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/regularizers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/utils/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/wrappers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/wrappers/scikit_learn/__init__.py
Traceback (most recent call last):
  File ""\\?\C:\Users\Benitch\AppData\Local\Temp\Bazel.runfiles_05mo_66t\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""\\?\C:\Users\Benitch\AppData\Local\Temp\Bazel.runfiles_05mo_66t\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""\\?\C:\Users\Benitch\AppData\Local\Temp\Bazel.runfiles_05mo_66t\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""E:\venv\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""E:\venv\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""\\?\C:\Users\Benitch\AppData\Local\Temp\Bazel.runfiles_05mo_66t\runfiles\org_tensorflow\tensorflow\python\tools\api\generator\create_python_api.py"", line 27, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""\\?\C:\Users\Benitch\AppData\Local\Temp\Bazel.runfiles_05mo_66t\runfiles\org_tensorflow\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""\\?\C:\Users\Benitch\AppData\Local\Temp\Bazel.runfiles_05mo_66t\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""\\?\C:\Users\Benitch\AppData\Local\Temp\Bazel.runfiles_05mo_66t\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""\\?\C:\Users\Benitch\AppData\Local\Temp\Bazel.runfiles_05mo_66t\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""\\?\C:\Users\Benitch\AppData\Local\Temp\Bazel.runfiles_05mo_66t\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""E:\venv\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""E:\venv\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 4789.707s, Critical Path: 581.88s
INFO: 4676 processes: 4676 local.
FAILED: Build did NOT complete successfully
```

At the moment I am now trying to recompile without using venv; as the issue looks like it begins when msys64 is invoked - I have set my PATH variable to use the venv version of Python I have set up; is there something else I am maybe missing? 

edit: Editing my Path didn't help either, any help would be appreciated!"
24535,initial_sparsity should be float but instead its integer in hparams,"
Hi,
im trying to determine an initial sparsity level so i can continue pruning a model of which i already pruned and has a known initial sparsity level,
but it seems i cannot use the initial sparsity level because it is determine as an integer in  Hparams.
i dont want to build a new directory and i would like to use the get_hparams function you kindly provided.
if this issue could be resolved it will be great!
thank you."
24534,The Windows GPU binary from googleapis.com cannot see the GPU device,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary: https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-windows-x86_64-1.12.0.zip
- TensorFlow version: 1.12.0
- Python version: N/A
- Installed using virtualenv? pip? conda?: no
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: CUDA 9.0, cuDNN64_7
- GPU model and memory: GeForce 1080 Ti



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

The Windows GPU binary I downloaded from [googleapis.com](https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-windows-x86_64-1.12.0.zip) seems unable to see the GPU device on my machine. I can only see the CPU device in the return value of `TF_SessionListDevices`:
> $/job:localhost/replica:0/task:0/device:CPU:0 ($CPU)

I tried 1.10.0, 1.11.0 and 1.12.0, and their RC releases, none of them worked.

However I can confirm that the PIP installed tensorflow-gpu can see the GPU device and works well on the same graph:

> 2018-12-24 02:46:37.352485: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
> 2018-12-24 02:46:37.687436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:
> name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645
> pciBusID: 0000:1f:00.0
> totalMemory: 11.00GiB freeMemory: 9.10GiB
> 2018-12-24 02:46:37.709856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
> 

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24533,The include path '%{computecpp_toolkit_path}' has an unrecognized %prefix%,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1 LTS on ODROID
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO
- TensorFlow installed from (source or binary): source (building from source)
- TensorFlow version: r1.12
- Python version: 3.6.7
- Installed using virtualenv? pip? conda?: n/a
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source):  7.3.0
- CUDA/cuDNN version: n/a (ComputeCpp Info (CE 1.0.3))
- GPU model and memory: (ARM Mali-T628)

**Describe the problem**
I am trying to build Tensorflow r1.12 with OpenCL support. I have ComputeCpp Info (CE 1.0.3). I am getting following error.

**ERROR: /home/odroid/.cache/bazel/_bazel_odroid/0a12176dc649286ae7843d8ea212d4e2/external/local_config_sycl/crosstool/BUILD:12:1: in cc_toolchain rule @local_config_sycl//crosstool:cc-compiler-local: The include path '%{computecpp_toolkit_path}' has an unrecognized %prefix%
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis of target '@local_config_sycl//crosstool:cc-compiler-local' failed; build aborted**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
 After ./configure with OpenCL and ComputeCpp, I execute following command

bazel build --local_resources 2048,.5,1.0 -c opt --config=sycl //tensorflow/tools/pip_package:build_pip_package

It gives:

Starting local Bazel server and connecting to it...
...................................................
**ERROR: /home/odroid/.cache/bazel/_bazel_odroid/0a12176dc649286ae7843d8ea212d4e2/external/local_config_sycl/crosstool/BUILD:12:1: in cc_toolchain rule @local_config_sycl//crosstool:cc-compiler-local: The include path '%{computecpp_toolkit_path}' has an unrecognized %prefix%
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted:** Analysis of target '@local_config_sycl//crosstool:cc-compiler-local' failed; build aborted
INFO: Elapsed time: 8.349s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (67 packages loaded)
    currently loading: tensorflow/python ... (3 packages)

Thanks for the help."
24531,RTX 2080TI Tensor Cores,"Hello,

I am wondering if there is a way to use tensor cores from rtx 2080ti in tensorflow. I tried to use float16 in dtypes but the performance is actually worse than when using float32. What could be the problem? Is there support for tensor cores in these new video cards?"
24530,I get problem when I run my tensorflow codes on multi gpu,"I still have problem after I add

```
sess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)
sess_config.gpu_options.allow_growth = True
with tf.Session(config=sess_config) as sess:
```

I use Tensorflow-gpu 1.12.0.I run BIDAF model on multi gpu and I get the problem below.


```

Device mapping:
/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device
/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:3b:00.0, compute capability: 6.1
Traceback (most recent call last):
  File ""/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    return fn(*args)
  File ""/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1317, in _run_fn
    self._extend_graph()
  File ""/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1352, in _extend_graph
    tf_session.ExtendSession(self._session)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation tower_0/passage_encoding/bidirectional_rnn/fw/fw/Assert/Assert: Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
Registered kernels:
  device='CPU'

         [[{{node tower_0/passage_encoding/bidirectional_rnn/fw/fw/Assert/Assert}} = Assert[T=[DT_STRING,DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=""/device:GPU:0""](tower_0/passage_encoding/bidirectional_rnn/fw/fw/All, tower_0/passage_encoding/bidirectional_rnn/fw/fw/Assert/Assert/data_0, tower_0/passage_encoding/bidirectional_rnn/fw/fw/stack, tower_0/passage_encoding/bidirectional_rnn/fw/fw/Assert/Assert/data_2, tower_0/passage_encoding/bidirectional_rnn/fw/fw/Shape_1)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""run.py"", line 408, in <module>
    run()
  File ""run.py"", line 399, in run
    multi_gpu_train(args)
  File ""run.py"", line 249, in multi_gpu_train
    model = RCModel_ngpus(args, data)
  File ""/home/home1/dmyan/codes/tensorflow/rc_model_multi_gpu.py"", line 80, in __init__
    self.sess.run(tf.global_variables_initializer())
  File ""/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation tower_0/passage_encoding/bidirectional_rnn/fw/fw/Assert/Assert: Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
Registered kernels:
  device='CPU'

         [[node tower_0/passage_encoding/bidirectional_rnn/fw/fw/Assert/Assert (defined at /home/home1/dmyan/codes/tensorflow/layers/basic_rnn.py:51)  = Assert[T=[DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=""/device:GPU:0""](tower_0/passage_encoding/bidirectional_rnn/fw/fw/All, tower_0/passage_encoding/bidirectional_rnn/fw/fw/Assert/Assert/data_0, tower_0/passage_encoding/bidirectional_rnn/fw/fw/stack, tower_0/passage_encoding/bidirectional_rnn/fw/fw/Assert/Assert/data_2, tower_0/passage_encoding/bidirectional_rnn/fw/fw/Shape_1)]]

Caused by op 'tower_0/passage_encoding/bidirectional_rnn/fw/fw/Assert/Assert', defined at:
  File ""run.py"", line 408, in <module>
    run()
  File ""run.py"", line 399, in run
    multi_gpu_train(args)
  File ""run.py"", line 249, in multi_gpu_train
    model = RCModel_ngpus(args, data)
  File ""/home/home1/dmyan/codes/tensorflow/rc_model_multi_gpu.py"", line 74, in __init__
    self._build_graph()
  File ""/home/home1/dmyan/codes/tensorflow/rc_model_multi_gpu.py"", line 87, in _build_graph
    self._encode()
  File ""/home/home1/dmyan/codes/tensorflow/rc_model_multi_gpu.py"", line 102, in _encode
    self.sep_p_encodes, _ = rnn('bi-lstm', self.p_emb, self.p_length, self.hidden_size)
  File ""/home/home1/dmyan/codes/tensorflow/layers/basic_rnn.py"", line 51, in rnn
    cell_bw, cell_fw, inputs, sequence_length=length, dtype=tf.float32
  File ""/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py"", line 439, in bidirectional_dynamic_rnn
    time_major=time_major, scope=fw_scope)
  File ""/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py"", line 651, in dynamic_rnn
    [_assert_has_shape(sequence_length, [batch_size])]):
  File ""/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py"", line 646, in _assert_has_shape
    packed_shape, "" but saw shape: "", x_shape])
  File ""/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py"", line 189, in wrapped
    return _add_should_use_warning(fn(*args, **kwargs))
  File ""/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 159, in Assert
    return gen_logging_ops._assert(condition, data, summarize, name=""Assert"")
  File ""/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/ops/gen_logging_ops.py"", line 52, in _assert
    name=name)
  File ""/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3274, in create_op
    op_def=op_def)
  File ""/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1770, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): Cannot assign a device for operation tower_0/passage_encoding/bidirectional_rnn/fw/fw/Assert/Assert: Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
Registered kernels:
  device='CPU'

         [[node tower_0/passage_encoding/bidirectional_rnn/fw/fw/Assert/Assert (defined at /home/home1/dmyan/codes/tensorflow/layers/basic_rnn.py:51)  = Assert[T=[DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=""/device:GPU:0""](tower_0/passage_encoding/bidirectional_rnn/fw/fw/All, tower_0/passage_encoding/bidirectional_rnn/fw/fw/Assert/Assert/data_0, tower_0/passage_encoding/bidirectional_rnn/fw/fw/stack, tower_0/passage_encoding/bidirectional_rnn/fw/fw/Assert/Assert/data_2, tower_0/passage_encoding/bidirectional_rnn/fw/fw/Shape_1)]]

```

"
24529,bijectors can't be used in eager execution mode,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

I can't use tfp.bijectors.Bijector in eager execution mode, because the back propagation is done with tf.GradientTape with tf.keras.Model. Whereas, tfp.bijectors.Bijector is not a derived class of tf.keras.Layer, so a list of trainable variable is hard to get when back propagation.

Without eager execution feature, creating bijectors is frustrating. Several bijectors such as ActNorm and Affinecoupling need initialization with the first batch of training data. This feature is hard to implement with graph barely.

If you make Bijector a derive class of tf.keras.Layer, all trainable variables are readily available with the built tf.keras.Model object. This will no doubt facilitate the building of custom bijectors.

**Will this change the current api? How?**

No change to current api. To add the feature, you need to add derivation relationship and add build member function to add all trainable variables of every bijector to the list as described [in the document](https://tensorflow.google.cn/guide/eager#build_a_model).

**Who will benefit with this feature?**

All developer building flow-based generative model with tensorflow will be benefit. 

**Any Other info.**

Currently, workable Glow model (https://arxiv.org/abs/1807.03039) is only available in pytorch code."
24528,matrix_triangular_solve is much slower on GPUs than on CPUs,"I'm running a fully connected mixture density network. What could be the possible reasons that matrix_triangular_solve routine takes much longer to compute on a GPU than on CPU (27ms vs 0.4ms)? I'm seeing similar behavior across different tensorflow verison (python 2.7), eg.

CPU: Intel Haswell (E5-2695 v3)
CUDA version: V9.0.176
tenserflow version : 1.7

GPU: Tesla P100-PCIE-16GB
CUDA version: V10.0.130
tenserflow version: 1.10

![gpu_timeline](https://user-images.githubusercontent.com/5102663/50380990-de63b880-0648-11e9-82ea-a08051b83a55.png)
![cpu_timeline](https://user-images.githubusercontent.com/5102663/50380991-de63b880-0648-11e9-97f4-6f59249b17e3.png)
"
24527,build failure for tensorflow v1.12.0 ,"Hello.  Having a problem with building tensorflow from source.  The most recent version I've been able to compile was 1.9.0, and that only after multiple attempts.  Here is the result from my recent attempt at building the build_pip_package target.  Any help here would be greatly appreciated.  Just let me know if I left out any valuable information, I'll be more than happy to provide it.  Thanks. 

**System information**

Distributor ID:	Debian
Description:	Debian GNU/Linux 9.6 (stretch)
Release:	9.6
Codename:	stretch

- TensorFlow installed from (source or binary): source
- TensorFlow version: v1.12.0 from git repos.
- Python version: 3.5.3
- Installed using virtualenv? pip? conda?: virtualenv or system
- Bazel version (if compiling from source): v0.21.0
- GCC/Compiler version (if compiling from source): gcc 4.8.4
- CUDA/cuDNN version: none 
- GPU model and memory: none. (cpu only)
**Describe the problem**
Unable to compile pip package

**Provide the exact sequence of commands / steps that you executed before running into the problem**

./configure   (enter default settings)

bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**

INFO: Invocation ID: b0ec9363-a8cc-4c61-9cbe-64b57af62a73
ERROR: /home/david/.cache/bazel/_bazel_david/1f9a24d051ca8bc02465362202ebba4c/external/protobuf_archive/BUILD:591:1: Traceback (most recent call last):
	File ""/home/david/.cache/bazel/_bazel_david/1f9a24d051ca8bc02465362202ebba4c/external/protobuf_archive/BUILD"", line 591
		internal_gen_well_known_protos_java(srcs = WELL_KNOWN_PROTOS)
	File ""/home/david/.cache/bazel/_bazel_david/1f9a24d051ca8bc02465362202ebba4c/external/protobuf_archive/protobuf.bzl"", line 269, in internal_gen_well_known_protos_java
		Label((""%s//protobuf_java"" % REPOSITOR...))
	File ""/home/david/.cache/bazel/_bazel_david/1f9a24d051ca8bc02465362202ebba4c/external/protobuf_archive/protobuf.bzl"", line 269, in Label
		REPOSITORY_NAME
The value 'REPOSITORY_NAME' has been removed in favor of 'repository_name()', please use the latter (https://docs.bazel.build/versions/master/skylark/lib/native.html#repository_name). You can temporarily allow the old name by using --incompatible_package_name_is_a_function=false
ERROR: /home/david/.cache/bazel/_bazel_david/1f9a24d051ca8bc02465362202ebba4c/external/protobuf_archive/BUILD:715:1: Target '@protobuf_archive//:use_fast_cpp_protos' contains an error and its package is in error and referenced by '@protobuf_archive//:protobuf_python'
ERROR: /home/david/tensorflow/tensorflow/python/BUILD:3546:1: Target '@protobuf_archive//:protobuf_python' contains an error and its package is in error and referenced by '//tensorflow/python:util'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed
INFO: Elapsed time: 0.447s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (2 packages loaded, 15 targets configured)
    currently loading: tensorflow/core

"
24525,Duplicated Gradient issue during TF Lite conversion,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Yes
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.0-dev20181222
- Python version: 3.6
- Bazel version (if compiling from source): / 
- GCC/Compiler version (if compiling from source): / 
- CUDA/cuDNN version: 9.1 
- GPU model and memory: 1080 Ti with 11Gib memory

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Code to reproduce the issue**
I was trying to convert the model in https://github.com/MIT-HAN-LAB/ProxylessNAS to TFLite, however, when following the tutorial, I got errors below 

```python
import tensorflow as tf
from proxyless_nas_tensorflow import proxyless_cpu, proxyless_gpu, proxyless_mobile, proxyless_mobile_14

net = proxyless_cpu()
net.is_training = False

converter = tf.lite.TFLiteConverter.from_session(net.sess, [net.images, net.labels], [net.cross_entropy])
tflite_model = converter.convert()
'''
~/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str)
    206       stderr = _try_convert_to_unicode(stderr)
    207       raise ConverterError(
--> 208           ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
    209   finally:
    210     # Must manually cleanup files.

ConverterError: TOCO failed. See console for info.
2018-12-22 13:38:13.626047: F tensorflow/core/framework/function.cc:1626] Check failed: GetOpGradFactory()->insert({op, func}).second Duplicated gradient for ReadVariableOp
Aborted (core dumped)
'''
```
Any advice would be appreciated!"
24524,AttributeError: module 'tensorflow.compat' has no attribute 'v1',"I ran cnn_mnist.py on my machine but got ""AttributeError: module 'tensorflow.compat' has no attribute 'v1'"". can anyone solve this problem?
"
24523,Update bazel to 0.20.0 for tensorflow/tensorflow:custom-op,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): `docker run -i -t --rm tensorflow/tensorflow:custom-op `
- TensorFlow version: `docker run -i -t --rm tensorflow/tensorflow:custom-op `
- Python version: n/a
- Installed using virtualenv? pip? conda?: docker
- Bazel version (if compiling from source): 0.15.0 (`tensorflow/tensorflow:custom-op`)
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version:n/a
- GPU model and memory:n/



**Describe the problem**
While tensorflow has updated bazel to 0.20.0, The `tensorflow/tensorflow:custom-op` docker image still uses 0.15.0. This cause issues when trying to build a custom op that requires higher versions of bazel (e..g, 0.17.1+).


Please upgrade bazel in `tensorflow/tensorflow:custom-op` to 0.20.0.


**Provide the exact sequence of commands / steps that you executed before running into the problem**


```
$ docker run -i -t --rm tensorflow/tensorflow:custom-op 
root@ubuntu:/# bazel version
Extracting Bazel installation...
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
Build label: 0.15.0
Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Tue Jun 26 12:10:19 2018 (1530015019)
Build timestamp: 1530015019
Build timestamp as int: 1530015019
```
**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24521,TF Model Lite Benchmark tool still failed to build on Windows,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 Pro 64bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source
- TensorFlow version:nightly build
- Python version:3.5
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):0.20.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the problem**
Thanks for @jdduke #24475 patch.
Unfortunately TF Model Lite Benchmark tool still failed to build on Windows.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel --output_base $(Build.BinariesDirectory) build -c opt //tensorflow/lite/tools/benchmark:benchmark_model

**Any other info / logs**
```
2018-12-22T17:16:35.9502732Z .\tensorflow/core/util/stats_calculator.h(41): error C2039: 'max': is not a member of 'std'
2018-12-22T17:16:35.9502783Z C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\map(15): note: see declaration of 'std'
2018-12-22T17:16:35.9502890Z .\tensorflow/core/util/stats_calculator.h(35): note: while compiling class template member function 'void tensorflow::Stat<int64_t,double>::UpdateStat(ValueType)'
2018-12-22T17:16:35.9503047Z         with
2018-12-22T17:16:35.9503198Z         [
2018-12-22T17:16:35.9503238Z             ValueType=int64_t
2018-12-22T17:16:35.9503347Z         ]
2018-12-22T17:16:35.9503394Z .\tensorflow/core/util/stats_calculator.h(150): note: see reference to function template instantiation 'void tensorflow::Stat<int64_t,double>::UpdateStat(ValueType)' being compiled
2018-12-22T17:16:35.9503433Z         with
2018-12-22T17:16:35.9503520Z         [
2018-12-22T17:16:35.9503557Z             ValueType=int64_t
2018-12-22T17:16:35.9503592Z         ]
2018-12-22T17:16:35.9503638Z .\tensorflow/core/util/stats_calculator.h(159): note: see reference to class template instantiation 'tensorflow::Stat<int64_t,double>' being compiled
2018-12-22T17:16:35.9503730Z .\tensorflow/core/util/stats_calculator.h(41): error C2660: 'tensorflow::Stat<int64_t,double>::max': function does not take 2 arguments
2018-12-22T17:16:35.9503782Z .\tensorflow/core/util/stats_calculator.h(42): error C2039: 'min': is not a member of 'std'
2018-12-22T17:16:35.9503825Z C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\map(15): note: see declaration of 'std'
2018-12-22T17:16:35.9503927Z .\tensorflow/core/util/stats_calculator.h(42): error C2660: 'tensorflow::Stat<int64_t,double>::min': function does not take 2 arguments
2018-12-22T17:16:35.9777999Z Target //tensorflow/lite/tools/benchmark:benchmark_model failed to build
2018-12-22T17:16:35.9810313Z Use --verbose_failures to see the command lines of failed build steps.
2018-12-22T17:16:35.9833736Z INFO: Elapsed time: 2.003s, Critical Path: 0.59s
```
For further information visit our [CI environment (see build_tflite_win Job)](https://dev.azure.com/mlops/build-tflite/_build/results?buildId=14)
"
24520,"""ValueError: Cannot take the length of Shape with unknown rank"". error when passing tf.data.Dataset tensors to model.fit","
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ""18.04.1 LTS (Bionic Beaver)""
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tf.VERSION = 1.12.0
- Python version: python3.6
- Bazel version (if compiling from source): no
- GCC/Compiler version (if compiling from source): no
- CUDA/cuDNN version: cuda9.0 with cuDNN 7.4.1
- GPU model and memory: GTX 1080 with 8 GB


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I am trying to pass the tfrecords read through tf.data.Dataset api into the model.fit .
Since the images could be of different sizes, I am storing the image shapes into tfrecords itself which are 
later on read and applied to the img data using  `tf.reshape` . But the tensorflow.keras is unable to determine the shape of this image data at this stage and throws the error.

```
def _parse_function(proto):
    keys_to_features = {""im_path"": tf.FixedLenSequenceFeature([], tf.string, allow_missing=True),
                        ""im_shape"": tf.FixedLenSequenceFeature([], tf.int64, allow_missing=True),
                        ""im_arr"": tf.FixedLenSequenceFeature([], tf.string, allow_missing=True),
                        ""label"": tf.FixedLenSequenceFeature([], tf.int64, allow_missing=True),
                        }

    parsed_features = tf.parse_single_example(serialized=proto, features=keys_to_features)
    parsed_features['im_arr'] = parsed_features['im_arr'][0]
    parsed_features['label'] = parsed_features['label'][0]
    parsed_features['im_arr'] = tf.decode_raw(parsed_features['im_arr'], tf.uint8)
    parsed_features['im_arr'] = tf.reshape(parsed_features['im_arr'], parsed_features['im_shape'])

    return parsed_features['im_arr'], parsed_features['label']
```

 The error thrown is as follows : 

	Traceback (most recent call last):
	  File ""issue/IssueScript.py"", line 75, in <module>
	    verbose=1)
	  File ""opt/github/example-classification/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1536, in fit
	    validation_split=validation_split)
	  File ""opt/github/example-classification/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 992, in _standardize_user_data
	    class_weight, batch_size)
	  File ""opt/github/example-classification/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1117, in _standardize_weights
	    exception_prefix='input')
	  File ""opt/github/example-classification/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py"", line 284, in standardize_input_data
	    data = [standardize_single_array(x) for x in data]
	  File ""opt/github/example-classification/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py"", line 284, in <listcomp>
	    data = [standardize_single_array(x) for x in data]
	  File ""opt/github/example-classification/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py"", line 218, in standardize_single_array
	    if x.shape is not None and len(x.shape) == 1:
	  File ""opt/github/example-classification/env/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py"", line 579, in __len__
	    raise ValueError(""Cannot take the length of Shape with unknown rank."")
	ValueError: Cannot take the length of Shape with unknown rank.


So as a debugging step, I removed the length check present in the `standardize_single_array` function by changing the check as (note the `False and` part which bypasses the length check)

```def standardize_single_array(x):
  if x is None:
    return None
  if False and (x.shape is not None and len(x.shape) == 1):
    if tensor_util.is_tensor(x):
      return array_ops.expand_dims(x, axis=1)
    else:
      return np.expand_dims(x, 1)
  return x
```
Then I get the following error 

	Traceback (most recent call last):
	  File ""issue/IssueScript.py"", line 75, in <module>
	    verbose=1)
	  File ""opt/github/example-classification/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1536, in fit
	    validation_split=validation_split)
	  File ""opt/github/example-classification/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 992, in _standardize_user_data
	    class_weight, batch_size)
	  File ""opt/github/example-classification/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1154, in _standardize_weights
	    exception_prefix='target')
	  File ""opt/github/example-classification/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py"", line 323, in standardize_input_data
	    'with shape ' + str(data_shape))
	ValueError: Error when checking target: expected activation_4 to have 2 dimensions, but got array with shape (None,)


I did the same with the above error. I removed the check present at line 323 by commenting out the length check as follows.
```
        """"""
        if len(data_shape) != len(shape):
          raise ValueError('Error when checking ' + exception_prefix +
                           ': expected ' + names[i] + ' to have ' +
                           str(len(shape)) + ' dimensions, but got array '
                           'with shape ' + str(data_shape))
        """"""
```
Now the training proceeds smoothly without error. I believe there is issue with tf.reshape when tensors are supplied as a shape to the function.


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Code : https://github.com/dineshdharme/tensorflow-issue1
Just run : `python3 issue/IssueScript.py`

I have also added a tfrecords generating script `tfrecords_utils.py` which you can call by
To generate tfrecords file using the image data present in the data folder : 
`python3 issue/tfrecords_utils.py`

"
24519,Unexpected behaviour of tf.map_fn(),"- OS Platform and Distribution: Windows 10 
- TensorFlow installed from: source
- TensorFlow version: 1.10.0
- Python version: 3.6.6

I am using K.map_fn() in my custom loss function where I am passing both y_true and y_pred of shape (None, None) as elems argument of this function. But when the function specified in map_fn is called, elements obtained in that function are of different shapes. And that's the problem.

Here's the example:

My custom loss function:
```
def negative_avg_log_error(y_true, y_pred):

    def sum_of_log_probabilities(true_and_pred):
        y_true, y_pred = true_and_pred
        print(K.int_shape(y_true))
        print(K.int_shape(y_pred))
        start_index = int(y_true[0])
        end_index = int(y_true[1])
        start_probability = y_pred[start_index]
        end_probability = y_pred[end_index]
        return K.log(start_probability) + K.log(end_probability)

    print(K.int_shape(y_true))
    print(K.int_shape(y_pred))
    batch_probability_sum = K.map_fn(lambda x: sum_of_log_probabilities(x), elems=[y_true, y_pred], dtype='float32')
    return -K.mean(batch_probability_sum, axis=1)
```

Code from which this loss function is called:

`model.compile(loss=negative_avg_log_error, optimizer='adadelta', metrics='loss')`


Error log:

```
(None, None)
(None, None)
(None,)
(None, 1)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-15-807b6d06a435> in <module>()
----> 1 model.compile(loss=negative_avg_log_error, optimizer='adadelta', metrics='loss')

C:\Python36\lib\site-packages\keras\engine\training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)
    331                 with K.name_scope(self.output_names[i] + '_loss'):
    332                     output_loss = weighted_loss(y_true, y_pred,
--> 333                                                 sample_weight, mask)
    334                 if len(self.outputs) > 1:
    335                     self.metrics_tensors.append(output_loss)

C:\Python36\lib\site-packages\keras\engine\training_utils.py in weighted(y_true, y_pred, weights, mask)
    401         """"""
    402         # score_array has ndim >= 2
--> 403         score_array = fn(y_true, y_pred)
    404         if mask is not None:
    405             # Cast the mask to floatX to avoid float64 upcasting in Theano

E:\Deep Learning Material\IMP for Project\model\scripts\loss_function.py in negative_avg_log_error(y_true, y_pred)
     16     print(K.int_shape(y_true))
     17     print(K.int_shape(y_pred))
---> 18     batch_probability_sum = K.map_fn(lambda x: sum_of_log_probabilities(x), elems=[y_true, y_pred], dtype='float32')
     19     return -K.mean(batch_probability_sum, axis=1)

C:\Python36\lib\site-packages\keras\backend\tensorflow_backend.py in map_fn(fn, elems, name, dtype)
   4229         Tensor with dtype `dtype`.
   4230     """"""
-> 4231     return tf.map_fn(fn, elems, name=name, dtype=dtype)
   4232 
   4233 

C:\Python36\lib\site-packages\tensorflow\python\ops\functional_ops.py in map_fn(fn, elems, dtype, parallel_iterations, back_prop, swap_memory, infer_shape, name)
    457         back_prop=back_prop,
    458         swap_memory=swap_memory,
--> 459         maximum_iterations=n)
    460     results_flat = [r.stack() for r in r_a]
    461 

C:\Python36\lib\site-packages\tensorflow\python\ops\control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)
   3230       ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, loop_context)
   3231     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants,
-> 3232                                     return_same_structure)
   3233     if maximum_iterations is not None:
   3234       return result[1]

C:\Python36\lib\site-packages\tensorflow\python\ops\control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants, return_same_structure)
   2950       with ops.get_default_graph()._mutation_lock():  # pylint: disable=protected-access
   2951         original_body_result, exit_vars = self._BuildLoop(
-> 2952             pred, body, original_loop_vars, loop_vars, shape_invariants)
   2953     finally:
   2954       self.Exit()

C:\Python36\lib\site-packages\tensorflow\python\ops\control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)
   2885         flat_sequence=vars_for_body_with_tensor_arrays)
   2886     pre_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access
-> 2887     body_result = body(*packed_vars_for_body)
   2888     post_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access
   2889     if not nest.is_sequence(body_result):

C:\Python36\lib\site-packages\tensorflow\python\ops\control_flow_ops.py in <lambda>(i, lv)
   3199         cond = lambda i, lv: (  # pylint: disable=g-long-lambda
   3200             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))
-> 3201         body = lambda i, lv: (i + 1, orig_body(*lv))
   3202 
   3203     if context.executing_eagerly():

C:\Python36\lib\site-packages\tensorflow\python\ops\functional_ops.py in compute(i, tas)
    446       """"""
    447       packed_values = input_pack([elem_ta.read(i) for elem_ta in elems_ta])
--> 448       packed_fn_values = fn(packed_values)
    449       nest.assert_same_structure(dtype or elems, packed_fn_values)
    450       flat_fn_values = output_flatten(packed_fn_values)

E:\Deep Learning Material\IMP for Project\model\scripts\loss_function.py in <lambda>(x)
     16     print(K.int_shape(y_true))
     17     print(K.int_shape(y_pred))
---> 18     batch_probability_sum = K.map_fn(lambda x: sum_of_log_probabilities(x), elems=[y_true, y_pred], dtype='float32')
     19     return -K.mean(batch_probability_sum, axis=1)

E:\Deep Learning Material\IMP for Project\model\scripts\loss_function.py in sum_of_log_probabilities(true_and_pred)
      8         print(K.int_shape(y_true))
      9         print(K.int_shape(y_pred))
---> 10         start_index = int(y_true[0])
     11         end_index = int(y_true[1])
     12         start_probability = y_pred[start_index]

TypeError: int() argument must be a string, a bytes-like object or a number, not 'Tensor'
```


I am currently focusing on:
In negative_avg_log_error() => K.int_shape(y_true)=(None, None) and K.int_shape(y_pred)=(None, None)
In sum_of_log_probabilities() => K.int_shape(y_true)=(None,) and K.int_shape(y_pred)=(None, 1)

But I think that the shape of both these tensors in sum_of_log_probabilities() should be (None, ).

Need help as understanding this will help me solve this error.
"
24518,Keras crossentropy with logits as loss function,"**System information**
- TensorFlow version (you are using): 1.13rc0 (Windows GPU build)
- Are you willing to contribute it (Yes/No): **Yes**

**Current behavior/state.**
In the keras api for tensorflow there does not seem to be a function for crossentropy with logits, (without sigmoid or softmax activations). This is quite confusing for those coming from the estimators (like me). I see that a `from_logits` argument is being added in the current release, but it needs to be used with a proxy function or a lambda.

```
...
y_pred = keras.layers.Dense(10)(x)

model = keras.Model(inputs=x, outputs=y_pred)
model.compile(""adam"", lambda x, y: keras.losses.sparse_categorical_crossentropy(x, y, from_logits=True))
```

**Will this change the current api? How?**
Instead of introducing argument, I think it's better to have a new loss function like 
```
model.compile(""adam"", keras.losses.sparse_categorical_crossentropy_with_logits)
```
**or**
loss name as string:
```
model.compile(""adam"", ""sparse_categorical_crossentropy_with_logits"")
```

**Who will benefit with this feature?**
- Using logits to calculate loss is actually efficient than activation then loss, so tf should encourage using crossentropy with logits, as it did in estimator API.
- Those who switch from estimators to Keras API won't have any confusion.
"
24517,BUG: code throws exceptions when using COND_V2,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):archlinux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):v1.12.0-rc2-0-g748435b8ef 1.12.0-rc2
- Python version:3.6
- Bazel version (if compiling from source):n/a
- GCC/Compiler version (if compiling from source):n/a
- CUDA/cuDNN version:n/a
- GPU model and memory:n/a

**Code to reproduce the issue**
```python
import os
os.environ['TF_ENABLE_COND_V2'] = '1'
import tensorflow as tf
a = tf.placeholder(tf.float32, shape=[3])
u = tf.distributions.Normal(a, scale=1., validate_args=True)
```

**Other info / logs**
It fails with the following message. However the code works well without cond_v2.
```
Traceback (most recent call last):                                                                                                                                                                 
  File ""/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 933, in convert                                                                                    
    x = ops.convert_to_tensor_or_indexed_slices(x)                                                                                                                                                 
  File ""/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1269, in convert_to_tensor_or_indexed_slices                                                        
    value=value, dtype=dtype, name=name, as_ref=False)                                                                                                                                             
  File ""/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1307, in internal_convert_to_tensor_or_indexed_slices                                               
    value, dtype=dtype, name=name, as_ref=as_ref)                                                                                                                                                  
  File ""/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1146, in internal_convert_to_tensor                                                                 
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)                                                                                                                            
  File ""/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 6168, in _operation_conversion_error                                                                
    name, as_ref))                                                                                                                                                                                 
TypeError: Can't convert Operation 'NoOp' to Tensor (target dtype=None, name=None, as_ref=False)
                                                                                                                                                                                                   
During handling of the above exception, another exception occurred:
                                                                                                                                                                                                   
Traceback (most recent call last):
  File ""a.py"", line 8, in <module>                                                                                                                                                                 
    u = tf.distributions.Normal(a, scale=1., validate_args=True)
  File ""/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/ops/distributions/normal.py"", line 140, in __init__                                                                         
    validate_args else []):
  File ""/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py"", line 204, in assert_positive                                                                             
    return assert_less(zero, x, data=data, summarize=summarize)
  File ""/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py"", line 569, in assert_less                                                                                 
    return control_flow_ops.Assert(condition, data, summarize=summarize)
  File ""/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py"", line 189, in wrapped                                                                                
    return _add_should_use_warning(fn(*args, **kwargs))
  File ""/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 167, in Assert                                                                               
    guarded_assert = cond(condition, no_op, true_assert, name=""AssertGuard"")
  File ""/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func                                                                                 
    return func(*args, **kwargs)
  File ""/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2040, in cond                                                                                
    return cond_v2_impl.cond_v2(pred, true_fn, false_fn, name)
  File ""/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/ops/cond_v2_impl.py"", line 70, in cond_v2                                                                                   
    true_name, true_fn, [], {})
  File ""/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 947, in func_graph_from_py_func                                                                    
    func_outputs = nest.map_structure(convert, func_outputs)
  File ""/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/util/nest.py"", line 381, in map_structure                                                                                   
    structure[0], [func(*x) for x in entries])
  File ""/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/util/nest.py"", line 381, in <listcomp>                                                                                      
    structure[0], [func(*x) for x in entries])
  File ""/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 939, in convert
    (str(python_func), type(x)))
TypeError: To be compatible with tf.contrib.eager.defun, Python functions must return zero or more Tensors; in compilation of <function no_op at 0x7f64b06a7ea0>, found return value of type <class
'tensorflow.python.framework.ops.Operation'>, which is not a Tensor.
```

cc @skye, who seems to work on cond_v2 in https://github.com/tensorflow/tensorflow/issues/15874#issuecomment-436833266."
24516,Tensorflow freezes on first Epoch then exits without exception,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: Windows 10 x64
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.12.0
- Python version: 3.6.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 9.0/7.2.0
- GPU model and memory: 980M 4GB

**I am trying to train a Siamese network.  When I get to the first epoch the program simply freezes for several minutes then exits with no diagnostic output. I'd built too large of a model before so I know it is allocating memory with CUDA. My assumption is this is an installation issue as it seems to be some sort of critical failure, but without additional diagnostic output i am unsure what to look into. **

** Steps **
1. Build Model
2. Build Generator for fit_generator
3. Execute Fit Generator for training

**Any other info / logs**
```
import os
import cv2
import numpy as np
import pandas as pd
from tqdm import tqdm
import numpy.random as rng
from whalegenerator import WhaleGenerator

import tensorflow as tf
from keras import backend as K
from keras.regularizers import l2
from keras.optimizers import SGD,Adam
from keras.losses import binary_crossentropy
from keras.models import Model, Sequential, save_model, load_model
from keras.callbacks import ModelCheckpoint, EarlyStopping
from keras.layers import Input, Conv2D, Lambda, Subtract, Dense, Flatten,MaxPooling2D

ProcessImages = True
TrainPath = '.\\processed\\train'
ProcessedPath = '.\\processed'
TrainTruthPath = '.\\train.csv'

def buildModel():
    #We are building a saimese network, whaling problem should use comparison
    input_shape = (256,256,1)
    left = Input(input_shape)
    right = Input(input_shape)

    convnet = Sequential()
    convnet.add(Conv2D(64,(9,9),activation='relu',input_shape=input_shape,kernel_initializer='random_normal',kernel_regularizer=l2(2E-4)))
    convnet.add(MaxPooling2D())
    convnet.add(Conv2D(128,(7,7),activation='relu',kernel_regularizer=l2(2E-4),kernel_initializer='random_normal',bias_initializer='random_normal'))
    convnet.add(MaxPooling2D())
    convnet.add(Conv2D(256,(5,5),activation='relu',kernel_initializer='random_normal',kernel_regularizer=l2(2e-4),bias_initializer='random_normal'))
    convnet.add(MaxPooling2D())
    convnet.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='random_normal',kernel_regularizer=l2(2e-4),bias_initializer='random_normal'))
    convnet.add(Flatten())
    convnet.add(Dense(2048,activation=""sigmoid"",kernel_regularizer=l2(1e-3),kernel_initializer='random_normal',bias_initializer='random_normal'))

    encodedL = convnet(left)
    encodedR = convnet(right)

    subtract = Subtract()([encodedL,encodedR])
    diff = Lambda(lambda x: K.abs(x))(subtract)
    prediction = Dense(1,activation='sigmoid',bias_initializer='random_normal')(diff)
    siamese_net = Model(inputs=[left,right],outputs=prediction)
    run_opts = tf.RunOptions(report_tensor_allocations_upon_oom = True)
    siamese_net.compile(loss=""binary_crossentropy"",optimizer=Adam(6E-5),options=run_opts,metrics=['accuracy'])
    return siamese_net 

model = buildModel()
print(model.count_params())
batch_size = 32
generator = WhaleGenerator(TrainPath,TrainTruthPath,batch_size,(256,256))

try:
        result = model.fit_generator(generator=generator,epochs=100,steps_per_epoch=int(np.floor(len(generator)/batch_size)),max_queue_size=50,verbose=2,callbacks=[
                ModelCheckpoint('.\\models\\whale_256.h5',save_best_only=True,monitor='accuracy')
                ])
except ValueError as e:
        print(e)
except Exception as e:
        print(e)
else:
        print('unkown error')

print('finished!')



```

```
import os
import cv2
import random
import numpy as np
import pandas as pd
from keras.utils import Sequence

class WhaleGenerator(Sequence):

    #Number of items in the group
    def __len__(self):
        return int(np.floor(len(self.imageList)/2))

    def __getitem__(self,index):
        X = [np.empty((self.batch_size, *self.dim, 1)) for i in range(2)]
        Y = np.empty((self.batch_size), dtype=float)

        # Generate data
        matches = 0
        matchKeys = list(self.hasMatches.items())
        for i in range(self.batch_size):
            if self.batch_size - i < self.min_match and matches < self.min_match:
                id1 = random.choice(matchKeys)[0]
                images1 = self.imgGroups[id1]
                X[0][i,] = images1[random.randint(0,len(images1)-1)]
                X[1][i,] = images1[random.randint(0,len(images1)-1)]
                Y[i] = 1.0
                matches += 1
            else:
                item1 = self.imageList[random.randint(0,len(self.imageList)-1)]
                id1 = item1[""Id""]
                X[0][i,] = item1[""Image""]
                images1 = self.imgGroups[id1]
                if id1 != ""new_whale"" and len(images1) > 1 and random.random() <= 0.5: 
                    X[1][i,] = images1[random.randint(0,len(images1)-1)]
                    Y[i] = 1.0
                    matches += 1.0
                else:
                    item2 = self.imageList[random.randint(0,len(self.imageList)-1)]
                    id2 = item2[""Id""]
                    X[1][i,] = item2[""Image""]
                    if id1 != ""new_whale"" and id1 == id2:
                        Y[i] = 1.0
                        matches += 1.0
                    else:
                        Y[i] = 0.0

        return X, Y

    def __init__(self, image_path, csv, batch_size, dim):

        self.min_match = 3
        self.imgGroups = { }
        self.imageList = [ ]
        self.hasMatches = { }
        self.dim = dim
        self.df = pd.read_csv(csv)
        self.batch_size = batch_size

        for i, row in self.df.iterrows():

            _id = row[""Id""]
            img = cv2.imread(os.path.join(image_path,row[""Image""]))

            if img is None:
                continue

            img = img[:,:,0]
            img = img.reshape((*self.dim,1))
            self.imageList.append({ ""Id"": _id, ""Image"": img })
            if not _id in self.imgGroups:
                self.imgGroups[_id] = [img]
            else:
                if _id != ""new_whale"":
                    self.hasMatches[_id] = True

                self.imgGroups[_id].append(img)
```

What console output i do get
```
Using TensorFlow backend.
83297857
Epoch 1/100
2018-12-21 22:05:42.560621: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2018-12-21 22:05:43.369992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:
name: GeForce GTX 980M major: 5 minor: 2 memoryClockRate(GHz): 1.1265
pciBusID: 0000:01:00.0
totalMemory: 4.00GiB freeMemory: 3.32GiB
2018-12-21 22:05:43.381093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
```
"
24515,tf.count_nonzero not working on TPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not Tested
- TensorFlow installed from (source or binary): Not sure (google colab)
- TensorFlow version (use command below): 1.12.0
- Python version: 2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.2
- GPU model and memory: None (TPU)

**Describe the current behavior**
according to the documentation [here](https://cloud.google.com/tpu/docs/tensorflow-ops), tf.count_nonzero is available in TPU, but it returned compilation error when used in TPU.

```
Compilation failure: While rewriting computation to not contain X64 element types, XLA encountered an HLO for which this rewriting is not implemented: %reduce.124.120 = s64[] reduce(s64[1,4]{1,0} %convert.124.113, s64[] %convert.124.115), dimensions={0,1}, to_apply=%total_example_Sum-reduction119, sharding={maximal device=0}, metadata={op_type=""Sum"" op_name=""total_example/Sum""}
	TPU compilation failed
```

**Describe the expected behavior**
tf.count_nonzero is working for TPU

**Code to reproduce the issue**
[Codes](https://colab.research.google.com/drive/14sja1ttnwgxUlIXqCxuAhSh0L9uSa1te) are available in google colab
```
tf.reset_default_graph()

a = np.random.normal(0,10, [8,4]).astype(np.float32)
b = np.random.normal(0,10, [8,4]).astype(np.float32)

x = tf.convert_to_tensor(a)
y = tf.convert_to_tensor(b)


def ops(x,y):
    mask =tf.greater(x,y)
    return tf.count_nonzero(mask, name='total_example')
                     
tpu_ops = tf.contrib.tpu.batch_parallel(ops,inputs=[x,y],num_shards=8)
                     
with tf.Session(tpu_address) as sess:
    sess.run(tf.contrib.tpu.initialize_system())
    out = sess.run(tpu_ops)
    sess.run(tf.contrib.tpu.shutdown_system())
    
print(np.array(a>b).astype(np.uint8))
print(out)
```

**Other info / logs**
The problem seems to be the XLA compilation, casting to tf.int64 is not working in the current tf.count_nonzero chain.
I made a code that similar but convert to float32 instead of int64, and it's working.
```
def count_nonzero(x, axis=None, name='nonzero'):

    count = tf.reduce_sum(
        tf.cast(
            tf.not_equal(x,tf.zeros([],dtype=x.dtype)), 
            dtype=tf.float32, # <-- the original tf.count_nonzero function cast to tf.int64
            name='casting'
        ), 
        axis=axis,
        name=name
    )
    return count
    #return tf.cast(count, tf.int64)

def ops(x,y):
    mask =tf.greater(x,y)
    return count_nonzero(mask, name='total_example')

tpu_ops = tf.contrib.tpu.batch_parallel(ops,inputs=[x,y],num_shards=8)
```

Furthermore, simple casting from bool to int64 is working, so i guess this is related to the XLA optimization

```
def ops(x,y):
    mask =tf.greater(x,y)
    return tf.cast(mask, tf.int64)

tpu_ops = tf.contrib.tpu.batch_parallel(ops,inputs=[x,y],num_shards=8)
```

All [codes](https://colab.research.google.com/drive/14sja1ttnwgxUlIXqCxuAhSh0L9uSa1te) are available on google colab."
24513,tfdbg memory leak,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

System information

Windows 7, 64
TensorFlow installed from source
TensorFlow version:1.8
Python version:3.6
Installed using pip
Describe the problem

        def get_tfdbg_watch_fn(self):
        assert(len(self.reg_list)!=0),""reg_list can not be the empty""
        regex = ""|""
        regex = regex.join(self.reg_list)
        raw_regex = ""%r"" % regex
        def watch_fn(fetches, feeds):
            del fetches, feeds
            return tfdbg.WatchOptions(
                debug_ops=[""DebugIdentity""],
                node_name_regex_whitelist=regex,
                tolerate_debug_op_creation_failures=False)
        return  watch_fn

    def run(self,output,feed_fit):
        if self.debug_session is None and self.dump_dirs_count()==0:
            watch_fn = self.get_tfdbg_watch_fn()
            self.debug_session = tfdbg.DumpingDebugWrapperSession(sess=self.sess, session_root=self.session_root, watch_fn=watch_fn,
                                                    log_usage=False)
        if self.debug_session is not None and self.debug_session.run_call_count>=self.dump_dirs_count() :
            self.debug_session.run(output,feed_fit)
            #self.sess.run(output,feed_fit)
            del output
            del feed_fit
        return
hi
i run the tfdbg to get the intermediate tensor of the graph,the tfdbg  works,but when i run the self.debug_session.run more times i find the memory usage is grwoing biger and biger and it finally cause the programe aborted. i use the objgraph to check the python obj used,It dosen't grow after the second run,So i guess maybe there are some c obj leak in tfdbg.
thks"
24510,pip install tensorflow - as described on tensorflow.org/install failed,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Windows 10 x64
- Python version: 3.7.1 (via Chocolatey)


**Describe the problem**

Trying to install tensorflow fails when following the instuctions on tensorflow.org/install

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. Starting with a clean Windows 10 x64 machine
2. choco install python   (in Powershell, which installed 3.7.1)
3. python -m pip install --upgrade pip
4. pip install tensorflow    (as described at tensorflow.org/install)
5. failed with:
  Could not find a version that satisfies the requirement tensorflow (from versions: )
No matching distribution found for tensorflow

**Any other info**

Google returned this stack overflow with lots of accepts:
https://stackoverflow.com/questions/38896424/tensorflow-not-found-using-pip

It suggested the following, which worked:
python -m pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.0-py3-none-any.whl

**PowerShell output**

PS C:\WINDOWS\system32> pip install tensorflow
Collecting tensorflow
  Could not find a version that satisfies the requirement tensorflow (from versions: )
No matching distribution found for tensorflow
You are using pip version 10.0.1, however version 18.1 is available.
You should consider upgrading via the 'python -m pip install --upgrade pip' command.
PS C:\WINDOWS\system32> python -m pip install --upgrade pip
Collecting pip
  Downloading https://files.pythonhosted.org/packages/c2/d7/90f34cb0d83a6c5631cf71dfe64cc1054598c843a92b400e55675cc2ac37/pip-18.1-py2.py3-none-any.whl (1.3MB)
    100% || 1.3MB ...
Installing collected packages: pip
  Found existing installation: pip 10.0.1
    Uninstalling pip-10.0.1:
      Successfully uninstalled pip-10.0.1
Successfully installed pip-18.1
PS C:\WINDOWS\system32> pip install tensorflow
Collecting tensorflow
  Could not find a version that satisfies the requirement tensorflow (from versions: )
No matching distribution found for tensorflow"
24509,Error message after importing Tensorflow,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Windows 7, 64
- 
- TensorFlow installed from source
- TensorFlow version:1.12
- Python version:3.6
- Installed using pip
- 

**Describe the problem**

Ran pip install tensorflow on Anaconda prompt, then wrote the following code on Jupiter notebook:

import tensorflow as tf

a = tf.Variable(1, name=""a"")
b = tf.Variable(2, name=""b"")
f = a + b

init = tf.global_variables_initializer()
with tf.Session() as s:
    init.run()
    print( f.eval() )

 got the following error:

ImportError: DLL load failed with error code -1073741795


Please help!
"
24507,Estimator inputs are not quantized using create_training_graph/create_eval_graph ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04 Ubuntu
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.12
- Python version: 3.6
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): No 
- CUDA/cuDNN version: No 
- GPU model and memory: No


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

Estimator inputs are not quantized using create_training_graph/create_eval_graph as this layer has no activation.

From https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/quantize/python/quantize.py

Though
```
# Activations that are supported by the quantization rewrite.
_ACTIVATION_TYPES = {'Relu', 'Relu6', 'Identity'}
```
The valid activations are only Relu*
```
_RELU_TYPES = {'Relu', 'Relu6'}
...
_PASS_THROUGH_OP = {'Reshape', 'Identity', 'BatchToSpaceND', 'SpaceToBatchND'}
_VALID_ACTIVATION_OP = {'Relu', 'Relu6'}
```
Thus, having something as
```
input = tf.identity(input, name='q_input')
...
does not produce the expected result.

The alternative is using a manual layer as FakeQuantOp, but in this case max/min should be provided.
It would be required that create_training_graph/create_eval_graph automatically quantize the input as any other activation.

Thanks
"
24506,failed to use toco to convert quantized pb file to tflite file,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  NO
- TensorFlow installed from (source or binary):  pip install tensorflow-gpu==1.9
- TensorFlow version (use command below):  1.9 gpu
- Python version:
- Bazel version (if compiling from source):   NO
- GCC/Compiler version (if compiling from source):  gcc  5.4.0 
- CUDA/cuDNN version: cuda 9.0 cudnn 7.1
- GPU model and memory: gtx 1070



**Describe the current behavior**
       I used tf.contrib.quantize.create_eval_graph() to quantized trainning a cnn network,  , and used 
freeze_graph.py to generate frozen pb file  successfully
      but when I used toco to convert the pb to tflite  ,,error occured    shown below 

      Array Slice, which is an input to the Conv operator producing the output array conv1/Relu, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results

It seems that the toco can not support tf.slice op ???

**Code to reproduce the issue**

my network is  shown below  

def model(x, y, is_training, is_quantize):
    with slim.arg_scope([slim.conv2d, slim.fully_connected],
                        activation_fn=tf.nn.relu,
                        biases_initializer = tf.constant_initializer(0.0),						
                        normalizer_fn=slim.batch_norm,
                        normalizer_params={'is_training': is_training, 'epsilon':1e-5, 'scale': True, 'updates_collections': tf.GraphKeys.UPDATE_OPS}):
        print('x',x.get_shape())		
        data_slice  = tf.slice(x, [ 0, 0, 0, 0], [ train_batch, 64, 60, 3])
        conv1 = slim.conv2d(data_slice,  16,    [3, 3], scope='conv1')

**Other info / logs**
$ toco  --output_file=quant.tflite --graph_def_file=quant.pb --input_arrays=input  --output_arrays=fc2/act_quant/FakeQuantWithMinMaxVars  --output_format=TFLITE --inference_type=QUANTIZED_UINT8 --mean_values=0 --std_dev_values=1
2018-12-21 20:50:10.927248: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-12-21 20:50:11.011200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-12-21 20:50:11.011620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.8225
pciBusID: 0000:01:00.0
totalMemory: 7.92GiB freeMemory: 7.77GiB
2018-12-21 20:50:11.011633: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2018-12-21 20:50:11.194928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-12-21 20:50:11.194963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2018-12-21 20:50:11.194970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2018-12-21 20:50:11.195190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7498 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
Traceback (most recent call last):
  File ""/home/icare/.local/bin/toco"", line 11, in <module>
    sys.exit(main())
  File ""/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 320, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/home/icare/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 316, in run_main
    _convert_model(tflite_flags)
  File ""/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 121, in _convert_model
    output_data = converter.convert()
  File ""/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/lite.py"", line 309, in convert
    allow_custom_ops=self.allow_custom_ops)
  File ""/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py"", line 225, in toco_convert
    input_data.SerializeToString())
  File ""/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py"", line 107, in toco_convert_protos
    (stdout, stderr))
RuntimeError: TOCO failed see console for info.
2018-12-21 20:50:12.108082: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 113 operators, 173 arrays (0 quantized)
2018-12-21 20:50:12.108759: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 113 operators, 173 arrays (0 quantized)
2018-12-21 20:50:12.112645: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 20 operators, 37 arrays (1 quantized)
2018-12-21 20:50:12.112836: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 19 operators, 35 arrays (1 quantized)
2018-12-21 20:50:12.112978: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 18 operators, 33 arrays (1 quantized)
2018-12-21 20:50:12.113106: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 18 operators, 33 arrays (1 quantized)
2018-12-21 20:50:12.113179: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 12 operators, 27 arrays (1 quantized)
2018-12-21 20:50:12.113252: F tensorflow/contrib/lite/toco/tooling_util.cc:1589] Array Slice, which is an input to the Conv operator producing the output array conv1/Relu, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.
Aborted (core dumped)

None




what should I do to solve the problem ???

"
24505,Is there any plan to integrate NCCL 2.0 or horovod into CollectiveAllReduceStrategy?,"**System information**
- TensorFlow version (you are using): master
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
`Horovod` brings significant performance to distribution execution especially when using `NCCL` to do `all_reduce`. Is there any plan to do integration  or simply support NCCL 2.0 in `CollectiveAllReduceStrategy`? I am willing to contribute for it!

**Will this change the current api? How?**
Noit will be another `cross_device_ops` for candidate.

**Who will benefit with this feature?**
Anyone who wants to use `CollectiveAllReduceStrategy` to improve performance.


**Any Other info.**
Currently I am working for distribution optimization and I hope to have a discussion. Thanks.

@yuefengz 
"
24504,Add parameter for exporting TensorFlow models to override existed files,"
**System information**
- TensorFlow version (you are using): TensorFlow 1.8.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**

Now we have `saved_model.simple_save()` and `saved_model.builder` APIs to export the TensorFlow models. However, there is no parameter to override the model files if we want to do that. Using extra HDFS APIs to delete remote files requires more dependencies for user's Python scripts. And we can do that with the TensorFlow FileSystem APIs.

**Will this change the current api? How?**

Yes. Add the new parameter for `saved_model.simple_save()` function. It could not override the files by default so it may be compatible with the older versions.

**Who will benefit with this feature?**

The TensorFlow end users.

**Any Other info.**

No."
24503,Support overriding model files when exporting model with extra parameter ,"
**System information**
- TensorFlow version (you are using): TensorFlow 1.8.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**

Now we have `saved_model.simple_save()` and `saved_model.builder` APIs to export the TensorFlow models. However, there is no parameter to override the model files if we want to do that. Using extra HDFS APIs to delete remote files requires more dependencies for user's Python scripts. And we can do that with the TensorFlow FileSystem APIs.

**Will this change the current api? How?**

Yes. Add the new parameter for `saved_model.simple_save()` function. It could not override the files by default so it may be compatible with the older versions.

**Who will benefit with this feature?**

The TensorFlow end users.

**Any Other info.**

No."
24500,while install tensorflow 1.12 with cudnn 9  GETTING ERROR AS DLL:SPECIFIED PROCEDURE COUDN'T FIND,"TENSORFLOW ISSUE GETTNG SPECIFIED PROCEDURE COUDN'T FIND 
WHIL COMUTER IS HAVING
 WINDOWS 7 OS , 
GRAPHIC IS NVIDIA GEFORCE 7025 
INSTALLED TENSORFLOW 1.12 WITH CUDNN 9
IN ANACONDA 1.9.6 LATEST 2018 NOV VERSION

GET ME HELP ..... 
(SRI) C:\Users\SRIKANTH>python -c ""import tensorflow""
Traceback (most recent call last):
  File ""C:\Users\SRIKANTH\Anaconda3\envs\SRI\lib\site-packages\tensorflow\python
\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\SRIKANTH\Anaconda3\envs\SRI\lib\site-packages\tensorflow\python
\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\SRIKANTH\Anaconda3\envs\SRI\lib\site-packages\tensorflow\python
\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript
ion)
  File ""C:\Users\SRIKANTH\Anaconda3\envs\SRI\lib\imp.py"", line 243, in load_modu
le
    return load_dynamic(name, filename, file)
  File ""C:\Users\SRIKANTH\Anaconda3\envs\SRI\lib\imp.py"", line 343, in load_dyna
mic
    return _load(spec)
ImportError: DLL load failed: The specified procedure could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\SRIKANTH\Anaconda3\envs\SRI\lib\site-packages\tensorflow\__init
__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-im
port
  File ""C:\Users\SRIKANTH\Anaconda3\envs\SRI\lib\site-packages\tensorflow\python
\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\SRIKANTH\Anaconda3\envs\SRI\lib\site-packages\tensorflow\python
\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\SRIKANTH\Anaconda3\envs\SRI\lib\site-packages\tensorflow\python
\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\SRIKANTH\Anaconda3\envs\SRI\lib\site-packages\tensorflow\python
\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\SRIKANTH\Anaconda3\envs\SRI\lib\site-packages\tensorflow\python
\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript
ion)
  File ""C:\Users\SRIKANTH\Anaconda3\envs\SRI\lib\imp.py"", line 243, in load_modu
le
    return load_dynamic(name, filename, file)
  File ""C:\Users\SRIKANTH\Anaconda3\envs\SRI\lib\imp.py"", line 343, in load_dyna
mic
    return _load(spec)
ImportError: DLL load failed: The specified procedure could not be found.


Failed to load the native TensorFlow runtime.
"
24499,Bazel C++ example link error on Windows 10 with Tensorflow 1.12 CPU,"<em>C++ example build error on Windows 10 with Tensorflow 1.12</em>

**System information**
- OS Platform and Distribution Windows 10:
- TensorFlow installed from source:
- TensorFlow version: 1.12
- Python version: 3.6
- Installed using conda:
- Bazel version (if compiling from source): 0.20.0
- GCC/Compiler version (if compiling from source): Visual Studio 2015 tools
- CUDA/cuDNN version: without GPU
- GPU model and memory: without GPU

**I cloned the Tensorflow Git repository and got the latest version from the master branch (commit  [fe84b75] from  Thursday, December 20, 2018 7:37:28 AM. I installed Bazel and I tried to compile the C++ example provided on the web site https://www.tensorflow.org/guide/extend/cc following the instructions. The required libraries from Tensorflow were compiled successfully but the example.cpp has a link error:**

**C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/link.exe /nologo /OUT:bazel-out/x64_windows-opt/bin/tensorflow/cc/example/example.exe /SUBSYSTEM:CONSOLE -DEFAULTLIB:advapi32.lib /MACHINE:X64 @bazel-out/x64_windows-opt/bin/tensorflow/cc/example/example.exe-2.params /OPT:ICF /OPT:REF
Execution platform: @bazel_tools//platforms:host_platform
   Creating library bazel-out/x64_windows-opt/bin/tensorflow/cc/example/example.lib and object bazel-out/x64_windows-opt/bin/tensorflow/cc/example/example.exp
pin_to_host_optimizer.lib(pin_to_host_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported
utils.lib(utils.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported
batch_kernels.lo.lib(batch_kernels.obj) : warning LNK4217: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported in function ""void __cdecl tensorflow::`dynamic initializer for 'registrar__body__0__object''(void)"" (??__Eregistrar__body__0__object@tensorflow@@YAXXZ)
captured_function.lib(captured_function.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported
arithmetic_optimizer.lib(arithmetic_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported
memory_optimizer.lib(memory_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported
unicode_ops.lo.lib(unicode_ops.obj) : warning LNK4217: locally defined symbol ??3UMemory@icu_62@@SAXPEAX@Z (public: static void __cdecl icu_62::UMemory::operator delete(void *)) imported in function ""public: virtual void * __cdecl icu_62::StringByteSink<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::`scalar deleting destructor'(unsigned int)"" (??_G?$StringByteSink@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@icu_62@@UEAAPEAXI@Z)
unicode_ops.lo.lib(unicode_ops.obj) : warning LNK4217: locally defined symbol ??0UnicodeStringAppendable@icu_62@@QEAA@AEAVUnicodeString@1@@Z (public: __cdecl icu_62::UnicodeStringAppendable::UnicodeStringAppendable(class icu_62::UnicodeString &)) imported in function ""public: virtual void __cdecl tensorflow::UnicodeEncodeOp::Compute(class tensorflow::OpKernelContext *)"" (?Compute@UnicodeEncodeOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
unicode_ops.lo.lib(unicode_ops.obj) : warning LNK4217: locally defined symbol ??1UnicodeStringAppendable@icu_62@@UEAA@XZ (public: virtual __cdecl icu_62::UnicodeStringAppendable::~UnicodeStringAppendable(void)) imported in function ""public: virtual void __cdecl tensorflow::UnicodeEncodeOp::Compute(class tensorflow::OpKernelContext *)"" (?Compute@UnicodeEncodeOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
unicode_ops.lo.lib(unicode_ops.obj) : warning LNK4217: locally defined symbol ?appendCodePoint@UnicodeStringAppendable@icu_62@@UEAACH@Z (public: virtual signed char __cdecl icu_62::UnicodeStringAppendable::appendCodePoint(int)) imported in function ""public: virtual void __cdecl tensorflow::UnicodeEncodeOp::Compute(class tensorflow::OpKernelContext *)"" (?Compute@UnicodeEncodeOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
unicode_ops.lo.lib(unicode_ops.obj) : warning LNK4217: locally defined symbol ??1ByteSink@icu_62@@UEAA@XZ (public: virtual __cdecl icu_62::ByteSink::~ByteSink(void)) imported in function ""public: class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > & __cdecl icu_62::UnicodeString::toUTF8String<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > &)const "" (??$toUTF8String@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@UnicodeString@icu_62@@QEBAAEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEAV23@@Z)
unicode_ops.lo.lib(unicode_ops.obj) : warning LNK4217: locally defined symbol ?toUTF8@UnicodeString@icu_62@@QEBAXAEAVByteSink@2@@Z (public: void __cdecl icu_62::UnicodeString::toUTF8(class icu_62::ByteSink &)const ) imported in function ""public: class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > & __cdecl icu_62::UnicodeString::toUTF8String<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > &)const "" (??$toUTF8String@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@UnicodeString@icu_62@@QEBAAEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEAV23@@Z)
unicode_ops.lo.lib(unicode_ops.obj) : warning LNK4217: locally defined symbol ?countChar32@UnicodeString@icu_62@@QEBAHHH@Z (public: int __cdecl icu_62::UnicodeString::countChar32(int,int)const ) imported in function ""void __cdecl tensorflow::`anonymous namespace'::Encode(enum tensorflow::UnicodeEncoding,class icu_62::UnicodeString const &,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > *)"" (?Encode@?A0xb9816742@tensorflow@@YAXW4UnicodeEncoding@2@AEBVUnicodeString@icu_62@@PEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z)
unicode_ops.lo.lib(unicode_ops.obj) : warning LNK4217: locally defined symbol ?append@UnicodeString@icu_62@@QEAAAEAV12@H@Z (public: class icu_62::UnicodeString & __cdecl icu_62::UnicodeString::append(int)) imported in function ""private: void __cdecl tensorflow::UnicodeTranscodeOp::TranslateCodepoints(class icu_62::UnicodeString *,bool *,int,int,bool)"" (?TranslateCodepoints@UnicodeTranscodeOp@tensorflow@@AEAAXPEAVUnicodeString@icu_62@@PEA_NHH_N@Z)
unicode_ops.lo.lib(unicode_ops.obj) : warning LNK4217: locally defined symbol ??0UnicodeString@icu_62@@QEAA@XZ (public: __cdecl icu_62::UnicodeString::UnicodeString(void)) imported in function ""public: virtual void __cdecl tensorflow::UnicodeEncodeOp::Compute(class tensorflow::OpKernelContext *)"" (?Compute@UnicodeEncodeOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
unicode_ops.lo.lib(unicode_ops.obj) : warning LNK4217: locally defined symbol ??1UnicodeString@icu_62@@UEAA@XZ (public: virtual __cdecl icu_62::UnicodeString::~UnicodeString(void)) imported in function ""public: virtual void __cdecl tensorflow::UnicodeEncodeOp::Compute(class tensorflow::OpKernelContext *)"" (?Compute@UnicodeEncodeOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
arithmetic_optimizer.lib(arithmetic_optimizer.obj) : warning LNK4217: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported in function ""private: bool __cdecl tensorflow::grappler::`anonymous namespace'::ReorderCastLikeAndValuePreserving::NodeIsOnCpuOrGpu(class tensorflow::NodeDef const *)const "" (?NodeIsOnCpuOrGpu@ReorderCastLikeAndValuePreserving@?A0x956ba610@grappler@tensorflow@@AEBA_NPEBVNodeDef@4@@Z)
layout_optimizer.lib(layout_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported
memory_optimizer.lib(memory_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported
pin_to_host_optimizer.lib(pin_to_host_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported
unicode_ops.lo.lib(unicode_ops.obj) : error LNK2019: unresolved external symbol ""__declspec(dllimport) public: virtual int __cdecl icu_62::UCharCharacterIterator::next32PostInc(void)"" (__imp_?next32PostInc@UCharCharacterIterator@icu_62@@UEAAHXZ) referenced in function ""void __cdecl tensorflow::`anonymous namespace'::Encode(enum tensorflow::UnicodeEncoding,class icu_62::UnicodeString const &,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > *)"" (?Encode@?A0xb9816742@tensorflow@@YAXW4UnicodeEncoding@2@AEBVUnicodeString@icu_62@@PEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z)
unicode_ops.lo.lib(unicode_ops.obj) : error LNK2019: unresolved external symbol ""__declspec(dllimport) public: virtual signed char __cdecl icu_62::UCharCharacterIterator::hasNext(void)"" (__imp_?hasNext@UCharCharacterIterator@icu_62@@UEAACXZ) referenced in function ""void __cdecl tensorflow::`anonymous namespace'::Encode(enum tensorflow::UnicodeEncoding,class icu_62::UnicodeString const &,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > *)"" (?Encode@?A0xb9816742@tensorflow@@YAXW4UnicodeEncoding@2@AEBVUnicodeString@icu_62@@PEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z)
unicode_ops.lo.lib(unicode_ops.obj) : error LNK2019: unresolved external symbol ""__declspec(dllimport) public: __cdecl icu_62::StringCharacterIterator::StringCharacterIterator(class icu_62::UnicodeString const &)"" (__imp_??0StringCharacterIterator@icu_62@@QEAA@AEBVUnicodeString@1@@Z) referenced in function ""void __cdecl tensorflow::`anonymous namespace'::Encode(enum tensorflow::UnicodeEncoding,class icu_62::UnicodeString const &,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > *)"" (?Encode@?A0xb9816742@tensorflow@@YAXW4UnicodeEncoding@2@AEBVUnicodeString@icu_62@@PEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z)
unicode_ops.lo.lib(unicode_ops.obj) : error LNK2019: unresolved external symbol ""__declspec(dllimport) public: virtual __cdecl icu_62::StringCharacterIterator::~StringCharacterIterator(void)"" (__imp_??1StringCharacterIterator@icu_62@@UEAA@XZ) referenced in function ""void __cdecl tensorflow::`anonymous namespace'::Encode(enum tensorflow::UnicodeEncoding,class icu_62::UnicodeString const &,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > *)"" (?Encode@?A0xb9816742@tensorflow@@YAXW4UnicodeEncoding@2@AEBVUnicodeString@icu_62@@PEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z)
unicode_script_op.lo.lib(unicode_script_op.obj) : error LNK2019: unresolved external symbol ""__declspec(dllimport) public: virtual __cdecl icu_62::ErrorCode::~ErrorCode(void)"" (__imp_??1ErrorCode@icu_62@@UEAA@XZ) referenced in function ""public: virtual void __cdecl tensorflow::UnicodeScriptOp::Compute(class tensorflow::OpKernelContext *)"" (?Compute@UnicodeScriptOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
unicode_script_op.lo.lib(unicode_script_op.obj) : error LNK2019: unresolved external symbol ""__declspec(dllimport) public: signed char __cdecl icu_62::ErrorCode::isSuccess(void)const "" (__imp_?isSuccess@ErrorCode@icu_62@@QEBACXZ) referenced in function ""public: virtual void __cdecl tensorflow::UnicodeScriptOp::Compute(class tensorflow::OpKernelContext *)"" (?Compute@UnicodeScriptOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
unicode_script_op.lo.lib(unicode_script_op.obj) : error LNK2019: unresolved external symbol ""__declspec(dllimport) public: enum UErrorCode __cdecl icu_62::ErrorCode::reset(void)"" (__imp_?reset@ErrorCode@icu_62@@QEAA?AW4UErrorCode@@XZ) referenced in function ""public: virtual void __cdecl tensorflow::UnicodeScriptOp::Compute(class tensorflow::OpKernelContext *)"" (?Compute@UnicodeScriptOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
unicode_script_op.lo.lib(unicode_script_op.obj) : error LNK2019: unresolved external symbol ""__declspec(dllimport) const icu_62::ErrorCode::`vftable'"" (__imp_??_7ErrorCode@icu_62@@6B@) referenced in function ""public: virtual void __cdecl tensorflow::UnicodeScriptOp::Compute(class tensorflow::OpKernelContext *)"" (?Compute@UnicodeScriptOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
icuuc.lib(udata.obj) : error LNK2019: unresolved external symbol __imp_icudt62_dat referenced in function ""struct UDataMemory * __cdecl openCommonData(char const *,int,enum UErrorCode *)"" (?openCommonData@@YAPEAUUDataMemory@@PEBDHPEAW4UErrorCode@@@Z)
bazel-out/x64_windows-opt/bin/tensorflow/cc/example/example.exe : fatal error LNK1120: 9 unresolved externals
Target //tensorflow/cc/example:example failed to build
INFO: Elapsed time: 19976.467s, Critical Path: 13898.60s
INFO: 1461 processes: 1461 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
**

Thank you for help!"
24497,this graph contains an operator of type SquaredDifference for which the quantized form is not yet implemented,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu14.04
- TensorFlow installed from (source or binary):pip tf-nightly
- TensorFlow version (or github SHA if from source): tf-nightly1.13.0.dev20181216


**Provide the text output from tflite_convert**

```
I convert pb to lite use the following code:
`import tensorflow as tf
converter = tf.contrib.lite.TFLiteConverter.from_frozen_graph('tflite_graph.pb',[""input_image""],[""result""], input_shapes={""input_image"":[1,626,360,3]})
converter.allow_custom_ops = True
converter.inference_type = tf.contrib.lite.constants.QUANTIZED_UINT8
converter.quantized_input_stats = {""input_image"" : (0., 2.)}
converter.default_ranges_stats=(0, 6)
tflite_quantized_model=converter.convert()
open(""model.tflite"", ""wb"").write(tflite_quantized_model)`

I get the following error:
2018-12-21 11:26:06.351171: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-12-21 11:26:06.354986: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3392410000 Hz
2018-12-21 11:26:06.355300: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x53e0ee0 executing computations on platform Host. Devices:
2018-12-21 11:26:06.355325: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
Traceback (most recent call last):
  File ""test.py"", line 25, in <module>
    tflite_quantized_model=converter.convert()
  File ""/home/zhoushaohuang/Virtualenv/python3.4/lib/python3.4/site-packages/tensorflow/lite/python/lite.py"", line 455, in convert
    **converter_kwargs)
  File ""/home/zhoushaohuang/Virtualenv/python3.4/lib/python3.4/site-packages/tensorflow/lite/python/convert.py"", line 442, in toco_convert_impl
    input_data.SerializeToString())
  File ""/home/zhoushaohuang/Virtualenv/python3.4/lib/python3.4/site-packages/tensorflow/lite/python/convert.py"", line 205, in toco_convert_protos
    ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
2018-12-21 11:26:07.312638: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 168 operators, 271 arrays (0 quantized)
2018-12-21 11:26:07.314127: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 168 operators, 271 arrays (0 quantized)
2018-12-21 11:26:07.323240: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 102 operators, 183 arrays (1 quantized)
2018-12-21 11:26:07.324611: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 96 operators, 171 arrays (1 quantized)
2018-12-21 11:26:07.325812: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 96 operators, 171 arrays (1 quantized)
2018-12-21 11:26:07.326413: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 90 operators, 165 arrays (1 quantized)
2018-12-21 11:26:07.327324: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before default min-max range propagation graph transformations: 90 operators, 165 arrays (1 quantized)
2018-12-21 11:26:07.327972: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After default min-max range propagation graph transformations pass 1: 90 operators, 165 arrays (1 quantized)
2018-12-21 11:26:07.328720: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 90 operators, 165 arrays (1 quantized)
2018-12-21 11:26:07.328791: W tensorflow/lite/toco/graph_transformations/quantize.cc:127] Constant array conv1/conv/weight lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-12-21 11:26:07.328936: F tensorflow/lite/toco/graph_transformations/quantize.cc:491] Unimplemented: this graph contains an operator of type SquaredDifference for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).
Aborted (core dumped)

```


Also, please include a link to a GraphDef or the model if possible.
I use SquaredDifference in following code:
`def instance_norm(x):
    epsilon = 1e-9

    mean = tf.reduce_mean(x,[1,2])
    mean = tf.expand_dims(mean,[1])
    mean = tf.expand_dims(mean,[1])
    s = x.get_shape()
    var = tf.reduce_sum(tf.squared_difference(x, mean), [1,2], keep_dims=True)/(s[1].value*s[2].value)
    result = tf.div(tf.subtract(x, mean), tf.sqrt(tf.add(var, epsilon)))
    return result`
![2018-12-21 15 18 10](https://user-images.githubusercontent.com/28701781/50329687-49bb6680-0533-11e9-9672-68c7d451a888.png)

**Any other info / logs**
I generate pb use the following code:
`input_saver_def = saver.as_saver_def()
 frozen_graph_def = freeze_graph.freeze_graph_with_def_protos(input_graph_def=tf.get_default_graph().as_graph_def(),input_saver_def=input_saver_def,input_checkpoint = FLAGS.model_file,output_node_names='result',restore_op_name='save/restore_all', filename_tensor_name='save/Const:0',clear_devices=True,output_graph='',initializer_nodes='')
binary_graph = 'tflite_graph.pb'
with tf.gfile.GFile(binary_graph, 'wb') as f:
       f.write(frozen_graph_def.SerializeToString())`


Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24496,Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes and No (described below)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Manjaro
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): tf-nightly-gpu (Dec 19, r1.13)
- TensorFlow version (use command below): 1.13.0-dev20181219
- Python version: 3.7.1
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10 with cuDNN 7.4.1
- GPU model and memory: RTX 2070 8GB

**Describe the current behavior**
I'm running the CNN model on MNIST. When I'm running with the GPU, I am encountering
```2018-12-20 20:09:13.644176: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR```

I did some digging and realized that it is a memory issue (which shouldn't be the case as I have 32GB of RAM and 64GB of swap. I ran htop when running the model and I have 20+GB free, which is more than enough to fit the 8GB vRAM mappings. 

Using the `gpu_options.allow_growth = True` gets the model to work properly, and setting `os.environ['CUDA_VISIBLE_DEVICES'] = '-1'` also works. This means that I AM facing a memory issue, but I don't see how. 

Also, using `gpu_options.allow_growth = True` does not fix the same issue when trying to run tensorflow/models/official/mnist/ model, which should have a similar behavior with my code. 

**Code to reproduce the issue**
```
import os
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
import math
import time
# Killing optional CPU driver warnings
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
tf.logging.set_verbosity(tf.logging.ERROR)


class Model:

    def __init__(self, image, label):
        """"""
        A Model class contains a computational graph that classifies images
        to predictions. Each of its methods builds part of the graph
        on Model initialization. Do not modify the constructor, as doing so
        would break the autograder. You may, however, add class variables
        to use in your graph-building. e.g. learning rate, 

        image: the input image to the computational graph as a tensor
        label: the correct label of an image as a tensor
        prediction: the output prediction of the computational graph,
                    produced by self.forward_pass()
        optimize: the model's optimizing tensor produced by self.optimizer()
        loss: the model's loss produced by computing self.loss_function()
        accuracy: the model's prediction accuracy
        """"""
        self.image = image
        self.label = label

        # TO-DO: Add any class variables you want to use.

        self.prediction = self.forward_pass()
        self.loss = self.loss_function()
        self.optimize = self.optimizer()
        self.accuracy = self.accuracy_function()

    def forward_pass(self):
        """"""
        Predicts a label given an image using convolution layers

        :return: the prediction as a tensor
        """"""
        filter_1 = tf.Variable(tf.truncated_normal([3, 3, 1, 8], stddev=0.1))
        conv_1 = tf.nn.conv2d(self.image, filter_1, [1, 1, 1, 1], ""SAME"")

        reshaped = tf.reshape(conv_1, shape=[50, -1])

        L1 = reshaped.shape[1].value
        L2 = 500
        W1 = tf.Variable(tf.random_normal([L1, L2], mean=0, stddev=0.01))
        b1 = tf.Variable(tf.random_normal([L2], mean=0, stddev=0.01))
        relu_1 = tf.nn.relu(tf.matmul(reshaped, W1) + b1)

        W2 = tf.Variable(tf.random_normal([L2, 10], mean=0, stddev=0.01))
        b2 = tf.Variable(tf.random_normal([10], mean=0, stddev=0.01))
        logits = tf.nn.relu(tf.matmul(relu_1, W2) + b2)
        return logits

    def loss_function(self):
        """"""
        Calculates the model cross-entropy loss

        :return: the loss of the model as a tensor
        """"""
        loss = tf.losses.softmax_cross_entropy(onehot_labels=self.label, logits=self.prediction)
        return loss

    def optimizer(self):
        """"""
        Optimizes the model loss using an Adam Optimizer

        :return: the optimizer as a tensor
        """"""
        learning_rate = 0.1
        sgd = tf.train.GradientDescentOptimizer(learning_rate)
        train = sgd.minimize(self.loss)
        return train

    def accuracy_function(self):
        """"""
        Calculates the model's prediction accuracy by comparing
        predictions to correct labels  no need to modify this

        :return: the accuracy of the model as a tensor
        """"""
        correct_prediction = tf.equal(tf.argmax(self.prediction, 1),
                                      tf.argmax(self.label, 1))
        return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))


def main():
    t_start = time.time()

    mnist = input_data.read_data_sets(""data/mnist/"", one_hot=True)
    batch_sz = 50
    batch = 2000

    inputs = tf.placeholder(shape=[batch_sz, 28, 28, 1], dtype=tf.float32)
    labels = tf.placeholder(shape=[batch_sz, 10], dtype=tf.float32)

    model = Model(inputs, labels)

    session_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))
    sess = tf.Session(config=session_config)

    # sess = tf.Session()

    sess.run(tf.global_variables_initializer())
    for i in range(batch):
        next_image, next_label = mnist.train.next_batch(batch_sz)
        next_image = next_image.reshape((batch_sz, 28, 28, 1))
        sess.run(model.optimize, feed_dict={inputs: next_image, labels: next_label})

    acc, test_images, test_labels = 0, mnist.test.images, mnist.test.labels
    test_batch = math.ceil(len(test_images) / batch_sz)
    for i in range(test_batch):
        batch_images = test_images[i * batch_sz: (i + 1) * batch_sz]
        batch_images = batch_images.reshape((batch_sz, 28, 28, 1))
        batch_labes = test_labels[i * batch_sz: (i + 1) * batch_sz]
        acc += sess.run(model.accuracy, feed_dict={inputs: batch_images, labels: batch_labes})
    acc /= test_batch
    print(acc)

    print(time.time() - t_start, 'seconds')

    return


if __name__ == '__main__':
    main()
```
"
24495,Lite audio_microfrontend num_frame compute not same to prepare ,"[num_frames compute](https://github.com/tensorflow/tensorflow/blob/d4ddccd3cca4fc837c66ae1dfa190739420ad122/tensorflow/lite/experimental/microfrontend/audio_microfrontend.cc#L128 ) not equal to [Prepare](https://github.com/tensorflow/tensorflow/blob/d4ddccd3cca4fc837c66ae1dfa190739420ad122/tensorflow/lite/experimental/microfrontend/audio_microfrontend.cc#L107)
"
24494,from tensorflow.python.data import Dataset  ModuleNotFoundError: No module named 'tensorflow.python.data',"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (windows 10):
- TensorFlow installed from (source or binary):
- TensorFlow version (cpu 1.2.1):
- Python version: 3.6.2

--------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-77-256f8ffc6dfe> in <module>()
----> 1 from tensorflow.python.data import Dataset

ModuleNotFoundError: No module named 'tensorflow.python.data'
"
24491,Cannot convert custom .pb file to .tflite file ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  macOS Mojave, version 10.14.2
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.12.0
- **Python version**: 3.6.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:


### Describe the problem

I have a custom model pre-trained in Pytorch framework. I have converted it to ONNX model, and then to TensorFlow model, so that it can be further converted to TensorFlow Lite model for deployment on mobile devices. 

I used the tflite_convert command for the conversion,

tflite_convert\
 --graph_def_file=model.pb \
 --output_file=model.lite \
 --input_format=TENSORFLOW_GRAPH \
 --output_format=TFLITE \
 --input_arrays=0 \
 --output_arrays=add_51 \

Yet I got the following error message:

Traceback (most recent call last):
 File ""/miniconda3/bin/tflite_convert"", line 11, in <module>
  sys.exit(main())
 File ""/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 412, in main app.run(main=run_main, argv=sys.argv[:1])
 File ""/miniconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
  _sys.exit(main(argv))
 File ""/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 408, in run_main _convert_model(tflite_flags)
 File ""/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 162, in _convert_model output_data = converter.convert()
 File ""/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/lite/python/lite.py"", line 453, in convert **converter_kwargs)
 File ""/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/lite/python/convert.py"", line 342, in toco_convert_impl input_data.SerializeToString())
 File ""/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/lite/python/convert.py"", line 135, in toco_convert_protos(stdout, stderr))

RuntimeError: TOCO failed see console for info.
b""2018-12-20 17:44:31.127219: F tensorflow/contrib/lite/toco/import_tensorflow.cc:2137] Check failed: status.ok() Unexpected value for attribute 'data_format'. Expected 'NHWC'\n""
None



The custom pre-trained model is adapted from a Pytorch ResNet model. Therefore I also evaluated converting the ResNet model from the torchvision package to TensorFlow model through the ONNX framework, and further to TensorFlow Lite model. And I got the following error message:

Converting unsupported operation: PyFunc\n2018-12-20 18:09:03.357810: F tensorflow/contrib/lite/toco/import_tensorflow.cc:112] Check failed: attr.value_case() == AttrValue::kType (1 vs. 6)\n'
None



I wonder if the errors above were due to some unsupported operators from the model, probably caused from converting Pytorch model to TensorFlow model through the ONNX framework. As I can convert the pre-trained Mobilenet model (https://www.tensorflow.org/lite/models) to tflite model without error on my platform. Thanks.  


"
24490,quantization aware training in InceptionV3,"Hi, 

In the Figure 7 of whitepaper (Quantizing deep convolutional networks for efficient inference 2018), you mentioned that after the ""concat"" op, the ""fQ"" op will be inserted. 

however, on the evaluation/training graph of quantized InceptionV3, the ""fQ"" op is not appear after the ""concat"" op. 

could you help me to explain this problem?

the version of our Tensorflow is based on v1.12
![insertquant](https://user-images.githubusercontent.com/21340280/50321399-b6226f80-050b-11e9-8913-19c4b935d7a6.png)

Thanks

"
24488,ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory when use screen command,"I try use the **screen** command, but have met this problem.

when I use current screen:
```
~$ nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2017 NVIDIA Corporation
Built on Fri_Sep__1_21:08:03_CDT_2017
Cuda compilation tools, release 9.0, V9.0.176


~$ pip list
Package         Version  
--------------- ---------
tensorboard     1.10.0   
tensorflow-gpu  1.10.1   
termcolor       1.1.0    


~$python3
Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 18:21:58) 
[GCC 7.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
>>> 

```
Obviously, I can use tensorflow.

Now,  
```
$-->screen -list
There is a screen on:
	18536.copy	(12/20/2018 09:49:48 PM)	(Detached)
```
when I use tensorflow in **copy** screen:

```
~$ screen -r copy 

~$ nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2017 NVIDIA Corporation
Built on Fri_Sep__1_21:08:03_CDT_2017
Cuda compilation tools, release 9.0, V9.0.176

~$ pip list
Package         Version  
--------------- ---------
tensorboard     1.10.0   
tensorflow-gpu  1.10.1   
termcolor       1.1.0    


~$ python3
Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 18:21:58)
[GCC 7.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""/home/wangqianlong/miniconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/wangqianlong/miniconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/wangqianlong/miniconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/wangqianlong/miniconda3/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/wangqianlong/miniconda3/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/wangqianlong/miniconda3/lib/python3.6/site-packages/tensorflow/__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/home/wangqianlong/miniconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/wangqianlong/miniconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/wangqianlong/miniconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/wangqianlong/miniconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/wangqianlong/miniconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/wangqianlong/miniconda3/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/wangqianlong/miniconda3/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

```

**current and copy screen is on the same server machine**
Could you give me a suggestion ?
Thanks very much."
24487,"Enable MKL in TensorFlow for c++, could not create a primitive descriptor iterator , mkl_relu_op.cc:871","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version:1.8, 1.9, 1.11.0, 1.2.0
- Python version: 2.7.12
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):0.17.1, 0.18.0
- GCC/Compiler version (if compiling from source): gcc version 5.4.0 
- CUDA/cuDNN version:
- GPU model and memory:
- MKL version: 0.172, 0.13


**Describe the problem**

Run my network test code, and get this error:
 2018-12-21 09:03:42.470347: W tensorflow/core/framework/op_kernel.cc:1407] OP_REQUIRES failed at mkl_relu_op.cc:874 : Aborted: Operation received an exception:Status: 5, message: could not create a primitive descriptor iterator, in file tensorflow/core/kernels/mkl_relu_op.cc:871
2018-12-21 09:03:42.470495: F /home/yhj/faster_rcnn_tf/src/faster_rcnn/networks/src/network.cpp:313] Non-OK-status: session.Run(feed_map, {Shape(scope, input)}, &outputs) status: Aborted: Operation received an exception:Status: 5, message: could not create a primitive descriptor iterator, in file tensorflow/core/kernels/mkl_relu_op.cc:871
	 [[{{node fc6}}]] = _MklRelu[T=DT_FLOAT, _kernel=""MklOp"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Add, DMT/_42)]]

**Provide the exact sequence of commands / steps that you executed before running into the problem**

./configure
bazel build --config=mkl --copt=""-DEIGEN_USE_VML"" -c opt //tensorflow:libtensorflow_cc.so
**bazel build --config=mkl --config=opt //tensorflow:libtensorflow_cc.so

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24483,[TensorFlow Java] Generated factory methods for building operations do not take input list,"**System information**
- TensorFlow version: 1.12

**Describe the current behavior**
There are automatically generated wrapper classes for Graph Operations that provide a neat interface and consistency in practice. 

However, we found that for certain Operations that required multiple inputs (i.e. Merge, Concat) the generated class was incorrect as it took a single input instead of an inputList. 

For example in Merge.java:
```  
public static <T> Merge<T> create(Scope scope, Operand<T> inputs) { // inputs should be a list
    OperationBuilder opBuilder = scope.graph().opBuilder(""Merge"", scope.makeOpName(""Merge""));
    opBuilder.addInput(inputs.asOutput()); //Should be .addInputList() not .addInput()
    return new Merge<T>(opBuilder.build());
}
```

**Describe the expected behavior**
Regarding the code snippet above, we need `inputs` to be a list rather than a single Operand and `.addInput()` should be `.addInputList()`. 

**Code to reproduce the issue**

1. This code below compiles but errors during the run with the error below.
```
    Constant constant1 = Constant.create(scope, 1);
    Enter enter = Enter.create(scope, constant1, ""Loop"");
    Merge.create(scope, enter);
```
```
[error] Exception in thread ""main"" java.lang.IllegalArgumentException: Single tensor passed to 'inputs', expected list while building NodeDef 'Merge' using Op<name=Merge; signature=inputs:N*T -> output:T, value_index:int32; attr=T:type; attr=N:int,min=1>
```

2. On the other hand the below code does not compile:
```
    Constant constant1 = Constant.create(scope, 1);
    Enter enter = Enter.create(scope, constant1, ""Loop"");
    Operand[] inputList = {enter};
    Merge.create(scope, inputList);
```
```
[info] Compiling 1 Java source to /Users/irenedea/linreg/target/scala-2.12/classes ...
[error] /Users/irenedea/linreg/src/main/java/GraphBuilder.java:33:1: method create in class org.tensorflow.op.core.Merge<T> cannot be applied to given types;
[error]   required: org.tensorflow.op.Scope,org.tensorflow.Operand<T>
[error]   found: org.tensorflow.op.Scope,org.tensorflow.Operand[]
[error]   reason: cannot infer type-variable(s) T
[error]     (argument mismatch; org.tensorflow.Operand[] cannot be converted to org.tensorflow.Operand<T>)
[error]     Merge.create(scope, inputList);
```

**Other info / logs**
Created a hack around this issue by building the merge from scratch with the opBuilder, but would be best to have the generated code to be correct as it provides a neat interface :)
```
  public <T> Operation merge(Scope scope, Output<T>[] inputs) {
    return graph.opBuilder(""Merge"", scope.makeOpName(""Merge""))
            .addInputList(inputs).build();
  }
```
```
    Constant constant1 = Constant.create(scope, 1);
    Enter enter = Enter.create(scope, constant1, ""Loop"");
    Output[] inputList = {enter.asOutput()};
    merge(scope, inputList);
```
"
24480,GPU detected on 1.11 not 1.12,"### System information
Followed tensorflow and nvidia install instructions.
conda env python=3.6

Windows 10
GPU = Quadro P1000 compute 6.1
Cuda = 9.0 with patches 2,3,4. Patch 1 doesn't seem to go through. 
cuDNN = 7.4.2.24

tensorflow = 1.12

Added cuda, cudnn, and cupti to path.

### Describe the problem
Installation goes smoothly for tensorflow-gpu for both 1.11 and 1.12. However when using the command:

```
from tensorflow.python.client import device_lib
device_lib.list_local_devices()
```

No GPU device is listed for v1.12. GPU is detected on v.1.11

"
24479,How to define a loss function that needs to input numpy array(not tensor) when build a tensorflow graph?,"hello,
I want to add a constraint option in my loss function. The definition of this constraint option needs numpy array type as input. So, I can not define it as a tensor type as a graph node in tensorflow. How can I define this part in graph so as to join in the network optimization?

"
24477,FusedBatchNorm with is_training = True is folded into unsupported ops,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): pip binary for 1.12
- TensorFlow version (or github SHA if from source):  ('v1.12.0-0-ga6d8ffae09', '1.12.0')


**Provide the text output from tflite_convert**

```
RuntimeError: TOCO failed see console for info.
2018-12-20 11:50:10.890031: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Size
2018-12-20 11:50:10.890062: I tensorflow/contrib/lite/toco/import_tensorflow.cc:189] Unsupported data type in placeholder op: 2
2018-12-20 11:50:10.890069: I tensorflow/contrib/lite/toco/import_tensorflow.cc:189] Unsupported data type in placeholder op: 2
2018-12-20 11:50:10.890075: I tensorflow/contrib/lite/toco/import_tensorflow.cc:189] Unsupported data type in placeholder op: 2
2018-12-20 11:50:10.890241: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 28 operators, 42 arrays (0 quantized)
2018-12-20 11:50:10.890357: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 28 operators, 42 arrays (0 quantized)
2018-12-20 11:50:10.890666: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 20 operators, 31 arrays (1 quantized)
2018-12-20 11:50:10.890848: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 20 operators, 31 arrays (1 quantized)
2018-12-20 11:50:10.890954: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 19 operators, 30 arrays (1 quantized)
2018-12-20 11:50:10.891050: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before default min-max range propagation graph transformations: 19 operators, 30 arrays (1 quantized)
2018-12-20 11:50:10.891093: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After default min-max range propagation graph transformations pass 1: 19 operators, 30 arrays (1 quantized)
2018-12-20 11:50:10.891172: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 19 operators, 30 arrays (1 quantized)
2018-12-20 11:50:10.891194: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:100] Constant array filter_0 lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-12-20 11:50:10.891217: F tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:474] Unimplemented: this graph contains an operator of type (Unsupported TensorFlow op: Size) for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).
Aborted (core dumped)

```

**Any other info / logs**

When quantizing a FusedBatchNorm op with attribute is_training = True, [this method in fold_batch_norms.py ](https://github.com/tensorflow/tensorflow/blob/1fc046c3a8eb62690cd78a6da1b62463e9133f6d/tensorflow/contrib/quantize/python/fold_batch_norms.py#L267) performs a Bessel correction which adds currently unsupported Size and Cast operators to the graph.

```python
"""""" Example of issue with TFLite support caused by FoldBatchNorms

    By applying a bessel correction to the graph using unsupported ops, the
    resulting graph cannot be exported as a UINT8, quantized, TFLite model
""""""

import os
import tensorflow as tf
from tensorflow.python.tools import freeze_graph
from tensorflow.contrib.lite.python import lite_constants as constants
from foldbatchnorms import FoldBatchNorms
from tensorflow.contrib.quantize.python import quantize

# inits
CHECKPOINT_PATH = os.path.join('checkpoint', 'chkp')
QUANT_EVAL_GRAPH = 'model_quant_eval.pb'
FREEZE_GRAPH = 'model_quant_eval_freeze.pb'
TFLITE_PATH = 'model_quant_eval_freeze.tflite'

OUTPUT_NODE = 'out'
INPUT_NODE = 'input'
INPUT_STAT = {'input' : (0., 1.)}

# define a simple graph which includes a FusedBatchNorm operator
inp = tf.placeholder(shape=[None,299,299,3], dtype=tf.float32, name='input')
scale = tf.ones(shape=[32])
offset = tf.zeros(shape=[32])
_filter = tf.get_variable(""filter"", shape=[3,3,3,32],
                          initializer=tf.ones_initializer())

conv = tf.nn.conv2d(input=inp,
                    filter=_filter,
                    data_format='NHWC',
                    dilations=[1, 1, 1, 1],
                    strides=[1, 2, 2, 1],
                    padding='VALID')

y, _, _ = tf.nn.fused_batch_norm(x=conv,
                                 scale=scale,
                                 offset=offset,
                                 epsilon=0.0010000000474974513,
                                 is_training=True,
                                 data_format='NHWC')
tf.nn.relu(y, name='out')


with tf.Session() as sess:

    # quantize graph in-place using FakeQuant* ops (calls fold_batch_norms.py)
    tf.contrib.quantize.create_eval_graph()

    # init vars
    sess.run(tf.global_variables_initializer())

    # save metagraph .pb for converting to TFLite
    with open(QUANT_EVAL_GRAPH, 'w') as f:
        f.write(str(sess.graph.as_graph_def()))

    saver = tf.train.Saver()
    saver.save(sess, CHECKPOINT_PATH)

    # freeze the quantized (FakeQuant*) graph
    freeze_graph.freeze_graph(input_graph=QUANT_EVAL_GRAPH,
                              input_saver='',
                              input_binary=False,
                              input_checkpoint=CHECKPOINT_PATH,
                              output_node_names=OUTPUT_NODE,
                              restore_op_name=""save/restore_all"",
                              filename_tensor_name=""save/Const:0"",
                              output_graph=FREEZE_GRAPH,
                              clear_devices=False,
                              initializer_nodes='')

# convert frozen, quantized, eval graph to UINT8-quant TFLite model
converter = tf.contrib.lite.TFLiteConverter.from_frozen_graph(
                                        graph_def_file=FREEZE_GRAPH,
                                        input_arrays=[INPUT_NODE],
                                        output_arrays=[OUTPUT_NODE])

converter.inference_type = constants.QUANTIZED_UINT8
converter.inference_input_type = constants.QUANTIZED_UINT8
converter.quantized_input_stats = INPUT_STAT
converter.default_ranges_stats = (0,128) # <- bypass Conv2D missing Min/Max data
tflite_model = converter.convert() # <- this will fail
```

Pre-quantization graph containing FusedBatchNorm, Conv2D and Relu ops:
![pre_quant_graph](https://user-images.githubusercontent.com/3189865/50296689-0e446d80-0449-11e9-9d84-2ce60e8d88f2.png)

Post-quantization graph containing unsupported ops:
![post_quant_graph](https://user-images.githubusercontent.com/3189865/50296694-100e3100-0449-11e9-96de-18a7f6906d48.png)
"
24476,[XLA] tf.where not working with xla.compile,"tf.where doesn't seem to be supported when using xla.compile. Are there any ways around the issue, or would it be possible to implement?

Example code:

```
import tensorflow as tf
import tensorflow.contrib.compiler.xla as xla

test_tensor = tf.constant([[1, 2, 3, 0.00004, 5], [6, 7, 8, 9, 10]], dtype=tf.float32)


def func():
    return tf.where(test_tensor < 1e-2)


with tf.device(""/device:CPU:0""):
    compiled_func = xla.compile(func)

with tf.Session() as session:
    res = session.run(compiled_func)
    print(res)
```

Error output:

tensorflow.python.framework.errors_impl.InvalidArgumentError: Detected unsupported operations when trying to compile graph cluster_5308246905621824101[] on XLA_CPU_JIT: Where (No registered 'Where' OpKernel for XLA_CPU_JIT devices compatible with node node Where
	.  Registered:  device='CPU'; T in [DT_BOOL]
  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_BFLOAT16]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_INT8]
  device='CPU'; T in [DT_UINT8]
  device='CPU'; T in [DT_INT16]
  device='CPU'; T in [DT_UINT16]
  device='CPU'; T in [DT_INT32]
  device='CPU'; T in [DT_INT64]"
24475,Can't build TF Model Lite Benchmark tool on Windows,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 Pro 64bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source
- TensorFlow version:nightly build
- Python version:3.5
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):0.20.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the problem**
Can't build TF Model Lite Benchmark tool on Windows.
This code seems to dispatch using PLATFORM_WINDOWS macro.
However PLATFORM_WINDOWS is not defined in the first place.
Why not use portable std::this_thread::sleep_for()?

**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel --output_base $(Build.BinariesDirectory) build -c opt //tensorflow/lite/tools/benchmark:benchmark_model

**Any other info / logs**

```
2018-12-20T15:02:11.7495615Z tensorflow/lite/tools/benchmark/benchmark_model.cc(41): error C3861: 'nanosleep': identifier not found
2018-12-20T15:02:11.7840550Z INFO: Elapsed time: 2.614s, Critical Path: 0.99s
2018-12-20T15:02:11.7840932Z INFO: 0 processes.
2018-12-20T15:02:11.7841202Z FAILED: Build did NOT complete successfully
```
For further information visit [our CI environment](https://dev.azure.com/mlops/build-tflite/_build/results?buildId=11)"
24474,How to build from source with hdfs support on. No option in configure?,"**System information**
- OS Platform and Distribution: Mac OS Mojave 10.14.1 (18B75)
- TensorFlow installed from (source or binary): source
- TensorFlow version: branch r1.12
- Python version: 3.6.6 
- Installed using virtualenv? pip? conda?: building is in a virtualenv created by `pyenv virtualenv 3.6.6 my_tf_build_env`
- Bazel version (if compiling from source): from source. 0.15.0
- GCC/Compiler version (if compiling from source): Apple LLVM version 10.0.0 (clang-1000.11.45.5)
- CUDA/cuDNN version:  no cuda, build only for cpu
- GPU model and memory: Intel Core i7, 16 GB

**Describe the problem**
When I reading threcord in my local hdfs, I encounter error below:

```
  File ""/Users/eva/.pyenv/versions/pycharm_v3.6.6/lib/python3.6/site-packages/tensorflow/python/ops/io_ops.py"", line 166, in read
    return gen_io_ops.reader_read_v2(self._reader_ref, queue_ref, name=name)
  File ""/Users/eva/.pyenv/versions/pycharm_v3.6.6/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 944, in reader_read_v2
    queue_handle=queue_handle, name=name)
  File ""/Users/eva/.pyenv/versions/pycharm_v3.6.6/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/Users/eva/.pyenv/versions/pycharm_v3.6.6/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/Users/eva/.pyenv/versions/pycharm_v3.6.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3274, in create_op
    op_def=op_def)
  File ""/Users/eva/.pyenv/versions/pycharm_v3.6.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1770, in __init__
    self._traceback = tf_stack.extract_stack()

NotFoundError (see above for traceback): dlopen(libhdfs.dylib, 6): image not found
```
I fallowed [this guide](https://www.tensorflow.org/deploy/hadoop) to set hadoop pathes, and I'm sure my `libhdfs.dylib` is in my `HADOOP_HDFS_HOME`. (I build hadoop natively for mac)

I'm not sure if my tensorflow, which is installed via brew, is hdfs enabled. So I just want to build from source with hdfs enable, but when I run `./configure`, there is no option for hdfs?  My questions are:

1/ is the tensorflow shipped via brew hdfs enabled?
2/ how to build from source with hdfs turn on?"
24473,num_examples issue,"For the value of num_examples, what value should we get from the original value? When the amount we get is close to the total number of data.
 Has anyone solved this problem?
 the errors:
ValueError: not enough values to unpack (expected 2, got 1)"
24472,get ctc_greedy_decoder results error C-api ?,"
**System information**
- Have I written custom code 
- OS Platform and Distribution (Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.12
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version: not available
- GPU model and memory: not available

Python code:
self.seqLen = tf.placeholder(tf.int32, [None])
decoder = tf.nn.ctc_greedy_decoder(inputs=ctcIn3dTBC, sequence_length=self.seqLen)

Python result:
`([SparseTensorValue(indices=array([[ 0,  0],
       [ 0,  1],
       [ 0,  2],
       [ 0,  3],
       [ 1,  0],
       [ 1,  1],
       [ 1,  2],
       [ 1,  3],
       [ 2,  0],
       [ 2,  1],
       [ 2,  2],
       [ 2,  3],
       [ 3,  0],
       [ 3,  1],
       [ 3,  2],
       [ 3,  3],
       [ 4,  0],
       [ 4,  1],
       [ 4,  2],
       [ 4,  3],
       [ 5,  0],
       [ 5,  1],
       [ 5,  2],
       [ 5,  3],
       [ 6,  0],
       [ 6,  1],
       [ 6,  2],
       [ 6,  3],
       [ 7,  0],
       [ 7,  1],
       [ 7,  2],
       [ 7,  3],
       [ 8,  0],
       [ 8,  1],
       [ 8,  2],
       [ 8,  3],
       [ 9,  0],
       [ 9,  1],
       [ 9,  2],
       [ 9,  3],
       [10,  0],
       [10,  1],
       [10,  2],
       [10,  3],
       [11,  0],
       [11,  1],
       [11,  2],
       [11,  3],
       [12,  0],
       [12,  1],
       [12,  2],
       [12,  3],
       [13,  0],
       [13,  1],
       [13,  2],
       [13,  3],
       [14,  0],
       [14,  1],
       [14,  2],
       [14,  3],
       [15,  0],
       [15,  1],
       [15,  2],
       [15,  3],
       [16,  0],
       [16,  1],
       [16,  2],
       [16,  3],
       [17,  0],
       [17,  1],
       [17,  2],
       [17,  3],
       [18,  0],
       [18,  1],
       [18,  2],
       [18,  3],
       [19,  0],
       [19,  1],
       [19,  2],
       [19,  3],
       [20,  0],
       [20,  1],
       [20,  2],
       [20,  3],
       [21,  0],
       [21,  1],
       [21,  2],
       [21,  3],
       [22,  0],
       [22,  1],
       [22,  2],
       [22,  3],
       [23,  0],
       [23,  1],
       [23,  2],
       [23,  3],
       [24,  0],
       [24,  1],
       [24,  2],
       [24,  3],
       [25,  0],
       [25,  1],
       [25,  2],
       [25,  3],
       [26,  0],
       [26,  1],
       [26,  2],
       [26,  3],
       [27,  0],
       [27,  1],
       [27,  2],
       [27,  3],
       [28,  0],
       [28,  1],
       [28,  2],
       [28,  3],
       [29,  0],
       [29,  1],
       [29,  2],
       [29,  3],
       [30,  0],
       [30,  1],
       [30,  2],
       [30,  3],
       [31,  0],
       [31,  1],
       [31,  2],
       [31,  3],
       [32,  0],
       [32,  1],
       [32,  2],
       [32,  3],
       [33,  0],
       [33,  1],
       [33,  2],
       [33,  3],
       [34,  0],
       [34,  1],
       [34,  2],
       [34,  3],
       [35,  0],
       [35,  1],
       [35,  2],
       [35,  3],
       [36,  0],
       [36,  1],
       [36,  2],
       [36,  3],
       [37,  0],
       [37,  1],
       [37,  2],
       [37,  3],
       [38,  0],
       [38,  1],
       [38,  2],
       [38,  3],
       [39,  0],
       [39,  1],
       [39,  2],
       [39,  3],
       [40,  0],
       [40,  1],
       [40,  2],
       [40,  3],
       [41,  0],
       [41,  1],
       [41,  2],
       [41,  3],
       [42,  0],
       [42,  1],
       [42,  2],
       [42,  3],
       [43,  0],
       [43,  1],
       [43,  2],
       [43,  3],
       [44,  0],
       [44,  1],
       [44,  2],
       [44,  3],
       [45,  0],
       [45,  1],
       [45,  2],
       [45,  3],
       [46,  0],
       [46,  1],
       [46,  2],
       [46,  3],
       [47,  0],
       [47,  1],
       [47,  2],
       [47,  3],
       [48,  0],
       [48,  1],
       [48,  2],
       [48,  3],
       [49,  0],
       [49,  1],
       [49,  2],
       [49,  3]], dtype=int64), values=array([53, 59, 64, 57, 53, 59, 64, 57, 53, 59, 64, 57, 53, 59, 64, 57, 53,
       59, 64, 57, 53, 59, 64, 57, 53, 59, 64, 57, 53, 59, 64, 57, 53, 59,
       64, 57, 53, 59, 64, 57, 53, 59, 64, 57, 53, 59, 64, 57, 53, 59, 64,
       57, 53, 59, 64, 57, 53, 59, 64, 57, 53, 59, 64, 57, 53, 59, 64, 57,
       53, 59, 64, 57, 53, 59, 64, 57, 53, 59, 64, 57, 53, 59, 64, 57, 53,
       59, 64, 57, 53, 59, 64, 57, 53, 59, 64, 57, 53, 59, 64, 57, 53, 59,
       64, 57, 53, 59, 64, 57, 53, 59, 64, 57, 53, 59, 64, 57, 53, 59, 64,
       57, 53, 59, 64, 57, 53, 59, 64, 57, 53, 59, 64, 57, 53, 59, 64, 57,
       53, 59, 64, 57, 53, 59, 64, 57, 53, 59, 64, 57, 53, 59, 64, 57, 53,
       59, 64, 57, 53, 59, 64, 57, 53, 59, 64, 57, 53, 59, 64, 57, 53, 59,
       64, 57, 53, 59, 64, 57, 53, 59, 64, 57, 53, 59, 64, 57, 53, 59, 64,
       57, 53, 59, 64, 57, 53, 59, 64, 57, 53, 59, 64, 57], dtype=int64), dense_shape=array([50,  4], dtype=int64))], array([[-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455],
       [-297.2455]], dtype=float32))`

Graph-def:
`483: CTCGreedyDecoder type: CTCGreedyDecoder device:  number inputs: 2 number outputs: 4
Number inputs: 2
0 type : TF_FLOAT
1 type : TF_INT32
Number outputs: 4
0 type : TF_INT64
 dims: 2 [-1,2]
1 type : TF_INT64
 dims: 1 [-1]
2 type : TF_INT64
 dims: 1 [2]
3 type : TF_FLOAT
 dims: 2 [50,1]`

C-API result:

raw ouput:
0 0
1 0
2 0
3 0
4 0
5 0
6 0
7 0
8 0
9 0
10 0
11 0
12 0
13 0
14 0
15 0
16 0
17 0
18 0
19 0
20 0
21 0
22 0
23 0
24 0
25 0
26 0
27 0
28 0
29 0
30 0
31 0
32 0
33 0
34 0
35 0
36 0
37 0
38 0
39 0
40 0
41 0
42 0
43 0
44 0
45 0
46 0
47 0
48 0
49 0
-1759315024 1332898372
-1746890864 -1804888032
-1759345360 -1759345360
-1759317136 -1759317136
-1759345360 -1759345360
-1759343376 -1759343376
-572662307 -572662307
-572662307 -572662307
-572662307 2104715895
544096522 544498024
1702258025 1768778100
1881173615 1919251557
1696623983 1735289188
1953722216 779711087
1700929652 1701078371
1126195488 1701999975
1684365933 1953722184
1952540788 1886724211
1919902496 1651336480
1948280686 1852793632
1769234802 544370502
744844400 1886330995
1684370293 1920230770
1685091616 1918988320
544436837 1931502962
543236199 1768910955
1918988298 980644453
1869835877 1767994478
1701344288 1768186981
1701601889 1952804193
1931505524 1953068832
1634732645 1936876916
1835343981 543649385
544433524 1752440935
1634887521 1768778100
1629515375 1835562089
1970107747 540701554
1919906670 1852399988
543516788 1852400740
543517794 1634497901
544175136 1769414757
1879729512 1919251557
1696623983 1735289188
1936028769 1948280686
1919377764 1835627632
544108393 1752459634
1701601889 1310734949
1948280431 1818386804
544502645 543236200
544106784 1347690528
1768186981 1734960750
544108393 1864900719
1936024681 1767859564
1601009006 540701540
1864397413 1935962721
1752637551 543516788
1852400740 1936026722
1986618400 1752369710
540697705 1768319348
1752375398 544370534
1701867296 170815087
1684627301 544761188
544434536 1852383333
1651328288 1632397166
1768320623 1852795252
537554804 1667592816
667177 1768319348
1752375398 1081240139
-1804996768 -1804848800
1684627301 544761188
544434536 1852383333
1651328288 1632397166
1768320623 1852795252
537554804 1667592816
667177 74672654
544096522 544498024
1886330995 1769234810
1835102817 1852383347
1713392962 1684365933
1967988782 1919945317
1646290021 1718513475
1431327845 1852400740
1886330996 1702043764
1752440944 1667592818
1684301154 1818386804
1734960750 778989417
1635280160 1752440876
1936269424 1869881444
1819042164 1952804193
1952540788 1634692128
1836020326 1801676136
1700929652 1948279072
543649385 175335712
1684370549 1835102817
1092631155 544370547
1768843617 1763730792
1696623713 1735289188
1881171308 1919251557
1702065440 1700949349
1869351527 1937055859
543516788 1953525536
1869182049 1769107303
1836018954 541139002
1663070831 1852403305
1852383333 1835343980
543649385 1869422693
1869881441 544106784
1852400740 1886743407
543649385 1296122945
2053729641 1818304622
778922100 1953063791
1948270880 1868767346
1735289198 1768843552
1651336480 1948280686
1818588704 544433513
1763730803 1684301154
1802465132 1852404597
1145118821 1835627632
544108393 1752459634
1701601889 1310734949
1948280431 1818386804
544502645 543236200
544106784 1347690528
1768186981 1734960750
544108393 1864900719
1936024681 1767859564
1601009006 540701540
1864397413 1935962721
1752637551 543516788
1852400740 1936026722
1986618400 1752369710
540697705 1768319348
1752375398 544370534
1701867296 170815087
1684627301 544761188
544434536 1852383333
1651328288 1632397166
1768320623 1852795252
537554804 1667592816
667177 0
0 175335936
544096522 544498024
1886330995 1769234810
1835102817 1852383347
1713392962 1684365933
1967988782 1919945317
1646290021 1718513475
1431327845 1852400740
1886330996 1702043764
1752440944 1667592818

I'am blocked to correctly get  result of ctc_greedy_decoder in c-API, please help?
Thanks.
"
24471,Error installing Tensorflow GPU with CUDA 10.0 for python on Windows ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): -- Error Installing
- TensorFlow version: -- Error Installing
- Python version: 3.6
- Installed using virtualenv? pip? conda?: 
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0/7.4.2
- GPU model and memory: GeForce GTX 1060 6gb

I followed instructions in this tutorial:
https://www.pytorials.com/how-to-install-tensorflow-gpu-with-cuda-10-0-for-python-on-windows/
only difference is I'm using CuDNN version 7.4.2 instead of 7.3.1

Everything went well till step 12:
$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

Error Trace:
$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
WARNING: Option 'experimental_shortened_obj_file_path' is deprecated
Loading:
Loading: 0 packages loaded
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (2 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (7 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (11 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (14 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (18 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (21 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (23 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (26 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (28 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (57 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (71 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (73 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (74 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (74 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (75 packages loaded)
DEBUG: C:/users/lb/_bazel_lb/wvk7snnt/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest Visual C++ installed.
DEBUG: C:/users/lb/_bazel_lb/wvk7snnt/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variables, eg. VS140COMNTOOLS
DEBUG: C:/users/lb/_bazel_lb/wvk7snnt/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: Visual C++ build tools found at C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (77 packages loaded)
WARNING: C:/users/lb/_bazel_lb/wvk7snnt/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/lb/_bazel_lb/wvk7snnt/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: C:/users/lb/_bazel_lb/wvk7snnt/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/lb/_bazel_lb/wvk7snnt/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: C:/users/lb/_bazel_lb/wvk7snnt/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/lb/_bazel_lb/wvk7snnt/external/grpc/bazel/grpc_build_system.bzl:172:12
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (80 packages loaded)
WARNING: C:/tensorflow/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: C:/tensorflow/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
WARNING: C:/tensorflow/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:73:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: C:/tensorflow/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:354:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: C:/tensorflow/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:230:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: C:/tensorflow/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: C:/tensorflow/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: C:/tensorflow/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (82 packages loaded).
INFO: Found 1 target...
[0 / 2] [-----] BazelWorkspaceStatusAction stable-status.txt
[1,100 / 5,102] Compiling tensorflow/contrib/lite/profiling/time.cc; 1s local ... (2 actions, 1 running)
INFO: From Compiling tensorflow/contrib/lite/profiling/time.cc:
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\time.h(36): warning C4820: '_timespec64': '4' bytes padding added after data member '_timespec64::tv_nsec'
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\time.h(43): warning C4820: 'timespec': '4' bytes padding added after data member 'timespec::tv_nsec'
tensorflow/contrib/lite/profiling/time.cc(32): warning C4365: 'return': conversion from '__int64' to 'uint64_t', signed/unsigned mismatch
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_math.h(290): warning C4514: 'fpclassify': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_math.h(295): warning C4514: 'fpclassify': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_math.h(300): warning C4514: 'fpclassify': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_math.h(305): warning C4514: 'signbit': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_math.h(310): warning C4514: 'signbit': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_math.h(315): warning C4514: 'signbit': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_math.h(320): warning C4514: '_fpcomp': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_math.h(325): warning C4514: '_fpcomp': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_math.h(330): warning C4514: '_fpcomp': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_math.h(808): warning C4514: '_chgsignl': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_math.h(815): warning C4514: '_copysignl': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_math.h(869): warning C4514: '_hypotl': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(17): warning C4514: 'abs': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(22): warning C4514: 'pow': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(30): warning C4514: 'abs': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(35): warning C4514: 'acos': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(40): warning C4514: 'acosh': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(45): warning C4514: 'asin': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(50): warning C4514: 'asinh': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(55): warning C4514: 'atan': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(60): warning C4514: 'atanh': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(65): warning C4514: 'atan2': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(70): warning C4514: 'cbrt': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(75): warning C4514: 'ceil': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(80): warning C4514: 'copysign': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(86): warning C4514: 'cos': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(91): warning C4514: 'cosh': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(96): warning C4514: 'erf': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(101): warning C4514: 'erfc': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(106): warning C4514: 'exp': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(111): warning C4514: 'exp2': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(116): warning C4514: 'expm1': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(121): warning C4514: 'fabs': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(126): warning C4514: 'fdim': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(131): warning C4514: 'floor': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(136): warning C4514: 'fma': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(142): warning C4514: 'fmax': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(147): warning C4514: 'fmin': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(152): warning C4514: 'fmod': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(157): warning C4514: 'frexp': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(162): warning C4514: 'hypot': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(167): warning C4514: 'ilogb': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(172): warning C4514: 'ldexp': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(177): warning C4514: 'lgamma': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(182): warning C4514: 'llrint': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(187): warning C4514: 'llround': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(192): warning C4514: 'log': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(197): warning C4514: 'log10': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(202): warning C4514: 'log1p': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(207): warning C4514: 'log2': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(212): warning C4514: 'logb': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(217): warning C4514: 'lrint': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(222): warning C4514: 'lround': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(227): warning C4514: 'modf': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(232): warning C4514: 'nearbyint': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(237): warning C4514: 'nextafter': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(242): warning C4514: 'nexttoward': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(248): warning C4514: 'pow': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(254): warning C4514: 'pow': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(262): warning C4514: 'remainder': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(267): warning C4514: 'remquo': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(273): warning C4514: 'rint': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(278): warning C4514: 'round': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(283): warning C4514: 'scalbln': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(288): warning C4514: 'scalbn': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(293): warning C4514: 'sin': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(298): warning C4514: 'sinh': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(303): warning C4514: 'sqrt': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(308): warning C4514: 'tan': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(313): warning C4514: 'tanh': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(318): warning C4514: 'tgamma': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(323): warning C4514: 'trunc': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(328): warning C4514: 'abs': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(333): warning C4514: 'acos': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(338): warning C4514: 'acosh': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(343): warning C4514: 'asin': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(348): warning C4514: 'asinh': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(353): warning C4514: 'atan': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(358): warning C4514: 'atanh': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(363): warning C4514: 'atan2': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(369): warning C4514: 'cbrt': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(374): warning C4514: 'ceil': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(379): warning C4514: 'copysign': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(385): warning C4514: 'cos': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(390): warning C4514: 'cosh': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(395): warning C4514: 'erf': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(400): warning C4514: 'erfc': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(405): warning C4514: 'exp': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(410): warning C4514: 'exp2': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(415): warning C4514: 'expm1': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(420): warning C4514: 'fabs': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(425): warning C4514: 'fdim': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(431): warning C4514: 'floor': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(436): warning C4514: 'fma': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(442): warning C4514: 'fmax': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(448): warning C4514: 'fmin': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(454): warning C4514: 'fmod': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(460): warning C4514: 'frexp': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(466): warning C4514: 'hypot': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(472): warning C4514: 'ilogb': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(477): warning C4514: 'ldexp': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(483): warning C4514: 'lgamma': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(488): warning C4514: 'llrint': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(493): warning C4514: 'llround': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(498): warning C4514: 'log': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(503): warning C4514: 'log10': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(508): warning C4514: 'log1p': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(513): warning C4514: 'log2': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(518): warning C4514: 'logb': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(523): warning C4514: 'lrint': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(528): warning C4514: 'lround': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(533): warning C4514: 'modf': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(539): warning C4514: 'nearbyint': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(544): warning C4514: 'nextafter': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(550): warning C4514: 'nexttoward': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(556): warning C4514: 'pow': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(562): warning C4514: 'pow': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(571): warning C4514: 'remainder': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(577): warning C4514: 'remquo': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(583): warning C4514: 'rint': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(588): warning C4514: 'round': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(593): warning C4514: 'scalbln': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(599): warning C4514: 'scalbn': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(605): warning C4514: 'sin': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(610): warning C4514: 'sinh': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(615): warning C4514: 'sqrt': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(620): warning C4514: 'tan': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(625): warning C4514: 'tanh': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(630): warning C4514: 'tgamma': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cmath(635): warning C4514: 'trunc': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdlib.h(359): warning C4514: 'abs': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdlib.h(364): warning C4514: 'abs': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdlib.h(369): warning C4514: 'div': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdlib.h(374): warning C4514: 'div': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_memcpy_s.h(64): warning C4514: 'memmove_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wconio.h(100): warning C4514: '_vcwprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wconio.h(127): warning C4514: '_vcwprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wconio.h(154): warning C4514: '_vcwprintf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wconio.h(167): warning C4514: '_cwprintf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wconio.h(185): warning C4514: '_cwprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wconio.h(202): warning C4514: '_cwprintf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wconio.h(220): warning C4514: '_cwprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wconio.h(237): warning C4514: '_cwprintf_p_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wconio.h(255): warning C4514: '_cwprintf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wconio.h(303): warning C4514: '_vcwscanf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wconio.h(335): warning C4514: '_vcwscanf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wconio.h(348): warning C4514: '_cwscanf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wconio.h(371): warning C4514: '_cwscanf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wconio.h(393): warning C4514: '_cwscanf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wconio.h(411): warning C4514: '_cwscanf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wio.h(225): warning C4514: '_wopen': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wio.h(238): warning C4514: '_wsopen': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(311): warning C4514: 'vfwprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(342): warning C4514: 'vfwprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(373): warning C4514: '_vfwprintf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(387): warning C4514: '_vwprintf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(401): warning C4514: 'vwprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(414): warning C4514: '_vwprintf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(430): warning C4514: 'vwprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(445): warning C4514: '_vwprintf_p_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(459): warning C4514: '_vwprintf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(472): warning C4514: '_fwprintf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(491): warning C4514: 'fwprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(509): warning C4514: '_fwprintf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(530): warning C4514: 'fwprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(550): warning C4514: '_fwprintf_p_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(569): warning C4514: '_fwprintf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(587): warning C4514: '_wprintf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(605): warning C4514: 'wprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(622): warning C4514: '_wprintf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(642): warning C4514: 'wprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(661): warning C4514: '_wprintf_p_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(679): warning C4514: '_wprintf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(728): warning C4514: 'vfwscanf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(761): warning C4514: 'vfwscanf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(776): warning C4514: '_vwscanf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(790): warning C4514: 'vwscanf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(803): warning C4514: '_vwscanf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(819): warning C4514: 'vwscanf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(834): warning C4514: '_fwscanf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(853): warning C4514: 'fwscanf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(871): warning C4514: '_fwscanf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(892): warning C4514: 'fwscanf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(912): warning C4514: '_wscanf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(930): warning C4514: 'wscanf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(947): warning C4514: '_wscanf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(967): warning C4514: 'wscanf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(1094): warning C4514: '_vsnwprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(1122): warning C4514: '_vsnwprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(1171): warning C4514: '_vswprintf_c': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(1223): warning C4514: '_vswprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(1276): warning C4514: 'vswprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(1323): warning C4514: '_vswprintf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(1358): warning C4514: '_vscwprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(1391): warning C4514: '_vscwprintf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(1405): warning C4514: '__swprintf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(1425): warning C4514: '_swprintf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(1446): warning C4514: '_swprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(1465): warning C4514: 'swprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(1508): warning C4514: '_swprintf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(1530): warning C4514: 'swprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(1559): warning C4514: '_swprintf_p_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(1580): warning C4514: '_swprintf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(1600): warning C4514: '_swprintf_c_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(1621): warning C4514: '_swprintf_c': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(1641): warning C4514: '_snwprintf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(1667): warning C4514: '_snwprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(1692): warning C4514: '_snwprintf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(1714): warning C4514: '_snwprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(1742): warning C4514: '_scwprintf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(1761): warning C4514: '_scwprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(1779): warning C4514: '_scwprintf_p_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(1798): warning C4514: '_scwprintf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(1822): warning C4514: 'swprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(1837): warning C4514: 'vswprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(1848): warning C4514: '_swprintf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(1863): warning C4514: '_vswprintf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(1921): warning C4514: 'vswscanf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(1956): warning C4514: 'vswscanf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(2019): warning C4514: '_swscanf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(2039): warning C4514: 'swscanf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(2058): warning C4514: '_swscanf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(2080): warning C4514: 'swscanf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(2101): warning C4514: '_snwscanf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(2127): warning C4514: '_snwscanf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(2152): warning C4514: '_snwscanf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstdio.h(2173): warning C4514: '_snwscanf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstring.h(166): warning C4514: 'wcsnlen_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstring.h(247): warning C4514: '_wcstok': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstring.h(261): warning C4514: 'wcstok': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstring.h(539): warning C4514: 'wcschr': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstring.h(545): warning C4514: 'wcspbrk': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstring.h(551): warning C4514: 'wcsrchr': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wstring.h(558): warning C4514: 'wcsstr': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wtime.h(185): warning C4514: '_wctime': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_wtime.h(192): warning C4514: '_wctime_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\sys/stat.h(234): warning C4514: 'fstat': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\sys/stat.h(239): warning C4514: 'stat': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\wchar.h(180): warning C4514: 'fwide': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\wchar.h(189): warning C4514: 'mbsinit': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\wchar.h(268): warning C4514: 'wmemchr': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(648): warning C4514: 'vfprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(679): warning C4514: 'vfprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(710): warning C4514: '_vfprintf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(724): warning C4514: '_vprintf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(738): warning C4514: 'vprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(751): warning C4514: '_vprintf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(767): warning C4514: 'vprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(782): warning C4514: '_vprintf_p_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(796): warning C4514: '_vprintf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(809): warning C4514: '_fprintf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(828): warning C4514: 'fprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(852): warning C4514: '_fprintf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(873): warning C4514: 'fprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(893): warning C4514: '_fprintf_p_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(912): warning C4514: '_fprintf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(930): warning C4514: '_printf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(948): warning C4514: 'printf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(965): warning C4514: '_printf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(985): warning C4514: 'printf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(1004): warning C4514: '_printf_p_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(1022): warning C4514: '_printf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(1070): warning C4514: 'vfscanf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(1104): warning C4514: 'vfscanf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(1120): warning C4514: '_vscanf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(1134): warning C4514: 'vscanf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(1147): warning C4514: '_vscanf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(1163): warning C4514: 'vscanf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(1178): warning C4514: '_fscanf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(1197): warning C4514: 'fscanf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(1215): warning C4514: '_fscanf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(1236): warning C4514: 'fscanf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(1256): warning C4514: '_scanf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(1274): warning C4514: 'scanf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(1291): warning C4514: '_scanf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(1311): warning C4514: 'scanf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(1469): warning C4514: 'vsprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(1510): warning C4514: 'vsprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(1557): warning C4514: '_vsprintf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(1595): warning C4514: '_vsnprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(1623): warning C4514: 'vsnprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(1668): warning C4514: '_vscprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(1733): warning C4514: '_vsnprintf_c': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(1749): warning C4514: '_sprintf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(1774): warning C4514: 'sprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(1810): warning C4514: '_sprintf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(1833): warning C4514: 'sprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(1862): warning C4514: '_sprintf_p_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(1883): warning C4514: '_sprintf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(1903): warning C4514: '_snprintf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(1940): warning C4514: 'snprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(1961): warning C4514: '_snprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(1991): warning C4514: '_snprintf_c_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(2012): warning C4514: '_snprintf_c': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(2032): warning C4514: '_snprintf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(2054): warning C4514: '_snprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(2082): warning C4514: '_scprintf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(2100): warning C4514: '_scprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(2117): warning C4514: '_scprintf_p_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(2135): warning C4514: '_scprintf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(2183): warning C4514: 'vsscanf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(2244): warning C4514: '_sscanf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(2263): warning C4514: 'sscanf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(2281): warning C4514: '_sscanf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(2302): warning C4514: 'sscanf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(2330): warning C4514: '_snscanf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(2354): warning C4514: '_snscanf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(2378): warning C4514: '_snscanf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\stdio.h(2402): warning C4514: '_snscanf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\corecrt_memory.h(100): warning C4514: 'memchr': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\string.h(371): warning C4514: 'strnlen_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\string.h(504): warning C4514: 'strchr': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\string.h(510): warning C4514: 'strpbrk': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\string.h(516): warning C4514: 'strrchr': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\string.h(522): warning C4514: 'strstr': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\vcruntime_new.h(86): warning C4514: 'operator new': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\vcruntime_new.h(92): warning C4514: 'operator delete': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\vcruntime_new.h(101): warning C4514: 'operator new[]': unreferenced inline function has been removed
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\vcruntime_new.h(107): warning C4514: 'operator delete[]': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\time.h(476): warning C4514: 'ctime': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\time.h(487): warning C4514: 'difftime': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\time.h(496): warning C4514: 'gmtime': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\time.h(506): warning C4514: 'localtime': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\time.h(517): warning C4514: '_mkgmtime': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\time.h(525): warning C4514: 'mktime': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\time.h(532): warning C4514: 'time': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\time.h(540): warning C4514: 'timespec_get': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\time.h(550): warning C4514: 'ctime_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\time.h(560): warning C4514: 'gmtime_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt\time.h(569): warning C4514: 'localtime_s': unreferenced inline function has been removed
INFO: From Compiling external/com_google_absl/absl/base/dynamic_annotations.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/grpc/third_party/address_sorting/address_sorting_posix.c:
cl : Command line warning D9002 : ignoring unknown option '-std=c99'
INFO: From Compiling external/grpc/third_party/address_sorting/address_sorting_windows.c:
cl : Command line warning D9002 : ignoring unknown option '-std=c99'
INFO: From Compiling external/grpc/third_party/address_sorting/address_sorting.c:
cl : Command line warning D9002 : ignoring unknown option '-std=c99'
[1,414 / 5,840] Compiling external/protobuf_archive/src/google/protobuf/compiler/js/embed.cc; 0s local ... (2 actions, 1 running)
INFO: From Executing genrule //tensorflow/core:version_info_gen:
fatal: Invalid path '/c/users/lb/_bazel_lb/wvk7snnt/execroot/org_tensorflow/C:': No such file or directory
INFO: From Linking external/protobuf_archive/python/google/protobuf/internal/_api_implementation.so:
   Creating library bazel-out/x64_windows-opt/bin/external/protobuf_archive/python/google/protobuf/internal/lib_api_implementation.so.ifso and object bazel-out/x64_windows-opt/bin/external/protobuf_archive/python/google/protobuf/internal/lib_api_implementation.so.exp
[1,536 / 5,952] Executing genrule @local_config_cuda//cuda:cuda-include; 72s local ... (2 actions, 1 running)
INFO: From Compiling external/com_google_absl/absl/numeric/int128.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
[1,630 / 5,967] Executing genrule @local_config_cuda//cuda:cuda-include; 162s local ... (2 actions, 1 running)
INFO: From Compiling external/com_google_absl/absl/strings/internal/utf8.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/internal/ostringstream.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
ERROR: C:/users/lb/_bazel_lb/wvk7snnt/external/jpeg/BUILD.bazel:401:1: Executing genrule @jpeg//:simd_win_x86_64_assemble failed (Illegal instruction): bash.exe failed: error executing command
  cd C:/users/lb/_bazel_lb/wvk7snnt/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET PATH=C:\msys64\usr\bin;C:\msys64\bin;C:\bazel;C:\msys64\usr\local\bin;C:\msys64\usr\bin;C:\msys64\usr\bin;C:\msys64\opt\bin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\bin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\libnvvp;C:\Program Files\Microsoft MPI\Bin;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0;C:\Program Files (x86)\Microsoft SQL Server\140\Tools\Binn;C:\Program Files\Microsoft SQL Server\140\Tools\Binn;C:\Program Files (x86)\Microsoft SQL Server\140\DTS\Binn;C:\Program Files\Microsoft SQL Server\140\DTS\Binn;C:\Program Files\Microsoft SQL Server\Client SDK\ODBC\130\Tools\Binn;C:\Program Files\dotnet;C:\Program Files\Microsoft SQL Server\130\Tools\Binn;C:\Program Files (x86)\Microsoft SQL Server\Client SDK\ODBC\130\Tools\Binn;C:\Program Files (x86)\Microsoft SQL Server\140\Tools\Binn\ManagementStudio;C:\WINDOWS\System32\OpenSSH;C:\Program Files\nodejs;C:\Program Files\Git\cmd;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\NVIDIA Corporation\NVIDIA NvDLISR;C:\Program Files\Microsoft VS Code\bin;C:\Users\LB\AppData\Local\Programs\Python\Python36\Scripts;C:\Users\LB\AppData\Local\Programs\Python\Python36;C:\Users\LB\AppData\Local\Microsoft\WindowsApps;C:\Users\LB\AppData\Local\Programs\Microsoft VS Code\bin;C:\Users\LB\AppData\Roaming\npm;C:\Users\LB\AppData\Local\Programs\Microsoft VS Code Insiders\bin;C:\Program Files\JetBrains\PyCharm 2018.3.2\bin;C:\msys64\usr\bin\site_perl;C:\msys64\usr\bin\vendor_perl;C:\msys64\usr\bin\core_perl
    SET PYTHON_BIN_PATH=C:/Users/LB/AppData/Local/Programs/Python/Python36/python.exe
    SET PYTHON_LIB_PATH=C:/Users/LB/AppData/Local/Programs/Python/Python36/lib/site-packages
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=5.0
    SET TF_CUDA_VERSION=10.0
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
  C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; for out in bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jccolor-avx2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jccolor-sse2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jcgray-avx2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jcgray-sse2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jchuff-sse2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jcphuff-sse2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jcsample-avx2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jcsample-sse2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jdcolor-avx2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jdcolor-sse2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jdmerge-avx2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jdmerge-sse2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jdsample-avx2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jdsample-sse2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jfdctflt-sse.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jfdctfst-sse2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jfdctint-avx2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jfdctint-sse2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jidctflt-sse2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jidctfst-sse2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jidctint-avx2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jidctint-sse2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jidctred-sse2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jquantf-sse2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jquanti-avx2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jquanti-sse2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jsimdcpu.obj; do
  bazel-out/x64_windows-opt/bin/external/nasm/nasm -fwin64 -DWIN64 -D__x86_64__    -I $(dirname external/jpeg/simd/x86_64/jccolext-sse2.asm)/    -I $(dirname external/jpeg/simd/nasm/jdct.inc)/    -I $(dirname external/jpeg/simd/nasm/jdct.inc)/../../win/    -o $out    $(dirname external/jpeg/simd/x86_64/jccolext-sse2.asm)/$(basename ${out%.obj}.asm)
done: bash.exe failed: error executing command
  cd C:/users/lb/_bazel_lb/wvk7snnt/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET PATH=C:\msys64\usr\bin;C:\msys64\bin;C:\bazel;C:\msys64\usr\local\bin;C:\msys64\usr\bin;C:\msys64\usr\bin;C:\msys64\opt\bin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\bin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\libnvvp;C:\Program Files\Microsoft MPI\Bin;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0;C:\Program Files (x86)\Microsoft SQL Server\140\Tools\Binn;C:\Program Files\Microsoft SQL Server\140\Tools\Binn;C:\Program Files (x86)\Microsoft SQL Server\140\DTS\Binn;C:\Program Files\Microsoft SQL Server\140\DTS\Binn;C:\Program Files\Microsoft SQL Server\Client SDK\ODBC\130\Tools\Binn;C:\Program Files\dotnet;C:\Program Files\Microsoft SQL Server\130\Tools\Binn;C:\Program Files (x86)\Microsoft SQL Server\Client SDK\ODBC\130\Tools\Binn;C:\Program Files (x86)\Microsoft SQL Server\140\Tools\Binn\ManagementStudio;C:\WINDOWS\System32\OpenSSH;C:\Program Files\nodejs;C:\Program Files\Git\cmd;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\NVIDIA Corporation\NVIDIA NvDLISR;C:\Program Files\Microsoft VS Code\bin;C:\Users\LB\AppData\Local\Programs\Python\Python36\Scripts;C:\Users\LB\AppData\Local\Programs\Python\Python36;C:\Users\LB\AppData\Local\Microsoft\WindowsApps;C:\Users\LB\AppData\Local\Programs\Microsoft VS Code\bin;C:\Users\LB\AppData\Roaming\npm;C:\Users\LB\AppData\Local\Programs\Microsoft VS Code Insiders\bin;C:\Program Files\JetBrains\PyCharm 2018.3.2\bin;C:\msys64\usr\bin\site_perl;C:\msys64\usr\bin\vendor_perl;C:\msys64\usr\bin\core_perl
    SET PYTHON_BIN_PATH=C:/Users/LB/AppData/Local/Programs/Python/Python36/python.exe
    SET PYTHON_LIB_PATH=C:/Users/LB/AppData/Local/Programs/Python/Python36/lib/site-packages
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=5.0
    SET TF_CUDA_VERSION=10.0
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
  C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; for out in bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jccolor-avx2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jccolor-sse2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jcgray-avx2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jcgray-sse2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jchuff-sse2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jcphuff-sse2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jcsample-avx2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jcsample-sse2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jdcolor-avx2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jdcolor-sse2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jdmerge-avx2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jdmerge-sse2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jdsample-avx2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jdsample-sse2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jfdctflt-sse.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jfdctfst-sse2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jfdctint-avx2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jfdctint-sse2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jidctflt-sse2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jidctfst-sse2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jidctint-avx2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jidctint-sse2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jidctred-sse2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jquantf-sse2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jquanti-avx2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jquanti-sse2.obj bazel-out/x64_windows-opt/genfiles/external/jpeg/simd/x86_64/jsimdcpu.obj; do
  bazel-out/x64_windows-opt/bin/external/nasm/nasm -fwin64 -DWIN64 -D__x86_64__    -I $(dirname external/jpeg/simd/x86_64/jccolext-sse2.asm)/    -I $(dirname external/jpeg/simd/nasm/jdct.inc)/    -I $(dirname external/jpeg/simd/nasm/jdct.inc)/../../win/    -o $out    $(dirname external/jpeg/simd/x86_64/jccolext-sse2.asm)/$(basename ${out%.obj}.asm)
done
/usr/bin/bash: line 1:  1896 Illegal instruction     bazel-out/x64_windows-opt/bin/external/nasm/nasm -fwin64 -DWIN64 -D__x86_64__ -I $(dirname external/jpeg/simd/x86_64/jccolext-sse2.asm)/ -I $(dirname external/jpeg/simd/nasm/jdct.inc)/ -I $(dirname external/jpeg/simd/nasm/jdct.inc)/../../win/ -o $out $(dirname external/jpeg/simd/x86_64/jccolext-sse2.asm)/$(basename ${out%.obj}.asm)
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 713.407s, Critical Path: 172.97s
INFO: 192 processes: 192 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully


"
24470,Error installing Tensorflow 1.12,"Hi - 

I have 
- Windows 7
- Anaconda 3
- Python 3.6.5
- Tensorflow upgraded from 1.10 to 1.12 (via anaconda)
- No GPU
- CUDA toolkit 9.0 (conda installed)
- cuDNN 7.1.4 (conda installed)
- Bazel version: not installed
- Mobile device: None

All I did was updating tensorflow via anaconda: conda update tensorflow. It updated from 1.10 to 1.12:
2018-12-20 13:54:12  (rev 4)
     conda  {4.5.11 -> 4.5.12}
     tensorboard  {1.10.0 -> 1.12.0}
     tensorflow  {1.10.0 -> 1.12.0}
     tensorflow-base  {1.10.0 -> 1.12.0}
    +_tflow_select-2.1.0
    +cudatoolkit-9.0
    +cudnn-7.1.4
    +keras-applications-1.0.6
    +keras-preprocessing-1.0.5


I get the error message below when importing tensorflow into python:




---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
~\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

~\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>()
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

~\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

~\Anaconda3\lib\imp.py in load_module(name, file, filename, details)
    242         else:
--> 243             return load_dynamic(name, filename, file)
    244     elif type_ == PKG_DIRECTORY:

~\Anaconda3\lib\imp.py in load_dynamic(name, path, file)
    342             name=name, loader=loader, origin=path)
--> 343         return _load(spec)
    344 

ImportError: DLL load failed: Det angivne modul blev ikke fundet.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-1-9b31ac0d4d77> in <module>()
      1 import pandas as pd
----> 2 import tensorflow as tf
      3 from datetime import datetime
      4 import matplotlib.pyplot as plt
      5 import numpy as np

~\Anaconda3\lib\site-packages\tensorflow\__init__.py in <module>()
     22 
     23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     25 
     26 try:

~\Anaconda3\lib\site-packages\tensorflow\python\__init__.py in <module>()
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 from tensorflow.python.tools import component_api_helper

~\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\Users\jet\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\jet\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\jet\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\jet\Anaconda3\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\jet\Anaconda3\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Det angivne modul blev ikke fundet.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

"
24469,Practical use of CudnnRNN,"Hi @houtoms @protoget @ebrevdo 

I imagined that #23588 would replace TensorFlow's `LSTMCell` implementation, but some main limitations at the moment are the `RNNCell` wrappers such as Dropout, Attention or Residual, all frequently used in research projects and essential for learning. Even if cuDNN was open-source, it would still be impractical for people to code research ideas in c++ and compile the toolkit every time.

Would it be possible to make `LSTMCell` run as fast as `CudnnLSTM` while retaining the flexibility of the Python API ? Could XLA-generated code match the performance of cudnn given sufficient effort from both TF and Nvidia ? Or are there any plans to support these popular features in cuDNN soon ?
"
24468,Allow workers to be extensible/scalable in distributed training with PS servers,"Inside an AI-driven business organization, we are exploring large-scale asynchronized & distributed training with help of TensorFlow. Below is the feature contribution from our industrial practice.

**System information**
- TensorFlow version (you are using): **TensorFlow r1.13**
- Are you willing to contribute it (Yes/No): **Yes**

**Describe the feature and the current behavior/state.**

One of the most helpful feature is ClusterSpec Propagation which simplifies the setup of TF servers. TF worker sessions can get cluster topology from the master session in runtime.

However, we have found that when ClusterSpec Propagation is enabled, the stateful OPs like VariableV2 are no longer shared among worker sessions in the same PS server. Digging into the source, we found the sharing switch is hard coded to be off at [master_session.cc](
https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/core/distributed_runtime/master_session.cc#L1287).

I have submitted a pull request titled ""**Respect isolate_session_state in config when ClusterSpec propagation is enabled**(#24466)"". By canceling the isolation, we may achieve the scenario where worker number can be dynamic-increased in a between-graph distributed training.

With this code change, newly launched workers can join the existing distributed training in a peaceful way without restarting all PS servers and workers using an updated ClusterSpec.

**Will this change the current api? How?**

No API change.

However, in the old version, if 2 master sessions are created with the ClusterSpec field set in `tf.ConfigProto`, worker sessions sessions won't share the state in default when they are owned by different master sessions.

After my code change, a customer need explicitly set `isolate_session_state` to `True` in `tf.ConfigProto` if he or she want to keep the old behavior where variable values are isolated between 2 worker sessions. Otherwise, variables with the same name in a PS server will be shared.

**Who will benefit with this feature?**

Those exploring large-scale distributed training with help of TensorFlow PS servers, like data scientists and cloud engineers.

**Any Other info.**
There is no performance or data corruption after we enabled this feature in our scenario with tens of PS servers and hundreds of workers.
"
24465,Unexpected result for tf.sqrt under certain conditions,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave 10.14.1
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6

**Describe the current behavior**

The output `a_sqrt_1` of [tf.sqrt](https://www.tensorflow.org/api_docs/python/tf/math/sqrt) in the provided snippet is incorrect. Concretely, when `a` is a [tf.Variable](https://www.tensorflow.org/api_docs/python/tf/Variable) and `a_sqrt` is run together with `a_scaled`, the output `a_sqrt_1` is `a_sqrt_2 ** -1`.

**Describe the expected behavior**
The expected result is `a_sqrt_2`.

**Code to reproduce the issue**
```
import tensorflow as tf

x = 100.0
a = tf.Variable(x)
# a = tf.constant(x)  # Interestingly, it works when `a` is a constant
a_sqrt = tf.sqrt(a)
a_scaled = a / a_sqrt


with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    a_sqrt_1, _ = sess.run([a_sqrt, a_scaled])
    print(a_sqrt_1)  # prints 0.1  (!!!)

    a_sqrt_2 = sess.run(a_sqrt)
    print(a_sqrt_2)  # prints 10.0
```
"
24464,Tensorboard could not bind to unsupported address family,"Hi,

I am not convinced whether this a bug or not but I believe it is nice information to report anyway.

**System information**
Under OS Debian Stretch
tensorboard               1.12.1
tensorflow                1.12.0          
tensorflow-base           1.12.0 
python                    3.6.7 

**Describe the current behavior**
When executing in terminal : 
`tensorboard --logdir=logs/`
I get the following error : 
`E1220 09:47:24.720813 MainThread program.py:201] Tensorboard could not bind to unsupported address family ::
E1220 09:47:24.720813 139960141604608 program.py:201] Tensorboard could not bind to unsupported address family ::
ERROR: Tensorboard could not bind to unsupported address family ::`

However, when downgrading tensorboard to 1.8.0, everything works fine !

I found this workaround on a  blog, but I have not found any reporting of this bug... Sorry in advance if it an obvious mistake of my part or a known bug / error

"
24460,Problem on bazel building target with GPU support(Ubuntu16.04)," >>bazel build --config=opt --config=cuda tensorflow/tools/pip_package:build_pi
p_package:
DEBUG: /home/ubuntu/.cache/bazel/_bazel_ubuntu/712f0e98e13675fc83790945ec267f95/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5: 
Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default
ERROR: cc_toolchain_suite '@local_config_cuda//crosstool:toolchain' does not contain a toolchain for CPU 'k8', you may want to add an entry for 'local|compiler' into toolchains and toolchain_identifier 'local_linux' into the corresponding cc_toolchain rule (see --incompatible_disable_cc_toolchain_label_from_crosstool_proto).
INFO: Elapsed time: 0.181s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
"
24459,Tensorflow crashes when trying to feed > 2GiB to TPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu Linux 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.12
- Python version: N/A
- Bazel version (if compiling from source): Bazel 0.20.0
- GCC/Compiler version (if compiling from source): 7.2.0
- CUDA/cuDNN version: N/A
- GPU model and memory: Cloud TPU

**Describe the current behavior**

I'm trying to feed a cloud TPU (v3) some data. However, whenever the total feed size goes above 2GiB, I get the following crash inside libtensorflow:

```
E1220 02:28:43.129226740   14451 proto_buffer_writer.h:83]   assertion failed: byte_count_ < total_size_

signal (6): Aborted
in expression starting at no file:0
gsignal at /lib/x86_64-linux-gnu/libc.so.6 (unknown line)
abort at /lib/x86_64-linux-gnu/libc.so.6 (unknown line)
_ZN4grpc11CoreCodegen11assert_failEPKcS2_i at /home/keno/.julia/packages/TensorFlow/YWnga/src/../deps/usr/lib/libtensorflow.so (unknown line)
_ZN4grpc17ProtoBufferWriter4NextEPPvPi at /home/keno/.julia/packages/TensorFlow/YWnga/src/../deps/usr/lib/libtensorflow.so (unknown line)
_ZN6google8protobuf2io17CodedOutputStream7RefreshEv at /home/keno/.julia/packages/TensorFlow/YWnga/src/../deps/usr/lib/libtensorflow_framework.so (unknown line)
_ZN6google8protobuf2io17CodedOutputStreamC2EPNS1_20ZeroCopyOutputStreamEb at /home/keno/.julia/packages/TensorFlow/YWnga/src/../deps/usr/lib/libtensorflow_framework.so (unknown line)
_ZNK6google8protobuf11MessageLite25SerializeToZeroCopyStreamEPNS0_2io20ZeroCopyOutputStreamE at /home/keno/.julia/packages/TensorFlow/YWnga/src/../deps/usr/lib/libtensorflow_framework.so (unknown line)
_ZN4grpc16GenericSerializeINS_17ProtoBufferWriterEN10tensorflow14RunStepRequestEEENS_6StatusERKN6google8protobuf7MessageEPNS_10ByteBufferEPb at /home/keno/.julia/packages/TensorFlow/YWnga/src/../deps/usr/lib/libtensorflow.so (unknown line)
_ZN4grpc8internal21BlockingUnaryCallImplIN10tensorflow14RunStepRequestENS2_15RunStepResponseEEC1EPNS_16ChannelInterfaceERKNS0_9RpcMethodEPNS_13ClientContextERKS3_PS4_ at /home/keno/.julia/packages/TensorFlow/YWnga/src/../deps/usr/lib/libtensorflow.so (unknown line)
_ZN10tensorflow4grpc13MasterService4Stub7RunStepEPN4grpc13ClientContextERKNS_14RunStepRequestEPNS_15RunStepResponseE at /home/keno/.julia/packages/TensorFlow/YWnga/src/../deps/usr/lib/libtensorflow.so (unknown line)
_ZN10tensorflow16GrpcRemoteMaster7RunStepEPNS_11CallOptionsEPNS_21RunStepRequestWrapperEPNS_29MutableRunStepResponseWrapperE at /home/keno/.julia/packages/TensorFlow/YWnga/src/../deps/usr/lib/libtensorflow.so (unknown line)
_ZN10tensorflow11GrpcSession8RunProtoEPNS_11CallOptionsEPNS_28MutableRunStepRequestWrapperEPNS_29MutableRunStepResponseWrapperE at /home/keno/.julia/packages/TensorFlow/YWnga/src/../deps/usr/lib/libtensorflow.so (unknown line)
_ZN10tensorflow11GrpcSession9RunHelperERKNS_10RunOptionsERKSt6vectorISt4pairISsNS_6TensorEESaIS7_EERKS4_ISsSaISsEESF_PS4_IS6_SaIS6_EEPNS_11RunMetadataERKSs at /home/keno/.julia/packages/TensorFlow/YWnga/src/../deps/usr/lib/libtensorflow.so (unknown line)
_ZN10tensorflow11GrpcSession3RunERKNS_10RunOptionsERKSt6vectorISt4pairISsNS_6TensorEESaIS7_EERKS4_ISsSaISsEESF_PS4_IS6_SaIS6_EEPNS_11RunMetadataE at /home/keno/.julia/packages/TensorFlow/YWnga/src/../deps/usr/lib/libtensorflow.so (unknown line)
_ZL13TF_Run_HelperPN10tensorflow7SessionEPKcPK9TF_BufferRKSt6vectorISt4pairISsNS_6TensorEESaISA_EERKS7_ISsSaISsEEPP9TF_TensorSI_PS4_P9TF_Status at /home/keno/.julia/packages/TensorFlow/YWnga/src/../deps/usr/lib/libtensorflow.so (unknown line)
TF_SessionRun at /home/keno/.julia/packages/TensorFlow/YWnga/src/../deps/usr/lib/libtensorflow.so (unknown line)
```

**Describe the expected behavior**

Ideally it should work (after all there's 128GiB of HBM on the other side of this pipe so 2GiB is really not unreasonable here), worst case I'd have expected an error status to be returned rather than a hard assert.

**Code to reproduce the issue**

Any code that passes feeds totaling > 2GiB to TF_SessionRun connected over a grpc connection. In this particular example, I was trying to use the `InfeedEnqueueTuple` op (with the inputs being placeholders that I'm feeding).
"
24458,XLA Invalid __global__ read in fusion_514 (cublas bug?),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Maybe. I modified the BERT network to do fp16 math and compiled with XLA.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
n/a
- TensorFlow installed from (source or binary):
NVIDIA 18.12 development container
- TensorFlow version (use command below):
1.12
- Python version:
3.5.2
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
10.0.130
- GPU model and memory:
Tesla V100-SXM2-32GB

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
Code runs for a while and then suddenly generates NaNs. Code runs normally when XLA is disabled.
cuda-memcheck caught a bunch of these:

========= Invalid __global__ read of size 4
=========     at 0x00000880 in fusion_514
=========     by thread (959,0,0) in block (10,0,0)
=========     Address 0x7f359f6e6e04 is out of bounds
=========     Device Frame:fusion_514 (fusion_514 : 0x880)
=========     Saved host backtrace up to driver entry point at kernel launch time


[xla_invalid_read_fusion_514_hlo_dumps.zip](https://github.com/tensorflow/tensorflow/files/2697101/xla_invalid_read_fusion_514_hlo_dumps.zip)
**Describe` the expected behavior**
Code runs without generating NaNs.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Should be reviewed by @jlebar 
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24457,Latest code in master branch won't compile on CentOS,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
CentOS Linux release 7.5.1804
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
Source
- TensorFlow version:
Latest code in master branch
- Python version:
2.7
- Installed using virtualenv? pip? conda?:
pip
- Bazel version (if compiling from source):
1.9
- GCC/Compiler version (if compiling from source):
6.3.0
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A



**Describe the problem**
When compile with the following command line: 
bazel build -c dbg //tensorflow/tools/pip_package:build_pip_package
I got the following error:
external/eigen_archive/Eigen/src/Core/products/GeneralBlockPanelKernel.h: In member function 'void Eigen::internal::gebp_kernel<LhsScalar, RhsScalar, Index, DataMapper, mr, nr, ConjugateLhs, ConjugateRhs>::operator()(const DataMapper&, const LhsScalar*, const RhsScalar*, Index, Index, Index, Eigen::internal::gebp_kernel<LhsScalar, RhsScalar, Index, DataMapper, mr, nr, ConjugateLhs, ConjugateRhs>::ResScalar, Index, Index, Index, Index) [with LhsScalar = float; RhsScalar = float; Index = long int; DataMapper = Eigen::internal::blas_data_mapper<float, long int, 0, 0>; int mr = 8; int nr = 4; bool ConjugateLhs = false; bool ConjugateRhs = false]':
external/eigen_archive/Eigen/src/Core/products/GeneralBlockPanelKernel.h:1419:13: error: inconsistent operand constraints in an 'asm'
             EIGEN_GEBGP_ONESTEP(0);
             ^
and more similar errors.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
see the commands above

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24453,DLPack support in tensorflow,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): v1.12.0
- Are you willing to contribute it (Yes/No): I could help but I'm not a Tensorflow expert.

**Describe the feature and the current behavior/state.**
[DLPack](https://github.com/dmlc/dlpack) is a community effort to define a common tensor data structure that could be shared by different frameworks. Currently, Pytorch and MXNet have adopted this interface. Using DLPack can benefit many projects in the ecosystem: let it be backend compiler like [TVM](https://github.com/dmlc/tvm) or front-end package like [DGL](https://github.com/dmlc/dgl). So it will be really great if Tensorflow can also adopt this interface to help new ideas derived from the mainstream tensor-based frameworks.

**Will this change the current api? How?**
Here is a draft proposal. Two new APIs to convert `tf.Tensor` from/to dlpack format. Examples in `torch` are `dl = torch.utils.dlpack.to_dlpack(tsor)` and `tsor = torch.utils.dlpack.from_dlpack(dl)`.

**Who will benefit with this feature?**
Many projects that wish to improve, optimize or be complementary to the mainstream tensor frameworks.

@tqchen @zzhang-cn"
24450,TF RPI build fails to work properly with Go API,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux rpi3-0 4.14.79-v7+ #1159 SMP Sun Nov 4 17:50:20 GMT 2018 armv7l GNU/Linux

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
Nope

- TensorFlow installed from (source or binary):
Source

- TensorFlow version (use command below):
At 9fd8253129

- Python version:
Python 2.7.13

- Bazel version (if compiling from source):
```
$ bazel version
INFO: Invocation ID: e77ca562-1751-40be-9b60-addbf843411d
Build label: 0.20.0- (@non-git)
Build target: bazel-out/arm-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Dec 13 03:56:33 2018 (1544673393)
Build timestamp: 1544673393
Build timestamp as int: 1544673393
```

- GCC/Compiler version (if compiling from source):
gcc (Raspbian 4.8.5-4) 4.8.5

- CUDA/cuDNN version:
N/A

- GPU model and memory:
N/A


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I am using Go API for TensorFlow on Raspberry PI. `go test` fails with following error mesg:
```
$ go test -v github.com/tensorflow/tensorflow/tensorflow/go
# github.com/tensorflow/tensorflow/tensorflow/go [github.com/tensorflow/tensorflow/tensorflow/go.test]
./attrs.go:173:15: type [1073741824]_Ctype_longlong larger than address space
./attrs.go:173:15: type [1073741824]_Ctype_longlong too large
FAIL	github.com/tensorflow/tensorflow/tensorflow/go [build failed]

```

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24448,I run the code in distributed mode and my code run good in asynchronous mode; but the  code run unsuccessful in synchronous mode,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):centos7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below):1.11
- Python version:3.6
- Bazel version (if compiling from source):0.19
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:no GPU


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Traceback (most recent call last):
  File ""/usr/local/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1
292, in _do_call    return fn(*args)
  File ""/usr/local/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1
277, in _run_fn    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/usr/local/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1
367, in _call_tf_sessionrun    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef missing attr 'reduction_type'
 from Op<name=ConditionalAccumulator; signature= -> handle:Ref(string); attr=dtype:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=shape:shape; attr=container:string,default=""""; attr=shared_name:string,default=""""; attr=reduction_type:string,default=""MEAN"",allowed=[""MEAN"", ""SUM""]; is_stateful=true>; NodeDef: {{node sync_replicas/conditional_accumulator}} = ConditionalAccumulator[_class=[""loc:@sync_replicas/SetGlobalStep""], container="""", dtype=DT_FLOAT, shape=[3,3,3,16], shared_name=""conv0/conv:0/grad_accum"", _device=""/job:ps/replica:0/task:0/device:CPU:0""]()
During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""distributed1.py"", line 318, in <module>
    main()
  File ""distributed1.py"", line 217, in main
    with tf.train.MonitoredTrainingSession(master=server.target, is_chief=True,hooks=[sync_replicas
_hook]) as sess:  File ""/usr/local/python3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session
.py"", line 504, in MonitoredTrainingSession    stop_grace_period_secs=stop_grace_period_secs)
  File ""/usr/local/python3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session
.py"", line 921, in __init__    stop_grace_period_secs=stop_grace_period_secs)
  File ""/usr/local/python3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session
.py"", line 643, in __init__    self._sess = _RecoverableSession(self._coordinated_creator)
  File ""/usr/local/python3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session
.py"", line 1107, in __init__    _WrappedSession.__init__(self, self._create_session())
  File ""/usr/local/python3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session
.py"", line 1112, in _create_session    return self._sess_creator.create_session()
  File ""/usr/local/python3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session
.py"", line 807, in create_session    hook.after_create_session(self.tf_sess, self.coord)
  File ""/usr/local/python3/lib/python3.6/site-packages/tensorflow/python/training/sync_replicas_opt
imizer.py"", line 494, in after_create_session    session.run(self._local_init_op)
  File ""/usr/local/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 8
87, in run    run_metadata_ptr)
  File ""/usr/local/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1
110, in _run    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1
286, in _do_run    run_metadata)
  File ""/usr/local/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1
308, in _do_call    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef missing attr 'reduction_type'
 from Op<name=ConditionalAccumulator; signature= -> handle:Ref(string); attr=dtype:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=shape:shape; attr=container:string,default=""""; attr=shared_name:string,default=""""; attr=reduction_type:string,default=""MEAN"",allowed=[""MEAN"", ""SUM""]; is_stateful=true>; NodeDef: {{node sync_replicas/conditional_accumulator}} = ConditionalAccumulator[_class=[""loc:@sync_replicas/SetGlobalStep""], container="""", dtype=DT_FLOAT, shape=[3,3,3,16], shared_name=""conv0/conv:0/grad_accum"", _device=""/job:ps/replica:0/task:0/device:CPU:0""]()"
24447,"Illegal ambiguous match on configurable attribute ""deps"" in //tensorflow/tools/pip_package:included_headers_gather","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: r1.13
- Python version: 2.7.12
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): 0.19.1
- GCC/Compiler version (if compiling from source): gcc 5.4.0
- CUDA/cuDNN version: 9.2 / 7
- GPU model and memory: 4x Titan XP

```
$ bazel clean
INFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.
$ ./configure 
WARNING: Running Bazel server needs to be killed, because the startup options are different.
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.19.1 installed.
Please specify the location of python. [Default is /home/erick/.virtualenvs/p2/bin/python]: 


Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
AttributeError: 'module' object has no attribute 'getsitepackages'
Found possible Python library paths:
  /home/erick/.virtualenvs/p2/lib/python2.7/site-packages
Please input the desired Python library path to use.  Default is [/home/erick/.virtualenvs/p2/lib/python2.7/site-packages]

Do you wish to build TensorFlow with XLA JIT support? [Y/n]: n
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: 9.2


Please specify the location where CUDA 9.2 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 


Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 


Do you wish to build TensorFlow with TensorRT support? [y/N]: n
No TensorRT support will be enabled for TensorFlow.

Please specify the locally installed NCCL version you want to use. [Default is to use https://github.com/nvidia/nccl]: 


Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 6.1]: 


Do you want to use clang as CUDA compiler? [y/N]: y
Clang will be used as CUDA compiler.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: n
Clang will not be downloaded.

Please specify which clang should be used as device and host compiler. [Default is /usr/bin/clang]: 


Do you wish to build TensorFlow with MPI support? [y/N]: n
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=gdr         	# Build with GDR support.
	--config=verbs       	# Build with libverbs support.
	--config=ngraph      	# Build with Intel nGraph support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=noaws       	# Disable AWS S3 filesystem support.
	--config=nogcp       	# Disable GCP support.
	--config=nohdfs      	# Disable HDFS support.
	--config=noignite    	# Disable Apacha Ignite support.
	--config=nokafka     	# Disable Apache Kafka support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished
$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
Starting local Bazel server and connecting to it...
ERROR: /home/erick/dev/tensorflow/tensorflow/tools/pip_package/BUILD:41:1: Illegal ambiguous match on configurable attribute ""deps"" in //tensorflow/tools/pip_package:included_headers_gather:
@local_config_cuda//cuda:using_nvcc
@local_config_cuda//cuda:using_clang
Multiple matches are not allowed unless one is unambiguously more specialized.
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: 

/home/erick/dev/tensorflow/tensorflow/tools/pip_package/BUILD:41:1: Illegal ambiguous match on configurable attribute ""deps"" in //tensorflow/tools/pip_package:included_headers_gather:
@local_config_cuda//cuda:using_nvcc
@local_config_cuda//cuda:using_clang
Multiple matches are not allowed unless one is unambiguously more specialized.
INFO: Elapsed time: 12.939s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (132 packages loaded, 419 targets configured)
    currently loading: tensorflow/core ... (3 packages)
```

"
24446,TOCO failed.Covert saved_model.pb to tflite,"### Describe the problem
I trained a [classification ](https://codelabs.developers.google.com/codelabs/mlimmersion-image-flowerstxf/index.html?index=..%2F..%2Fcloud#0)  model using ML engine.I got the model folder with saved model.pb and variables folder. Now I want to convert the saved_model.pb to tflite.I used the following code.
```
import tensorflow as tf

converter = tf.lite.TFLiteConverter.from_saved_model('/home/ubuntu/*****/model')
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)`
```

It gives the following error 
""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: TOCO failed.""




### Source code / logs
2018-12-19 10:10:33.807941: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-12-19 10:10:33.812470: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400070000 Hz
2018-12-19 10:10:33.812612: I tensorflow/compiler/xla/service/service.cc:161] XLA service 0x3a4e660 executing computations on platform Host. Devices:
2018-12-19 10:10:33.812633: I tensorflow/compiler/xla/service/service.cc:168]   StreamExecutor device (0): <undefined>, <undefined>
WARNING:tensorflow:From /home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/lite/python/convert_saved_model.py:61: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.
WARNING:tensorflow:The saved meta_graph is possibly from an older release:
'model_variables' collection should be of type 'byte_list', but instead is of type 'node_list'.
WARNING:tensorflow:From /home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
WARNING:tensorflow:The saved meta_graph is possibly from an older release:
'model_variables' collection should be of type 'byte_list', but instead is of type 'node_list'.
WARNING:tensorflow:From /home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/lite/python/convert_saved_model.py:275: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.compat.v1.graph_util.convert_variables_to_constants
WARNING:tensorflow:From /home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.compat.v1.graph_util.extract_sub_graph
Traceback (most recent call last):
  File ""export_from_saved.py"", line 4, in <module>
    tflite_model = converter.convert()
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/lite/python/lite.py"", line 455, in convert
    **converter_kwargs)
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/lite/python/convert.py"", line 442, in toco_convert_impl
    input_data.SerializeToString())
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/lite/python/convert.py"", line 205, in toco_convert_protos
    ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
2018-12-19 10:10:40.009636: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayV3
2018-12-19 10:10:40.020516: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2018-12-19 10:10:40.020613: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayScatterV3
2018-12-19 10:10:40.020649: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayV3
2018-12-19 10:10:40.020667: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2018-12-19 10:10:40.020678: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter
2018-12-19 10:10:40.020689: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter
2018-12-19 10:10:40.020710: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter
2018-12-19 10:10:40.020728: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: LoopCond
2018-12-19 10:10:40.020748: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter
2018-12-19 10:10:40.020763: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2018-12-19 10:10:40.020772: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter
2018-12-19 10:10:40.020784: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayReadV3
2018-12-19 10:10:40.020801: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: DecodeJpeg
2018-12-19 10:10:40.020834: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter
2018-12-19 10:10:40.020851: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2018-12-19 10:10:40.020865: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayWriteV3
2018-12-19 10:10:40.020885: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Exit
2018-12-19 10:10:40.020902: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArraySizeV3
2018-12-19 10:10:40.020926: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayGatherV3
2018-12-19 10:10:40.112748: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1302 operators, 1816 arrays (0 quantized)
2018-12-19 10:10:40.176386: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1302 operators, 1816 arrays (0 quantized)
2018-12-19 10:10:40.332638: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 159 operators, 371 arrays (0 quantized)
2018-12-19 10:10:40.337273: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 156 operators, 365 arrays (0 quantized)
2018-12-19 10:10:40.342033: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 156 operators, 365 arrays (0 quantized)
2018-12-19 10:10:40.351531: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 0 bytes, theoretical optimal value: 0 bytes.
2018-12-19 10:10:40.352994: E tensorflow/lite/toco/toco_tooling.cc:421] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, AVERAGE_POOL_2D, CAST, CONCATENATION, CONV_2D, EXPAND_DIMS, FULLY_CONNECTED, LESS, LOGISTIC, MAX_POOL_2D, RANGE, RESHAPE, RESIZE_BILINEAR, SQUEEZE. Here is a list of operators for which you will need custom implementations: DecodeJpeg, Enter, Exit, LoopCond, Merge, Switch, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3.
Traceback (most recent call last):
  File ""/home/ubuntu/.local/bin/toco_from_protos"", line 11, in <module>
    sys.exit(main())
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 59, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 33, in execute
    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, AVERAGE_POOL_2D, CAST, CONCATENATION, CONV_2D, EXPAND_DIMS, FULLY_CONNECTED, LESS, LOGISTIC, MAX_POOL_2D, RANGE, RESHAPE, RESIZE_BILINEAR, SQUEEZE. Here is a list of operators for which you will need custom implementations: DecodeJpeg, Enter, Exit, LoopCond, Merge, Switch, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3."
24445,Bad accuracy on ML Kit with InceptionV3 tflite model,"Hi, I have a problem with InceptionV3 tflite model executed on MLKit SDK. I'm not using the ILSVRC Dataset for reasons of storage space and execution time. I'm using the Caltech Dataset , find it [here](http://www.vision.caltech.edu/Image_Datasets/Caltech101/). I have also remove categories that Inception V3 is not able to recognize. I'm doing a comparison of the new Mobile Machine Learning SDKs, and i try to use the new Snapdragon Neural Processing Engine (SNPE) and MLKit. I ran inceptionV3 on both SDKs, and with SNPE I have reached the 82% of correct classifications, and with MLKit the 47%. For MlKit, I have download the tflite float model from [here](https://www.tensorflow.org/lite/models).
I build the network with the following code:

            long startBuild = SystemClock.elapsedRealtime();
            FirebaseLocalModelSource localSource =
                    new FirebaseLocalModelSource.Builder(""inception_v3"")  // Assign a name for this model
                            .setAssetFilePath(""inception_v3.tflite"")
                            .build();
            FirebaseModelManager.getInstance().registerLocalModelSource(localSource);
            FirebaseModelOptions options = new FirebaseModelOptions.Builder()
                    .setLocalModelName(""inception_v3"")
                    .build();
            FirebaseModelInterpreter firebaseInterpreter =
                    FirebaseModelInterpreter.getInstance(options);
            FirebaseModelInputOutputOptions inputOutputOptions =
                    new FirebaseModelInputOutputOptions.Builder()
                            .setInputFormat(0, FirebaseModelDataType.FLOAT32, new int[]{1, 299, 299, 3})
                            .setOutputFormat(0, FirebaseModelDataType.FLOAT32, new int[]{1, 1001})
                            .build();
            long endBuild = SystemClock.elapsedRealtime();

And then I preprocess image:

            resized_image =  Bitmap.createScaledBitmap(image, 299,299, false);
            input = new float[1][299][299][3];
            for (int x = 0; x < 299; x++) {
                for (int y = 0; y < 299; y++) {
                    int pixel = resized_image.getPixel(x, y);
                    // Normalize channel values to [0.0, 1.0]. This requirement varies by
                    // model. For example, some models might require values to be normalized
                    // to the range [-1.0, 1.0] instead.

                    float b = ((pixel)       & 0xFF);
                    float g = ((pixel >>  8) & 0xFF);
                    float r = ((pixel >> 16) & 0xFF);
                    input[batchNum][x][y][0] = (r - 127) / 128.0f;
                    input[batchNum][x][y][1] = (g - 127) / 128.0f;
                    input[batchNum][x][y][2] = (b - 127) / 128.0f;
                    //input[batchNum][x][y][0] = (Color.red(pixel) - 128) / 128.0f;
                    //input[batchNum][x][y][1] = (Color.green(pixel) - 128) / 128.0f;
                    //input[batchNum][x][y][2] = (Color.blue(pixel) - 128) / 128.0f;
                }
            }

            inputs = new FirebaseModelInputs.Builder()
                    .add(input)  // add() as many input arrays as your model requires
                    .build();



            Task<FirebaseModelOutputs> task = firebaseInterpreter.run(inputs, inputOutputOptions);
"
24444,TypeError: Input 'split_dim' of 'Split' Op has type float32 that does not match expected type of int32.,"Hello,
I am using TF1.4,
Ubuntu16.04
I am implementing https://github.com/zhenkaiwang/im2txt_attention
I encountered during the training phase:
Traceback (most recent call last):
File ""/home/hsu/im2txt_attention-master/bazel-bin/im2txt/train.runfiles/im2txt/im2txt/train.py"", line 114, in <module>
Tf.app.run()
File ""/home/hsu/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 48, in run
_sys.exit(main(_sys.argv[:1] + flags_passthrough))
File ""/home/hsu/im2txt_attention-master/bazel-bin/im2txt/train.runfiles/im2txt/im2txt/train.py"", line 65, in main
Model.build()
File ""/home/hsu/im2txt_attention-master/bazel-bin/im2txt/train.runfiles/im2txt/im2txt/show_and_tell_model.py"", line 550, in build
Self.build_model()
File ""/home/hsu/im2txt_attention-master/bazel-bin/im2txt/train.runfiles/im2txt/im2txt/show_and_tell_model.py"", line 392, in build_model
_, initial_state = lstm_cell(self.image_embeddings, zero_state)
File ""/home/hsu/im2txt_attention-master/bazel-bin/im2txt/train.runfiles/im2txt/im2txt/ops/rnn_cell_ops.py"", line 805, in __call__
Output, new_state, _,_,_ = self._cell(inputs, state, scope)
File ""/home/hsu/im2txt_attention-master/bazel-bin/im2txt/train.runfiles/im2txt/im2txt/ops/rnn_cell_ops.py"", line 435, in __call__
i, j, f, o = array_ops.split(1, 4, concat)
File ""/home/hsu/.local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py"", line 1265, in split
Split_dim=axis, num_split=num_or_size_splits, value=value, name=name)
File ""/home/hsu/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 5094, in _split
Name=name)
File ""/home/hsu/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 533, in _apply_op_helper
(prefix, dtypes.as_dtype(input_arg.type).name))
TypeError: Input 'split_dim' of 'Split' Op has type float32 that does not match expected type of int32.
Then I found some solutions as follows
Original code
State_tuple = tf.split(split_dim=1, num_split=2, value=state_feed)
After modification
State_tuple = tf.split(value=state_feed, num_or_size_splits=2, axis=1

Error code still appears
TypeError: Input 'split_dim' of 'Split' Op has type float32 that does not match expected type of int32.


Can someone help me?
"
24443,"How to export eager model as graph, and how to restore object-based checkpoint which generate by eager mode from estimator","How to export eager model as graph, and how to restore object-based checkpoint which generate by eager mode from estimator"
24442,tf.contrib.copy_graph.copy_variable_to_graph does not set the shape of copied variable,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Archlinux
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.10
- Python version: 3.6
- CUDA/cuDNN version: cuda 9.0, cudnn 7.1
- GPU model and memory: GeForce GTX 1060

**Describe the current behavior**
Variables copied using tf.contrib.copy_graph.copy_variable_to_graph have unknown shape. This make this function unusable and useless. 

**Describe the expected behavior**
Variables copied using tf.contrib.copy_graph.copy_variable_to_graph should have the same shape of original variables in order to make possible restoring from a checkpoint.

**Code to reproduce the issue**
```python
import tensorflow as tf

x = tf.Variable((), name=""x"")

g = tf.get_default_graph()

g2 = tf.Graph()

with g2.as_default():
    x2 = tf.contrib.copy_graph.copy_variable_to_graph(x, g2)

print(x.get_shape()) # (0,)
print(x2.get_shape()) # <unknown>
```

**Fixing**
This issue is simply solved by changing lines: 
https://github.com/tensorflow/tensorflow/blob/a6d8ffae097d0132989ae4688d224121ec6d8f35/tensorflow/contrib/copy_graph/python/util/copy_elements.py#L89-L96

to:
```python
  #Initialize the new variable
  with to_graph.as_default():
    new_var = Variable(
        init_value,
        trainable,
        name=new_name,
        collections=collections,
        validate_shape=True)
```

In my opinion there's no reason to set the field `validate_shape` to False, since in order to create the new variable we need to evaluate the old variable to get the init_value, so we have the actual shape of variables.
"
24441,AttributeError: module 'tensorflow.contrib.saved_model' has no attribute 'saved_keras_model',"**System information**
- TensorFlow version:1.9.0
- Doc Link: https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/save_and_restore_models.ipynb

**Describe the documentation issue**
I followed the notebook and all okies until this line of code:
saved_model_path = tf.contrib.saved_model.save_keras_model(model, ""./saved_models"")
The error is: 
AttributeError: module 'tensorflow.contrib.saved_model' has no attribute 'saved_keras_model'"
24439,TypeError: Can't convert Operation 'MutableHashTable' to Tensor,"

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): No
- TensorFlow version (use command below): 1.8.0
- Python version: 3.6.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


**Describe the current behavior**

meta graph exported by export_meta_graph cannot be imported by import_meta_graph

**Describe the expected behavior**

export_meta_graph and import_meta_graph works.

**Code to reproduce the issue**
```python
import tensorflow as tf
from tensorflow.contrib.lookup.lookup_ops import MutableHashTable
from tensorflow.contrib.lookup.lookup_ops import MutableDenseHashTable

export_dir = 'minimal_saved_model'
#builder = tf.saved_model.builder.SavedModelBuilder(export_dir)

with tf.Session(graph=tf.Graph()) as sess:

    #table = MutableDenseHashTable(key_dtype=tf.int64, \
    # value_dtype=tf.int64, default_value=-1, empty_key=0)
    table = MutableHashTable(key_dtype=tf.int64, value_dtype=tf.int64, default_value=-1)
    aa = tf.get_variable('xxx', shape=(3,))

    meta = tf.train.export_meta_graph()
    saver = tf.train.Saver()


with tf.Session(graph=tf.Graph()) as sess1:
    tf.train.import_meta_graph(meta)
```
**Other info / logs**
related to #11888 . It seems that SaveableObject is not recreated correctly.

stack traces:

```python
  File ""minimal_case.py"", line 22, in <module>
    tf.train.import_meta_graph(meta)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1970, in import_meta_graph
    return Saver()
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1338, in __init__
    self.build()
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1347, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1384, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 813, in _build_internal
    saveables = self._ValidateAndSliceInputs(names_to_saveables)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 661, in _ValidateAndSliceInputs
    names_to_saveables = BaseSaverBuilder.OpListToDict(names_to_saveables)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 629, in OpListToDict
    var = ops.internal_convert_to_tensor(var, as_ref=True)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1104, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 6130, in _operation_conversion_error
    name, as_ref))
TypeError: Can't convert Operation 'MutableHashTable' to Tensor (target dtype=None, name=None, as_ref=True)
```
"
24434,Build from source issue - Bezel test fails,"### System information
- **OS Platform and Distribution: Linux Ubuntu 18.04:
- **TensorFlow version (git cloned from https://github.com/tensorflow/tensorflow (master):
- **Python version 3.6+ (virtual environment created with anaconda 5.3.1:
- **Bazel version 0.20.0:
- **GCC/Compiler version gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
- built-in intelGPU and memory: 8Gb:
- bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/lite/...

 information using environment capture script:

[tf_env.txt](https://github.com/tensorflow/tensorflow/files/2692606/tf_env.txt)

### Description of the problem
When I try to compile TF from source as described in the documentation, bazel test fails.
[tf_build_errors.txt](https://github.com/tensorflow/tensorflow/files/2692548/tf_build_errors.txt)

### Source code / logs
I face the problem just following the documentation step by step up to:
bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/lite/...
"
24431,No TensorFlow-GPU install from pip (seems to be only CPU version),"Hello,
I am coming back to you for help. I am installing a DevBox under Ubuntu 18.04.1 LTS with 4 RTX 2080Ti Nvidia cards.
I have performed multiple Linux/Drivers installation without success. 
There is no GPU use by Keras and fore sure no multi_GPU_model calculation. It seems that despite of the pip install TensorFlow-gpu install, no gpu install is performed. 
At this time, the Nvidia drivers are 410.78 with CUDA 10.0.
The Nvidia-smi function shows me the good data : all the 4 GPUs, Drivers version, Cuda Revision
I am using Anaconda Python 3.6.5.
TensorFlow : 1.12.0 with GPU version
Keras : TensorFlow Backend 2.2.4

Additionally, is there anybody who has experienced starting issue with Ubuntu 18.04.1 LTS and Nvidia driver such as no starting ?
Thank you for your help
Best Regards
Edouard

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem."
24430,tf.nn.depthwise_conv2d issue with float16 => nan output,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary (using docker image tensorflow/tensorflow:1.12.0-gpu)
- TensorFlow version (use command below):  ('v1.12.0-0-ga6d8ffae09', '1.12.0')
- Python version: 2.7.12
- CUDA/cuDNN version: cuda 9.0, cudnn 7.2.1
- GPU model and memory: nvidia Tesla V100-DGXS-16GB

**Describe the current behavior**
tf.nn.depthwise_conv2d_native returns nan when used with float16. Multiple call to same sess.run(op) return different results.

**Describe the expected behavior**
No nan in the output. Similar value when running same op multiple times.

**Code to reproduce the issue**

```
import numpy as np
import tensorflow as tf

with tf.Session() as sess:
    tf_input1 = tf.ones(shape=[1, 22, 28, 28], dtype=tf.float16) / 5.0
    tf_input2 = tf.ones(shape=[7, 7, 22, 1], dtype=tf.float16) / 10.0

    mat = tf.random.uniform( [4096,4096], minval=0, maxval=None, dtype=tf.float16, seed=0, name=None)
    res_mat = tf.matmul(mat, mat)
    #res_mat = tf.no_op()  # un-comment this line and the issue dissapears

    sess.run(res_mat)
    res = tf.nn.depthwise_conv2d_native(tf_input1, tf_input2, strides=[1, 1, 1, 1], padding=""SAME"", data_format='NCHW')
    v1 = sess.run(res)
    sess.run(res_mat)

    v2 = sess.run(res)
    if np.isnan(v2).any():
       print(""issue: nan detected"")
    np.testing.assert_equal(v1, v2)
```

**Other info / logs**

Issue occurs only on nvidia hardward that support float16 operation (on cpu no issue)
replacing res_mat with tf.no_op() => v1 and v2 are equal without nan
Using float32 => no nan

In my understanding the float16 version of the op does not reinitialized some buffer content.

"
24429,Forced to compile Keras model for inference,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.1
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6.7 |Anaconda, Inc.| (default, Oct 23 2018, 14:01:38)
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a


**Describe the current behavior**
Calling `model.predict()` with a `tf.data.Dataset` as input is only possible when compiling the model first and also providing labels. The `predict` method is supposed to not require compiling before use, but the method `_standardize_user_data` ( called at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training.py#L1113) raises the following exception if model is not compiled, and a (x,y) tuple is returned from the dataset:

> RuntimeError: You must compile a model before training/testing. Use `model.compile(optimizer, loss)`.

If the datset only returns the input tensor(s), x, another exception is raised:

> ValueError: Please provide model inputs as a list or tuple of 2  or 3 elements: (input, target) or (input, target, sample_weights) Received <next_element>

**Describe the expected behavior**
I expect behaviour not to differ from when providing arrays or lists of arrays as input. I should be able to provide a `tf.data.Dataset` as input, that provides a single tensor or a list of tensors.

**Code to reproduce the issue**
```
import tensorflow.keras as keras
import tensorflow.data as data


dataset_values = [1, 2, 3, 4, 5]


def single_element_input_fn():
    return data.Dataset.from_tensor_slices(dataset_values)


def tuple_input_fn():
    dataset = data.Dataset.from_tensor_slices(dataset_values)
    # Spoof some labels in order to satisfy shape requirement
    return dataset.map(lambda x: (x, 0))


model_input = keras.layers.Input(shape=(1, ))
model_output = keras.layers.Activation('tanh')(model_input)
model = keras.models.Model(inputs=model_input, outputs=model_output)

# passing python list of values works
print(model.predict(dataset_values))
try:
    # Raises ValueError
    print(model.predict(single_element_input_fn(), steps=5))
except ValueError as e:
    print(e)
try:
    # Raises RuntimeError
    print(model.predict(tuple_input_fn(), steps=5))
except RuntimeError as e:
    print(e)
```

**Other info / logs**
See code above.
"
24428,Ability to Profile Estimators,"**System information**
- TensorFlow version (you are using): 1.12.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**

There seems to be no straight-forward way to profile an `Estimator`. As an ML practitioner, being able to check the speed of predictions on a given architecture is *vital*.

**Will this change the current api? How?**

Yes. 

I'd imagine the `estimator.RunConfig` object would take an optional reference to a `tensorflow.RunMetadata` object? I'm not an expert on the way profiling works in tf, so any guidance would be appreciated 

**Who will benefit with this feature?**

Any ML practitioner (which is who the Estimator API was written for, as I understand) will benefit by being able to profile the speed and memory requirements of Estimators on different hardware. This will aid them selecting given architecture over another and determining whether use of a particular model is feasible for throughput requirements. 

**Any Other info.**
"
24427,Can Tensorflow restore Graph with gradient flows from checkpoint files?,"
Generally, checkpoint is used for model evaluation or inference, or fault tolerance. However, can checkpoint file be used to restore training processing, that  is, 

- Load checkpoint file, and build forward and backward graph flow based on these checkpoint files.
- We assume that we have no the model graph python code, just only checkpoint files. 

In general, we can restore forward flow immediately with inference engine, even with other  inference engine. But we wish to restore training graph with gradient flow. Does it  works? Is it big project or little   code hacking, or already APIs provided? 

Thanks a lot. 


"
24426,where  'tf.image.resize_bilinear' is defined ?,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version:v1.9
- Doc Link:
https://tensorflow.google.cn/api_docs/python/tf/image/resize_bilinear

**Describe the documentation issue**
i want to see the 'tf.image.resize_bilinear' source code.
but i can not find 'tensorflow/python/ops/gen_image_ops.py'.

"
24425,tf.nn.softmax_cross_entropy_with_logits_v2 changes tensor shape,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12.0
- Python version: 3.5
- CUDA/cuDNN version: 9.0 / 7.4.1
- GPU model and memory: 1070 Ti / 8GB

I have a simple network that outputs logits via a dense layer in the model_fn (using estimator API):
```
logits = tf.layers.dense(inputs=dense1, units=2, activation=tf.nn.relu, name='logits')
```

I then calculate the loss for the training with:
```
if mode in (tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL):
  print(logits.shape)
  print_op = tf.print(""LABELS: "", logits, tf.shape(logits))
  with tf.control_dependencies([print_op]):
    logits = tf.identity(logits)
  loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logits)
  tf.summary.scalar('cross_entropy', loss)
```

Which results in an error, as the tensor shapes appear not to match:
```bash
Caused by op 'Reshape_3', defined at:
  File ""train.py"", line 40, in <module>
    fire.Fire(train)
  File ""/usr/local/lib/python3.5/dist-packages/fire/core.py"", line 127, in Fire
    component_trace = _Fire(component, args, context, name)
  File ""/usr/local/lib/python3.5/dist-packages/fire/core.py"", line 366, in _Fire
    component, remaining_args)
  File ""/usr/local/lib/python3.5/dist-packages/fire/core.py"", line 542, in _CallCallable
    result = fn(*varargs, **kwargs)
  File ""train.py"", line 36, in train
    learning.train(model, input, model_name, exp_name, config_file, exp_restore)
  File ""/data/code/bp-labeling/learning/learning/train.py"", line 93, in train
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py"", line 471, in train_and_evaluate
    return executor.run()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py"", line 610, in run
    return self.run_local()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py"", line 711, in run_local
    saving_listeners=saving_listeners)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 1207, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 1237, in _train_model_default
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 1195, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/data/code/bp-labeling/learning/learning/model.py"", line 86, in model_fn
    return self.classifier_fn(features, images, labels, mode, logits, params, cls_count, global_step)
  File ""/data/code/bp-labeling/learning/models/sample_net.py"", line 83, in classifier_fn
    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op, training_hooks=[hook])
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/model_fn.py"", line 201, in __new__
    loss = array_ops.reshape(loss, [])
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 6482, in reshape
    ""Reshape"", tensor=tensor, shape=shape, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 3274, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 1770, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): Input to reshape is a tensor with 10 values, but the requested shape has 1
	 [[node Reshape_3 (defined at /data/code/bp-labeling/learning/models/sample_net.py:83)  = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](softmax_cross_entropy_with_logits/Reshape_2, Reshape_3/shape)]]
	 [[{{node Reshape_3/_397}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_124_Reshape_3"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
```

The `print` function is only called once with the following output:
```bash
LABELS:  [[0.255024195 0.00465549901]
 [0.550343394 0]
 [0.0432553887 0]
 ...
 [0.0975153148 0]
 [0 0]
 [0 0]] [1 2]
```
(output from `print(logits.shape)` is `(?, 2)`).

Which appears strange, as the shape is given as `[1, 2]`, which is obviously not the case.

If I change the loss function to:
```
loss = tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)
```

the code runs fine and the output of the print command is e.g.:
```bash
LABELS:  [[0 0]
 [0 20.4618092]
 [0 0]
 ...
 [0 1.23361325]
 [0 0]
 [0 0]] [10 2]
```

Am I missing something or is this a bug?"
24424,tf.contrib.ffmpeg: No mention of alternatives after deprecation,"**System information**
- TensorFlow version: 1.12
- Doc Link: https://www.tensorflow.org/api_docs/python/tf/contrib/ffmpeg/decode_audio

**Describe the documentation issue**
Is there a plan to replace `tf.contrib.ffmpeg`? I currently rely on it in a tf.data pipeline and didn't think it was clear from the docs if there is or is not a plan on replacing the functionality. I would appreciate some clarification in the docs on that matter.

E.g. the [Simple Audio Recognition](https://www.tensorflow.org/tutorials/sequences/audio_recognition) seems to use [audio_ops](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/framework/python/ops/audio_ops.py) which AFAIK is completely missing from the docs. Is this perhaps meant to replace (some of) the functionality of `tf.contrib.ffmpeg`, or is it expected that all of the audio decoding functionality will be phased out from tensorflow?

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
Yes, if clear update recommendations are provided."
24423,Difference in performance of Python and cpp libraries:,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.8
- Python version:3.5.0
- Bazel version (if compiling from source):1.19
- GCC/Compiler version (if compiling from source):msvc 2015
- CUDA/cuDNN version:No
- GPU model and memory:No


Issue:
The issue is related to the performance of tensorflow on Cpp and python
I have installed tensorflow(1.8) on both windows and python (binary)
I am using tensorflow to predict the score of any image using a model as .pb files
But the time taken by tensorflow graph function in python is about 50ms and 
that of cpp is about 500ms.
I had assumed that cpp would be more faster but the results were opposite.
What can i do to increase the performace of the tensorflow in winodows.
I am using the pre-built binary in windows build using cmake release mode.
"
24421,Retrain Model & Serv it,"Dear Forum Users,

i was creating a Model with retrain.py on Ubuntu 18.04 in a virtual Machine on my Server (https://www.tensorflow.org/hub/tutorials/image_retraining)
Tensorflow is installed via pip installtion. I'm only using CPU.

after testing it with the python label_image.py it works everthing fine and the detection works great!

so i try'd to serve my own model with the tensorflow-serving (https://github.com/tensorflow/serving.git).

on the github serving-example the execution of the model ""half_plus_two"" works also pretty well and i was able to connect via the RESTful API.

then i try'd to host my own model (out of the /tmp folder from the retrain.py)
i copy'd/renamed the files from the /tmp folders to the structure of the ""half plus two"" model, like in the example 
""/saved_model_half_plus_two_cpu/"" and ""/half_plus_two/"".

when i try to start the serving via the following cmd:
docker run -t --rm -p 8561:8561 \
   -v ""$TESTDATA/saved_model_half_plus_two_cpu:/models/half_plus_two"" \
   -e MODEL_NAME=half_plus_two \
   tensorflow/serving &

the application says: ""No versions of servable half_plus_two found under base path /models/half_plus_two""
could anyone help?

thank you and regards

"
24420,TF doesn't build for Raspberry Pi,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux 4.15 x86_64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source, building ci_build.sh from TF distro
- TensorFlow version: latest from master
- Python version: tried both 2.7 and 3
- Installed using virtualenv? pip? conda?: 
- Bazel version (if compiling from source): tried both 0.20.0 and 0.19.2
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

When following instructions at https://www.tensorflow.org/install/source_rpi to cross-compile TF for RPi, the build fails, see log below.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. clone TF source
2. `tensorflow/tools/ci_build/ci_build.sh PI  tensorflow/tools/ci_build/pi/build_raspberry_pi.sh`
3. see the error below

I found following issue but downgrading bazel to 0.19.2 didn't help: https://github.com/tensorflow/tensorflow/issues/24124

Another issue: from build log I see that armv6 was used to build BLAS, but I would expect armv7 instead for RPi 3, and tensorflow/tools/ci_build/pi/build_raspberry_pi.sh should use it. What's going on?

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
 OpenBLAS build complete. (BLAS CBLAS)

  OS               ... Linux             
  Architecture     ... arm               
  BINARY           ... 32bit                 
  C compiler       ... GCC  (command line : /tmp/toolchain_install//tools/arm-bcm2708/arm-rpi-4.9.3-linux-gnueabihf/bin/arm-linux-gnueabihf-gcc)
  Library Name     ... libopenblas_armv6p-r0.3.1.dev.a (Multi threaded; Max num-threads is 4)

To install the library, you can run ""make PREFIX=/path/to/your/installation install"".

make -j 4 -f Makefile.install install
make[1]: Entering directory `/tmp/openblas_src'
Generating openblas_config.h in /tmp/openblas_install//include
Generating f77blas.h in /tmp/openblas_install//include
Generating cblas.h in /tmp/openblas_install//include
Copying the static library to /tmp/openblas_install//lib
Copying the shared library to /tmp/openblas_install//lib
Generating openblas.pc in /tmp/openblas_install//lib/pkgconfig
Generating OpenBLASConfig.cmake in /tmp/openblas_install//lib/cmake/openblas
Generating OpenBLASConfigVersion.cmake in /tmp/openblas_install//lib/cmake/openblas
Install OK!
make[1]: Leaving directory `/tmp/openblas_src'
Building for the Pi Two/Three, with NEON acceleration
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
INFO: Invocation ID: f1dbc59c-324c-431f-b0eb-408f1a81c125
Loading: 
Loading: 0 packages loaded
Loading: 0 packages loaded
ERROR: cc_toolchain_suite '@local_config_arm_compiler//:toolchain' does not contain a toolchain for CPU 'armeabi', you may want to add an entry for 'armeabi|compiler' into toolchains and toolchain_identifier 'arm-linux-gnueabihf' into the corresponding cc_toolchain rule (see --incompatible_disable_cc_toolchain_label_from_crosstool_proto).
```

```
$ docker --version
Docker version 18.04.0-ce, build 3d479c0af6
```"
24419,Import Custom C++ Op Error: dynamic module does not define module export function,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.4 LTS server
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.12.0
- Python version: 3.6.5
- Installed using virtualenv? pip? conda?: pip3
- GPU model and memory: GTX 1080 12G


**Describe the problem**

I write a custom op by C++ which can work by `tf.load_op_library`. But furthermore, I want to import the function encapsulating the op, which results in `ImportError: dynamic module does not define module export function`. So, how can I import the python function or module which depends on the custom Op?
Thank you very much.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Custom Op C++ file `remap.cc`:
```
#include <unordered_map>
#include ""tensorflow/core/framework/op.h""
#include ""tensorflow/core/framework/op_kernel.h""
#include ""tensorflow/core/framework/shape_inference.h""

using namespace tensorflow;

REGISTER_OP(""Remap"")
.Input(""input: int32"")
.Input(""base: int32"")
.Output(""output: int32"")
.SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
  ::tensorflow::shape_inference::ShapeHandle input;
  TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &input));
  c->set_output(0, c->input(0));
  return Status::OK();
});


class RemapOp : public OpKernel {
public:
  explicit RemapOp(OpKernelConstruction* context) : OpKernel(context) {}

  void Compute(OpKernelContext* context) override {
    // Grab the input tensor
    const Tensor& input_tensor = context->input(0);
    auto input = input_tensor.flat<int32>();
    const Tensor& base_tensor = context->input(1);
    auto base = base_tensor.flat<int32>();

    // Build hash table
    std::unordered_map<int32, int32> map;
    const int base_N = base.size();
    for (int i = 0; i < base_N; i++) {
      map[base(i)] = i;
    }

    // Create an output tensor
    Tensor* output_tensor = NULL;
    OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),
          &output_tensor));
    auto output_flat = output_tensor->flat<int32>();

    // Set all elements of output tensor
    const int N = input.size();
    for (int i = 0; i < N; i++) {
      output_flat(i) = map.at(input(i));
    }
  }
};
```

makefile:
```
TF_CFLAGS = $(shell python -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_compile_flags()))')
TF_LFLAGS = $(shell python -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_link_flags()))')

all:
  g++ -std=c++11 -shared remap.cc -o remap.so -fPIC ${TF_CFLAGS} ${TF_LFLAGS} -O2
```

Python function encapsulating the Op `remap.py`:
```
import tensorflow as tf

remap_module = tf.load_op_library('./remap.so')
remap = remap_module.remap

def foo(a):
  ret = remap(a)
  # Do something about ret
  return ret
```

Python `main.py`
```
import tensorflow as tf
from remap import foo

print(foo([1, 2, 3], [1, 2, 3, 4, 5]))
```

Compile the Op by makefile and then run `python main.py`, then we have the error:
```
Traceback (most recent call last):
  File ""main.py"", line 2, in <module>
    from remap import foo
ImportError: dynamic module does not define module export function (PyInit_remap)
```"
24417,Tensorflow Lite Dockerfile Error,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

Tensorflow is being installed from the Dockerfile according to https://github.com/tensorflow/models/blob/master/research/object_detection/dockerfiles/android/README.md. I am using CPU - NO GPU. I use Ubuntu 16.04.

**Describe the problem**
When I run ""docker build --tag detect-tf ."", I get the below error in the terminal. How can I fix it?

Step 3/21 : RUN export CLOUD_SDK_REPO=""cloud-sdk-$(lsb_release -c -s)"" &&     echo ""deb http://packages.cloud.google.com/apt $CLOUD_SDK_REPO main"" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list &&     curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - &&     apt-get update -y && apt-get install google-cloud-sdk -y
 ---> Running in e985c9945ecd
deb http://packages.cloud.google.com/apt cloud-sdk-bionic main
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Warning: apt-key output should not be parsed (stdout is not a terminal)
100  1326  100  1326    0     0   5715      0 --:--:-- --:--:-- --:--:--  5715
gpg: failed to start agent '/usr/bin/gpg-agent': No such file or directory
gpg: can't connect to the agent: No such file or directory
gpg: failed to start agent '/usr/bin/gpg-agent': No such file or directory
gpg: can't connect to the agent: No such file or directory
The command '/bin/sh -c export CLOUD_SDK_REPO=""cloud-sdk-$(lsb_release -c -s)"" &&     echo ""deb http://packages.cloud.google.com/apt $CLOUD_SDK_REPO main"" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list &&     curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - &&     apt-get update -y && apt-get install google-cloud-sdk -y' returned a non-zero code: 2

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I followed the steps from https://github.com/tensorflow/models/blob/master/research/object_detection/dockerfiles/android/README.md.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24414,ppc64le: no_mkl_dnn_contraction_kernel define causes build failure,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 ppc64le
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: master
- Python version: 3.6
- Installed using virtualenv? pip? conda?:  source
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source):  4.8
- CUDA/cuDNN version:  10.0/7.3
- GPU model and memory: V100



**Describe the problem**
A recent commit (https://github.com/tensorflow/tensorflow/commit/10ef7edc881ee715eaae48656fcb431fe128441f) changed the default contraction kernel to be MKL based. Code was added for non-intel platforms to avoid MKL, but it has an error.


**Provide the exact sequence of commands / steps that you executed before running into the problem**

Basic build: `bazel build //tensorflow/tools/pip_package:build_pip_package`

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
ERROR: /home/jenkins/workspace/TensorFlow_PPC64LE_GPU_Build/tensorflow/core/kernels/BUILD:610:12: Illegal ambiguous match on configurable attribute ""deps"" in //tensorflow/core/kernels:eigen_contraction_kernel:
//tensorflow:linux_ppc64le
//tensorflow/core/kernels:no_mkldnn_contraction_kernel
Multiple matches are not allowed unless one is unambiguously more specialized.
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: 

/home/jenkins/workspace/TensorFlow_PPC64LE_GPU_Build/tensorflow/core/kernels/BUILD:610:12: Illegal ambiguous match on configurable attribute ""deps"" in //tensorflow/core/kernels:eigen_contraction_kernel:
//tensorflow:linux_ppc64le
//tensorflow/core/kernels:no_mkldnn_contraction_kernel
Multiple matches are not allowed unless one is unambiguously more specialized.
```

There ends up being multiple matches in the select call:
```
defines = select({
        ""//tensorflow:android"": [],
        ""//tensorflow:arm"": [],
        ""//tensorflow:ios"": [],
        ""//tensorflow:linux_ppc64le"": [],
        "":no_mkldnn_contraction_kernel"": [],
        ""//conditions:default"": [
            ""TENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL"",
            ""TENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL"",
        ],
    }),
```
Either the arch-specific entries in the `select` call should be removed, or the `no_mkldnn_contraction_kernel` entry should be removed for this to work. 

I'll follow up with a proposed patch set.

"
24412,TPU support is incomplete,"TensorFlow version (use command below): **2.0 preview**

TPU support is work in progress, and the 2.0 preview does not yet contain a DistributionStrategy for TPU.

This is a tracking issue which will be updated when progress is made on this issue."
24410,Euclidean Distance Transform ,"Given the importance and wide use of the distance transform of binary images in different computer vision applications, I wonder if it would be possible to add a Euclidean distance transform with gradient support. 

There exists very nice O(n) solutions already implemented in C/C++. For example. Opencv has implemented this [paper](https://www.realestatetrading.com/storage/documents/Borgefors86__Work.pdf) [here](https://docs.opencv.org/4.0.0/d7/d1b/group__imgproc__misc.html#ga8a0b7fdfcb7a13dde018988ba3a43042). 

Here's another fast [implementation](https://diplib.github.io/diplib-docs/group__distance.html#ga5feb444b79b00710cdd5b371d55fd221) in PyDIP. 

This would be a tremendously useful feature. Thanks! "
24409,Partially-defined shapes and group_norm,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): `v1.12.0-0-ga6d8ffae09 1.12.0`
- Python version: 3.6.5
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 9.0
- GPU model and memory: 1080 Ti

**Describe the current behavior**
Given the following minimal code raises an error:
```python
scan_var = tf.placeholder(tf.float32, [None, 128, 128, 64], name='scan')
norm = tf.contrib.layers.group_norm(scan_var, groups=16, channels_axis=3, reduction_axes=(1, 2))
```
When using `tf.contrib.layers.group_norm` with a Tensor that has partially-defined shape (e.g. `[None, H, W, C]`), the following error is raised:
```
TypeError: Failed to convert object of type <class 'list'> to Tensor. Contents: [None, 128, 128, 16, 4]. Consider casting elements to a supported type.
```
The error is due to [normalization.py#L303](https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/contrib/layers/python/layers/normalization.py#L303), since the value of variable `inputs_shape` is `[None, 128, 128, 16, 4]` which cannot be converted to a Tensor. The correct value should be `[-1, 128, 128, 16, 4]`.
This error can be fixed (probably addressing this [TODO](https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/contrib/layers/python/layers/normalization.py#L250)) with the addition of the two following lines after Line 252 in `normalization.py`:
```python
original_shape = [-1 if s.value is None else s for s in original_shape]
if len([s for s in original_shape if s == -1]) > 1:
    raise ValueError('Only one axis dimension can be undefined in the input tensor')
```
and the following line after Line 302:
```python
inputs_shape = [-1 if s is None else s for s in inputs_shape]
```

If it seems reasonable I can submit a pull request."
24408,CheckpointInputPipelineHook not restoring the correct iterator state,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOs / Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.11 / 1.12
- Python version: 3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


**Describe the current behavior**
1. CheckpointInputPipelineHook is not restoring the input checkpoint if there is multiple ""estimator.train"" method calls. Probably because the restore of the input checkpoint is done during the init of the hook. It restarts from the beginning of the dataset.
2. If both MirroredStrategy and CheckpointInputPipelineHook are used, one step is by passing when the input checkpoint is restored. It restarts from (state step + 1).

**Describe the expected behavior**
After restoring the input checkpoint, the train restart at the right dataset iterator step.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
In this sample code, I have one dataset (range between 0 and 7, batch of 1, 1 epoch), I train the model for 4 steps, stop it, restart for 4 steps the training by restoring the checkpoints (model and input).
Expected:
Train First Run: 4 steps -> features 0 to 3
Save checkpoint model-ckpt and input-ckpt 4
Restore model-ckpt and input-ckpt 4
Train Second Run: 4 steps -> features 4 to 7
Save checkpoint 8
```
import tensorflow as tf

sample_count = 8


def run(model_dir: str, reinit_hook: bool = False, distribution_strategy: bool = False, gpu_count: int = 1):
    # Config
    if distribution_strategy:
        session_config = tf.ConfigProto(allow_soft_placement=True)
        train_distribute = tf.contrib.distribute.MirroredStrategy(num_gpus=gpu_count)
    else:
        session_config = None
        train_distribute = None

    run_config = tf.estimator.RunConfig(
        model_dir=model_dir,
        session_config=session_config,
        train_distribute=train_distribute)

    # Estimator
    estimator = tf.estimator.Estimator(model_fn=model_fn, config=run_config)

    # Hook
    hooks = [tf.contrib.data.CheckpointInputPipelineHook(estimator)]

    # First Run 1/2
    estimator.train(input_fn=train_input_fn,
                    steps=sample_count // 2 // gpu_count,
                    hooks=hooks)

    # Reinit Hook
    if reinit_hook:
        hooks = [tf.contrib.data.CheckpointInputPipelineHook(estimator)]

    # Second Run 1/2
    estimator.train(input_fn=train_input_fn,
                    steps=sample_count // 2 // gpu_count,
                    hooks=hooks)


def train_input_fn():
    return tf.data.Dataset \
        .range(sample_count) \
        .map(lambda x: (x, x * 2)) \
        .batch(1) \
        .repeat(1)


def model_fn(features, labels, mode):
    input_layer = tf.cast(tf.reshape(features, [-1, 1]), tf.float32)
    expected_output = tf.cast(tf.reshape(labels, [-1, 1]), tf.float32)

    logit = tf.layers.dense(input_layer, 1, None, False)
    loss = tf.losses.mean_squared_error(expected_output, logit)

    logging_hook = tf.train.LoggingTensorHook(tensors={""feature_value"": features.name}, every_n_iter=1)

    if mode == tf.estimator.ModeKeys.TRAIN:
        optimizer = tf.train.AdamOptimizer(0.001)
        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())
        return tf.estimator.EstimatorSpec(mode=mode,
                                          loss=loss,
                                          train_op=train_op,
                                          training_hooks=[logging_hook])


if __name__ == '__main__':
    tf.logging.set_verbosity(tf.logging.DEBUG)

    # Expected:
    # Train First Run: 4 steps -> features 0 to 3
    # Save checkpoint model-ckpt and input-ckpt 4
    # Restore model-ckpt and input-ckpt 4
    # Train Second Run: 4 steps -> features 4 to 7
    # Save checkpoint 8

    # run_1: 1 gpu / no distribution strategy / Only one init of the hook
    # Actual KO: dataset restarts from 0
    # 
    # Train First Run: 4 steps -> features 0 to 3
    # Save checkpoint model-ckpt and input-ckpt 4
    # Restore model-ckpt 4
    # Train Second Run: 4 steps -> features 0 to 3
    # Save checkpoint 8
    run(model_dir='output_run_1', reinit_hook=False, distribution_strategy=False)

    # run_2: 1 gpu / no distribution strategy / reinit of the hook between two trainings
    # Actual OK
    # Train First Run: 4 steps -> features 0 to 3
    # Save checkpoint model-ckpt and input-ckpt 4
    # Restore model-ckpt and input-ckpt 4
    # Train Second Run: 4 steps -> features 4 to 7
    # Save checkpoint 8
    run(model_dir='output_run_2', reinit_hook=True, distribution_strategy=False)

    # run_3:
    # Actual KO: dataset restarts from iterator state step + 1
    # Train First Run: 4 steps -> features 0 to 3
    # Save checkpoint model-ckpt and input-ckpt 4
    # Restore model-ckpt and input-ckpt 4
    # Train Second Run: 3 steps -> features 5 to 7
    # Save checkpoint 7
    run(model_dir='output_run_3', reinit_hook=True, distribution_strategy=True)

    # run_4:
    # Actual KO: dataset restarts from iterator state step + 1
    # Train First Run: 2 steps -> features 0 to 3
    # Save checkpoint model-ckpt and input-ckpt 2
    # Restore model-ckpt and input-ckpt 2
    # Train Second Run: 1 steps -> features 6 to 7
    # Save checkpoint 3
    run(model_dir='output_run_4', reinit_hook=True, distribution_strategy=True, gpu_count=2)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24407,[TFLite] Feature request: Add support for CropAndResize op,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): Source
- TensorFlow version (or github SHA if from source): ebe98d94a5340386429da4b6e930b8cda9c7e854
- Are you willing to contribute it (Yes/No): Yes, if feasible / not under current development

```
2018-12-17 15:14:34.782227: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 4345 operators, 7950 arrays (0 quantized)
2018-12-17 15:14:35.080813: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 4054 operators, 7480 arrays (0 quantized)
2018-12-17 15:14:35.472064: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 4054 operators, 7480 arrays (0 quantized)
2018-12-17 15:14:35.713793: F tensorflow/lite/toco/graph_transformations/resolve_constant_slice.cc:59] Check failed: dim_size >= 1 (0 vs. 1)
Aborted (core dumped)
```

Link to a GraphDef or the model: http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet101_coco_2018_01_28.tar.gz

#### Summary
Currently CropAndResize is not supported in the TFLite runtime, and because of this Faster R-CNN object detection models can not be deployed on mobile. Faster R-CNN has been [shown to outperform SSD models w.r.t accuracy](https://arxiv.org/pdf/1611.10012.pdf), and as compute speed increases it seems intuitive to support this architecture.

My team and I would be interested in adding this functionality, but wanted to make sure that it's feasible and not under current development. Very much related is tensorflow/models/issues/4848, where it was said that there is no fundamental issue with implementing it. "
24406,Tensorflow GPU installation in Windows 10 with anaconda,"**System information**
- **OS Platform and Distribution**: Windows 10
- **TensorFlow installed from (source or binary)**: I don't know
- **TensorFlow version**: 1.12.0
- **Python version**:  3.6.7 (v3.6.7:6ec5cf24b7, Oct 20 2018, 13:35:33) [MSC v.1900 64 bit (AMD64)]
- **Installed using virtualenv? pip? conda?**: I don't remember
- **Bazel version (if compiling from source)**: I don't know
- **GCC/Compiler version (if compiling from source)**: I don't know
- **CUDA/cuDNN version**: Cuda compilation tools, release 10.0, V10.0.130 / cuDNN v. 7.4.1
- **GPU model and memory**:  GeForce GT 635M   

**Describe the problem**

I can't install the GPU version of Tensorflow

**Provide the exact sequence of commands / steps that you executed before running into the problem**

I try to install the GPU version of Tensorflow
So I run the following lines in a Windows command prompt

```
conda create --name tf_gpu
activate tf_gpu
conda install tensorflow-gpu
```
After running `conda create --name tf_gpu`, I got the following message 

> Preparing transaction: done
> Verifying transaction: \
> SafetyError: The package for setuptools located at C:\Users\User\Miniconda4\pkgs\setuptools-40.6.2-py36_0
> appears to be corrupted. The path Scripts/easy_install.exe
> has a sha256 mismatch.
>  reported sha256: 993203a406e04936a07829b1f482fd27d739b640482e213f4c49ea1ee78a5fcf
>  actual sha256: dd0f7f55b91f4c77c6064f6ed3219e3573ae2b4ada1190094887da40d6b7fde1
> SafetyError: The package for wheel located at C:\Users\User\Miniconda4\pkgs\wheel-0.32.3-py36_0
> appears to be corrupted. The path Scripts/wheel.exe
> has a sha256 mismatch.
>  reported sha256: 993203a406e04936a07829b1f482fd27d739b640482e213f4c49ea1ee78a5fcf
>  actual sha256: e94002e198d87676ccfef870c0648d33d57bd30288ba00dbab112597e223759d
> done
> Executing transaction: done

But I think, that is a minor problem. However, when I run `conda install tensorflow-gpu` I get this 

> 20181216 16:15:23.825085: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX
> 20181216 16:15:24.615875: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] **failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected**
> 20181216 16:15:24.623130: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] retrieving CUDA diagnostic information for host: DESKTOP-85SA7VT
> 20181216 16:15:24.627495: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:170] hostname: DESKTOP-85SA7VT
> Device mapping: no known devices.
> 20181216 16:15:24.645904: I tensorflow/core/common_runtime/direct_session.cc:307] Device mapping:
> .
> 

Is there something wrong with the CUDA/cuDNN installation ?

Tensorflow with CPU does work but not GPU Tensorflow. Thanks"
24405,Improper behaviour of recurrent batch normalization with bidirectional rnn,"I have already asked this question on [stackoverflow](https://stackoverflow.com/questions/53812656/recurrent-batch-normalization-problem-to-get-the-update-op-properly) and I got no answer so that is why I'm posting it here.


My recurrent batch normalization class is as following:
```python
class BNLSTMCell(RNNCell):



def __init__(
    self,
    num_units,
    is_training=True,
    use_peepholes=False,
    cell_clip=None,
    initializer=None,
    num_proj=None,
    proj_clip=None,
    forget_bias=1.0,
    state_is_tuple=True,
    activation=tf.tanh,
    reuse=None,
    ):


    super(BNLSTMCell, self).__init__(_reuse=reuse)
    if not state_is_tuple:
        tf.logging.log_first_n(tf.logging.WARN,
                               '%s: Using a concatenated state is slower and  will soon be deprecated.  Use state_is_tuple=True.'
                               , 1, self)

    self.num_units = num_units
    self.is_training = is_training
    self.use_peepholes = use_peepholes
    self.cell_clip = cell_clip
    self.num_proj = num_proj
    self.proj_clip = proj_clip
    self.initializer = initializer
    self.forget_bias = forget_bias
    self.state_is_tuple = state_is_tuple
    self.activation = activation

    if num_proj:
        self._state_size = (LSTMStateTuple(num_units,
                            num_proj) if state_is_tuple else num_units
                            + num_proj)
        self._output_size = num_proj
    else:
        self._state_size = (LSTMStateTuple(num_units,
                            num_units) if state_is_tuple else 2
                            * num_units)
        self._output_size = num_units

@property
def state_size(self):
    return self._state_size

@property
def output_size(self):
    return self._output_size

def call(self, inputs, state):


    num_proj = (self.num_units if self.num_proj
                is None else self.num_proj)

    if self.state_is_tuple:
        (c_prev, h_prev) = state
    else:
        c_prev = tf.slice(state, [0, 0], [-1, self.num_units])
        h_prev = tf.slice(state, [0, self.num_units], [-1,
                          num_proj])

    dtype = inputs.dtype
    input_size = inputs.get_shape().with_rank(2)[1]

    if input_size.value is None:
        raise ValueError('Could not infer input size from inputs.get_shape()[-1]'
                         )
    scope = tf.get_variable_scope()
    with tf.variable_scope(scope or type(self).__name__):

        W_xh = tf.get_variable('input_kernel', [input_size, 4
                               * self.num_units],
                               initializer=orthogonal_initializer())
        W_hh = tf.get_variable('state_kernel', [num_proj, 4
                               * self.num_units],
                               initializer=bn_lstm_identity_initializer(0.95))

        xh = tf.matmul(inputs, W_xh)
        hh = tf.matmul(h_prev, W_hh)

        bn_xh = batch_norm(xh, 'input', self.is_training)
        bn_hh = batch_norm(hh, 'state', self.is_training)

        bias = tf.get_variable('bias', [4 * self.num_units])

  # i:input gate, j:new input, f:forget gate, o:output gate

        lstm_matrix = tf.nn.bias_add(tf.add(bn_xh, bn_hh), bias)
        (i, j, f, o) = tf.split(value=lstm_matrix,
                                num_or_size_splits=4, axis=1)

  # Diagonal connections

        if self.use_peepholes:
            w_f_diag = tf.get_variable('W_F_diag',
                    shape=[self.num_units], dtype=dtype)
            w_i_diag = tf.get_variable('W_I_diag',
                    shape=[self.num_units], dtype=dtype)
            w_o_diag = tf.get_variable('W_O_diag',
                    shape=[self.num_units], dtype=dtype)

        if self.use_peepholes:
            c = c_prev * tf.sigmoid(f + self.forget_bias + w_f_diag
                    * c_prev) + tf.sigmoid(i + w_i_diag * c_prev) \
                * self.activation(j)
        else:
            c = c_prev * tf.sigmoid(f + self.forget_bias) \
                + tf.sigmoid(i) * self.activation(j)

        if self.cell_clip is not None:
            c = tf.clip_by_value(c, -self.cell_clip, self.cell_clip)

        bn_c = batch_norm(c, 'cell', self.is_training)

        if self.use_peepholes:
            h = tf.sigmoid(o + w_o_diag * c) * self.activation(bn_c)
        else:
            h = tf.sigmoid(o) * self.activation(bn_c)

        if self.num_proj is not None:
            w_proj = tf.get_variable('projection/kernel',
                    [self.num_units, num_proj], dtype=dtype)

            h = tf.matmul(h, w_proj)
            if self.proj_clip is not None:
                h = tf.clip_by_value(h, -self.proj_clip,
                        self.proj_clip)

    new_state = (LSTMStateTuple(c,
                 h) if self.state_is_tuple else tf.concat(values=[c,
                 h], axis=1))
    return (h, new_state)


```

and my batch normalization function is as following 
```python
def batch_norm(x, name_scope, is_training):
with tf.variable_scope(name_scope):
    return tf.layers.batch_normalization(inputs=x,training=is_training,fused=True)
```

In my trainer.py file I have a function that handles the update_op in which we take care of the moving_mean and moving_variance of the batch_normalization.

```python
def _update(self, loss, learning_rate, cluster):
    '''
    create the op to update the model

    args:
        loss: the loss to minimize
        learning_rate: the learning rate
        cluster: the tf cluster

    returns: the update op
    '''

    #create the optimizer
    optimizer = tf.train.AdamOptimizer(learning_rate)

    #create an optimizer that aggregates gradients
    if int(self.conf['numbatches_to_aggregate']) > 0:
        if 'local' in cluster.as_dict():
            num_workers = 1
        else:
            num_workers = len(cluster.as_dict()['worker'])

        optimizer = tf.train.SyncReplicasOptimizer(
            opt=optimizer,
            replicas_to_aggregate=int(
                self.conf['numbatches_to_aggregate']),
            total_num_replicas=num_workers)


    tf.summary.scalar('training_loss', loss,
                      collections=['training_summaries'])

    #get the list of trainable variables
    trainable = tf.trainable_variables()

    #get the list of variables to be removed from the trainable
    #variables
    untrainable = tf.get_collection('untrainable')

    #remove the variables
    trainable = [var for var in trainable
                 if var not in untrainable]

    #compute the gradients
    grads_and_vars = optimizer.compute_gradients(
        loss=loss,
        var_list=trainable)

    with tf.variable_scope('clip'):
        #clip the gradients
        grads_and_vars = [(tf.clip_by_value(grad, -1., 1.), var)
                          for grad, var in grads_and_vars]


    #opperation to apply the gradients
    apply_gradients_op = optimizer.apply_gradients(
        grads_and_vars=grads_and_vars,
        name='apply_gradients')

    #all remaining operations with the UPDATE_OPS GraphKeys
    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
    print(""update_ops {}"".format(update_ops))
    print(""################"")
    #create an operation to update the gradients, the batch_loss
    #and do all other update ops
    update_op = tf.group(
        *([apply_gradients_op] + update_ops),
        name='update')

    return update_op

```

The error message is as following

> node train/update (defined at /home/ubuntu/workspace/reproduce/jobs/nabu/nabu/neuralnetworks/trainers/trainer.py:578) has inputs from different frames. The input node train/Listener/features/layer1/BLSTM/bidirectional_rnn/fw/fw/while/fw/bnlstm_cell/bnlstm_cell/state/batch_normalization/AssignMovingAvg (defined at /home/ubuntu/workspace/reproduce/jobs/nabu/nabu/neuralnetworks/components/recurrent_batch.py:61) is in frame 'train/Listener/features/layer1/BLSTM/bidirectional_rnn/fw/fw/while/while_context'. The input node train/apply_gradients/Assign (defined at /home/ubuntu/workspace/reproduce/jobs/nabu/nabu/neuralnetworks/trainers/trainer.py:569) is in frame ''.

Any idea how to fix this?"
24404,Support SparseReduceSum on GPU ,"**System information**
- TensorFlow version (you are using): 1.12.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
From a generated trace I see that tensorflow performs SparseReduceSum on CPU while the rest of computation on GPU. I'm using `tf.sparse.reduce_sum`

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
Everyone, reduction on CPU kills performance completely. 

**Any Other info.**
Could you point to data structures and/or algorithms that you are going to use, if any?
"
24403,frechet_classifier_distance triggers cuSolverDN issues,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Manjaro Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): r1.13 from source
- TensorFlow version (use command below): 'v1.12.0-4609-g040ee45aa8' 1.12.0
- Python version: 3.6.7
- Bazel version (if compiling from source): 0.19
- GCC/Compiler version (if compiling from source): 7.3.1
- CUDA/cuDNN version: CUDA 10.0 with cuDNN 7.4.1
- GPU model and memory: RTX 2070 8 GB

**Describe the current behavior**
Trying to calculate FID triggers `Check failed: cusolverDnCreate(&cusolver_dn_handle) == CUSOLVER_STATUS_SUCCESS Failed to create cuSolverDN instance.`, and the program fails to execute. 

I have checked that `tf.contrib.gan.eval.run_inception` runs without problem, so the problem must be in `tf.contrib.gan.eval.frechet_classifier_distance`. 

**Describe the expected behavior**
It should calculate a frechet inception distance. 

**Code to reproduce the issue**
```
import tensorflow as tf

real = tf.zeros([1, 299, 299, 3], tf.float32)
fake = tf.ones([1, 299, 299, 3], tf.float32)
calc_fid = tf.contrib.gan.eval.frechet_classifier_distance(real, fake, tf.contrib.gan.eval.run_inception)

sess = tf.Session()
fid = sess.run(calc_fid)
```

**Other info / logs**

```
WARNING:tensorflow:From /home/michael/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2018-12-17 11:39:37.256810: W tensorflow/core/framework/op_def_util.cc:355] Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().
WARNING:tensorflow:From /home/michael/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /home/michael/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/gan/python/eval/python/classifier_metrics_impl.py:701: to_double (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
2018-12-17 11:39:39.198829: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-12-17 11:39:39.218429: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 4001425000 Hz
2018-12-17 11:39:39.218916: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55b50cbbed30 executing computations on platform Host. Devices:
2018-12-17 11:39:39.218937: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2018-12-17 11:39:39.359019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-12-17 11:39:39.359747: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55b50c9b9ca0 executing computations on platform CUDA. Devices:
2018-12-17 11:39:39.359771: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce RTX 2070, Compute Capability 7.5
2018-12-17 11:39:39.360173: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.71
pciBusID: 0000:1c:00.0
totalMemory: 7.76GiB freeMemory: 7.16GiB
2018-12-17 11:39:39.360188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2018-12-17 11:39:39.584743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-12-17 11:39:39.584776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2018-12-17 11:39:39.584784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2018-12-17 11:39:39.585158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6882 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:1c:00.0, compute capability: 7.5)
2018-12-17 11:39:42.809448: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
2018-12-17 11:39:44.695693: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x55b50fba7850
2018-12-17 11:39:44.812389: F tensorflow/core/kernels/cuda_solvers.cc:94] Check failed: cusolverDnCreate(&cusolver_dn_handle) == CUSOLVER_STATUS_SUCCESS Failed to create cuSolverDN instance.
[1]    25740 abort (core dumped)  python test.py
```"
24401,How can I use the gradients of variable 'a' to update variable 'b'? ,"Hi, I have two sets of weights 'a' and 'b' in my model, I want to use the gradients of 'a' to update 'b'. Can I call ""compute_gradients"" on 'a' to get the gradients, and use the result to call ""apply_gradients"" for 'b' to update weight 'b'? If feasible , what should I do?

"
24400,ImportError: DLL load failed:    .,"**System information**
- OS: `Windows 10 Pro x64`
- TensorFlow installed from `pip`
- TensorFlow version: `1.12.1`
- Python version: `3.6.5`
- CUDA version: `10.0`, cuDNN version: `7`
- GPU: `940MX with GDDR5 memory`

Let's start do describe this.
I used to create some variables in tensorflow/python/platforms/build_info.py
```python
some_path = ""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/bin/""

msvcp_dll_name = 'C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/jre/bin/msvcp140.dll'
cudart_dll_name = some_path + 'cudart64_100.dll'
cuda_version_number = '10.0'
# nvcuda_dll_name = 'nvcuda.dll'
cudnn_dll_name = some_path + 'cudnn64_7.dll'
cudnn_version_number = '7'
```

But I'd got an error. Logs:
```
C:\Users\Ar4ikov\PycharmProjects\QTApp\similar_text>python similar_text.py
Traceback (most recent call last):
  File ""C:\Users\Ar4ikov\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Ar4ikov\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Ar4ikov\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Ar4ikov\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Ar4ikov\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed:    .

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""similar_text.py"", line 8, in <module>
    import tflearn as tf
  File ""C:\Users\Ar4ikov\AppData\Local\Programs\Python\Python36\lib\site-packages\tflearn\__init__.py"", line 4, in <module>
    from . import config
  File ""C:\Users\Ar4ikov\AppData\Local\Programs\Python\Python36\lib\site-packages\tflearn\config.py"", line 3, in <module>
    import tensorflow as tf
  File ""C:\Users\Ar4ikov\AppData\Roaming\Python\Python36\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Ar4ikov\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Ar4ikov\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Ar4ikov\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Ar4ikov\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Ar4ikov\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Ar4ikov\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Ar4ikov\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed:    .


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

What's wrong?"
24399,"Tensorflow Dataset API, OutOfRangeError: End of sequence","Hi - 

I am trying to do a RNN in Tensorflow using Dataset API for handling the ""spoon feeding"" of data into the estimation process. But, I get a OutOf Range Error for no apparent reason. My code is simple and I cannot figure out why the problem occurs. 

Br, 

`
```
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

t_min, t_max = 0, 30
resolution = 0.1

def time_series(t):
    return t * np.sin(t) / 3 + 2 * np.sin(t*5)

t = np.linspace(t_min, t_max, int((t_max - t_min) / resolution))

n_steps = 20
t_instance = np.linspace(12.2, 12.2 + resolution * (n_steps + 1), n_steps + 1)

x_state = time_series(t)
x_state = np.c_[x_state]

x_test = time_series(t_instance)
x_test = np.c_[x_test]

# spoonfed for Tensorflow 
data_x = np.array(x_state,dtype=np.float32)
data_test = np.array(x_test,dtype=np.float32)
# model parameters 


n_inputs = 1
n_neurons = 100
n_outputs = 1


m = len(data_x)     # number of observations
n_dims = n_inputs   # number of input dimension
batch_size = 40    # the number of batches
#n_steps = 20        # size of the batch = n_steps in the RNN model
n_shift = 1         # target is one time steps into the future for the label
n_epochs = 70       # number of the epochs


DataTensor = tf.placeholder(tf.float32, [m, n_inputs])


series_dataset = tf.data.Dataset.from_tensor_slices(DataTensor)

def shifted_slices(offset):
    return series_dataset.skip(offset).batch(n_steps + n_shift, drop_remainder=True)

def split_in_out(sequence):
    return sequence[:, :n_steps], sequence[:, n_shift:]

dataset = tf.data.Dataset.range(n_steps + n_shift)
dataset = dataset.interleave(shifted_slices, n_steps + n_shift)
dataset = dataset.shuffle(buffer_size=m)
dataset = dataset.repeat(n_epochs)
dataset = dataset.batch(batch_size)
dataset = dataset.map(split_in_out)
dataset = dataset.prefetch(1)

sess = tf.Session()

iterator = dataset.make_initializable_iterator()
#iterator = dataset.make_one_shot_iterator()
feature, label = iterator.get_next()

cell = tf.contrib.rnn.OutputProjectionWrapper(
    tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu),output_size=n_outputs)
outputs, states = tf.nn.dynamic_rnn(cell, feature, dtype=tf.float32)

learning_rate = 0.001

loss = tf.reduce_mean(tf.square(outputs - label)) # MSE
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
training_op = optimizer.minimize(loss)

saver = tf.train.Saver()
init = tf.global_variables_initializer()

with tf.Session() as sess:
    init.run()
    sess.run(iterator.initializer, feed_dict={ DataTensor: data_x } )
    print('Start training process ...')
    
    for i in range (n_epochs):
            tot_loss = 0
            for _ in range (batch_size):
                _, loss_value = sess.run([training_op, loss])
                tot_loss += loss_value
            print(""Iter: {}, Loss: {:.4f}"".format(i, tot_loss / batch_size)) 

    saver.save(sess, ""./SavedModels/Trial"")
```
`
The output: 

`Start training process ...
Iter: 0, Loss: 3.4887
Iter: 1, Loss: 0.7238
Iter: 2, Loss: 0.4668
Iter: 3, Loss: 0.2963
Iter: 4, Loss: 0.1895
Iter: 5, Loss: 0.1205
Iter: 6, Loss: 0.0831
Iter: 7, Loss: 0.0679
Iter: 8, Loss: 0.0613
Iter: 9, Loss: 0.0585
Iter: 10, Loss: 0.0566
Iter: 11, Loss: 0.0548
---------------------------------------------------------------------------
OutOfRangeError                           Traceback (most recent call last)`"
24398,Errors in runtime binaries with selective registration ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v1.9
- Python version: 3.6.4
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


I am trying to build using the selective registration and able to generate the ops_to_register.h and tensorflow binary but it crashes at the run time.

I have tried this https://www.bountysource.com/issues/43940878-errors-when-building-ios-binaries-with-selective-registration but that seems to have lot of compilation issues for me.

Below is the crash log
#0  1.8.0/tensorflow/contrib/makefile/gen/protobuf-host/include/google/protobuf/arenastring.h:83
#1  name (this=<optimized out>) at /usr/src/debug/tensorflow-1.8.0/tensorflow/contrib/makefile/gen/proto/tensorflow/core/framework/node_def.pb.h:336
#2  _ZNK10tensorflow4Node4nameB5cxx11Ev (this=this@entry=0x0) at tensorflow/core/graph/graph.cc:140
#3  0xf699877a in name (this=0xab499320) at tensorflow/cc/framework/ops.h:76
#4  tensorflow::ClientSession::Run (this=this@entry=0xf11f8bc0, run_options=..., inputs=..., fetch_outputs=..., run_outputs=..., outputs=outputs@entry=0xf11f8be8, run_metadata=run_metadata@entry=0x0)
    at tensorflow/cc/client/client_session.cc:118
#5  0xf6998af4 in tensorflow::ClientSession::Run (this=0xf11f8bc0, inputs=..., fetch_outputs=..., run_outputs=..., outputs=outputs@entry=0xf11f8be8) at tensorflow/cc/client/client_session.cc:90
#6  0xf6998bf6 in tensorflow::ClientSession::Run (this=<optimized out>, inputs=..., fetch_outputs=..., outputs=0xf11f8be8) at tensorflow/cc/client/client_session.cc:82


Is the below correct or we need to use the strstrc
define SHOULD_REGISTER_OP_KERNEL(clz) (find_in<sizeof(kNecessaryOpKernelClasses) / sizeof(*kNecessaryOpKernelClasses)>::f(clz, kNecessaryOpKernelClasses))

Any suggestion will be helpful.

"
24397,tf.losses.softmax_cross_entropy does not handle sequence loss when using label smoothing,"When using `tf.losses.softmax_cross_entropy` with a 3D Tensor for `labels` and `logits` (which you may do if your output is a sequence), as well as the `label_smoothing` parameter, the result is incorrect. From a cursory inspection of the python source I would guess that in [line 796](https://github.com/tensorflow/tensorflow/blob/a6d8ffae097d0132989ae4688d224121ec6d8f35/tensorflow/python/ops/losses/losses_impl.py#L796), the number of classes is assumed to be `shape(onehot_labels)[1]`, rather than `shape(onehot_labels)[-1]`. The former assumes a 2D labels tensor which is neither described by the documentation nor enforced by the method.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Mint Tumbleweed
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0
- Python version: 3.5.2
- CUDA/cuDNN version: 9.0/7.1.2
- GPU model and memory: GeForce GTX 1080 Ti 11 GB
"
24396,FeedInputs: unable to find feed output input,"- Have I written custom code : No
- OS Platform and Distribution : Linux Ubuntu 16.04
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.12
- Python version:3.5

I have got below error:
**FeedInputs: unable to find feed output input**
Expected : No issue such as unable to find feed output input


I have used tf.keras to build the model and used model.fit to train the model instead of using sess.run  & feed_dict. 
 
Command used to run transfrom graph is as follows :
bazel-bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=~/saved_model2.pb --out_graph=optimized_graph1.pb --inputs=input --outputs=output/kernel --transforms='strip_unused_nodes(type=float, shape=""1,28,28,1"") remove_nodes(op=Identity, op=CheckNumerics) fold_constants fold_batch_norms fold_old_batch_norms'  


"
24393,Equivalent c_api for TFE_Py_RecordGradient,"Hi, I'm trying to implement a C# wrapper for Tensorflow, but stuck in the c_api which is work like `TFE_Py_RecordGradient`.

Is there any equivalent c_api? or any work around?

I want to implement a C# code to do `var x = tf.placeholder(tf.float32, shape: new TensorShape(1024, 1024));`

Any related information will be helpful, thanks."
24392,TFLite:Models with multiple inputs and outputs,"How can I handle the situations when my tensorflow models have mutiple inputs and outputs? It's not possible to give a fixed input_shape, so can any one help?"
24390,Variable sized tf records,"While storing data in tf records, say an image of size (N,3,224,224), its possible to keep N as variable, but is it possible to have another dimension variable. Like I have voxels of objects that I need to store and number of objects in an 3D world is random, so it will be something like (batch_size, num_objects, 128,128,128). Tensorflow does not seem to allow having one more dimension. Is there a way to solve this?"
24389,Build issue with triSYCL - EIGEN_MAX_ALIGN_BYTES redefined,"**Describe the problem**

I am trying to build tensorflow with triSYCL (the open source implementation). But the compilation fails because of a macro redefinition:
```
<command-line>: warning: ""EIGEN_MAX_ALIGN_BYTES"" redefined
<command-line>: note: this is the location of the previous definition
ERROR: /home/emile/Workspace/MyAurPackages/tensorflow-triSYCL-git/src/tensorflow/tensorflow/python/BUILD:537:1: C++ compilation of rule '//tensorflow/python:python_op_gen' failed (Exit 1)
```

**System information**
- OS Platform and Distribution: Archlinux
- TensorFlow version: last from github
- Bazel version (if compiling from source): 0.19.2- (@non-git)
- GCC/Compiler version (if compiling from source): gcc version 8.2.1 20180831 (GCC) 


**Provide the exact sequence of commands / steps that you executed before running into the problem**

To compile, I use
```
./configure
bazel build --config=opt --config=sycl_trisycl //tensorflow:libtensorflow.so //tensorflow/tools/pip_package:build_pip_package
```

**Any other info / logs**
The most complete version of the error is:
```
<command-line>: warning: ""EIGEN_MAX_ALIGN_BYTES"" redefined
<command-line>: note: this is the location of the previous definition
ERROR: /home/emile/Workspace/MyAurPackages/tensorflow-triSYCL-git/src/tensorflow/tensorflow/python/BUILD:537:1: C++ compilation of rule '//tensorflow/python:python_op_gen' failed (Exit 1)
<command-line>: warning: ""EIGEN_MAX_ALIGN_BYTES"" redefined
<command-line>: note: this is the location of the previous definition
In file included from /usr/include/CL/sycl/device/detail/host_device.hpp:18,
                 from /usr/include/CL/sycl/device.hpp:23,
                 from /usr/include/CL/sycl/context/detail/context.hpp:12,
                 from /usr/include/CL/sycl/context/detail/host_context.hpp:18,
                 from /usr/include/CL/sycl/context.hpp:14,
                 from /usr/include/CL/sycl/buffer/detail/buffer_base.hpp:27,
                 from /usr/include/CL/sycl/command_group/detail/task.hpp:23,
                 from /usr/include/CL/sycl/accessor/detail/local_accessor.hpp:21,
                 from /usr/include/CL/sycl/accessor.hpp:15,
                 from /usr/include/CL/sycl.hpp:37,
                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:22,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/framework/types.h:23,
                 from tensorflow/python/framework/python_op_gen_internal.cc:33:
/usr/include/CL/sycl/device/detail/device.hpp:59:16: error: 'any' in namespace 'std' does not name a type
   virtual std::any get_info(info::device param) const = 0;
                ^~~
/usr/include/CL/sycl/device/detail/device.hpp:59:11: note: 'std::any' is only available from C++17 onwards
   virtual std::any get_info(info::device param) const = 0;
           ^~~
In file included from /usr/include/CL/sycl/device.hpp:23,
                 from /usr/include/CL/sycl/context/detail/context.hpp:12,
                 from /usr/include/CL/sycl/context/detail/host_context.hpp:18,
                 from /usr/include/CL/sycl/context.hpp:14,
                 from /usr/include/CL/sycl/buffer/detail/buffer_base.hpp:27,
                 from /usr/include/CL/sycl/command_group/detail/task.hpp:23,
                 from /usr/include/CL/sycl/accessor/detail/local_accessor.hpp:21,
                 from /usr/include/CL/sycl/accessor.hpp:15,
                 from /usr/include/CL/sycl.hpp:37,
                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:22,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/framework/types.h:23,
                 from tensorflow/python/framework/python_op_gen_internal.cc:33:
/usr/include/CL/sycl/device/detail/host_device.hpp:101:8: error: 'any' in namespace 'std' does not name a type
   std::any
        ^~~
/usr/include/CL/sycl/device/detail/host_device.hpp:101:3: note: 'std::any' is only available from C++17 onwards
   std::any
   ^~~
In file included from /usr/include/CL/sycl/context/detail/context.hpp:12,
                 from /usr/include/CL/sycl/context/detail/host_context.hpp:18,
                 from /usr/include/CL/sycl/context.hpp:14,
                 from /usr/include/CL/sycl/buffer/detail/buffer_base.hpp:27,
                 from /usr/include/CL/sycl/command_group/detail/task.hpp:23,
                 from /usr/include/CL/sycl/accessor/detail/local_accessor.hpp:21,
                 from /usr/include/CL/sycl/accessor.hpp:15,
                 from /usr/include/CL/sycl.hpp:37,
                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:22,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/framework/types.h:23,
                 from tensorflow/python/framework/python_op_gen_internal.cc:33:
/usr/include/CL/sycl/device.hpp: In member function 'typename cl::sycl::info::param_traits<cl::sycl::info::device, Param>::return_type cl::sycl::device::get_info() const':
/usr/include/CL/sycl/device.hpp:222:17: error: 'any_cast' is not a member of 'std'
     return std::any_cast<
                 ^~~~~~~~
/usr/include/CL/sycl/device.hpp:222:17: note: 'std::any_cast' is only available from C++17 onwards
/usr/include/CL/sycl/device.hpp:224:7: error: expected '(' before '>' token
       >(implementation->get_info(param));
       ^
       (
/usr/include/CL/sycl/device.hpp:224:25: error: 'using element_type = class cl::sycl::detail::device' {aka 'class cl::sycl::detail::device'} has no member named 'get_info'; did you mean 'get_platform'?
       >(implementation->get_info(param));
                         ^~~~~~~~
                         get_platform
In file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:108,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/framework/types.h:23,
                 from tensorflow/python/framework/python_op_gen_internal.cc:33:
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceSycl.h: In member function 'void* Eigen::QueueInterface::allocate(std::size_t) const':
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceSycl.h:163:108: error: no matching function for call to 'cl::sycl::buffer<unsigned char, 1, SyclAllocator<unsigned char, 16> >::get_access<discard_write, host_buffer>()'
     auto ptr =buf.get_access<cl::sycl::access::mode::discard_write, cl::sycl::access::target::host_buffer>().get_pointer();
                                                                                                            ^
In file included from /usr/include/CL/sycl.hpp:40,
                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:22,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/framework/types.h:23,
                 from tensorflow/python/framework/python_op_gen_internal.cc:33:
/usr/include/CL/sycl/buffer.hpp:347:3: note: candidate: 'template<cl::sycl::access::mode Mode, cl::sycl::access::target Target> cl::sycl::accessor<T, Dimensions, Mode, Target> cl::sycl::buffer<T, Dimensions, Allocator>::get_access(cl::sycl::handler&) [with cl::sycl::access::mode Mode = Mode; cl::sycl::access::target Target = Target; T = unsigned char; int Dimensions = 1; Allocator = SyclAllocator<unsigned char, 16>]'
   get_access(handler &command_group_handler) {
   ^~~~~~~~~~
/usr/include/CL/sycl/buffer.hpp:347:3: note:   template argument deduction/substitution failed:
In file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:108,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/framework/types.h:23,
                 from tensorflow/python/framework/python_op_gen_internal.cc:33:
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceSycl.h:163:108: note:   candidate expects 1 argument, 0 provided
     auto ptr =buf.get_access<cl::sycl::access::mode::discard_write, cl::sycl::access::target::host_buffer>().get_pointer();
                                                                                                            ^
In file included from /usr/include/CL/sycl.hpp:40,
                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:22,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/framework/types.h:23,
                 from tensorflow/python/framework/python_op_gen_internal.cc:33:
/usr/include/CL/sycl/buffer.hpp:376:3: note: candidate: 'template<cl::sycl::access::mode Mode> cl::sycl::accessor<T, Dimensions, Mode, (cl::sycl::access::target)2018> cl::sycl::buffer<T, Dimensions, Allocator>::get_access() [with cl::sycl::access::mode Mode = Mode; T = unsigned char; int Dimensions = 1; Allocator = SyclAllocator<unsigned char, 16>]'
   get_access() {
   ^~~~~~~~~~
/usr/include/CL/sycl/buffer.hpp:376:3: note:   template argument deduction/substitution failed:
In file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:108,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/framework/types.h:23,
                 from tensorflow/python/framework/python_op_gen_internal.cc:33:
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceSycl.h:163:108: error: wrong number of template arguments (2, should be 1)
     auto ptr =buf.get_access<cl::sycl::access::mode::discard_write, cl::sycl::access::target::host_buffer>().get_pointer();
                                                                                                            ^
In file included from external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorSycl.h:110,
                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:156,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/framework/types.h:23,
                 from tensorflow/python/framework/python_op_gen_internal.cc:33:
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h: In member function 'void Eigen::EigenConvolutionKernel1D<CoeffReturnType, KernelType, HostExpr, FunctorExpr, Index, InputDims, Kernel_accessor, Buffer_accessor, Local_accessor, TupleType>::operator()(cl::sycl::nd_item<2>)':
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:54:47: error: 'struct cl::sycl::nd_item<2>' has no member named 'get_local'; did you mean 'set_local'?
     const size_t plane_kernel_offset = itemID.get_local(1) * num_x_input;
                                               ^~~~~~~~~
                                               set_local
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:56:95: error: 'struct cl::sycl::nd_item<2>' has no member named 'get_global'; did you mean 'set_global'?
     const size_t plane_tensor_offset =indexMapper.mapCudaInputPlaneToTensorInputOffset(itemID.get_global(1));
                                                                                               ^~~~~~~~~~
                                                                                               set_global
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:58:28: error: 'struct cl::sycl::nd_item<2>' has no member named 'get_local'; did you mean 'set_local'?
     for (size_t i = itemID.get_local(0); i < num_x_input ; i += itemID.get_local_range()[0]) {
                            ^~~~~~~~~
                            set_local
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:61:72: error: 'struct cl::sycl::nd_item<2>' has no member named 'get_global'; did you mean 'set_global'?
       if(((i + first_input_start) < (range_x +kernelSize-1)) && itemID.get_global(1)< range_y){
                                                                        ^~~~~~~~~~
                                                                        set_global
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:71:15: error: 'struct cl::sycl::nd_item<2>' has no member named 'get_global'; did you mean 'set_global'?
     if(itemID.get_global(0)< range_x && itemID.get_global(1)< range_y){
               ^~~~~~~~~~
               set_global
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:71:48: error: 'struct cl::sycl::nd_item<2>' has no member named 'get_global'; did you mean 'set_global'?
     if(itemID.get_global(0)< range_x && itemID.get_global(1)< range_y){
                                                ^~~~~~~~~~
                                                set_global
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:73:56: error: 'struct cl::sycl::nd_item<2>' has no member named 'get_local'; did you mean 'set_local'?
       const size_t index = plane_kernel_offset+ itemID.get_local(0);
                                                        ^~~~~~~~~
                                                        set_local
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:77:93: error: 'struct cl::sycl::nd_item<2>' has no member named 'get_global'; did you mean 'set_global'?
       const size_t tensor_index = indexMapper.mapCudaOutputPlaneToTensorOutputOffset(itemID.get_global(1))
                                                                                             ^~~~~~~~~~
                                                                                             set_global
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:78:67: error: 'struct cl::sycl::nd_item<2>' has no member named 'get_local'; did you mean 'set_local'?
       +indexMapper.mapCudaOutputKernelToTensorOutputOffset(itemID.get_local(0) + first_output_start);
                                                                   ^~~~~~~~~
                                                                   set_local
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h: In member function 'void Eigen::EigenConvolutionKernel2D<CoeffReturnType, KernelType, HostExpr, FunctorExpr, Index, InputDims, Kernel_accessor, Buffer_accessor, Local_accessor, TupleType>::operator()(cl::sycl::nd_item<3>)':
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:112:95: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_global'; did you mean 'set_global'?
     const size_t plane_input_offset = indexMapper.mapCudaInputPlaneToTensorInputOffset(itemID.get_global(2));
                                                                                               ^~~~~~~~~~
                                                                                               set_global
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:113:47: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_local'; did you mean 'set_local'?
     const size_t plane_kernel_offset = itemID.get_local(2) * num_y_input;
                                               ^~~~~~~~~
                                               set_local
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:118:28: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_local'; did you mean 'set_local'?
     for (size_t j = itemID.get_local(1); j < num_y_input; j += itemID.get_local_range()[1]) {
                            ^~~~~~~~~
                            set_local
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:120:30: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_local'; did you mean 'set_local'?
       for (size_t i = itemID.get_local(0); i < num_x_input ; i += itemID.get_local_range()[0]) {
                              ^~~~~~~~~
                              set_local
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:123:137: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_global'; did you mean 'set_global'?
         if(((i + first_x_input_start) < (range_x +kernelSize_x-1))  &&((j + first_y_input_start) < (range_y +kernelSize_y-1)) && itemID.get_global(2)< range_z){
                                                                                                                                         ^~~~~~~~~~
                                                                                                                                         set_global
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:135:15: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_global'; did you mean 'set_global'?
     if(itemID.get_global(0)< range_x && itemID.get_global(1)< range_y && itemID.get_global(2)< range_z){
               ^~~~~~~~~~
               set_global
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:135:48: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_global'; did you mean 'set_global'?
     if(itemID.get_global(0)< range_x && itemID.get_global(1)< range_y && itemID.get_global(2)< range_z){
                                                ^~~~~~~~~~
                                                set_global
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:135:81: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_global'; did you mean 'set_global'?
     if(itemID.get_global(0)< range_x && itemID.get_global(1)< range_y && itemID.get_global(2)< range_z){
                                                                                 ^~~~~~~~~~
                                                                                 set_global
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:139:76: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_local'; did you mean 'set_local'?
         const size_t index = (num_x_input*(plane_kernel_offset + j+ itemID.get_local(1))) + itemID.get_local(0);
                                                                            ^~~~~~~~~
                                                                            set_local
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:139:100: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_local'; did you mean 'set_local'?
         const size_t index = (num_x_input*(plane_kernel_offset + j+ itemID.get_local(1))) + itemID.get_local(0);
                                                                                                    ^~~~~~~~~
                                                                                                    set_local
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:144:93: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_global'; did you mean 'set_global'?
       const size_t tensor_index = indexMapper.mapCudaOutputPlaneToTensorOutputOffset(itemID.get_global(2))
                                                                                             ^~~~~~~~~~
                                                                                             set_global
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:145:67: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_local'; did you mean 'set_local'?
       +indexMapper.mapCudaOutputKernelToTensorOutputOffset(itemID.get_local(0) + fitst_x_output_start, itemID.get_local(1) + fitst_y_output_start);
                                                                   ^~~~~~~~~
                                                                   set_local
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:145:111: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_local'; did you mean 'set_local'?
       +indexMapper.mapCudaOutputKernelToTensorOutputOffset(itemID.get_local(0) + fitst_x_output_start, itemID.get_local(1) + fitst_y_output_start);
                                                                                                               ^~~~~~~~~
                                                                                                               set_local
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h: In member function 'void Eigen::EigenConvolutionKernel3D<CoeffReturnType, KernelType, HostExpr, FunctorExpr, Index, InputDims, Kernel_accessor, Buffer_accessor, Local_accessor, TupleType>::operator()(cl::sycl::nd_item<3>)':
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:189:30: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_local'; did you mean 'set_local'?
       for (size_t k = itemID.get_local(2); k < num_z_input; k += itemID.get_local_range()[2]) {
                              ^~~~~~~~~
                              set_local
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:190:32: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_local'; did you mean 'set_local'?
         for (size_t j = itemID.get_local(1); j < num_y_input; j += itemID.get_local_range()[1]) {
                                ^~~~~~~~~
                                set_local
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:191:34: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_local'; did you mean 'set_local'?
           for (size_t i = itemID.get_local(0); i < num_x_input ; i += itemID.get_local_range()[0]) {
                                  ^~~~~~~~~
                                  set_local
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:208:17: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_global'; did you mean 'set_global'?
       if(itemID.get_global(0)< range_x && itemID.get_global(1)< range_y && itemID.get_global(2)< range_z){
                 ^~~~~~~~~~
                 set_global
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:208:50: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_global'; did you mean 'set_global'?
       if(itemID.get_global(0)< range_x && itemID.get_global(1)< range_y && itemID.get_global(2)< range_z){
                                                  ^~~~~~~~~~
                                                  set_global
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:208:83: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_global'; did you mean 'set_global'?
       if(itemID.get_global(0)< range_x && itemID.get_global(1)< range_y && itemID.get_global(2)< range_z){
                                                                                   ^~~~~~~~~~
                                                                                   set_global
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:214:54: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_local'; did you mean 'set_local'?
               const size_t local_index = ((i+ itemID.get_local(0))+  num_x_input*((j+ itemID.get_local(1)) + num_y_input * (k+ itemID.get_local(2))));
                                                      ^~~~~~~~~
                                                      set_local
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:214:94: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_local'; did you mean 'set_local'?
               const size_t local_index = ((i+ itemID.get_local(0))+  num_x_input*((j+ itemID.get_local(1)) + num_y_input * (k+ itemID.get_local(2))));
                                                                                              ^~~~~~~~~
                                                                                              set_local
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:214:135: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_local'; did you mean 'set_local'?
               const size_t local_index = ((i+ itemID.get_local(0))+  num_x_input*((j+ itemID.get_local(1)) + num_y_input * (k+ itemID.get_local(2))));
                                                                                                                                       ^~~~~~~~~
                                                                                                                                       set_local
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:220:69: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_local'; did you mean 'set_local'?
         +indexMapper.mapCudaOutputKernelToTensorOutputOffset(itemID.get_local(0) + fitst_x_output_start, itemID.get_local(1) + fitst_y_output_start, itemID.get_local(2) + fitst_z_output_start );
                                                                     ^~~~~~~~~
                                                                     set_local
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:220:113: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_local'; did you mean 'set_local'?
         +indexMapper.mapCudaOutputKernelToTensorOutputOffset(itemID.get_local(0) + fitst_x_output_start, itemID.get_local(1) + fitst_y_output_start, itemID.get_local(2) + fitst_z_output_start );
                                                                                                                 ^~~~~~~~~
                                                                                                                 set_local
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConvolutionSycl.h:220:157: error: 'struct cl::sycl::nd_item<3>' has no member named 'get_local'; did you mean 'set_local'?
         +indexMapper.mapCudaOutputKernelToTensorOutputOffset(itemID.get_local(0) + fitst_x_output_start, itemID.get_local(1) + fitst_y_output_start, itemID.get_local(2) + fitst_z_output_start );
                                                                                                                                                             ^~~~~~~~~
                                                                                                                                                             set_local
In file included from external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorSycl.h:115,
                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:156,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/framework/types.h:23,
                 from tensorflow/python/framework/python_op_gen_internal.cc:33:
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorSyclFunctors.h: In member function 'void Eigen::TensorSycl::internal::GenericKernelReducer<CoeffReturnType, OP, OutputAccessor, InputAccessor, LocalAccessor>::operator()(cl::sycl::nd_item<1>)':
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorSyclFunctors.h:31:32: error: 'struct cl::sycl::nd_item<1>' has no member named 'get_global'; did you mean 'set_global'?
       size_t globalid = itemID.get_global(0);
                                ^~~~~~~~~~
                                set_global
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorSyclFunctors.h:32:31: error: 'struct cl::sycl::nd_item<1>' has no member named 'get_local'; did you mean 'set_local'?
       size_t localid = itemID.get_local(0);
                               ^~~~~~~~~
                               set_local
In file included from external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorSycl.h:117,
                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:156,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/framework/types.h:23,
                 from tensorflow/python/framework/python_op_gen_internal.cc:33:
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContractionSycl.h: In member function 'void Eigen::KernelConstructor<HostExpr, OutScalar, LhsScalar, RhsScalar, LHSFunctorExpr, RHSFunctorExpr, LhsLocalAcc, RhsLocalAcc, OutAccessor, Index, ContractT, LeftNocontractT, RightNocontractT, lhs_inner_dim_contiguous, rhs_inner_dim_contiguous, rhs_inner_dim_reordered, TileSizeDimM, TileSizeDimN, TileSizeDimK, WorkLoadPerThreadM, WorkLoadPerThreadN, LocalThreadSizeM, LocalThreadSizeN, LoadPerThreadLhs, LoadPerThreadRhs, LHSTupleType, RHSTupleType, Device>::operator()(cl::sycl::nd_item<2>)':
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContractionSycl.h:232:43: error: 'struct cl::sycl::nd_item<2>' has no member named 'get_local'; did you mean 'set_local'?
       const Index mLocalThreadId = itemID.get_local(0); // Local ID row
                                           ^~~~~~~~~
                                           set_local
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContractionSycl.h:233:43: error: 'struct cl::sycl::nd_item<2>' has no member named 'get_local'; did you mean 'set_local'?
       const Index nLocalThreadId = itemID.get_local(1); // Local ID col
                                           ^~~~~~~~~
                                           set_local
In file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:157,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/framework/types.h:23,
                 from tensorflow/python/framework/python_op_gen_internal.cc:33:
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h: At global scope:
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:400:7: error: template argument 'Eigen::internal::IsTileable<Eigen::SyclDevice, Expression>::value' involves template parameter(s)
 class TensorExecutor<Expression, SyclDevice, Vectorizable> {
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
tensorflow/python/framework/python_op_gen_internal.cc: In member function 'virtual std::__cxx11::string tensorflow::python_op_gen_internal::GenPythonOp::Code()':
tensorflow/python/framework/python_op_gen_internal.cc:545:44: warning: comparison of integer expressions of different signedness: 'int' and 'std::vector<tensorflow::python_op_gen_internal::ParamNames>::size_type' {aka 'long unsigned int'} [-Wsign-compare]
   for (int i = op_def_.input_arg_size(); i < params_no_default.size(); ++i) {
                                          ~~^~~~~~~~~~~~~~~~~~~~~~~~~~
tensorflow/python/framework/python_op_gen_internal.cc:548:21: warning: comparison of integer expressions of different signedness: 'int' and 'std::vector<tensorflow::python_op_gen_internal::ParamNames>::size_type' {aka 'long unsigned int'} [-Wsign-compare]
   for (int i = 0; i < params_with_default.size(); ++i) {
                   ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~

```
"
24388,Way to use exported graph from speech recognition/commands example in TF serving,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.12.0
- Are you willing to contribute it (Yes/No): yes



**Describe the feature and the current behavior/state.**
I am using speech commands TF example and have trained and built the model with filename extension .pb and I would like to use somehow that model in tensorflow serving and thats where we have problem.
Tensorflow serving accepts other format of model that have trainedmodel.pb file and ""variables"" subdirectory.

TF serving accepted model view:
![image](https://user-images.githubusercontent.com/37185376/50054342-2e221b80-0141-11e9-9176-66a964c8a844.png)

exported model from speech recognition example:
![image](https://user-images.githubusercontent.com/37185376/50054362-5ad63300-0141-11e9-8953-870a8c9e9ea5.png)

If we are missing something here, excuse our incompetence and would likely help in any way here.

Also, if request is valid, there could be be used two approaches to tackle this request, one is allowing exporting of model from tf examples to format accepted by tf serving or to add feature to tf serving to accept more model variants.


**Will this change the current api? How?**

**Who will benefit with this feature?**
Any developer that is using tensorflow where model version variances would not be an obstacle to use tensorflow serving

**Any Other info.**
"
24387,[Feature Request]Deformable Convolutional Op Support,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):1.12.0
- Are you willing to contribute it (Yes/No):Yes

**Describe the feature and the current behavior/state.**
Deformable Convolutional Network(https://arxiv.org/pdf/1703.06211.pdf,https://arxiv.org/pdf/1703.06211.pdf) has achieved good results in the du detection task.But the current version of Tensorflow (1.12)does not seem to implement deformable convolution operations.The efficiency of some third-party implementations of deformable convolution operations is lower than that of the mxnet version(https://github.com/msracver/Deformable-ConvNets).I hope that Tensorflow can add native deformable convolution operations in future versions.
Thanks.
**Will this change the current api? How?**
Yes,Add a new API
**Who will benefit with this feature?**
Anyone who wants to use Tensorflow for object detection

**Any Other info.**
N/A"
24386,TensorForest: Performance issues in time and space,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary, pip install tensorflow==1.12.0
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6.4
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: No usage
- GPU model and memory: No usage
- Machine RAM and #Cores: **64GB RAM and 12 CPU Cores** 

**Describe the current behavior**

Currently, I trained TensorForest on a relatively large train data (1.5M examples, 1024 features) with 500 trees, max_nodes. Moreover, I compared its performance with Scikit-learn implementation.
It seems that the TensorForest is inferior to Scikit-Learn ExtraTrees implementation both in time and space.

**In terms of space costs**, training a TensorForest forest  consists of 500 trees with 10k nodes will throw the following exception in my machine (64GB RAM and 12 CPU Cores).
```
ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1500000,500,10] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
	 [[node stack (defined at /home/experiment/huqiu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/tensor_forest/python/tensor_forest.py:519)  = Pack[N=500, T=DT_FLOAT, axis=1, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](TreePredictionsV4, TreePredictionsV4_1, TreePredictionsV4_2, TreePredictionsV4_3, TreePredictionsV4_4, ...... TreePredictionsV4_499)
```
But Scikit-learn based extra trees classifier only costs up to **18GB** of memory.

This issue makes it unfeasible to train TensorForest on large datasets or large number of trees.
As the paper said, It seems that TensorForest should not have such a big performance gap compared with scikit-learn, right? 
Thus I wonder whether it is my mis-use?

**In terms of time costs**,  training a TensorForest forest  consists of **only 2** trees with 10k nodes will cost **1235 second** with 116 training steps on the machine. But Scikit-learn based implementation costs only **20** seconds.
This 60-fold gap is clearly unreasonable, right?

**Describe the expected behavior**

1. Isn't it an online algorithm? It seems that the memory consumption of the TensorForest is relatively large?
2. The original paper said that, in large dataset (like HIGGS, 11M examples, 28 features), TensorForest with 100 trees and 10k nodes per tree trains in about **one percent** of the time taken by scikit-learn, even without taking advantage of distributed training. But I cannot reproduce that fast training speed.

Thus I am sincerely looking forward to your help or explanations.
Great, Great, Great thanks : )
@yupbank @yongtang @tensorflower-gardener

**Code to reproduce the issue**

Complete Code Snippet, you can run it directely without any modification.
============================== Space Cost Comparison =========================
TensorForest:
```
import time
import tensorflow as tf
tf.logging.set_verbosity(tf.logging.INFO)
tf.keras.backend.set_image_data_format('channels_first')

(x_train, y_train), _ = tf.keras.datasets.cifar10.load_data()
x_train = x_train.reshape((-1, 32*32)).astype('float32')   # produce more examples
y_train = y_train.repeat(3, axis=0).astype('int')
x_train = x_train.repeat(10, axis=0)     # repeat 10 times to produce more examples
y_train = y_train.repeat(10, axis=0).reshape(-1)

print(x_train.shape, y_train.shape)      # totally 1500000 examples
# (1500000, 1024) (1500000,)

start_time = time.time()
est_args = {'num_classes': 10, 'num_features': 1024, 'regression': False,
                   'num_trees': 500, 'max_nodes': 10000,
                   'base_random_seed': 0}

params = tf.contrib.tensor_forest.python.tensor_forest.ForestHParams(**est_args)
estimator = tf.contrib.tensor_forest.client.random_forest.TensorForestEstimator(params)
estimator.fit(x_train, y_train)
print(""Time Cost: {}"".format(time.time() - start_time))

# Exception: ResourceExhaustedError
```

Scikit-learn:
```
import time
import tensorflow as tf
tf.logging.set_verbosity(tf.logging.INFO)
tf.keras.backend.set_image_data_format('channels_first')

(x_train, y_train), _ = tf.keras.datasets.cifar10.load_data()
x_train = x_train.reshape((-1, 32*32)).astype('float32')   # produce more examples
y_train = y_train.repeat(3, axis=0).astype('int')
x_train = x_train.repeat(10, axis=0)     # repeat 10 times to produce more examples
y_train = y_train.repeat(10, axis=0).reshape(-1)

print(x_train.shape, y_train.shape)      # totally 1500000 examples
# (1500000, 1024) (1500000,)

start_time = time.time()
est_args = {'num_classes': 10, 'num_features': 1024, 'regression': False,
                   'num_trees': 500, 'max_nodes': 10000,
                   'base_random_seed': 0}

from sklearn.ensemble import ExtraTreesClassifier
estimator = ExtraTreesClassifier(n_estimators=500, n_jobs=-1)
estimator.fit(x_train, y_train)
print(""Time Cost: {}"".format(time.time() - start_time))

# Time Cost: 885.44 (seconds)
```

============================== Time Cost Comparison =========================
TensorForest:
```
import time
import tensorflow as tf
tf.logging.set_verbosity(tf.logging.INFO)
tf.keras.backend.set_image_data_format('channels_first')

(x_train, y_train), _ = tf.keras.datasets.cifar10.load_data()
x_train = x_train.reshape((-1, 32*32)).astype('float32')   # produce more examples
y_train = y_train.repeat(3, axis=0).astype('int')
x_train = x_train.repeat(10, axis=0)     # repeat 10 times to produce more examples
y_train = y_train.repeat(10, axis=0).reshape(-1)

print(x_train.shape, y_train.shape)      # totally 1500000 examples
# (1500000, 1024) (1500000,)

start_time = time.time()
est_args = {'num_classes': 10, 'num_features': 1024, 'regression': False,
                   'num_trees': 2, 'max_nodes': 10000,
                   'base_random_seed': 0}

params = tf.contrib.tensor_forest.python.tensor_forest.ForestHParams(**est_args)
estimator = tf.contrib.tensor_forest.client.random_forest.TensorForestEstimator(params)
estimator.fit(x_train, y_train)
print(""Time Cost: {}"".format(time.time() - start_time))

# Time Cost: 1235 (seconds)
```

Scikit-learn:
```
import time
import tensorflow as tf
tf.logging.set_verbosity(tf.logging.INFO)
tf.keras.backend.set_image_data_format('channels_first')

(x_train, y_train), _ = tf.keras.datasets.cifar10.load_data()
x_train = x_train.reshape((-1, 32*32)).astype('float32')   # produce more examples
y_train = y_train.repeat(3, axis=0).astype('int')
x_train = x_train.repeat(10, axis=0)     # repeat 10 times to produce more examples
y_train = y_train.repeat(10, axis=0).reshape(-1)

print(x_train.shape, y_train.shape)      # totally 1500000 examples
# (1500000, 1024) (1500000,)

start_time = time.time()
est_args = {'num_classes': 10, 'num_features': 1024, 'regression': False,
                   'num_trees': 500, 'max_nodes': 10000,
                   'base_random_seed': 0}

from sklearn.ensemble import ExtraTreesClassifier
estimator = ExtraTreesClassifier(n_estimators=500, n_jobs=-1)
estimator.fit(x_train, y_train)
print(""Time Cost: {}"".format(time.time() - start_time))

# Time Cost: 20.39 (seconds)
```

@yupbank 
"
24385,Tensorflow : Encountered error while reading extension file 'closure/defs.bzl' issue,"

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 18.04):
- TensorFlow installed from (source):
- TensorFlow version:latest
- Python version:3
- Installed using pip
- Bazel version (0.20):
- GCC/Compiler version (if compiling from source):

-Compiling on Odroid



**Describe the problem**

I am compiling tensorflow on arm architecture and getting the following error
Command Run:
odroid@odroid:~/Desktop/Pratik/tensorflow$ bazel build -c opt --copt=""-mfpu=neon-vfpv4"" --copt=""-funsafe-math-optimizations"" --copt=""-ftree-vectorize"" --copt=""-fomit-frame-pointer"" --local_resources 1024,1.0,1.0 --verbose_failures tensorflow/tools/pip_package:build_pip_package

Error:
WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
/home/odroid/Desktop/Pratik/tensorflow/tools/bazel.rc
INFO: Invocation ID: 09bae6ef-fd25-4864-86b4-a9ccaaba3340
ERROR: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': The native http_archive rule is deprecated. load(""@bazel_tools//tools/build_defs/repo:http.bzl"", ""http_archive"") for a drop-in replacement.
Use --incompatible_remove_native_http_archive=false to temporarily continue using the native rule.
ERROR: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': The native http_archive rule is deprecated. load(""@bazel_tools//tools/build_defs/repo:http.bzl"", ""http_archive"") for a drop-in replacement.
Use --incompatible_remove_native_http_archive=false to temporarily continue using the native rule.
INFO: Elapsed time: 0.307s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
    Fetching @io_bazel_rules_closure; fetching

"
24384,tf.control_dependencies() not respected with Keras layers,"**System information**
- TensorFlow version: v1.12.0-0-ga6d8ffae09 1.12.0
- Python version: 3.6.7

I want to switch dropout on and off while processing one single batch of data (i.e., feeding data just once). For this I replace the _learning_phase_ placeholder used by Keras with a variable, which I can then assign dynamically, and use `tf.control_dependencies()` to control the order of operations. Order of execution does not seem to be respected, leading to random behavior.
More details: In the context of reinforcement learning, for each batch of data, I want to: (1) do a forward pass over my network with dropout disable to build targets; and (2) do a training step with those targets with dropout enabled.   

**Code to reproduce the issue**
The code below is a toy example that resembles what I want to do. I would expect `o1` and `o3` to be equal and different than `o2`. I would also expect to run w/o crashing. 

```
from tensorflow.keras.layers import Dropout, Dense, Input, Lambda, Layer, Add, RepeatVector, LSTM
from tensorflow.python.keras import backend as K
from tensorflow.keras import Model
import tensorflow as tf
import numpy as np

lph = tf.Variable(False, dtype=tf.bool, name='lph')
g = tf.get_default_graph()
K._GRAPH_LEARNING_PHASES[g] = lph

repeat = RepeatVector(2)
lstm = LSTM(1, dropout=0.5, recurrent_dropout=0.5)

ph1 = tf.placeholder(tf.float32, shape=(None, 1), name='ph1')
h = repeat(ph1)
o1 = lstm(h)

with g.control_dependencies([lph.assign(True), tf.identity(o1)]):
  target1 = tf.stop_gradient(o1)

  ph2 = tf.placeholder(tf.float32, shape=(None, 1), name='ph2')
  h = repeat(ph2)
  o2 = lstm(h)
  
  with g.control_dependencies([lph.assign(False), tf.identity(o2)]):
    target2 = tf.stop_gradient(o2)

    ph3 = tf.placeholder(tf.float32, shape=(None, 1), name='ph3')
    h = repeat(ph3)
    o3 = lstm(h)
    
    loss = target1 + target2 - o2
    opt = tf.train.GradientDescentOptimizer(0.1)
    train = opt.minimize(loss)

sess = tf.Session()
K.set_session(sess)
sess.run(tf.global_variables_initializer())

data = np.ones((1,1))
sess.run([o1, o2, o3, train], feed_dict={ph1:data, ph2:data, ph3:data})
```
"
24383,Segmentation Fault running conv2d,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.12.0
- Python version: 3.6.7
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: CUDA 9.0 + cuDNN 7.3.1
- GPU model and memory: RTX 2070 8GB



**Running Example Convolutional Network Facing Segmentation Fault**

I installed the latest 4.10 Nvidia drivers (earlier drivers does not support new RTX cards), and downloaded and installed CUDA 9.0 (runfile) and cuDNN 7.3.1 (tarball) from Nvidia's website. I then installed tensorflow on a brand new miniconda 3 environment. I tried to run the mnist [convolutional model](https://github.com/tensorflow/models/blob/master/tutorials/image/mnist/convolutional.py) from the tutorial, but gets segmentation fault. 

```
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
2018-12-15 02:10:42.928599: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-12-15 02:10:43.065251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-12-15 02:10:43.065776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.71
pciBusID: 0000:1c:00.0
totalMemory: 7.76GiB freeMemory: 7.04GiB
2018-12-15 02:10:43.065794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-12-15 02:10:43.293803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-12-15 02:10:43.293835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2018-12-15 02:10:43.293843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2018-12-15 02:10:43.294030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6768 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:1c:00.0, compute capability: 7.5)
Initialized!
[1]    29907 segmentation fault (core dumped)  python convolutional.py
```
"
24380,TensorFlow Tutorial. TensorFlowLite example app crashes: Tensor shape does not match.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy S6
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.9
- Python version: 2.7
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source): 6
- CUDA/cuDNN version: 9.0/7.14
- GPU model and memory: GeForce GTX 1060 6GB


**Describe the current behavior**
I retrained a ssd_mobilenet_v1 as described in this [guide](https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193).
So i converted my model to .tflite as shown under ""Running on mobile with TensorFlow Lite""
When i try to run it on my Phone, the App just crashes. So i installed Android Studio and tried to run it from there. Now i got following error:
`Process: org.tensorflow.lite.demo, PID: 12449
    java.lang.IllegalArgumentException: Cannot copy between a TensorFlowLite tensor with shape [1, 10, 4] and a Java object with shape [1, 4, 4].
        at org.tensorflow.lite.Tensor.throwExceptionIfTypeIsIncompatible(Tensor.java:240)
        at org.tensorflow.lite.Tensor.copyTo(Tensor.java:116)
        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:152)
        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:216)
        at org.tensorflow.demo.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:194)
        at org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:249)
        at android.os.Handler.handleCallback(Handler.java:751)
        at android.os.Handler.dispatchMessage(Handler.java:95)
        at android.os.Looper.loop(Looper.java:154)
        at android.os.HandlerThread.run(HandlerThread.java:61)`

The important line, i think, is **Cannot copy between a TensorFlowLite tensor with shape [1, 10, 4] and a Java object with shape [1, 4, 4].**

How do i know what tensor shape is needed? Can i change that in the app? Or do i need to change my model in some way?

**Describe the expected behavior**

Running example app with retrained model as shown in the [guide](https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193)

**Code to reproduce the issue**
converted the model via:
`bazel run -c opt tensorflow/contrib/lite/toco:toco -- \`
`--input_file=$OUTPUT_DIR/tflite_graph.pb \`
`--output_file=$OUTPUT_DIR/detect.tflite \`
`--input_shapes=1,300,300,3 \`
`--input_arrays=normalized_input_image_tensor \`
`--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'  \`
`--inference_type=QUANTIZED_UINT8 \`
`--mean_values=128 \`
`--std_values=128 \`
`--change_concat_input_ranges=false \`
`--allow_custom_ops`

Changes in **DetectorActivity.java** to match my model and labels:
`private static final int TF_OD_API_INPUT_SIZE = 300;`
  `private static final boolean TF_OD_API_IS_QUANTIZED = true;`
  `private static final String TF_OD_API_MODEL_FILE = ""detectquant.tflite"";`
  `private static final String TF_OD_API_LABELS_FILE = ""file:///android_asset/icons4cats.txt"";`


"
24379,meta_graph_transform does not work with default checkpoint_path,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS High Sierra 10.13.4
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.10.1
- Python version: 3.6.5
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

```
import tensorflow as tf
from tensorflow.saved_model import simple_save
from tensorflow.saved_model import tag_constants
from tensorflow.contrib.meta_graph_transform.meta_graph_transform import (
    meta_graph_transform)

with tf.Session() as sess:
    meta_graph_def = tf.saved_model.loader.load(
        sess,
        [tag_constants.SERVING],
        ""1544816647"")

    new_meta_def = meta_graph_transform(
        base_meta_graph_def=meta_graph_def,
        input_names=[""Placeholder""],
        output_names=[""dnn/head/predictions/class_ids"", ""init_all_tables""],
        transforms=[""freeze_graph""],
        tags=[tag_constants.SERVING],
        checkpoint_path=None)

    assert new_meta_def.graph_def
    with tf.Graph().as_default() as graph:
        tf.import_graph_def(new_meta_def.graph_def)

        in_tensor = graph.get_tensor_by_name(""import/Placeholder:0"")
        out_tensor = graph.get_tensor_by_name(""dnn/head/predictions/class_ids:0"")
        simple_save(
            sess,
            ""./frozen"",
            inputs={""inputs"": in_tensor},
            outputs={""outputs"": out_tensor})
```
Error:
Can't load save_path when it is None."
24377,Error to flat the output of the session->Run C++,"**System information**
- Tensorflow version: 1.4
- Programming Language: C++
- Operational System: Windows 10
- CPU implementation

**Describe the current behavior**

I am trying to run an .pb file on C++. The objective is to run and get the prediction result. However, after the `session->Run` is performed, I try to make an flat of the output: `outputs[0].flat<float>()`. When i try to do the flat, the system execution is interrupted, the error are:` Check failed: dtype() == expected_dtype (9 vs. 1).`

To understand the result of the output, I tried to print the content of my output, but I cannot.

I hope that the estimation result is the tensor output, but I cannot read the result.

_### **___________THE ERROR ARE AT THE LAST LINE IN THE CODE___________**_

**Describe the expected behavior**

The expected behavior is Run the code and get the result of the estimation through of the `output `tensor.

**Code to reproduce the issue**

```
#include ""TFUtils.h""
#include <cstdio>
#include <cstdint>
#include <string>

#include ""tensorflow/cc/client/client_session.h""
#include ""tensorflow/cc/ops/standard_ops.h""
#include ""tensorflow/core/framework/tensor.h""
#include <tensorflow/core/public/session.h>
#include <tensorflow/core/protobuf/meta_graph.pb.h>

using namespace std;
using namespace tensorflow;
using tensorflow::Flag;
using tensorflow::Tensor;
using tensorflow::Status;
using tensorflow::string;
using tensorflow::int32;
using namespace std;

const char __DSDL_OCR_LETTERS[] = { 'S', 'o', 'f', 'a', 'r', 'h', 'd', 'u', 't', 'i', 'n',
'k', 'g', 'l', 'e', 's', 'm', '""', 'W', 'w', 'v', 'c', 'p', 'y', '.', 'P', ',', 'b',
'G', 'O', 'D', 'R', '!', 'T', 'F', 'H', 'j', '-', 'A', ':', 'I', '?', 'C', '2', ';',
'B','\'', 'Y', 'L', 'N', 'x', 'E', 'M', '1', '8', '3', '4', ' / ', '(', '9', '5', ')',
'0','U', 'q', 'K', 'z', '6', 'J', '7', 'V', '&', '#', 'X', 'Q', '*', 'Z', ' + ', ' | ', ' ' };


int main(int argc, char* argv[]) {

	string image = ""D:\\IMAGE_FOR_TEST_IN_C\\imAux13.png"";
	string graph =""models\\Mode_epoch_269.pb""; //D:\svn\image_thread\engines\dsir\bin\resources\dll\dl\models
									
	string labels =	""models\\ocr_model_labels.txt"";

	int32 input_width = 1940;
	string out_fn = ""features"";
	int target_len_TAMANHO = 23;
	int seq_len_TAMANHO = 454;
	int32 input_height = 128;
	float input_mean = 0;
	float input_std = 255;
	string input_layer = ""Inputs/inputs"";
	string output_layer = ""predictions/CTCBeamSearchDecoder"";

	bool self_test = false;
	string root_dir = ""D:\\svn\\image_thread\\engines\\dsir\\bin\\resources\\dll\\dl"";
	uint32_t sizeTransLenght = sizeof(__DSDL_OCR_LETTERS);


	std::vector<Flag> flag_list = {
		Flag(""inputs"", &image, ""image to be processed""),
		Flag(""out_fn"", &out_fn, ""name of the fn to save the features""),
		Flag(""graph"", &graph, ""graph to be executed""),
		Flag(""labels"", &labels, ""name of file containing labels""),
		Flag(""input_width"", &input_width, ""resize image to this width in pixels""),
		Flag(""input_height"", &input_height, ""resize image to this height in pixels""),
		Flag(""input_mean"", &input_mean, ""scale pixel values to this mean""),
		Flag(""input_std"", &input_std, ""scale pixel values to this std deviation""),
		Flag(""input_layer"", &input_layer, ""name of input layer""),
		Flag(""output_layer"", &output_layer, ""name of output layer""),
		Flag(""self_test"", &self_test, ""run a self test""),
		Flag(""root_dir"", &root_dir, ""interpret image and graph file names relative to this directory""),
	};

	cout << ""\n"" << argv[0] << ""\n"";
	string usage = tensorflow::Flags::Usage(argv[0], flag_list);
	const bool parse_result = tensorflow::Flags::Parse(&argc, argv, flag_list);

	if (!parse_result) {
		LOG(ERROR) << usage;
		return -1;
	}

	// We need to call this to set up global state for TensorFlow.
	tensorflow::port::InitMain(argv[0], &argc, &argv);
	if (argc > 1) {
		LOG(ERROR) << ""Unknown argument "" << argv[1] << ""\n"" << usage;
		return -1;
	}

	// First we load and initialize the model.
	std::unique_ptr<tensorflow::Session> session;
	string graph_path = tensorflow::io::JoinPath(root_dir, graph);
	Status load_graph_status = LoadGraph(graph_path, &session);
	if (!load_graph_status.ok()) {
		LOG(ERROR) << load_graph_status;
		return -1;
	}

	// Get the image from disk as a float array of numbers, resized and normalized
	// to the specifications the main graph expects.
	std::vector<Tensor> resized_tensors;
	string image_path = image; // tensorflow::io::JoinPath(root_dir, image);
	Status read_tensor_status =
		ReadTensorFromImageFile(image_path, input_height, input_width, input_mean,
			input_std, &resized_tensors);

	if (!read_tensor_status.ok()) {
		LOG(ERROR) << read_tensor_status;
		return -1;
	}
	const Tensor& resized_tensor = resized_tensors[0];

	Tensor targets_len(DT_INT32, TensorShape({}));
	targets_len.scalar<int32>()() = target_len_TAMANHO;

	Tensor seq_len(DT_INT32, TensorShape({1}));
	seq_len.scalar<int32>()() = seq_len_TAMANHO;

	int tamanho_batche = 16;
	Tensor batche_size(DT_INT32, TensorShape({}));
	batche_size.scalar<int32>()() = tamanho_batche;

	
	// Actually run the image through the model.
	std::vector<Tensor> outputs;
	Status run_status = session->Run({ { input_layer, resized_tensor },{ ""targets_len"", targets_len },{ ""seq_len"", seq_len } },
	{ output_layer }, {}, &outputs);

	if (!run_status.ok()) {
		LOG(ERROR) << ""Running model failed: "" << run_status;
		return -1;
	}

	FILE *filePtr;
	fopen_s(&filePtr, out_fn.c_str(), ""w"");
	auto flat = outputs[0].flat<float>();	//The error is here

```

**Other info / logs**

The erros are both bellow:

`Check failed: dtype() == expected_dtype (9 vs. 1)`

![image](https://user-images.githubusercontent.com/18532570/50023582-6fd68900-ffbe-11e8-8496-4e59a23006b6.png)

"
24376,Conv2d causes Process finished with exit code 139 (interrupted by signal 11: SIGSEGV),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1 LTS
- TensorFlow installed from (source or binary): source (conda)
- TensorFlow version (use command below): 1.12.0 
- Python version: 2.7
- CUDA/cuDNN version: 9.2/ 7.2.1
- GPU model and memory: RTX2070 8GB


**Describe the current behavior**

Process finished with exit code 139 (interrupted by signal 11: SIGSEGV);

**Describe the expected behavior**

Run a Convolutional Neural network with no error messages

**Code to reproduce the issue**
	# simplified example

	import tensorflow as tf
	mnist = tf.keras.datasets.mnist

	(x_train, y_train),(x_test, y_test) = mnist.load_data()
	x_train, x_test = x_train / 255.0, x_test / 255.0
	w, h = 28, 28

	# Reshape input data from (28, 28) to (28, 28, 1)
	x_train = x_train.reshape(x_train.shape[0], w, h, 1)
	x_test = x_test.reshape(x_test.shape[0], w, h, 1)

	# buggy example -> what is going on with conv2d?
	#==========================================================================
	model = tf.keras.models.Sequential([
	    tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28,28,1)), # error
	    tf.keras.layers.Flatten(),
	    tf.keras.layers.Dense(512, activation=tf.nn.relu),
	    tf.keras.layers.Dropout(0.2),
	    tf.keras.layers.Dense(10, activation=tf.nn.softmax)
	])
	===========================================================================
	model.compile(optimizer='adam',
		      loss='sparse_categorical_crossentropy',
		      metrics=['accuracy'])

	model.fit(x_train, y_train, epochs=5)
	model.evaluate(x_test, y_test)


**Other info / logs**
The error appears, when Conv2D layer is used; if i uncomment likeso:

	   #tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', 

everythink works. 

More complicated examples like
https://colab.research.google.com/github/margaretmz/deep-learning/blob/master/fashion_mnist_keras.ipynb

fail with the same error message."
24375,Set different activation in CuDNNLSTM,"Can we add an ""activation"" input arg to CuDNNLSTM where uses can set different activation functions? Basically similar to ""activation"" input arg in LSTM. 
I see **[this](https://github.com/tensorflow/tensorflow/issues/23297)** but it has been closed (@harshini-gadige)."
24374,tf.einsum doesn't compute the trace correctly,"**System information**
- Have I written custom code: yes
- OS Platform and Distribution: MacOS Mojave 10.14.1
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12
- Python version: 3.6.6

**Describe the current behavior**
The formula to compute the trace of a matrix with `einsum` computes instead the sum of all the elements of the matrix.

**Describe the expected behavior**
The result should the the sum of the elements on the diagonal (compare with `np.einsum('ii', matrix)`)

**Code to reproduce the issue**
```python
import tensorflow as tf

matrix = tf.constant([[1, 2, 3],
                      [4, 5, 6],
                      [7, 8, 9]])

with tf.Session() as sess:
    print(tf.einsum('ii',matrix).eval())

# 45 (should be 15)
```

"
24373,Branch 1.13 build fails with undefined symbol: _ZNSaIPKN3xla14HloInstructionEEC1Ev,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 28
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): source 
- TensorFlow version: 1.13
- Python version: 3.6.7
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source):  0.19.2
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: 9.2 / 7.1.4
- GPU model and memory: Quadro M2200

```
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
```

fails:

```
  File ""/home/key/.cache/bazel/_bazel_key/8de93c72df6c3fec3e289f10fadff72b/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/key/.cache/bazel/_bazel_key/8de93c72df6c3fec3e289f10fadff72b/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/key/.cache/bazel/_bazel_key/8de93c72df6c3fec3e289f10fadff72b/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/key/anaconda3/envs/tf-13-1213/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/key/anaconda3/envs/tf-13-1213/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: /home/key/.cache/bazel/_bazel_key/8de93c72df6c3fec3e289f10fadff72b/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZNSaIPKN3xla14HloInstructionEEC1Ev

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/key/.cache/bazel/_bazel_key/8de93c72df6c3fec3e289f10fadff72b/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 27, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/home/key/.cache/bazel/_bazel_key/8de93c72df6c3fec3e289f10fadff72b/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/key/.cache/bazel/_bazel_key/8de93c72df6c3fec3e289f10fadff72b/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/key/.cache/bazel/_bazel_key/8de93c72df6c3fec3e289f10fadff72b/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/key/.cache/bazel/_bazel_key/8de93c72df6c3fec3e289f10fadff72b/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/key/.cache/bazel/_bazel_key/8de93c72df6c3fec3e289f10fadff72b/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/key/anaconda3/envs/tf-13-1213/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/key/anaconda3/envs/tf-13-1213/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: /home/key/.cache/bazel/_bazel_key/8de93c72df6c3fec3e289f10fadff72b/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZNSaIPKN3xla14HloInstructionEEC1Ev


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 3986.489s, Critical Path: 241.90s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]
INFO: 5801 processes: 5801 local.
FAILED: Build did NOT complete successfully
```"
24372,TF RPI build fails with AWS lib related linker errors,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux rpi3-0 4.14.79-v7+ #1159 SMP Sun Nov 4 17:50:20 GMT 2018 armv7l GNU/Linux

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
n/a

- TensorFlow installed from (source or binary):
Source

- TensorFlow version:
v1.12.0

- Python version:
Python 2.7.13

- Installed using virtualenv? pip? conda?:
nope

- Bazel version (if compiling from source):
$ bazel version
WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
/home/pi/gocode/tensorflow/tools/bazel.rc
INFO: Invocation ID: a32f1f82-b910-4312-b73c-335b3090412a
Build label: 0.20.0- (@non-git)
Build target: bazel-out/arm-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Dec 13 03:56:33 2018 (1544673393)
Build timestamp: 1544673393
Build timestamp as int: 1544673393

- GCC/Compiler version (if compiling from source):
gcc (Raspbian 4.8.5-4) 4.8.5

- CUDA/cuDNN version:
n/a

- GPU model and memory:
n/a

**Describe the problem**
My use case is running a binary, on Raspberry Pi, serving a TF model that has dynamic dependency on libtensorflow.so. Since I could not find pre-build TF C-library for RPI I tried to build it from source. After much pain and suffering I was able to get the code to compile, but saw linker errors related to AWS library. It would be great if users could download a pre-build library but in the mean time how can I resolve these errors. I do not need cloud support in TF, so if easier I would simply like to switch off such functionality.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

- Built proto (success)
- Build bazel (success)
- Check out v1.12.0 for TF code

Manually edit files with these commands:
- grep -Rl 'lib64' | xargs sed -i 's/lib64/lib/g'
- sed -i ""s|#define IS_MOBILE_PLATFORM|//#define IS_MOBILE_PLATFORM|g"" tensorflow/core/platform/platform.h

Install a few after seeing compile issues
- sudo apt-get install gcc-4.8 g++-4.8
- sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-4.8 100
- sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-4.8 100
- sudo apt-get install libc-ares-dev

No compile issues at this point after I added following flags to bazel command: `--copt=""-std=gnu99"" --define=grpc_no_ares=true`. So I think `sudo apt-get install libc-ares-dev` is probably not required.

Anyways, I then ran following command to build TF c-library:
`bazel build -c opt --copt=""-mfpu=neon-vfpv4"" --copt=""-funsafe-math-optimizations"" --copt=""-ftree-vectorize"" --copt=""-fomit-frame-pointer"" --jobs 1 --local_resources 1024,1.0,1.0 --verbose_failures --genrule_strategy=standalone --spawn_strategy=standalone --incompatible_remove_native_http_archive=false --copt=""-std=gnu99"" --define=grpc_no_ares=true //tensorflow:libtensorflow.so`


**Any other info / logs**
It took 10 hours but all the code compiled fine, however, I ran into linker issues as follows:
```
ERROR: /home/pi/gocode/tensorflow/tensorflow/BUILD:423:1: Linking of rule '//tensorflow:libtensorflow.so' failed (Exit 1): gcc failed: error executing command
  (cd /home/pi/.cache/bazel/_bazel_pi/e36eb5edf9626338e60e5f0e9f0b5230/execroot/org_tensorflow && \
  exec env - \
    PATH=/home/pi/go/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/games:/usr/games \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
    TF_DOWNLOAD_CLANG=0 \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL_SYCL=0 \
    TF_NEED_ROCM=0 \
  /usr/bin/gcc -shared -o bazel-out/arm-opt/bin/tensorflow/libtensorflow.so -z defs -Wl,--version-script tensorflow/c/version_script.lds '-Wl,-rpath,$ORIGIN/' -Wl,-soname,libtensorflow.so -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread '-fuse-ld=gold' -Wl,-no-as-needed -Wl,-z,relro,-z,now -B/usr/bin -pass-exit-codes -Wl,--gc-sections -Wl,@bazel-out/arm-opt/bin/tensorflow/libtensorflow.so-2.params)
Execution platform: @bazel_tools//platforms:host_platform
bazel-out/arm-opt/bin/external/aws/_objs/aws/AWSCredentialsProvider.pic.o:AWSCredentialsProvider.cpp:function Aws::Auth::EnvironmentAWSCredentialsProvider::GetAWSCredentials(): error: undefined reference to 'Aws::Environment::GetEnv(char const*)'
bazel-out/arm-opt/bin/external/aws/_objs/aws/AWSCredentialsProvider.pic.o:AWSCredentialsProvider.cpp:function Aws::Auth::EnvironmentAWSCredentialsProvider::GetAWSCredentials(): error: undefined reference to 'Aws::Environment::GetEnv(char const*)'
bazel-out/arm-opt/bin/external/aws/_objs/aws/AWSCredentialsProvider.pic.o:AWSCredentialsProvider.cpp:function Aws::Auth::EnvironmentAWSCredentialsProvider::GetAWSCredentials(): error: undefined reference to 'Aws::Environment::GetEnv(char const*)'
bazel-out/arm-opt/bin/external/aws/_objs/aws/AWSCredentialsProvider.pic.o:AWSCredentialsProvider.cpp:function Aws::Auth::ProfileConfigFileAWSCredentialsProvider::GetConfigProfileFilename(): error: undefined reference to 'Aws::FileSystem::GetHomeDirectory()'
bazel-out/arm-opt/bin/external/aws/_objs/aws/AWSCredentialsProvider.pic.o:AWSCredentialsProvider.cpp:function Aws::Auth::ProfileConfigFileAWSCredentialsProvider::GetCredentialsProfileFilename(): error: undefined reference to 'Aws::Environment::GetEnv(char const*)'
bazel-out/arm-opt/bin/external/aws/_objs/aws/AWSCredentialsProvider.pic.o:AWSCredentialsProvider.cpp:function Aws::Auth::ProfileConfigFileAWSCredentialsProvider::GetCredentialsProfileFilename(): error: undefined reference to 'Aws::FileSystem::GetHomeDirectory()'
bazel-out/arm-opt/bin/external/aws/_objs/aws/ClientConfiguration.pic.o:ClientConfiguration.cpp:function Aws::Client::ClientConfiguration::ClientConfiguration(): error: undefined reference to 'Aws::OSVersionInfo::ComputeOSVersionString()'
bazel-out/arm-opt/bin/external/aws/_objs/aws/DateTimeCommon.pic.o:DateTimeCommon.cpp:function Aws::Utils::DateTime::ConvertTimestampStringToTimePoint(char const*, Aws::Utils::DateFormat): error: undefined reference to 'Aws::Time::TimeGM(tm*)'
bazel-out/arm-opt/bin/external/aws/_objs/aws/DateTimeCommon.pic.o:DateTimeCommon.cpp:function Aws::Utils::DateTime::ConvertTimestampToLocalTimeStruct() const: error: undefined reference to 'Aws::Time::LocalTime(tm*, long)'
bazel-out/arm-opt/bin/external/aws/_objs/aws/DateTimeCommon.pic.o:DateTimeCommon.cpp:function Aws::Utils::DateTime::ConvertTimestampToGmtStruct() const: error: undefined reference to 'Aws::Time::GMTime(tm*, long)'
bazel-out/arm-opt/bin/external/aws/_objs/aws/TempFile.pic.o:TempFile.cpp:function .LTHUNK5: error: undefined reference to 'Aws::FileSystem::RemoveFileIfExists(char const*)'
bazel-out/arm-opt/bin/external/aws/_objs/aws/TempFile.pic.o:TempFile.cpp:function Aws::Utils::ComputeTempFileName(char const*, char const*): error: undefined reference to 'Aws::FileSystem::CreateTempFilePath()'
collect2: error: ld returned 1 exit status
Target //tensorflow:libtensorflow.so failed to build
INFO: Elapsed time: 173.724s, Critical Path: 126.80s
INFO: 0 processes.
FAILED: Build did NOT complete successfully
```

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24371,FailedPreconditionError when using callbacks,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
This is a modification of the MNIST tutorial. I have included callbacks for TensorBoard and also for a custom history callback, though that callback has been reduced to a no-op.

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10 (Build 17134.471))
Spyder 3.2.8

- TensorFlow installed from (source or binary):  pip install TensorFlow

- TensorFlow version (use command below):  1.12.0
- Python version:  3.6.5; Qt 5.9.4, PyQt5 5.9.2 on Windows
- GPU model and memory:  GeForce GTX 1070 Ti, 8G RAM

**Describe the current behavior**
When I include callbacks to TensorBoard and a custom History callback, I get a FailedPreconditionError.

I was getting a similar error when just trying to use TensorBoard.  Changing the import statement to be more specific (from tensorflow.keras.callbacks import TensorBoard) fixed that one, but then I added the history callback, and it is unhappy again.

**Describe the expected behavior**
I include callbacks and do not get an error.

**Code to reproduce the issue**

import os

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.callbacks import TensorBoard
from keras import backend as K

class LossLearningRateScheduler(keras.callbacks.History):
    """"""
    *** By fergosuci on Kaggle.com ****
    
    A learning rate scheduler that relies on changes in loss function
    """"""
    def __init__(self):
        super(LossLearningRateScheduler, self).__init__()

    def on_epoch_begin(self, epoch, logs=None):       
        return K.get_value(self.model.optimizer.lr)

fashion_mnist = keras.datasets.fashion_mnist

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

train_images = train_images / 255.0
test_images = test_images / 255.0

model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(128),
    keras.layers.BatchNormalization(),
    keras.layers.Activation('relu'),
    keras.layers.Dense(10, activation=tf.nn.softmax)
])

model.compile(optimizer=keras.optimizers.SGD(), 
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

runNumber = len(os.listdir('logs'))
tensorboard = TensorBoard(log_dir=f""logs/run_{runNumber}"", write_grads=True, write_graph=False)
learningRateScheduler = LossLearningRateScheduler()

model.fit(train_images, train_labels, epochs=5, callbacks=[tensorboard, learningRateScheduler])


**Other info / logs**
Traceback (most recent call last):

  File ""<ipython-input-7-52fc1b0c66af>"", line 1, in <module>
    runfile('D:/Dev/Training/TensorFlow Tutorials/MNIST Fashion/ReproduceIssue.py', wdir='D:/Dev/Training/TensorFlow Tutorials/MNIST Fashion')

  File ""D:\dev\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 705, in runfile
    execfile(filename, namespace)

  File ""D:\dev\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 102, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""D:/Dev/Training/TensorFlow Tutorials/MNIST Fashion/ReproduceIssue.py"", line 44, in <module>
    model.fit(train_images, train_labels, epochs=5, callbacks=[tensorboard, learningRateScheduler])

  File ""D:\dev\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1639, in fit
    validation_steps=validation_steps)

  File ""D:\dev\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training_arrays.py"", line 215, in fit_loop
    outs = f(ins_batch)

  File ""D:\dev\Anaconda3\lib\site-packages\tensorflow\python\keras\backend.py"", line 2986, in __call__
    run_metadata=self.run_metadata)

  File ""D:\dev\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1439, in __call__
    run_metadata_ptr)

  File ""D:\dev\Anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 528, in __exit__
    c_api.TF_GetCode(self.status.status))

FailedPreconditionError: Error while reading resource variable training_5/SGD/Variable_2 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/training_5/SGD/Variable_2/class tensorflow::Var does not exist.
	 [[{{node training_5/SGD/mul_4/ReadVariableOp}} = ReadVariableOp[dtype=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](training_5/SGD/Variable_2)]]"
24370,Use retrained graph with tensorflow in C,"Hi guys I'm trying to use retrained graph created with image retraining with C api. I retrainded with bitmap image with size 444x440. The C code that i wrote is the following:

```
#include ""neural_network.h""

#define GRAPH_NAME (""retrained_graph.pb"")
#define IN_NAME (""Placeholder"")
#define OUT_NAME (""final_result"")

// Change this or array data with less useless name
typedef unsigned int data_t;

void free_buffer(void *data, size_t length) { free(data); }

/* Read file containing the graph of the neural network and initialize the
 * TF_Buffer
 */
TF_Buffer *read_file(const char *file) {
    FILE *f = fopen(file, ""rb"");
    fseek(f, 0, SEEK_END);
    long fsize = ftell(f);
    fseek(f, 0, SEEK_SET);

    void *data = malloc(fsize);
    fread(data, fsize, 1, f);
    fclose(f);

    TF_Buffer *buf = TF_NewBuffer();
    buf->data = data;
    buf->length = fsize;
    buf->data_deallocator = free_buffer;
    return buf;
}

// import the graph in the main graph
void import_graph(TF_Graph *graph, TF_Status *status) {
    TF_Buffer *graph_def = read_file(GRAPH_NAME);

    TF_ImportGraphDefOptions *opts = TF_NewImportGraphDefOptions();
    TF_GraphImportGraphDef(graph, graph_def, opts, status);
    TF_DeleteImportGraphDefOptions(opts);
    // check status
    if (TF_GetCode(status) != TF_OK) {
        fprintf(stderr, ""ERROR: Unable to import graph %s"", TF_Message(status));
        return;
    }

    TF_DeleteBuffer(graph_def);
}

static void Deallocator(void *data, size_t length, void *arg) {}

TF_Code run_session(TF_Graph *graph, TF_Status *status, data_t *data) {
    float *result;

    // Number of bytes of input
    const int nb_in = EL_W * ACT_IMG_H * sizeof(data_t);

    // Number of bytes of output
    const int nb_out = 3 * sizeof(float);

    // Input dimensions
    int64_t in_dims[] = {1, EL_W, ACT_IMG_H, 0};
    int n_in_dims = sizeof(in_dims) / sizeof(int64_t);

    // Output dimensions
    int64_t out_dims[] = {1, 3};
    int n_out_dims = sizeof(out_dims) / sizeof(int64_t);

    TF_Output input_op = {TF_GraphOperationByName(graph, IN_NAME), 0};

    TF_Tensor *input_tensor = TF_NewTensor(TF_FLOAT, in_dims, n_in_dims, data,
                                           nb_in, &Deallocator, 0);

    TF_Output output = {TF_GraphOperationByName(graph, OUT_NAME), 0};

    TF_Tensor *output_values =
        TF_AllocateTensor(TF_FLOAT, out_dims, n_out_dims, nb_out);

    TF_SessionOptions *sess_opts = TF_NewSessionOptions();
    TF_Session *session = TF_NewSession(graph, sess_opts, status);

    TF_SessionRun(session,
                  NULL,                        // Run options
                  &input_op, &input_tensor, 1, // in: tensor, values, number
                  &output, &output_values, 1,  // out: tensor, vlaues, number
                  NULL, 0,                     // target operation, num targets
                  NULL,                        // metadata
                  status                       // outputs status
    );

    // result = TF_TensorData(output_values);
    // printf(""%f %f %f\n"", result[0], result[1], result[2]);

    printf(""%s\n"", TF_Message(status));
    return TF_GetCode(status);
}

void image_linearization(data_t *data) {
    BITMAP *image;
    acquire_screen();
    image = create_sub_bitmap(screen, IMG_XT, IMG_YT, EL_W, ACT_IMG_H);

    ssize_t x;
    ssize_t line;

    for (line = 0; line < image->h; ++line)
        for (x = 0; x < image->w; ++x) {
            data[line * image->h + x] = ((data_t *)image->line[line])[x];
        }

    destroy_bitmap(image);
    release_screen();
}

void *neural_network_task(void *period) {
    struct timespec t;
    data_t data[EL_W * ACT_IMG_H];

    TF_Graph *graph = TF_NewGraph();
    TF_Status *status = TF_NewStatus();

    import_graph(graph, status);

    set_activation(&t, *(int *)period);

    /*
    TF_Operation *decode = TF_GraphOperationByName(graph, IN_NAME);
    printf(""P: %p\n"", decode);
    printf(""NO: %i\n"", TF_OperationNumOutputs(decode));
    printf(""NI: %i\n"", TF_OperationNumInputs(decode));

    decode = TF_GraphOperationByName(graph, OUT_NAME);
    printf(""P: %p\n"", decode);
    printf(""NO: %i\n"", TF_OperationNumOutputs(decode));
    printf(""NI: %i\n"", TF_OperationNumInputs(decode));
    */

    while (1) {
        image_linearization(data);

        printf(""%i %s\n"", run_session(graph, status, data));
        // TF_Message(status));
        wait_for_activation(&t, *(int *)period);
    }
}
```

I had linearized the bitmap with the function and placed it into array with ""width*height"" element, then i following some example for creating tensor and run session but it didn't works. The tensor it's just one and the output tensor it's for 3 type of class.

The error that tensorflow gives me it's the following:

```
Generic conv implementation does not support grouped convolutions for now.
         [[{{node module_apply_default/InceptionV3/InceptionV3/Conv2d_1a_3x3/BatchNorm/FusedBatchNorm/Mul}} = Conv2D[T=DT_FLOAT, data_format=""NHWC"", dilations=[1, 1, 1, 1], padding=""VALID"", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](module_apply_default/hub_input/Sub, module_apply_default/InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D/merged_input)]]
```

How i can fix it?"
24369,ValueError: No variables to save,"**System information**
- Have I written custom code  : No
- OS Platform and Distribution : Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12
- Python version: Python3.5
- Bazel version (if compiling from source): Bazel-0.18.0

I have used tf.keras to build a model and set keras backend session as sess where sess=tf.Session()

Saved the graph_def file using below function 
tf.io.write_graph(sess.graph_def, '/tmp/model', 'model _graph.pbtxt')

Generated ckpt file using callbacks present in tensorflow as follows
cp_callback=keras.callbacks.ModelCheckpoint(checkpoint_path, verbose=1, save_weights_only=True, period=1)

When I am trying to generate frozen file using bazel I am getting below error

Traceback (most recent call last):
  File ""/root/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 491, in <module>
    run_main()
  File ""/root/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 488, in run_main
    app.run(main=my_main, argv=[sys.argv[0]] + unparsed)
  File ""/root/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/root/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 487, in <lambda>
    my_main = lambda unused_args: main(unused_args, flags)
  File ""/root/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 381, in main
    flags.saved_model_tags, checkpoint_version)
  File ""/root/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 363, in freeze_graph
    checkpoint_version=checkpoint_version)
  File ""/root/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 190, in freeze_graph_with_def_protos
    var_list=var_list, write_version=checkpoint_version)
  File ""/root/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py"", line 1123, in __init__
    self.build()
  File ""/root/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py"", line 1135, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/root/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py"", line 1160, in _build
    raise ValueError(""No variables to save"")
ValueError: No variables to save


How to solve this ValueError ? "
24368,Advanced Tensorflow Neural Network visualization,"I have gone through the beautiful tool given by Tensorflow. I am talking about the Playground. The tool is available at https://github.com/tensorflow/playground.
Even though it an excellent tool but it doesn't help in analyzing the datasets given by the user. It takes its own 2D random numbers and let the people play with it. 
My request for feature is that I want to have a similar tool but it should let the user use their datasets for analyzing the neural network offered by tensorflow.

**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): Yes ofcourse. if I get cleared with how the tool is working. I tried hard to understand but the tool has many hard coded values and is making is difficult to understand.

**Describe the feature and the current behavior/state.**

**Will this change the current api? How?** Never.

**Who will benefit with this feature?**

Everyone who is planning to create a machine learning model and want to start with Tensorflow. The newbie or even Researchers will be benefited by this tool. The visualization will give them a clear idea without any pain.

**Any Other info.**
There isn't anything yet. If there is any in future I will add."
24367,"Exception ignored, Session closed","**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Arch Linux
- TensorFlow installed from (source or binary): Official repository
- TensorFlow version: v1.11.0-0-gc19e29306c 1.11.0
- Python version: 3.6.6
- GPU model: AMD Radeon HD 7850


**Describe the current behavior (+ code)**
Code: found in my second comment below.
It uses the environment `AcrobotForever-v1` which is really just a modification of the Acrobot env that never terminates. (additionally, I made it have 4 state variables: `(angle 1, angle 2, angle_vel 1, angle_vel 2)`. Code for the modified environment: https://github.com/Ploppz/gym-custom-envs

My code defines an agent acting in the Acrobot environment. It has one neural networks which it fits and uses to predict the next state.

While training I get this error after some random amount of time:

```
Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x7f1ec3fcd9e8>>
Traceback (most recent call last):
  File ""/home/ploppz/.pyenv/versions/3.6.6/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1415, in __del__
    self._session._session, self._handle, status)
  File ""/home/ploppz/.pyenv/versions/3.6.6/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 526, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.CancelledError: Session has been closed.
Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x7f1ec3497e10>>
Traceback (most recent call last):
  File ""/home/ploppz/.pyenv/versions/3.6.6/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1415, in __del__
    self._session._session, self._handle, status)
  File ""/home/ploppz/.pyenv/versions/3.6.6/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 526, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.CancelledError: Session has been closed.
Iter 5, timesteps=6000: coverage = 6.2595697434743025
Iter 6, timesteps=6000: coverage = 5.409617917650649
Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x7f1ec3670da0>>
Traceback (most recent call last):
  File ""/home/ploppz/.pyenv/versions/3.6.6/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1415, in __del__
    self._session._session, self._handle, status)
  File ""/home/ploppz/.pyenv/versions/3.6.6/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 526, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.CancelledError: Session has been closed.
```
"
24365,Tensor flow installation problem : Mac,"I have Mac 10.12 Sierra, My python version is 2.7. 

Installation steps are. 
```
$source ~/tensorflow/bin/activate
(tensorflow)$ pip install tensorflow
Requirement already satisfied: tensorflow in /usr/local/lib/python2.7/site-packages (1.12.0)
Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/site-packages (from tensorflow) (1.11.0)
Requirement already satisfied: protobuf>=3.6.1 in /usr/local/Cellar/protobuf/3.6.1/libexec/lib/python2.7/site-packages (from tensorflow) (3.6.1)
Requirement already satisfied: enum34>=1.1.6 in /usr/local/lib/python2.7/site-packages (from tensorflow) (1.1.6)
Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python2.7/site-packages (from tensorflow) (0.2.0)
Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python2.7/site-packages (from tensorflow) (1.0.5)
Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/site-packages (from tensorflow) (0.7.1)
Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/site-packages (from tensorflow) (1.17.0)
Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python2.7/site-packages (from tensorflow) (1.0.6)
Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/site-packages (from tensorflow) (2.0.0)
Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python2.7/site-packages (from tensorflow) (1.15.0)
Requirement already satisfied: tensorboard<1.13.0,>=1.12.0 in /usr/local/lib/python2.7/site-packages (from tensorflow) (1.12.0)
Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/site-packages (from tensorflow) (1.1.0)
Requirement already satisfied: wheel in ./python2.7/site-packages (from tensorflow) (0.32.3)
Requirement already satisfied: backports.weakref>=1.0rc1 in /usr/local/lib/python2.7/site-packages (from tensorflow) (1.0.post1)
Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python2.7/site-packages (from tensorflow) (0.6.1)
Requirement already satisfied: setuptools in ./python2.7/site-packages (from protobuf>=3.6.1->tensorflow) (40.6.3)
Requirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/site-packages (from grpcio>=1.8.6->tensorflow) (3.2.0)
Requirement already satisfied: h5py in /usr/local/lib/python2.7/site-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)
Requirement already satisfied: funcsigs>=1; python_version < ""3.3"" in /usr/local/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow) (1.0.2)
Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow) (4.2.0)
Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python2.7/site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow) (0.14.1)
Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow) (3.0.1)

(tensorflow)$python
```

```
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/Cellar/python@2/2.7.15_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/usr/local/Cellar/python@2/2.7.15_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 51, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/Cellar/python@2/2.7.15_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/Cellar/python@2/2.7.15_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/Cellar/python@2/2.7.15_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/Cellar/python@2/2.7.15_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: dlopen(/usr/local/Cellar/python@2/2.7.15_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 10): Library not loaded: @rpath/libcublas.8.0.dylib
  Referenced from: /usr/local/Cellar/python@2/2.7.15_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
  Reason: image not found


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>> 
```


Please help me to install it over mac."
24364,Block Matrix / Block Diagonal Matrix,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): python 1.12.0
- Are you willing to contribute it (Yes/No): I would but I have very little knowledge of the TF internals



**Describe the feature and the current behavior/state.**
I am requesting the development of a block diagonal function for tensors. In `scipy` this is included in the `scipy.linalg.block_diag` function. Converted to tensors, it makes sense that given a `[M, K, X, ...]` tensor, when calling the proposed `tf.block_diag` it would return a `[M*K, M*K, X, ...]` matrix with the diagonal elements containing the elements from the `[K, X, ...]` submatrix. This seems to be implemented as a LinearOperator but it is not conducive to the tensor-level functions that exist for the rest of the linear algebra domain.

**Will this change the current api? How?**
No, this should not change current api, only add an extra function

**Who will benefit with this feature?**
I will benefit from this feature, as well as anyone else trying to convert code from numpy/scipy to TF. It is a seemingly standard operator in linear algebra and so it makes sense for it to exist in Tensorflow.

**Any Other info.**
I have a cludgy working version that uses padding and concatenation
Given Tensor `input` of shape `(B, K, K, ...)` will return a Tensor of shape `(K*B, K*B)` zero filled except for the `KxK` diagonal elements
```python
import tensorflow as tf
B, K, = input.shape[:2]
paddings = tf.constant([[0, (B-1)*K)],[0, 0]])
diag = tf.pad(input[0], paddings)
for i in range(1, B):
    paddings = tf.constant([[i*K, (B-i-1)*K],[0, 0]])
    zeros = tf.pad(input[i], paddings)
    diag = tf.concat([diag, zeros], 1)
```
"
24362,tf.cumsum low performance in 1.12.0,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 1803 Build 17134.407
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.12.0-gpu
- Python version: 3.6.0
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: v9.0.176
- GPU model and memory: GTX-970M, NVIDIA-SMI 385.54

**Describe the current behavior**
In tensorflow-1.12.0-GPU, it takes around 0.8s for each iteration and the cumsum operation takes around 0.13s/run x 3class x 2run/class. So the cumsum operation nearly takes 0.78s for each iteration. After switching the code interpreter to python 3.6.0 with tensorflow-1.11.0-CPU, the iteration time decreases to around 0.14s and the highest time-consuming operation become the sorting operation.
I notice that this issue has been reported in [https://github.com/tensorflow/tensorflow/issues/19570](url). I wonder whether this issue has been fixed in version 1.12.0? Or is there something wrong in my code caused this issue? I'm still learning how to use tensorflow recently. The lovasz_loss imported can be found in [https://github.com/bermanmaxim/LovaszSoftmax/blob/master/tensorflow/lovasz_losses_tf.py](url)

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
`
import tensorflow as tf
import numpy as np
import time
import lovasz_loss
from tensorflow.python.client import timeline

print(tf.__version__)

IMAGE_SIZE = 512
IMAGE_CHANNEL = 2
CLASS_NUM = 3
IMAGE_NUM = 1
X_train = np.random.randn(IMAGE_NUM,IMAGE_SIZE,IMAGE_SIZE,IMAGE_CHANNEL)
y_train = np.random.randint(0,CLASS_NUM,size=[IMAGE_NUM,IMAGE_SIZE,IMAGE_SIZE],dtype=np.int32)

X = tf.placeholder(dtype=tf.float32,shape=[None,IMAGE_SIZE,IMAGE_SIZE,IMAGE_CHANNEL],name='image_input')
y = tf.placeholder(dtype=tf.int32,shape=[None,IMAGE_SIZE,IMAGE_SIZE],name='image_output')

w = tf.get_variable('w',initializer=tf.truncated_normal(shape=[1,1,IMAGE_CHANNEL,CLASS_NUM],stddev=0.02))
y_logits = tf.nn.conv2d(X,w,strides=[1,1,1,1],padding='SAME')
y_prob = tf.nn.softmax(y_logits,axis=3)

loss = lovasz_loss.lovasz_softmax(y_prob,y)
train_op = tf.train.AdamOptimizer(0.005).minimize(loss)

config = tf.ConfigProto()
config.gpu_options.allow_growth = True

run_option = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
run_metadata = tf.RunMetadata()

with tf.Session(config=config) as sess:
    tf.global_variables_initializer().run()
    tf.local_variables_initializer().run()
    for i in range(100):
        st = time.time()
        sess.run(train_op,feed_dict={X:X_train,y:y_train},options=run_option,run_metadata=run_metadata)
        if i>5:
            tl = timeline.Timeline(run_metadata.step_stats)
            ctf = tl.generate_chrome_trace_format()
            with open('timeline_lova.json','w') as f:
                f.write(ctf)
        print('Iter: {num}, duration: {sec}'.format(num=i,sec=time.time()-st))
`
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24359,ERROR: Installing Tensorflow GPU with CUDA 10.0 for python on Windows,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (Windows 10):
- TensorFlow installed from (source):
- TensorFlow version: 1.12
- Python version:3.6
- Installed using pip:
- Bazel version (0.20.0):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:Cuda 10.0 cuDNN 7.3.1
- GPU model and memory: GTX 1060 6GB



I'm installing tensorflowgpu from source and following a guide from https://www.python36.com/how-to-install-tensorflow-gpu-with-cuda-10-0-for-python-on-windows/.
i followed all instructions as far as i know, no errors are seen all through out the installation.

At step 12, i got an error log

Admin@Thesis-CCD MSYS /c/tensorflow/tensorflow
$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --config=monolithic
WARNING: The following configs were expanded more than once: [cuda, monolithic]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
WARNING: Option 'experimental_shortened_obj_file_path' is deprecated
Loading:
Loading: 0 packages loaded
ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
        File ""C:/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1447
                _create_local_cuda_repository(repository_ctx)
        File ""C:/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1187, in _create_local_cuda_repository
                _get_cuda_config(repository_ctx)
        File ""C:/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl"", line 911, in _get_cuda_config
                _cudnn_version(repository_ctx, cudnn_install_base..., ...)
        File ""C:/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl"", line 614, in _cudnn_version
                auto_configure_fail((""cuDNN version detected from %s...)))
        File ""C:/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl"", line 317, in auto_configure_fail
                fail((""\n%sCuda Configuration Error:%...)))

Cuda Configuration Error: cuDNN version detected from C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/include/cudnn.h (7.3.1) does not match TF_CUDNN_VERSION (7)
WARNING: Target pattern parsing failed.
ERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
        File ""C:/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1447
                _create_local_cuda_repository(repository_ctx)
        File ""C:/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1187, in _create_local_cuda_repository
                _get_cuda_config(repository_ctx)
        File ""C:/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl"", line 911, in _get_cuda_config
                _cudnn_version(repository_ctx, cudnn_install_base..., ...)
        File ""C:/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl"", line 614, in _cudnn_version
                auto_configure_fail((""cuDNN version detected from %s...)))
        File ""C:/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl"", line 317, in auto_configure_fail
                fail((""\n%sCuda Configuration Error:%...)))

Cuda Configuration Error: cuDNN version detected from C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/include/cudnn.h (7.3.1) does not match TF_CUDNN_VERSION (7)
INFO: Elapsed time: 0.245s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
FAILED: Build did NOT complete successfully (0 packages loaded)


ANY HELP ABOUT THIS?"
24358,"TFLite bazel compile error, std::round vs round","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.5 LTS Server
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): this repo
- TensorFlow version: this repo, last commit is on Dec 11 17:41:45
- Python version: 2.7.12
- Installed using virtualenv? pip? conda?: no 
- Bazel version (if compiling from source): 0.20.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: no
- GPU model and memory: no



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
followed this doc [demo_android.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/demo_android.md#build-the-source-code), when run `bazel build //tensorflow/lite/java/demo/app/src/main:TfLiteCameraDemo --cxxopt=--std=c++11 --verbose_failures`, I got an error says
```
no member named 'round' in namespace 'std'; did you mean simply 'round'?
  return std::round(x);
         ^~~~~~~~~~
         round
```

after Google searched, I think it's a c++11 problem and don't know why `--std=c++11` doesn't work. Some says should use `c++_static` instead of `gnustl_static` and I don't know how to config this in bazel




**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
ERROR: /home/ubuntu/deeplearning/tensorflow/tensorflow/lite/kernels/BUILD:158:1: C++ compilation of rule '//tensorflow/lite/kernels:builtin_op_kernels' failed (Exit 1): clang failed: error executing command 
  (cd /home/ubuntu/.cache/bazel/_bazel_ubuntu/98810ecd1430da6ed1ea4de676f87bcd/execroot/org_tensorflow && \
  exec env - \
    ANDROID_BUILD_TOOLS_VERSION=28.0.3 \
    ANDROID_NDK_API_LEVEL=16 \
    ANDROID_NDK_HOME=/home/ubuntu/Android/Sdk/ndk-bundle \
    ANDROID_SDK_API_LEVEL=28 \
    ANDROID_SDK_HOME=/home/ubuntu/Android/Sdk \
    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/ubuntu/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
    TF_DOWNLOAD_CLANG=0 \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL_SYCL=0 \
    TF_NEED_ROCM=0 \
  external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin/clang '-D__ANDROID_API__=16' -isystemexternal/androidndk/ndk/sysroot/usr/include/arm-linux-androideabi -gcc-toolchain external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64 -fpic -ffunction-sections -funwind-tables -fstack-protector-strong -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -no-canonical-prefixes -fno-integrated-as -target armv7-none-linux-androideabi '-march=armv7-a' '-mfloat-abi=softfp' '-mfpu=vfpv3-d16' -mthumb -Os -g -DNDEBUG -MD -MF bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/kernels/_objs/builtin_op_kernels/conv.d '-frandom-seed=bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/kernels/_objs/builtin_op_kernels/conv.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -iquote . -iquote bazel-out/android-armeabi-v7a-opt/genfiles -iquote bazel-out/android-armeabi-v7a-opt/bin -iquote external/bazel_tools -iquote bazel-out/android-armeabi-v7a-opt/genfiles/external/bazel_tools -iquote bazel-out/android-armeabi-v7a-opt/bin/external/bazel_tools -iquote external/com_google_absl -iquote bazel-out/android-armeabi-v7a-opt/genfiles/external/com_google_absl -iquote bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl -iquote external/gemmlowp -iquote bazel-out/android-armeabi-v7a-opt/genfiles/external/gemmlowp -iquote bazel-out/android-armeabi-v7a-opt/bin/external/gemmlowp -iquote external/arm_neon_2_x86_sse -iquote bazel-out/android-armeabi-v7a-opt/genfiles/external/arm_neon_2_x86_sse -iquote bazel-out/android-armeabi-v7a-opt/bin/external/arm_neon_2_x86_sse -iquote external/androidndk -iquote bazel-out/android-armeabi-v7a-opt/genfiles/external/androidndk -iquote bazel-out/android-armeabi-v7a-opt/bin/external/androidndk -iquote external/eigen_archive -iquote bazel-out/android-armeabi-v7a-opt/genfiles/external/eigen_archive -iquote bazel-out/android-armeabi-v7a-opt/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/android-armeabi-v7a-opt/genfiles/external/local_config_sycl -iquote bazel-out/android-armeabi-v7a-opt/bin/external/local_config_sycl -iquote external/flatbuffers -iquote bazel-out/android-armeabi-v7a-opt/genfiles/external/flatbuffers -iquote bazel-out/android-armeabi-v7a-opt/bin/external/flatbuffers -iquote external/fft2d -iquote bazel-out/android-armeabi-v7a-opt/genfiles/external/fft2d -iquote bazel-out/android-armeabi-v7a-opt/bin/external/fft2d -iquote external/farmhash_archive -iquote bazel-out/android-armeabi-v7a-opt/genfiles/external/farmhash_archive -iquote bazel-out/android-armeabi-v7a-opt/bin/external/farmhash_archive -isystem external/eigen_archive -isystem bazel-out/android-armeabi-v7a-opt/genfiles/external/eigen_archive -isystem bazel-out/android-armeabi-v7a-opt/bin/external/eigen_archive -isystem tensorflow/lite/schema -isystem bazel-out/android-armeabi-v7a-opt/genfiles/tensorflow/lite/schema -isystem bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/schema -isystem external/flatbuffers/include -isystem bazel-out/android-armeabi-v7a-opt/genfiles/external/flatbuffers/include -isystem bazel-out/android-armeabi-v7a-opt/bin/external/flatbuffers/include -isystem external/farmhash_archive/src -isystem bazel-out/android-armeabi-v7a-opt/genfiles/external/farmhash_archive/src -isystem bazel-out/android-armeabi-v7a-opt/bin/external/farmhash_archive/src '--std=c++11' -DFARMHASH_NO_CXX_STRING '-mfpu=neon' '-mfloat-abi=softfp' '-std=c++11' -O3 -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -fno-rtti -DGOOGLE_PROTOBUF_NO_RTTI -DGOOGLE_PROTOBUF_NO_STATIC_INITIALIZER '-Wno-error=reorder' '--sysroot=external/androidndk/ndk/platforms/android-16/arch-arm' -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward -isystemexternal/androidndk/ndk/sysroot/usr/include -c tensorflow/lite/kernels/conv.cc -o bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/kernels/_objs/builtin_op_kernels/conv.o)
Execution platform: @bazel_tools//platforms:host_platform
In file included from tensorflow/lite/kernels/conv.cc:23:
./tensorflow/lite/c/builtin_op_data.h:154:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]
typedef struct {
        ^
./tensorflow/lite/c/builtin_op_data.h:157:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]
typedef struct {
        ^
./tensorflow/lite/c/builtin_op_data.h:227:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]
typedef struct {
        ^
./tensorflow/lite/c/builtin_op_data.h:230:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]
typedef struct {
        ^
./tensorflow/lite/c/builtin_op_data.h:269:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]
typedef struct {
        ^
In file included from tensorflow/lite/kernels/conv.cc:27:
In file included from ./tensorflow/lite/kernels/internal/optimized/multithreaded_conv.h:32:
In file included from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:37:
In file included from ./tensorflow/lite/kernels/internal/quantization_util.h:23:
./tensorflow/lite/kernels/internal/round.h:33:10: error: no member named 'round' in namespace 'std'; did you mean simply 'round'?
  return std::round(x);
         ^~~~~~~~~~
         round
external/androidndk/ndk/sysroot/usr/include/math.h:255:8: note: 'round' declared here
double round(double __x);
       ^
In file included from tensorflow/lite/kernels/conv.cc:26:
In file included from ./tensorflow/lite/kernels/gemm_support.h:18:
In file included from external/gemmlowp/public/gemmlowp.h:19:
In file included from external/gemmlowp/public/../internal/dispatch_gemm_shape.h:23:
In file included from external/gemmlowp/public/../internal/multi_thread_gemm.h:24:
In file included from external/gemmlowp/public/../internal/single_thread_gemm.h:26:
In file included from external/gemmlowp/public/../internal/compute.h:24:
In file included from external/gemmlowp/public/../internal/pack.h:430:
external/gemmlowp/public/../internal/pack_neon.h:303:46: warning: array index 1 is past the end of the array (which contains 1 element) [-Warray-bounds]
      int16x8_t sums8 = vpaddq_s16(sums4[0], sums4[1]);
                                             ^     ~
external/gemmlowp/public/../internal/pack.h:369:13: note: in instantiation of member function 'gemmlowp::PackingRegisterBlock<gemmlowp::SideMap<const unsigned char, gemmlowp::SideMapOrder::WidthMajor>, gemmlowp::PackedSideBlock<gemmlowp::KernelSideFormatInt8<gemmlowp::CellFormat<2, 16, gemmlowp::CellOrder::WidthMajor>, 1> > >::Pack' requested here
          b.Pack(packed_side_block_, start_width);
            ^
external/gemmlowp/public/../internal/pack.h:339:7: note: in instantiation of member function 'gemmlowp::PackSideBlockImpl<gemmlowp::SideMap<const unsigned char, gemmlowp::SideMapOrder::WidthMajor>, gemmlowp::PackedSideBlock<gemmlowp::KernelSideFormatInt8<gemmlowp::CellFormat<2, 16, gemmlowp::CellOrder::WidthMajor>, 1> > >::PackRun' requested here
      PackRun(start_width + w, ws, start_depth, depth);
      ^
external/gemmlowp/public/../internal/pack.h:328:9: note: in instantiation of member function 'gemmlowp::PackSideBlockImpl<gemmlowp::SideMap<const unsigned char, gemmlowp::SideMapOrder::WidthMajor>, gemmlowp::PackedSideBlock<gemmlowp::KernelSideFormatInt8<gemmlowp::CellFormat<2, 16, gemmlowp::CellOrder::WidthMajor>, 1> > >::PackL1' requested here
        PackL1(w, ws, d, ds);
        ^
external/gemmlowp/public/../internal/pack.h:424:8: note: in instantiation of member function 'gemmlowp::PackSideBlockImpl<gemmlowp::SideMap<const unsigned char, gemmlowp::SideMapOrder::WidthMajor>, gemmlowp::PackedSideBlock<gemmlowp::KernelSideFormatInt8<gemmlowp::CellFormat<2, 16, gemmlowp::CellOrder::WidthMajor>, 1> > >::PackL2' requested here
  impl.PackL2();
       ^
external/gemmlowp/public/../internal/multi_thread_gemm.h:661:5: note: in instantiation of function template specialization 'gemmlowp::PackRhs<gemmlowp::PackedSideBlock<gemmlowp::KernelSideFormatInt8<gemmlowp::CellFormat<2, 16, gemmlowp::CellOrder::WidthMajor>, 1> >, gemmlowp::MatrixMap<const unsigned char, gemmlowp::MapOrder::ColMajor> >' requested here
    PackRhs(&packed_rhs, rhs.block(0, c, depth, cs));
    ^
external/gemmlowp/public/../internal/dispatch_gemm_shape.h:182:3: note: in instantiation of function template specialization 'gemmlowp::MultiThreadGemm<gemmlowp::KernelFormat<gemmlowp::KernelSideFormatInt8<gemmlowp::CellFormat<4, 16, gemmlowp::CellOrder::WidthMajor>, 1>, gemmlowp::KernelSideFormatInt8<gemmlowp::CellFormat<2, 16, gemmlowp::CellOrder::WidthMajor>, 1> >, unsigned char, unsigned char, gemmlowp::BitDepthParams<gemmlowp::OperandRange<1, 255>, gemmlowp::OperandRange<0, 255> >, gemmlowp::MapOrder::RowMajor, gemmlowp::MapOrder::ColMajor, gemmlowp::MapOrder::RowMajor, gemmlowp::VectorDup<const int, gemmlowp::VectorShape::Row>, gemmlowp::VectorDup<const int, gemmlowp::VectorShape::Col>, std::tuple<gemmlowp::OutputStageBiasAddition<gemmlowp::VectorMap<const int, gemmlowp::VectorShape::Row> >, gemmlowp::OutputStageScaleInt32ByFixedPointAndExponent, gemmlowp::OutputStageClamp, gemmlowp::OutputStageSaturatingCastToUint8>, gemmlowp::GemmContext>' requested here
  MultiThreadGemm<typename Kernel::Format, InputScalar, OutputScalar,
  ^
external/gemmlowp/public/../internal/dispatch_gemm_shape.h:175:12: note: in instantiation of function template specialization 'gemmlowp::DispatchGemmShape<unsigned char, unsigned char, gemmlowp::BitDepthParams<gemmlowp::OperandRange<1, 255>, gemmlowp::OperandRange<0, 255> >, gemmlowp::MapOrder::RowMajor, gemmlowp::MapOrder::ColMajor, gemmlowp::MapOrder::RowMajor, gemmlowp::VectorDup<const int, gemmlowp::VectorShape::Row>, gemmlowp::VectorDup<const int, gemmlowp::VectorShape::Col>, std::tuple<gemmlowp::OutputStageBiasAddition<gemmlowp::VectorMap<const int, gemmlowp::VectorShape::Row> >, gemmlowp::OutputStageScaleInt32ByFixedPointAndExponent, gemmlowp::OutputStageClamp, gemmlowp::OutputStageSaturatingCastToUint8>, gemmlowp::GemmContext>' requested here
    return DispatchGemmShape<InputScalar, OutputScalar, BitDepthParams>(
           ^
external/gemmlowp/public/gemmlowp.h:63:3: note: in instantiation of function template specialization 'gemmlowp::DispatchGemmShape<unsigned char, unsigned char, gemmlowp::BitDepthParams<gemmlowp::OperandRange<1, 255>, gemmlowp::OperandRange<0, 255> >, gemmlowp::MapOrder::RowMajor, gemmlowp::MapOrder::ColMajor, gemmlowp::MapOrder::ColMajor, gemmlowp::VectorDup<const int, gemmlowp::VectorShape::Col>, gemmlowp::VectorDup<const int, gemmlowp::VectorShape::Row>, std::tuple<gemmlowp::OutputStageBiasAddition<gemmlowp::VectorMap<const int, gemmlowp::VectorShape::Col> >, gemmlowp::OutputStageScaleInt32ByFixedPointAndExponent, gemmlowp::OutputStageClamp, gemmlowp::OutputStageSaturatingCastToUint8>, gemmlowp::GemmContext>' requested here
  DispatchGemmShape<InputScalar, OutputScalar, BitDepthParams>(
  ^
./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:1082:13: note: in instantiation of function template specialization 'gemmlowp::GemmWithOutputPipeline<unsigned char, unsigned char, gemmlowp::BitDepthParams<gemmlowp::OperandRange<1, 255>, gemmlowp::OperandRange<0, 255> >, gemmlowp::MapOrder::RowMajor, gemmlowp::MapOrder::ColMajor, gemmlowp::MapOrder::ColMajor, std::tuple<gemmlowp::OutputStageBiasAddition<gemmlowp::VectorMap<const int, gemmlowp::VectorShape::Col> >, gemmlowp::OutputStageScaleInt32ByFixedPointAndExponent, gemmlowp::OutputStageClamp, gemmlowp::OutputStageSaturatingCastToUint8>, gemmlowp::GemmContext>' requested here
  gemmlowp::GemmWithOutputPipeline<uint8, uint8,
            ^
external/gemmlowp/public/../internal/pack_neon.h:297:5: note: array 'sums4' declared here
    int16x8_t sums4[Width / 2];
    ^
6 warnings and 1 error generated.
```"
24357,crash when run distributed training while using s3 tensorflow,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I wrote a custom estimator
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): tensorflow/tensorflow:1.11.0-gpu-py3 docker image
- TensorFlow version (use command below): tensorflow:1.11.0
- Python version: Python 3.5.2
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: cuda:9.0
- GPU model and memory: K80 (AWS EC2 P2.8xlarge instance)

I am using [s3 tensorflow](https://www.tensorflow.org/deploy/s3) for reading the data and writing models-checkpoints/logs 

**Other info / logs**
```
2018-12-14 01:07:09.544639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10755 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:0f.0, compute capability: 3.7)
2018-12-14 01:07:09.544952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10755 MB memory) -> physical GPU (device: 1, name: Tesla K80, pci bus id: 0000:00:10.0, compute capability: 3.7)
2018-12-14 01:07:09.545164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10755 MB memory) -> physical GPU (device: 2, name: Tesla K80, pci bus id: 0000:00:11.0, compute capability: 3.7)
2018-12-14 01:07:09.545382: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 10755 MB memory) -> physical GPU (device: 3, name: Tesla K80, pci bus id: 0000:00:12.0, compute capability: 3.7)
2018-12-14 01:07:09.545589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:4 with 10755 MB memory) -> physical GPU (device: 4, name: Tesla K80, pci bus id: 0000:00:13.0, compute capability: 3.7)
2018-12-14 01:07:09.545833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:5 with 10755 MB memory) -> physical GPU (device: 5, name: Tesla K80, pci bus id: 0000:00:14.0, compute capability: 3.7)
2018-12-14 01:07:09.546042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:6 with 10755 MB memory) -> physical GPU (device: 6, name: Tesla K80, pci bus id: 0000:00:15.0, compute capability: 3.7)
2018-12-14 01:07:09.546242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:7 with 10755 MB memory) -> physical GPU (device: 7, name: Tesla K80, pci bus id: 0000:00:16.0, compute capability: 3.7)
2018-12-14 01:07:09.546421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:8 with 10755 MB memory) -> physical GPU (device: 8, name: Tesla K80, pci bus id: 0000:00:17.0, compute capability: 3.7)
2018-12-14 01:07:09.546635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:9 with 10755 MB memory) -> physical GPU (device: 9, name: Tesla K80, pci bus id: 0000:00:18.0, compute capability: 3.7)
2018-12-14 01:07:09.546835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:10 with 10755 MB memory) -> physical GPU (device: 10, name: Tesla K80, pci bus id: 0000:00:19.0, compute capability: 3.7)
2018-12-14 01:07:09.547045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:11 with 10755 MB memory) -> physical GPU (device: 11, name: Tesla K80, pci bus id: 0000:00:1a.0, compute capability: 3.7)
2018-12-14 01:07:09.547274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:12 with 10755 MB memory) -> physical GPU (device: 12, name: Tesla K80, pci bus id: 0000:00:1b.0, compute capability: 3.7)
2018-12-14 01:07:09.547484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:13 with 10755 MB memory) -> physical GPU (device: 13, name: Tesla K80, pci bus id: 0000:00:1c.0, compute capability: 3.7)
2018-12-14 01:07:09.547688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:14 with 10755 MB memory) -> physical GPU (device: 14, name: Tesla K80, pci bus id: 0000:00:1d.0, compute capability: 3.7)
2018-12-14 01:07:09.547886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:15 with 10755 MB memory) -> physical GPU (device: 15, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)
2018-12-14 01:07:09.551770: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-12-14 01:07:09.627089: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404
2018-12-14 01:07:09.627194: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.
2018-12-14 01:07:09.627373: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-12-14 01:10:25.265510: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-12-14 01:10:25.330682: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404
2018-12-14 01:10:25.330719: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.
2018-12-14 01:10:25.330857: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-12-14 01:12:58.847158: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-12-14 01:13:06.793996: I tensorflow/core/platform/s3/aws_logging.cc:54] Deleting file: /tmp/s3_filesystem_XXXXXX20181214T0112571544749977988
2018-12-14 01:13:06.861344: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-12-14 01:13:06.883504: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-12-14 01:13:09.892293: E tensorflow/core/platform/s3/aws_logging.cc:60] Curl returned error code 28
2018-12-14 01:13:09.892400: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.
2018-12-14 01:13:09.892433: W tensorflow/core/platform/s3/aws_logging.cc:57] Request failed, now waiting 0 ms before attempting again.
2018-12-14 01:13:09.892867: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-12-14 01:13:13.899888: E tensorflow/core/platform/s3/aws_logging.cc:60] Curl returned error code 28
2018-12-14 01:13:13.899977: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.
2018-12-14 01:13:13.899996: W tensorflow/core/platform/s3/aws_logging.cc:57] Request failed, now waiting 50 ms before attempting again.
2018-12-14 01:13:13.950970: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-12-14 01:13:17.957128: E tensorflow/core/platform/s3/aws_logging.cc:60] Curl returned error code 28
2018-12-14 01:13:17.957192: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.
2018-12-14 01:13:17.957212: W tensorflow/core/platform/s3/aws_logging.cc:57] Request failed, now waiting 100 ms before attempting again.
2018-12-14 01:13:18.058715: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-12-14 01:13:22.065071: E tensorflow/core/platform/s3/aws_logging.cc:60] Curl returned error code 28
2018-12-14 01:13:22.065155: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.
2018-12-14 01:13:22.065181: W tensorflow/core/platform/s3/aws_logging.cc:57] Request failed, now waiting 200 ms before attempting again.
2018-12-14 01:13:22.266342: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-12-14 01:13:26.273044: E tensorflow/core/platform/s3/aws_logging.cc:60] Curl returned error code 28
2018-12-14 01:13:26.273111: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.
2018-12-14 01:13:26.273143: W tensorflow/core/platform/s3/aws_logging.cc:57] Request failed, now waiting 400 ms before attempting again.
2018-12-14 01:13:26.674870: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-12-14 01:13:30.681963: E tensorflow/core/platform/s3/aws_logging.cc:60] Curl returned error code 28
2018-12-14 01:13:30.682039: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.
2018-12-14 01:13:30.682059: W tensorflow/core/platform/s3/aws_logging.cc:57] Request failed, now waiting 800 ms before attempting again.
2018-12-14 01:13:31.483287: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-12-14 01:13:35.489697: E tensorflow/core/platform/s3/aws_logging.cc:60] Curl returned error code 28
2018-12-14 01:13:35.489748: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.
2018-12-14 01:13:35.489766: W tensorflow/core/platform/s3/aws_logging.cc:57] Request failed, now waiting 1600 ms before attempting again.
2018-12-14 01:13:37.090948: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-12-14 01:13:41.097136: E tensorflow/core/platform/s3/aws_logging.cc:60] Curl returned error code 28
2018-12-14 01:13:41.097251: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.
2018-12-14 01:13:41.097272: W tensorflow/core/platform/s3/aws_logging.cc:57] Request failed, now waiting 3200 ms before attempting again.
2018-12-14 01:13:44.299654: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-12-14 01:13:48.307090: E tensorflow/core/platform/s3/aws_logging.cc:60] Curl returned error code 28
2018-12-14 01:13:48.307182: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.
2018-12-14 01:13:48.307208: W tensorflow/core/platform/s3/aws_logging.cc:57] Request failed, now waiting 6400 ms before attempting again.
2018-12-14 01:13:54.709486: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-12-14 01:13:58.715801: E tensorflow/core/platform/s3/aws_logging.cc:60] Curl returned error code 28
2018-12-14 01:13:58.715869: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.
2018-12-14 01:13:58.715895: W tensorflow/core/platform/s3/aws_logging.cc:57] Request failed, now waiting 12800 ms before attempting again.
2018-12-14 01:14:11.518412: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-12-14 01:14:15.525502: E tensorflow/core/platform/s3/aws_logging.cc:60] Curl returned error code 28
2018-12-14 01:14:15.525595: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.
2018-12-14 01:14:15.526863: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
Traceback (most recent call last):
File ""train.py"", line 191, in <module>
tf.app.run()
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
_sys.exit(main(argv))
File ""train.py"", line 182, in main
cyclegan_estimator.train(train_input_fn, steps=FLAGS.steps_per_eval)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 356, in train
loss = self._train_model(input_fn, hooks, saving_listeners)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 1179, in _train_model
return self._train_model_distributed(input_fn, hooks, saving_listeners)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 1326, in _train_model_distributed
saving_listeners)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 1406, in _train_with_estimator_spec
log_step_count_steps=self._config.log_step_count_steps) as mon_sess:
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py"", line 504, in MonitoredTrainingSession
stop_grace_period_secs=stop_grace_period_secs)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__
stop_grace_period_secs=stop_grace_period_secs)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__
self._sess = _RecoverableSession(self._coordinated_creator)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__
_WrappedSession.__init__(self, self._create_session())
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session
return self._sess_creator.create_session()
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py"", line 807, in create_session
hook.after_create_session(self.tf_sess, self.coord)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 558, in after_create_session
""graph.pbtxt"")
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/graph_io.py"", line 71, in write_graph
text_format.MessageToString(graph_def))
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py"", line 436, in atomic_write_string_to_file
rename(temp_pathname, filename, overwrite)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py"", line 415, in rename
compat.as_bytes(oldname), compat.as_bytes(newname), overwrite, status)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py"", line 526, in __exit__
c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.UnknownError: : Unable to connect to endpoint
```"
24351,RAM-efficient shuffling in dataset,"**System information**
- TensorFlow version (you are using): 1.12.0
- Are you willing to contribute it (Yes/No): Maybe, if I get some direction

**Describe the feature and the current behavior/state.**

Currently to shuffle datasets, `shuffle()` needs to read `shuffle_buffer` worth of records into memory. If the records are ~large and the dataset contains ordered/consequitve/correlated records then the user may need to pre-shuffle and/or split the data and/or commmit a good chunk of ram for shuffling the records sufficiently to avoid overfitting to features present in the particular part of the dataset.

It would be possible to trade off reading the dataset just once in order to shuffle more efficiently in terms of memory usage. For example, the ram-efficient dataset shuffler could read the dataset multiple times starting from offsets `0 .. N` and skip over `N` records on each pass. 

**Will this change the current api? How?**

It would add a new method to `tf.data.Dataset`, or maybe an option/experiment for `shuffle()`.

**Who will benefit with this feature?**

Anyne training on sequential (I guess correleated) datasets where individual records are on large enough to become a limiting factor for large `shuffle_buffer` sizes."
24346,`import tensorflow` fails after `import pandas`; RUNPATH seemingly ignored,"### System information
| Category | Info |
| --- | --- |
| Have I written custom code? | no |
| OS/distro | Linux, RHEL 7.4 |
| TF installed from | source |
| TF version | 1.9.0 |
| Python version | 3.6.5 |
| Bazel version | 0.14.1 |
| GCC version | 7.3.0 |
| CUDA/cuDNN version | 9.2.88 and 7.1.4 |
| GPU model/memory | 16 GB |
| Command to reporoduce | `import pandas; import tensorflow` |

### Problem description

The successful import of pandas and TensorFlow depends on the order in which they are loaded:

```python
# Works
import tensorflow
import pandas
```

```python
# Fails
import pandas
import tensorflow
```

The full error message is below, but here's an excerpt:

```
Traceback (most recent call last):
  File ""/apps/python-3.6.5-tensorflow/1.9.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
...
  File ""/apps/python/3.6.5/gcc-7.3.0/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: /lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /apps/python-3.6.5-tensorflow/1.9.0/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
```

What's confusing for me: `_pywrap_tensorflow_internal.so`'s RUNPATH contains `/apps/gcc/7.3.0/lib64`, which has the version of `libstdc++.so.6` that should be (and normally is) used, and LD_LIBRARY_PATH is not set--so I'm confused as to why `/lib64/libstdc++.so.6` even enters the equation. It also seems wrong that the order of import dictates which `libstdc++.so.6` is used.

Just to be sure, I temporarily changed  `_pywrap_tensorflow_internal.so`'s RUNPATH to an RPATH in case pandas was (weirdly) setting LD_LIBRARY_PATH, but it made no difference--the same error persists.

Reproducing might be annoying/not worth the time, but in case anyone wants to try I built Python and TensorFlow from source using GCC 7.3.0 (which was itself built from source) and installed pandas with pip. I would be just as happy with a suggestion on what to try next/where to look around, though.

### Source code / logs

Full error message:

```
>>> import pandas
>>> import tensorflow
Traceback (most recent call last):
  File ""/apps/python-3.6.5-tensorflow/1.9.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/apps/python-3.6.5-tensorflow/1.9.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/apps/python-3.6.5-tensorflow/1.9.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/apps/python/3.6.5/gcc-7.3.0/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/apps/python/3.6.5/gcc-7.3.0/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: /lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /apps/python-3.6.5-tensorflow/1.9.0/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/apps/python-3.6.5-tensorflow/1.9.0/lib/python3.6/site-packages/tensorflow/__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/apps/python-3.6.5-tensorflow/1.9.0/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/apps/python-3.6.5-tensorflow/1.9.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/apps/python-3.6.5-tensorflow/1.9.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/apps/python-3.6.5-tensorflow/1.9.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/apps/python-3.6.5-tensorflow/1.9.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/apps/python/3.6.5/gcc-7.3.0/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/apps/python/3.6.5/gcc-7.3.0/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: /lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /apps/python-3.6.5-tensorflow/1.9.0/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```"
24344,Pretrained tensorflow resnet-101 on MPII human pose data,"Hello,

do you know if a pretrained resnet-101 tensorflow model exists on MPII human pose dataset? I could not find any so far..
"
24343,Broadcasting batch_gather,"**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No):



**Describe the feature and the current behavior/state.**

Batch gather works like this

```python3
result[i, .., k] = input[i, ..., k-1, indexes[i, ..., k]]
```
Now, lets say that `indexes.shape == [s1,s2,s3, k]` and `input.shape == [s1, 1, s3]`.  In order to be able to use batch_gather we must do

```python3
input_ = tf.tile(input, (1, s2, 1))
result = tf.batch_gather(input_, indexes)
``` 
which is wasteful, as we needlessly copy the tensor, for tiling.

In case this feature already exists and I missed it, please direct and forgive me.

**Will this change the current api? How?** No

**Who will benefit with this feature?** Most serious users, who work with masks and such.

**Any Other info.**
"
24342, Predict fuel efficiency: regression example should not normalize one-hot values,"**Describe the current behavior**
In the Predict fuel efficiency: regression lesson (TensorFlow->Learn->Tutorials->Learn and use ML->Regression), the normalize method is called to normalize the entire dataset, including the one-hot columns.

**Describe the expected behavior**
The one-hot columns in the dataset should not be normalized.

**Code to reproduce the issue**
dataset['USA'] = (origin == 1)*1.0
dataset['Europe'] = (origin == 2)*1.0
dataset['Japan'] = (origin == 3)*1.0
train_dataset = dataset.sample(frac=0.8,random_state=0)
train_stats = train_dataset.describe()
train_stats.pop(""MPG"")
train_stats = train_stats.transpose()

def norm(x):
  return (x - train_stats['mean']) / train_stats['std']
normed_train_data = norm(train_dataset)

"
24340,Can't open tensorflow.org,"I can't open https://www.tensorflow.org/ but I can open https://tensorflow.google.cn, why is that? If tensorflow has changed its URL why didn't update it? When I google tensorflow it also leads to https://www.tensorflow.org, but it cannot be accessed.
Does anyone else having this problem?"
24339,tf.contrib.copy_graph.copy_op_to_graph Error when copying a foldl operation,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Archlinux
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12
- Python version: 3.6
- CUDA/cuDNN version: cuda 9.0, cudnn 7.1
- GPU model and memory: GeForce GTX 1060 

**Describe the current behavior**
tf.contrib.copy_graph.copy_op_to_graph gives a 
```
RecursionError: maximum recursion depth exceeded while calling a Python object
```
when called on a foldl operation.
tf.contrib.copy_graph.copy_op_to_graph does not manages foldl operation. Maybe it is because of the recursive structure of the operation.

**Describe the expected behavior**
tf.contrib.copy_graph.copy_op_to_graph should copy the part of the graph containing the fold operation. Maybe it can manage in a particular way recursive blocks like fold.

**Code to reproduce the issue**
```python
import tensorflow as tf
x = tf.ones((2,1))
acc =  tf.ones((1,1))
# definition of the fold operation
y = tf.foldl(lambda a,y: return a + y, x, acc) 
# tf.Session().run(y) gives [[3.]], as expected

# copy the fold to another graph
g = tf.Graph()
with g.as_default():
    tf.contrib.copy_graph.copy_op_to_graph(y, g, [])
    # RecursionError: maximum recursion depth exceeded while calling a Python object
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Full traceback attached.
[tf_bug.zip](https://github.com/tensorflow/tensorflow/files/2675691/tf_bug.zip)

Basically the issue is a recursion issue, tf does not understands the recursive nature of the foldl op.
Reduced traceback: 
```
RecursionError: maximum recursion depth exceeded while calling a Python object
```



"
24338,tensorflow1.7 hangs at LocalMaster::RunStep with tf.train.MonitoredTrainingSession in sync mode,"I'm training gan on wavenet model with tf.train.MonitoredTrainingSession, with 1 ps and 2 workers. It hangs when using tf.train.SyncReplicasOptimizer, but works well in async mode. And it works well with Vanilla GAN demo in sync mode.

With debug the chief worker, I found master  is waiting for worker to response. However, I don't know what the worker is doing, and which thread hangs?

So, how to debug this problem?

More infos: tenosrflow 1.7, M40, sorry can not paste the source code

chief worker gdb info as follows:

```
(gdb) bt
#0  0x00007f9f001796d5 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f9e39a098dc in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/local/gcc-5.3.0/lib64/libstdc++.so.6
#2  0x00007f9e4758566b in nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00007f9e47584f71 in nsync::nsync_sem_wait_with_cancel_(nsync::waiter*, timespec, nsync::nsync_note_s_*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00007f9e47582402 in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00007f9e475828e3 in nsync::nsync_cv_wait_with_deadline(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*, timespec, nsync::nsync_note_s_*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#6  0x00007f9e4472a04b in tensorflow::(anonymous namespace)::WaitForNotification(tensorflow::CallOptions*, long long, tensorflow::Notification*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#7  0x00007f9e4472a984 in tensorflow::LocalMaster::RunStep(tensorflow::CallOptions*, tensorflow::RunStepRequestWrapper*, tensorflow::MutableRunStepResponseWrapper*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#8  0x00007f9e447125ea in tensorflow::GrpcSession::RunProto(tensorflow::CallOptions*, tensorflow::MutableRunStepRequestWrapper*, tensorflow::MutableRunStepResponseWrapper*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#9  0x00007f9e447134a2 in tensorflow::GrpcSession::RunHelper(tensorflow::RunOptions const&, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#10 0x00007f9e44713c60 in tensorflow::GrpcSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#11 0x00007f9e449dd3d1 in TF_Run_Helper(tensorflow::Session*, char const*, TF_Buffer const*, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, TF_Tensor**, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, TF_Buffer*, TF_Status*) [clone .constprop.703]
    () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#12 0x00007f9e449de69a in TF_Run () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#13 0x00007f9e446bb213 in tensorflow::TF_Run_wrapper_helper(TF_DeprecatedSession*, char const*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#14 0x00007f9e446bb323 in tensorflow::TF_Run_wrapper(TF_DeprecatedSession*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#15 0x00007f9e4467b010 in _wrap_TF_Run () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#16 0x00000000004b8ef4 in PyEval_EvalFrameEx ()
#17 0x00000000004b9b98 in PyEval_EvalCodeEx ()
#18 0x00000000004b89cd in PyEval_EvalFrameEx ()
#19 0x00000000004b9b98 in PyEval_EvalCodeEx ()
#20 0x000000000052f2d8 in function_call ()
#21 0x00000000004235da in PyObject_Call ()
#22 0x00000000004b4ee2 in PyEval_EvalFrameEx ()
#23 0x00000000004b9b98 in PyEval_EvalCodeEx ()
#24 0x00000000004b89cd in PyEval_EvalFrameEx ()
#25 0x00000000004b9b98 in PyEval_EvalCodeEx ()
#26 0x00000000004b89cd in PyEval_EvalFrameEx ()
#27 0x00000000004b9b98 in PyEval_EvalCodeEx ()
#28 0x00000000004b89cd in PyEval_EvalFrameEx ()
#29 0x00000000004b9b98 in PyEval_EvalCodeEx ()
#30 0x000000000052f3cf in function_call ()
#31 0x00000000004235da in PyObject_Call ()
#32 0x00000000004b4ee2 in PyEval_EvalFrameEx ()
#33 0x00000000004b9b98 in PyEval_EvalCodeEx ()
#34 0x000000000052f3cf in function_call ()
#35 0x00000000004235da in PyObject_Call ()
#36 0x0000000000427bb5 in instancemethod_call ()
#37 0x00000000004235da in PyObject_Call ()
#38 0x00000000004b4396 in PyEval_EvalFrameEx ()
#39 0x00000000004b9b98 in PyEval_EvalCodeEx ()
#40 0x000000000052f3cf in function_call ()
#41 0x00000000004235da in PyObject_Call ()
#42 0x00000000004b4ee2 in PyEval_EvalFrameEx ()
#43 0x00000000004b9b98 in PyEval_EvalCodeEx ()
#44 0x00000000004b89cd in PyEval_EvalFrameEx ()
#45 0x00000000004b9b98 in PyEval_EvalCodeEx ()
#46 0x00000000004b89cd in PyEval_EvalFrameEx ()
#47 0x00000000004b9b98 in PyEval_EvalCodeEx ()
#48 0x00000000004b89cd in PyEval_EvalFrameEx ()
---Type <return> to continue, or q <return> to quit---
```
woker 1 gdb info:

```
(gdb) bt
#0  0x00007ff6c5ace6d5 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007ff5ff35e8dc in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/local/gcc-5.3.0/lib64/libstdc++.so.6
#2  0x00007ff60ceda66b in nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00007ff60ced9f71 in nsync::nsync_sem_wait_with_cancel_(nsync::waiter*, timespec, nsync::nsync_note_s_*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00007ff60ced7402 in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00007ff60ced78e3 in nsync::nsync_cv_wait_with_deadline(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*, timespec, nsync::nsync_note_s_*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#6  0x00007ff60a07f04b in tensorflow::(anonymous namespace)::WaitForNotification(tensorflow::CallOptions*, long long, tensorflow::Notification*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#7  0x00007ff60a07f984 in tensorflow::LocalMaster::RunStep(tensorflow::CallOptions*, tensorflow::RunStepRequestWrapper*, tensorflow::MutableRunStepResponseWrapper*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#8  0x00007ff60a0675ea in tensorflow::GrpcSession::RunProto(tensorflow::CallOptions*, tensorflow::MutableRunStepRequestWrapper*, tensorflow::MutableRunStepResponseWrapper*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#9  0x00007ff60a0684a2 in tensorflow::GrpcSession::RunHelper(tensorflow::RunOptions const&, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#10 0x00007ff60a068c60 in tensorflow::GrpcSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#11 0x00007ff60a3323d1 in TF_Run_Helper(tensorflow::Session*, char const*, TF_Buffer const*, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, TF_Tensor**, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, TF_Buffer*, TF_Status*) [clone .constprop.703]
    () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#12 0x00007ff60a33369a in TF_Run () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#13 0x00007ff60a010213 in tensorflow::TF_Run_wrapper_helper(TF_DeprecatedSession*, char const*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#14 0x00007ff60a010323 in tensorflow::TF_Run_wrapper(TF_DeprecatedSession*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#15 0x00007ff609fd0010 in _wrap_TF_Run () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#16 0x00000000004b8ef4 in PyEval_EvalFrameEx ()
#17 0x00000000004b9b98 in PyEval_EvalCodeEx ()
#18 0x00000000004b89cd in PyEval_EvalFrameEx ()
#19 0x00000000004b9b98 in PyEval_EvalCodeEx ()
#20 0x000000000052f2d8 in function_call ()
#21 0x00000000004235da in PyObject_Call ()
#22 0x00000000004b4ee2 in PyEval_EvalFrameEx ()
#23 0x00000000004b9b98 in PyEval_EvalCodeEx ()
#24 0x00000000004b89cd in PyEval_EvalFrameEx ()
#25 0x00000000004b9b98 in PyEval_EvalCodeEx ()
#26 0x00000000004b89cd in PyEval_EvalFrameEx ()
#27 0x00000000004b9b98 in PyEval_EvalCodeEx ()
#28 0x00000000004b89cd in PyEval_EvalFrameEx ()
#29 0x00000000004b9b98 in PyEval_EvalCodeEx ()
#30 0x000000000052f3cf in function_call ()
#31 0x00000000004235da in PyObject_Call ()
#32 0x00000000004b4ee2 in PyEval_EvalFrameEx ()
#33 0x00000000004b9b98 in PyEval_EvalCodeEx ()
#34 0x000000000052f3cf in function_call ()
#35 0x00000000004235da in PyObject_Call ()
#36 0x0000000000427bb5 in instancemethod_call ()
#37 0x00000000004235da in PyObject_Call ()
#38 0x00000000004b4396 in PyEval_EvalFrameEx ()
#39 0x00000000004b9b98 in PyEval_EvalCodeEx ()
#40 0x000000000052f3cf in function_call ()
#41 0x00000000004235da in PyObject_Call ()
#42 0x00000000004b4ee2 in PyEval_EvalFrameEx ()
#43 0x00000000004b9b98 in PyEval_EvalCodeEx ()
#44 0x00000000004b89cd in PyEval_EvalFrameEx ()
#45 0x00000000004b9b98 in PyEval_EvalCodeEx ()
#46 0x00000000004b89cd in PyEval_EvalFrameEx ()
#47 0x00000000004b9b98 in PyEval_EvalCodeEx ()
#48 0x00000000004b89cd in PyEval_EvalFrameEx ()
---Type <return> to continue, or q <return> to quit---
```
"
24337,TOCO cannot identify dilated convolution correctly,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): N/A
- Python version: N/A
- Bazel version (if compiling from source): 0.19.1
- GCC/Compiler version (if compiling from source): gcc 7.3.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

In this line of [comment](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/toco/graph_transformations/identify_dilated_conv.cc#L116), it said the `Bias op is required before or after BatchToSpace`. However, in the DeepLab model ([mobilenetv2_coco_voc_trainval](http://download.tensorflow.org/models/deeplabv3_mnv2_pascal_trainval_2018_01_29.tar.gz)), there isn't any BiasAdd around the `BatchToSpace`. So the TOCO will fail to combine a sequence of `SpaceToBatchND` - `(Depthwise)Conv2D` - `BatchToSpaceND` into dilated convolution. Instead, the `SpaceToBatchND` and `BatchToSpaceND` will still remain in the graph.
"
24336,"I  train the model use tf.estimator and use tonado for serve,when i request  the service  it got stuck,how to fix it?","This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
24333,How can i use inspect_checkpoint tool to print all the weight ?,"I've try using inspect_checkpoint.py tool to see all weight in my checkpoint:
my checkpoint file contain:
model.ckpt-10.data-00000-of-00001
model.ckpt-10.index
model.ckpt-10.meta
my command was:
python inspect_checkpoint.py --file_name=model.ckpt-10 --all_tensors=True --tensor_name=''
But the weight missing it showed with ""...""
For example:
tensor_name: dnn/hiddenlayer_4/weights
[[ 0.11010675 -0.00205228 0.06564941 ... 0.04844228 -0.06266475
0.07596292]
[ 0.06760433 0.12344839 0.12087007 ... -0.04885229 -0.15109406
0.03623438]
[-0.11817194 0.0303787 0.06215482 ... 0.02974839 0.09252764
0.25440046]
...
[-0.03469304 -0.19408804 0.1232834 ... -0.101989 -0.11033994
0.00296944]
[-0.01577727 0.13655508 -0.07761313 ... -0.08856598 -0.13241452
-0.00495515]
[ 0.00119148 0.03261929 -0.096873 ... -0.04134168 0.02477826
0.10813464]]

the shape of the tensor_name is: dnn/hiddenlayer_4/weights (DT_FLOAT) [128,128]
How can i really print all the weight ?
Thank you.
"
24331, Malformed TF_STRING tensor; too short to hold number of elements,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  CentOS 7.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): Java API 1.12
- Python version: 
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
I am using Java API for prediction and 
I tried to create a tensor of String type using the following Java API:
```java
                    String[] strs = {""element1"", ""thisissecondelement""};  
                    ByteArrayOutputStream outputStream = new ByteArrayOutputStream( );   
                    for (String s : strs) {  
                        outputStream.write(s.getBytes());  
                    }  
                    long [] shape = new long[1];
                    shape[0] = strs.length;
                    Tensor<String> t = Tensor.create(String.class, shape,  ByteBuffer.wrap(outputStream.toByteArray());
```

However, when calling ""runner.fetch(""predict/Sigmoid:0"").run().get(0)"" I got the following excetion:
```
Exception in thread ""main"" java.lang.IllegalArgumentException: Malformed TF_STRING tensor; too short to hold number of elements.
```

I looked into the code:
![g1m018793wkgcqlwrvg-abv9paaa0cknmbeo475](https://user-images.githubusercontent.com/1213386/49913535-d5e0e480-fec8-11e8-8d94-4260800120a6.PNG)

It uses sizeof(tensorflow::uint64) as the size of the element in the byte array. However, the  size of the string element is not fixed. 

**Describe the expected behavior**
The API  `runner.fetch(""predict/Sigmoid:0"").run().get(0)`  should run without error.

**Code to reproduce the issue**
Create a String Tensor using `public static <T> Tensor<T> create(Class<T> type, long[] shape, ByteBuffer data) ` in org.tensorflow.Tensor.

"
24329,Missing Docker Images referenced in Documentation,"**System information**
- TensorFlow version: Docker version
- Doc Link: https://www.tensorflow.org/install/docker


**Describe the documentation issue**
In the install with Docker documentation, there are examples of how to pull images by adding multiple parameters together, there is am implicit reference to latest-gpu-jupyter. On [docker-hub](https://hub.docker.com/r/tensorflow/tensorflow/tags/), it appears that tag is no longer valid or is no longer being built. This can cause a confusing error relating to a missing manifest when trying to pull that image.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
Of course, I just need to know if this is a permanent deprecation of latest-gpu-py3-jupyter and latest-gpu-jupyter, or if it's just a mistake, the nightly builds do contain those tags (nightly-gpu-py3-jupyter) for example
"
24328,tensorflow.keras printing control characters in progress bars,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04, docker hub tag: 1.12.0-gpu-py3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): docker pull
- TensorFlow version (use command below): 1.12.0
- Python version: 3.5
- Bazel version (if compiling from source): not source
- GCC/Compiler version (if compiling from source): not source
- CUDA/cuDNN version: 7.1.4
- GPU model and memory: P80 12GB


**Describe the current behavior**
I'm running tensorflow.keras through a docker container and displaying the output in a Jupyter notebook that is outside of the container (using Amazon SageMaker notebooks). Regular Keras printed progress bars correctly, but switching to tensorflow.Keras now spams my screen with control characters when trying to print progress bars. For example when Keras is downloading ImageNet weights for a model it outputs this:

```
Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.4/xception_weights_tf_dim_ordering_tf_kernels_notop.h5
#015    8192/83683744 [..............................] - ETA: 12:27#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015   40960/83683744 [..............................] - ETA: 5:00 #010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015   90112/83683744 [..............................] - ETA: 3:24#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015  212992/83683744 [..............................] - ETA: 1:55#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015  442368/83683744 [..............................] - ETA: 1:09#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015  892928/83683744 [..............................] - ETA: 40s #010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 1810432/83683744 [..............................] - ETA: 23s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010
```



**Describe the expected behavior**
It shouldn't print control characters

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Reproducing this exactly may require running through SageMaker. The example I modified is this: https://github.com/awslabs/amazon-sagemaker-examples/tree/master/hyperparameter_tuning/keras_bring_your_own 
Specifically, I changed all of the Keras imports in this file: https://github.com/awslabs/amazon-sagemaker-examples/blob/master/hyperparameter_tuning/keras_bring_your_own/trainer/start.py to be tensorflow.keras imports. 

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24326,with tf.device('/cpu:0') is ignored when using MirroredStrategy,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
    Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
    Ubuntu 16.04
- TensorFlow installed from (source or binary):
    pip / binary
- TensorFlow version (use command below):
    tf-nightly-gpu==1.13.0.dev20181022
- Python version:
   3.6
- Bazel version (if compiling from source):
    N/A
- GCC/Compiler version (if compiling from source):
    N/A
- CUDA/cuDNN version:
   7.2.1.38
- GPU model and memory:
    8x v100 (16 gb per)

**Describe the current behavior**
If I call with tf.device while my distribution strategy is set to **MirroredStrategy** then my embeddings are forced onto gpu.
```
...
with tf.device('/cpu:0'):
    with tf.variable_scope(""testvar""):
        embeddings = tf.get_variable(
            name='weights',
            shape=[10, 5],
            trainable=True)
print(embeddings)
...
```
>>>
MirroredVariable:{'/replica:0/task:0/device:**GPU**:0': <tf.Variable 'fh_universal_transformer_encoder/word_symbol_modality_1917597_300/input_emb/embeddings/weights:0' shape=(1917597, 300)

**Describe the expected behavior**
```
...
with tf.device('/cpu:0'):
    with tf.variable_scope(""testvar""):
        embeddings = tf.get_variable(
            name='weights',
            shape=[10, 5],
            trainable=True)
print(embeddings)
...
```
>>>
MirroredVariable:{'/replica:0/task:0/device:**CPU**:0': <tf.Variable 'fh_universal_transformer_encoder/word_symbol_modality_1917597_300/input_emb/embeddings/weights:0' shape=(1917597, 300)


"
24324,Missing CPU AVX2 FMA and registered OpKernels,"I'm trying to run a known working model but my tensorflow configuration is not setup right.

I'm using Ubuntu 18, CUDA Version 9.0.176

I'm trying to figure out what I should do to get these error messages to resolve.
I figure I have to rebuild tensorflow with these supporting features.

`No OpKernel was registered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU,XLA_CPU], Registered kernels:
  <no registered kernels>`

`Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA`


Is this the right track?

So far I've been trying to build Tensorflow v1.19 with GPU support as thats what the code instructs me to use. I also ready I need bazel v15 as that is the most tested and used version of bazel in the tensorflow community.

Can anybody provide insight into these error messages and guidance on how to get the right version of tensorflow setup so I may proceed with my work?


**Full-Trace:**
~~~
Tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Traceback (most recent call last):
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    return fn(*args)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1317, in _run_fn
    self._extend_graph()
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1352, in _extend_graph
    tf_session.ExtendSession(self._session)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU,XLA_CPU], Registered kernels:
  <no registered kernels>

	 [[{{node cu_dnnlstm_1/CudnnRNN}} = CudnnRNN[T=DT_FLOAT, direction=""unidirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode=""lstm"", seed=87654321, seed2=0](cu_dnnlstm_1/transpose, cu_dnnlstm_1/ExpandDims_1, cu_dnnlstm_1/ExpandDims_2, cu_dnnlstm_1/concat_1)]]

During handling of the above exception, another exception occurred:
~~~

Comment: This error is triggered when I load a pretrained `.h5` model... The model likely calls for OpKernels that are not setup with tensorflow or Cuda/Cudnn ... not sure though

~~~
Traceback (most recent call last):
  File ""script.py"", line 105, in <module>
    model_best = load_model('/home/ubuntu/git/androidmodel/mod.h5')
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py"", line 419, in load_model
    model = _deserialize_model(f, custom_objects, compile)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py"", line 287, in _deserialize_model
    K.batch_set_value(weight_value_tuples)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 2470, in batch_set_value
    get_session().run(assign_ops, feed_dict=feed_dict)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 199, in get_session
    [tf.is_variable_initialized(v) for v in candidate_vars])
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU,XLA_CPU], Registered kernels:
  <no registered kernels>

	 [[node cu_dnnlstm_1/CudnnRNN (defined at /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py:922)  = CudnnRNN[T=DT_FLOAT, direction=""unidirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode=""lstm"", seed=87654321, seed2=0](cu_dnnlstm_1/transpose, cu_dnnlstm_1/ExpandDims_1, cu_dnnlstm_1/ExpandDims_2, cu_dnnlstm_1/concat_1)]]

Caused by op 'cu_dnnlstm_1/CudnnRNN', defined at:
  File ""script.py"", line 82, in <module>
    model.add(CuDNNLSTM(40, return_sequences=True, stateful=False, input_shape=(None, data_dim)))
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/engine/sequential.py"", line 165, in add
    layer(x)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/layers/recurrent.py"", line 532, in __call__
    return super(RNN, self).__call__(inputs, **kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/engine/base_layer.py"", line 457, in __call__
    output = self.call(inputs, **kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/layers/cudnn_recurrent.py"", line 90, in call
    output, states = self._process_batch(inputs, initial_state)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/layers/cudnn_recurrent.py"", line 517, in _process_batch
    is_training=True)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py"", line 1544, in __call__
    input_data, input_h, input_c, params, is_training=is_training)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py"", line 1435, in __call__
    seed=self._seed)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py"", line 922, in _cudnn_rnn
    outputs, output_h, output_c, _ = gen_cudnn_rnn_ops.cudnn_rnn(**args)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_cudnn_rnn_ops.py"", line 116, in cudnn_rnn
    is_training=is_training, name=name)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3274, in create_op
    op_def=op_def)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1770, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU,XLA_CPU], Registered kernels:
  <no registered kernels>

	 [[node cu_dnnlstm_1/CudnnRNN (defined at /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py:922)  = CudnnRNN[T=DT_FLOAT, direction=""unidirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode=""lstm"", seed=87654321, seed2=0](cu_dnnlstm_1/transpose, cu_dnnlstm_1/ExpandDims_1, cu_dnnlstm_1/ExpandDims_2, cu_dnnlstm_1/concat_1)]]
~~~"
24323,undefined symbol: _ZN6tflite12tensor_utils27NeonSymmetricQuantizeFloatsEPKfiPaPfS4_S4_\n,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspberry Pi 3B+ Linux
- TensorFlow installed from (source or binary): installed in Python using pip3
- TensorFlow version (use command below):  1.11.0
- Python version: 3.5.3

**Describe the current behavior**
Trying to convert a frozen graph file (.pb) to a TensorFlow Lite FlatBuffer file (.tflite),  Executing this command from the Linux command line:
cd ~/.local/bin
./tflite_convert --output_file=/home/pi/sols/demo/src/image_classification/network/fruit_models/fruit_model.tflite --graph_def_file=/home/pi/sols/demo/src/image_classification/network/fruit_models/frozen_graph.pb --input_arrays=X --output_arrays=softmax

It gives me this error:
RuntimeError: TOCO failed see console for info.
b'/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.4 of module \'tensorflow.python.framework.fast_tensor_util\' does not match runtime version 3.5\n  return f(*args, **kwds)\n/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: builtins.type size changed, may indicate binary incompatibility. Expected 432, got 412\n  return f(*args, **kwds)\nTraceback (most recent call last):\n  File ""/bin/toco_from_protos"", line 7, in <module>\n    from tensorflow.contrib.lite.toco.python.toco_from_protos import main\n  File ""/home/pi/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/toco/python/toco_from_protos.py"", line 22, in <module>\n    from tensorflow.contrib.lite.toco.python import tensorflow_wrap_toco\n  File ""/home/pi/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/toco/python/tensorflow_wrap_toco.py"", line 28, in <module>\n    _tensorflow_wrap_toco = swig_import_helper()\n  File ""/home/pi/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/toco/python/tensorflow_wrap_toco.py"", line 24, in swig_import_helper\n    _mod = imp.load_module(\'_tensorflow_wrap_toco\', fp, pathname, description)\n  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic\n    return _load(spec)\nImportError: /home/pi/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/toco/python/_tensorflow_wrap_toco.so: undefined symbol: _ZN6tflite12tensor_utils27NeonSymmetricQuantizeFloatsEPKfiPaPfS4_S4_\n'

Please help!

"
24322,java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model issue,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device: Samsung S5
- TensorFlow installed from (source or binary): Binary
- TensorFlow version: 1.12
- Python version: 3.5
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


**Describe the problem**
I have created my own architecture, on tensorflow and I have used Toco to convert to .tflite, However the android app still throws the following error: ""java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model.""

**Provide the exact sequence of commands / steps that you executed before running into the problem**
'''
       AssetFileDescriptor fileDescriptor = activity.getAssets().openFd(MODEL_PATH);
        FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());
        FileChannel fileChannel = inputStream.getChannel();
        long startOffset = fileDescriptor.getStartOffset();
        long declaredLength = fileDescriptor.getDeclaredLength();
        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);
'''

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24321,Nightly development containers still come with `1.12.0-rc0` pre-installed.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Ubuntu 18.04`
- TensorFlow installed from (source or binary): binary inside docker
- TensorFlow version: `1.12.0`
- Python version: `both 2.7 and 3.6` within development containers
- Installed using virtualenv? pip? conda?: `pip`
- Bazel version (if compiling from source): `0.15.0`
- GCC/Compiler version (if compiling from source): `7.3.0`
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**

After downloading nightly development `cpu` images for both Python2 and Python 3, I realized the version for per-install TensorFlow still reads `1.12.0-rc0`.

Here is the console logs for Python3 for example:
```
$ docker run -it tensorflow/tensorflow:nightly-devel-py3 bash
Unable to find image 'tensorflow/tensorflow:nightly-devel-py3' locally
nightly-devel-py3: Pulling from tensorflow/tensorflow
32802c0cfa4d: Already exists 
da1315cffa03: Already exists 
fa83472a3562: Already exists 
f85999a86bef: Already exists 
0f910117152a: Pull complete 
a65c5ef5ec56: Pull complete 
cf4bb4e1826b: Pull complete 
66ef88223fb9: Pull complete 
8aeb73a82dfe: Pull complete 
bc8f7b434f2b: Pull complete 
d265fbe2714c: Pull complete 
ddec192687d1: Pull complete 
78df3767776d: Pull complete 
b96b75a73e63: Pull complete 
a93b1cef267b: Pull complete 
afc439a6b9ad: Pull complete 
Digest: sha256:3f9e30100331fb4199aced4501c94d2718b2e20678fee4c1d398a756420ecab7
Status: Downloaded newer image for tensorflow/tensorflow:nightly-devel-py3
root@4f201309883c:~# ipython
Python 3.6.7 (default, Oct 22 2018, 11:32:17) 
Type 'copyright', 'credits' or 'license' for more information
IPython 7.2.0 -- An enhanced Interactive Python. Type '?' for help.

In [1]: import tensorflow as tf                                                                                                                                                                                                                                                            
tf
In [2]: tf.VERSION                                                                                                                                                                                                                                                                         
Out[2]: '1.12.0-rc0'
```

Looks like user still needs to manually install the nightly TensorFlow wheel inside the container to get the latest and greatest:
```
$ pip install -U tf-nightly
Collecting tf-nightly
  Downloading https://files.pythonhosted.org/packages/59/51/821e5c26631142d97e0ea832eff848e5d1e88db5346692584768ff8afc69/tf_nightly-1.13.0.dev20181212-cp36-cp36m-manylinux1_x86_64.whl (92.5MB)
    100% |################################| 92.5MB 519kB/s 
Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.16.1)
Collecting tf-estimator-nightly (from tf-nightly)
  Downloading https://files.pythonhosted.org/packages/02/71/0d1c7adae69ac5a2c77e5ba0c3ac3fd2de5ce2aeb1467aabfe8701a723ea/tf_estimator_nightly-1.12.0.dev20181208-py2.py3-none-any.whl (300kB)
    100% |################################| 307kB 8.7MB/s 
Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.7.1)
Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.11.0)
Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.32.3)
Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.6.1)
Collecting tb-nightly<1.14.0a0,>=1.13.0a0 (from tf-nightly)
  Downloading https://files.pythonhosted.org/packages/80/55/164d6a25d03cecfafa5c88a015564a56b5ee7e951e210fb66e17d935e104/tb_nightly-1.13.0a20181126-py3-none-any.whl (3.2MB)
    100% |################################| 3.2MB 9.1MB/s 
Requirement already satisfied, skipping upgrade: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.6.1)
Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.15.4)
Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)
Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.0.6)
Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.0.5)
Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.2.0)
Requirement already satisfied, skipping upgrade: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tf-estimator-nightly->tf-nightly) (2.0.0)
Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tf-nightly) (40.6.2)
Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a0,>=1.13.0a0->tf-nightly) (3.0.1)
Requirement already satisfied, skipping upgrade: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a0,>=1.13.0a0->tf-nightly) (0.14.1)
Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tf-nightly) (2.8.0)
Requirement already satisfied, skipping upgrade: pbr>=0.11 in /usr/local/lib/python3.6/dist-packages (from mock>=2.0.0->tf-estimator-nightly->tf-nightly) (5.1.1)
Installing collected packages: tf-estimator-nightly, tb-nightly, tf-nightly
Successfully installed tb-nightly-1.13.0a20181126 tf-estimator-nightly-1.12.0.dev20181208 tf-nightly-1.13.0.dev20181212
root@4f201309883c:~# ipython
Python 3.6.7 (default, Oct 22 2018, 11:32:17) 
Type 'copyright', 'credits' or 'license' for more information
IPython 7.2.0 -- An enhanced Interactive Python. Type '?' for help.

In [1]: import tensorflow as tf                                                                                                                                                                                                                                                            

In [2]: tf.VERSION                                                                                                                                                                                                                                                                         
Out[2]: '1.13.0-dev20181212'
```
"
24320,BatchNormalization produces NaN weights without NaN loss,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
  - Custom code.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
  - Ubuntu 17.10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
  - N/A
- TensorFlow installed from (source or binary):
  - pip package `tensorflow-gpu`
- TensorFlow version (use command below):
  - v1.8.0-0-g93bc2e2072 1.8.0
- Python version:
  - 3.6.3 
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
  - V9.0.176
- GPU model and memory:
  - GeForce GTX 1060 6GB

**Describe the current behavior**

Copying from [keras issue 11858](https://github.com/keras-team/keras/issues/11858), as requested by @ymodak.

Hi,

Not sure this can be labelled as a bug, but it's problematic. BatchNormalization seems to silently produce NaN weights when training a `multi_gpu_model` if the training dataset size is not a multiple of `batch_size`.

For example, with a training dataset (1825, 401, 401, 3), validation dataset (140, 401, 401, 3), `epochs=1`, `batch_size=16`, `gpu_number=2`

```python
    # instantiate model
    with tf.device('/cpu:0'):
        # DenseNet121: blocks=[6, 12, 24, 16]
        base_model = densenet.DenseNet121(include_top=False, weights=None,
                                       input_shape=(401, 401, 3), pooling='avg')
        x = Dense(units=1, activation='sigmoid', name='fc1')(base_model.output)
        model = Model(inputs=base_model.input, outputs=x)

    # compile model
    parallel_model = multi_gpu_model(model, gpus=gpu_number)
    parallel_model.compile(loss={'fc1': 'binary_crossentropy'},
                           optimizer='Adadelta',
                           metrics={'fc1': ['acc']})

    # train model
    tic = datetime.datetime.now()
    parallel_model.fit(train_onecell_im,
                       {'fc1': (train_onecell_dice >= quality_threshold).astype(np.float32)},
                       validation_data=(test_onecell_im,
                                        {'fc1': (test_onecell_dice >= quality_threshold).astype(np.float32)}),
                       batch_size=batch_size, epochs=epochs, initial_epoch=0)
    toc = datetime.datetime.now()
    print('Training duration: ' + str(toc - tic))
```

The training apparently goes fine

```
Train on 1825 samples, validate on 140 samples
Epoch 1/1
1825/1825 [==============================] - 108s 59ms/step - loss: 0.6604 - acc: 0.6323 - val_loss: 0.6932 - val_acc: 0.4643
Training duration: 0:02:08.115762

```

but the weights have NaNs, e.g.

```
model.get_layer('conv1/bn').get_weights()
[array([1.0001292 , 1.        , 0.9996672 , 0.9999442 , 1.000509  ,
       1.0001016 , 1.0002009 , 1.0004678 , 0.9999988 , 0.999962  ,
       1.0003603 , 1.0001667 , 0.9999296 , 0.9999381 , 1.00001   ,
       0.99967813, 0.9999821 , 0.99981546, 0.9999899 , 1.0002408 ,
       0.9999446 , 0.9999995 , 0.99989605, 1.0000395 , 1.0000094 ,
       0.9999432 , 0.999968  , 0.99994946, 0.9997129 , 1.0000957 ,
       0.99997395, 1.000016  , 0.99995   , 0.99981534, 0.99984217,
       0.9999743 , 0.99999624, 1.0005921 , 1.0001019 , 1.000008  ,
       0.99993116, 0.99998087, 0.9999631 , 0.9999878 , 0.9999804 ,
       1.0003394 , 0.999895  , 0.9997747 , 0.9999677 , 0.99998355,
       1.000003  , 0.9998863 , 0.9999338 , 0.9998308 , 1.0000825 ,
       1.000022  , 0.9999998 , 0.9997648 , 1.0000801 , 1.000631  ,
       1.0000259 , 0.9996165 , 1.0001084 , 0.9996289 ], dtype=float32), array([ 0.00369794, -0.00307915, -0.0148356 ,  0.01176912, -0.00456085,
        0.00461122,  0.00392016, -0.00510793, -0.00388927,  0.00678776,
       -0.0033672 ,  0.0020039 ,  0.00688829,  0.00877651,  0.00838199,
       -0.0217527 , -0.00673187, -0.01623467,  0.00523926, -0.0005527 ,
        0.00700372, -0.00372984, -0.01347521, -0.00636716,  0.00206494,
        0.00884918, -0.00814271, -0.00801541, -0.02038615,  0.00171547,
        0.00709944, -0.00221861,  0.00538696, -0.01515745, -0.01330438,
        0.00306095,  0.00399868, -0.0049634 , -0.00725381,  0.00373429,
       -0.01107734, -0.00610222, -0.00854702, -0.00504343, -0.0080514 ,
       -0.00920443,  0.00863727, -0.01750346,  0.00656873, -0.00534429,
        0.00434025, -0.01352841, -0.00819136, -0.01453205, -0.00043049,
       -0.00257635, -0.00448346, -0.01370949,  0.00355583, -0.00480247,
        0.00179911, -0.01858746, -0.00059417, -0.0123234 ], dtype=float32), array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32), array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
      dtype=float32)]
```

I think this happens because the training dataset of 1825 gets split into sets of `batch_size=16`. So there's going to be a set of 1 training image, and maybe that doesn't work with BatchNormalization.

Inference with the trained model gives

```
foo = model.predict(test_onecell_im)

foo
array([[nan],
       [nan],
       [nan],
...
       [nan],
       [nan]], dtype=float32)
```

A solution is to make sure that the number of training images is a multiple of `batch_size`.

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
24319,[tf.keras.layers.LSTM] reset_state performance issue,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): modified `tensorflow/python/keras/layers/recurrent.py` to get timings of LSTM reset (see code change below)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): 1.12.0, binary from PyPi
- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0
- Python version: 3.6.7
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: -
- GPU model and memory: -


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
Time required to execute `tensorflow.keras.Model` `reset_states()` increases non-linearly with the number of stateful `tensorflow.keras.layers.LSTM` units, but only the first time `reset_states()` is called. The second time `reset_states()` it basically doesn't take any time at all.
A few numbers to highlight this - all numbers refer to the reset of a single LSTM (one model layer), not the whole model:
- 2 LSTMs: reset of the first LSTM takes ~20ms and of the last takes ~30ms (could be just a random variation)
- 20 LSTMs: reset of the first LSTM takes ~300ms and of the last ~700ms
- 200 LSTMs: reset of the first LSTM takes ~3500ms and of the last ~7000ms
I traced the delay down to `tensorflow.keras.backend.set_value`, so it obviously is really setting of the state that takes extremely long.

**Describe the expected behavior**
I would expect that the reset of a single LSTM (one model layer) stays constant independent of the number of LSTMs the model consists of. More importantly, I would expect that the time it takes to reset a single LSTM doesn't increase, but is constant for all of the LSTMs of a model..

**Code to reproduce the issue**
Import module `datetime` in `tensorflow/python/keras/layers/recurrent.py` and replace method of `reset_states` of class `RNN` by
```
  def reset_states(self, states=None):
    import datetime
    if not self.stateful:
      raise AttributeError('Layer must be stateful.')
    batch_size = self.input_spec[0].shape[0]
    if not batch_size:
      raise ValueError('If a RNN is stateful, it needs to know '
                       'its batch size. Specify the batch size '
                       'of your input tensors: \n'
                       '- If using a Sequential model, '
                       'specify the batch size by passing '
                       'a `batch_input_shape` '
                       'argument to your first layer.\n'
                       '- If using the functional API, specify '
                       'the batch size by passing a '
                       '`batch_shape` argument to your Input layer.')
    # initialize state if None
    if self.states[0] is None:
      if _is_multiple_state(self.cell.state_size):
        self.states = [
            K.zeros([batch_size] + tensor_shape.as_shape(dim).as_list())
            for dim in self.cell.state_size
        ]
      else:
        self.states = [
            K.zeros([batch_size] +
                    tensor_shape.as_shape(self.cell.state_size).as_list())
        ]
    elif states is None:
      if _is_multiple_state(self.cell.state_size):
        now = datetime.datetime.now()
        for state, dim in zip(self.states, self.cell.state_size):
          K.set_value(state,
                      np.zeros([batch_size] +
                               tensor_shape.as_shape(dim).as_list()))
        print(f""LSTM reset time: {datetime.datetime.now() - now}"")  # TODO
      else:
        K.set_value(self.states[0], np.zeros(
            [batch_size] +
            tensor_shape.as_shape(self.cell.state_size).as_list()))
    else:
      if not isinstance(states, (list, tuple)):
        states = [states]
      if len(states) != len(self.states):
        raise ValueError('Layer ' + self.name + ' expects ' +
                         str(len(self.states)) + ' states, '
                         'but it received ' + str(len(states)) +
                         ' state values. Input received: ' + str(states))
      for index, (value, state) in enumerate(zip(states, self.states)):
        if _is_multiple_state(self.cell.state_size):
          dim = self.cell.state_size[index]
        else:
          dim = self.cell.state_size
        if value.shape != tuple([batch_size] +
                                tensor_shape.as_shape(dim).as_list()):
          raise ValueError(
              'State ' + str(index) + ' is incompatible with layer ' +
              self.name + ': expected shape=' + str(
                  (batch_size, dim)) + ', found shape=' + str(value.shape))
        # TODO(fchollet): consider batch calls to `set_value`.
        K.set_value(state, value)
```

Then set `COUNT_LSTMS` to the number of LSTMs that should be used in the following script and execute it:
```
import tensorflow as tf
import numpy as np

COUNT_LSTMS = 200

BATCH_SIZE = 100
UNITS_INPUT_OUTPUT = 5
UNITS_LSTMS = 20
BATCHES_TO_GENERATE = 2
SEQUENCE_LENGTH = 20

# build model
my_input = tf.keras.layers.Input(batch_shape=(BATCH_SIZE,
                                              None,
                                              UNITS_INPUT_OUTPUT))
my_lstm_layers = [tf.keras.layers.LSTM(units=UNITS_LSTMS,
                                       stateful=True,
                                       return_sequences=True)(my_input)
                  for _ in range(COUNT_LSTMS)]
my_output_layer = tf.keras.layers.Dense(UNITS_INPUT_OUTPUT)
my_output = tf.keras.layers.TimeDistributed(my_output_layer)(
    tf.keras.layers.concatenate(my_lstm_layers, axis=-1))
my_model = tf.keras.Model(my_input, my_output)


# generation
pred_input = np.random.randn(BATCH_SIZE, 1, UNITS_INPUT_OUTPUT)

for batch in range(BATCHES_TO_GENERATE):
    print('resetting states')
    my_model.reset_states()
    print(f""start generation of batch {batch}"")
    for _ in range(SEQUENCE_LENGTH):
        pred_input = my_model.predict(pred_input, batch_size=BATCH_SIZE)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24318,Build Fail gcc: internal compiler error: Killed ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Ubuntu 18.04
- TensorFlow installed from source
- TensorFlow version: git master
- Python version:
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: CUDA 10.0/ cuDNN 7.3
- GPU model and memory: GTX980Ti



**Describe the problem**
Buit fail
**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
tensorflow/core/common_runtime/eager/execute.cc: In function 'tensorflow::Status tensorflow::{anonymous}::ValidateInputTypeAndPlacement(tensorflow::EagerContext*, tensorflow::Device*, tensorflow::EagerOperation*, const tensorflow::OpKernel*, tensorflow::RunMetadata*)':
tensorflow/core/common_runtime/eager/execute.cc:179:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < op->Inputs().size(); ++i) {
                   ~~^~~~~~~~~~~~~~~~~~~~~
tensorflow/core/common_runtime/eager/execute.cc: In function 'tensorflow::Status tensorflow::{anonymous}::EagerRemoteExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*)':
tensorflow/core/common_runtime/eager/execute.cc:490:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < op->Inputs().size(); i++) {
                   ~~^~~~~~~~~~~~~~~~~~~~~
tensorflow/core/common_runtime/eager/execute.cc:524:20: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   if (*num_retvals != output_dtypes.size()) {
       ~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~
tensorflow/core/common_runtime/eager/execute.cc: In lambda function:
tensorflow/core/common_runtime/eager/execute.cc:565:33: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
               for (int i = 0; i < retvals.size(); i++) {
                               ~~^~~~~~~~~~~~~~~~
tensorflow/core/common_runtime/eager/execute.cc: In function 'tensorflow::Status tensorflow::{anonymous}::MaybeUpdateOpDevice(tensorflow::EagerOperation*)':
tensorflow/core/common_runtime/eager/execute.cc:626:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < op->Inputs().size(); ++i) {
                   ~~^~~~~~~~~~~~~~~~~~~~~
tensorflow/core/common_runtime/eager/execute.cc:622:8: warning: variable 'device_set_for_resource_variable' set but not used [-Wunused-but-set-variable]
   bool device_set_for_resource_variable = false;
        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
tensorflow/core/common_runtime/eager/execute.cc: In function 'tensorflow::Status tensorflow::EagerExecute(tensorflow::EagerContext*, tensorflow::Device*, const absl::InlinedVector<tensorflow::TensorHandle*, 4>&, tensorflow::KernelAndDevice*, tensorflow::NodeExecStats*, tensorflow::StepStats*, tensorflow::GraphCollector*, tensorflow::TensorHandle**, int)':
tensorflow/core/common_runtime/eager/execute.cc:716:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < op_inputs.size(); ++i) {
                   ~~^~~~~~~~~~~~~~~~~~
tensorflow/core/common_runtime/eager/execute.cc:753:43: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
       while (step_stats->dev_stats_size() < ctx->devices()->size()) {
              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~
tensorflow/core/common_runtime/eager/execute.cc:758:25: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
       for (int i = 0; i < ctx->devices()->size(); ++i) {
                       ~~^~~~~~~~~~~~~~~~~~~~~~~~
tensorflow/core/common_runtime/eager/execute.cc:713:27: warning: variable 'output_memory_types' set but not used [-Wunused-but-set-variable]
   const MemoryTypeVector* output_memory_types = nullptr;
                           ^~~~~~~~~~~~~~~~~~~
In file included from ./tensorflow/core/platform/default/logging.h:24:0,
                 from ./tensorflow/core/platform/logging.h:25,
                 from ./tensorflow/core/lib/core/refcount.h:22,
                 from ./tensorflow/core/platform/tensor_coding.h:21,
                 from ./tensorflow/core/framework/resource_handle.h:19,
                 from ./tensorflow/core/framework/allocator.h:24,
                 from ./tensorflow/core/common_runtime/device.h:35,
                 from ./tensorflow/core/common_runtime/eager/execute.h:18,
                 from tensorflow/core/common_runtime/eager/execute.cc:16:
./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetTensorDim(tensorflow::gtl::ArraySlice<T>, tensorflow::TensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':
./tensorflow/core/util/tensor_format.h:452:47:   required from here
./tensorflow/core/util/tensor_format.h:420:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(index >= 0 && index < dimension_attributes.size())
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/core/util/tensor_format.h:420:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attributes.size())
   ^
./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':
./tensorflow/core/util/tensor_format.h:461:54:   required from here
./tensorflow/core/util/tensor_format.h:435:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(index >= 0 && index < dimension_attribute.size())
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/core/util/tensor_format.h:435:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attribute.size())
   ^
ERROR: /home/xx/tensorflow/tensorflow/core/kernels/BUILD:762:1: C++ compilation of rule '//tensorflow/core/kernels:broadcast_to_op' failed (Exit 4)
gcc: internal compiler error: Killed (program cc1plus)
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1255.664s, Critical Path: 397.27s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]
INFO: 5547 processes: 5547 local.
FAILED: Build did NOT complete successfully

```
"
24317,Build Tensorflow / Tensorflow Lite for Linux on ARM 64 bit,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 16
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
Sources
- TensorFlow version:
- Python version:
2.7
- Installed using virtualenv? pip? conda?:
No
- Bazel version (if compiling from source):
15.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

Hi , I am looking for a way to BUILD   Tensorflow/Tensorflow  Lite shared c++ library to run on ARM 64 bit with Ubuntu OS.
Thanks. 

"
24316,[Feature Request] Nested HParams,"// -

"
24313,Issue building Tensorflow 1.8.0 from sources,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OEL 7.4 (Oracle Linux)
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 3.4.1
- **Bazel version (if compiling from source)**: 0.19.2
- **GCC/Compiler version (if compiling from source)**: 4.8.5
- **CUDA/cuDNN version**: Not using Cuda
- **GPU model and memory**: NA
- **Exact command to reproduce**: bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package


```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
While trying to build tensorflow from source, it failed stating:
""gcc: error: pywrap_tensorflow_internal_versionscript.lds: No such file or directory""

### Source code / logs
ERROR: /scratch/pransen/tensorflow-1.8.0/tensorflow/python/BUILD:3315:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed (Exit 1) gcc failed: error executing command /bin/gcc -shared -o bazel-out/host/bin/tensorflow/python/_pywrap_tensorflow_internal.so -Wl,--version-script pywrap_tensorflow_internal_versionscript.lds '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..' ... (remaining 90 argument(s) skipped)
"
24312,@nccl_archive//:nccl: missing input file '@nccl_archive//:src/nccl.h',"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 9585202ed095ec63c1a6f947a0197fce852e9036
- Python version: 2.7.12
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source): Ubuntu 5.4.0-6ubuntu1~16.04.10
- CUDA/cuDNN version: 9.0/7.2.1.38
- GPU model and memory: GTX 1080Ti

**Describe the problem**

d6a46850353acfe26625c5ab1ffe7bd5c5a4aaf0 breaks the build. Reverting it fix the problem. Ping @chsigg.

```
ERROR: missing input file '@nccl_archive//:src/nccl.h'
ERROR: /home/byronyi/.cache/bazel/_bazel_byronyi/fe336f47afec8b033fb4b29a6628548f/external/nccl_archive/BUILD.bazel:104:1: @nccl_archive//:nccl: missing input file '@nccl_archive//:src/nccl.h'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /home/byronyi/.cache/bazel/_bazel_byronyi/fe336f47afec8b033fb4b29a6628548f/external/nccl_archive/BUILD.bazel:104:1 1 input file(s) do not exist
INFO: Elapsed time: 194.156s, Critical Path: 14.83s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]
INFO: 634 processes: 634 local.
FAILED: Build did NOT complete successfully
```"
24311,TensorFlow Lite - Interpreter.resizeInput method fails after using a square shape during conversion,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX / Ubuntu 16.04 for creation of the model
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Nexus 5X
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.13.0-dev20181129
- Python version: 3.6.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I created a custom keras model (cnn network with Resnet layers).  I defined the input to accept variable sized grey scale images (None, None, None, 1).  When I created TensorFlow lite model I was forced to provide input shape parameters (eg., 1,48,48,1).  Per documentation I should be able to change the input size using Interpreter.resizeInput() method.  I have to do this to accept variable sized images.  However this is not working ... I keep getting this error in Android studio.

java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/kernel_util.cc:129 d1 == d2 || d1 == 1 || d2 == 1 was not true.Node number 21 (ADD) failed to prepare.
  

**Describe the expected behavior**

I should be able to pass a variable size image to the model.  The model (*pb file) works fine in Ubuntu.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.



**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24309,[Tensorflow tutorial: speech commands] svdf model doesn't work in the demo app ,"**Describe the current behavior**
Hi all,
I'm trying the speech commands tutorial of Tensorflow.
But when I run the low latency svdf model in the demo app, it doesn't work.
It showed the error as **Other info / logs** below.
I saw the same issue here: https://github.com/tensorflow/tensorflow/issues/13868
But it seems there's no specific and detailed solution.
Could anyone help me with this?
Thanks! 
David 

**Code to reproduce the issue**
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/speech_commands
https://www.tensorflow.org/versions/master/tutorials/audio_recognition

**Other info / logs**
E/zygote64: No implementation found for long org.tensorflow.contrib.android.RunStats.allocate() (tried Java_org_tensorflow_contrib_android_RunStats_allocate and Java_org_tensorflow_contrib_android_RunStats_allocate__)

E/AndroidRuntime: FATAL EXCEPTION: main
                  Process: org.tensorflow.demo, PID: 13070
                  java.lang.RuntimeException: Unable to start activity ComponentInfo{org.tensorflow.demo/org.tensorflow.demo.SpeechActivity}: java.lang.RuntimeException: Failed to load model from 'file:///android_asset/low_latency_svdf-135000.pb'
                      at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2904)
                      at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2979)
                      at android.app.ActivityThread.-wrap11(Unknown Source:0)
                      at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1643)
                      at android.os.Handler.dispatchMessage(Handler.java:105)
                      at android.os.Looper.loop(Looper.java:180)
                      at android.app.ActivityThread.main(ActivityThread.java:6944)
                      at java.lang.reflect.Method.invoke(Native Method)
                      at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240)
                      at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:853)
                   Caused by: java.lang.RuntimeException: Failed to load model from 'file:///android_asset/low_latency_svdf-135000.pb'
                      at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:113)
                      at org.tensorflow.demo.SpeechActivity.onCreate(SpeechActivity.java:152)
                      at android.app.Activity.performCreate(Activity.java:6986)
                      at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1232)
                      at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2857)
                      at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2979)
                      at android.app.ActivityThread.-wrap11(Unknown Source:0)
                      at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1643)
                      at android.os.Handler.dispatchMessage(Handler.java:105)
                      at android.os.Looper.loop(Looper.java:180)
                      at android.app.ActivityThread.main(ActivityThread.java:6944)
                      at java.lang.reflect.Method.invoke(Native Method)
                      at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240)
                      at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:853)
                   Caused by: java.io.IOException: Not a valid TensorFlow Graph serialization: Value for attr 'T' of int64 is not in the list of allowed values: float, int32, qint8, quint8, qint32
                  	; NodeDef: {{node count_nonzero/Sum}} = Sum[T=DT_INT64, Tidx=DT_INT32, keep_dims=false](count_nonzero/ToInt64, count_nonzero/Const); Op<name=Sum; signature=input:T, reduction_indices:Tidx -> output:T; attr=keep_dims:bool,default=false; attr=T:type,allowed=[DT_FLOAT, DT_INT32, DT_QINT8, DT_QUINT8, DT_QINT32]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
                      at org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph(TensorFlowInferenceInterface.java:561)
                      at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:105)
                      	... 13 more"
24308,enable_select_tf_ops=true cannot work?,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux 4.15.0-42-generic Ubuntu 18.04 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below):b'v1.12.0-rc0-4289-g68834966da' 1.12.0-rc0
- Python version: 3.6
- Bazel version (if compiling from source): 0.20.0
- GCC/Compiler version (if compiling from source):7.3.0
- CUDA/cuDNN version:None
- GPU model and memory:None

I know some ops are not supporting yet by tflite.So I use **TensorFlow Lite with select TensorFlow ops** as:
""
bazel run --define=with_select_tf_ops=true //tensorflow/lite/toco:toco -- \
  --output_file=/tmp/out.tflite \
  --input_file=/models/inf.pb \
  --input_arrays=image_tensor \
  --output_arrays=boxes,scores,num_boxes \
  --input_shapes=1,1024,1024,3 \
  --inference_type=FLOAT \
  --target_ops=TFLITE_BUILTINS,SELECT_TF_OPS \
  --enable_select_tf_ops=true
""
BUT **it still crashed** when converting:
2018-12-11 23:14:32.493249: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter
2018-12-11 23:14:32.493271: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2018-12-11 23:14:32.493309: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayWriteV3
2018-12-11 23:14:32.493355: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Exit
2018-12-11 23:14:32.493380: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Exit
2018-12-11 23:14:32.493415: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Exit
2018-12-11 23:14:32.493421: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArraySizeV3
2018-12-11 23:14:32.493437: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayGatherV3
2018-12-11 23:14:32.493454: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArraySizeV3
2018-12-11 23:14:32.493498: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayGatherV3
2018-12-11 23:14:32.493520: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArraySizeV3
2018-12-11 23:14:32.493549: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayGatherV3
2018-12-11 23:14:32.503265: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 795 operators, 1359 arrays (0 quantized)
2018-12-11 23:14:32.516861: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 787 operators, 1344 arrays (0 quantized)
2018-12-11 23:14:32.535269: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 787 operators, 1344 arrays (0 quantized)
2018-12-11 23:14:32.554909: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 547 operators, 936 arrays (0 quantized)
2018-12-11 23:14:32.571142: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 547 operators, 936 arrays (0 quantized)
2018-12-11 23:14:32.578976: F tensorflow/lite/toco/tooling_util.cc:627] Check failed: dim >= 1 (0 vs. 1)

What should I do to make `select_tf_ops` mechanism work?


"
24307,some bug in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): liunx ubuntun 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: huawei mate 9
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below):1.12.0
- Python version:3.6
- Bazel version (if compiling from source):no
- GCC/Compiler version (if compiling from source):6
- CUDA/cuDNN version:9.0
- GPU model and memory:2080


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
 I need to transplant my net to mobilephone, first i run the demo in office,it works well,when i change the DetectiorMode to YOLO or MULTIBOX which provide in office code it didn't works
**Describe the current behavior** 
when i change the mode to MULTIBOX there is no predictions
when i change the mode to YOLO, the app is  crash
I wonder if there is no implement in source code,
wondering for you replay
**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24306,contrib\cmake: version_info.cc not found,"
**System information**
- OS Windows 10
- TensorFlow version: lastest on master branch
- Python version: 3.5
- GCC/Compiler version (if compiling from source): Visual-studio 14 2015 win64
- CUDA/cuDNN version: 9.0
- GPU model and memory: 6.1/8Gb

**Describe the problem**

I followed the instruction in contrib/cmake to build tensorflow on windows, in generation step: 

`          Configuring done
          CMake Error at tf_core_framework.cmake:332 (add_library):
          Cannot find source file:

    E:/work_space/projectcpp/tensorflow/tensorflow/tensorflow/core/util/version_info.cc

    Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm
           .hpp .hxx .in .txx
          Call Stack (most recent call first):
           CMakeLists.txt:587 (include)
`
It seems like a known-bug but it is still there, please help.
Thank"
24305,TensorFlow docker image JupyterNotebook does not start,"1. I built the docker image using Dockerfile present in `tensorflow/tensorflow/tools/docker/Dockerfile`, When I run the image it does gives me url for notebook

```
Copy/paste this URL into your browser when you connect for the first time,
    to login with a token:
        http://(e01f14b69e8e or 127.0.0.1):8888/?token=da541XXXXXXXXXXXXXXXXXXXXXX
```

However opening the url fails with error 
`127.0.0.1 didnt send any data.
`

Following is the port-mapping output 

```
debug1: client_input_global_request: rtype hostkeys-00@openssh.com want_reply 0
debug1: Connection to port 8888 forwarding to localhost port 8888 requested.
debug1: channel 2: new [direct-tcpip]
debug1: Connection to port 8888 forwarding to localhost port 8888 requested.
debug1: channel 3: new [direct-tcpip]
channel 2: open failed: connect failed: Connection refused
channel 3: open failed: connect failed: Connection refused
debug1: channel 2: free: direct-tcpip: listening port 8888 for localhost port 8888, connect from 127.0.0.1 port 51331 to 127.0.0.1 port 8888, nchannels 4

```"
24304,Tensorflow Lite: ResNet example model gave VERY poor result during validation with ImageNet,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO. 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No.
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): The version I download from master branch
- Python version: 3.6
- Bazel version (if compiling from source): 0.15
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

I am studying tensorflow lite. I downloaded the ResNet frozen graph ResNet_V2_101 from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models.md#image-classification-float-models .

And then I followed https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tutorials/post_training_quant.ipynb to convert this frozen graph to both Lite model and quantized lite model.
```
import tensorflow as tf
import pathlib
import sys
import tensorflow as tf
from tensorflow.python.saved_model import tag_constants
import time
graph_def_file = ""resnet_saved_model/resnet_v2_101_299_frozen.pb""
input_arrays = [""input""]
output_arrays = [""output""]
converter = tf.lite.TocoConverter.from_frozen_graph(str(graph_def_file),input_arrays,output_arrays,input_shapes = {""input"":[1,299,299,3]})
tflite_model = converter.convert()
open(""saved_model/resnet_v2_101_299_frozen.tflite"", ""wb"").write(tflite_model) 

converter.post_training_quantize = True
tflite_quantized_model = converter.convert()
open(""saved_model/resnet_v2_101_299_frozen_quantize.tflite"", ""wb"").write(tflite_quantized_model) 
```
Then I followed https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/accuracy/ilsvrc to evalute its accuracy using ImageNet Validation Dataset (50000 images) on my desktop.

However, when I run 
`bazel run -c opt   --cxxopt='--std=c++11'   --   //tensorflow/lite/tools/accuracy/ilsvrc:imagenet_accuracy_eval   --model_file=""/home/kathy/saved_model/ResNet_V2_101.tflite""   --ground_truth_images_path=""/media/kathy/Documents/val_imgs""   --ground_truth_labels=""/home/kathy/workspace/tensorflow/tensorflow/lite/tools/accuracy/ilsvrc/VALIDATION_LABELS.txt""   --model_output_labels=""/home/kathy/workspace/tensorflow/tensorflow/lite/tools/accuracy/ilsvrc/resnet_output_labels.txt""   --output_file_path=""/tmp/accuracy_output.txt"" --num_images=0`
and checked the output accuracy_output.txt. The accuracy is very poor. I can capture some results among the 50000 images. 
```
Top 1, Top 2, Top 3, Top 4, Top 5, Top 6, Top 7, Top 8, Top 9, Top 10
0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000
0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000
0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000
0.000, 0.000, 0.000, 25.000, 25.000, 25.000, 25.000, 25.000, 25.000, 25.000
0.000, 0.000, 0.000, 20.000, 20.000, 20.000, 20.000, 20.000, 20.000, 20.000
0.000, 0.000, 0.000, 16.667, 16.667, 16.667, 16.667, 16.667, 16.667, 16.667
0.000, 0.000, 0.000, 14.286, 14.286, 14.286, 14.286, 14.286, 14.286, 14.286
0.000, 0.000, 0.000, 12.500, 12.500, 12.500, 12.500, 12.500, 12.500, 12.500
0.000, 0.000, 0.000, 11.111, 11.111, 11.111, 11.111, 11.111, 11.111, 11.111
0.000, 0.000, 0.000, 10.000, 10.000, 10.000, 10.000, 10.000, 10.000, 10.000
0.000, 0.000, 0.000, 9.091, 9.091, 9.091, 9.091, 9.091, 9.091, 9.091
0.000, 0.000, 0.000, 8.333, 8.333, 8.333, 8.333, 8.333, 8.333, 8.333

```

After running 50000 validation images, the top-1 to top-10 results are
`0.080, 0.146, 0.230, 0.324, 0.408, 0.518, 0.608, 0.678, 0.770, 0.888 `
However, according to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tutorials/post_training_quant.ipynb, the top-1 accuracy can reach 76.8 but my attempt even cannot reach 1% in the end. Why this happens? Where I did wrong? Thanks!"
24300,Failed to compile a graph using XLA AOT? Executing genrule @org_tensorflow//:gen_graph failed,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04 LTS
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: tf-nightly  -> b'v1.12.0-rc0-3642-g2ce0bec9da' 1.13.0-dev20181203
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: 0.20.0      libprotoc 3.3.0
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: GTX 1080 Ti - 11GB

### Issue 
I am new to XLA compilation and run into some issue while compiling our model. Any help would be appreciated!  I was trying to use tfcompile to compile a pre-trained [ssd_resnet_50_fpn_coco model](http://download.tensorflow.org/models/object_detection/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03.tar.gz) unsuccessfully. I was running into Error message shown below. 

### Code to reproduce the issue:
The `BUILD` file looks like this
```C
load('@org_tensorflow//tensorflow/compiler/aot:tfcompile.bzl', 'tf_library')

tf_library(
    name = 'graph',
    config = 'graph.config.pbtxt',
    cpp_class = 'Graph',
    graph = 'graph.pb',
)
```
`graph.config.pbtxt` looks like this
```JavaScript
feed {
      id {
        node_name: ""image_tensor""
      }
      shape {
        dim {
          size: 1
        }
        dim {
          size: 640
        }
        dim {
          size: 640
        }
        dim {
          size: 3
        }
      }
    }
    fetch {
      id {
        node_name: ""detection_boxes""
      }
    }
    fetch {
      id {
        node_name: ""detection_scores""
      }
    }
    fetch {
      id {
        node_name: ""detection_classes""
      }
    }
    fetch {
      id {
        node_name: ""num_detections""
      }
    }
```
Then I run `bazel build --show_progress_rate_limit=600 @org_tensorflow//:graph`

> ERROR: /home/syntech/.cache/bazel/_bazel_syntech/186a880d737f29aade7a2d700e840c56/external/org_tensorflow/BUILD:4:1: Executing genrule @org_tensorflow//:gen_graph failed (Exit 1)
INVALID ARGUMENTS: Merge of two inputs that differ on more than one predicate {s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater:0,else), s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/Greater/_303__cf__306:0,then), s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/cond/pred_id/_304__cf__307:0,then)} and {s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater:0,else), s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/Greater/_303__cf__306:0,else), s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/cond/pred_id/_304__cf__307:0,else)}
	for node {{node Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/cond/Merge}}
 tfcompile performs ahead-of-time compilation of a TensorFlow graph,
resulting in an object file compiled for your target architecture, and a
header file that gives access to the functionality in the object file.
A typical invocation looks like this:
  $ tfcompile --graph=mygraph.pb --config=myfile.pbtxt --cpp_class=""mynamespace::MyComputation""
usage: bazel-out/host/bin/external/org_tensorflow/tensorflow/compiler/aot/tfcompile
Flags:
	--graph=""""                       	string	Input GraphDef file.  If the file ends in '.pbtxt' it is expected to be in the human-readable proto text format, otherwise it is expected to be in the proto binary format.
	--config=""""                      	string	Input file containing Config proto.  If the file ends in '.pbtxt' it is expected to be in the human-readable proto text format, otherwise it is expected to be in the proto binary format.
	... ( I have omitted some of the irrelevant output here) 
        ...
        ...
Target @org_tensorflow//:graph failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1373.473s, Critical Path: 57.61s
INFO: 3686 processes: 3686 local.
FAILED: Build did NOT complete successfully

Running with `--verbose_failures` gives 
>INFO: Invocation ID: 7d6f9793-2517-4ea7-afab-faf4fa30eaf9
INFO: Analysed target @org_tensorflow//:graph (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: /home/syntech/.cache/bazel/_bazel_syntech/186a880d737f29aade7a2d700e840c56/external/org_tensorflow/BUILD:4:1: Executing genrule @org_tensorflow//:gen_graph failed (Exit 1): bash failed: error executing command
  (cd /home/syntech/.cache/bazel/_bazel_syntech/186a880d737f29aade7a2d700e840c56/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64:/usr/local/cuda-9.0/extras/CUPTI/lib64:/home/syntech/TensorRT-4.0.1.6/lib/ \
    PATH=/home/syntech/tx_dev/venv/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/syntech/bin \
    PYTHON_BIN_PATH=/home/syntech/tx_dev/venv/bin/python \
    PYTHON_LIB_PATH=/home/syntech/tx_dev/venv/lib/python3.5/site-packages \
    TF_DOWNLOAD_CLANG=0 \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL_SYCL=0 \
    TF_NEED_ROCM=0 \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; CUDA_VISIBLE_DEVICES='\'''\'' bazel-out/host/bin/external/org_tensorflow/tensorflow/compiler/aot/tfcompile --graph=external/org_tensorflow/models/frozen_inference_graph.pb --config=external/org_tensorflow/ssdresnet.config.pbtxt --entry_point=____graph --cpp_class=Model --target_triple=x86_64-pc-linux --out_header=bazel-out/k8-opt/genfiles/external/org_tensorflow/graph.h --out_metadata_object=bazel-out/k8-opt/genfiles/external/org_tensorflow/graph_tfcompile_metadata.o --out_function_object=bazel-out/k8-opt/genfiles/external/org_tensorflow/graph_tfcompile_function.o  ')
Execution platform: @bazel_tools//platforms:host_platform
INVALID ARGUMENTS: Merge of two inputs that differ on more than one predicate {s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater:0,else), s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/Greater/_303__cf__306:0,then), s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/cond/pred_id/_304__cf__307:0,then)} and {s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater:0,else), s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/Greater/_303__cf__306:0,else), s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/cond/pred_id/_304__cf__307:0,else)}
	for node {{node Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/cond/Merge}}

And when I try to compile our own customized SSD-ResNet (with additional preprocessing operation to decode an encoded image string).  The following error occur:
>INFO: Invocation ID: 63eccf9a-fa37-427f-ac89-e746f28c80b5
INFO: Analysed target @org_tensorflow//:graph (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: /home/syntech/.cache/bazel/_bazel_syntech/186a880d737f29aade7a2d700e840c56/external/org_tensorflow/BUILD:4:1: Executing genrule @org_tensorflow//:gen_graph failed (Exit 1)
2018-12-07 15:25:48.709118: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
INVALID ARGUMENTS: Detected unsupported operations when trying to compile graph tfcompile on XLA_CPU_JIT: Shape (No registered 'Shape' OpKernel for XLA_CPU_JIT devices compatible with node {{node map/Shape}}
	 (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_STRING, out_type=DT_INT32
	.  Registered:  device='XLA_CPU_JIT'; out_type in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_HALF, DT_UINT32, DT_UINT64]
){{node map/Shape}}


"
24299,Keras + Estimator training step evaluated twice in tensorflow 1.10 but not in >=1.11,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave 10.14
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): pipenv
- TensorFlow version (use command below): (v1.10.0-rc1-19-g656e7a2b34 1.10.0) and (v1.11.0-rc2-4-gc19e29306c 1.11.0)
- Python version: 3.5.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


**Describe the current behavior**

In 1.10 (**latest ml-engine supported tensorflow**), using Keras + Estimator API results in the training step seemingly being evaluated twice.

I've also tried writing my own custom estimator and the problem goes away, which indicates this is an interaction between Keras and the Estimator API.

The initial reason why I found this is because my model was getting worse results in gcloud than in local, which led me down this path.

cc: @fchollet 

https://stackoverflow.com/questions/53699535/getting-worse-results-on-gcloud-vs-local-training/

**Describe the expected behavior**

In >=1.11, the training step is evaluated once (correctly). However, 1.11 is not supported in gcloud ml-engine.

**Code to reproduce the issue**
1. Download `https://github.com/GoogleCloudPlatform/cloudml-samples/tree/master/imdb`
2. Edit `./tensorflow/keras/requirements.txt` to use `tensorflow==1.10`, install dependencies with your favorite tool (I use pipenv)
3. Edit `sample.sh`, comment out the `rm ...` lines
4. Run `./sample.sh`
5. Edit `./tensorflow/keras/requirements.txt` to use `tensorflow==1.11`, install dependencies with your favorite tool (I use pipenv)
6. Run `./sample.sh`

**Tensorboard evidence**

Here's a sample tensorboard training loss:
![image](https://user-images.githubusercontent.com/692818/49831612-27626580-fd49-11e8-8532-6aacb7719714.png)
Orange is tensorflow 1.10, Red is tensorflow 1.11

#### Output log for 1.10

Calling attention on the doubled training lines, e.g.:
```
INFO:tensorflow:loss = 0.33765173, step = 1202 (0.980 sec)
INFO:tensorflow:global_step/sec: 205.573
INFO:tensorflow:global_step/sec: 196.979
```

```
(keras) bash-3.2$ ./sample.sh
Running 'tensorflow/keras' code sample.
Copying gs://cloud-samples-data/ml-engine/imdb/imdb.npz...
\ [1 files][ 16.7 MiB/ 16.7 MiB]
Operation completed over 1 objects/16.7 MiB.
Copying gs://cloud-samples-data/ml-engine/imdb/imdb_word_index.json...
- [1 files][  1.6 MiB/  1.6 MiB]
Operation completed over 1 objects/1.6 MiB.
/Users/davidma/.local/share/virtualenvs/keras-jsYBpDFJ/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5
  return f(*args, **kwds)
INFO:tensorflow:Using the Keras model provided.
INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_save_checkpoints_steps': 500, '_num_worker_replicas': 1, '_evaluation_master': '', '_session_config': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x116fee9b0>, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_is_chief': True, '_master': '', '_tf_random_seed': None, '_save_summary_steps': 100, '_train_distribute': None, '_task_type': 'worker', '_keep_checkpoint_every_n_hours': 10000, '_device_fn': None, '_log_step_count_steps': 100, '_model_dir': 'iris_20181211_132929', '_global_id_in_cluster': 0, '_service': None, '_task_id': 0}
2018-12-11 13:29:38.241599: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
INFO:tensorflow:Running training and evaluation locally (non-distributed).
INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 500 or save_checkpoints_secs None.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Restoring parameters from iris_20181211_132929/keras_model.ckpt
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Saving checkpoints for 0 into iris_20181211_132929/model.ckpt.
INFO:tensorflow:loss = 0.70380175, step = 2
INFO:tensorflow:global_step/sec: 79.0904
INFO:tensorflow:global_step/sec: 52.5143
INFO:tensorflow:loss = 0.6854124, step = 202 (3.168 sec)
INFO:tensorflow:global_step/sec: 200.878
INFO:tensorflow:loss = 0.6641856, step = 402 (1.008 sec)
INFO:tensorflow:global_step/sec: 196.043
INFO:tensorflow:Saving checkpoints for 500 into iris_20181211_132929/model.ckpt.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Starting evaluation at 2018-12-11-21:29:58
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Restoring parameters from iris_20181211_132929/model.ckpt-500
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Finished evaluation at 2018-12-11-21:29:59
INFO:tensorflow:Saving dict for global step 500: accuracy = 0.7399388, global_step = 500, loss = 0.6379081
INFO:tensorflow:Saving 'checkpoint_path' summary for global step 500: iris_20181211_132929/model.ckpt-500
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Signatures INCLUDED in export for Regress: None
INFO:tensorflow:Signatures INCLUDED in export for Classify: None
INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']
INFO:tensorflow:Signatures INCLUDED in export for Train: None
INFO:tensorflow:Signatures INCLUDED in export for Eval: None
INFO:tensorflow:Restoring parameters from iris_20181211_132929/model.ckpt-500
INFO:tensorflow:Assets added to graph.
INFO:tensorflow:No assets to write.
INFO:tensorflow:SavedModel written to: iris_20181211_132929/export/exporter/temp-b'1544563800'/saved_model.pb
INFO:tensorflow:global_step/sec: 27.1525
INFO:tensorflow:loss = 0.5980946, step = 602 (4.209 sec)
INFO:tensorflow:global_step/sec: 189.933
INFO:tensorflow:global_step/sec: 200.593
INFO:tensorflow:loss = 0.4988567, step = 802 (1.005 sec)
INFO:tensorflow:global_step/sec: 197.686
INFO:tensorflow:global_step/sec: 204.341
INFO:tensorflow:Saving checkpoints for 1000 into iris_20181211_132929/model.ckpt.
INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (10 secs).
INFO:tensorflow:loss = 0.40151688, step = 1002 (1.476 sec)
INFO:tensorflow:global_step/sec: 100.741
INFO:tensorflow:global_step/sec: 201.636
INFO:tensorflow:loss = 0.33765173, step = 1202 (0.980 sec)
INFO:tensorflow:global_step/sec: 205.573
INFO:tensorflow:global_step/sec: 196.979
INFO:tensorflow:loss = 0.31514037, step = 1402 (0.998 sec)
INFO:tensorflow:global_step/sec: 207.539
INFO:tensorflow:Saving checkpoints for 1500 into iris_20181211_132929/model.ckpt.
INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (10 secs).
INFO:tensorflow:global_step/sec: 102.045
INFO:tensorflow:loss = 0.24642324, step = 1602 (1.475 sec)
INFO:tensorflow:global_step/sec: 202.219
INFO:tensorflow:global_step/sec: 200.376
INFO:tensorflow:loss = 0.23321539, step = 1802 (0.993 sec)
INFO:tensorflow:global_step/sec: 200.815
INFO:tensorflow:global_step/sec: 206.724
INFO:tensorflow:Saving checkpoints for 1954 into iris_20181211_132929/model.ckpt.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Starting evaluation at 2018-12-11-21:30:09
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Restoring parameters from iris_20181211_132929/model.ckpt-1954
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Finished evaluation at 2018-12-11-21:30:10
INFO:tensorflow:Saving dict for global step 1954: accuracy = 0.875825, global_step = 1954, loss = 0.3054119
INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1954: iris_20181211_132929/model.ckpt-1954
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Signatures INCLUDED in export for Regress: None
INFO:tensorflow:Signatures INCLUDED in export for Classify: None
INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']
INFO:tensorflow:Signatures INCLUDED in export for Train: None
INFO:tensorflow:Signatures INCLUDED in export for Eval: None
INFO:tensorflow:Restoring parameters from iris_20181211_132929/model.ckpt-1954
INFO:tensorflow:Assets added to graph.
INFO:tensorflow:No assets to write.
INFO:tensorflow:SavedModel written to: iris_20181211_132929/export/exporter/temp-b'1544563810'/saved_model.pb
INFO:tensorflow:Loss for final step: 0.2535662.
Python script succeeded
```

#### Output log for 1.11

Note expected behavior one one training line per loss line:
```
INFO:tensorflow:loss = 0.10183072, step = 1601 (0.973 sec)
INFO:tensorflow:global_step/sec: 100.407
```

```
(keras) bash-3.2$ ./sample.sh
Running 'tensorflow/keras' code sample.
Copying gs://cloud-samples-data/ml-engine/imdb/imdb.npz...
- [1 files][ 16.7 MiB/ 16.7 MiB]
Operation completed over 1 objects/16.7 MiB.
Copying gs://cloud-samples-data/ml-engine/imdb/imdb_word_index.json...
- [1 files][  1.6 MiB/  1.6 MiB]
Operation completed over 1 objects/1.6 MiB.
INFO:tensorflow:Using the Keras model provided.
2018-12-11 13:10:57.544772: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
INFO:tensorflow:Using config: {'_train_distribute': None, '_service': None, '_is_chief': True, '_num_worker_replicas': 1, '_eval_distribute': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11998c0b8>, '_keep_checkpoint_every_n_hours': 10000, '_device_fn': None, '_tf_random_seed': None, '_global_id_in_cluster': 0, '_log_step_count_steps': 100, '_task_id': 0, '_save_checkpoints_steps': 500, '_master': '', '_experimental_distribute': None, '_save_checkpoints_secs': None, '_keep_checkpoint_max': 5, '_save_summary_steps': 100, '_evaluation_master': '', '_num_ps_replicas': 0, '_protocol': None, '_model_dir': 'iris_20181211_131049', '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_task_type': 'worker'}
INFO:tensorflow:Running training and evaluation locally (non-distributed).
INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 500 or save_checkpoints_secs None.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='iris_20181211_131049/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})
INFO:tensorflow:Warm-starting from: ('iris_20181211_131049/keras/keras_model.ckpt',)
INFO:tensorflow:Warm-starting variable: dense/bias; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: dense/kernel; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: embedding/embeddings; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: dense_1/kernel; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: dense_1/bias; prev_var_name: Unchanged
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Saving checkpoints for 0 into iris_20181211_131049/model.ckpt.
INFO:tensorflow:loss = 0.69291425, step = 1
INFO:tensorflow:global_step/sec: 58.3576
INFO:tensorflow:loss = 0.6530326, step = 101 (1.713 sec)
INFO:tensorflow:global_step/sec: 103.954
INFO:tensorflow:loss = 0.52952975, step = 201 (0.961 sec)
INFO:tensorflow:global_step/sec: 98.6123
INFO:tensorflow:loss = 0.38874263, step = 301 (1.014 sec)
INFO:tensorflow:global_step/sec: 104.086
INFO:tensorflow:loss = 0.29528034, step = 401 (0.961 sec)
INFO:tensorflow:Saving checkpoints for 500 into iris_20181211_131049/model.ckpt.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Starting evaluation at 2018-12-11-21:11:17
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Restoring parameters from iris_20181211_131049/model.ckpt-500
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Finished evaluation at 2018-12-11-21:11:19
INFO:tensorflow:Saving dict for global step 500: accuracy = 0.8717262, global_step = 500, loss = 0.31936663
INFO:tensorflow:Saving 'checkpoint_path' summary for global step 500: iris_20181211_131049/model.ckpt-500
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Signatures INCLUDED in export for Train: None
INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']
INFO:tensorflow:Signatures INCLUDED in export for Regress: None
INFO:tensorflow:Signatures INCLUDED in export for Classify: None
INFO:tensorflow:Signatures INCLUDED in export for Eval: None
INFO:tensorflow:Restoring parameters from iris_20181211_131049/model.ckpt-500
WARNING:tensorflow:From /Users/davidma/.local/share/virtualenvs/keras-jsYBpDFJ/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py:1018: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.
Instructions for updating:
Pass your op to the equivalent parameter main_op instead.
INFO:tensorflow:Assets added to graph.
INFO:tensorflow:No assets to write.
INFO:tensorflow:SavedModel written to: iris_20181211_131049/export/exporter/temp-b'1544562680'/saved_model.pb
INFO:tensorflow:global_step/sec: 23.9337
INFO:tensorflow:loss = 0.2431845, step = 501 (4.178 sec)
INFO:tensorflow:global_step/sec: 100.357
INFO:tensorflow:loss = 0.23459233, step = 601 (0.997 sec)
INFO:tensorflow:global_step/sec: 101.69
INFO:tensorflow:loss = 0.19313249, step = 701 (0.983 sec)
INFO:tensorflow:global_step/sec: 102.563
INFO:tensorflow:loss = 0.16869232, step = 801 (0.975 sec)
INFO:tensorflow:global_step/sec: 103.525
INFO:tensorflow:loss = 0.18477763, step = 901 (0.966 sec)
INFO:tensorflow:Saving checkpoints for 1000 into iris_20181211_131049/model.ckpt.
INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (10 secs).
INFO:tensorflow:global_step/sec: 68.294
INFO:tensorflow:loss = 0.17220864, step = 1001 (1.464 sec)
INFO:tensorflow:global_step/sec: 99.2842
INFO:tensorflow:loss = 0.1683223, step = 1101 (1.007 sec)
INFO:tensorflow:global_step/sec: 101.347
INFO:tensorflow:loss = 0.16812363, step = 1201 (0.986 sec)
INFO:tensorflow:global_step/sec: 101.03
INFO:tensorflow:loss = 0.13969234, step = 1301 (0.990 sec)
INFO:tensorflow:global_step/sec: 103.339
INFO:tensorflow:loss = 0.14550006, step = 1401 (0.968 sec)
INFO:tensorflow:Saving checkpoints for 1500 into iris_20181211_131049/model.ckpt.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Starting evaluation at 2018-12-11-21:11:31
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Restoring parameters from iris_20181211_131049/model.ckpt-1500
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Finished evaluation at 2018-12-11-21:11:32
INFO:tensorflow:Saving dict for global step 1500: accuracy = 0.8762469, global_step = 1500, loss = 0.32633907
INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1500: iris_20181211_131049/model.ckpt-1500
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Signatures INCLUDED in export for Train: None
INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']
INFO:tensorflow:Signatures INCLUDED in export for Regress: None
INFO:tensorflow:Signatures INCLUDED in export for Classify: None
INFO:tensorflow:Signatures INCLUDED in export for Eval: None
INFO:tensorflow:Restoring parameters from iris_20181211_131049/model.ckpt-1500
INFO:tensorflow:Assets added to graph.
INFO:tensorflow:No assets to write.
INFO:tensorflow:SavedModel written to: iris_20181211_131049/export/exporter/temp-b'1544562692'/saved_model.pb
INFO:tensorflow:global_step/sec: 31.6362
INFO:tensorflow:loss = 0.11526476, step = 1501 (3.161 sec)
INFO:tensorflow:global_step/sec: 102.767
INFO:tensorflow:loss = 0.10183072, step = 1601 (0.973 sec)
INFO:tensorflow:global_step/sec: 100.407
INFO:tensorflow:loss = 0.092377335, step = 1701 (0.996 sec)
INFO:tensorflow:global_step/sec: 104.123
INFO:tensorflow:loss = 0.077209204, step = 1801 (0.960 sec)
INFO:tensorflow:global_step/sec: 102.508
INFO:tensorflow:loss = 0.1030057, step = 1901 (0.976 sec)
INFO:tensorflow:Saving checkpoints for 1954 into iris_20181211_131049/model.ckpt.
INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (10 secs).
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Starting evaluation at 2018-12-11-21:11:38
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Restoring parameters from iris_20181211_131049/model.ckpt-1954
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Finished evaluation at 2018-12-11-21:11:39
INFO:tensorflow:Saving dict for global step 1954: accuracy = 0.86955726, global_step = 1954, loss = 0.38054743
INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1954: iris_20181211_131049/model.ckpt-1954
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Signatures INCLUDED in export for Train: None
INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']
INFO:tensorflow:Signatures INCLUDED in export for Regress: None
INFO:tensorflow:Signatures INCLUDED in export for Classify: None
INFO:tensorflow:Signatures INCLUDED in export for Eval: None
INFO:tensorflow:Restoring parameters from iris_20181211_131049/model.ckpt-1954
INFO:tensorflow:Assets added to graph.
INFO:tensorflow:No assets to write.
INFO:tensorflow:SavedModel written to: iris_20181211_131049/export/exporter/temp-b'1544562699'/saved_model.pb
INFO:tensorflow:Loss for final step: 0.05532141.
Python script succeeded
```"
24296,Compilation failed when building keras model with CTC on TPU,"**System information**
- Google Colab with TPU


**Describe the current behavior**
Keras model with CTC on TPU gives error

**Describe the expected behavior**
should work as it does on GPU and CPU backends

**Code to reproduce the issue**
I made a small modification to the MNIST TPU example (https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/keras_mnist.ipynb)
just to test whether TPUs currently support CTC
```
def ctc_batch_cost(args):
    y_pred,y_true = args
    input_length = y_true[:,1:2]+5
    label_length = y_true[:,1:2]+5
    return K.ctc_batch_cost(y_true, y_pred, input_length, label_length)
  
inp = tf.keras.Input(shape=(28*28,))
x = l.Reshape(input_shape=(28*28,), target_shape=(28, 28))(inp)
x = l.Dense(10, activation='softmax')(x)
loss = l.Lambda(ctc_batch_cost, output_shape=(1,), name='ctc')([x, inp ])

model = tf.keras.Model(inputs=[inp], outputs=loss)

model.compile(optimizer='adam', 
              loss={'ctc': lambda y_true, y_pred: y_pred},
              metrics=['accuracy'])
```
unfortunately, while this code works with the GPU backend, for the TPU backend, it gives

> RuntimeError: Compilation failed: Compilation failure: Detected unsupported operations when trying to compile graph cluster_1_6470206916041614137_f15n_1[] on XLA_TPU_JIT: Where (No registered 'Where' OpKernel for XLA_TPU_JIT devices compatible with node {{node tpu_139921521852368/ctc/boolean_mask/Where}} = Where[T=DT_BOOL, _device=""/device:TPU_REPLICATED_CORE""](tpu_139921521852368/ctc/boolean_mask/Reshape_1)
> 	.  Registered:  device='CPU'; T in [DT_BOOL]
>   device='CPU'; T in [DT_COMPLEX128]
>   device='CPU'; T in [DT_COMPLEX64]
>   device='CPU'; T in [DT_DOUBLE]
>   device='CPU'; T in [DT_FLOAT]
>   device='CPU'; T in [DT_BFLOAT16]
>   device='CPU'; T in [DT_HALF]
>   device='CPU'; T in [DT_INT8]
>   device='CPU'; T in [DT_UINT8]
>   device='CPU'; T in [DT_INT16]
>   device='CPU'; T in [DT_UINT16]
>   device='CPU'; T in [DT_INT32]
>   device='CPU'; T in [DT_INT64]
> ){{node tpu_139921521852368/ctc/boolean_mask/Where}}

is there any workaround for this ? 
CTC has only a CPU implementation, and the GPU backed offloads it to the CPU, why isn't this also the case for TPUs ?

Thanks,"
24292,tf.contrib.saved_model.save_keras_model(),"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
24291,gtx 1050ti rog mining problem,"ETH: 1 pool is specified
Main Ethereum pool is eu1-etc.ethermine.org:4444
DCR: 0 pool is specified
No AMD cards in the list.

Driver 368.81 is recommended for best performance and compatibility
Be careful with overclocking, use default clocks for first tests
Press ""s"" for current statistics, ""0""..""9"" to turn on/off cards
CUDA initializing...

NVIDIA Cards available: 1
CUDA Driver Version/Runtime Version: 9.2/6.5
GPU #0: GeForce GTX 1050 Ti, 4096 MB available, 6 compute units, capability: 6.1

Total cards: 1
ETH: Stratum - connecting to 'eu1-etc.ethermine.org' <172.65.232.146> port 4444
ETH: Stratum - Connected (eu1-etc.ethermine.org:4444)
No pool specified for Decred! Ethereum-only mining mode is enabled
ETH: Authorized
ETHEREUM-ONLY MINING MODE ENABLED (-mode 1)
ETH: eth-proxy stratum mode
Watchdog enabled
Remote management is enabled on port 3333

ETH: Cannot find seed that was received from pool!
Setting epoch failed.
ETH: Cannot find seed that was received from pool!
Setting epoch failed.
ETH: 12/11/18-16:34:38 - New job from eu1-etc.ethermine.org:4444
ETH - Total Speed: 0.000 Mh/s, Total Shares: 0, Rejected: 0, Time: 00:00
ETH: GPU0 0.000 Mh/s
ETH: 12/11/18-16:34:50 - New job from eu1-etc.ethermine.org:4444
ETH - Total Speed: 0.000 Mh/s, Total Shares: 0, Rejected: 0, Time: 00:00
ETH: GPU0 0.000 Mh/s
ETH: 12/11/18-16:34:52 - New job from eu1-etc.ethermine.org:4444
ETH - Total Speed: 0.000 Mh/s, Total Shares: 0, Rejected: 0, Time: 00:00
ETH: GPU0 0.000 Mh/s
GPU0 t=26C fan=100%
WATCHDOG: GPU error, you need to restart miner :(
Restarting OK, exit...

what is this help
pls i dont know "
24289,TFLITE for microcontroller installation fails,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24288,"tensorflow.data.dataset and producing a sample weight with tf.py_func: ValueError: Found a sample_weight array with shape (?, 1). ","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
1.12
- Python version:
3.6

**Describe the current behavior**

I have a dataset created with tf.data. In the map function that is called a sample_weight is retrieved from a Python dictionary. Because Tensorflow cannot know the shape of an value return by py_func I set the shape by sample_weight.set_shape(1). However, when calling model.fit() this results in

> ValueError: Found a sample_weight array with shape (?, 1). In order to use timestep-wise sample weights, you should specify sample_weight_mode=""temporal"" in compile(). If you just mean to use sample-wise weights, make sure your sample_weight array is 1D.

**Describe the expected behavior**

I want a 1D sample_weight

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
def get_sample_weight(team, number):
    # Some code
    return np.array([0.5]).astype(np.float32) #return weight

def preprocess_tfrecord_example(example, sample_weights):
    image = tf.image.decode_png(example['detection/encoded'])
    team = example['detection/team_name']

    sample_weight = tf.py_func(get_sample_weight, [team, example['detection/number']], tf.float32)
    sample_weight.set_shape(1)
    image = tf.image.resize_image_with_pad(image,56,108)

    number = tf.expand_dims(tf.cast(example['detection/number'], tf.float32), -1)
    return image/255., number, sample_weight

```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24286,wrong command for cloning tensorflow in TF for microcontroller doc,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: master
- Doc Link: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro


**Describe the documentation issue**

In ""Getting Started"" chapter, the command for cloning tensorflos is: 

Download the TensorFlow source with git clone https://github.com/tensorflow

It should be : 

git clone https://github.com/tensorflow/tensorflow.git

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**

Yes"
24284,I ,"# Convolutional Neural Network

# Installing Theano
# pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git

# Installing Tensorflow
# pip install tensorflow

# Installing Keras
# pip install --upgrade keras

# Part 1 - Building the CNN

# Importing the Keras libraries and packages
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Flatten
from keras.layers import Dense

# Initialising the CNN
classifier = Sequential()

# Step 1 - Convolution
classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))

# Step 2 - Pooling
classifier.add(MaxPooling2D(pool_size = (2, 2)))

# Adding a second convolutional layer
classifier.add(Conv2D(32, (3, 3), activation = 'relu'))
classifier.add(MaxPooling2D(pool_size = (2, 2)))

# Step 3 - Flattening
classifier.add(Flatten())

# Step 4 - Full connection
classifier.add(Dense(units = 128, activation = 'relu'))
classifier.add(Dense(units = 1, activation = 'sigmoid'))

# Compiling the CNN
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

# Part 2 - Fitting the CNN to the images

from keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(rescale = 1./255,
                                   shear_range = 0.2,
                                   zoom_range = 0.2,
                                   horizontal_flip = True)

test_datagen = ImageDataGenerator(rescale = 1./255)

training_set = train_datagen.flow_from_directory('dataset/training_set',
                                                 target_size = (64, 64),
                                                 batch_size = 32,
                                                 class_mode = 'binary')

test_set = test_datagen.flow_from_directory('dataset/test_set',
                                            target_size = (64, 64),
                                            batch_size = 32,
                                            class_mode = 'binary')

classifier.fit_generator(training_set,
                         steps_per_epoch = 1000,
                         epochs = 25,
                         validation_data = test_set,
                         validation_steps = 500)

from keras.models import load_model

classifier.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'


classifier = load_model('acanth.h5')

import numpy as np
from keras.preprocessing import image
test_image = image.load_img('dataset/single_prediction/acanth.jpg', target_size = (64, 64))
test_image = image.img_to_array(test_image)
test_image = np.expand_dims(test_image, axis = 0)
result = classifier.predict(test_image)
training_set.class_indices
if result[0][0] == 0:
    prediction = 'Acanthoplus'
else:
    prediction = 'either cut worm or army worm'"
24281,Install Go TensorFlow error,"**System information**
- OS Platform and Distribution (Linux Ubuntu 18.10 x64)
- Go Version: go version go1.11.2 linux/amd64


I refer to this [guide](https://www.tensorflow.org/install/lang_go) to install Go TensorFlow, I have encountered some problems

**Describe the problem**

install TensorFlow-C

run shell in command line:

```shell

 TF_TYPE=""cpu"" # Change to ""gpu"" for GPU support
 TARGET_DIRECTORY='/usr/local'
 curl -L \
   ""https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-${TF_TYPE}-$(go env GOOS)-x86_64-1.8.0.tar.gz"" |
 sudo tar -C $TARGET_DIRECTORY -xz

```

```shell
cd /usr/local/lib

ls |grep *tensorflow*
 
libtensorflow.so
```


```shell

sudo ldconfig

```

```shell

go get github.com/tensorflow/tensorflow/tensorflow/go

```

error info

```text
# github.com/tensorflow/tensorflow/tensorflow/go
/usr/bin/ld: $WORK/b001/_x003.o: in function `_cgo_8f42b3a84d7b_Cfunc_TF_AddGradientsWithPrefix':
/tmp/go-build/cgo-gcc-prolog:63: undefined reference to `TF_AddGradientsWithPrefix'
collect2: error: ld returned 1 exit status
```

how can i fix this problem
"
24280,Why the source code of tensorflow 1.12.0 after installation is quite different from the source code in github?,"I have installed tensorflow 1.12.0, when I check the source code inside python2.7/site-packages, I found some files are quite different from the code in github?

Specifically, /keras/engine/training.py and /keras/engine/training_utiles.py are different.

May I know what is the problem?"
24277,Attributeerror: module tensorflow.contrib.lite.python.lite has no attribute TFLiteConverter,"**System information**
- Raspberry Pi 3B+ Linux
- TensorFlow installed in Python using pip3; TensorFlow Lite installed and built (static library) from source
- TensorFlow version (installed in Python) is 1.11.0

Trying to convert a frozen graph file (.pb) to a TensorFlow Lite FlatBuffer file (.tflite), I am getting the error:
Attributeerror: module 'tensorflow.contrib.lite.python.lite' has no attribute 'TFLiteConverter'.

Here is the Python code (derived from https://www.tensorflow.org/lite/convert/python_api):

#!/usr/bin/python3
import tensorflow as tf
print(""tf version = "" + tf.__version__)
graph_def_file = ""/home/pi/sols/demo/src/image_classification/network/fruit_models/frozen_graph.pb""
input_arrays = [""X""]
output_arrays = [""softmax""]
converter = tf.contrib.lite.TFLiteConverter.from_frozen_graph(
  graph_def_file, input_arrays, output_arrays)
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)

Please help!
"
24276,"When train a model with Dataset, model.predict() does not work properly","
**Describe the current behavior**
In Tensorflow's guide about feeding a model with dataset: Input tf.data datasets. The way given by the example to use dataset to train and predict cannot work properly.
Version used: **1.12.0**

**Describe the expected behavior**
model.predict() will return the correct predicted value for the given data.

**Code to reproduce the issue**
```
model = tf.keras.Sequential([
layers.Dense(64, activation='relu'),
layers.Dense(64, activation='relu'),
layers.Dense(10, activation='softmax')])
model.compile(optimizer=tf.train.AdamOptimizer(0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])
data = np.random.random((1000, 32)).astype(np.float32)
labels = np.random.random((1000, 10)).astype(np.float32)
dataset = tf.data.Dataset.from_tensor_slices((data, labels))
dataset = dataset.batch(32)
dataset = dataset.repeat()
model.fit(dataset, epochs=10, steps_per_epoch=30)
result = model.predict(data, batch_size=32)
print(result.shape)
```
**Other info / logs**
Stack trace:

> Traceback (most recent call last):
>   File ""xxx.py"", line 255, in <module>
>     result = model.predict(data, batch_size=32)
>   File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/training.py"", line 1864, in predict
>     x, check_steps=True, steps_name='steps', steps=steps)
>   File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/training.py"", line 992, in _standardize_user_data
>     class_weight, batch_size)
>   File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/training.py"", line 1117, in _standardize_weights
>     exception_prefix='input')
>   File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/training_utils.py"", line 257, in standardize_input_data
>     'expected no data, but got:', data)
> ValueError: ('Error when checking model input: expected no data, but got:', array([[5.7695776e-01, 2.4554299e-01, 6.4072663e-01, ..., 5.3916293e-01,
>         5.7996696e-01, 1.7735572e-05],
>        [9.4239563e-01, 6.3614279e-01, 5.1306045e-01, ..., 3.2898217e-01,
>         1.9013086e-01, 1.5709637e-01],
>        [9.4871348e-01, 3.7692937e-01, 1.2676715e-02, ..., 2.7022544e-01,
>         7.6909864e-01, 8.5889444e-02],
>        ...,
>        [8.3822417e-01, 9.3324584e-01, 8.0759123e-02, ..., 1.3924435e-01,
>         2.8058958e-01, 3.2993883e-01],
>        [8.9985609e-01, 1.6618696e-01, 2.4748170e-01, ..., 5.6301647e-01,
>         6.0354501e-01, 1.3682739e-01],
>        [2.7904582e-01, 8.5806495e-01, 1.5504396e-01, ..., 3.0398217e-01,
>         4.7553891e-01, 4.4561669e-01]], dtype=float32))
"
24274,model uses GPU memory even for explicit CPU calls and fails,"Hello,

Every bit of my code remains on the CPU until model.fit() is called as below:

with tf.device('/cpu:0'):
    model.fit(ar_train_data,train_label,validation_data=(ar_valid_data,valid_label),epochs=10,batch_size=batch_size)
As seen, am trying to run the code explicitly on CPU but, it doesn't seem to work and the model takes up all of the free GPU. (PID 4469 is my process as suggested)
Even, on trying to run on GPU the Jupyter notebook kernel just crashes when model.fit() is called.

![49684205-9ac06900-faf6-11e8-8825-890c938a6e79](https://user-images.githubusercontent.com/42785357/49767165-4d760000-fc8c-11e8-8fdf-b173c627f4c5.png)

I tried controlling this behaviour such that it uses 50 pc GPU like so, but that didn't work either:

config = tf.ConfigProto(device_count={'GPU':1,'CPU':1})
config.gpu_options.allow_growth = True
config.gpu_options.per_process_gpu_memory_fraction = 0.5
sess = tf.Session(config=config)
k.set_session(sess)
Keras version: 2.2.4
Tensorflow version: 1.5.0

Can anyone help here?
Is there any issue with my versions?

Regards"
24270,NameError: name 'disclaim_key_flags' is not defined,"After update version 1.12 the error shows as below;
----> 1 import tensorflow as tf

~/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py in <module>()
     20
     21 # pylint: disable=g-bad-import-order
---> 22 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     23
     24 try:

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py in <module>()
     61
     62 # Framework
---> 63 from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin
     64 from tensorflow.python.framework.versions import *
     65 from tensorflow.python.framework import errors

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/framework_lib.py in <module>()
     23 # Classes used when building a Graph.
     24 from tensorflow.python.framework.device import DeviceSpec
---> 25 from tensorflow.python.framework.ops import Graph
     26 from tensorflow.python.framework.ops import Operation
     27 from tensorflow.python.framework.ops import Tensor

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in <module>()
     53 from tensorflow.python.framework import versions
     54 from tensorflow.python.ops import control_flow_util
---> 55 from tensorflow.python.platform import app
     56 from tensorflow.python.platform import tf_logging as logging
     57 from tensorflow.python.util import compat

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py in <module>()
     22 import sys as _sys
     23
---> 24 from tensorflow.python.platform import flags
     25 from tensorflow.python.util.tf_export import tf_export
     26

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/flags.py in <module>()
     31 # Since we wrap absl.flags DEFINE functions, we need to declare this module
     32 # does not affect key flags.
---> 33 disclaim_key_flags()  # pylint: disable=undefined-variable
     34
     35

NameError: name 'disclaim_key_flags' is not defined

please help!!"
24268,Check failed: is_weights() on creating tensorrt inference graph,"**System information**
- OS Platform and Distribution: Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): r1.12
- Python version: 3.6.7
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: 10.0, 7.4.1.5
- GPU model and memory: GTX 1050ti, 4000mb

I am trying to create optimized inference graph with tensorflow.contrib.tensorrt.create_inference_graph. But when I call the function, it runs for a while, and than fails. The last line at logs is:

```2018-12-10 22:23:42.564811: F tensorflow/contrib/tensorrt/convert/convert_nodes.cc:421] Check failed: is_weights()```"
24267,Pruning RuntimeError: Run called even after should_stop requested.,"Hello when i run the code ""model_pruning example"", I change the parameter ""max_steps"" equal to 500, there is a bug: ""RuntimeError: Run called even after should_stop requested."" at ""mon_sess.run(mask_update_op)"". Can you tell me the reason? Thank you very much!"
24265,tf.data equivalent for tf.estimator.inputs.numpy_input_fn,"**System information**
- TensorFlow version (you are using): 1.11.0
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**

Using `tf.estimator.inputs.numpy_input_fn` in current TensorFlow raises deprecation warnings and suggests using tf.data instead. Unfortunately, tf.data does not have exact replacement. Moreover, [`tf.estimator.inputs.numpy_input_fn` seems to be working out-of-box](https://stackoverflow.com/questions/53677345/passing-2gb-data-to-tf-estimator) for some problems where tf.data raises errors and needs workarounds.

**Will this change the current api? How?**

Provide replacement function in tf.data:
`tf.estimator.inputs.numpy_input_fn` -> `tf.data.numpy_input_fn`

**Who will benefit with this feature?**

Anyone using numpy arrays as input for TensorFlow. Especially for arrays >2GB large. 

**Any Other info.**

None.
"
24263,TFLite Image recognization app is getting crashed with model Inception_resnet_V2,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):
- Python version:3.0
- Bazel version (if compiling from source):[bazel release 0.16.1]
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


**Describe the current behavior**;
Im using the source code from below repo:https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/android/app
Compiled using the readme in above link
The app works fine without any changes.

Enabled:
setUseNNAPI(true) 
Changed buffer size for float model:
DIM_IMG_SIZE_X = 299;
DIM_IMG_SIZE_Y = 299;
IMAGE_MEAN = 128;
IMAGE_STD = 128.0f;

Downloaded float inception resnet v2 model from: https://www.tensorflow.org/lite/models#other_models
unziped and placed in assests folder of source code: tensorflow/tensorflow/contrib/lite/examples/android/app/src/main/assets
Renamed the model name in ClassifierActivity.java

Compiled the app using bazel
While launching the app it crashed with File not found exception as below
====================================================================
12-10 11:09:02.319 15168 15168 I tensorflow: CameraActivity: Camera API lv2?: true
12-10 11:09:02.324 15168 15168 D tensorflow: CameraActivity: onStart org.tensorflow.demo.ClassifierActivity@ada003c
12-10 11:09:02.324 15168 15168 D tensorflow: CameraActivity: onResume org.tensorflow.demo.ClassifierActivity@ada003c
12-10 11:09:02.359 15168 15168 I tensorflow: CameraConnectionFragment: Desired size: 640x480, min size: 480x480
12-10 11:09:02.359 15168 15168 I tensorflow: CameraConnectionFragment: Valid preview sizes: [640x480, 800x600, 864x480, 960x544, 960x720, 1024x576, 1184x656, 1280x720, 1280x960]
12-10 11:09:02.360 15168 15168 I tensorflow: CameraConnectionFragment: Rejected preview sizes: [160x120, 320x176, 320x240, 432x240, 544x288, 640x360, 752x416, 800x448]
12-10 11:09:02.360 15168 15168 I tensorflow: CameraConnectionFragment: Exact size match found.
12-10 11:09:02.361 15168 15168 E TFLiteImageClassifier: Reading labels from: inception_v3_labels.txt
12-10 11:09:02.366 15168 15168 D AndroidRuntime: Shutting down VM
--------- beginning of crash
12-10 11:09:02.368 15168 15168 E AndroidRuntime: FATAL EXCEPTION: main
12-10 11:09:02.368 15168 15168 E AndroidRuntime: Process: org.tensorflow.lite.demo, PID: 15168
12-10 11:09:02.368 15168 15168 E AndroidRuntime: java.lang.RuntimeException: java.io.FileNotFoundException: inception_resnet_v2.tflite
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at org.tensorflow.demo.TFLiteImageClassifier.create(TFLiteImageClassifier.java:124)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at org.tensorflow.demo.ClassifierActivity.onPreviewSizeChosen(ClassifierActivity.java:105)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at org.tensorflow.demo.CameraActivity$5.onPreviewSizeChosen(CameraActivity.java:362)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at org.tensorflow.demo.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:401)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at org.tensorflow.demo.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:408)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at org.tensorflow.demo.CameraConnectionFragment.access$000(CameraConnectionFragment.java:64)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at org.tensorflow.demo.CameraConnectionFragment$1.onSurfaceTextureAvailable(CameraConnectionFragment.java:95)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.TextureView.getTextureLayer(TextureView.java:390)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.TextureView.draw(TextureView.java:339)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.View.updateDisplayListIfDirty(View.java:19082)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.View.draw(View.java:19935)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.ViewGroup.drawChild(ViewGroup.java:4333)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4112)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.View.updateDisplayListIfDirty(View.java:19073)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.View.draw(View.java:19935)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.ViewGroup.drawChild(ViewGroup.java:4333)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4112)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.View.draw(View.java:20210)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.View.updateDisplayListIfDirty(View.java:19082)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.View.draw(View.java:19935)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.ViewGroup.drawChild(ViewGroup.java:4333)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4112)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.View.updateDisplayListIfDirty(View.java:19073)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.View.draw(View.java:19935)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.ViewGroup.drawChild(ViewGroup.java:4333)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4112)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.View.updateDisplayListIfDirty(View.java:19073)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.View.draw(View.java:19935)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.ViewGroup.drawChild(ViewGroup.java:4333)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4112)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.View.draw(View.java:20210)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at com.android.internal.policy.DecorView.draw(DecorView.java:780)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.View.updateDisplayListIfDirty(View.java:19082)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.ThreadedRenderer.updateViewTreeDisplayList(ThreadedRenderer.java:686)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.ThreadedRenderer.updateRootDisplayList(ThreadedRenderer.java:692)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.ThreadedRenderer.draw(ThreadedRenderer.java:801)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.ViewRootImpl.draw(ViewRootImpl.java:3312)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.ViewRootImpl.performDraw(ViewRootImpl.java:3116)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:2485)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:1460)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:7184)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.Choreographer$CallbackRecord.run(Choreographer.java:949)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.Choreographer.doCallbacks(Choreographer.java:761)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.Choreographer.doFrame(Choreographer.java:696)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:935)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.os.Handler.handleCallback(Handler.java:873)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.os.Handler.dispatchMessage(Handler.java:99)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.os.Looper.loop(Looper.java:193)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.app.ActivityThread.main(ActivityThread.java:6669)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at java.lang.reflect.Method.invoke(Native Method)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:493)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:858)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: Caused by: java.io.FileNotFoundException: inception_resnet_v2.tflite
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.content.res.AssetManager.nativeOpenAssetFd(Native Method)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at android.content.res.AssetManager.openFd(AssetManager.java:768)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at org.tensorflow.demo.TFLiteImageClassifier.loadModelFile(TFLiteImageClassifier.java:77)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 	at org.tensorflow.demo.TFLiteImageClassifier.create(TFLiteImageClassifier.java:122)
12-10 11:09:02.368 15168 15168 E AndroidRuntime: 
==========================================================================

**Other info / logs**
The same changes for model; Inception_v3 downloaded from : https://www.tensorflow.org/lite/models#other_models

If I re-name model inception_resnet_v2 to inception_v3 the loads the model and works fine"
24262,Cannot build profiler on windows 10,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.12.0
- Python version: 3.6
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 1.16.1
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.0, 7.3.1
- GPU model and memory: NVidia GTX 1080




**Describe the problem**

I get an error while trying to build the profiler from source.


**Provide the exact sequence of commands / steps that you executed before running into the problem**
C:\Users\vignesh\Documents\tensorflow-1.12.0>bazel build --config=opt tensorflow/core/profiler:profiler
INFO: Build options have changed, discarding analysis cache.
INFO: Analysed target //tensorflow/core/profiler:profiler (0 packages loaded).
INFO: Found 1 target...
INFO: From Compiling external/com_google_absl/absl/base/internal/throw_delegate.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/base/internal/spinlock_wait.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/types/optional.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/internal/memutil.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/internal/charconv_parse.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/internal/charconv_bigint.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/str_replace.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/substitute.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/ascii.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/string_view.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/match.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/charconv.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/numbers.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/str_split.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
ERROR: C:/users/vignesh/_bazel_vignesh/g2rooyha/external/linenoise/BUILD.bazel:9:1: C++ compilation of rule '@linenoise//:linenoise' failed (Exit 2): msvc_wrapper_for_nvcc.bat failed: error executing command
  cd C:/users/vignesh/_bazel_vignesh/g2rooyha/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0
    SET CUDNN_INSTALL_PATH=C:/tools/cuda
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt;C:\Program Files (x86)\Windows Kits\8.1\include\shared;C:\Program Files (x86)\Windows Kits\8.1\include\um;C:\Program Files (x86)\Windows Kits\8.1\include\winrt;
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.10240.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\8.1\lib\winv6.3\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\WINDOWS\Microsoft.NET\Framework64\v4.0.30319;C:\WINDOWS\Microsoft.NET\Framework64\;C:\Program Files (x86)\Windows Kits\8.1\bin\x64;C:\Program Files (x86)\Windows Kits\8.1\bin\x86;;C:\WINDOWS\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/vignesh/AppData/Local/Programs/Python/Python36/python.exe
    SET PYTHON_LIB_PATH=C:/Users/vignesh/AppData/Local/Programs/Python/Python36/lib/site-packages
    SET TEMP=C:\Users\vignesh\AppData\Local\Temp
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1
    SET TF_CUDA_VERSION=9.0
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
    SET TMP=C:\Users\vignesh\AppData\Local\Temp
  external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.bat /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /Iexternal/linenoise /Ibazel-out/x64_windows-opt/genfiles/external/linenoise /Ibazel-out/x64_windows-opt/bin/external/linenoise /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/genfiles/external/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX /Fobazel-out/x64_windows-opt/bin/external/linenoise/_objs/linenoise/linenoise.o /c external/linenoise/linenoise.c
external/linenoise/linenoise.c(106): fatal error C1083: Cannot open include file: 'termios.h': No such file or directory
Target //tensorflow/core/profiler:profiler failed to build
INFO: Elapsed time: 19.622s, Critical Path: 11.13s
INFO: 138 processes: 138 local.
FAILED: Build did NOT complete successfully

"
24259,Build On aarch64 Server,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: v1.11.0
- Python version: 3.6
- Installed using virtualenv? pip? conda?: no
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source): 4.8.5
- CUDA/cuDNN version: no
- GPU model and memory:

**Describe the problem**
I build tensorflow on aarch64 server. But always got error. I do not understand!!

**Provide the exact sequence of commands / steps that you executed before running into the problem**
./configure

WARNING: Running Bazel server needs to be killed, because the startup options are different.
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.15.0- (@non-git) installed.
Please specify the location of python. [Default is /usr/bin/python]: /usr/local/python3/bin/python3


Found possible Python library paths:
  /usr/local/python3/lib/python3.6/site-packages
Please input the desired Python library path to use.  Default is [/usr/local/python3/lib/python3.6/site-packages]

Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: n
No jemalloc as malloc support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n
No Google Cloud Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: n
No Hadoop File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Amazon AWS Platform support? [Y/n]: n
No Amazon AWS Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: n
No Apache Kafka Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [y/N]: n
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with GDR support? [y/N]: n
No GDR support will be enabled for TensorFlow.

Do you wish to build TensorFlow with VERBS support? [y/N]: n
No VERBS support will be enabled for TensorFlow.

Do you wish to build TensorFlow with nGraph support? [y/N]: n
No nGraph support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: n
No CUDA support will be enabled for TensorFlow.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: n
Clang will not be downloaded.

Do you wish to build TensorFlow with MPI support? [y/N]: n
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]:


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See tools/bazel.rc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
Configuration finished



bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
Starting local Bazel server and connecting to it...
....................
WARNING: /home/fei/tensorflow/tensorflow/core/BUILD:2463:1: in includes attribute of cc_library rule //tensorflow/core:framework_internal_headers_lib: '../../external/com_google_absl' resolves to 'external/com_google_absl' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/fei/tensorflow/tensorflow/tensorflow.bzl:1373:20
WARNING: /home/fei/tensorflow/tensorflow/core/BUILD:2548:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/com_google_absl' resolves to 'external/com_google_absl' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/fei/tensorflow/tensorflow/tensorflow.bzl:1373:20
WARNING: /home/fei/.cache/bazel/_bazel_fei/6d85cbe619157ee2e987c2321228a66d/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/fei/.cache/bazel/_bazel_fei/6d85cbe619157ee2e987c2321228a66d/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /home/fei/.cache/bazel/_bazel_fei/6d85cbe619157ee2e987c2321228a66d/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/fei/.cache/bazel/_bazel_fei/6d85cbe619157ee2e987c2321228a66d/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /home/fei/.cache/bazel/_bazel_fei/6d85cbe619157ee2e987c2321228a66d/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/fei/.cache/bazel/_bazel_fei/6d85cbe619157ee2e987c2321228a66d/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /home/fei/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: /home/fei/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
WARNING: /home/fei/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:354:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/fei/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:230:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/fei/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:73:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/fei/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/fei/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/fei/tensorflow/tensorflow/core/BUILD:2562:1: in includes attribute of cc_library rule //tensorflow/core:stream_executor_headers_lib: '../../external/com_google_absl' resolves to 'external/com_google_absl' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/fei/tensorflow/tensorflow/tensorflow.bzl:1373:20
WARNING: /home/fei/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (305 packages loaded).
INFO: Found 1 target...
ERROR: /home/fei/.cache/bazel/_bazel_fei/6d85cbe619157ee2e987c2321228a66d/external/com_google_absl/absl/base/BUILD.bazel:117:1: C++ compilation of rule '@com_google_absl//absl/base:base' failed (Exit 1)
external/com_google_absl/absl/base/internal/cycleclock.cc:1:0: error: unknown value 'native' for -march
 // Copyright 2017 The Abseil Authors.
 ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 12.825s, Critical Path: 0.34s
INFO: 0 processes.
FAILED: Build did NOT complete successfully


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

  tensorflow git:(c19e293) bazel build --config=opt  --verbose_failures //tensorflow/tools/pip_package:build_pip_package
WARNING: /home/fei/tensorflow/tensorflow/core/BUILD:2463:1: in includes attribute of cc_library rule //tensorflow/core:framework_internal_headers_lib: '../../external/com_google_absl' resolves to 'external/com_google_absl' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/fei/tensorflow/tensorflow/tensorflow.bzl:1373:20
WARNING: /home/fei/tensorflow/tensorflow/core/BUILD:2548:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/com_google_absl' resolves to 'external/com_google_absl' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/fei/tensorflow/tensorflow/tensorflow.bzl:1373:20
WARNING: /home/fei/.cache/bazel/_bazel_fei/6d85cbe619157ee2e987c2321228a66d/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/fei/.cache/bazel/_bazel_fei/6d85cbe619157ee2e987c2321228a66d/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /home/fei/.cache/bazel/_bazel_fei/6d85cbe619157ee2e987c2321228a66d/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/fei/.cache/bazel/_bazel_fei/6d85cbe619157ee2e987c2321228a66d/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /home/fei/.cache/bazel/_bazel_fei/6d85cbe619157ee2e987c2321228a66d/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/fei/.cache/bazel/_bazel_fei/6d85cbe619157ee2e987c2321228a66d/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /home/fei/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: /home/fei/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
WARNING: /home/fei/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:354:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/fei/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:230:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/fei/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:73:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/fei/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/fei/tensorflow/tensorflow/core/BUILD:2562:1: in includes attribute of cc_library rule //tensorflow/core:stream_executor_headers_lib: '../../external/com_google_absl' resolves to 'external/com_google_absl' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/fei/tensorflow/tensorflow/tensorflow.bzl:1373:20
WARNING: /home/fei/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/fei/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded).
INFO: Found 1 target...
ERROR: /home/fei/.cache/bazel/_bazel_fei/6d85cbe619157ee2e987c2321228a66d/external/gif_archive/BUILD.bazel:8:1: C++ compilation of rule '@gif_archive//:gif' failed (Exit 1): gcc failed: error executing command
  (cd /home/fei/.cache/bazel/_bazel_fei/6d85cbe619157ee2e987c2321228a66d/execroot/org_tensorflow && \
  exec env - \
    PATH=/usr/local/bin:/usr/bin:/home/fei/bin:/usr/local/sbin:/usr/sbin \
    PWD=/proc/self/cwd \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/host/bin/external/gif_archive/_objs/gif/external/gif_archive/lib/dgif_lib.d -iquote external/gif_archive -iquote bazel-out/host/genfiles/external/gif_archive -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -isystem external/gif_archive/lib -isystem bazel-out/host/genfiles/external/gif_archive/lib -isystem bazel-out/host/bin/external/gif_archive/lib -g0 '-march=native' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/gif_archive/lib/dgif_lib.c -o bazel-out/host/bin/external/gif_archive/_objs/gif/external/gif_archive/lib/dgif_lib.o)
external/gif_archive/lib/dgif_lib.c:1:0: error: unknown value 'native' for -march
 /******************************************************************************
 ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 2.137s, Critical Path: 0.07s
INFO: 0 processes.
FAILED: Build did NOT complete successfully

"
24258,ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory,"**System information**
- OS Platform and Distribution (Linux Ubuntu 16.04):
- TensorFlow installed from (binary):
- TensorFlow version: 1.12.0 (tensorflow-gpu)
- Python version:2.7
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: 10
- GPU model and memory: NVIDIA GTX 1060 with Max-Q ; 16 gb DDR4 



**Describe the problem**

executing 
```shell
python -c ""import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))""
```
gives the error 

```
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/pence/venv/local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/home/pence/venv/local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/pence/venv/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/pence/venv/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/pence/venv/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/pence/venv/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

"
24257,ImportError: DLL load failed: The specified module could not be found.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.12.0
- Python version: 3.6.7(also tried 3.7.1)
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): Na
- GCC/Compiler version (if compiling from source): Na
- CUDA/cuDNN version: 9.0
- GPU model and memory: GeForce GTX 1060 6GB



**Describe the problem**

I executed ""pip install tensorflow"" command and then wanted to test it by opening python on command prompt. I typed ""python"" and ""import tensorflow"" then I got this error message below: 
 
Python 3.6.7 (v3.6.7:6ec5cf24b7, Oct 20 2018, 13:35:33) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""C:\Users\(User)\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\(User)\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\(User)\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\(User)\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\(User)\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\(User)\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\(User)\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\(User)\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\(User)\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\(User)\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\(User)\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\(User)\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\(User)\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
24254,Right bracket missing in code provided by tensorflow tutorial,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.

I believe that a right bracket is missing at the end of following code segment
```
plt.xlabel(""{} {:2.0f}% ({})"".format(class_names[predicted_label],
                                100*np.max(predictions_array),
                                class_names[true_label]),
                                color=color)
```
in part *Prediction*.
And the turtorial link is https://www.tensorflow.org/tutorials/keras/basic_classification
Not a big deal.
"
24253,Not able to read objets from private s3 bucket,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10, 7.3
- GPU model and memory: 4x 1080ti 11Gb


**Describe the current behavior**
i am not able to access any data from my private s3  bucket, i just keep getting this error
NotFoundError: Object s3://bla/bla does not exist
i have checked that my local machine has aws credentials in ~/.aws/credentials 
not only that i have also tried setting environment variables
```
os.environ['AWS_REGION'] = <my region>
os.environ['AWS_ACCESS_KEY_ID'] = < my creds > 
os.environ['AWS_SECRET_ACCESS_KEY'] = < my creds > 
```
It still doesnt let me access my data from s3
**Code to reproduce the issue**
```
from tensorflow.python.lib.io import file_io
file_io.stat(s3_image_url)
```
s3_image_url is a valid url for s3 object which i confirmed by downloading it with awscli


**logs**
```
2018-12-09 20:42:26.203155: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key
2018-12-09 20:42:26.203221: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-12-09 20:42:27.158491: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 403
2018-12-09 20:42:27.158586: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.
2018-12-09 20:42:27.158702: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key
2018-12-09 20:42:27.158882: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-12-09 20:42:27.388300: W tensorflow/core/platform/s3/aws_logging.cc:57] Encountered AWSError
SignatureDoesNotMatch
The request signature we calculated does not match the signature you provided. Check your key and signing method.:
2018-12-09 20:42:27.388399: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.
```"
24252,3D * 2D matrix multiplication without changing batch dimension,"I'm trying to implement tensordot in other ways because of some tensorrt issues.

In tf.tensordot op, there is a batch-size modification in the process of 3d * 2d matmul.

```
M = tf.random_normal((batch_size, n, m))  # (3,6,9)
N = tf.random_normal((m, p)) # (9,9)

MT = tf.reshape(M, [batch_size*n, m]) # (18,9)
MTN = tf.matmul(M_T, N) # (18,9)

MN = tf.reshape(MTN, [batch_size, n, p]) # (3,6,9)
```

But I want 3d*2d matmul without changing batch-size dimension.
Is there a way?
"
24251,Unicode header for cmake build,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.12
- Python version: N/A
- Installed using virtualenv? pip? conda?: no
- Bazel version (if compiling from source): cmake
- GCC/Compiler version (if compiling from source): MSVC 2015
- CUDA/cuDNN version:  9.0
- GPU model and memory: N/A



**Describe the problem**
[unicode_ops.cc](https://github.com/tensorflow/tensorflow/commits/d09a4a1cf2ef4fd0cda021ae73f942ae8b32598a/tensorflow/core/kernels/unicode_ops.cc) includes the ICU header, however this only available on  https://www.npcglib.org/~stathis/blog/precompiled-icu/ for a pre-built release, with 7zip format

Can anyone help to add this to cmake contrib?

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24248,Support for `skip_mismatch` in `tf.keras.engine.saving.load_weights_from_hdf5_group_by_name`,"**System information**
- TensorFlow version (you are using): 
```
tensorflow                1.12.0          gpu_py36he68c306_0    anaconda
tensorflow-base           1.12.0          gpu_py36h8e0ae2d_0    anaconda
tensorflow-gpu            1.12.0               h0d30ee6_0    anaconda
```

The feature is already supported in `keras-team/keras` and the behaviour and motivation is already explained in: https://github.com/keras-team/keras/pull/8462
"
24244,"build windows ""Failed to load the native TensorFlow runtime.""","i got this error when i ran this
Microsoft Windows [Version 10.0.17134.407]
(c) 2018 Microsoft Corporation. All rights reserved.

C:\Users\Nichole>git bash
git: 'bash' is not a git command. See 'git --help'.

The most similar command is
        stash

C:\Users\Nichole>git
usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]
           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]
           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]
           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]
           <command> [<args>]

These are common Git commands used in various situations:

start a working area (see also: git help tutorial)
   clone      Clone a repository into a new directory
   init       Create an empty Git repository or reinitialize an existing one

work on the current change (see also: git help everyday)
   add        Add file contents to the index
   mv         Move or rename a file, a directory, or a symlink
   reset      Reset current HEAD to the specified state
   rm         Remove files from the working tree and from the index

examine the history and state (see also: git help revisions)
   bisect     Use binary search to find the commit that introduced a bug
   grep       Print lines matching a pattern
   log        Show commit logs
   show       Show various types of objects
   status     Show the working tree status

grow, mark and tweak your common history
   branch     List, create, or delete branches
   checkout   Switch branches or restore working tree files
   commit     Record changes to the repository
   diff       Show changes between commits, commit and working tree, etc
   merge      Join two or more development histories together
   rebase     Reapply commits on top of another base tip
   tag        Create, list, delete or verify a tag object signed with GPG

collaborate (see also: git help workflows)
   fetch      Download objects and refs from another repository
   pull       Fetch from and integrate with another repository or a local branch
   push       Update remote refs along with associated objects

'git help -a' and 'git help -g' list available subcommands and some
concept guides. See 'git help <command>' or 'git help <concept>'
to read about a specific subcommand or concept.

C:\Users\Nichole>git clone https://github.com/yenchenlin1994/DeepLearningFlappyBird.git
Cloning into 'DeepLearningFlappyBird'...
remote: Enumerating objects: 246, done.
remote: Total 246 (delta 0), reused 0 (delta 0), pack-reused 246
Receiving objects: 100% (246/246), 66.29 MiB | 1.01 MiB/s, done.
Resolving deltas: 100% (121/121), done.
Checking out files: 100% (49/49), done.

C:\Users\Nichole>cd DeepLearningFlappyBird

C:\Users\Nichole\DeepLearningFlappyBird>python deep_q_network.py
Traceback (most recent call last):
  File ""C:\Users\Nichole\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])
  File ""C:\Users\Nichole\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Nichole\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Nichole\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Nichole\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow_internal
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""deep_q_network.py"", line 4, in <module>
    import tensorflow as tf
  File ""C:\Users\Nichole\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Nichole\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Nichole\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Nichole\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])
  File ""C:\Users\Nichole\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Nichole\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Nichole\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Nichole\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow_internal
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

C:\Users\Nichole\DeepLearningFlappyBird>Failed to load the native TensorFlow runtime.
"
24243,failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error,"**System information**
- Ubuntu 16.04 LTS
- TensorFlow version: 1.12
- Python version: 3.6
- CUDA: 9.0
- CUDNN: 7.3
- GPU model and memory: NVIDIA GeForce GTX 1050 Ti

**Code**
`import tensorflow as tf`
`sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))`

**Output**
`2018-12-08 22:20:54.515356: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error`
`Device mapping:
/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device
/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device`

I have tried `nvidia-smi` and it works as expected:

Sat Dec  8 22:24:22 2018
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.130                Driver Version: 384.130                   |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 105...  Off  | 00000000:01:00.0 Off |                  N/A |
| N/A   51C    P0    N/A /  N/A |    500MiB /  4038MiB |     14%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1357      G   /usr/lib/xorg/Xorg                           300MiB |
|    0      2937      G   compiz                                        58MiB |
|    0      9866      G   ...quest-channel-token=7879820401391808802    92MiB |
|    0     26531      G   ...-token=5979D08CF023D5A85228AD97A282F087    47MiB |
+-----------------------------------------------------------------------------+

Also, `nvidia-modprobe` is installed.
Even rebooting has not solved the issue."
24241,window10+cuda10.0+anaconda3+tensorflow-r1.12,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):window 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source
- TensorFlow version:1.12
- Python version:3.7
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):0.20
- GCC/Compiler version (if compiling from source):Visual Studio 2015
- CUDA/cuDNN version:10.0
- GPU model and memory:6.0

**Describe the problem**
  'build' options: --action_env PYTHON_BIN_PATH=C:/Soft/Anaconda3/python.exe --action_env PYTHON_LIB_PATH=C:/Soft/Anaconda3/lib/site-packages --python_path=C:/Soft/Anaconda3/python.exe --define with_ignite_support=true --action_env TF_NEED_OPENCL_SYCL=0 --action_env TF_NEED_ROCM=0 --action_env TF_NEED_CUDA=1 --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0 --action_env TF_CUDA_VERSION=10.0 --action_env CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0 --action_env TF_CUDNN_VERSION=7 --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.0 --action_env TF_CUDA_CLANG=0 --config=cuda --config monolithic --copt=-w --host_copt=-w --verbose_failures --distinct_host_configuration=false --experimental_shortened_obj_file_path=true --define=no_tensorflow_py_deps=true --define=override_eigen_strong_inline=true
ERROR: Config value cuda is not defined in any .rc file
INFO: Invocation ID: 958bce6e-94dc-4d56-932a-6fcdc505865d
**Provide the exact sequence of commands / steps that you executed before running into the problem**
INFO: Reading rc options for 'build' from d:\code\tensorflow-r1.12\.tf_configure.bazelrc:
build --action_env PYTHON_BIN_PATH=""C:/Soft/Anaconda3/python.exe""
build --action_env PYTHON_LIB_PATH=""C:/Soft/Anaconda3/lib/site-packages""
build --python_path=""C:/Soft/Anaconda3/python.exe""
build --define with_ignite_support=true
build:xla --define with_xla_support=true
build --action_env TF_NEED_OPENCL_SYCL=""0""
build --action_env TF_NEED_ROCM=""0""
build --action_env TF_NEED_CUDA=""1""
build --action_env CUDA_TOOLKIT_PATH=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0""
build --action_env TF_CUDA_VERSION=""10.0""
build --action_env CUDNN_INSTALL_PATH=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0""
build --action_env TF_CUDNN_VERSION=""7""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""6.0""
build --action_env TF_CUDA_CLANG=""0""
build --config=cuda
test --config=cuda
build:opt --copt=/arch:AVX
build:opt --define with_default_optimizations=true
build --config monolithic
build --copt=-w --host_copt=-w
build --verbose_failures
build --distinct_host_configuration=false
build --experimental_shortened_obj_file_path=true
build --define=no_tensorflow_py_deps=true
build --define=override_eigen_strong_inline=true
build:v2 --define=tf_api_version=2
"
24240,build from source Windows toolchain for CPU error,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.12
- Python version: 3.6
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source):0.20
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0/7.4.1.5
- GPU model and memory: AMD 2700

Building tensorflow from source I followed this description
https://medium.com/@amsokol.com/how-to-build-and-install-tensorflow-gpu-cpu-for-windows-from-source-code-using-bazel-d047d9342b44

I launch bazel and get the following error 
(tensorflow) C:\Users\Stefan\tensorflow-build\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
c:\users\stefan\tensorflow-build\tensorflow/.bazelrc
c:\users\stefan\tensorflow-build\tensorflow/tools/bazel.rc
WARNING: Option 'experimental_shortened_obj_file_path' is deprecated

INFO: Invocation ID: 1f1e1f31-ec58-4119-9c1d-385bb8e68863
ERROR: cc_toolchain_suite '@local_config_cuda//crosstool:toolchain' does not contain a toolchain for CPU 'x64_windows', you may want to add an entry for 'x64_windows|msvc-cl' into toolchains and toolchain_identifier 'local_windows' into the corresponding cc_toolchain rule (see --incompatible_disable_cc_toolchain_label_from_crosstool_proto).
INFO: Elapsed time: 3.855s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)

How can I fix this?

Thanks for helping"
24239,AttributeError: module 'tensorflow' has no attribute 'lite',"I have tried to convert following keras model into tflite, but got following error. How I can solve this?

**System information**
- OS Platform: Windows 10
- TensorFlow installed from using anaconda environment
- TensorFlow version: 1.12
- Python version: 3.6.7
- Keras version: 2.2.4


**Output from tflite_convert**

```
converter = tf.lite.TFLiteConverter.from_keras_model_file(keras_file)
AttributeError: module 'tensorflow' has no attribute 'lite'
```

**Source Code**

```
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras import backend as K
import tensorflow as tf

# dimensions of our images.
img_width, img_height = 150, 150

train_data_dir = 'D:\\My Projects\\Dataset\\dataset6_2clz\\train'
validation_data_dir = 'D:\\My Projects\\Dataset\\dataset6_2clz\\validation'



nb_train_samples = 75
nb_validation_samples = 50
#epochs = 50
#batch_size = 16
epochs = 5
batch_size = 4

if K.image_data_format() == 'channels_first':
    input_shape = (3, img_width, img_height)
else:
    input_shape = (img_width, img_height, 3)

model = Sequential()
model.add(Conv2D(32, (3, 3), input_shape=input_shape))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(32, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(64))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(1))
model.add(Activation('sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])



train_datagen = ImageDataGenerator(
    rescale=1. / 255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True)


test_datagen = ImageDataGenerator(rescale=1. / 255)

train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='binary')

validation_generator = test_datagen.flow_from_directory(
    validation_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='binary')

model.fit_generator(
    train_generator,
    steps_per_epoch=nb_train_samples // batch_size,
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=nb_validation_samples // batch_size)


# Save tf.keras model in HDF5 format.
keras_file = ""7_try.h5""
model.save('7_try.h5')


# Convert to TensorFlow Lite model.

converter = tf.lite.TFLiteConverter.from_keras_model_file(keras_file)
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)
```"
24236,"XLA support for Python  instance, class, and static methods","

**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

The current XLA compile API does not support  instance methods.  Existing  code that uses Python objects along with TF api codes needs lot of refactoring to enable XLA. 

**Will this change the current api? How?**
No change in the  current API (e.g. xla.compile ). 

**Who will benefit with this feature?**
Any  Python developer using XLA

**Any Other info.**
"
24234,tf.contrib.eager.Saver.__init__ var_list argument should be optional,"**System information**
- TensorFlow version (you are using): 1.12.0
- Are you willing to contribute it (Yes/No): Yes!



**Describe the feature and the current behavior/state.**

[tf.contrib.eager.Saver](https://www.tensorflow.org/api_docs/python/tf/contrib/eager/Saver) (aka `tfe.Saver`) claims to be ""A tf.train.Saver adapter for use when eager execution is enabled."" However, the current api for `__init__()` requires that a `var_list` argument be passed in. 

Looking at [the code](https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/contrib/eager/python/saver.py#L138), the `var_list` argument is simply delegated to `tf.train.Saver`. The [tf.train.Saver](https://www.tensorflow.org/api_docs/python/tf/train/Saver#__init__) does _not_ require `var_list` to be pass in, having it default to `None`. I propose that `tfe.Saver` do the same.

**Will this change the current api? How?**

Yes, but it is a backwards-compatible change. Users of `tfe.Saver` will no longer have to specify a `var_list` argument. Any arguments already present will not change as they will override the default value.

**Who will benefit with this feature?**

My use-case for this feature is that I've written a custom estimator that's run on Cloud ML Engine, and I would like to write a local script that can restore the estimator and be evaluated against held out set of data different from the evaluation set the estimator was trained/tuned with. I'd like to take advantage of eager execution to do this. Not having to invoke `tfe.Saver` with a spurious `None` argument would make the code align with the original `tf.train.Saver` APIs.

**Any Other info.**

Thanks for the consideration! I know the team gets inundated with a ton of issues and as I mentioned above, I'm more than happy to make the change.

It also looks like there's a lot of other kwargs missing from `tfe.Saver`...perhaps it should just accept `*args` and `**kwargs` and forward them to `tf.train.Saver`?
"
24233,Build Tensorflow 1.12-GPU with CUDA10 Failed On Windows,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):MS Windows10 X64 1809 17763
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A
- TensorFlow installed from (source or binary):source
- TensorFlow version:1.12
- Python version:3.6
- Installed using virtualenv? pip? conda?:pip
- Bazel version (if compiling from source):0.15.0
- GCC/Compiler version (if compiling from source):MSVC2015 Update3
- CUDA/cuDNN version:10.0/7.4.1
- GPU model and memory:NVIDIA Geforce RTX2080TI 11GB *2



**Describe the problem**
I am trying to compile tensorflow from source but failed.Follow the guide on the official website(https://tensorflow.google.cn/install/source_windows) to run bazel build and report an error:
.\tensorflow/compiler/tf2xla/cpu_function_runtime.h(71): error C2338:
......
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 97.739s, Critical Path: 37.10s
INFO: 234 processes: 234 local.
FAILED: Build did NOT complete successfully

**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
N/A"
24232,tensorflow.keras: evaluate with custom loss and metric gives wrong output,"This issue is about tensorflow.keras

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip virtual env
- TensorFlow version (use command below):('v1.11.0-0-gc19e29306c', '1.11.0')
- Python version: 2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: Geforce 940mx

**Describe the current behavior**
I have a custom metric
`def root_mean_squared_error(y_true, y_pred):
    return K.sqrt(K.mean(K.square(y_pred - y_true)))`
while training I get the following output:

> Train on 161532 samples, validate on 40385 samples
> Epoch 1/5
>  - 132s - loss: 3.6781 - root_mean_squared_error: 3.6781 - val_loss: 3.6463 - val_root_mean_squared_error: 3.6463
> Epoch 2/5
>  - 129s - loss: 3.6528 - root_mean_squared_error: 3.6528 - val_loss: 3.6287 - val_root_mean_squared_error: 3.6287
> Epoch 3/5
>  - 143s - loss: 3.6352 - root_mean_squared_error: 3.6352 - val_loss: 3.6210 - val_root_mean_squared_error: 3.6210
> Epoch 4/5
>  - 134s - loss: 3.6223 - root_mean_squared_error: 3.6223 - val_loss: 3.6369 - val_root_mean_squared_error: 3.6369
> Epoch 5/5

after running `model.evaluate(val_data, val_label, verbose = 2)`
I get the following output:
> [3.028181584421266, 3.028181584421266]
for `model.metrics_names` 
> ['loss', 'root_mean_squared_error']

after running
`from sklearn.metrics import mean_squared_error
np.sqrt(mean_squared_error(val_label,np.squeeze(model.predict(val_data, batch_size=256))))`
I get
> 3.7112323807806966`

**Describe the expected behavior**
all outputs should be the same

My model looks like this:

```
def get_model(input_size):
    input1 = tf.keras.layers.Input(shape=(input_size,))
    x1 = Dense(input_size, activation='elu')(input1)  # input_size heisst, so viele neurons wie inputs

    x2 = Dense(300, activation='relu')(x1)
    concat1 = tf.keras.layers.Concatenate(axis=-1)([x1, x2])
    x3 = Dense(300, activation='relu')(concat1)
    concat2 = tf.keras.layers.Concatenate(axis=-1)([concat1, x3])
    x4 = Dense(300, activation='relu')(concat2)
    concat3 = tf.keras.layers.Concatenate(axis=-1)([concat2, x4])
    x5 = Dense(300, activation='relu')(concat3)
    concat4 = tf.keras.layers.Concatenate(axis=-1)([concat3, x5])
    x6 = Dense(300, activation='relu')(concat4)

    # output Layer
    output = Dense(1, activation='linear')(x6)
    model = tf.keras.models.Model(inputs=input1, outputs=output)
    my_opt = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
    my_loss = root_mean_squared_error
    model.compile(optimizer=my_opt, loss=my_loss,
                  metrics=[root_mean_squared_error])
    return model
```
===========================

If I run the exact same thing with
`def my_mean_squared_error(y_true, y_pred):
    return K.mean(K.square(y_pred - y_true))`
and
`model.compile(optimizer=my_opt, loss=my_loss,
                  metrics=[my_mean_squared_error,""mse""])`
and
`mean_squared_error(val_label,np.squeeze(model.predict(val_data, batch_size=256)))`
it works fine.

so it seems, that there is something wrong with K.sqrt()"
24231,Adding Backwards Edge in Tensorflow Java,"**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
As we started working with Tensorflow Java, we realized that it isn't yet possible to implement while loops in the graph because we cannot mutate the nodes after they have been created. Therefore, we cannot make a backwards edge to create the loop from the NextIteration node to the Merge node, as seen in [the design of while loops](https://github.com/tensorflow/tensorflow/blob/2a050766bf0556d7d92eea62d40fd2bebbcb399f/tensorflow/cc/ops/while_loop.cc#L148).

**Will this change the current api? How?**
Yes. This will allow the graph to be mutated after a node has been created.

**Who will benefit with this feature?**
Those who want to use Tensorflow Java to make a while loop or other sorts of loops.

**Any Other info.**
"
24230,Problem importing Tensorflow,"
**System information**
- OS Platform Windows 10 
- Dell Inspiron 5500
- TensorFlow installed from (source or binary): pip install
- TensorFlow version:  N/A
- Python version: Python 3.6.7 :: Anaconda custom (64-bit)
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version: CUDA 9.0
- GPU model and memory: N/A





**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
 Go into Python shell and import tensorflow and it failed...

**Any other info / logs**
(tensorflow) C:\Users\user>python
Python 3.6.7 |Anaconda custom (64-bit)| (default, Oct 28 2018, 19:44:12) [MSC v.1915 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\user\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\user\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\__init__.py"", line 88, in <module>
    from tensorflow.python import keras
  File ""C:\Users\user\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\__init__.py"", line 24, in <module>
    from tensorflow.python.keras import activations
  File ""C:\Users\user\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\activations\__init__.py"", line 22, in <module>
    from tensorflow.python.keras._impl.keras.activations import elu
  File ""C:\Users\user\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\_impl\keras\__init__.py"", line 21, in <module>
    from tensorflow.python.keras._impl.keras import activations
  File ""C:\Users\user\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\_impl\keras\activations.py"", line 23, in <module>
    from tensorflow.python.keras._impl.keras import backend as K
  File ""C:\Users\user\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\_impl\keras\backend.py"", line 37, in <module>
    from tensorflow.python.layers import base as tf_base_layers
  File ""C:\Users\user\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\layers\base.py"", line 25, in <module>
    from tensorflow.python.keras.engine import base_layer
  File ""C:\Users\user\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\engine\__init__.py"", line 23, in <module>
    from tensorflow.python.keras.engine.base_layer import InputSpec
  File ""C:\Users\user\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 35, in <module>
    from tensorflow.python.keras import backend
  File ""C:\Users\user\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\backend\__init__.py"", line 22, in <module>
    from tensorflow.python.keras._impl.keras.backend import abs
ImportError: cannot import name 'abs'
>>>"
24227,Device mapping: no known device tensorflow-gpu Windows 10,
24226,DistributionStrategy and Keras models: support for sample_weight_mode,"**System information**
- TensorFlow version (you are using): master (Nov. 30, 2018)
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
When using a DistributionStrategy with a Keras model that includes sample_weights, a 'NotImplemented' exception is thrown.  For example,

 ```
    strategy = mirrored_strategy.MirroredStrategy()
     model.compile(loss='categorical_crossentropy',
                     optimizer=opt,
                     metrics=['accuracy'],
                     weighted_metrics=['accuracy'],
                     sample_weight_mode='temporal',
                     distribute=strategy)
```

The exception originates in tensorflow/python/keras/engine/training.py
      
```
 if sample_weight_mode:
	        raise NotImplementedError('sample_weight_mode is not supported with '
	                                  'DistributionStrategy.')
```

Can this be supported or can you give an estimated timeline?

Many thanks


**Will this change the current api? How?**
No.  It looks like the API should support this.

**Who will benefit with this feature?**
Anyone using keras models with sample_weights in a DistributedStrategy mode.

**Any Other info.**

"
24225,Why there are two nnapi_delegate.cc with the same name?,"In r1.12

There is one under the folder https://github.com/tensorflow/tensorflow/tree/r1.12/tensorflow/contrib/lite/delegates/nnapi

The other one is 

https://github.com/tensorflow/tensorflow/tree/r1.12/tensorflow/contrib/lite

Can anybody explain what the two files are doing? Can they be named differently? Looks very confusing. 
"
24223,TensorflowLite conversion failed using embedding layer.,"**System information**
- Windows 10 version 1803 x64
- TensorFlow installed from pip
- tf-nightly 1.13.0.dev20181129


**Provide the text output from tflite_convert**

```
2018-12-07 12:30:04.392840: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: ResourceGather
2018-12-07 12:30:04.393575: I tensorflow/lite/toco/import_tensorflow.cc:1329] Unable to determine output type for op: ResourceGather
2018-12-07 12:30:04.518747: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 18 operators, 28 arrays (0 quantized)
2018-12-07 12:30:04.519109: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 18 operators, 28 arrays (0 quantized)
2018-12-07 12:30:04.519819: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 7 operators, 14 arrays (0 quantized)
2018-12-07 12:30:04.707328: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 6 operators, 13 arrays (0 quantized)
2018-12-07 12:30:04.707811: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 5 operators, 11 arrays (0 quantized)
2018-12-07 12:30:04.708213: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 5 operators, 11 arrays (0 quantized)
2018-12-07 12:30:04.708689: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 1216 bytes, theoretical optimal value: 1216 bytes.
2018-12-07 12:30:04.709343: E tensorflow/lite/toco/toco_tooling.cc:420] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CAST, FULLY_CONNECTED, LOGISTIC. Here is a list of operators for which you will need custom implementations: ResourceGather.
Traceback (most recent call last):
  File ""c:\users\dorme\anaconda3\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\users\dorme\anaconda3\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\Dorme\Anaconda3\Scripts\toco_from_protos.exe\__main__.py"", line 9, in <module>
  File ""c:\users\dorme\anaconda3\lib\site-packages\tensorflow\lite\toco\python\toco_from_protos.py"", line 59, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""c:\users\dorme\anaconda3\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""c:\users\dorme\anaconda3\lib\site-packages\tensorflow\lite\toco\python\toco_from_protos.py"", line 33, in execute
    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CAST, FULLY_CONNECTED, LOGISTIC. Here is a list of operators for which you will need custom implementations: ResourceGather.
```

Using an Embedding, Flatten, Dense and Dense layers for text seems impossible i'm also using the library tokenizer from keras for preprocessing the data.
"
24221,`tf.estimator.Estimator` methods `.evaluate`/`.train` calls deplete CPU RAM,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): True
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Null
- TensorFlow installed from (source or binary): `pip install tensorflow-gpu`
- TensorFlow version (use command below): v1.10.0-0-g656e7a2b34 1.10.0
- Python version: 3.6.7
- Bazel version (if compiling from source): Null
- GCC/Compiler version (if compiling from source): Null
- CUDA/cuDNN version: cudNN 7.4.1
- GPU model and memory: NVIDIA GTX 1070 8GB

**Describe the current behavior**
While using a `tf.estimator.Estimator` class, each separate call of `.train`/`.evaluate` results on extra CPU RAM permanent usage. In my use case, every time I call `.evaluate` I get another 200MB RAM allocated on my CPU (even though I'm training on the GPU) RAM, which then never gets free. This ultimately results in `MemoryError` since after a few training/validation rounds the RAM gets full.

**Describe the expected behavior**
Memory used by these methods should be released when done.

**Code to reproduce the issue**
```
EXAMPLE_TYPE = Tuple[str, List[int], List[int], List[int], int]
DATASET_TYPE = List[EXAMPLE_TYPE]

class ExampleGenerator:
    def __init__(self, train: DATASET_TYPE, validation: DATASET_TYPE):
        self._data = {
            'train': train,
            'validation': validation,
        }

    def generate(self, source: str, shuffle: bool = False) -> Generator[
        EXAMPLE_TYPE, None, None]:
        """"""
        Yields examples

        :param source: Source to generate from (train/validation)
        :return: Example
        """"""

        if shuffle:
            random.shuffle(self._data[source])

        for example in self._data[source]:
            yield example

def input_fn(example_generator: ExampleGenerator, source: str, batch_size: int):
    dataset = tf.data.Dataset.from_generator(generator=lambda: example_generator.generate(source, shuffle=True),
                                             output_types=(tf.string, tf.int32, tf.int32, tf.int32, tf.int32),
                                             output_shapes=(
                                                 tf.TensorShape([]), tf.TensorShape([None]), tf.TensorShape([None]),
                                                 tf.TensorShape([None]), tf.TensorShape([])))

    # Shuffle, repeat, and batch the examples.
    dataset = dataset.padded_batch(batch_size,
                                   padded_shapes=([], [None], [None], [None], []))

    dataset = dataset.prefetch(buffer_size=batch_size)

    iterator = dataset.make_one_shot_iterator()

    eid, text, pos, dep, label = iterator.get_next()

    return {'eid': eid, 'text': text, 'text_pos': pos, 'text_dep': dep}, label

def model(...):
    ...

for _ in range(params['epochs']):
    classifier.train(input_fn=lambda: input_fn(example_generator, source='train', batch_size=params['batch_size']))
    classifier.evaluate(input_fn=lambda: input_fn(example_generator, source='validation', batch_size=params['batch_size']))
```

To figure out why I get `MemoryError`s, I have used the Python interactive mode, and called `.evaluate(...)` and `.train(...)` while observing RAM usage, after each call, another 200MB RAM got occupied on the CPU RAM, eventually depleting all available RAM."
24220,No Device Mapped with Tensorflow-gpu - Ubuntu 18.04,"## System information
- OS Platform and Distribution: Ubuntu 18.04.1 LTS x86_64
- TensorFlow installed from (source or binary): Miniconda (used cmd: 'conda install -c anaconda tensorflow-gpu')
- TensorFlow version: Tensorflow-gpu (Tried both 1.12.0 and 1.9.0, and got the same result)
- Python version: 3.6.7
- Installed using virtualenv? pip? conda?: Conda
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): GCC 7.3.0
- CUDA/cuDNN version: 9.0
- GPU model and memory: As below.

## CPU and GPU info
CPU: Intel i7-8750H (12) @ 4.100GHz
GPU: NVIDIA GeForce GTX 1070 Mobile
GPU: Intel Device 3e9b


## Describe the problem

It looks like in the 'Device Mapping' section as below, no GPU is linked, though no error msg noted in code. Strangely though, the GPU name/mode has been detected. Would you please help?

Details as below.

Python 3.6.7 |Anaconda, Inc.| (default, Oct 23 2018, 19:16:44) 
[GCC 7.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> import os
>>> os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
>>> tf.Session(config=tf.ConfigProto(log_device_placement=True))
2018-12-08 00:44:56.363230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2018-12-08 00:44:56.363327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-12-08 00:44:56.363349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2018-12-08 00:44:56.363370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2018-12-08 00:44:56.363659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6738 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1)
Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1
2018-12-08 00:44:56.371266: I tensorflow/core/common_runtime/direct_session.cc:288] Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1

<tensorflow.python.client.session.Session object at 0x7f693a136ef0>




## Provide the exact sequence of commands / steps that you executed before running into the problem
 Pretty much followed this post:
https://mc.ai/install-conda-tensorflow-gpu-and-keras-on-ubuntu-18-04/



## Any other info / logs

## GPU Driver Info 

Sat Dec  8 00:56:11 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.57                 Driver Version: 410.57                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 107...  Off  | 00000000:01:00.0 Off |                  N/A |
| N/A   46C    P3    24W /  N/A |    936MiB /  8119MiB |      2%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1142      G   /usr/lib/xorg/Xorg                           535MiB |
|    0      1313      G   /usr/bin/gnome-shell                         217MiB |
|    0      1941      G   ...quest-channel-token=4040395960093113594   181MiB |
+-----------------------------------------------------------------------------+


## cuDNN ver

cudnn-9.0-linux-x64-v7.1.tgz 


## Full info for CUDA

$ ./deviceQuery
./deviceQuery Starting...

 CUDA Device Query (Runtime API) version (CUDART static linking)

Detected 1 CUDA Capable device(s)

Device 0: ""GeForce GTX 1070 with Max-Q Design""
  CUDA Driver Version / Runtime Version          10.0 / 9.0
  CUDA Capability Major/Minor version number:    6.1
  Total amount of global memory:                 8120 MBytes (8513978368 bytes)
  (16) Multiprocessors, (128) CUDA Cores/MP:     2048 CUDA Cores
  GPU Max Clock rate:                            1266 MHz (1.27 GHz)
  Memory Clock rate:                             4004 Mhz
  Memory Bus Width:                              256-bit
  L2 Cache Size:                                 2097152 bytes
  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)
  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  2048
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
  Maximum memory pitch:                          2147483647 bytes
  Texture alignment:                             512 bytes
  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)
  Run time limit on kernels:                     Yes
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Disabled
  Device supports Unified Addressing (UVA):      Yes
  Supports Cooperative Kernel Launch:            Yes
  Supports MultiDevice Co-op Kernel Launch:      Yes
  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0
  Compute Mode:
     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >

deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.0, CUDA Runtime Version = 9.0, NumDevs = 1
Result = PASS


## Check if cards are recognised on the bus:
$ lspci | grep -i nvidia
01:00.0 VGA compatible controller: NVIDIA Corporation GP104M [GeForce GTX 1070 Mobile] (rev a1)
01:00.1 Audio device: NVIDIA Corporation GP104 High Definition Audio Controller (rev a1)


## Check Drivers:
$ dmesg | grep NVRM
[    3.286908] NVRM: loading NVIDIA UNIX x86_64 Kernel Module  410.57  Tue Sep 18 23:25:09 CDT 2018 (using threaded interrupts)


Thanks very much in advance!

"
24219,FP16 support on GPU for scatter_max,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):  v1.12.0
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**
scatter_max supports fp32, fp64 on GPU and fp16, fp32, fp64 on CPU.
It will be nice if scatte_max can support fp16 on GPU.

**Will this change the current api? How?**
no

**Who will benefit with this feature?**
Everyone who wants to use scatter_max with fp16 in gpu.

**Any Other info.**
```python
import tensorflow as tf

with tf.device(""/device:GPU:0""):
  # dtype=tf.float16 won't work in this code sample
  u = tf.constant([[1,6,8], [2,4,9], [3,5,7]]*3, dtype=tf.float32)
  r = tf.get_variable(""r"", [4, 3], dtype=tf.float32)
  y = tf.scatter_max(r, [0,0,0,1,1,2,2,3,3], u)

  with tf.Session() as sess:
    tf.initialize_all_variables().run()
    print(sess.run(y))
```
"
24218,Compilation error when building tfcompile on Windows 10,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): r1.12
- Python version: 3.5
- Bazel version (if compiling from source): 0.15
- GCC/Compiler version (if compiling from source): VS 2015 14.0.25431.01 Update 3
- CUDA/cuDNN version: 9.0/7.3.1
- GPU model and memory: GTX 1080, 8GBs

**Describe the current behavior**
Checked out the release 1.12 branch of the TF repo, configured bazel (configuration is below) and ran 
`bazel build -c opt --config=opt //tensorflow/compiler/aot:tfcompile`

Compilation fails at `.\tensorflow/compiler/tf2xla/cpu_function_runtime.h(71): error C2338`, it seems a static assert is failing: 
```
std::pair<uint64, uint64> Encode() const {
    static_assert(sizeof(*this) == 16, """"); //<<<<<<<<<<<<<<<<< 
    uint64 upper = Pack(kind(), size_);
    uint64 lower = entry_param_number_;
    return {upper, lower};
  }
```

**Describe the expected behavior**
The code compiles.

**Other info / logs**

Bazel config:

```
build --action_env PYTHON_BIN_PATH=""C:/Users/1/Anaconda3/envs/tensorflow/python.exe""
build --action_env PYTHON_LIB_PATH=""C:/Users/1/Anaconda3/envs/tensorflow/lib/site-packages""
build --python_path=""C:/Users/1/Anaconda3/envs/tensorflow/python.exe""
build:ignite --define with_ignite_support=true
build:xla --define with_xla_support=true
build --action_env TF_NEED_OPENCL_SYCL=""0""
build --action_env TF_NEED_ROCM=""0""
build --action_env TF_NEED_CUDA=""1""
build --action_env CUDA_TOOLKIT_PATH=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0""
build --action_env TF_CUDA_VERSION=""9.0""
build --action_env CUDNN_INSTALL_PATH=""C:/Program Files/NVIDIA GPU Computing Toolkit/cuDNN/7.3.1""
build --action_env TF_CUDNN_VERSION=""7""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""5.2,6.1""
build --action_env TF_CUDA_CLANG=""0""
build --config=cuda
test --config=cuda
build:opt --copt=/arch:AVX2
build:opt --define with_default_optimizations=true
build --config monolithic
build --copt=-w --host_copt=-w
build --verbose_failures
build --distinct_host_configuration=false
build --experimental_shortened_obj_file_path=true
build --define=no_tensorflow_py_deps=true
build --define=override_eigen_strong_inline=true
build:v2 --define=tf_api_version=2
```"
24217," After compiling tensorflow for golang, it was found that python tensorflow could not be imported normally.","when i import tensorflow ,it happend errors.
python 3.6.5, macOS
----> 1 import tensorflow as tf

/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/__init__.py in <module>
     22
     23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     25
     26 try:

/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/__init__.py in <module>
     47 import numpy as np
     48
---> 49 from tensorflow.python import pywrap_tensorflow
     50
     51 from tensorflow.python.tools import component_api_helper

/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: __ZN10tensorflow13DeviceFactory10AddDevicesERKNS_14SessionOptionsERKNSt3__112basic_stringIcNS4_11char_traitsIcEENS4_9allocatorIcEEEEPNS4_6vectorIPNS_6DeviceENS8_ISF_EEEE
  Referenced from: /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
  Expected in: /Users/wqq/go/src/github.com/tensorflow/tensorflow/bazel-bin/tensorflow/libtensorflow_framework.so
 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so"
24216,Can't use tf.gradients to get the gradients of a tf.gather_np op,"I wanna get the gradients of a tf.gather(params, indices) as for indices, but when I do gradients = tf.gradients(ys=out_1, xs=indices). Gradients is None.

```
params = tf.constant(np.array([[1,2], [3,4]]))
indices = tf.constant(np.array([[[0,0],[0,1]],[[1,1],[0,1]]]))
w = tf.constant(np.random.randn(4,3), dtype=tf.float32)
with tf.Session() as sess:
    out_1 = tf.gather_nd(params=params, indices=indices)
    print(sess.run(out_1))
    gradients = tf.gradients(ys=out_1, xs=indices)
    print(gradients)
    print(sess.run(gradients))
```

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-39-c4fb622d33f8> in <module>()
     10     gradients = tf.gradients(ys=out_1, xs=indices)
     11     print(gradients)
---> 12     print(sess.run(gradients))

C:\Users\11198\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\client\session.py in run(self, fetches, feed_dict, options, run_metadata)
    927     try:
    928       result = self._run(None, fetches, feed_dict, options_ptr,
--> 929                          run_metadata_ptr)
    930       if run_metadata:
    931         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

C:\Users\11198\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\client\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1135     # Create a fetch handler to take care of the structure of fetches.
   1136     fetch_handler = _FetchHandler(
-> 1137         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)
   1138 
   1139     # Run request and get response.

C:\Users\11198\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\client\session.py in __init__(self, graph, fetches, feeds, feed_handles)
    469     """"""
    470     with graph.as_default():
--> 471       self._fetch_mapper = _FetchMapper.for_fetch(fetches)
    472     self._fetches = []
    473     self._targets = []

C:\Users\11198\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\client\session.py in for_fetch(fetch)
    259     elif isinstance(fetch, (list, tuple)):
    260       # NOTE(touts): This is also the code path for namedtuples.
--> 261       return _ListFetchMapper(fetch)
    262     elif isinstance(fetch, collections.Mapping):
    263       return _DictFetchMapper(fetch)

C:\Users\11198\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\client\session.py in __init__(self, fetches)
    368     """"""
    369     self._fetch_type = type(fetches)
--> 370     self._mappers = [_FetchMapper.for_fetch(fetch) for fetch in fetches]
    371     self._unique_fetches, self._value_indices = _uniquify_fetches(self._mappers)
    372 

C:\Users\11198\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\client\session.py in <listcomp>(.0)
    368     """"""
    369     self._fetch_type = type(fetches)
--> 370     self._mappers = [_FetchMapper.for_fetch(fetch) for fetch in fetches]
    371     self._unique_fetches, self._value_indices = _uniquify_fetches(self._mappers)
    372 

C:\Users\11198\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\client\session.py in for_fetch(fetch)
    256     if fetch is None:
    257       raise TypeError('Fetch argument %r has invalid type %r' % (fetch,
--> 258                                                                  type(fetch)))
    259     elif isinstance(fetch, (list, tuple)):
    260       # NOTE(touts): This is also the code path for namedtuples.

TypeError: Fetch argument None has invalid type <class 'NoneType'>


"
24215,bazel error for saved_model_cli,"Hi,

i get the follwing error when i try to build saved_model_cli (following this tutorial: https://www.tensorflow.org/guide/saved_model). Any idea whats the issue?

```
$ bazel build tensorflow/python/tools:saved_model_cli
WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
/Users/geri/Documents/tensorflow/tools/bazel.rc
Starting local Bazel server and connecting to it...
INFO: Invocation ID: 466cba21-c0aa-4740-970a-fd0d67250a82
WARNING: /Users/geri/Documents/tensorflow/tensorflow/python/BUILD:2985:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/contrib/gan/BUILD:136:1: in py_library rule //tensorflow/contrib/gan:losses_impl: target '//tensorflow/contrib/gan:losses_impl' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/contrib/metrics/BUILD:16:1: in py_library rule //tensorflow/contrib/metrics:metrics_py: target '//tensorflow/contrib/metrics:metrics_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/python/BUILD:76:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:233:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:356:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:76:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
INFO: Analysed target //tensorflow/python/tools:saved_model_cli (309 packages loaded, 16772 targets configured).
INFO: Found 1 target...
INFO: From Linking external/protobuf_archive/libprotobuf_lite.a [for host]:
/Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/protobuf_archive/_objs/protobuf_lite/arenastring.o has no symbols
/Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/protobuf_archive/_objs/protobuf_lite/io_win32.o has no symbols
INFO: From Linking tensorflow/core/libplatform_base.a [for host]:
/Library/Developer/CommandLineTools/usr/bin/libtool: warning same member name (env_time.o) in output file used for input files: bazel-out/host/bin/tensorflow/core/_objs/platform_base/0/env_time.o and: bazel-out/host/bin/tensorflow/core/_objs/platform_base/1/env_time.o (due to use of basename, truncation, blank padding or duplicate input files)
INFO: From Linking external/protobuf_archive/libprotobuf.a [for host]:
/Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/protobuf_archive/_objs/protobuf/gzip_stream.o has no symbols
/Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/protobuf_archive/_objs/protobuf/error_listener.o has no symbols
INFO: From ProtoCompile tensorflow/core/lib/core/error_codes.pb.cc [for host]:
bazel-out/host/genfiles/external/protobuf_archive/src: warning: directory does not exist.
INFO: From ProtoCompile tensorflow/core/grappler/costs/op_performance_data.pb.cc:
bazel-out/darwin-fastbuild/genfiles/external/protobuf_archive/src: warning: directory does not exist.
bazel-out/darwin-fastbuild/genfiles/external/protobuf_archive/src: warning: directory does not exist.
bazel-out/darwin-fastbuild/genfiles/external/protobuf_archive/src: warning: directory does not exist.
bazel-out/darwin-fastbuild/genfiles/external/protobuf_archive/src: warning: directory does not exist.
INFO: From ProtoCompile tensorflow/core/lib/core/error_codes.pb.cc:
bazel-out/darwin-fastbuild/genfiles/external/protobuf_archive/src: warning: directory does not exist.
INFO: From ProtoCompile tensorflow/core/grappler/costs/op_performance_data.pb.cc [for host]:
bazel-out/host/genfiles/external/protobuf_archive/src: warning: directory does not exist.
bazel-out/host/genfiles/external/protobuf_archive/src: warning: directory does not exist.
bazel-out/host/genfiles/external/protobuf_archive/src: warning: directory does not exist.
bazel-out/host/genfiles/external/protobuf_archive/src: warning: directory does not exist.
INFO: From ProtoCompile tensorflow/core/example/example.pb.cc:
bazel-out/darwin-fastbuild/genfiles/external/protobuf_archive/src: warning: directory does not exist.
bazel-out/darwin-fastbuild/genfiles/external/protobuf_archive/src: warning: directory does not exist.
INFO: From ProtoCompile tensorflow/core/example/example.pb.cc [for host]:
bazel-out/host/genfiles/external/protobuf_archive/src: warning: directory does not exist.
bazel-out/host/genfiles/external/protobuf_archive/src: warning: directory does not exist.
ERROR: /private/var/tmp/_bazel_geri/af336600bf091f11bdb6522728b81044/external/flatbuffers/BUILD.bazel:57:1: C++ compilation of rule '@flatbuffers//:flatc_library' failed (Exit 1) cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 44 argument(s) skipped)

Use --sandbox_debug to see verbose messages from the sandbox
external/flatbuffers/src/util.cpp:50:3: error: use of undeclared identifier 'FLATBUFFERS_ASSERT'
  FLATBUFFERS_ASSERT(g_load_file_function);
  ^
external/flatbuffers/src/util.cpp:55:3: error: use of undeclared identifier 'FLATBUFFERS_ASSERT'
  FLATBUFFERS_ASSERT(g_file_exists_function);
  ^
2 errors generated.
Target //tensorflow/python/tools:saved_model_cli failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 302.643s, Critical Path: 46.91s
INFO: 396 processes: 396 darwin-sandbox.
FAILED: Build did NOT complete successfully
```

more infos:
```
$ bazel build tensorflow/python/tools:saved_model_cli --sandbox_debug
WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
/Users/geri/Documents/tensorflow/tools/bazel.rc
INFO: Invocation ID: b46f49c0-8fce-46a2-8bfc-4d3ad5ba2ef7
WARNING: /Users/geri/Documents/tensorflow/tensorflow/python/BUILD:2985:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/python/BUILD:76:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/contrib/metrics/BUILD:16:1: in py_library rule //tensorflow/contrib/metrics:metrics_py: target '//tensorflow/contrib/metrics:metrics_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/contrib/gan/BUILD:136:1: in py_library rule //tensorflow/contrib/gan:losses_impl: target '//tensorflow/contrib/gan:losses_impl' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:233:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:76:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:356:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
INFO: Analysed target //tensorflow/python/tools:saved_model_cli (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
INFO: From Linking external/grpc/libtsi.a:
/Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/darwin-fastbuild/bin/external/grpc/_objs/tsi/ssl_session_openssl.pic.o has no symbols
INFO: From Linking external/grpc/libgrpc++_base.a:
/Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/darwin-fastbuild/bin/external/grpc/_objs/grpc++_base/rpc_method.pic.o has no symbols
ERROR: /private/var/tmp/_bazel_geri/af336600bf091f11bdb6522728b81044/external/grpc/BUILD:1515:1: C++ compilation of rule '@grpc//:grpc_resolver_dns_ares' failed (Exit 1) sandbox-exec failed: error executing command 
  (cd /private/var/tmp/_bazel_geri/af336600bf091f11bdb6522728b81044/execroot/org_tensorflow && \
  exec env - \
    PATH=/Users/geri/.pyenv/shims:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/usr/local/go/bin:/Users/geri/intelligence/bin \
    PWD=/proc/self/cwd \
    TMPDIR=/var/folders/7t/9l0n105n4_jbrq076r887pthz96c2m/T/ \
  /usr/bin/sandbox-exec -f /private/var/tmp/_bazel_geri/af336600bf091f11bdb6522728b81044/sandbox/darwin-sandbox/61/sandbox.sb /var/tmp/_bazel_geri/install/c71a8bada5761bf76ced20ca6672b491/_embedded_binaries/process-wrapper '--timeout=0' '--kill_delay=15' external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer '-std=c++0x' -MD -MF bazel-out/darwin-fastbuild/bin/external/grpc/_objs/grpc_resolver_dns_ares/grpc_ares_wrapper.pic.d '-frandom-seed=bazel-out/darwin-fastbuild/bin/external/grpc/_objs/grpc_resolver_dns_ares/grpc_ares_wrapper.pic.o' -fPIC '-DPB_FIELD_32BIT=1' -iquote external/grpc -iquote bazel-out/darwin-fastbuild/genfiles/external/grpc -iquote bazel-out/darwin-fastbuild/bin/external/grpc -iquote external/bazel_tools -iquote bazel-out/darwin-fastbuild/genfiles/external/bazel_tools -iquote bazel-out/darwin-fastbuild/bin/external/bazel_tools -iquote external/zlib_archive -iquote bazel-out/darwin-fastbuild/genfiles/external/zlib_archive -iquote bazel-out/darwin-fastbuild/bin/external/zlib_archive -iquote external/com_github_nanopb_nanopb -iquote bazel-out/darwin-fastbuild/genfiles/external/com_github_nanopb_nanopb -iquote bazel-out/darwin-fastbuild/bin/external/com_github_nanopb_nanopb -isystem external/grpc/include -isystem bazel-out/darwin-fastbuild/genfiles/external/grpc/include -isystem bazel-out/darwin-fastbuild/bin/external/grpc/include -isystem external/zlib_archive -isystem bazel-out/darwin-fastbuild/genfiles/external/zlib_archive -isystem bazel-out/darwin-fastbuild/bin/external/zlib_archive -isystem external/grpc/third_party/address_sorting/include -isystem bazel-out/darwin-fastbuild/genfiles/external/grpc/third_party/address_sorting/include -isystem bazel-out/darwin-fastbuild/bin/external/grpc/third_party/address_sorting/include -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc -o bazel-out/darwin-fastbuild/bin/external/grpc/_objs/grpc_resolver_dns_ares/grpc_ares_wrapper.pic.o)
external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc:29:10: fatal error: 'ares.h' file not found
#include <ares.h>
         ^~~~~~~~
1 error generated.
Target //tensorflow/python/tools:saved_model_cli failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 13.689s, Critical Path: 4.82s
INFO: 57 processes: 57 darwin-sandbox.
FAILED: Build did NOT complete successfully
```

than i installed ares:
```
$ ruby -e ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"" < /dev/null 2> /dev/null
$ brew install c-ares
```

and tried it again, and got this error:
```
$ bazel build tensorflow/python/tools:saved_model_cli --sandbox_debug
WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
/Users/geri/Documents/tensorflow/tools/bazel.rc
INFO: Invocation ID: 2e76455c-b877-4561-85f0-7a34a9337adc
WARNING: /Users/geri/Documents/tensorflow/tensorflow/python/BUILD:2985:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/python/BUILD:76:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/contrib/metrics/BUILD:16:1: in py_library rule //tensorflow/contrib/metrics:metrics_py: target '//tensorflow/contrib/metrics:metrics_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/contrib/gan/BUILD:136:1: in py_library rule //tensorflow/contrib/gan:losses_impl: target '//tensorflow/contrib/gan:losses_impl' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:233:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:76:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:356:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /Users/geri/Documents/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
INFO: Analysed target //tensorflow/python/tools:saved_model_cli (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
INFO: From ProtoCompile external/protobuf_archive/python/google/protobuf/any_pb2.py [for host]:
external/protobuf_archive/python: warning: directory does not exist.
INFO: From ProtoCompile tensorflow/core/example/example_pb2.py [for host]:
external/protobuf_archive/python: warning: directory does not exist.
external/protobuf_archive/python: warning: directory does not exist.
INFO: From Compiling tensorflow/core/framework/function.cc [for host]:
tensorflow/core/framework/function.cc:1107:18: warning: reading variable 'function_defs_' requires holding mutex 'clone.mu_' [-Wthread-safety-precise]
  for (auto iter : clone.function_defs_) {
                 ^
tensorflow/core/framework/function.cc:1107:18: note: found near match 'mu_'
tensorflow/core/framework/function.cc:1107:18: warning: reading variable 'function_defs_' requires holding mutex 'clone.mu_' [-Wthread-safety-precise]
  for (auto iter : clone.function_defs_) {
                 ^
tensorflow/core/framework/function.cc:1107:18: note: found near match 'mu_'
tensorflow/core/framework/function.cc:1117:18: warning: reading variable 'func_grad_' requires holding mutex 'clone.mu_' [-Wthread-safety-precise]
  for (auto iter : clone.func_grad_) {
                 ^
tensorflow/core/framework/function.cc:1117:18: note: found near match 'mu_'
tensorflow/core/framework/function.cc:1117:18: warning: reading variable 'func_grad_' requires holding mutex 'clone.mu_' [-Wthread-safety-precise]
  for (auto iter : clone.func_grad_) {
                 ^
tensorflow/core/framework/function.cc:1117:18: note: found near match 'mu_'
4 warnings generated.
INFO: From Compiling tensorflow/core/util/example_proto_fast_parsing.cc [for host]:
In file included from tensorflow/core/util/example_proto_fast_parsing.cc:15:
In file included from ./tensorflow/core/util/example_proto_fast_parsing.h:32:
./tensorflow/core/util/sparse/sparse_tensor.h:77:15: warning: 'SparseTensor' is deprecated: Use Create() functions instead of constructors directly. [-Wdeprecated-declarations]
    *result = SparseTensor(ix, vals, shape, order);
              ^
./tensorflow/core/util/sparse/sparse_tensor.h:113:3: note: 'SparseTensor' has been explicitly marked deprecated here
  ABSL_DEPRECATED(""Use Create() functions instead of constructors directly."")
  ^
external/com_google_absl/absl/base/macros.h:149:49: note: expanded from macro 'ABSL_DEPRECATED'
#define ABSL_DEPRECATED(message) __attribute__((deprecated(message)))
                                                ^
In file included from tensorflow/core/util/example_proto_fast_parsing.cc:15:
In file included from ./tensorflow/core/util/example_proto_fast_parsing.h:32:
./tensorflow/core/util/sparse/sparse_tensor.h:77:15: warning: 'SparseTensor' is deprecated: Use Create() functions instead of constructors directly. [-Wdeprecated-declarations]
    *result = SparseTensor(ix, vals, shape, order);
              ^
./tensorflow/core/util/sparse/sparse_tensor.h:113:3: note: 'SparseTensor' has been explicitly marked deprecated here
  ABSL_DEPRECATED(""Use Create() functions instead of constructors directly."")
  ^
external/com_google_absl/absl/base/macros.h:149:49: note: expanded from macro 'ABSL_DEPRECATED'
#define ABSL_DEPRECATED(message) __attribute__((deprecated(message)))
                                                ^
In file included from tensorflow/core/util/example_proto_fast_parsing.cc:15:
In file included from ./tensorflow/core/util/example_proto_fast_parsing.h:32:
./tensorflow/core/util/sparse/sparse_tensor.h:132:9: warning: 'SparseTensor' is deprecated: Use Create() functions instead of constructors directly. [-Wdeprecated-declarations]
      : SparseTensor(other.ix_, other.vals_, other.shape_, other.order_) {}
        ^
./tensorflow/core/util/sparse/sparse_tensor.h:113:3: note: 'SparseTensor' has been explicitly marked deprecated here
  ABSL_DEPRECATED(""Use Create() functions instead of constructors directly."")
  ^
external/com_google_absl/absl/base/macros.h:149:49: note: expanded from macro 'ABSL_DEPRECATED'
#define ABSL_DEPRECATED(message) __attribute__((deprecated(message)))
                                                ^
In file included from tensorflow/core/util/example_proto_fast_parsing.cc:15:
In file included from ./tensorflow/core/util/example_proto_fast_parsing.h:32:
./tensorflow/core/util/sparse/sparse_tensor.h:135:9: warning: 'SparseTensor' is deprecated: Use Create() functions instead of constructors directly. [-Wdeprecated-declarations]
      : SparseTensor(std::move(other.ix_), std::move(other.vals_),
        ^
./tensorflow/core/util/sparse/sparse_tensor.h:113:3: note: 'SparseTensor' has been explicitly marked deprecated here
  ABSL_DEPRECATED(""Use Create() functions instead of constructors directly."")
  ^
external/com_google_absl/absl/base/macros.h:149:49: note: expanded from macro 'ABSL_DEPRECATED'
#define ABSL_DEPRECATED(message) __attribute__((deprecated(message)))
                                                ^
In file included from tensorflow/core/util/example_proto_fast_parsing.cc:15:
In file included from ./tensorflow/core/util/example_proto_fast_parsing.h:32:
./tensorflow/core/util/sparse/sparse_tensor.h:568:10: warning: 'SparseTensor' is deprecated: Use Create() functions instead of constructors directly. [-Wdeprecated-declarations]
  return SparseTensor(output_ix, output_vals, final_shape, final_order);
         ^
./tensorflow/core/util/sparse/sparse_tensor.h:113:3: note: 'SparseTensor' has been explicitly marked deprecated here
  ABSL_DEPRECATED(""Use Create() functions instead of constructors directly."")
  ^
external/com_google_absl/absl/base/macros.h:149:49: note: expanded from macro 'ABSL_DEPRECATED'
#define ABSL_DEPRECATED(message) __attribute__((deprecated(message)))
                                                ^
In file included from tensorflow/core/util/example_proto_fast_parsing.cc:15:
In file included from ./tensorflow/core/util/example_proto_fast_parsing.h:32:
./tensorflow/core/util/sparse/sparse_tensor.h:568:10: warning: 'SparseTensor' is deprecated: Use Create() functions instead of constructors directly. [-Wdeprecated-declarations]
  return SparseTensor(output_ix, output_vals, final_shape, final_order);
         ^
./tensorflow/core/util/sparse/sparse_tensor.h:113:3: note: 'SparseTensor' has been explicitly marked deprecated here
  ABSL_DEPRECATED(""Use Create() functions instead of constructors directly."")
  ^
external/com_google_absl/absl/base/macros.h:149:49: note: expanded from macro 'ABSL_DEPRECATED'
#define ABSL_DEPRECATED(message) __attribute__((deprecated(message)))
                                                ^
In file included from tensorflow/core/util/example_proto_fast_parsing.cc:15:
In file included from ./tensorflow/core/util/example_proto_fast_parsing.h:32:
./tensorflow/core/util/sparse/sparse_tensor.h:726:10: warning: 'SparseTensor' is deprecated: Use Create() functions instead of constructors directly. [-Wdeprecated-declarations]
  return SparseTensor(output_indices, output_values, output_shape);
         ^
./tensorflow/core/util/sparse/sparse_tensor.h:99:3: note: 'SparseTensor' has been explicitly marked deprecated here
  ABSL_DEPRECATED(""Use Create() functions instead of constructors directly."")
  ^
external/com_google_absl/absl/base/macros.h:149:49: note: expanded from macro 'ABSL_DEPRECATED'
#define ABSL_DEPRECATED(message) __attribute__((deprecated(message)))
                                                ^
In file included from tensorflow/core/util/example_proto_fast_parsing.cc:15:
In file included from ./tensorflow/core/util/example_proto_fast_parsing.h:32:
./tensorflow/core/util/sparse/sparse_tensor.h:726:10: warning: 'SparseTensor' is deprecated: Use Create() functions instead of constructors directly. [-Wdeprecated-declarations]
  return SparseTensor(output_indices, output_values, output_shape);
         ^
./tensorflow/core/util/sparse/sparse_tensor.h:99:3: note: 'SparseTensor' has been explicitly marked deprecated here
  ABSL_DEPRECATED(""Use Create() functions instead of constructors directly."")
  ^
external/com_google_absl/absl/base/macros.h:149:49: note: expanded from macro 'ABSL_DEPRECATED'
#define ABSL_DEPRECATED(message) __attribute__((deprecated(message)))
                                                ^
8 warnings generated.
INFO: From Compiling tensorflow/core/util/strided_slice_op.cc [for host]:
tensorflow/core/util/strided_slice_op.cc:275:33: warning: lambda capture 'i' is not used [-Wunused-lambda-capture]
    auto canonical = [stride_i, i, dim_i, masks, valid_range](int64 x, int c) {
                                ^
1 warning generated.
INFO: From Compiling tensorflow/core/framework/dataset.cc [for host]:
In file included from tensorflow/core/framework/dataset.cc:15:
In file included from ./tensorflow/core/framework/dataset.h:27:
./tensorflow/core/framework/model.h:235:13: warning: writing variable 'buffered_bytes_' requires holding mutex 'result->mu_' exclusively [-Wthread-safety-precise]
    result->buffered_bytes_ = buffered_bytes_;
            ^
./tensorflow/core/framework/model.h:235:13: note: found near match 'mu_'
./tensorflow/core/framework/model.h:236:13: warning: writing variable 'processing_time_' requires holding mutex 'result->mu_' exclusively [-Wthread-safety-precise]
    result->processing_time_ = processing_time_;
            ^
./tensorflow/core/framework/model.h:236:13: note: found near match 'mu_'
./tensorflow/core/framework/model.h:237:13: warning: writing variable 'num_elements_' requires holding mutex 'result->mu_' exclusively [-Wthread-safety-precise]
    result->num_elements_ = num_elements_;
            ^
./tensorflow/core/framework/model.h:237:13: note: found near match 'mu_'
./tensorflow/core/framework/model.h:238:13: warning: writing variable 'parameters_' requires holding mutex 'result->mu_' exclusively [-Wthread-safety-precise]
    result->parameters_ = parameters_;
            ^
./tensorflow/core/framework/model.h:238:13: note: found near match 'mu_'
4 warnings generated.
INFO: From Compiling tensorflow/core/util/example_proto_helper.cc [for host]:
In file included from tensorflow/core/util/example_proto_helper.cc:15:
In file included from ./tensorflow/core/util/example_proto_helper.h:31:
./tensorflow/core/util/sparse/sparse_tensor.h:77:15: warning: 'SparseTensor' is deprecated: Use Create() functions instead of constructors directly. [-Wdeprecated-declarations]
    *result = SparseTensor(ix, vals, shape, order);
              ^
./tensorflow/core/util/sparse/sparse_tensor.h:113:3: note: 'SparseTensor' has been explicitly marked deprecated here
  ABSL_DEPRECATED(""Use Create() functions instead of constructors directly."")
  ^
external/com_google_absl/absl/base/macros.h:149:49: note: expanded from macro 'ABSL_DEPRECATED'
#define ABSL_DEPRECATED(message) __attribute__((deprecated(message)))
                                                ^
In file included from tensorflow/core/util/example_proto_helper.cc:15:
In file included from ./tensorflow/core/util/example_proto_helper.h:31:
./tensorflow/core/util/sparse/sparse_tensor.h:77:15: warning: 'SparseTensor' is deprecated: Use Create() functions instead of constructors directly. [-Wdeprecated-declarations]
    *result = SparseTensor(ix, vals, shape, order);
              ^
./tensorflow/core/util/sparse/sparse_tensor.h:113:3: note: 'SparseTensor' has been explicitly marked deprecated here
  ABSL_DEPRECATED(""Use Create() functions instead of constructors directly."")
  ^
external/com_google_absl/absl/base/macros.h:149:49: note: expanded from macro 'ABSL_DEPRECATED'
#define ABSL_DEPRECATED(message) __attribute__((deprecated(message)))
                                                ^
In file included from tensorflow/core/util/example_proto_helper.cc:15:
In file included from ./tensorflow/core/util/example_proto_helper.h:31:
./tensorflow/core/util/sparse/sparse_tensor.h:132:9: warning: 'SparseTensor' is deprecated: Use Create() functions instead of constructors directly. [-Wdeprecated-declarations]
      : SparseTensor(other.ix_, other.vals_, other.shape_, other.order_) {}
        ^
./tensorflow/core/util/sparse/sparse_tensor.h:113:3: note: 'SparseTensor' has been explicitly marked deprecated here
  ABSL_DEPRECATED(""Use Create() functions instead of constructors directly."")
  ^
external/com_google_absl/absl/base/macros.h:149:49: note: expanded from macro 'ABSL_DEPRECATED'
#define ABSL_DEPRECATED(message) __attribute__((deprecated(message)))
                                                ^
In file included from tensorflow/core/util/example_proto_helper.cc:15:
In file included from ./tensorflow/core/util/example_proto_helper.h:31:
./tensorflow/core/util/sparse/sparse_tensor.h:135:9: warning: 'SparseTensor' is deprecated: Use Create() functions instead of constructors directly. [-Wdeprecated-declarations]
      : SparseTensor(std::move(other.ix_), std::move(other.vals_),
        ^
./tensorflow/core/util/sparse/sparse_tensor.h:113:3: note: 'SparseTensor' has been explicitly marked deprecated here
  ABSL_DEPRECATED(""Use Create() functions instead of constructors directly."")
  ^
external/com_google_absl/absl/base/macros.h:149:49: note: expanded from macro 'ABSL_DEPRECATED'
#define ABSL_DEPRECATED(message) __attribute__((deprecated(message)))
                                                ^
In file included from tensorflow/core/util/example_proto_helper.cc:15:
In file included from ./tensorflow/core/util/example_proto_helper.h:31:
./tensorflow/core/util/sparse/sparse_tensor.h:568:10: warning: 'SparseTensor' is deprecated: Use Create() functions instead of constructors directly. [-Wdeprecated-declarations]
  return SparseTensor(output_ix, output_vals, final_shape, final_order);
         ^
./tensorflow/core/util/sparse/sparse_tensor.h:113:3: note: 'SparseTensor' has been explicitly marked deprecated here
  ABSL_DEPRECATED(""Use Create() functions instead of constructors directly."")
  ^
external/com_google_absl/absl/base/macros.h:149:49: note: expanded from macro 'ABSL_DEPRECATED'
#define ABSL_DEPRECATED(message) __attribute__((deprecated(message)))
                                                ^
In file included from tensorflow/core/util/example_proto_helper.cc:15:
In file included from ./tensorflow/core/util/example_proto_helper.h:31:
./tensorflow/core/util/sparse/sparse_tensor.h:568:10: warning: 'SparseTensor' is deprecated: Use Create() functions instead of constructors directly. [-Wdeprecated-declarations]
  return SparseTensor(output_ix, output_vals, final_shape, final_order);
         ^
./tensorflow/core/util/sparse/sparse_tensor.h:113:3: note: 'SparseTensor' has been explicitly marked deprecated here
  ABSL_DEPRECATED(""Use Create() functions instead of constructors directly."")
  ^
external/com_google_absl/absl/base/macros.h:149:49: note: expanded from macro 'ABSL_DEPRECATED'
#define ABSL_DEPRECATED(message) __attribute__((deprecated(message)))
                                                ^
In file included from tensorflow/core/util/example_proto_helper.cc:15:
In file included from ./tensorflow/core/util/example_proto_helper.h:31:
./tensorflow/core/util/sparse/sparse_tensor.h:726:10: warning: 'SparseTensor' is deprecated: Use Create() functions instead of constructors directly. [-Wdeprecated-declarations]
  return SparseTensor(output_indices, output_values, output_shape);
         ^
./tensorflow/core/util/sparse/sparse_tensor.h:99:3: note: 'SparseTensor' has been explicitly marked deprecated here
  ABSL_DEPRECATED(""Use Create() functions instead of constructors directly."")
  ^
external/com_google_absl/absl/base/macros.h:149:49: note: expanded from macro 'ABSL_DEPRECATED'
#define ABSL_DEPRECATED(message) __attribute__((deprecated(message)))
                                                ^
In file included from tensorflow/core/util/example_proto_helper.cc:15:
In file included from ./tensorflow/core/util/example_proto_helper.h:31:
./tensorflow/core/util/sparse/sparse_tensor.h:726:10: warning: 'SparseTensor' is deprecated: Use Create() functions instead of constructors directly. [-Wdeprecated-declarations]
  return SparseTensor(output_indices, output_values, output_shape);
         ^
./tensorflow/core/util/sparse/sparse_tensor.h:99:3: note: 'SparseTensor' has been explicitly marked deprecated here
  ABSL_DEPRECATED(""Use Create() functions instead of constructors directly."")
  ^
external/com_google_absl/absl/base/macros.h:149:49: note: expanded from macro 'ABSL_DEPRECATED'
#define ABSL_DEPRECATED(message) __attribute__((deprecated(message)))
                                                ^
8 warnings generated.
INFO: From Compiling tensorflow/core/framework/model.cc [for host]:
In file included from tensorflow/core/framework/model.cc:16:
./tensorflow/core/framework/model.h:235:13: warning: writing variable 'buffered_bytes_' requires holding mutex 'result->mu_' exclusively [-Wthread-safety-precise]
    result->buffered_bytes_ = buffered_bytes_;
            ^
./tensorflow/core/framework/model.h:235:13: note: found near match 'mu_'
./tensorflow/core/framework/model.h:236:13: warning: writing variable 'processing_time_' requires holding mutex 'result->mu_' exclusively [-Wthread-safety-precise]
    result->processing_time_ = processing_time_;
            ^
./tensorflow/core/framework/model.h:236:13: note: found near match 'mu_'
./tensorflow/core/framework/model.h:237:13: warning: writing variable 'num_elements_' requires holding mutex 'result->mu_' exclusively [-Wthread-safety-precise]
    result->num_elements_ = num_elements_;
            ^
./tensorflow/core/framework/model.h:237:13: note: found near match 'mu_'
./tensorflow/core/framework/model.h:238:13: warning: writing variable 'parameters_' requires holding mutex 'result->mu_' exclusively [-Wthread-safety-precise]
    result->parameters_ = parameters_;
            ^
./tensorflow/core/framework/model.h:238:13: note: found near match 'mu_'
4 warnings generated.
INFO: From ProtoCompile tensorflow/core/grappler/costs/op_performance_data_pb2.py [for host]:
external/protobuf_archive/python: warning: directory does not exist.
external/protobuf_archive/python: warning: directory does not exist.
external/protobuf_archive/python: warning: directory does not exist.
external/protobuf_archive/python: warning: directory does not exist.
INFO: From Linking tensorflow/core/liblib_internal_impl.a [for host]:
/Library/Developer/CommandLineTools/usr/bin/libtool: warning same member name (env.o) in output file used for input files: bazel-out/host/bin/tensorflow/core/_objs/lib_internal_impl/0/env.o and: bazel-out/host/bin/tensorflow/core/_objs/lib_internal_impl/1/env.o (due to use of basename, truncation, blank padding or duplicate input files)
/Library/Developer/CommandLineTools/usr/bin/libtool: warning same member name (tracing.o) in output file used for input files: bazel-out/host/bin/tensorflow/core/_objs/lib_internal_impl/0/tracing.o and: bazel-out/host/bin/tensorflow/core/_objs/lib_internal_impl/1/tracing.o (due to use of basename, truncation, blank padding or duplicate input files)
/Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/tensorflow/core/_objs/lib_internal_impl/android_armv7a_cpu_utils_helper.o has no symbols
ERROR: /private/var/tmp/_bazel_geri/af336600bf091f11bdb6522728b81044/external/flatbuffers/BUILD.bazel:99:1: C++ compilation of rule '@flatbuffers//:flatc' failed (Exit 1) sandbox-exec failed: error executing command 
  (cd /private/var/tmp/_bazel_geri/af336600bf091f11bdb6522728b81044/execroot/org_tensorflow && \
  exec env - \
    PATH=/Users/geri/.pyenv/shims:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/usr/local/go/bin:/Users/geri/intelligence/bin \
    PWD=/proc/self/cwd \
    TMPDIR=/var/folders/7t/9l0n105n4_jbrq076r887pthz96c2m/T/ \
  /usr/bin/sandbox-exec -f /private/var/tmp/_bazel_geri/af336600bf091f11bdb6522728b81044/sandbox/darwin-sandbox/381/sandbox.sb /var/tmp/_bazel_geri/install/c71a8bada5761bf76ced20ca6672b491/_embedded_binaries/process-wrapper '--timeout=0' '--kill_delay=15' external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/host/bin/external/flatbuffers/_objs/flatc/idl_gen_grpc.d '-frandom-seed=bazel-out/host/bin/external/flatbuffers/_objs/flatc/idl_gen_grpc.o' -iquote external/flatbuffers -iquote bazel-out/host/genfiles/external/flatbuffers -iquote bazel-out/host/bin/external/flatbuffers -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -iquote bazel-out/host/bin/external/bazel_tools -isystem external/flatbuffers/grpc -isystem bazel-out/host/genfiles/external/flatbuffers/grpc -isystem bazel-out/host/bin/external/flatbuffers/grpc -isystem external/flatbuffers/include -isystem bazel-out/host/genfiles/external/flatbuffers/include -isystem bazel-out/host/bin/external/flatbuffers/include -g0 -g0 -Wno-implicit-fallthrough -fexceptions -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/flatbuffers/src/idl_gen_grpc.cpp -o bazel-out/host/bin/external/flatbuffers/_objs/flatc/idl_gen_grpc.o)
external/flatbuffers/src/idl_gen_grpc.cpp:57:21: error: no member named 'doc_comment' in 'flatbuffers::RPCCall'; did you mean 'rpc_comment'?
    return method_->doc_comment;
                    ^~~~~~~~~~~
                    rpc_comment
/usr/local/include/flatbuffers/idl.h:349:28: note: 'rpc_comment' declared here
  std::vector<std::string> rpc_comment;
                           ^
external/flatbuffers/src/idl_gen_grpc.cpp:174:9: error: use of undeclared identifier 'FLATBUFFERS_ASSERT'
        FLATBUFFERS_ASSERT(indent_ >= 0);
        ^
2 errors generated.
Target //tensorflow/python/tools:saved_model_cli failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 166.831s, Critical Path: 93.66s
INFO: 379 processes: 378 darwin-sandbox, 1 local.
FAILED: Build did NOT complete successfully
```

bazel version:
```
bazel version
WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
/Users/geri/Documents/tensorflow/tools/bazel.rc
INFO: Invocation ID: 143fe03e-0926-4e69-9516-0e132c8ed3b5
Build label: 0.20.0
Build target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Nov 30 14:38:25 2018 (1543588705)
Build timestamp: 1543588705
Build timestamp as int: 1543588705
```

osx version:
```
10.14.2 (18C54)
```

Python version:
```
$ python3 --version
Python 3.6.0
```

tensorflow version:
```
$ pip3 install --upgrade tensorflow
Requirement already up-to-date: tensorflow in /Users/geri/.pyenv/versions/3.6.0/lib/python3.6/site-packages (1.12.0)
Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /Users/geri/.pyenv/versions/3.6.0/lib/python3.6/site-packages (from tensorflow) (0.2.0)
Requirement already satisfied, skipping upgrade: six>=1.10.0 in /Users/geri/.pyenv/versions/3.6.0/lib/python3.6/site-packages (from tensorflow) (1.11.0)
Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /Users/geri/.pyenv/versions/3.6.0/lib/python3.6/site-packages (from tensorflow) (0.7.1)
Requirement already satisfied, skipping upgrade: absl-py>=0.1.6 in /Users/geri/.pyenv/versions/3.6.0/lib/python3.6/site-packages (from tensorflow) (0.6.1)
Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /Users/geri/.pyenv/versions/3.6.0/lib/python3.6/site-packages (from tensorflow) (1.17.0)
Requirement already satisfied, skipping upgrade: wheel>=0.26 in /Users/geri/.pyenv/versions/3.6.0/lib/python3.6/site-packages (from tensorflow) (0.32.3)
Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /Users/geri/.pyenv/versions/3.6.0/lib/python3.6/site-packages (from tensorflow) (1.0.6)
Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /Users/geri/.pyenv/versions/3.6.0/lib/python3.6/site-packages (from tensorflow) (1.0.5)
Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /Users/geri/.pyenv/versions/3.6.0/lib/python3.6/site-packages (from tensorflow) (3.6.1)
Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /Users/geri/.pyenv/versions/3.6.0/lib/python3.6/site-packages (from tensorflow) (1.14.5)
Requirement already satisfied, skipping upgrade: tensorboard<1.13.0,>=1.12.0 in /Users/geri/.pyenv/versions/3.6.0/lib/python3.6/site-packages (from tensorflow) (1.12.0)
Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /Users/geri/.pyenv/versions/3.6.0/lib/python3.6/site-packages (from tensorflow) (1.1.0)
Requirement already satisfied, skipping upgrade: h5py in /Users/geri/.pyenv/versions/3.6.0/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)
Requirement already satisfied, skipping upgrade: setuptools in /Users/geri/.pyenv/versions/3.6.0/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow) (28.8.0)
Requirement already satisfied, skipping upgrade: werkzeug>=0.11.10 in /Users/geri/.pyenv/versions/3.6.0/lib/python3.6/site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow) (0.14.1)
Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /Users/geri/.pyenv/versions/3.6.0/lib/python3.6/site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow) (3.0.1)
```

gcc version:
```
$ gcc --version
Configured with: --prefix=/Library/Developer/CommandLineTools/usr --with-gxx-include-dir=/Library/Developer/CommandLineTools/SDKs/MacOSX10.14.sdk/usr/include/c++/4.2.1
Apple LLVM version 10.0.0 (clang-1000.10.44.4)
Target: x86_64-apple-darwin18.2.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin
```

g++ version:
```
MM-MAC-3270:tensorflow gstanje$ g++ --version
Configured with: --prefix=/Library/Developer/CommandLineTools/usr --with-gxx-include-dir=/Library/Developer/CommandLineTools/SDKs/MacOSX10.14.sdk/usr/include/c++/4.2.1
Apple LLVM version 10.0.0 (clang-1000.10.44.4)
Target: x86_64-apple-darwin18.2.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin
```"
24214,[Tensorrt] Cannot convert when the batch dimension is changed,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
I'm trying to convert seq2seq model graph to trt graph. 
But it seems that the Reshape OP cannot be converted when the batch dimension is changed.
If possible, do  you have any plan to provide support for this feature?


**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**
"
24213,Whether Tf has the operations table?,"I want to know about TF, whether it has the operations table?
Can any one tell me to know about it?
Thanks.
"
24212,Bazel test fail. Error deduced conflicting types. ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: Mac OS Mojave 10.14.1 (18B75)
- TensorFlow installed from (source or binary): source
- TensorFlow version: branch r1.12
- Python version: 3.6.6 
- Installed using virtualenv? pip? conda?: building is in a virtualenv created by `pyenv virtualenv 3.6.6 my_tf_build_env`
- Bazel version (if compiling from source): from source. 0.15.0
- GCC/Compiler version (if compiling from source): Apple LLVM version 10.0.0 (clang-1000.11.45.5)
- CUDA/cuDNN version:  no cuda, build only for cpu
- GPU model and memory: Intel Core i7, 16 GB



**Describe the problem**

I was following the tutorial to build tensorflow from source.  I was on the `r1.12` branch, root directory, typed `bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/lite/...`. Error Message:

![image](https://user-images.githubusercontent.com/1506580/49625289-efd57f80-fa0f-11e8-9ac5-e66542c8ff04.png)


**Provide the exact sequence of commands / steps that you executed before running into the problem**

commands sequence is same as this [tutorial](https://www.tensorflow.org/install/source).
In the `./configure` part, my configuration is like below:

![image](https://user-images.githubusercontent.com/1506580/49625409-61adc900-fa10-11e8-80bf-687a5ca32db0.png)


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24211,something between tf 1.2.1 -> 1.12.0 causing call stack issues in embedded python,"https://github.com/pythonnet/pythonnet/issues/792

This code running in pythonnet now crashes:

```
import tensorflow as tf
flags = tf.app.flags
flags.DEFINE_integer(""batch_size"", 32, ""Number of samples per batch"")
```

in scriptcs:

```
> #r C:\Python\Python35_64b\lib\site-packages\Python.Runtime.dll
> using Python.Runtime;
> PythonEngine.Initialize();
> dynamic tf = Py.Import(""tensorflow"");
> tf.app.flags.DEFINE_integer(""batch_size"", 32, ""Number of samples per batch"");
ValueError : call stack is not deep enough
```
"
24210,Saving Tesnors as Image in C++ api,"Hi ,
In tensorflow C++ api I want to save the output tensor as an image to a local file.
 `Status run_status = session->Run({{input_layer, resized_tensor}}, {output_layer}, {}, &outputs);`
After running session my output is actually a tensor of size 1,720,1280,1. How do I directly save it to a local binary file or image without using opencv?


- TensorFlow version :1.8
- Are you willing to contribute it (Yes/No): Yes
- Ubuntu 14.0.4



"
24209,Tensorflow-GPU ImportError: DLL load failed,"### System information
- *OS:* Windows 10
- **TensorFlow version**: Tensorflow v 1.12
- **Python version**: 3.6.0
- **CUDA/cuDNN version**: CUDA 10.0, cuDNN 7.4.1.5
- **GPU model and memory**: 2080 ti

I'm trying to get Tensorflow set-up on my machine. I've followed about 10 different tutorials but always come back to this same issue. I see that a bunch of other people have the same issue and just downgraded their CUDA to 9.0 due to Tensorflow not supporting 10.0. My problem is that the 2080 ti is only supported with CUDA 10.0

Is anyone else using one of the RTX cards and have they had any of the same issues? My log is below.

### Source code / logs
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\cmacphail\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\cmacphail\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\cmacphail\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\cmacphail\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\cmacphail\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\cmacphail\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\cmacphail\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\cmacphail\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.
"
24208,Can Tensorflow (not lite) run on Android NDK C++,"It looks like features like Dropout are not supported on Tensorflow Lite + Android NDK.

I've got a Keras model that uses GPU, Dropout, CuDnnLSTM layer and I want to convert it to a tflite file or run it on Tensorflow for Android.

Is this possible? Is there a guide that tells me what is and is not supported on tflite?"
24205,Hard arbitrary limit on Saved model size,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Both
- TensorFlow version (use command below): 1.10+
- Python version: N/A
- Bazel version (if compiling from source): 17.2
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


**Describe the current behavior**
I am trying to load a 1.4GiB binary saved model and am getting an error saying that the protobuf message is too big i.e. over 1GiB

**Describe the expected behavior**
Current default limits for protobuf messages loaded by a CodedInputStream are 2GiB so my model should load without error.

**Code to reproduce the issue**

Comment out line 503 in env.cc:
//coded_stream.SetTotalBytesLimit(1024LL << 20, 512LL << 20);
rebuild and everything works as expected. 

**Other info / logs**

Also the second parameter to SetTotalBytesLimit has been deprecated and is no longer enforced. 
"
24204,Spurious syntax highlighting of code blocks in site documentation,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: N/A
- Doc Link: Various, but see for example [/community/documentation](https://www.tensorflow.org/community/documentation), [/guide/debugger](https://www.tensorflow.org/guide/debugger), or [/guide/datasets_for_estimators](https://www.tensorflow.org/guide/datasets_for_estimators)


**Describe the documentation issue**
Many of the code blocks in [this guide on Writing Tensorflow Documentation](https://www.tensorflow.org/community/documentation#description_of_the_docstring_sections) are meant to represent markdown, but they're syntax highlighted as... something else. e.g. the word ""from"" is highlighted as if it were a keyword in:
```
Flips an image horizontally from left to right.
```

These blocks should either have no syntax highlighting, or markdown syntax highlighting (if the library you're using for syntax highlighting allows it).

In some cases, authors have explicitly used a language identifier of 'none' in fenced code blocks, presumably to try to suppress syntax highlighting, but it doesn't look like that's respected when rendering. For example, the first two code blocks in [/guide/debugger](https://www.tensorflow.org/guide/debugger) are tagged 'none' in [the corresponding md file](https://github.com/tensorflow/docs/blob/master/site/en/guide/debugger.md), but the ""Accuracy at step..."" block is still lit up like a Christmas tree.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
I wish I could, but as far as I can tell, the markdown -> html part of the docs pipeline isn't part of any public repo. "
24203,Markdown formatting issue in tf.keras.layers.Layer docs,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: r1.12
- Doc Link: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer


**Describe the documentation issue**
Below the 'Arguments' list, it looks like there are supposed to be lists of read-only properties and mutable properties, but they render as paragraphs rather than lists.

Seems like the issue is that the parser splits the docstring into sections based on a magic list of keywords, `detail_keywords = '|'.join(['Args', 'Arguments', 'Fields', 'Returns', 'Yields', 'Raises', 'Attributes'])`, and that list does not include the strings 'Read-only properties', or 'Mutable properties'. So it treats everything below ""Arguments"" as part of the list of arguments, and trips over itself.

(There's also some question as to whether these lists should be there at all, or whether this information should be in the [Properties](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#properties) section)

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
Not with my current understanding of the docs system. I was able to find a script for generating a markdown index of the docs locally (`tensorflow/tools/docs/generate2.py`), but when I ran it, I got [a markdown file that didn't seem to have the issue I see on the site](https://gist.github.com/colinmorris/0a180ea671e12ccdb65fe6cc9f0319fa). Maybe it's because I used `generate2.py` but the docs on the site are generated by `generate.py`? Maybe the lists are fine in GitHub-flavoured markdown but not whatever engine is used when converting the docs from md to html for the site? (I'll file a separate issue about this meta-documentation issue)"
24202,Saved model incompatible betweeen v1.4 and v1.8,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution: Ubuntu 14.04 / Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source / Binary
- TensorFlow version (use command below): 1.4 / 1.8
- Python version: 2.7 / 2.7
- Bazel version (if compiling from source): 0.5.4 / 0.14.0
- GCC/Compiler version (if compiling from source): 4.8.4 / 7.3.0
- CUDA/cuDNN version: 8.0/6.0.21 / None
- GPU model and memory: Nvidia GTX1080 + 32GB RAM / No GPU + 16GB RAM


I trained a network using a machine running Ubuntu 14.04 + TF v1.4 (running on GPU), and then tried to restore the model on another machine running Ubuntu 18.04 + TF v.18 (running on CPU). Note that I don't restore the .meta file; rather I run the same Python code on both machines to build the graph, and then restore the .data-00000-of-00001 file.

The problem is that even though there are no errors in running the script, the output of the network is completely different as compared to when run on the original machine (the output is mostly 0 everywhere). Is there anything I can change to fix this issue? I thought saved models were cross-compatible for the same major release (v1.x)

In the info above, I have listed the details in the format <SPEC FOR 14.04 MACHINE> / <SPEC FOR 18.04 MACHINE>.
"
24200,"TypeError: Expected binary or unicode string, got None","**System information**
- OS Platform and Distribution Linux Ubuntu 16.04
- TensorFlow version (use command below):
- Python version 3.5:
- CUDA/cuDNN version9.0:
- GPU model and memory: tesla k80


"
24199,Python 3.6.7 DLL  load failed on GPU version,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro Build 17134
- TensorFlow installed from (source or binary): Package (pip install tensorflow-gpu)
- TensorFlow version: 1.12.0
- Python version: 3.6.7
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: CUDA 9/CuDNN 7.4.1.5
- GPU model and memory: GTX 1070ti 416.34



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I have been working with some code in keras that works fine with the cpu version of tensorflow (pip install tensorflow)  Since each epoch was taking a minute to run, I wanted to throw my 1070ti at the problem. I installed tensorflow-gpu (pip install tensorflow-gpu) along with CUDA and CuDNN.  I then did a quick restart but when I went to run my code again it didn't make keras use the gpu version of tensorflow.  I uninstalled tensorflow (pip uninstall tensorflow) gave it another restart and now getting the following error with the DLL load failing.  Reinstalling the cpu version of Tensorflow getting it working again but only on the CPU.  I can't see what I have done wrong but any help is appreciated.  I have a look through some of the past issues but the solutions only seem to be for the CPU package not importing.  Might have missed some finesse there so please do point out the obvious.

**Any other info / logs**
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\natha\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\natha\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\natha\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\natha\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\natha\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<pyshell#0>"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\natha\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\natha\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\natha\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\natha\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\natha\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\natha\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\natha\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\natha\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
24198,TextLineDataset supporting start/end file positions,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.11.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
When constructing a text dataset pipeline, we often want to extract text lines from part of a large text file, instead of from the whole file. For example, 

- using 99% lines at the head of a large file as train set and the remaining 1% as valid set.

- when training a model in parallel on multiple GPUs or nodes, we want to split large text files into equal-lengthed parts for each GPU to train one part of them.

Traditionally, to achieve this goal we have to do file operations such as splitting, copying and renaming. These operations are tedious, error-pruning and cost more disk space and time. It also messes up directory/file structures. More over, it is hard to share between different environments with different count of GPUs/nodes, or with different text file set.
Here I have created a `TextLineBlockDataset` op kernel as well as a python `dataset_ops.Dataset` by mimicking `TextLineDataset`. `TextLineBlockDataset` uses a [begin_offset, end_offset) range to mark a block of a text file, freeing one from tedious file splitting preparations.

This is the registration of the dataset op:
```
REGISTER_OP(""TextLineBlockDataset"")
    .Input(""filenames: string"")
    .Input(""begin_offsets: int64"")
    .Input(""end_offsets: int64"")
    .Input(""buffer_size: int64"")
    .Output(""handle: variant"")
    .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked
        // stateful to inhibit constant folding.
    .SetShapeFn([](shape_inference::InferenceContext* c) {
      shape_inference::ShapeHandle unused;
      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(0), 1, &unused));
      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(1), 1, &unused));
      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(2), 1, &unused));
      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));
      return shape_inference::ScalarShape(c);
    });
```
and the python API interface to it:
```
@tf_export(""data.TextLineBlockDataset"")
class TextLineBlockDataset(dataset_ops.Dataset):
    """"""A `Dataset` comprising lines from one or more text file blocks.""""""

    def __init__(self, filenames, begin_offsets, end_offsets, buffer_size=None):
        """"""Creates a `TextLineBlockDataset`.
        An element in zip(filenames, begin_offsets, end_offsets) denotes a text
        file block in file filename, beginning at begin_offset and ends at
        end_offset, in byte.
        `begin_offsets` and `end_offsets` will be smoothed to match text line
        boundaries under the hood.

        Args:
          filenames: A `tf.string` tensor containing one or more filenames.
          begin_offsets: A `tf.int64` 1-d tensor denoting begin offsets.
          end_offsets: A `tf.int64` 1-d tensor denoting end offsets.
          buffer_size: (Optional.) A `tf.int64` scalar denoting the number of bytes
            to buffer for each block, for each block. A value of 0 results in the 
            default values chosen on the compression type.
        """"""
```

This is how I use it:
Suppose I have 3 large text files, I want to extract text lines ranging from ratio 2/4 to 3/4 of each text file. Each text file block will be split into 8 smaller blocks for better shuffle effect.
```
if __name__ == '__main__':
    filenames = [""data/train.txt"", ""data/dev.txt"", ""data/test.txt""]
    n_files = len(filenames)
    # this is the 2nd of the 4 towers which consumes range 2/4 ~ 3/4 text lines of each file
    n_towers, tower_id = 4, 2
    n_splits = 8    # each range will be further split into 8 parts for better shuffle
    buffer_size = 256 * 1024

    block_params = []
    for file_id in range(n_files):
        file_size = os.path.getsize(filenames[file_id])
        split_size = file_size / (n_towers * n_splits)
        for split_id in range(n_splits):
            begin_offset = int(split_size * (tower_id * n_splits + split_id))
            end_offset = int(split_size * (tower_id * n_splits + split_id + 1))
            block_params.append((filenames[file_id], begin_offset, end_offset))
    dataset = TextLineBlockDataset(*zip(*block_params), buffer_size)
    dataset = dataset.shuffle(32000)  # shuffle in the same manner as `TextLineDataset`
    get_next = dataset.make_one_shot_iterator().get_next()

    sess = tf.Session()
    while True:
        try:
            print(sess.run(get_next).decode(""utf-8""))
        except tf.errors.OutOfRangeError:
            break
```

**Will this change the current api? How?**
Yes.
A new dataset op kernal will be registered in the C++ side, and a corresponding python 
class tf.data.TextLineBlockDataset will be added.

**Who will benefit with this feature?**
Tensorflow users who deal with large text files.

**Any Other info.**
@mrry @eaplatanios @rachellim
"
24197,Building Tensorflow lite on ARM64,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- Python version: 2.7.15rc/3.6.7
- Bazel version (if compiling from source): 0.20.0- (@non-git)
- GCC/Compiler version (if compiling from source): gcc (Ubuntu/Linaro 7.3.0-27ubuntu1~18.04) 7.3.0

**Describe the problem**

I'm trying to install tensorflow lite using bazel. I'm trying to compile it on AWS ARM 64 bit machine. 
I compiled tehnsorflow using the command:
`bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/lite/... --incompatible_remove_native_http_archive=false`
During the compilation I received this error message:

```
ImportError: Traceback (most recent call last):
  File ""/home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN3Aws11Environment6GetEnvB5cxx11EPKc

Failed to load the native TensorFlow runtime.

```
"
24196,Possibly buffer overflow in tf.nn.conv2d on GPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12
- Python version: 3.5.3
- CUDA/cuDNN version: 9.0 / 7.2.1
- GPU model and memory: Tesla M60 (also tested on GeForce GTX TITAN Black)

**Describe the current behavior**
I've created a tensor set to zero, except a single entry at [:,-1,-1,0] that was set to a large number 1e10. I've then convolved the tensor with a random kernel. When the batch size is small (e.g. 5) then the output at [:,0,0,:] is zero (because the input array is zero there), but when the batch size is large (e.g. 100) then these entries contain numbers significantly larger than zero. This only happens on the GPU.
**Describe the expected behavior**
The expected behavior should be zero regardless if running on CPU or GPU, because windows at these locations contain only zero entries. Because the only non-zero entry is outside the window used to compute the value, then this is most likely a buffer overflow (or some other memory access issue).
**Code to reproduce the issue**
If the following does not immediately produces the bug, then try increasing the batch size from 100 to something greater.
```python
import tensorflow as tf
import numpy as np


def test(sess):
	w = np.random.uniform(size=(5,5,64,64)).astype(np.float32)
	x_t = tf.placeholder(dtype=tf.float32, shape=(None, 32, 32, 64))
	w_t = tf.constant(w)
	o_t = tf.nn.conv2d(x_t, w_t, [1,1,1,1], 'VALID', data_format='NHWC')
	x_good = np.zeros((5,32,32, 64), dtype=np.float32)
	x_good[:,-1,-1,0] = 1e10
	x_bad = np.zeros((100,32,32, 64), dtype=np.float32)
	x_bad[:,-1,-1,0] = 1e10
	o_good = sess.run(o_t, feed_dict={x_t : x_good})
	o_bad = sess.run(o_t, feed_dict={x_t : x_bad})
	print('Number nonzero (good): ', np.count_nonzero(o_good[:,0,0,:]))
	print('Number nonzero (bad): ', np.count_nonzero(o_bad[:,0,0,:]))
	if np.count_nonzero(o_bad[:,0,0,:]) > 0:
		print(o_bad[0,0,0,:])

with tf.Session() as sess:
	with tf.device('cpu:0'):
		print('Testing on cpu -- should succeed')
		test(sess)
	with tf.device('gpu:0'):
		print('Testing on gpu -- typically fails')
		test(sess)
```
"
24192,Add tf.metrics.std or tf.metrics.var,"**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): Maybe, it depends on the complexity



**Describe the feature and the current behavior/state.**
Currently the module tf.metrics does not contain tf.metrics.std or tf.metrics.var for the calculation of standard deviation and variance over multiple batches, respectively. It would be very useful to have these type of metrics.

**Will this change the current api? How?**
This will change the current api by adding two more apis: tf.metrics.std  and  tf.metrics.var

**Who will benefit with this feature?**
Everyone who wants to calculate statistics over multiple batches.

**Any Other info.**
Currently it is possible to use tf.nn.moments and perform some basic math in order to calculate the std over multiple batches. However tf.metrics is a simple way to obtain the same result."
24191,tensorflow for golang did not use GPU to speed up,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
```
package main

import (
	""flag""
	""fmt""
	""time""

	tf ""github.com/tensorflow/tensorflow/tensorflow/go""
	""github.com/tensorflow/tensorflow/tensorflow/go/op""
)

func main() {
        fmt.Println(tf.Version())
	total := flag.Int(""total"", 10000, """")
	batch := flag.Int(""batch"", 1000, """")
	flag.Parse()

	const inerSize = 10000
	root := op.NewScope()
	A := op.Placeholder(root.SubScope(""input""), tf.Int32, op.PlaceholderShape(tf.MakeShape(-1, inerSize)))
	x := op.Placeholder(root.SubScope(""input""), tf.Int32, op.PlaceholderShape(tf.MakeShape(inerSize, 1)))
	product := op.MatMul(root, A, x)

	graph, err := root.Finalize()
	if err != nil {
		panic(err.Error())
	}

	var sess *tf.Session
	sess, err = tf.NewSession(graph, &tf.SessionOptions{})
	if err != nil {
		panic(err.Error())
	}

	matrix := make([][inerSize]int32, *total)
	column := [inerSize][1]int32{}

	tx, _ := tf.NewTensor(column)

	t1 := time.Now()
	for i := 0; i*(*batch) < *total; i++ {
		start, end := i*(*batch), (i+1)*(*batch)
		if end > *total {
			end = *total
		}
		tA, _ := tf.NewTensor(matrix[start:end])
		_, err = sess.Run(
			map[tf.Output]*tf.Tensor{A: tA, x: tx},
			[]tf.Output{product},
			nil,
		)
		if err != nil {
			panic(err.Error())
		}
	}

	fmt.Println(""cost:"", time.Since(t1))
}
```
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 16.04.5 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
1.5.0
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
cuda-9.2
cudnn-7.2.1
- GPU model and memory:
GeForce GTX 1080 Ti 11GB

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
session Run cost 4s on GPU:
```
2018-12-06 17:03:14.781911: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
2018-12-06 17:03:14.985388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-12-06 17:03:14.985940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
pciBusID: 0000:83:00.0
totalMemory: 10.92GiB freeMemory: 10.61GiB
2018-12-06 17:03:14.985972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:83:00.0, compute capability: 6.1)
cost: 4.133031516s
```
session Run cost 4s on CPU:
```
2018-12-06 17:03:30.681019: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
cost: 4.23027643s
```

**Describe the expected behavior**
running on GPU should be much faster than on CPU

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
1, Download tensorflow binary of both CPU and GPU version
2, Set LD_LIBRARY_PATH to /tensorflow-cpu/lib and run the above golang code
3, Set LD_LIBRARY_PATH to /tensorflow-gpu/lib and run the above golang code
``` 

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24190,Cannot convert between a TensorFlowLite buffer with 1072812 bytes and a ByteBuffer with 270000 bytes.,"**Train model:**
`python3 /Users/tianchuangxin1/hub/examples/image_retraining/retrain.py     
--image_dir /Users/tianchuangxin1/tensorflow_image    
--how_many_training_steps=2000   
----model_dir=/Users/tianchuangxin1/tensorflow_image  
--output_graph  /Users/tianchuangxin1/tensorflow_image/output_graph.pb  
--output_labels /Users/tianchuangxin1/tensorflow_image/output_labels.txt `
**Test model:**
`python3 /Users/tianchuangxin1/tensorflow/tensorflow/examples/label_image/label_image.py \
--graph=/Users/tianchuangxin1/tensorflow_image/output_graph.pb --labels=/Users/tianchuangxin1/tensorflow_image/output_labels.txt \
--input_layer=Placeholder \
--output_layer=final_result \
--image=/Users/tianchuangxin1/tensorflow_image/apple/apple5.jpg
`
**Get the result:**
`2018-12-06 15:16:48.733220: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
apple 0.99963677
watch 0.0003632232`

**Turn to tflite format:**
`tflite_convert \
  --graph_def_file=/Users/tianchuangxin1/tensorflow_image/output_graph.pb \
  --output_file=/Users/tianchuangxin1/tensorflow_image/output_graph.tflite \
  --output_format=TFLITE \
  --input_shape=1,299,299,3 \
  --input_array=Placeholder \
  --output_array=final_result \
  --inference_type=FLOAT \
  --input_data_type=FLOAT `

**All above have none error.**

**But, i got the crash on Android device:**

`java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 1072812 bytes and a ByteBuffer with 270000 bytes.
        at org.tensorflow.lite.Tensor.throwExceptionIfTypeIsIncompatible(Tensor.java:221)
        at org.tensorflow.lite.Tensor.setTo(Tensor.java:93)
        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:141)
        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:229)
        at org.tensorflow.demo.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:194)
        at org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:249)
        at android.os.Handler.handleCallback(Handler.java:751)
        at android.os.Handler.dispatchMessage(Handler.java:95)
        at android.os.Looper.loop(Looper.java:159)
        at android.os.HandlerThread.run(HandlerThread.java:61)
`

**Which step wrong , i have no idea! And spend for a long time!!!!**


"
24188,LSTM's .h5 to .tflite,"thanks for your reading.
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below):1.12.0
- Python version:3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the problem**
1.used the tf.keras to train the LSTM's model, and get the model.h5 file.
2.want convert the .h5 file to .tflite file ,and coded:
```
converter = tf.lite.TFLiteConverter.from_keras_model_file(""model.h5"")
tflite_model = converter.convert()
open(""_model.tflite"", ""wb"").write(tflite_model)
```
and get the error:
```
WARNING:tensorflow:From D:\python\python3\lib\site-packages\tensorflow\python\framework\graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.compat.v1.graph_util.extract_sub_graph
Traceback (most recent call last):
  File ""D:/code/load_forecast/test/test1.py"", line 5, in <module>
    tflite_model = converter.convert()
  File ""D:\python\python3\lib\site-packages\tensorflow\lite\python\lite.py"", line 456, in convert
    **converter_kwargs)
  File ""D:\python\python3\lib\site-packages\tensorflow\lite\python\convert.py"", line 397, in toco_convert_impl
    input_data.SerializeToString())
  File ""D:\python\python3\lib\site-packages\tensorflow\lite\python\convert.py"", line 172, in toco_convert_protos
    ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
2018-12-06 13:41:26.456495: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: TensorArrayV3
.........
2018-12-06 13:41:26.495220: I tensorflow/lite/toco/import_tensorflow.cc:1329] Unable to determine output type for op: TensorArrayGatherV3
2018-12-06 13:41:26.506452: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 120 operators, 212 arrays (0 quantized)
2018-12-06 13:41:26.508258: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 120 operators, 212 arrays (0 quantized)
2018-12-06 13:41:26.513681: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 106 operators, 194 arrays (0 quantized)
2018-12-06 13:41:26.515613: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 106 operators, 194 arrays (0 quantized)
2018-12-06 13:41:26.517624: F tensorflow/lite/toco/tooling_util.cc:625] Check failed: dim >= 1 (0 vs. 1)
```
3.but success to convert the linear's and cnn's .h5 file to .tflite 

want to ask the question: Did someone successfully transform the .h5 model to .tflite?

look forward to your answer."
24187,Load multiple tflite models in APP,"Hi experts,
I got some errorsit seems to be the APP crash or collapse when I load multiple SSD_mobilenet tflite models for object detection in a APP with sufficient memory,where these models have different the number of classes.  However, APP work well when I only load a model. So, Could I load multiple tflite models in a APP? Could you please give my advises and instruction of solving this problem? Thanks."
24185,Problem importing Tensorflow with Python 3.6.7,"Hi experts,
    I've installed Anaconda 3 64-bit and Python 3.6.7 on my Windows 10 laptop, also I've installed Tensorflow, however when I try to import Tensorflow, it fails, messages are appended below, could you please give my advises and instruction of solving this problem? Thanks.



```
(tensorflow) C:\Users\user>python
Python 3.6.7 |Anaconda custom (64-bit)| (default, Oct 28 2018, 19:44:12) [MSC v.1915 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.


>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\user\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])
  File ""C:\Users\user\Anaconda3\envs\tensorflow\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\user\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\user\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\user\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow_internal
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\user\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *  # pylint: disable=redefined-builtin
  File ""C:\Users\user\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\user\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\user\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])
  File ""C:\Users\user\Anaconda3\envs\tensorflow\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\user\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\user\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\user\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow_internal
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'
```"
24184,[ppc64le] Building tf 1.8 from source fails on power9 machine,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
LSB Version:	:core-4.1-noarch:core-4.1-ppc64le
Distributor ID:	RedHatEnterpriseServer
Description:	Red Hat Enterprise Linux Server release 7.5 (Maipo)
Release:	7.5
Codename:	Maipo

- TensorFlow installed from (source or binary): Source
- TensorFlow version: 1.8
- Python version: 3.6.5
- Installed using virtualenv? pip? conda?: conda 5.2
- Bazel version (if compiling from source): 0.11.1
- GCC/Compiler version (if compiling from source): 4.8.5
- CUDA/cuDNN version: 9.2.148/ 7.1.4
- GPU model and memory: Nvidia Tesla V100



**Describe the problem**
Building tensorflow 1.8 from source fails with the following error.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
CUDA_VER=9.2
CUDNN_VER=7.1.4
NCCL_VER=2.3.7-1
BAZEL_VER=0.11.1
TF_VER=1.8.0
CONDA_VER=5.2.0
PY_VER=3.6

#module load cuda/9.2.148
CUDA_DIR=$(pwd)/9.2.148
#install pre-built anaconda
wget https://repo.anaconda.com/archive/Anaconda3-$CONDA_VER-Linux-ppc64le.sh -O anaconda3.sh
bash ./anaconda3.sh -b -p anaconda3
export PATH=$(pwd)/anaconda3/bin:$PATH
pip install --upgrade pip
conda install -y keras-applications --no-deps
conda install -y keras-preprocessing --no-deps
#install bazel 
wget https://github.com/bazelbuild/bazel/releases/download/$BAZEL_VER/bazel-$BAZEL_VER-dist.zip
unzip bazel-$BAZEL_VER-dist.zip -d bazel
cd bazel
./compile.sh
export PATH=$(pwd)/output:$PATH
cd ../

#workaround for nccl2 
git clone https://github.com/NVIDIA/nccl
cd nccl
git checkout v$NCCL_VER
make -j160 src.build CUDA_HOME=$CUDA_DIR
make pkg.txz.build CUDA_HOME=$CUDA_DIR
tar -xf  build/pkg/txz/* -C ..
cd ..
ln -s nccl_$NCCL_VER* nccl2
cd nccl2
ln -s LICENSE.txt NCCL-SLA.txt
cd ..

CUDNN_DIR=$CUDA_DIR

#setup env var
export PYTHON_BIN_PATH=$(pwd)/anaconda3/bin/python3
export PYTHON_LIB_PATH=$(pwd)/anaconda3/lib/python$PY_VER/site-packages
export TF_NEED_MKL=0
export CC_OPT_FLAGS=""-march=native""
export TF_NEED_JEMALLOC=1
export TF_NEED_GCP=0
export TF_NEED_HDFS=0
export TF_ENABLE_XLA=0
export TF_NEED_OPENCL=0
export TF_NEED_CUDA=1
export TF_CUDA_CLANG=0
export TF_CUDA_VERSION=$CUDA_VER
export CUDA_TOOLKIT_PATH=$CUDA_DIR
export TF_CUDNN_VERSION=$CUDNN_VER
export CUDNN_INSTALL_PATH=$CUDNN_DIR
export TF_CUDA_COMPUTE_CAPABILITIES=""7.0""
export TF_NEED_VERBS=0
export TF_NEED_AWS=0
export TF_NEED_NGRAPH=0
export TF_NEED_S3=0
export TF_NEED_GDR=0
export TF_NEED_OPENCL_SYCL=0
export GCC_HOST_COMPILER_PATH=/usr/bin/gcc
export TF_NEED_MPI=0
export TF_NEED_KAFKA=0
export TF_NEED_ROCM=0
export TF_NEED_IGNITE=0
export TF_NEED_TENSORRT=0
export TF_SET_ANDROID_WORKSPACE=0
export TF_NCCL_VERSION=$(echo $NCCL_VER | cut -d. -f1,2)
export NCCL_INSTALL_PATH=""$(pwd)/nccl2""
export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:$CUDA_DIR/lib64/stubs

#build tensorflow
git clone --recursive https://github.com/tensorflow/tensorflow
cd tensorflow
git checkout v$TF_VER
wget https://github.com/tensorflow/tensorflow/commit/8f8a3c5.patch
patch -p1 third_party/png.BUILD 8f8a3c5.patch
#bug fix commit
#git cherry-pick 5aefa441
./configure
bazel build -c opt --config=cuda tensorflow/tools/pip_package:build_pip_package
bazel-bin/tensorflow/tools/pip_package/build_pip_package ../tensorflow_pkg
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
WARNING: /autofs/nccs-svm1_home1/shubhankar/.cache/bazel/_bazel_shubhankar/7ed9dc3ce0223066bb07a37deb908baa/external/protobuf_archive/WORKSPACE:1: Workspace name in /autofs/nccs-svm1_home1/shubhankar/.cache/bazel/_bazel_shubhankar/7ed9dc3ce0223066bb07a37deb908baa/external/protobuf_archive/WORKSPACE (@com_google_protobuf) does not match the name given in the repository's definition (@protobuf_archive); this will cause a build error in future versions
ERROR: /autofs/nccs-svm1_home1/shubhankar/summit/tensorflow/tensorflow/tools/pip_package/BUILD:117:1: no such package '@aws//': /autofs/nccs-svm1_home1/shubhankar/.cache/bazel/_bazel_shubhankar/7ed9dc3ce0223066bb07a37deb908baa/external/aws/.nfs000000000e4cb4e200000242 (Device or resource busy) and referenced by '//tensorflow/tools/pip_package:licenses'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@aws//': /autofs/nccs-svm1_home1/shubhankar/.cache/bazel/_bazel_shubhankar/7ed9dc3ce0223066bb07a37deb908baa/external/aws/.nfs000000000e4cb4e200000242 (Device or resource busy)
INFO: Elapsed time: 2.634s
FAILED: Build did NOT complete successfully (46 packages loaded)
```"
24181,tf.image (CLAHE) contrast limited adaptive histogram equalization.,"**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
`tf.image.clahe` would perform contrast limited adapative histogram equalization to enhance images natively on the tensorflow graph.

**Will this change the current api? How?**
No.

**Who will benefit with this feature?**
Those that want to run image processing natively on tensorflow ops.
"
24179,[Feature Request] Batch size scheduler to speed up training,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): Yes, but never contributed on TensorFlow code before.



**Describe the feature and the current behavior/state.**

I am looking for a batch size scheduler. Some recent papers like this one from Google (https://openreview.net/pdf?id=B1Yy1BxCZ) show that increasing batch size during training may have the same impact as decreasing the learning rate on validation accuracy. The advantage of increasing the batch size during training is that it requires less iterations per epoch, therefore reducing training time.

**Will this change the current api? How?**
Not sure.

**Who will benefit with this feature?**
Everyone in the TensorFlow community which are willing to train Neural Networks faster.

**Any Other info.**

**Reference paper:** *Don't Decay the Learning Rate, Increase the Batch Size 
Samuel L. Smith, Pieter-Jan Kindermans, Chris Ying, Quoc V. Le*
Link: https://openreview.net/pdf?id=B1Yy1BxCZ "
24176,"HELP!Compile tensorflow source code WITH cuda10.0(2080ti),CUDNN 7.4.1,bazel0.15 anaconda2","ERROR: /home/xhs/XHS/tensorflow-master/tensorflow/BUILD:573:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1)
Traceback (most recent call last):
  File ""/home/xhs/.cache/bazel/_bazel_xhs/1843eb251a57c74a19c1281e427cacbd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 27, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/home/xhs/.cache/bazel/_bazel_xhs/1843eb251a57c74a19c1281e427cacbd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 81, in <module>
    from tensorflow.python import keras
  File ""/home/xhs/.cache/bazel/_bazel_xhs/1843eb251a57c74a19c1281e427cacbd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/keras/__init__.py"", line 29, in <module>
    from tensorflow.python.keras import datasets
  File ""/home/xhs/.cache/bazel/_bazel_xhs/1843eb251a57c74a19c1281e427cacbd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/keras/datasets/__init__.py"", line 25, in <module>
    from tensorflow.python.keras.datasets import imdb
  File ""/home/xhs/.cache/bazel/_bazel_xhs/1843eb251a57c74a19c1281e427cacbd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/keras/datasets/imdb.py"", line 25, in <module>
    from tensorflow.python.keras.preprocessing.sequence import _remove_long_seq
  File ""/home/xhs/.cache/bazel/_bazel_xhs/1843eb251a57c74a19c1281e427cacbd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/keras/preprocessing/__init__.py"", line 21, in <module>
    import keras_preprocessing
ImportError: No module named keras_preprocessing
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 6674.103s, Critical Path: 2867.88s
INFO: 10631 processes: 10631 local.
FAILED: Build did NOT complete successfully



This is the part I chose to get the error. Please help me to see how it can be compiled."
24174,bazel 0.15.0 cuda10,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24172,Importing tensorflow changes CPU affinity to single core on main thread ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS 6.5
- TensorFlow installed from (source or binary): Anaconda3
- TensorFlow version (use command below): 1.10.0 (1.10.0-mkl_py36hdb377fd_0)
- Python version: 3.6.6

**Describe the current behavior**
Importing tensorflow causes Cpus allowed list to single core on main thread. That is, when I import tensorflow (even not using it). It will break down multi-thread computing of numpy and gensim (only using single core for matrix multiplication or learning model)

**Describe the expected behavior**
Importing tensorflow should not influence the behavior of other toolkits.

**Code to reproduce the issue**
1. Install tensorflow using anaconda and run ipython:
    ```bash
    conda create -n test tensorflow==1.10.0 gensim ipython
    conda activate test
    ipython
    ```

2. Run the following code in ipython:
    ```python
    import tensorflow
    import numpy as np
    from gensim.models import Word2Vec
    ```

3. Check Cpus allowed list for main thread(in another bash):
    ```bash
    cat /proc/<pid>/status | grep Cpus_allowed_list
    ```
    Output (may vary):
    ```bash
    Cpus_allowed_list:      55-72,108-125
    ```

4. Perform matrix multiplication in ipython:
    ```python
    A = np.random.rand(10000, 10000)
    A = A.dot(A.T)
    del A
    ```

5. Check again Cpus allowed list for main thread:
    ```bash
    cat /proc/<pid>/status | grep Cpus_allowed_list
    ```
    
    Output (may vary):
    
    ```bash
    Cpus_allowed_list:      108
    ```
    
    It become single core! **Note if we do not import tensorflow in step 2. Then the output here will be the same as step 3**. Actually the computing in step 4 can be paralleled. But as we will see in step 6, the other toolkit such as gensim will not work using multicore.

6. Run the following code (make sure you have already run the ipython code above):
    ```python
    cpus = 32 # change it depending on your machine setting
    B = np.random.randint(1,100, size=(5000,5000)).astype('str').tolist()
    model = Word2Vec(B, workers=cpus)
    del model
    ```
    
    You can check in top or htop that ipython will only use one core in executing the third line. **However, if we do not import tensorflow, then it will use 32 cpus and all thing will work fine.**


**Other info**
I don't know whether it is a tensorflow issue or numpy issue (since if we exchange S4 and S6, then it will work fine) or just MKL issue (since if we do not use mkl, but use tensorflow-gpu for numpy without mkl, then it cannot doing parallel matrix computing but gensim will work fine). But I considering that it is importing tensorflow that causes the issue, so I hope someone can give me some idea about this problem.
"
24171,bazel build tensorflow/python/tools:freeze_graph error,"ERROR: /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/grpc/BUILD:1515:1: C++ compilation of rule '@grpc//:grpc_resolver_dns_ares' failed (Exit 1) gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 62 argument(s) skipped)

Use --sandbox_debug to see verbose messages from the sandbox
external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc:56:30: error: field 'dns_server_addr' has incomplete type 'ares_addr_port_node'
   struct ares_addr_port_node dns_server_addr;
                              ^
external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc:56:10: note: forward declaration of 'struct ares_addr_port_node'
   struct ares_addr_port_node dns_server_addr;
          ^
external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc: In function 'void on_txt_done_locked(void*, int, int, unsigned char*, int)':
external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc:313:53: error: 'ares_parse_txt_reply_ext' was not declared in this scope
   status = ares_parse_txt_reply_ext(buf, len, &reply);
                                                     ^
external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc:316:58: error: invalid use of incomplete type 'struct on_txt_done_locked(void*, int, int, unsigned char*, int)::ares_txt_ext'
   for (result = reply; result != nullptr; result = result->next) {
                                                          ^
external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc:309:10: note: forward declaration of 'struct on_txt_done_locked(void*, int, int, unsigned char*, int)::ares_txt_ext'
   struct ares_txt_ext* result = nullptr;
          ^
external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc:317:15: error: invalid use of incomplete type 'struct on_txt_done_locked(void*, int, int, unsigned char*, int)::ares_txt_ext'
     if (result->record_start &&
               ^
external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc:309:10: note: forward declaration of 'struct on_txt_done_locked(void*, int, int, unsigned char*, int)::ares_txt_ext'
   struct ares_txt_ext* result = nullptr;
          ^
external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc:318:22: error: invalid use of incomplete type 'struct on_txt_done_locked(void*, int, int, unsigned char*, int)::ares_txt_ext'
         memcmp(result->txt, g_service_config_attribute_prefix, prefix_len) ==
                      ^
external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc:309:10: note: forward declaration of 'struct on_txt_done_locked(void*, int, int, unsigned char*, int)::ares_txt_ext'
   struct ares_txt_ext* result = nullptr;
          ^
external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc:325:39: error: invalid use of incomplete type 'struct on_txt_done_locked(void*, int, int, unsigned char*, int)::ares_txt_ext'
     size_t service_config_len = result->length - prefix_len;
                                       ^
external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc:309:10: note: forward declaration of 'struct on_txt_done_locked(void*, int, int, unsigned char*, int)::ares_txt_ext'
   struct ares_txt_ext* result = nullptr;
          ^
external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc:328:47: error: invalid use of incomplete type 'struct on_txt_done_locked(void*, int, int, unsigned char*, int)::ares_txt_ext'
     memcpy(*r->service_config_json_out, result->txt + prefix_len,
                                               ^
external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc:309:10: note: forward declaration of 'struct on_txt_done_locked(void*, int, int, unsigned char*, int)::ares_txt_ext'
   struct ares_txt_ext* result = nullptr;
          ^
external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc:330:25: error: invalid use of incomplete type 'struct on_txt_done_locked(void*, int, int, unsigned char*, int)::ares_txt_ext'
     for (result = result->next; result != nullptr && !result->record_start;
                         ^
external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc:309:10: note: forward declaration of 'struct on_txt_done_locked(void*, int, int, unsigned char*, int)::ares_txt_ext'
   struct ares_txt_ext* result = nullptr;
          ^
external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc:330:61: error: invalid use of incomplete type 'struct on_txt_done_locked(void*, int, int, unsigned char*, int)::ares_txt_ext'
     for (result = result->next; result != nullptr && !result->record_start;
                                                             ^
external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc:309:10: note: forward declaration of 'struct on_txt_done_locked(void*, int, int, unsigned char*, int)::ares_txt_ext'
   struct ares_txt_ext* result = nullptr;
          ^
external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc:331:25: error: invalid use of incomplete type 'struct on_txt_done_locked(void*, int, int, unsigned char*, int)::ares_txt_ext'
          result = result->next) {
                         ^
external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc:309:10: note: forward declaration of 'struct on_txt_done_locked(void*, int, int, unsigned char*, int)::ares_txt_ext'
   struct ares_txt_ext* result = nullptr;
          ^
external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc:334:50: error: invalid use of incomplete type 'struct on_txt_done_locked(void*, int, int, unsigned char*, int)::ares_txt_ext'
                       service_config_len + result->length + 1));
                                                  ^
external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc:309:10: note: forward declaration of 'struct on_txt_done_locked(void*, int, int, unsigned char*, int)::ares_txt_ext'
   struct ares_txt_ext* result = nullptr;
          ^
external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc:335:70: error: invalid use of incomplete type 'struct on_txt_done_locked(void*, int, int, unsigned char*, int)::ares_txt_ext'
       memcpy(*r->service_config_json_out + service_config_len, result->txt,
                                                                      ^
external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc:309:10: note: forward declaration of 'struct on_txt_done_locked(void*, int, int, unsigned char*, int)::ares_txt_ext'
   struct ares_txt_ext* result = nullptr;
          ^
external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc:336:20: error: invalid use of incomplete type 'struct on_txt_done_locked(void*, int, int, unsigned char*, int)::ares_txt_ext'
              result->length);
                    ^
external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc:309:10: note: forward declaration of 'struct on_txt_done_locked(void*, int, int, unsigned char*, int)::ares_txt_ext'
   struct ares_txt_ext* result = nullptr;
          ^
external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc:337:35: error: invalid use of incomplete type 'struct on_txt_done_locked(void*, int, int, unsigned char*, int)::ares_txt_ext'
       service_config_len += result->length;
                                   ^
external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc:309:10: note: forward declaration of 'struct on_txt_done_locked(void*, int, int, unsigned char*, int)::ares_txt_ext'
   struct ares_txt_ext* result = nullptr;
          ^
external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc: In function 'void grpc_dns_lookup_ares_continue_after_check_localhost_and_ip_literals_locked(grpc_ares_request*, const char*, const char*, const char*, grpc_pollset_set*, bool, grpc_combiner*)':
external/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc:420:70: error: 'ares_set_servers_ports' was not declared in this scope
     int status = ares_set_servers_ports(*channel, &r->dns_server_addr);
                                                                      ^
Target //tensorflow/python/tools:freeze_graph failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 827.444s, Critical Path: 131.84s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]
INFO: 2143 processes: 2142 linux-sandbox, 1 local.
FAILED: Build did NOT complete successfully
"
24170,'TensorBoard' object has no attribute 'sess',"

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac Os
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.12
- Python version:3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:N/A
- GPU model and memory:N/A


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
v1.12.0-rc2-3-ga6d8ffae09 1.12.0
**Describe the current behavior**
I want to use the tensorboard callback in keras to show the embeddings in the projector view
**Describe the expected behavior**
I'm getting an error while training.
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
    tensorboard = keras.callbacks.TensorBoard(
        log_dir='./logs',
        batch_size=128,
        histogram_freq=1,
        embeddings_freq=1,
        embeddings_metadata='./metadata.tsv',
        embeddings_data=X_Test
    )
    history = model.fit(ready_data, ready_labels, batch_size=128, epochs=20,validation_data=(X_Test,Y_test),callbacks=[tensorboard])
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
Traceback (most recent call last):
  File ""/Users/alcaljos/Projects/issue-classifer-ml/src/main.py"", line 193, in <module>
    history = model.fit(ready_data, ready_labels, batch_size=128, epochs=20,validation_data=(X_Test,Y_test),callbacks=[tensorboard,checkpointer])
  File ""/Users/alcaljos/anaconda/envs/issue-classifier/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1639, in fit
    validation_steps=validation_steps)
  File ""/Users/alcaljos/anaconda/envs/issue-classifier/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py"", line 239, in fit_loop
    callbacks.on_epoch_end(epoch, epoch_logs)
  File ""/Users/alcaljos/anaconda/envs/issue-classifier/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py"", line 214, in on_epoch_end
    callback.on_epoch_end(epoch, logs)
  File ""/Users/alcaljos/anaconda/envs/issue-classifier/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py"", line 1144, in on_epoch_end
    self.sess.run(self.assign_embeddings, feed_dict=feed_dict)
AttributeError: 'TensorBoard' object has no attribute 'sess'
```
"
24168,What version of Tensorflow  to install with CUDA 9.2 and libcudnn 6.0 ?,"Im upgrading from tensorflow-gpu 1.4 to a recent version but when i try to run tensorflow-gpu 1.12.0 on a cluster which contains Cuda 9.2, i get this error : 

`ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory`

I've tried even tensorflow-gpu 1.10 but it keeps the same error , so i would like to know which version can i install so it can run perfectly with Cuda 9.2 

Thank you"
24167,Import Error,"Using TensorFlow backend.
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>()
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

~\AppData\Local\Continuum\anaconda3\lib\imp.py in load_module(name, file, filename, details)
    242         else:
--> 243             return load_dynamic(name, filename, file)
    244     elif type_ == PKG_DIRECTORY:

~\AppData\Local\Continuum\anaconda3\lib\imp.py in load_dynamic(name, path, file)
    342             name=name, loader=loader, origin=path)
--> 343         return _load(spec)
    344 

ImportError: Module use of python36.dll conflicts with this version of Python.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-7-3222c6a3cf4e> in <module>()
      7 from sklearn.preprocessing import LabelEncoder
      8 from sklearn.metrics import mean_squared_error
----> 9 from keras.wrappers.scikit_learn import KerasClassifier
     10 from sklearn.model_selection import KFold
     11 from sklearn.model_selection import cross_val_score

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\__init__.py in <module>()
      1 from __future__ import absolute_import
      2 
----> 3 from . import utils
      4 from . import activations
      5 from . import applications

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\utils\__init__.py in <module>()
      4 from . import data_utils
      5 from . import io_utils
----> 6 from . import conv_utils
      7 
      8 # Globally-importable utils.

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\utils\conv_utils.py in <module>()
      7 from six.moves import range
      8 import numpy as np
----> 9 from .. import backend as K
     10 
     11 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\backend\__init__.py in <module>()
     87 elif _BACKEND == 'tensorflow':
     88     sys.stderr.write('Using TensorFlow backend.\n')
---> 89     from .tensorflow_backend import *
     90 else:
     91     # Try and load external backend.

~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py in <module>()
      3 from __future__ import print_function
      4 
----> 5 import tensorflow as tf
      6 from tensorflow.python.framework import ops as tf_ops
      7 from tensorflow.python.training import moving_averages

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\__init__.py in <module>()
     22 
     23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     25 
     26 try:

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\__init__.py in <module>()
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 from tensorflow.python.tools import component_api_helper

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\Users\EURIPAB\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\EURIPAB\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\EURIPAB\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\EURIPAB\AppData\Local\Continuum\anaconda3\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\EURIPAB\AppData\Local\Continuum\anaconda3\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: Module use of python36.dll conflicts with this version of Python.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
24165,Initiate ClusterSpec.as_dict failed with only chief and ps,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux 4.4.95.x86_64 #1 SMP Tue Jun 5 16:07:34 CST 2018 x86_64 x86_64 x86_64 GNU/Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No
- TensorFlow installed from (source or binary):
 Binary by pip
- TensorFlow version (use command below):
1.11
- Python version:
2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
9.0.176
- GPU model and memory:
Tesla V100, 32G

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

When initiate the RunConfig in the following code with only chief and ps, a error happends.

INFO:tensorflow:TF_CONFIG environment variable: {u'environment': u'cloud', u'cluster': {u'ps': [u'10.50.219.105:2222'], u'chief': [u'10.50.219.11:2222'], u'worker': []}, u'task': {u'index': 0, u'type': u'ps'}}

```
    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8, allow_growth=True)
    config = tf.ConfigProto(gpu_options=gpu_options,
                            log_device_placement=True)
    run_config = tf.estimator.RunConfig(save_checkpoints_steps=FLAGS.train_step / 10,
                                        keep_checkpoint_max=FLAGS.train_epochs * 100,
                                        log_step_count_steps=int(FLAGS.train_step / 400),
                                        save_summary_steps=int(FLAGS.train_step / 10),
                                        session_config=config
                                        )
```


Error tracing,

```
Traceback (most recent call last):
File ""DCN/wide_dcn_onehot.py"", line 490, in
tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
File ""/app/python_venv/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
_sys.exit(main(argv))
File ""DCN/wide_dcn_onehot.py"", line 464, in main
model = build_estimator(FLAGS.model_dir)
File ""DCN/wide_dcn_onehot.py"", line 412, in build_estimator
save_summary_steps=int(FLAGS.train_step / 10)
File ""/app/python_venv/lib/python2.7/site-packages/tensorflow/python/estimator/run_config.py"", line 528, in __init__
self._init_distributed_setting_from_environment_var(tf_config)
File ""/app/python_venv/lib/python2.7/site-packages/tensorflow/python/estimator/run_config.py"", line 597, in _init_distributed_setting_from_environment_var
self._num_ps_replicas = _count_ps(self._cluster_spec)
File ""/app/python_venv/lib/python2.7/site-packages/tensorflow/python/estimator/run_config.py"", line 136, in _count_ps
return len(cluster_spec.as_dict().get(TaskType.PS, []))
File ""/app/python_venv/lib/python2.7/site-packages/tensorflow/python/training/server_lib.py"", line 335, in as_dict
if max(task_indices) + 1 == len(task_indices):
ValueError: max() arg is an empty sequence
```

**Describe the expected behavior**

Initiate success.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24163,django tensorflow ,"django version:2.1.1
python version:3.5.2
tensorflow-gpu version:1.9.0
keras version:2.2.4
anaconda:4.5.11
I have a tensorflow project, I ran it with pycharm and it worked. then I want to run it in django ,but there have some import problem. through set source path or  append os.path, it didn't work.
the frame of the project:
tmp/
---ocr_django/
-------ocr_django/
----------ocr/
-------------ctpn/
-------------demo/
-------------densenet/
-------------train/
-------------__init__.py
-------------demo.py
-------------ocr.py
-------------recongnize.py
----------__init__.py      
----------original.py
----------setting.py
----------test.py
----------urls.py
----------view.py
----------wsgi.py

- the problem as follow:
Traceback (most recent call last):
  File ""/home/cq/anaconda3/lib/python3.5/site-packages/django/utils/autoreload.py"", line 225, in wrapper
    fn(*args, **kwargs)
  File ""/home/cq/anaconda3/lib/python3.5/site-packages/django/core/management/commands/runserver.py"", line 117, in inner_run
    self.check(display_num_errors=True)
  File ""/home/cq/anaconda3/lib/python3.5/site-packages/django/core/management/base.py"", line 379, in check
    include_deployment_checks=include_deployment_checks,
  File ""/home/cq/anaconda3/lib/python3.5/site-packages/django/core/management/base.py"", line 366, in _run_checks
    return checks.run_checks(**kwargs)
  File ""/home/cq/anaconda3/lib/python3.5/site-packages/django/core/checks/registry.py"", line 71, in run_checks
    new_errors = check(app_configs=app_configs)
  File ""/home/cq/anaconda3/lib/python3.5/site-packages/django/core/checks/urls.py"", line 13, in check_url_config
    return check_resolver(resolver)
  File ""/home/cq/anaconda3/lib/python3.5/site-packages/django/core/checks/urls.py"", line 23, in check_resolver
    return check_method()
  File ""/home/cq/anaconda3/lib/python3.5/site-packages/django/urls/resolvers.py"", line 396, in check
    for pattern in self.url_patterns:
  File ""/home/cq/anaconda3/lib/python3.5/site-packages/django/utils/functional.py"", line 37, in __get__
    res = instance.__dict__[self.name] = self.func(instance)
  File ""/home/cq/anaconda3/lib/python3.5/site-packages/django/urls/resolvers.py"", line 533, in url_patterns
    patterns = getattr(self.urlconf_module, ""urlpatterns"", self.urlconf_module)
  File ""/home/cq/anaconda3/lib/python3.5/site-packages/django/utils/functional.py"", line 37, in __get__
    res = instance.__dict__[self.name] = self.func(instance)
  File ""/home/cq/anaconda3/lib/python3.5/site-packages/django/urls/resolvers.py"", line 526, in urlconf_module
    return import_module(self.urlconf_name)
  File ""/home/cq/anaconda3/lib/python3.5/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 673, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 665, in exec_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
  File ""/home/cq//tmp/ocr_django/ocr_django/urls.py"", line 18, in <module>
    from . import view
  File ""/home/cq//tmp/ocr_django/ocr_django/view.py"", line 22, in <module>
    from .ocr.recognize import recognize
  File ""/home/cq//tmp/ocr_django/ocr_django/ocr/recognize.py"", line 7, in <module>
    from .ctpn.text_detect import text_detect
  File ""/home/cq//tmp/ocr_django/ocr_django/ocr/ctpn/text_detect.py"", line 7, in <module>
    from .lib.utils.timer import Timer
  File ""/home/cq//tmp/ocr_django/ocr_django/ocr/ctpn/lib/__init__.py"", line 4, in <module>
    from ocr_django.ocr.ctpn.lib import fast_rcnn
  File ""/home/cq//tmp/ocr_django/ocr_django/ocr/ctpn/lib/fast_rcnn/__init__.py"", line 5, in <module>
    from . import train
  File ""/home/cq//tmp/ocr_django/ocr_django/ocr/ctpn/lib/fast_rcnn/train.py"", line 5, in <module>
    from ..roi_data_layer.layer import RoIDataLayer
  File ""/home/cq//tmp/ocr_django/ocr_django/ocr/ctpn/lib/roi_data_layer/__init__.py"", line 4, in <module>
    from . import roidb
  File ""/home/cq//tmp/ocr_django/ocr_django/ocr/ctpn/lib/roi_data_layer/roidb.py"", line 5, in <module>
    from ocr_django.ocr.ctpn.lib.utils.bbox import bbox_overlaps
  File ""/home/cq//tmp/ocr_django/ocr_django/ocr/ctpn/lib/utils/__init__.py"", line 11, in <module>
    from ocr_django.ocr.ctpn.lib.utils import bbox
ImportError: cannot import name 'bbox'

there are three files named:""bbox.c"", ""bbox.pyx"",""bbox.cpython-36m-x86_64-linux-gnu.so"" in the package of ocr/ctpn/lib/utils. I've found a lot of solutions online, but none of them succeeded.
the solutions include:
1.right-click the project name in pycharm - > Mark Directory as - > source root to set the package as sources root
2.file - >setting - >project structure to set the package as source
3.change the code ""from . import bbox"" in /home/cq//tmp/ocr_django/ocr_django/ocr/ctpn/lib/utils/__init__.py to ""from ocr_django.ocr.ctpn.lib.utils import bbox""

      
"
24162,bazel build issue ('tensorflow::DeviceBase::name': must return a value),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.10
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.19.1
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version: 9

**Describe the problem**
> c:\users\adm\_bazel_adm\atj3lgjk\execroot\org_tensorflow\tensorflow\core\framework\device_base.cc(34) : error C4716: 'tensorflow::DeviceBase::name': must return a value
> c:\users\adm\_bazel_adm\atj3lgjk\execroot\org_tensorflow\tensorflow\core\framework\device_base.cc(30) : error C4716: 'tensorflow::DeviceBase::attributes': must return a value
> ERROR: I/O error while writing action log: null during journal append
> Target //tensorflow/tools/graph_transforms:transform_graph failed to build





**Provide the exact sequence of commands / steps that you executed before running into the problem**
 

- bazel build tensorflow/tools/graph_transforms:transform_graph

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24161, tf.image.resize_bilinear ops have not support,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source):1.12


i want to convert deeplabv3  mobilenet to tflite but it do not support?
when will you add the 'tf.image.resize_bilinear' ops in tensorflow lite?thank you."
24159,dynamic_rnn claims it supports nested tuples but actually doesn't,"**System information**
- TensorFlow version:  1.12.0
- Doc Link: https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn

The documentation of dynamic_rnn claims it can accept **nested tuples**. I quote:
> inputs: The RNN inputs. If time_major == False (default), this must be a Tensor of shape: [batch_size, max_time, ...], or a nested tuple of such elements. If time_major == True, this must be a Tensor of shape: [max_time, batch_size, ...], **_or a nested tuple of such elements_**. This may also be a (possibly nested) tuple of Tensors satisfying this property ...

So I write a simple demo to test the feature
```python
# encoding=utf-8-sig
import tensorflow as tf

batch_size = None

# nested tuple
x_placeholder = [tf.placeholder(tf.float32, [batch_size, 10, 20], 'input_1'),
                 [tf.placeholder(tf.float32, [batch_size, 10, 20], 'input_2'),
                 tf.placeholder(tf.float32, [batch_size, 10, 20], 'input_3')]]
sequence_size = tf.placeholder(tf.int32, [batch_size], 'sequence_size')
cell = tf.nn.rnn_cell.LSTMCell(15)
tf.nn.dynamic_rnn(cell=cell, inputs=x_placeholder, sequence_length=sequence_size, 
                  initial_state=None, time_major=True, dtype=tf.float32)
```

and the console raises a error:
> ValueError: Layer lstm_cell_1 expects 1 inputs, but it received 3 input tensors. Inputs received: [<tf.Tensor 'rnn/while/TensorArrayReadV3:0' shape=(10, 20) dtype=float32>, <tf.Tensor 'rnn/while/TensorArrayReadV3_1:0' shape=(10, 20) dtype=float32>, <tf.Tensor 'rnn/while/TensorArrayReadV3_2:0' shape=(10, 20) dtype=float32>]

and I find the place that raise the error
the line 826 of tensorflow/tensorflow/python/ops/rnn.py defines how to generate the new state, i.e.,
```python
call_cell = lambda: cell(input_t, state)
```
In the demo, the cell function is actually __call__() function of LSTMCell. And the documentation of LSTMCell claims the input must be a 2-D tensor. Nevertheless, if we feed a nested tuple to dynamic_rnn function, the input must be a 3-D tensor, so the LSTM raise a error

So, my question is, is dynamic_rnn really support nested tuples ?"
24155,InfeedEnqueueTuple placed on TPU device blocks indefinitely,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): N/A
- TensorFlow version (use command below): Cloud nightly
- Python version: N/A
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: Cloud TPUv3

**Describe the current behavior**

There are two documented ways to use the TPU infeed ops:
1. Place them on the CPU device and set device_ordinal to the requisite device
2. Place them on the TPU device and leave device_ordinal empty

The first way works fine, the second way does transfer the data to the device, but then blocks, never returning control to the client. Of note, this deadlock only happens if there is currently an op running on the TPU device (via XRTExecute, but potentially others).

**Describe the expected behavior**

Both methods should work."
24152,Measuring Tensor Flow Input Pipeline Shuffle,"Hello,

I am trying to measure the time spent executing the shuffle operation of a data set. I an using tf.data.Dataset.shuffle() API along with the tf.data.Iterator.from_structure() API to create an iterator. I have tried editing and compiling the shuffle_dataset_op.cc to print out the time statement of when the operation is being performed however I do believe this is the correct time measurement of just the shuffle operation. 

I am currently using Tensorflow 1.10 with Ubuntu 14.04.5 LTS and Cuda 9.0.

Any help or advice would be greatly appreciated. Thank you.


Sincerely,
Abenezer Wudenhe"
24150,Tensorflow Error in Jupyter Notebooks,"I receive the errors below when importing tensorflow in a Jupyter Notebook.

---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
~\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

~\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>()
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

~\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

~\Anaconda3\lib\imp.py in load_module(name, file, filename, details)
    242         else:
--> 243             return load_dynamic(name, filename, file)
    244     elif type_ == PKG_DIRECTORY:

~\Anaconda3\lib\imp.py in load_dynamic(name, path, file)
    342             name=name, loader=loader, origin=path)
--> 343         return _load(spec)
    344 

ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-5-64156d691fe5> in <module>()
----> 1 import tensorflow as tf

~\Anaconda3\lib\site-packages\tensorflow\__init__.py in <module>()
     22 
     23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     25 
     26 try:

~\Anaconda3\lib\site-packages\tensorflow\python\__init__.py in <module>()
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 from tensorflow.python.tools import component_api_helper

~\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\Users\ashbat\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\ashbat\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\ashbat\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\ashbat\Anaconda3\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\ashbat\Anaconda3\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found."
24148,[ppc64le] //tensorflow/lite/experimental/micro unit test fail.,"You can assign this issue to me, as I'm about ready to submit a PR to fix this.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux ppc64le Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): commit 92854f2426c2b91f2b09831696ffebb5b793933d from Dec 4th, 2018
- Python version: 2.7
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source): gcc version 5.4.0 20160609 (Ubuntu/IBM 5.4.0-6ubuntu1~16.04.10)
- CUDA/cuDNN version: N/A 
- GPU model and memory: N/A


**Describe the current behavior**
All tensorflow/lite/experimental/micro unit test fail
See: https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Build_and_Test/31/testReport/

**Describe the expected behavior**
All unit test pass

**Code to reproduce the issue**
Unit test invoked by Jenkins:
`./tensorflow/tools/ci_build/ci_build.sh cpu --dockerfile tensorflow/tools/ci_build/Dockerfile.cpu.ppc64le ./tensorflow/tools/ci_build/linux/ppc64le/cpu/run_py2.sh`

`./tensorflow/tools/ci_build/linux/ppc64le/cpu/run_py2.sh` can be modified to just run `//tensorflow/lite/experimental/micro/...`

Also the getting started section here recreates two of the issues:
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro



**Other info / logs**
Running the getting started example: make -f tensorflow/lite/experimental/micro/tools/make/Makefile test_micro_speech

This warning below is flagged and run the test runs it seg faults. As I understand the warning is because your not allowed to pass a string constant in this case. (It works however on x86)

```
g++ -O3 -DNDEBUG --std=c++11 -g -DTF_LITE_STATIC_MEMORY -I. -Itensorflow/lite/experimental/micro/tools/make/../../../../../ -Itensorflow/lite/experimental/micro/tools/make/../../../../../../ -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_speech_test.cc -o tensorflow/lite/experimental/micro/tools/make/gen/linux_ppc64le/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_speech_test.o

In file included from tensorflow/lite/experimental/micro/examples/micro_speech/micro_speech_test.cc:22:0:
tensorflow/lite/experimental/micro/examples/micro_speech/micro_speech_test.cc: In function 'int main(int, char**)':
./tensorflow/lite/experimental/micro/testing/micro_test.h:95:51: warning: ISO C++ forbids converting a string constant to 'va_list {aka char*}' [-Wwrite-strings]
   micro_test::reporter->Report(""Testing %s"", #name);                       \
                                                   ^
tensorflow/lite/experimental/micro/examples/micro_speech/micro_speech_test.cc:28:1: note: in expansion of macro 'TF_LITE_MICRO_TEST'
 TF_LITE_MICRO_TEST(TestInvoke) {
 ^~~~~~~~~~~~~~~~~~
```


Fixing it and running the example: make -f tensorflow/lite/experimental/micro/tools/make/Makefile test identifies the same problem in another place:

```
g++ -O3 -DNDEBUG --std=c++11 -g -DTF_LITE_STATIC_MEMORY -I. -Itensorflow/lite/experimental/micro/tools/make/../../../../../ -Itensorflow/lite/experimental/micro/tools/make/../../../../../../ -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/experimental/micro/micro_error_reporter_test.cc -o tensorflow/lite/experimental/micro/tools/make/gen/linux_ppc64le/obj/tensorflow/lite/experimental/micro/micro_error_reporter_test.o
tensorflow/lite/experimental/micro/micro_error_reporter_test.cc: In function 'int main(int, char**)':
tensorflow/lite/experimental/micro/micro_error_reporter_test.cc:24:56: warning: ISO C++ forbids converting a string constant to 'va_list {aka char*}' [-Wwrite-strings]
   error_reporter->Report(""~~~%s~~~"", ""ALL TESTS PASSED"");
                                                        ^
```

Fixing that allows the make test to complete, but when running inside of bazel some test still fail because they fail to compile:

```
ERROR: ^[[0m/workspace/tensorflow/lite/experimental/micro/examples/micro_speech/BUILD:185:1: Couldn't build file tensorflow/lite/experimental/micro/examples/micro_speech/feature_provider_test_binary: Linking of rule '//tensorflow/lite/experimental/micro/examples/micro_speech:feature_provider_test_binary' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command
  (cd /mnt/pai/home/wdirons/tmp/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda-9.2 \
    CUDNN_INSTALL_PATH=/usr/local/cuda-9.2 \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \
    OMP_NUM_THREADS=1 \
    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python2 \
    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=3.7 \
    TF_CUDA_VERSION=9.2 \
    TF_CUDNN_VERSION=7 \
    TF_NCCL_VERSION='' \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL_SYCL=0 \
    TF_NEED_ROCM=0 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/ppc-opt/bin/tensorflow/lite/experimental/micro/examples/micro_speech/feature_provider_test_binary -Wl,-no-as-needed -pie -Wl,-z,relro,-z,now '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -no-canonical-prefixes -fno-canonical-system-headers -B/usr/bin -Wl,--gc-sections -Wl,@bazel-out/ppc-opt/bin/tensorflow/lite/experimental/micro/examples/micro_speech/feature_provider_test_binary-2.params)
/usr/bin/ld: bazel-out/ppc-opt/bin/tensorflow/lite/experimental/micro/examples/micro_speech/libpreprocessor_reference.a(preprocessor.o): undefined reference to symbol 'cos@@GLIBC_2.17'
//lib/powerpc64le-linux-gnu/libm.so.6: error adding symbols: DSO missing from command line

```
Passing the link option ""-lm"" resolves this error. The makefile already does this. 


"
24147,Description mismatch,"In the Repository Description, Tensorflow is Framework.
But in the README, Tensorflow is Library.
Which one is right?

"
24146,No gradient defined for operation 'MatrixLogarithm',"I would like to optimize a function containing tf.linalg.logm. However,

LookupError: gradient registry has no entry for: MatrixLogarithm"
24145,How to build example - tfjs-examples,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **MS Windows 7**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): **npm**
- TensorFlow version: **tfjs-node@0.1.21**
- Python version:**2.7**
- Node version: **8.9.4**
- Installed using virtualenv? pip? conda?: **pip**
- Bazel version (if compiling from source):NA
- GCC/Compiler version (if compiling from source):NA
- CUDA/cuDNN version:NA
- GPU model and memory: Intel



**Describe the problem**
I am following steps as described [here](https://github.com/tensorflow/tfjs-examples) to install tfjs-examples

**Provide the exact sequence of commands / steps that you executed before running into the problem**
`npm i @tensorflow/tfjs-node -g` - ok
`git clone https://github.com/tensorflow/tfjs-examples.git` -ok
`cd tfjs-example` - ok
`cd minst-code` - ok
`npm install` - error

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
31428 error code ELIFECYCLE
31429 error errno 1
31430 error tfjs-examples-mnist-core@0.1.0 postinstall: `yarn upgrade --pattern @tensorflow`
31430 error Exit status 1
31431 error Failed at the tfjs-examples-mnist-core@0.1.0 postinstall script.
31431 error This is probably not a problem with npm. There is likely additional logging output above.
31432 verbose exit [ 1, true ]
```
Q: do i have to have `yarn` installed ? 
On [this page](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md) it says either `yarn/npm` - unless i misunderstood.
Please advice.
Thanks."
24144,Build issue while building tensorflow on raspberry pi 3B+,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspberry pi 3B+, Raspbian GNU/Linux 9.4 

- TensorFlow installed from (source or binary): tried installing from source as mentioned in [link](https://www.tensorflow.org/install/source_rpi) and using pip as mentioned in [link](https://www.tensorflow.org/install/pip)

- TensorFlow version: r1.9
- Python version: 2.7.13
- Installed using virtualenv? pip? conda?: source and pip


**Describe the problem**
I tried to install tensorflow r1.9 on raspberry pi using the commands given in the [link](https://www.tensorflow.org/install/source_rpi) but its failing to build at step 6/14 and could not install ffmpeg. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
git checkout r1.9
tensorflow/tools/ci_build/ci_build.sh PI \
    tensorflow/tools/ci_build/pi/build_raspberry_pi.sh
```

**Any other info / logs**
```Package ffmpeg is not available, but is referred to by another package.
This may mean that the package is missing, has been obsoleted, or
is only available from another source

E: Package 'ffmpeg' has no installation candidate
The command '/bin/sh -c /install/install_deb_packages.sh' returned a non-zero code: 100
ERROR: docker build failed. Dockerfile is at /home/pi/tensorflow/tensorflow/tools/ci_build/Dockerfile.pi
```"
24143,I am getting Thread 1: signal SIGABRT while installing the camera example tensorflowlite ,I am getting Thread 1: signal SIGABRT while installing the camera example tensorflowlite app. Kindly let me know can we able to run this in simulator.
24141,ERROR: /root/tensorflow/tensorflow/lite/BUILD:202:1: in cc_library rule //tensorflow/lite:string_util: cycle in dependency graph,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: master
- Python version: 3.6
- Installed using virtualenv? pip? conda?:no
- Bazel version (if compiling from source):0.15
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:none
- GPU model and memory:



**Describe the problem**
when trying to build toco with with_select_tf_ops=true, error happens.

bazel build -c opt  --define=with_select_tf_ops=true //tensorflow/lite/toco:toco 

ERROR: /root/tensorflow/tensorflow/lite/BUILD:202:1: in cc_library rule //tensorflow/lite:string_util: cycle in dependency graph
.......
ERROR: /root/tensorflow/tensorflow/lite/BUILD:202:1: in cc_library rule //tensorflow/lite:string_util: cycle in dependency graph:
    //tensorflow/lite/toco:toco
    //tensorflow/lite/toco:toco_tooling
    //tensorflow/lite/toco/tflite:export
    //tensorflow/lite/toco/tflite:operator
    //tensorflow/lite/toco/tflite:types
.-> //tensorflow/lite:string_util
|   //tensorflow/lite:framework
|   //tensorflow/lite/delegates/flex:delegate
|   //tensorflow/lite/delegates/flex:delegate_only_runtime
|   //tensorflow/lite/delegates/flex:delegate_data
|   //tensorflow/lite/delegates/flex:buffer_map
`-- //tensorflow/lite:string_util
This cycle occurred because of a configuration option
ERROR: Analysis of target '//tensorflow/lite/toco:toco' failed; build aborted

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24140,tf.contrib.factorization.python.ops.factorization_ops.WALSModel greedy parallelization,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04 and macOS 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12+
- Python version: 3.5
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: 9.2
- GPU model and memory: Titan


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

when trying to train a WALSModel in parallel (e.g. with n different parameter sets or any other tf model) using the python multiprocessing module (or in two different scripts), CUDA errors occur. It seems that WALSModel is written to automatically parallelize itself and in a greedy manner...

looking at the __init__, I can not see a way to specify to limit this...
```
def __init__(self,
               input_rows,
               input_cols,
               n_components,
               unobserved_weight=0.1,
               regularization=None,
               row_init=""random"",
               col_init=""random"",
               num_row_shards=1,
               num_col_shards=1,
               row_weights=1,
               col_weights=1,
               use_factors_weights_cache=True,
               use_gramian_cache=True,
               use_scoped_vars=False):
```

**Describe the expected behavior**



**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24139,encrypt decrypt tf lite model in android,"I found an issue https://github.com/tensorflow/tensorflow/issues/21501. When i decrypt my model to byte[],  use wrap() method to convert byte[] to HeapByteBuffer, init the Interpreter. The NativeInterpreterWrapper will throw an exception. Is there a way to solve this problem

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/src/main/java/org/tensorflow/lite/NativeInterpreterWrapper.java
` NativeInterpreterWrapper(ByteBuffer buffer, Interpreter.Options options) {
    if (buffer == null
        || (!(buffer instanceof MappedByteBuffer)
            && (!buffer.isDirect() || buffer.order() != ByteOrder.nativeOrder()))) {
      throw new IllegalArgumentException(
          ""Model ByteBuffer should be either a MappedByteBuffer of the model file, or a direct ""
              + ""ByteBuffer using ByteOrder.nativeOrder() which contains bytes of model content."");
    }
    this.modelByteBuffer = buffer;
    long errorHandle = createErrorReporter(ERROR_BUFFER_SIZE);
    long modelHandle = createModelWithBuffer(modelByteBuffer, errorHandle);
    init(errorHandle, modelHandle, options);
  }`"
24138,How can I convert custom pb file to tflite?,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:-
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):1.15.0
- Python version:2.7.12
- Bazel version (if compiling from source):0.19.2
- GCC/Compiler version (if compiling from source):-
- CUDA/cuDNN version:-
- GPU model and memory: no GPU / 8GB memory(VM)

I tried to convert frozen & custom .pb file(Specifically, SSD) to .tflite with toco tools.
but I get some error & core dump.(below)

*convert command & results is below:******************************************************************
$   (path)/.local/bin/toco 
--input_file=(path)/ssd1.pb   
--output_file=(path)/ssd2.tflite 
--input_format=TENSORFLOW_GRAPHDEF  
 --output_format=TFLITE 
--input_shape=1,300,300,3 
--input_array=input_13   
--output_array=predictions/concat 
--input_data_type=FLOAT   
--inference_type=FLOAT 
--allow_custom_ops
2018-12-04 11:39:52.856924: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: Prod
2018-12-04 11:39:52.857097: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: Prod
2018-12-04 11:39:52.857156: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: Prod
2018-12-04 11:39:52.857214: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: Prod
2018-12-04 11:39:52.857271: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: Prod
2018-12-04 11:39:52.857389: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: Prod
2018-12-04 11:39:52.857474: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: Prod
2018-12-04 11:39:52.857533: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: Prod
2018-12-04 11:39:52.857586: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: Prod
2018-12-04 11:39:52.857638: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: Prod
2018-12-04 11:39:52.857720: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: StridedSlice
2018-12-04 11:39:52.857784: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: StridedSlice
2018-12-04 11:39:52.857878: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: StridedSlice
2018-12-04 11:39:52.857976: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: StridedSlice
2018-12-04 11:39:52.858055: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: StridedSlice
2018-12-04 11:39:52.858156: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: StridedSlice
2018-12-04 11:39:52.858226: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: StridedSlice
2018-12-04 11:39:52.858287: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: StridedSlice
2018-12-04 11:39:52.858337: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: StridedSlice
2018-12-04 11:39:52.858382: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: Exp
2018-12-04 11:39:52.862297: I   tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39]   Before general graph transformations: 309 operators, 499 arrays (0 quantized)
2018-12-04 11:39:53.124941: F   tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:785]   Check failed: crops_data[1] == 0 (5 vs. 0)
Coredump
*************************************************************************************************************

*model summarize is below:(trained environmental is keras. and I converted .json & .h5 to .pb)**********
$   bazel-bin/tensorflow/tools/graph_transforms/summarize_graph   
 --in_graph=(path)/ssd1.pb

Found 1 possible inputs: (name=input_13,   type=float(1), shape=[?,300,300,3])
No variables spotted.
Found 1 possible outputs:   (name=predictions/concat, op=ConcatV2)
Found 25824093 (25.82M) const   parameters, 0 (0) variable parameters, and 0 control_edges
Op types used: 189 Const, 73 Identity,   33 BiasAdd, 31 Conv2D, 21 Relu, 19 Pack, 19 StridedSlice, 19 Shape, 13   Reshape, 10 Prod, 6 Tile, 6 ExpandDims, 5 MaxPool, 4 ConcatV2, 2 Sum, 2 Mul,   2 MatMul, 1 RealDiv, 1 Sub, 1 Square, 1 SpaceToBatchND, 1 Exp, 1 Rsqrt, 1   Placeholder, 1 Pad, 1 BatchToSpaceND, 1 Mean, 1 Maximum, 1 Max
To use with   tensorflow/tools/benchmark:benchmark_model try these arguments:
bazel run   tensorflow/tools/benchmark:benchmark_model --   
--graph=(path)/ssd1.pb --show_flops   --input_layer=input_13 
--input_layer_type=float   --input_layer_shape=-1,300,300,3 --output_layer=predictions/concat
**************************************************************************************************************

How can I convert pb file to tflite?
or Is Tensorflow lite unsupported this problem still now?
or process of convert pb file has something wrong?
    (because I feel that the number of output_layer is small...)"
24136,AttributeError: 'OneDeviceStrategy' object has no attribute 'call_for_each_tower',"I downloaded the up-to-date source code from this page and build from source. Then I tried to train mnist using the code in https://github.com/tensorflow/models/tree/master/official/mnist
But I got the following error message:
```

I1203 18:58:06.548315 140153645283136 run_config.py:530] Initializing RunConfig with distribution strategies.
I1203 18:58:06.548501 140153645283136 estimator_training.py:166] Not using Distribute Coordinator.
I1203 18:58:06.548817 140153645283136 estimator.py:197] Using config: {'_model_dir': '/tmp/mnist_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.contrib.distribute.python.one_device_strategy.OneDeviceStrategy object at 0x7f77577a59e8>, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f77577a5ac8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': None}
W1203 18:58:06.572286 140153645283136 deprecation.py:317] From /home/kathy/workspace/test/models/official/mnist/dataset.py:100: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
W1203 18:58:06.609055 140153645283136 deprecation.py:317] From /home/kathy/.local/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py:175: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
Traceback (most recent call last):
  File ""mnist.py"", line 236, in <module>
    absl_app.run(main)
  File ""/home/kathy/.local/lib/python3.6/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/home/kathy/.local/lib/python3.6/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""mnist.py"", line 230, in main
    run_mnist(flags.FLAGS)
  File ""mnist.py"", line 211, in run_mnist
    mnist_classifier.train(input_fn=train_input_fn, hooks=train_hooks)
  File ""/home/kathy/.local/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 354, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/kathy/.local/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1181, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/home/kathy/.local/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1287, in _train_model_distributed
    grouped_estimator_spec = self._train_distribution.call_for_each_tower(
AttributeError: 'OneDeviceStrategy' object has no attribute 'call_for_each_tower'

```

But if I install from tf-nightly from binary, it can be trained. Please help out!Thanks. "
24135,bazel build failed,"**System information**
- Linux Ubuntu 18.04:
- TensorFlow installed from source:
- TensorFlow version1.12:
- Python version3.6:
- Installed using virtualenv? pip? conda?:
- Bazel version 0.18.0:
- CUDA/cuDNN version10.0/7.4.1:
- GPU model and memory1080ti*4:
- location, China

**Describe the problem**
I tried using bazel to build customized tensorflow for the machine. Everything is fine except downloading this package ""icu"", here's the error message:
-tensorflow/tensorflow/tools/pip_package/BUILD:132:1: no such package '@icu//': java.io.IOException: thread interrupted and referenced by '//tensorflow/tools/pip_package:licenses'
- tensorflow/tensorflow/tools/pip_package/BUILD:132:1: no such package '@icu//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/unicode-org/icu/archive/release-62-1.tar.gz, https://github.com/unicode-org/icu/archive/release-62-1.tar.gz] to .cache/bazel/_bazel_cuimi/27ec4687dad404026b78b3694ecd223c/external/icu/release-62-1.tar.gz: Checksum was 349da7084000736bb1fea503f7a8cbac6326279b6c8b07578c589953adff5725 but wanted e15ffd84606323cbad5515bf9ecdf8061cc3bf80fb883b9e6aa162e485aa9761 and referenced by '//tensorflow/tools/pip_package:licenses'
I believe the reason is the network, so I wonder if anyone can give me this specifically built version of tensorflow package to me or some way to solve it.


**Provide the exact sequence of commands / steps that you executed before running into the problem**
cmd: bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

"
24134,optimizing for mac users to use AVX2 FMA,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac 10.13.6
- TensorFlow installed from (source or binary): 1.12.0 binary

**Describe the problem**

according to the docs, `Note: We already provide well-tested, pre-built TensorFlow packages for Linux and macOS systems.` , however, when running that version, I still see `our CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA`

I wanted to know if there were plans to optimize the mac build to take full advantage of the CPU?

**Provide the exact sequence of commands / steps that you executed before running into the problem**

`pip install https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.12.0-py3-none-any.whl`




**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24133,Allow for flexible boolean mask similar to numpy,"Having flexible boolean masks would be something of advantage for the whole community. At the moment of writing using TF version `1.12.0` in order to construct a boolean mask one has to predefine the mask and use it using a specific function `tf.boolean_mask`. Instead it would be much more productive to have similar functionality that is found in numpy. For instance:
```
x = tf.random.normal(10, 4)
x[x > 0.01]
```
similar to 
```
x = np.random.randn(10, 4)
x[x > 0.01]
```

**System information**
- TensorFlow version (you are using):
 `1.12.0`


**Describe the feature and the current behavior/state.**
At the moment there's not an equivalent way of easily creating and applying on the fly boolean masks, except maybe from the `tf.where`

**Will this change the current api? How?**
Yes, it allow for more flexible and productive api.

**Who will benefit with this feature?**
Community
"
24132,tf.estimator.Estimator.train fails with 3+ keras layer calls and MirroredStrategy distribution,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
linux ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
binary (pip install tensorflow-gpu==1.8)
- TensorFlow version (use command below):
1.8.0
- Python version:
2.7.12
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
9.0.176
- GPU model and memory:
GeForce GTX 1080 Ti (11gb memory)


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
When applying a keras layer 3 or more times using tf.contrib.distribute.MirroredStrategy within the run config, training with a tensorflow estimator hits an exception when applying the layer at graph build time:
```
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/_impl/keras/engine/base_layer.py"", line 314, in __call__
    output = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py"", line 671, in __call__
    with scope_context_manager as scope:
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py"", line 1962, in __enter__
    self._current_name_scope.__enter__()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 5974, in __enter__
    % (self._name, self._default_name))
ValueError: At least one of name (None) and default_name (None) must be provided.
```

additional observations:
- runs fine if the layer is applied 2 times (using MirroredStrategy)
- runs fine if the layer is applied 3 times (using OneDeviceStrategy)


**Describe the expected behavior**
`estimator.train` should succeed with multiple gpus and multiple applications of a keras layer.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
import numpy as np
import tensorflow as tf


class ConvLayer(tf.keras.layers.Layer):
    def call(self, image):
        shape = (3, 3, 3, 16)
        stddev = 1
        weights = tf.get_variable(
            name='weights',
            initializer=tf.truncated_normal(shape, stddev=stddev),
        )
        conv = tf.nn.conv2d(image, weights, [1, 1, 1, 1], 'SAME')
        return conv


def get_train_op(loss):
    global_step = tf.train.get_global_step()
    optimizer = tf.contrib.optimizer_v2.AdamOptimizer(.001)  # use distribution-aware optimizer
    train_op = optimizer.minimize(loss, global_step=global_step)
    return train_op


def model_fn(features, labels, mode):
    image = features['image']

    # if num_layers >= 3, graph construction will error if more than 1 gpu is used
    num_layers = 3
    layer = ConvLayer()
    values = []
    for _ in xrange(num_layers):
        values.append(layer(image))
    stacked_values = tf.concat(values, axis=0)

    loss = tf.reduce_sum(stacked_values)
    mode = tf.estimator.ModeKeys.TRAIN
    train_op = get_train_op(loss)
    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)


def input_fn():

    shape = (100, 100, 3)

    def gen():
        while True:
            image = np.ones(shape, dtype=np.float32)
            yield ({'image': image}, [])

    ds = tf.data.Dataset.from_generator(
        gen,
        ({'image': tf.float32}, tf.float32),
        output_shapes=({'image': tf.TensorShape(shape)}, tf.TensorShape(None)),
    )
    ds = ds.repeat().batch(4)
    return ds


# top-level call
def run():
    gpus = ['/device:GPU:0', '/device:GPU:1']
    # MirroredStrategy fails
    distribution = tf.contrib.distribute.MirroredStrategy(gpus)
    # OneDeviceStrategy works
    # distribution = tf.contrib.distribute.OneDeviceStrategy(gpus[0])

    config = tf.estimator.RunConfig(
        train_distribute=distribution,
        model_dir='/path/to/output'
    )
    estimator = tf.estimator.Estimator(
        model_fn=model_fn,
        config=config,
    )
    estimator.train(
        input_fn=input_fn,
        steps=10,
    )
```

calling `run()` after setting `num_layers` to 1 or 2 works, but fails if `num_layers >= 3`.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 465, in run
    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 831, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""scott_exp/multigpu.py"", line 31, in model_fn
    values.append(layer(image))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/_impl/keras/engine/base_layer.py"", line 314, in __call__
    output = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py"", line 671, in __call__
    with scope_context_manager as scope:
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py"", line 1962, in __enter__
    self._current_name_scope.__enter__()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 5974, in __enter__
    % (self._name, self._default_name))
ValueError: At least one of name (None) and default_name (None) must be provided.
```

full stacktrace: https://gist.github.com/dogbox/dcceda3d7b0c813c139258ea436e73f1
"
24131,TFLite error: Op builtin_code out of range: 101.,"Used TF nightly (tf-nightly-gpu-1.13.0.dev20181129) to convert a frozen model to .tflite. When I try to load this model in Android, TFLite throws an error: ""Op builtin_code out of range: 101, are you using old TFLite binary with newer model?""

I am loading TFLite nightly in my APK like so:

```
dependencies {
compile 'org.tensorflow:tensorflow-lite:0.0.0-nightly'
}
```

What am I missing and what should I do to get this model loaded in Android?


**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Fedora r28
- TensorFlow installed from (source or binary): binary(conda)
- TensorFlow version (or github SHA if from source):tf-nightly-gpu-1.13.0.dev20181129
"
24128,Error when loading frozen graph,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):

> yes

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):

> CentOS Linux 7

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):

> source

- TensorFlow version (use command below):

> 1.8.0

- Python version:

> 2.7.15

- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):

> 4.8.2

- CUDA/cuDNN version:
- GPU model and memory:


**Describe the current behavior**
I trined GooglNet model and get the error when importing a frozen graph

`ValueError: Input 0 of node save/Assign_41 was passed float from auxiliary_classifier_1/classifier/biases/Adam_1:0 incompatible with expected float_ref.
`

**Code to reproduce the issue**

`def freeze_graph():

    # We retrieve our checkpoint fullpath
    checkpoint = tf.train.get_checkpoint_state(SAVE_PATH)
    input_checkpoint = checkpoint.model_checkpoint_path
    print(input_checkpoint)

    # We precise the file fullname of our freezed graph
    absolute_model_dir = ""/"".join(input_checkpoint.split('/')[:-1])
    output_graph = absolute_model_dir + ""/frozen_model.pb""

    # We clear devices to allow TensorFlow to control on which device it will load operations
    clear_devices = True

    # We start a session using a temporary fresh Graph
    with tf.Session(graph=tf.Graph()) as sess:
        # We import the meta graph in the current default Graph
        saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)

        # We restore the weights
        saver.restore(sess, input_checkpoint)

        gd = tf.get_default_graph().as_graph_def()

        # fix nodes
        for node in gd.node:
            if node.op == 'RefSwitch':
                node.op = 'Switch'
                for index in range(len(node.input)):
                    if 'moving_' in node.input[index]:
                        node.input[index] = node.input[index] + '/read'
            elif node.op == 'AssignSub':
                node.op = 'Sub'
                if 'use_locking' in node.attr:
                    del node.attr['use_locking']
            elif node.op == 'AssignAdd':
                node.op = 'Add'
                if 'use_locking' in node.attr:
                    del node.attr['use_locking']
            else:
                print(node.op , node.name)
            # elif node.op == '':

        output_node_names = [n.name for n in gd.node]

        # We use a built-in TF helper to export variables to constants
        output_graph_def = tf.graph_util.convert_variables_to_constants(
            sess, # The session is used to retrieve the weights
            gd, # The graph_def is used to retrieve the nodes
            output_node_names # The output node names are used to select the usefull nodes
        )

        # Finally we serialize and dump the output graph to the filesystem
        with tf.gfile.GFile(output_graph, ""wb"") as f:
            f.write(output_graph_def.SerializeToString())
        print(""%d ops in the final graph."" % len(output_graph_def.node))`


None of the answers I found on the net (eg https://github.com/tensorflow/tensorflow/issues/3628) do not answer this specific situation, where the problem is in the save op and Adam optimizer

"
24127,'realdiv' bug in Python tensorflow1.11.0,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):macOS Mojave 10.14
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):pip
- TensorFlow version (use command below):1.11.0
- Python version:3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
The description of 'realdiv' is a little confusing, that is :
r""""""Returns x / y element-wise for real types.

  If `x` and `y` are reals, this will return the floating-point division.

  *NOTE*: `Div` supports broadcasting. More about broadcasting
  [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)

  Args:
    **x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`.**
    y: A `Tensor`. Must have the same type as `x`.
    name: A name for the operation (optional).

  Returns:
    A `Tensor`. Has the same type as `x`.
  """"""
I define the dtype of x for int32, the same to y.Then I use tensorflow.realdiv(x, y), it was an error!

**Describe the expected behavior**
I really hope you can answer my questions!Thank you very much!

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```python
import tensorflow as tf
x = tf.constant([1,2,1])
y = tf.constant([1,1,3])
sess = tf.Session()
sess.run(tf.realdiv(x,y))
sess.close()
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
~/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1291     try:
-> 1292       return fn(*args)
   1293     except errors.OpError as e:

~/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1274       # Ensure any changes to the graph are reflected in the runtime.
-> 1275       self._extend_graph()
   1276       return self._call_tf_sessionrun(

~/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _extend_graph(self)
   1311     with self._graph._session_run_lock():  # pylint: disable=protected-access
-> 1312       tf_session.ExtendSession(self._session)
   1313 

InvalidArgumentError: No OpKernel was registered to support Op 'RealDiv' with these attrs.  Registered devices: [CPU], Registered kernels:
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_BFLOAT16]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_COMPLEX128]

	 [[{{node RealDiv_2}} = RealDiv[T=DT_INT32](Const_2, Const_3)]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-8-465bca8f95b0> in <module>
----> 1 sess.run(tf.realdiv(x,y))

~/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    885     try:
    886       result = self._run(None, fetches, feed_dict, options_ptr,
--> 887                          run_metadata_ptr)
    888       if run_metadata:
    889         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

~/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1108     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1109       results = self._do_run(handle, final_targets, final_fetches,
-> 1110                              feed_dict_tensor, options, run_metadata)
   1111     else:
   1112       results = []

~/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1284     if handle is None:
   1285       return self._do_call(_run_fn, feeds, fetches, targets, options,
-> 1286                            run_metadata)
   1287     else:
   1288       return self._do_call(_prun_fn, handle, feeds, fetches)

~/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1306           self._config.experimental.client_handles_error_formatting):
   1307         message = error_interpolation.interpolate(message, self._graph)
-> 1308       raise type(e)(node_def, op, message)
   1309 
   1310   def _extend_graph(self):

InvalidArgumentError: No OpKernel was registered to support Op 'RealDiv' with these attrs.  Registered devices: [CPU], Registered kernels:
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_BFLOAT16]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_COMPLEX128]

	 [[{{node RealDiv_2}} = RealDiv[T=DT_INT32](Const_2, Const_3)]]

Caused by op 'RealDiv_2', defined at:
  File ""/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/ipykernel/kernelapp.py"", line 505, in start
    self.io_loop.start()
  File ""/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tornado/platform/asyncio.py"", line 132, in start
    self.asyncio_loop.run_forever()
  File ""/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/asyncio/base_events.py"", line 427, in run_forever
    self._run_once()
  File ""/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/asyncio/base_events.py"", line 1440, in _run_once
    handle._run()
  File ""/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/asyncio/events.py"", line 145, in _run
    self._callback(*self._args)
  File ""/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tornado/ioloop.py"", line 758, in _run_callback
    ret = callback()
  File ""/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tornado/stack_context.py"", line 300, in null_wrapper
    return fn(*args, **kwargs)
  File ""/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tornado/gen.py"", line 1233, in inner
    self.run()
  File ""/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tornado/gen.py"", line 1147, in run
    yielded = self.gen.send(value)
  File ""/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 357, in process_one
    yield gen.maybe_future(dispatch(*args))
  File ""/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tornado/gen.py"", line 326, in wrapper
    yielded = next(result)
  File ""/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 267, in dispatch_shell
    yield gen.maybe_future(handler(stream, idents, msg))
  File ""/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tornado/gen.py"", line 326, in wrapper
    yielded = next(result)
  File ""/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 534, in execute_request
    user_expressions, allow_stdin,
  File ""/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tornado/gen.py"", line 326, in wrapper
    yielded = next(result)
  File ""/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/ipykernel/ipkernel.py"", line 294, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/ipykernel/zmqshell.py"", line 536, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2819, in run_cell
    raw_cell, store_history, silent, shell_futures)
  File ""/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2845, in _run_cell
    return runner(coro)
  File ""/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/IPython/core/async_helpers.py"", line 67, in _pseudo_sync_runner
    coro.send(None)
  File ""/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3020, in run_cell_async
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3191, in run_ast_nodes
    if (yield from self.run_code(code, result)):
  File ""/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3267, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-8-465bca8f95b0>"", line 1, in <module>
    sess.run(tf.realdiv(x,y))
  File ""/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 5989, in real_div
    ""RealDiv"", x=x, y=y, name=name)
  File ""/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op
    op_def=op_def)
  File ""/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'RealDiv' with these attrs.  Registered devices: [CPU], Registered kernels:
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_BFLOAT16]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_COMPLEX128]

	 [[{{node RealDiv_2}} = RealDiv[T=DT_INT32](Const_2, Const_3)]]"
24126,Compute Capability 3.0 GTX 770 ,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a 
- TensorFlow installed from (source or binary): source
- TensorFlow version: r1.8
- Python version: python2.7
- Installed using virtualenv? pip? conda?: pip (but I don't get that far)
- Bazel version (if compiling from source): 0.13.1
- GCC/Compiler version (if compiling from source): gcc 6.4.0
- CUDA/cuDNN version: Cuda 9.0.176/cuDNN 7.4/NCLL 2.3
- GPU model and memory: GTX 770 2GB 256-Bit GDDR5

**Config file**
```
You have bazel 0.13.0 installed.
Please specify the location of python. [Default is /usr/bin/python]:
Please input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]
Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: y
jemalloc as malloc support will be enabled for TensorFlow.
Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n
No Google Cloud Platform support will be enabled for TensorFlow.
Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: n
No Hadoop File System support will be enabled for TensorFlow.
Do you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n
No Amazon S3 File System support will be enabled for TensorFlow.
Do you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: n
No Apache Kafka Platform support will be enabled for TensorFlow.
Do you wish to build TensorFlow with XLA JIT support? [y/N]: y
XLA JIT support will be enabled for TensorFlow.
Do you wish to build TensorFlow with GDR support? [y/N]: n
No GDR support will be enabled for TensorFlow.
Do you wish to build TensorFlow with VERBS support? [y/N]: n
No VERBS support will be enabled for TensorFlow.
Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.
Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 9.0]:
Please specify the location where CUDA 9.1 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:
Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 7.1.4
Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:
Do you wish to build TensorFlow with TensorRT support? [y/N]: n
No TensorRT support will be enabled for TensorFlow.
Please specify the NCCL version you want to use. [Leave empty to default to NCCL 1.3]: 2.2.12
Please specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:/usr/local/cuda/nccl
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.0]
Do you want to use clang as CUDA compiler? [y/N]: n
nvcc will be used as CUDA compiler.
Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/x86_64-linux-gnu-gcc-7]: /usr/bin/gcc-6
Do you wish to build TensorFlow with MPI support? [y/N]: n
No MPI support will be enabled for TensorFlow.
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]:
Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.
Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See tools/bazel.rc for more details.
 --config=mkl          # Build with MKL support.

 --config=monolithic   # Config for mostly static monolithic build.

Configuration finished
```

**Describe the problem**

I have been struggling to build Tensorflow from source with bazel (I've tried releases 0.19.x - 0.13.x). I need to build from source in order to not have AVX instructions and also to used cuda compute capability 3.0. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Most recently, I tried building for GPU support using the latest nightly Docker image. That doesn't support compute capability 3.0, though. I tried [this guide](https://stackoverflow.com/questions/39023581/tensorflow-cuda-compute-capability-3-0-the-minimum-required-cuda-capability-is/50592978) which seemed promising, but could not get bazel to build successfully. Note that I'm using different versions of cuDNN and NCLL from that guide. Should I try the exact versions the OP used? Am I doing something obviously wrong? 

**Any other info / logs**

There are none to show. The failure with Bazel is literally different everytime. I can provide the next failure if it will help

**Some ideas I was going to try next**

-  revert to the versions of cuDNN (7.1.4) and NCCL (2.2.12) that the person from that guide used
- use the versions of gcc (4.8) and bazel (0.10.0) that are listed in the ""Tested build configurations"" table on the official documentation. Although, doesn't CUDA 9.0 need gcc 6.x? 

I've been working on this on/off for about a month now, would really appreciate a push in the right direction. 

"
24125,freeze a tensorflow graph which contains a LookupTable and use builder.save() for tf-serving  ,"I have a tf model which contains a LookupTable, and I used
 `table_init_op = tf.tables_initializer(name=""init_all_tables"")
        config.sess.run(table_init_op)
`
to initialize it,  and freezed the graph by
`output_graph_def = convert_variables_to_constants(config.sess,
                                                          config.sess.graph_def,
                                                          output_node_names=['init_all_tables', 'Inference/final_output/output', 'Inference/cur_state'])
`
finally I save the model by builder.save(), it is ok until i run my tf server, however, when i do inference, I got an error`grpc._channel._Rendezvous: <_Rendezvous of RPC that terminated with:
	status = StatusCode.INVALID_ARGUMENT
	details = ""NodeDef mentions attr 'Truncate' not in Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: Inference/affine_layers2/out_2/affine_transform/dense/Tensordot/Cast = Cast[DstT=DT_INT32, SrcT=DT_BOOL, Truncate=false, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Inference/affine_layers2/out_2/affine_transform/dense/Tensordot/GreaterEqual). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
	 [[Node: Inference/affine_layers2/out_2/affine_transform/dense/Tensordot/Cast = Cast[DstT=DT_INT32, SrcT=DT_BOOL, Truncate=false, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Inference/affine_layers2/out_2/affine_transform/dense/Tensordot/GreaterEqual)]]""
	debug_error_string = ""{""created"":""@1543839204.502783269"",""description"":""Error received from peer"",""file"":""src/core/lib/surface/call.cc"",""file_line"":1083,""grpc_message"":""NodeDef mentions attr 'Truncate' not in Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: Inference/affine_layers2/out_2/affine_transform/dense/Tensordot/Cast = Cast[DstT=DT_INT32, SrcT=DT_BOOL, Truncate=false, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Inference/affine_layers2/out_2/affine_transform/dense/Tensordot/GreaterEqual). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\n\t [[Node: Inference/affine_layers2/out_2/affine_transform/dense/Tensordot/Cast = Cast[DstT=DT_INT32, SrcT=DT_BOOL, Truncate=false, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Inference/affine_layers2/out_2/affine_transform/dense/Tensordot/GreaterEqual)]]"",""grpc_status"":3}""`
does the table can freeze to the graph,what should i do to freeze it?
thank you very much for your reply~"
24124,bazel build issue (does not contain a toolchain for CPU 'k8'),"**System information**
- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: `master` (6798656ce64201afe7d8d9ed5445cd5114bfd9b4)
- Bazel version (if compiling from source): 0.20.0
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: 10


**Describe the problem**
When building branch `v1.12.0` or current `master` (commit 6798656ce64201afe7d8d9ed5445cd5114bfd9b4) I get the following build error:

`ERROR: cc_toolchain_suite '@local_config_cuda//crosstool:toolchain' does not contain a toolchain for CPU 'k8', you may want to add an entry for 'local|compiler' into toolchains and toolchain_identifier 'local_linux' into the corresponding cc_toolchain rule (see --incompatible_disable_cc_toolchain_label_from_crosstool_proto).`


**Provide the exact sequence of commands / steps that you executed before running into the problem**
- install bazel as described here: https://docs.bazel.build/versions/master/install-ubuntu.html#install-on-ubuntu
- checkout the `v1.12.0` branch or 6798656ce64201afe7d8d9ed5445cd5114bfd9b4
- bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

"
24123,Error while running sample cpp program in tensorflow,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64-bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No
- TensorFlow installed from (source or binary):Source
- TensorFlow version (use command below):r1.11
- Python version: 3.6
- Bazel version (if compiling from source): 0.19
- GCC/Compiler version (if compiling from source):msvc 2017
- CUDA/cuDNN version:
- GPU model and memory:

Problem Statement:
I have build tensorflow r1.11 library. During the build it was giving following error:
https://github.com/tensorflow/tensorflow/issues/23402
As a solution to the problem i used the one of the suggested solution on the forum:
bazel build --define=grpc_no_ares=true -c opt //tensorflow:libtensorflow_cc.so
The bazel is built successfully and when i try to run a sample program i get the following error:
:1: error: dependent '..\..\..\..\Downloads\tensorflow-r1.11\bazel-tensorflow-r1.11\external\eigen_archive\unsupported\Eigen\src\SpecialFunctions\arch\CUDA\CudaSpecialFunctions.h' does not exist.
Eariler i have build other version of library but this was not the issue.
"
24122,"Build failed on Windows with AVX2, MKL and CUDA support.","**System information**
- OS Platform and Distribution: Windows 10 (1809)
- TensorFlow installed from (source or binary): Source
- TensorFlow version: r1.12
- Python version: 3.6
- Bazel version (if compiling from source): 0.19.2
- CUDA/cuDNN version: CUDA 10.0, cuDNN 7
- GPU model and memory: GTX 1060MQ 6GB


**Describe the problem**
I want to build TensorFlow with AVX2, MKL and CUDA support, but I failed (It built successfully without the AVX2 and MKL support). Here is my Bazel build commands:
```
bazel build --compilation_mode=opt --config=opt --copt=-mavx2 --copt=-nvcc_options=disable-warnings --config=cuda --config=mkl //tensorflow/tools/pip_package:build_pip_package
```
It seems that the ""--copt=-mavx2"" argument cannot take effect:
```
INFO: From Compiling tensorflow/core/kernels/batch_matmul_op_complex.cc:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
```
and there are another two error messages:
```
ERROR: D:/library/tensorflow/tensorflow/core/kernels/BUILD:6385:1: C++ compilation of rule '//tensorflow/core/kernels:mkl_relu_op' failed (Exit 2): msvc_wrapper_for_nvcc.bat failed: error executing command
```
```
INFO: From Executing genrule //tensorflow/core:version_info_gen:
fatal: Invalid path '/c/users/young/_bazel_young/a3m3eycs/execroot/org_tensorflow/C:': No such file or directory
```
What should I do to build the TensorFlow correctly? Thank you very much!


**Any other info / logs**
**The ""configure"" command**
```
D:\Library\tensorflow>python ./configure.py
WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
d:\library\tensorflow/tools/bazel.rc
nul
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.19.2 installed.
Please specify the location of python. [Default is C:\Program Files\Python36\python.exe]:


Found possible Python library paths:
  C:\Program Files\Python36\lib\site-packages
Please input the desired Python library path to use.  Default is [C:\Program Files\Python36\lib\site-packages]

Do you wish to build TensorFlow with Apache Ignite support? [Y/n]: n
No Apache Ignite support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [y/N]:
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: 10.0


Please specify the location where CUDA 10.0 toolkit is installed. Refer to README.md for more details. [Default is D:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]:


Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is D:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:


Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]: 6.1


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]: /arch:AVX2


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:
Eigen strong inline overridden.
```


**The ""build"" output (too long and simplified)**
```
D:\Library\tensorflow>bazel build --compilation_mode=opt --config=opt --copt=-mavx2 --copt=-nvcc_options=disable-warnings --config=cuda --config=mkl //tensorflow/tools/pip_package:build_pip_package
WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
d:\library\tensorflow/.bazelrc
d:\library\tensorflow/tools/bazel.rc
Starting local Bazel server and connecting to it...
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
WARNING: Option 'experimental_shortened_obj_file_path' is deprecated
WARNING: C:/users/young/_bazel_young/a3m3eycs/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/young/_bazel_young/a3m3eycs/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: C:/users/young/_bazel_young/a3m3eycs/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/young/_bazel_young/a3m3eycs/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: C:/users/young/_bazel_young/a3m3eycs/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/young/_bazel_young/a3m3eycs/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: D:/library/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: D:/library/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
WARNING: D:/library/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: D:/library/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:354:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: D:/library/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:230:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: D:/library/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: D:/library/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:73:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: D:/library/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (308 packages loaded, 13377 targets configured).
INFO: Found 1 target...
INFO: From Compiling tensorflow/core/lib/hash/crc32c_accelerate.cc:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'

...

INFO: From Compiling external/grpc/src/core/tsi/alts/zero_copy_frame_protector/alts_iovec_record_protocol.cc:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
INFO: From Linking external/grpc/libgrpc++_base.a:
server_posix.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library
rpc_method.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library
create_channel_posix.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library
INFO: From Compiling external/grpc/src/core/tsi/alts/frame_protector/alts_frame_protector.cc:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
INFO: From Compiling external/grpc/src/core/tsi/alts/frame_protector/alts_record_protocol_crypter_common.cc:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
INFO: From Compiling external/grpc/src/core/tsi/alts/zero_copy_frame_protector/alts_grpc_integrity_only_record_protocol.cc:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'

...

INFO: From Compiling external/protobuf_archive/src/google/protobuf/map_field.cc:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
INFO: From Linking external/protobuf_archive/libprotobuf_lite.a:
arenastring.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library
INFO: From Compiling external/protobuf_archive/src/google/protobuf/generated_message_table_driven.cc:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
INFO: From Compiling external/protobuf_archive/src/google/protobuf/source_context.pb.cc:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'

...

cl : Command line warning D9002 : ignoring unknown option '-mavx2'
INFO: From Compiling external/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
INFO: From Linking external/protobuf_archive/libprotobuf.a:
error_listener.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library
INFO: From Compiling external/protobuf_archive/python/google/protobuf/pyext/extension_dict.cc:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'

...

INFO: From Compiling tensorflow/python/framework/fast_tensor_util.cpp:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
c:\users\young\_bazel_young\a3m3eycs\execroot\org_tensorflow\bazel-out\x64_windows-opt\genfiles\external\local_config_python\numpy_include\numpy\npy_1_7_deprecated_api.h(12) : Warning Msg: Using deprecated NumPy API, disable it by #defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION
INFO: From Compiling tensorflow/core/protobuf/queue_runner.pb.cc:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
INFO: From Linking tensorflow/python/framework/fast_tensor_util.so:
   Creating library bazel-out/x64_windows-opt/bin/tensorflow/python/framework/libfast_tensor_util.so.ifso and object bazel-out/x64_windows-opt/bin/tensorflow/python/framework/libfast_tensor_util.so.exp
INFO: From Compiling tensorflow/core/framework/remote_fused_graph_execute_info.pb.cc:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'

...

INFO: From Compiling tensorflow/core/example/example.pb_text.cc:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
INFO: From Linking tensorflow/core/liblib_internal_impl.a:
android_armv7a_cpu_utils_helper.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library
random_distributions.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library
INFO: From Compiling tensorflow/core/framework/summary.pb_text.cc:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'

...

INFO: From Compiling tensorflow/core/kernels/cwise_op_gpu_logical_not.cu.cc:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
host_defines.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.
INFO: From Compiling tensorflow/core/kernels/cwise_op_gpu_tan.cu.cc:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
host_defines.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.
INFO: From Compiling tensorflow/contrib/tpu/ops/heartbeat_ops.cc:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
INFO: From Compiling tensorflow/contrib/tpu/ops/tpu_configuration_ops.cc:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
INFO: From Compiling tensorflow/contrib/coder/kernels/pmf_to_cdf_op.cc:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
INFO: From Compiling tensorflow/contrib/tpu/ops/heartbeat_ops.cc:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
INFO: From Compiling tensorflow/core/kernels/cwise_op_gpu_log1p.cu.cc:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
host_defines.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.
INFO: From Compiling tensorflow/contrib/framework/kernels/zero_initializer_op_gpu.cu.cc:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
host_defines.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.
INFO: From Compiling tensorflow/contrib/tpu/ops/outfeed_ops.cc:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'

...

INFO: From Compiling tensorflow/core/kernels/qr_op_double.cc:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
INFO: From Compiling tensorflow/core/grappler/costs/measuring_cost_estimator.cc:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
INFO: From Compiling tensorflow/core/kernels/cwise_op_bitwise_or.cc:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
INFO: From Compiling tensorflow/core/kernels/cwise_op_conj.cc:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
INFO: From Compiling tensorflow/core/kernels/cwise_op_sigmoid.cc:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
INFO: From Compiling tensorflow/core/kernels/cwise_op_gpu_zeta.cu.cc:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
host_defines.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.
INFO: From Compiling tensorflow/python/grappler/cost_analyzer.cc:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'

...

INFO: From Compiling tensorflow/core/kernels/mfcc_dct.cc:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
INFO: From Compiling tensorflow/core/kernels/batch_matmul_op_complex.cc:
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
ERROR: D:/library/tensorflow/tensorflow/core/kernels/BUILD:6385:1: C++ compilation of rule '//tensorflow/core/kernels:mkl_relu_op' failed (Exit 2): msvc_wrapper_for_nvcc.bat failed: error executing command
  cd C:/users/young/_bazel_young/a3m3eycs/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=D:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET CUDNN_INSTALL_PATH=D:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt;
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\WINDOWS\Microsoft.NET\Framework64\v4.0.30319;C:\WINDOWS\Microsoft.NET\Framework64\;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Windows Kits\10\bin\x86;;C:\WINDOWS\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Program Files/Python36/python.exe
    SET PYTHON_LIB_PATH=C:/Program Files/Python36/lib/site-packages
    SET TEMP=C:\Users\YOUNG\AppData\Local\Temp
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1
    SET TF_CUDA_VERSION=10.0
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
    SET TMP=C:\Users\YOUNG\AppData\Local\Temp
  external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.bat /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/genfiles/external/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive /Ibazel-out/x64_windows-opt/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-opt/genfiles/external/jpeg /Ibazel-out/x64_windows-opt/bin/external/jpeg /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/genfiles/external/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/genfiles/external/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/genfiles/external/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/genfiles/external/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/mkl_windows /Ibazel-out/x64_windows-opt/genfiles/external/mkl_windows /Ibazel-out/x64_windows-opt/bin/external/mkl_windows /Iexternal/mkl_dnn /Ibazel-out/x64_windows-opt/genfiles/external/mkl_dnn /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Ithird_party/eigen3/mkl_include /Ibazel-out/x64_windows-opt/genfiles/third_party/eigen3/mkl_include /Ibazel-out/x64_windows-opt/bin/third_party/eigen3/mkl_include /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/gif_archive/lib /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/lib /Ibazel-out/x64_windows-opt/bin/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/windows /Ibazel-out/x64_windows-opt/bin/external/gif_archive/windows /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/mkl_windows/include /Ibazel-out/x64_windows-opt/genfiles/external/mkl_windows/include /Ibazel-out/x64_windows-opt/bin/external/mkl_windows/include /Iexternal/mkl_dnn/include /Ibazel-out/x64_windows-opt/genfiles/external/mkl_dnn/include /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/include /Iexternal/mkl_dnn/src /Ibazel-out/x64_windows-opt/genfiles/external/mkl_dnn/src /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src /Iexternal/mkl_dnn/src/common /Ibazel-out/x64_windows-opt/genfiles/external/mkl_dnn/src/common /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/common /Iexternal/mkl_dnn/src/cpu /Ibazel-out/x64_windows-opt/genfiles/external/mkl_dnn/src/cpu /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/cpu /Iexternal/mkl_dnn/src/cpu/xbyak /Ibazel-out/x64_windows-opt/genfiles/external/mkl_dnn/src/cpu/xbyak /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/cpu/xbyak /Iexternal/mkl_dnn/src/cpu/gemm /Ibazel-out/x64_windows-opt/genfiles/external/mkl_dnn/src/cpu/gemm /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/cpu/gemm /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include/crt /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX2 -mavx2 -nvcc_options=disable-warnings -DGOOGLE_CUDA=1 -DINTEL_MKL=1 -DEIGEN_USE_VML -DENABLE_MKL -DTENSORFLOW_MONOLITHIC_BUILD /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COMPILE_LIBRARY /Fobazel-out/x64_windows-opt/bin/tensorflow/core/kernels/_objs/mkl_relu_op/mkl_relu_op.o /c tensorflow/core/kernels/mkl_relu_op.cc
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
tensorflow/core/kernels/mkl_relu_op.cc(1027): error C2398: Element '1': conversion from 'const std::size_t' to 'int' requires a narrowing conversion
tensorflow/core/kernels/mkl_relu_op.cc(895): note: while compiling class template member function 'void tensorflow::MklReluGradOpBase<Device,T,mkldnn::eltwise_tanh>::Compute(tensorflow::OpKernelContext *)'
        with
        [
            Device=tensorflow::CPUDevice,
            T=float
        ]
tensorflow/core/kernels/mkl_relu_op.cc(1217): note: see reference to class template instantiation 'tensorflow::MklReluGradOpBase<Device,T,mkldnn::eltwise_tanh>' being compiled
        with
        [
            Device=tensorflow::CPUDevice,
            T=float
        ]
tensorflow/core/kernels/mkl_relu_op.cc(1297): note: see reference to class template instantiation 'tensorflow::MklTanhGradOp<tensorflow::CPUDevice,float>' being compiled
.\tensorflow/core/util/tensor_format.h(451): note: see reference to class template instantiation 'absl::Span<const tensorflow::int64>' being compiled
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 705.262s, Critical Path: 158.20s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]
INFO: 2169 processes: 2169 local.
FAILED: Build did NOT complete successfully
```"
24121,deeplab v3+ quantization tflite,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 (train / export), macOS 10.14.1 (toco) 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy S8
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6
- Bazel version (if compiling from source): V9.0.176
- GCC/Compiler version (if compiling from source): Apple LLVM version 10.0.0 (clang-1000.11.45.5)
- CUDA/cuDNN version: 0.15.2-homebrew
- GPU model and memory: Nvidia Titan Xp 12GB x 4


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
 - I want to make realtime segmentation model using quantization form tensorflow lite on android device. The first step, I success to make quantized tflite. Unfortunately the result is always background. 
 - We make 2 class dataset background and sky. Here is visualization of tensor, that before argmax. [Graph Link](https://photos.app.goo.gl/ZjcCxstLHJK3dUmx8). Guessing, 8bit quantization make small difference, that makes it miss bias in last stage.
 - pb and float tflite(--drop_fake_quant=true) models are works. 

**Describe the expected behavior**

 - Quantized tflite models will works.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24120,TFLite Speech recognization app is getting crashed with setUseNNAPI(true),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):[NO]
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):
- Python version: 3.0
- Bazel version (if compiling from source):[bazel release 0.16.1]
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


Description: 
Im using the source code from below repo:https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/android/app
Compiled using the readme in above link
The app works fine without any changes.
However when i enable setUseNNAPI(true) for speechActivity the crashes with below logs:

======================================================================
### tflite  : Custom operations are not supported when using NNAPI.
12-03 14:10:16.545  7947  7969 E tflite  : Returning error since TFLite returned failure nnapi_delegate.cc:745.
12-03 14:10:16.545  7947  7969 E tflite  : Failed to build graph for NNAPI
12-03 14:10:16.545  7947  7969 E AndroidRuntime: FATAL EXCEPTION: Thread-3
12-03 14:10:16.545  7947  7969 E AndroidRuntime: Process: org.tensorflow.lite.demo, PID: 7947
12-03 14:10:16.545  7947  7969 E AndroidRuntime: java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: 
12-03 14:10:16.545  7947  7969 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)
12-03 14:10:16.545  7947  7969 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:140)
12-03 14:10:16.545  7947  7969 E AndroidRuntime: 	at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:216)
12-03 14:10:16.545  7947  7969 E AndroidRuntime: 	at org.tensorflow.demo.SpeechActivity.recognize(SpeechActivity.java:339)
12-03 14:10:16.545  7947  7969 E AndroidRuntime: 	at org.tensorflow.demo.SpeechActivity.access$100(SpeechActivity.java:67)
12-03 14:10:16.545  7947  7969 E AndroidRuntime: 	at org.tensorflow.demo.SpeechActivity$3.run(SpeechActivity.java:290)
12-03 14:10:16.545  7947  7969 E AndroidRuntime: 	at java.lang.Thread.run(Thread.java:764)
==============================================================================
**Other info / logs**
Note:
Image clasification works fine with same changes setUseNNAPI(true)

"
24119,How to integrate KMeansClustering with other module,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): source 
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6

**Problem**
When I want to integrate KMeansClustering into my model, I want to use minibatch kmeans to cluster the output of 2 fully connected layers. However, the error is one tensor of kmeans must be from the same graph as the input tensor. Is there any way to solve it? Thank you!"
24118,Build failed when using AVX2 and MKL support.,"**System information**
- OS Platform and Distribution: Windows 10 (1809)
- TensorFlow installed from (source or binary): source
- TensorFlow version: r1.12
- Python version: 3.6
- Bazel version (if compiling from source): 0.19.2
- CUDA/cuDNN version: CUDA 10.0, cuDNN 7
- GPU model and memory: GTX 1060MQ 6GB


**Describe the problem**
I want to build TensorFlow with AVX2, MKL and CUDA support, Here is my Bazel build instruction:
`bazel build --compilation_mode=opt --config=opt --copt=-mavx2 --copt=-nvcc_options=disable-warnings --config=cuda --config=mkl //tensorflow/tools/pip_package:build_pip_package`




**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24117,Build failed when using AVX2 and MKL support.,"**System information**
- OS Platform and Distribution: Windows 10 (1809)
- TensorFlow installed from (source or binary): source
- TensorFlow version: r1.12
- Python version: 3.6
- Bazel version (if compiling from source): 0.19.2
- CUDA/cuDNN version: CUDA 10.0, cuDNN 7
- GPU model and memory: GTX 1060MQ 6GB


**Describe the problem**
I want to build TensorFlow with AVX2, MKL and CUDA support, Here is my Bazel build instruction:
`bazel build --compilation_mode=opt --config=opt --copt=-mavx2 --copt=-nvcc_options=disable-warnings --config=cuda --config=mkl //tensorflow/tools/pip_package:build_pip_package`




**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24116,Build failed when using AVX2 and MKL support.,"**System information**
- OS Platform and Distribution: Windows 10 (1809)
- TensorFlow installed from (source or binary): source
- TensorFlow version: r1.12
- Python version: 3.6
- Bazel version (if compiling from source): 0.19.2
- CUDA/cuDNN version: CUDA 10.0, cuDNN 7
- GPU model and memory: GTX 1060MQ 6GB


**Describe the problem**
I want to build TensorFlow with AVX2, MKL and CUDA support, Here is my Bazel build instruction:
`bazel build --compilation_mode=opt --config=opt --copt=-mavx2 --copt=-nvcc_options=disable-warnings --config=cuda --config=mkl //tensorflow/tools/pip_package:build_pip_package`




**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24115,Build failed when using AVX2 and MKL support.,"**System information**
- OS Platform and Distribution: Windows 10 (1809)
- TensorFlow installed from (source or binary): source
- TensorFlow version: r1.12
- Python version: 3.6
- Bazel version (if compiling from source): 0.19.2
- CUDA/cuDNN version: CUDA 10.0, cuDNN 7
- GPU model and memory: GTX 1060MQ 6GB


**Describe the problem**
I want to build TensorFlow with AVX2, MKL and CUDA support, Here is my Bazel build instruction:
`bazel build --compilation_mode=opt --config=opt --copt=-mavx2 --copt=-nvcc_options=disable-warnings --config=cuda --config=mkl //tensorflow/tools/pip_package:build_pip_package`




**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24111,Bug when passing multiple output of Lambda Layer to the Model API,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
source
- TensorFlow version (use command below):
1.11.0
- Python version:
3.6.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
5.2.0
- CUDA/cuDNN version:
9/7
- GPU model and memory:
GTX 1080, 8GB


**Describe the current behavior**
Attempting to receive multiple outputs from a lambda layer.  When passing them through the Model API, causes them to have an unexpected behavior. if all the outputs of that lambda layer are stored into a single variable, passing that variable as output for model fails with 
`AssertionError: Could not compute output Tensor...`

Using index[0] gives me behavior I would have expected when receiving with no index specified. Using index 1 or 2 gives same error as not giving an index. TLDR: index 0 gives behavior you would expect from not having an index. Index 1,2 give error same as no index 
**Describe the expected behavior**
specifying no index means that I am accessing a list of tensors, where as specifying an index gives me the tensor at that index.

**Code to reproduce the issue**
```
from tensorflow.python.keras.layers import Input, Lambda
from tensorflow.python.keras.models import Model
import tensorflow as tf
import numpy as np

ins = Input([1])

def test(x):
    return tf.constant(2,name='one'),tf.constant([2]),tf.constant([[2],[4]])

ous = Lambda(test)(ins)
model = Model(ins,ous[0]) # changing this 0 to 1 or 2 fails, removing index also fails
# model = Model(ins,ous) # FAILS
# model = Model(ins,ous[1]) # FAILS

new = model(np.asarray([1]))

with tf.Session() as sess:
    print(sess.run(new))  #(2, array([2], dtype=int32), array([[2], [4]], dtype=int32))
```
**Other info / logs**"
24110,Tensorflow-gpu wont install Windows 10,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Windows 10  Home Ver 1803
- TensorFlow installed from pip
- TensorFlow version: 1.12.0 (tensorflow-gpu)
- Python version:3.6.3
- Installed using virtualenv? pip? conda?:pip
- CUDA/cuDNN version: 10.0.132
- GPU model and memory: GTX 1080



**Describe the problem**
Tensor flow-gpu is not properly installed.   

**Provide the exact sequence of commands / steps that you executed before running into the problem**
This is the only code
**import tensorflow**  
Thats it!!

If I install tensorflow-gpu and install tensor floe for the cpu it all works.

ImportError: DLL load failed: The specified module could not be found.

Failed to load the native TensorFlow runtime.

The entire traceback is listed below.
Installing the tensor flow 
That is as basic as you get  it states 
**Any other info / logs**
Python 3.6.3 (v3.6.3:2c5fed8, Oct  3 2017, 18:11:49) [MSC v.1900 64 bit (AMD64)] on win32
Type ""copyright"", ""credits"" or ""license()"" for more information.
>>> 
 RESTART: C:\Users\BillS\AppData\Local\Programs\Python\Python36\tensorgputest.py 
Traceback (most recent call last):
  File ""C:\Users\BillS\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\BillS\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\BillS\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\BillS\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\BillS\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\BillS\AppData\Local\Programs\Python\Python36\tensorgputest.py"", line 1, in <module>
    import tensorflow
  File ""C:\Users\BillS\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\BillS\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\BillS\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\BillS\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\BillS\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\BillS\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\BillS\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\BillS\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.


"
24109,Mixture of Multivariate distributions: Could not infer number of classes from cat ,"Hi,
I am facing this issue:
ds=tf.contrib.distributions
#input is computed by passing some place_holders for tensors through neural networks
sm_scores=tf.nn.softmax(input)#input=batch_sizexseq
cat=ds.Categorical(probs=sm_scores)
means=nmean x batchx dimens
sigs=nsigs x batchx dimens # mean & sigma are from some place_holders
comps = [ds.MultivariateNormalDiag(loc=means[0], scale_diag=sigs[0]),
ds.MultivariateNormalDiag(loc=means[1], scale_diag=sigs[1]),
ds.MultivariateNormalDiag(loc=means[2], scale_diag=sigs[2])]
mix=tf.Mixture(cat=cat, components=comps)

I get the following error:
python3.5/site-packages/tensorflow/contrib/distributions/python/ops/mixture.py"", line 147, in init
""Could not infer number of classes from cat and unable ""
ValueError: Could not infer number of classes from cat and unable to compare this value to the number of components passed in.

However, when I get the output sm_scores after running tf.session, perform the samething outside class, it works fine:

cat=ds.Categorical(probs=sm_scores)
means=nmean x batchx dimens
sigs=nsigs x batchx dimens # mean & sigma are from some place_holders
comps = [ds.MultivariateNormalDiag(loc=means[0], scale_diag=sigs[0]),
ds.MultivariateNormalDiag(loc=means[1], scale_diag=sigs[1]),
ds.MultivariateNormalDiag(loc=means[2], scale_diag=sigs[2])]
mix=tf.Mixture(cat=cat, components=comps)

Do you have any idea, why is this happening?"
24107,What's the problem? (bazel build),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:1.12
- Python version:3.6.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.17.2
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.0/7.4.1
- GPU model and memory: geforce 750

INFO: From Compiling external/grpc/third_party/address_sorting/address_sorting_windows.c:
cl : Command line warning D9002 : ignoring unknown option '-std=c99'

ERROR: C:/users/amsokol/development/tensorflow-build/tensorflow/tensorflow/contrib/lite/schema/BUILD:58:1: Generating flatbuffer files for schema_fbs_srcs: //tensorflow/contrib/lite/schema:schema_fbs_srcs failed (Illegal instruction): bash.exe failed: error executing command
 
 cd C:/users/minsu/_bazel_minsu/lx6zoh4k/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=D:/Program Files
    SET CUDNN_INSTALL_PATH=D:/Program Files
    SET PATH=C:\msys64\usr\bin;C:\msys64\bin;C:\Users\amsokol\tensorflow-v1.12\Scripts;D:\Program Files\bin;D:\Program Files\libnvvp;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\ffmpeg-4.1-win64-static\bin;C:\Users\minsu\AppData\Local\Programs\Python\Python36;C:\Users\minsu\AppData\Local\Programs\Python\Python36\Scripts;C:\Users\minsu\AppData\Local\cuda\bin;C:\bazel;D:\tensorflow\tensorflow\tools\pip_package;C:\msys64\usr\bin;D:\tensorflow;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC;C:\Users\minsu\Miniconda3;C:\Users\minsu\Miniconda3\Library\mingw-w64\bin;C:\Users\minsu\Miniconda3\Library\usr\bin;C:\Users\minsu\Miniconda3\Library\bin;C:\Users\minsu\Miniconda3\Scripts;C:\Users\minsu\AppData\Local\Microsoft\WindowsApps
    SET PYTHON_BIN_PATH=C:/Users/amsokol/tensorflow-v1.12/Scripts/python.exe
    SET PYTHON_LIB_PATH=C:/Users/amsokol/tensorflow-v1.12/Lib/site-packages
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=5.0
    SET TF_CUDA_VERSION=10.0
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
  C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; for f in tensorflow/contrib/lite/schema/schema.fbs; do bazel-out/x64_windows-opt/bin/external/flatbuffers/flatc --no-union-value-namespacing --gen-object-api  -c -o bazel-out/x64_windows-opt/genfiles/tensorflow/contrib/lite/schema $f; done: bash.exe failed: error executing command
  cd C:/users/minsu/_bazel_minsu/lx6zoh4k/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=D:/Program Files
    SET CUDNN_INSTALL_PATH=D:/Program Files
    SET PATH=C:\msys64\usr\bin;C:\msys64\bin;C:\Users\amsokol\tensorflow-v1.12\Scripts;D:\Program Files\bin;D:\Program Files\libnvvp;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\ffmpeg-4.1-win64-static\bin;C:\Users\minsu\AppData\Local\Programs\Python\Python36;C:\Users\minsu\AppData\Local\Programs\Python\Python36\Scripts;C:\Users\minsu\AppData\Local\cuda\bin;C:\bazel;D:\tensorflow\tensorflow\tools\pip_package;C:\msys64\usr\bin;D:\tensorflow;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC;C:\Users\minsu\Miniconda3;C:\Users\minsu\Miniconda3\Library\mingw-w64\bin;C:\Users\minsu\Miniconda3\Library\usr\bin;C:\Users\minsu\Miniconda3\Library\bin;C:\Users\minsu\Miniconda3\Scripts;C:\Users\minsu\AppData\Local\Microsoft\WindowsApps
    SET PYTHON_BIN_PATH=C:/Users/amsokol/tensorflow-v1.12/Scripts/python.exe
    SET PYTHON_LIB_PATH=C:/Users/amsokol/tensorflow-v1.12/Lib/site-packages
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=5.0
    SET TF_CUDA_VERSION=10.0
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
  C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; for f in tensorflow/contrib/lite/schema/schema.fbs; do bazel-out/x64_windows-opt/bin/external/flatbuffers/flatc --no-union-value-namespacing --gen-object-api  -c -o bazel-out/x64_windows-opt/genfiles/tensorflow/contrib/lite/schema $f; done
/usr/bin/bash: line 1:  7444 Illegal instruction     bazel-out/x64_windows-opt/bin/external/flatbuffers/flatc --no-union-value-namespacing --gen-object-api -c -o bazel-out/x64_windows-opt/genfiles/tensorflow/contrib/lite/schema $f
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 22.017s, Critical Path: 2.25s
INFO: 47 processes: 47 local.
FAILED: Build did NOT complete successfully

"
24106,Duplicate libiomp5.dylib & libmklml.dylib when install TF 1.12 with python 3.6.x using Anaconda3 in Mac OS 10.13,"

**System information**
- OS Platform and Distribution : MacOS 10.13 High Sierra
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.12
- Python version: 3.6.x
- Installed using : Anaconda 3, version 5.3.1

**Describe the problem**
Actually, my program meet a issue as [#23999 ](https://github.com/tensorflow/tensorflow/issues/23999)

I try to deep into the reason which cause the issue, 
and found that when I create a env (name 'py36' in my case) in Anaconda3 with Python 3.6 and install TensorFlow 1.12 ,
there are duplicate files `libiomp5.dylib` & `libmklml.dylib` under path
`~/anaconda3/envs/py36/lib/python3.6/site-packages/_solib_darwin/_U@mkl_Udarwin_S_S_Cmkl_Ulibs_Udarwin___Uexternal_Smkl_Udarwin_Slib/`

which are same files under path 
`~/anaconda3/envs/py36/lib`

No matter I delete which one, the program cannot run well.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Step 1. Install Anaconda 3 (Version 5.3.1)
Step2. Create env using Python 3.6
Step3. Install TensorFlow 1.12 and relative package.

BTW, If I create env (name 'py35' ) using Python 3.5, this issue is gone.
There are no folder `_solib_darwin` be created under `~/anaconda3/envs/py35/lib/python3.5/site-packages/` and no duplicate files `libiomp5.dylib` & `libmklml.dylib`

**Any other info / logs**


"
24105,Add an ability to terminate tf.data.choose_from_datasets/sample_from_datasets early,"**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): Don't know

**Describe the feature and the current behavior/state.**
Presently, choose_from_datasets terminates as soon all datasets finish. 
It would be nice ho have an option to terminate as soon as *any* of datasets finish as well.

Suppose you have a main dataset which you use for training and a secondary dataset you mix into your first one. They can be of completely different sizes, but you want training to terminate as soon as main dataset terminates without any regard to the secondary dataset.

**Will this change the current api? How?**
Add a flag to choose_from_datasets/sample_from_datasets

**Who will benefit with this feature?**
Users who write complex tf.data pipelines

**Any Other info.**
"
24104,ERROR: Config value monolithic is not defined in any .rc file,"Hello TensorFlow help team,

I am trying to build Tensorflow for Raspberry pi using instructions given on 

https://www.tensorflow.org/install/source_rpi

I hit to following error:

make[1]: Leaving directory `/tmp/openblas_src'
Building for the Pi Two/Three, with NEON acceleration
WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
/workspace/tools/bazel.rc
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=0 --terminal_columns=80
INFO: Reading rc options for 'build' from /etc/bazel.bazelrc:
  Inherited 'common' options: --color=yes
INFO: Reading rc options for 'build' from /etc/bazel.bazelrc:
  'build' options: --verbose_failures --spawn_strategy=standalone --genrule_strategy=standalone
INFO: Reading rc options for 'build' from /workspace/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.4/dist-packages --python_path=/usr/bin/python3 --define with_jemalloc=true --define with_gcp_support=true --define with_hdfs_support=true --define with_aws_support=true --define with_kafka_support=true --action_env TF_NEED_OPENCL_SYCL=0 --action_env TF_NEED_CUDA=0 --action_env TF_DOWNLOAD_CLANG=0 --define grpc_no_ares=true
**ERROR: Config value monolithic is not defined in any .rc file**
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
INFO: Invocation ID: 4d7baf49-d76f-47e7-9bbb-f2bc6304fe21


Could you please help me on this?

Thanks..
"
24103,SparseTensor file format,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.12+
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

Currently there does not seem to be a way to save just a `SparseTensor`.
This is problematic as 1. the initialization of an instance can take a _long_ time 2. some models might require a constant sparse tensor. 

This is further supported by the fact that the Saver class is under the `train` api.

I propose a `save_variable` method under `tf.data` which can save _any_ TensorFlow native variable to a file for later retrieval.

In addition, there should be a corresponding `load_variable` method under `tf.data`. 

Notably, unlike TensorFlow Records, `save_variable` should require nothing more than two arguments, `file` (where to save the variable) and `var` (the variable to save) and `load_varriable` should require nothing more than `file`. 

_Any_ necessary information for reconstructing the variable (e.g. shape, class type) can be hashed as a header in the file.


Alternatively, provide users with a serialization function so we can use `pickle` as a short term fix.

**Will this change the current api? How?**

It adds 2 methods to `tf.data`

**Who will benefit with this feature?**

Everyone.

**Any Other info.**
"
24102,Tensorflow Prebuilt Binaries are compiled for incompatible CUDA and cuDNN libraries," Tensorflow Prebuilt Binaries are compiled for CUDA 9.0 and cuDNN 7.2.1, however, cuDNN 7.2.1 download not available for CUDA 9.0"
24101,Choosing the right Bazel version to build TF is confusing,This bug was closed while the problem still exists: https://github.com/tensorflow/tensorflow/issues/21362
24099,"NotFoundError: No registered 'PyFunc' OpKernel for CPU devices compatible with node {{node PyFunc}} = PyFunc[Tin=[], Tout=[DT_INT6..............","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): google Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: dont think its possible
- TensorFlow installed from (source or binary): preinstalled
- TensorFlow version (use command below): tensorflow               1.12.0
- Python version: 3.6
- Bazel version (if compiling from source):NA
- GCC/Compiler version (if compiling from source):NA
- CUDA/cuDNN version: 
- GPU model and memory:NA but  **TPU**

**A bit of context**  im trying  to train on mnist dataset using TPUs just as an exercise, using colab  

**Describe the current behavior**
throwing the error same as the header of this issue
```
NotFoundError: No registered 'PyFunc' OpKernel for CPU devices compatible with node {{node PyFunc}} = PyFunc[Tin=[], Tout=[DT_INT64], token=""pyfunc_3"", _device=""/job:tpu_worker/task:0/device:CPU:0""]()
	.  Registered:  <no registered kernels>

	 [[{{node PyFunc}} = PyFunc[Tin=[], Tout=[DT_INT64], token=""pyfunc_3"", _device=""/job:tpu_worker/task:0/device:CPU:0""]()]]
	 [[node input_pipeline_task0/while/IteratorGetNext (defined at /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py:3053)  = IteratorGetNext[_class=[""loc:@input_pipeline_task0/while/InfeedQueue/split/1""], output_shapes=[[32,28,28], [32]], output_types=[DT_FLOAT, DT_INT32], _device=""/job:tpu_worker/replica:0/task:0/device:CPU:0""](input_pipeline_task0/while/IteratorGetNext/Enter, ^input_pipeline_task0/while/Identity)]] 
```

**Describe the expected behavior**
it should start training, or more probably throw some other error 
honestly, everything related to TPUs on Colab is in aplha, so i was kinda expecting bugs after bugs and so it happened, nothing personal  
this is the final bug i got stuck on 

**Code to reproduce the issue**
**_here's a  jupyter notebook:_  
[ here](https://colab.research.google.com/drive/1J0Ejxn1FLxshahlbgKErLh-1u-7MPb_I)** 
but if youre feeling lazy, here you go


```
from keras.datasets import mnist
from tensorflow import data as Data
import tensorflow as tf 
import numpy as np 

data = mnist.load_data()

x_train, y_train  = data[0][0], data[0][1] 
x_test, y_test  = data[1][0], data[1][1] 

# x_train, x_test, y_train, y_test = x_train[0:12000], x_test[0:12000], y_train[0:12000], y_test[0:12000]

def gen(x=x_train , y= y_train, batch_size = 32 ):
    
    for i in range(len(x)//batch_size):
        yield (x[i*batch_size : (i+1)*batch_size], y[i*batch_size : (i+1)*batch_size])

def cinp(params):
  bs = params['batch_size'] 
  train_data = Data.Dataset.from_generator( generator= gen,
                                             output_types= ( tf.float32, tf.int32),
                                             output_shapes= ((bs, 28,28),(bs)) )
    
  return train_data.shuffle(100)
def test_cinp():
    bs = params['batch_size'] 
    train_data = Data.Dataset.from_generator( generator= gen,
                                             output_types= ( tf.float32, tf.int32),
                                             output_shapes= ((bs, 28,28),(bs)), 
                                                args = (x_test, y_test))
    return train_data.shuffle(100)

img_size = 28
num_channels = 1 

def model_fn(features, labels, mode, params):
   
    
    x = features
    net = tf.reshape(x, [-1, img_size, img_size, num_channels])    

    # First convolutional layer.
    net = tf.layers.conv2d(inputs=net, name='layer_conv1',
                           filters=16, kernel_size=5,
                           padding='same', activation=tf.nn.relu)
    net = tf.layers.max_pooling2d(inputs=net, pool_size=2, strides=2)

    # Second convolutional layer.
    net = tf.layers.conv2d(inputs=net, name='layer_conv2',
                           filters=36, kernel_size=5,
                           padding='same', activation=tf.nn.relu)
    net = tf.layers.max_pooling2d(inputs=net, pool_size=2, strides=2)    
    # Flatten to a 2-rank tensor.
    net = tf.layers.flatten(net)
    net = tf.layers.dense(inputs=net, name='layer_fc1',
                          units=128, activation=tf.nn.relu)    
    net = tf.layers.dense(inputs=net, name='layer_fc2',
                          units=10)

    # Logits output of the neural network.
    logits = net

    # Softmax output of the neural network.
    y_pred = tf.nn.softmax(logits=logits)
    
    # Classification output of the neural network.
    y_pred_cls = tf.argmax(y_pred, axis=1)

    if mode == tf.estimator.ModeKeys.PREDICT:

        spec = tf.estimator.EstimatorSpec(mode=mode,
                                          predictions=y_pred_cls)
    else:
     
        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,
                                                                       logits=logits)
        loss = tf.reduce_mean(cross_entropy)

        # Define the optimizer for improving the neural network.

        optimizer = tf.contrib.tpu.CrossShardOptimizer(
            tf.train.AdamOptimizer(learning_rate=0.002, epsilon= 0.001))

        # Get the TensorFlow op for doing a single optimization step.
        train_op = optimizer.minimize(
            loss=loss, global_step=tf.train.get_global_step())

        metrics = \
        {
            ""accuracy"": tf.metrics.accuracy(labels, y_pred_cls)
        }

        # Wrap all of this in an EstimatorSpec.
        spec = tf.contrib.tpu.TPUEstimatorSpec(
            mode=mode,
            loss=loss,
            train_op=train_op)
        
    return spec


run_config = tf.contrib.tpu.RunConfig( 
      master='grpc://'+os.environ['COLAB_TPU_ADDR'], 
     
      session_config=tf.ConfigProto(
          allow_soft_placement=True, log_device_placement=True),          
          tpu_config=tf.contrib.tpu.TPUConfig( iterations_per_loop = 100,  )
      )

runn_config = run_config.replace(save_checkpoints_secs = None)
model = tf.contrib.tpu.TPUEstimator(model_fn= model_fn, 
                                    config = runn_config  , train_batch_size= 32 , )

model.train( cinp , steps = 1000, )   #this is where error occurs
 

```
**Other info / logs**
HERE's the complete log of the error 
```
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:TPU job name tpu_worker
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Initialized dataset iterators in 0 seconds
INFO:tensorflow:Installing graceful shutdown hook.
INFO:tensorflow:Creating heartbeat manager for ['/job:tpu_worker/replica:0/task:0/device:CPU:0']
INFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR

INFO:tensorflow:Init TPU system
INFO:tensorflow:Initialized TPU in 4 seconds
INFO:tensorflow:Starting infeed thread controller.
INFO:tensorflow:Starting outfeed thread controller.
INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.
INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.
INFO:tensorflow:Error recorded from infeed: No registered 'PyFunc' OpKernel for CPU devices compatible with node {{node PyFunc}} = PyFunc[Tin=[], Tout=[DT_INT64], token=""pyfunc_3"", _device=""/job:tpu_worker/task:0/device:CPU:0""]()
	.  Registered:  <no registered kernels>

	 [[{{node PyFunc}} = PyFunc[Tin=[], Tout=[DT_INT64], token=""pyfunc_3"", _device=""/job:tpu_worker/task:0/device:CPU:0""]()]]
	 [[node input_pipeline_task0/while/IteratorGetNext (defined at /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py:3053)  = IteratorGetNext[_class=[""loc:@input_pipeline_task0/while/InfeedQueue/split/1""], output_shapes=[[32,28,28], [32]], output_types=[DT_FLOAT, DT_INT32], _device=""/job:tpu_worker/replica:0/task:0/device:CPU:0""](input_pipeline_task0/while/IteratorGetNext/Enter, ^input_pipeline_task0/while/Identity)]]
INFO:tensorflow:Error recorded from training_loop: Step was cancelled by an explicit call to `Session::Close()`.
INFO:tensorflow:training_loop marked as finished
WARNING:tensorflow:Reraising captured error
---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1333     try:
-> 1334       return fn(*args)
   1335     except errors.OpError as e:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1318       return self._call_tf_sessionrun(
-> 1319           options, feed_dict, fetch_list, target_list, run_metadata)
   1320 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)
   1406         self._session, options, feed_dict, fetch_list, target_list,
-> 1407         run_metadata)
   1408 

NotFoundError: No registered 'PyFunc' OpKernel for CPU devices compatible with node {{node PyFunc}} = PyFunc[Tin=[], Tout=[DT_INT64], token=""pyfunc_3"", _device=""/job:tpu_worker/task:0/device:CPU:0""]()
	.  Registered:  <no registered kernels>

	 [[{{node PyFunc}} = PyFunc[Tin=[], Tout=[DT_INT64], token=""pyfunc_3"", _device=""/job:tpu_worker/task:0/device:CPU:0""]()]]
	 [[{{node input_pipeline_task0/while/IteratorGetNext}} = IteratorGetNext[_class=[""loc:@input_pipeline_task0/while/InfeedQueue/split/1""], output_shapes=[[32,28,28], [32]], output_types=[DT_FLOAT, DT_INT32], _device=""/job:tpu_worker/replica:0/task:0/device:CPU:0""](input_pipeline_task0/while/IteratorGetNext/Enter, ^input_pipeline_task0/while/Identity)]]

During handling of the above exception, another exception occurred:

NotFoundError                             Traceback (most recent call last)
<ipython-input-37-ec76835859c0> in <module>()
----> 1 model.train( cinp , steps = 1000, )

/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)
   2407     finally:
   2408       rendezvous.record_done('training_loop')
-> 2409       rendezvous.raise_errors()
   2410 
   2411   def evaluate(self, input_fn, steps=None, hooks=None, checkpoint_path=None,

/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/error_handling.py in raise_errors(self, timeout_sec)
    126       else:
    127         logging.warn('Reraising captured error')
--> 128         six.reraise(typ, value, traceback)
    129 
    130     for k, (typ, value, traceback) in kept_errors:

/usr/local/lib/python3.6/dist-packages/six.py in reraise(tp, value, tb)
    691             if value.__traceback__ is not tb:
    692                 raise value.with_traceback(tb)
--> 693             raise value
    694         finally:
    695             value = None

/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/error_handling.py in catch_errors(self, source, session)
     99     """"""Context manager to report any errors within a block.""""""
    100     try:
--> 101       yield
    102     except Exception:  # pylint: disable=broad-except
    103       self.record_error(source, sys.exc_info(), session)

/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py in _run_infeed(self, queue_ctx, session)
    440       else:
    441         for _ in queue_ctx.read_iteration_counts():
--> 442           session.run(self._enqueue_ops)
    443       logging.info('Infeed thread finished, shutting down.')
    444 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    927     try:
    928       result = self._run(None, fetches, feed_dict, options_ptr,
--> 929                          run_metadata_ptr)
    930       if run_metadata:
    931         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1150     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1151       results = self._do_run(handle, final_targets, final_fetches,
-> 1152                              feed_dict_tensor, options, run_metadata)
   1153     else:
   1154       results = []

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1326     if handle is None:
   1327       return self._do_call(_run_fn, feeds, fetches, targets, options,
-> 1328                            run_metadata)
   1329     else:
   1330       return self._do_call(_prun_fn, handle, feeds, fetches)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1346           pass
   1347       message = error_interpolation.interpolate(message, self._graph)
-> 1348       raise type(e)(node_def, op, message)
   1349 
   1350   def _extend_graph(self):

NotFoundError: No registered 'PyFunc' OpKernel for CPU devices compatible with node {{node PyFunc}} = PyFunc[Tin=[], Tout=[DT_INT64], token=""pyfunc_3"", _device=""/job:tpu_worker/task:0/device:CPU:0""]()
	.  Registered:  <no registered kernels>

	 [[{{node PyFunc}} = PyFunc[Tin=[], Tout=[DT_INT64], token=""pyfunc_3"", _device=""/job:tpu_worker/task:0/device:CPU:0""]()]]
	 [[node input_pipeline_task0/while/IteratorGetNext (defined at /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py:3053)  = IteratorGetNext[_class=[""loc:@input_pipeline_task0/while/InfeedQueue/split/1""], output_shapes=[[32,28,28], [32]], output_types=[DT_FLOAT, DT_INT32], _device=""/job:tpu_worker/replica:0/task:0/device:CPU:0""](input_pipeline_task0/while/IteratorGetNext/Enter, ^input_pipeline_task0/while/Identity)]]
`````

"
24096,Build failure with Ubuntu 18.04,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version: f1edf205b290c7fdeefe0b73d27c50a3cab67dcb
- Python version: Python 2.7.15rc1
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): Build label: 0.19.1
- GCC/Compiler version (if compiling from source): gcc version 7.3.0 (Ubuntu 7.3.0-27ubuntu1~18.04) 
- CUDA/cuDNN version: n/a
- GPU model and memory:n/a



**Describe the problem**

While trying to build tf from source (master branch) on Ubuntu 18.04, the following error surfaces:
```
ERROR: /home/ubuntu/tensorflow/tensorflow/core/BUILD:2211:1: C++ compilation of rule '//tensorflow/core:lib_internal_impl' failed (Exit 1)
In file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:150:0,
                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/lib/random/random_distributions.h:28,
                 from ./tensorflow/core/lib/random/simple_philox.h:24,
                 from ./tensorflow/core/lib/random/distribution_sampler.h:38,
                 from tensorflow/core/lib/random/random_distributions.cc:16:
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h: In instantiation of 'TgtPacket Eigen::internal::pcast(const SrcPacket&) [with SrcPacket = __vector(2) long long int; TgtPacket = __vector(4) float]':
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/SSE/../Default/GenericPacketMathFunctions.h:25:40:   required from 'Packet Eigen::internal::pfrexp_float(const Packet&, Packet&) [with Packet = __vector(4) float]'
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/SSE/PacketMath.h:574:33:   required from here
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h:140:10: error: invalid static_cast from type 'const __vector(2) long long int' to type '__vector(4) float'
   return static_cast<TgtPacket>(a);
          ^~~~~~~~~~~~~~~~~~~~~~~~~
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h: In instantiation of 'TgtPacket Eigen::internal::pcast(const SrcPacket&) [with SrcPacket = __vector(4) float; TgtPacket = __vector(2) long long int]':
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/SSE/../Default/GenericPacketMathFunctions.h:35:37:   required from 'Packet Eigen::internal::pldexp_float(Packet, Packet) [with Packet = __vector(4) float]'
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/SSE/PacketMath.h:578:33:   required from here
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h:140:10: error: invalid static_cast from type 'const __vector(4) float' to type '__vector(2) long long int'
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h: In instantiation of 'TgtPacket Eigen::internal::pcast(const SrcPacket&) [with SrcPacket = __vector(4) long long int; TgtPacket = __vector(8) float]':
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/SSE/../Default/GenericPacketMathFunctions.h:25:40:   required from 'Packet Eigen::internal::pfrexp_float(const Packet&, Packet&) [with Packet = __vector(8) float]'
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/AVX/PacketMath.h:423:33:   required from here
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h:140:10: error: invalid static_cast from type 'const __vector(4) long long int' to type '__vector(8) float'
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h: In instantiation of 'TgtPacket Eigen::internal::pcast(const SrcPacket&) [with SrcPacket = __vector(8) float; TgtPacket = __vector(4) long long int]':
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/SSE/../Default/GenericPacketMathFunctions.h:35:37:   required from 'Packet Eigen::internal::pldexp_float(Packet, Packet) [with Packet = __vector(8) float]'
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/AVX/PacketMath.h:427:33:   required from here
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h:140:10: error: invalid static_cast from type 'const __vector(8) float' to type '__vector(4) long long int'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 173.313s, Critical Path: 20.46s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]
INFO: 250 processes: 250 local.
FAILED: Build did NOT complete successfully
```
**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
$ ./configure # with default options
$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
```"
24095,Missing memmapped_file_system for Windows BUILD," 

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA
- TensorFlow installed from (source or binary): Source
- TensorFlow version: r1.12
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.17.1
- GCC/Compiler version (if compiling from source): msvc 
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the problem**
I'm building DeepSpeech for Windows that is using tensorflow r1.12, it uses memmapped code that is causing undeclared errors due to the exclusion of memmapped_file_system.h for Windows [Here](https://github.com/mozilla/tensorflow/blob/bea86c1e884730cf7f8615eb24d31872c198c766/tensorflow/core/BUILD#L2482), if I remove the empty select for Windows it compiles.
This a possible bug or there is a reason for excluding those files for Windows BUILD?

I posted on StackOverflow but no one answered.


 "
24094,Profiling is not working when two machines run with FULL_TRACE,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
None
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
 Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
None
- TensorFlow version (use command below):
1.11
- Python version:
2.7.12
- Bazel version (if compiling from source):
0.15.0
- GCC/Compiler version (if compiling from source):
5.4.0 
- CUDA/cuDNN version:
9.0, 7.0.5
- GPU model and memory:
TitanXP, 16GB

**Describe the current behavior**
I've added GPU tracing for distributed training to get more accurate GPU trace.
Profiling is not working when two machines(workers) run with FULL_TRACE in a distributed mode(PS). However, if only one machine tries it then it works.  I've seen this line (https://github.com/tensorflow/tensorflow/blob/284b2783c4676c9d00199a585db9d6d07d9e68be/tensorflow/core/platform/default/device_tracer.cc#L452). Is it related to this issue? Only one process can profile for a distributed job?

**Describe the expected behavior**
Profiling works for both machines.

**Code to reproduce the issue**
This is the TF code that I modified for GPU tracing in a distributed mode.
https://github.com/snuspl/tensorflow/tree/r1.11

**Other info / logs**
Processes are hanged after the log of ""successfully opened CUDA library libcupti.so.9.0 locally"".

```
INFO:139724566263552:PARALLAX:global step: 300, loss: 8.826963, throughput: 2.748203 steps/sec
INFO:139838103807744:PARALLAX:global step: 300, loss: 8.755314, throughput: 2.748064 steps/sec
INFO:139724566263552:PARALLAX:global step: 350, loss: 8.723333, throughput: 2.733742 steps/sec
INFO:139838103807744:PARALLAX:global step: 350, loss: 8.800816, throughput: 2.733653 steps/sec
INFO:139724566263552:PARALLAX:global step: 400, loss: 8.311481, throughput: 2.760322 steps/sec
INFO:139838103807744:PARALLAX:global step: 400, loss: 8.774604, throughput: 2.757400 steps/sec
2018-12-01 23:56:29.950572: I tensorflow/stream_executor/dso_loader.cc:151] successfully opened CUDA library libcupti.so.9.0 locally
2018-12-01 23:56:29.952852: I tensorflow/stream_executor/dso_loader.cc:151] successfully opened CUDA library libcupti.so.9.0 locally
2018-12-01 23:56:33.335883: I tensorflow/stream_executor/dso_loader.cc:151] successfully opened CUDA library libcupti.so.9.0 locally
2018-12-01 23:56:33.338704: I tensorflow/stream_executor/dso_loader.cc:151] successfully opened CUDA library libcupti.so.9.0 locally

```

"
24093,quant_graph caonvert to .tflite:  Converting unsupported operation: Dequantize,"I want to know where is wrong, please someone help me.

version:
Python 2.7.12 (default, Nov 12 2018, 14:36:49)

>>> import tensorflow as tf
>>> tf.__version__
'1.13.0-dev20181201'
>>> 

log:
super@super-virtual-machine:~/Downloads$ tflite_convert \
>   --output_file=/home/super/Downloads/mobilenet_v1_0.50_128/mobilenet_v1.tflite \
>   --graph_def_file=/home/super/Downloads/mobilenet_v1_0.50_128/quantized_graph.pb \
>   --inference_type=QUANTIZED_UINT8 \
>   --input_arrays=input \
>   --output_arrays=MobilenetV1/Predictions/Reshape_1 \
>   --mean_values=128 \
>   --std_dev_values=127
2018-12-01 22:45:07.489914: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Traceback (most recent call last):
  File ""/usr/local/bin/tflite_convert"", line 11, in <module>
    sys.exit(main())
  File ""/home/super/.local/lib/python2.7/site-packages/tensorflow/lite/python/tflite_convert.py"", line 442, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/home/super/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/home/super/.local/lib/python2.7/site-packages/tensorflow/lite/python/tflite_convert.py"", line 438, in run_main
    _convert_model(tflite_flags)
  File ""/home/super/.local/lib/python2.7/site-packages/tensorflow/lite/python/tflite_convert.py"", line 191, in _convert_model
    output_data = converter.convert()
  File ""/home/super/.local/lib/python2.7/site-packages/tensorflow/lite/python/lite.py"", line 455, in convert
    **converter_kwargs)
  File ""/home/super/.local/lib/python2.7/site-packages/tensorflow/lite/python/convert.py"", line 430, in toco_convert_impl
    input_data.SerializeToString())
  File ""/home/super/.local/lib/python2.7/site-packages/tensorflow/lite/python/convert.py"", line 204, in toco_convert_protos
    ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
2018-12-01 22:45:08.455379: I tensorflow/lite/toco/import_tensorflow.cc:1299] Converting unsupported operation: Dequantize
2018-12-01 22:45:08.471418: I tensorflow/lite/toco/import_tensorflow.cc:1299] Converting unsupported operation: Dequantize
2018-12-01 22:45:08.476136: I tensorflow/lite/toco/import_tensorflow.cc:1299] Converting unsupported operation: Dequantize
2018-12-01 22:45:08.476205: I tensorflow/lite/toco/import_tensorflow.cc:1299] Converting unsupported operation: Dequantize
2018-12-01 22:45:08.476247: I tensorflow/lite/toco/import_tensorflow.cc:1299] Converting unsupported operation: Dequantize
2018-12-01 22:45:08.476287: I tensorflow/lite/toco/import_tensorflow.cc:1299] Converting unsupported operation: Dequantize
2018-12-01 22:45:08.476516: I tensorflow/lite/toco/import_tensorflow.cc:1299] Converting unsupported operation: Dequantize
2018-12-01 22:45:08.477495: I tensorflow/lite/toco/import_tensorflow.cc:1299] Converting unsupported operation: Dequantize
2018-12-01 22:45:08.481725: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 138 operators, 226 arrays (0 quantized)
2018-12-01 22:45:08.497846: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 138 operators, 226 arrays (0 quantized)
2018-12-01 22:45:08.499427: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 57 operators, 132 arrays (1 quantized)
2018-12-01 22:45:08.506318: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 57 operators, 132 arrays (1 quantized)
2018-12-01 22:45:08.506970: F tensorflow/lite/toco/tooling_util.cc:1698] Array MobilenetV1/MobilenetV1/Conv2d_0/Relu6, which is an input to the DepthwiseConv operator producing the output array MobilenetV1/MobilenetV1/Conv2d_1_depthwise/Relu6, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.
Aborted (core dumped)

"
24091,how to open model.cpkt in ubuntu,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
24083,Constant folding doesn't fold weights that are using weight normalization ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.12.0
- Python version: 3.5.2
- Bazel version (if compiling from source): 0.15.0?
- GCC/Compiler version (if compiling from source):  5.4.0
- CUDA/cuDNN version: CUDA 10.0.130, CUDNN 7.4.1.5
- GPU model and memory: TITAN V 10956 MB

**Describe the current behavior**
I have a 1D convolution layer whose weights are using weight normalization (Salimans & Kingma, 2016). The weights to the convolution are processed using the following equation: `w = g * v/2-norm(v)`. After running the constfold optimizer, these operations on the weights are still present, even though they should be folded.

**Describe the expected behavior**
I would expect the normalization ops to be folding into the weights, so that only one Const/read node remains.

**Code to reproduce the issue**
```python
import tensorflow as tf
import math
from tensorflow.core.protobuf import meta_graph_pb2
from tensorflow.core.protobuf import rewriter_config_pb2
from tensorflow.python.grappler import tf_optimizer

def conv_graph(x, output_name='output', kernel_width=3, in_dim=1024, out_dim=1024):
  """"""Applies convolution with gated linear units on x.
  https://github.com/NVIDIA/OpenSeq2Seq/blob/master/open_seq2seq/parts/convs2s/conv_wn_layer.py
    Args:
      x: A float32 tensor with shape [batch_size, length, in_dim]
    Returns:
      float32 tensor with shape [batch_size, length, out_dim].
  """"""
  # Define Variables
  conv_out_size = 2 * out_dim
  V_std = math.sqrt(4.0 * 0.8 / (kernel_width * in_dim))
  V = tf.get_variable('V', shape=[kernel_width, in_dim, conv_out_size],
                      initializer=tf.random_normal_initializer(mean=0, stddev=V_std),
                      trainable=True)
  V_norm = tf.norm(V.initialized_value(), axis=[0, 1])
  g = tf.get_variable('g', initializer=V_norm, trainable=True)
  W = tf.reshape(g, [1, 1, conv_out_size]) * tf.nn.l2_normalize(V, [0, 1])

  output = tf.nn.conv1d(value=x, filters=W, stride=1, padding=""VALID"")
  output = tf.identity(output, name=output_name)
  return output

def apply_constfold(frozen_graph, output_nodes):
  graph = tf.Graph()
  with graph.as_default():
    tf.import_graph_def(frozen_graph, name="""")
  grappler_meta_graph_def = tf.train.export_meta_graph(graph_def=graph.as_graph_def(add_shapes=True), graph=graph)

  _to_bytes = lambda s: s.encode(""utf-8"", errors=""surrogateescape"")
  output_collection = meta_graph_pb2.CollectionDef()
  output_list = output_collection.node_list.value
  for i in output_nodes:
    if isinstance(i, tf.Tensor):
      output_list.append(_to_bytes(i.name))
    else:
      output_list.append(_to_bytes(i))
  # TODO(laigd): use another key as the outputs are really not train_op.
  grappler_meta_graph_def.collection_def[""train_op""].CopyFrom(output_collection)
  rewriter_config = rewriter_config_pb2.RewriterConfig()
  rewriter_config.optimizers.extend([""constfold""])

  session_config_with_trt = tf.ConfigProto()
  session_config_with_trt.graph_options.rewrite_options.CopyFrom(
      rewriter_config)
  frozen_graph = tf_optimizer.OptimizeGraph(session_config_with_trt, grappler_meta_graph_def, graph_id=b""tf_graph"")
  return frozen_graph

if __name__ == '__main__':
  with tf.Graph().as_default():
    # Create graph
    x = tf.placeholder(dtype=tf.float32, shape=(None, None, 1024), name='input')
    y = conv_graph(x)
    # Initialize
    with tf.Session() as sess:
      sess.run(tf.global_variables_initializer())
      # Freeze graph
      frozen_graph = tf.graph_util.convert_variables_to_constants(
          sess,
          sess.graph_def,
          output_node_names=['output'])

  print('Nodes before:')
  [print(n.name, n.op) for n in frozen_graph.node]

  # const folding
  frozen_graph = apply_constfold(frozen_graph, output_nodes=['output'])

  print('----------------------------------------')
  print('Nodes after:')
  [print(n.name, n.op) for n in frozen_graph.node]
```
**Output of script**
```
Nodes before:
input
V
V/read
g
g/read
Reshape/shape
Reshape
l2_normalize/Square
l2_normalize/Sum/reduction_indices
l2_normalize/Sum
l2_normalize/Maximum/y
l2_normalize/Maximum
l2_normalize/Rsqrt
l2_normalize
mul
conv1d/ExpandDims/dim
conv1d/ExpandDims
conv1d/ExpandDims_1/dim
conv1d/ExpandDims_1
conv1d/Conv2D
conv1d/Squeeze
output
----------------------------------------
Nodes after:
input
V
Reshape
l2_normalize/Sum/reduction_indices
l2_normalize/Maximum/y
conv1d/ExpandDims/dim
conv1d/ExpandDims_1/dim
V/read
conv1d/ExpandDims
l2_normalize/Square
l2_normalize/Sum
l2_normalize/Maximum
l2_normalize/Rsqrt
l2_normalize
mul
conv1d/ExpandDims_1
conv1d/Conv2D
conv1d/Squeeze
output
```

Edit: updated repro for 1.13"
24082,proto,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
24079,Inconsistent keras API regarding variable_scope,"I am trying to reuse some layers (share the weights), so I've used some variable_scope and inside I created some keras layers. For some type of layers like tf.keras.layers.Dense, the weights are shared, for others like tf.keras.layers.Embedding for example new weights are created instead of being reused.

Probably similar to https://github.com/tensorflow/tensorflow/issues/20426 and https://github.com/tensorflow/tensorflow/issues/14703 

Can you advise a fix as it seems variable_scope are being deprecated in TF2.0 ?

Thanks

```
import tensorflow as tf
import numpy as np

def mlp( s ):
    word_embeddings = tf.get_variable(""we"",[3, 10])
    emb = tf.nn.embedding_lookup(word_embeddings, s)
    return emb

def mlpkeras( s ):
    emb = tf.keras.layers.Embedding( 3, 10)(s)
    return emb

def mlp2( s ):
    emb = tf.keras.layers.Dense(10)(s)
    return emb

def testEmbeddingTFLayer():
    sess = tf.Session()
    input = tf.placeholder(tf.int32, ((None,20)))

    with (tf.variable_scope(""main"", reuse=False) ):
        res1 = mlp(input)

    with (tf.variable_scope(""main"", reuse=True) ):
        res2 = mlp(input)

    sess.run(tf.global_variables_initializer())
    print(""TestEmbeddingTFLayer :"")
    print( sess.run( [tf.reduce_sum( (res1-res2)**2)] , feed_dict= {input:np.zeros((1,20)) } ) ) 

def testDenseLayer():
    sess = tf.Session()
    input = tf.placeholder(tf.float32, ((None,20)))

    with (tf.variable_scope(""main"", reuse=False) ):
        res1 = mlp2(input)

    with (tf.variable_scope(""main"", reuse=True) ):
        res2 = mlp2(input)

    sess.run(tf.global_variables_initializer())
    print(""TestDenseLayer : "")
    print( sess.run( [tf.reduce_sum( (res1-res2)**2)] , feed_dict= {input:np.zeros((1,20)) } ) ) 


def testEmbeddingKerasLayer():
    sess = tf.Session()
    input = tf.placeholder(tf.int32, ((None,20)))

    with (tf.variable_scope(""main"", reuse=False) ):
        res1 = mlpkeras(input)

    with (tf.variable_scope(""main"", reuse=True) ):
        res2 = mlpkeras(input)

    sess.run(tf.global_variables_initializer())
    print(""TestEmbeddingKerasLayer :"")
    print( sess.run( [tf.reduce_sum( (res1-res2)**2)] , feed_dict= {input:np.zeros((1,20)) } ) ) 



testEmbeddingTFLayer() 
testDenseLayer()
testEmbeddingKerasLayer()
```
Prints 0 , 0 , and not 0 (expecting 0)


<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):v1.12.0-0-ga6d8ffae09 1.12.0
- Python version: 3.6
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: CUDA Driver Version / Runtime Version          9.2 / 8.0
- GPU model and memory: 1080Ti


"
24077,Tensorflow on AMD G.P.U when will you support??,"I love tensorflow but i have an amd GPU and training my models on my cpu is tedious 
when will you support amd or will it ever happen"
24076,CMake build Tensorflow C++ on Windows 10 Error : Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED - failed,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): source
- TensorFlow version: the latest version
- Python version: 3.6
- Bazel version (if compiling from source): I am using CMake
- GCC/Compiler version (if compiling from source): gcc 6.30
- CUDA/cuDNN version: Installing only the CPU version
- I am also using Microsoft Visual Studio Community 2017 Version 15.9.3, btw



**Describe the problem**
I am following the ""Step by step Windows Build"" from https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake. However, at step 3, I can't seem to pass `Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED`, which resulting failed building process. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
PS C:\Users\bw\tensorflow\tensorflow\contrib\cmake\build> cmake .. -A x64 -Thost=x64 -DCMAKE_BUILD_TYPE=Release `
>> -DSWIG_EXECUTABLE='C:\Program Files\swigwin-3.0.12\swig.exe' `
>> -DPYTHON_EXECUTABLE='C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\python.exe' `
>> -DPYTHON_LIBRARIES='C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\libs\python36.lib'
-- Building for: Visual Studio 15 2017
CMake Warning at CMakeLists.txt:9 (message):
  Your current cmake generator is set to use 32 bit toolset architecture.
  This may cause ""compiler out of heap space"" errors when building.  Consider
  using the flag -Thost=x64 when running cmake.


-- The C compiler identification is MSVC 19.16.27024.1
-- The CXX compiler identification is MSVC 19.16.27024.1
-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/Hostx64/x64/cl.exe
-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/Hostx64/x64/cl.exe -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/Hostx64/x64/cl.exe
-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/Hostx64/x64/cl.exe -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED
-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED - Failed
-- Performing Test MSVC_OPENMP_SUPPORT
-- Performing Test MSVC_OPENMP_SUPPORT - Success
-- Found PythonInterp: C:/Program Files (x86)/Microsoft Visual Studio/Shared/Python36_64/python.exe (found version ""3.6.7"")
-- Found PythonLibs: optimized;C:/Program Files (x86)/Microsoft Visual Studio/Shared/Python36_64/libs/python36.lib;debug;C:/Program Files (x86)/Microsoft Visual Studio/Shared/Python36_64/libs/python36_d.lib (found version ""3.6.7"")
-- Found SWIG: C:/Program Files/swigwin-3.0.12/swig.exe (found version ""3.0.12"")
CMake Error at tf_python.cmake:811 (string):
  string sub-command REPLACE requires at least four arguments.
Call Stack (most recent call first):
  CMakeLists.txt:583 (include)


CMake Error at tf_python.cmake:812 (string):
  string sub-command REPLACE requires at least four arguments.
Call Stack (most recent call first):
  CMakeLists.txt:583 (include)


CMake Error at tf_python.cmake:813 (string):
  string sub-command REPLACE requires at least four arguments.
Call Stack (most recent call first):
  CMakeLists.txt:583 (include)


-- Configuring incomplete, errors occurred!
See also ""C:/Users/bw/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/CMakeOutput.log"".
See also ""C:/Users/bw/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/CMakeError.log"".
```

**Any other info / logs**
CMakeOutput.log: https://www.dropbox.com/s/7fweyunxdbmxa1k/CMakeOutput.log?dl=0
CMakeError.log: https://www.dropbox.com/s/tucx0tl6346kdpd/CMakeError.log?dl=0

"
24075,steps_per_epoch not honored in tf.keras.fit,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): ('v1.12.0-0-ga6d8ffae09', '1.12.0')
- Python version: 2.7.12
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: libcudnn7_7.4.1.5-1+cuda9.0
- GPU model and memory:
Tesla P100-PCIE-12GB

**Describe the current behavior**

```
mnist_model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(filters=35,kernel_size=(3,3), strides=(1,1), padding='same', 
                           activation='relu', input_shape = (1, 28, 28), data_format=""channels_first"",
                           use_bias=True, bias_initializer=tf.keras.initializers.constant(0.01), 
                           kernel_initializer='glorot_normal'),
#     tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPool2D(pool_size=(2,2), padding='same', data_format='channels_first'),
    tf.keras.layers.Conv2D(filters=36,kernel_size=(3,3), strides=(1,1), padding='same', 
                           activation='relu', data_format=""channels_first"", use_bias=True,
                           bias_initializer=tf.keras.initializers.constant(0.01), kernel_initializer='glorot_normal'),
#     tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPool2D(pool_size=(2,2), padding='same', data_format='channels_first'),
    tf.keras.layers.Conv2D(filters=36,kernel_size=(3,3), strides=(1,1), padding='same',
                           activation='relu', data_format=""channels_first"", use_bias=True,
                           bias_initializer=tf.keras.initializers.constant(0.01), kernel_initializer='glorot_normal'),
#     tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPool2D(pool_size=(2,2), padding='same', data_format='channels_first'),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(576, activation='relu'),
    tf.keras.layers.Dense(10, activation='relu')
]) 
```

this model, when fitted with
```
(mnist_images, mnist_labels), _ = tf.keras.datasets.mnist.load_data()
dataset = tf.data.Dataset.from_tensor_slices(
  (tf.cast(mnist_images[...,tf.newaxis]/255, tf.float16),
   tf.cast(mnist_labels,tf.int8)))
dataset = dataset.shuffle(1000)
mnist_images = tf.convert_to_tensor(np.expand_dims(mnist_images, axis = 1))
mnist_model.compile(optimizer=tf.keras.optimizers.Adam(), loss=""categorical_crossentropy"", metrics=['accuracy'])
mnist_model.fit(mnist_images, tf.one_hot(mnist_labels, depth=10), epochs=2, steps_per_epoch=100)
```

generates error:
```
ResourceExhaustedError: OOM when allocating tensor with shape[60000,35,28,28] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node conv2d_19/Conv2D}} = Conv2D[T=DT_FLOAT, _class=[""loc:@training_6/Adam/gradients/conv2d_19/Conv2D_grad/Conv2DBackpropFilter""], data_format=""NCHW"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](_identity_conv2d_19_input_0, conv2d_19/Conv2D/ReadVariableOp)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

	 [[{{node ConstantFoldingCtrl/loss_6/dense_13_loss/broadcast_weights/assert_broadcastable/AssertGuard/Switch_0/_912}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_324_C...d/Switch_0"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
```

**Describe the expected behavior**
Batch size is 600000 / 100 = 6000, i.e., Keras should not be allocating tensors with shape [60000,35,28,28]. The steps_per_epoch param is not honored.

**Code to reproduce the issue**
See above.

**Other info / logs**
NA"
24074,"""Keyword argument not understood"" - Raspberry Pi","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspberry Pi 3B+ (latest dist)
- TensorFlow installed from (source or binary): pip
- TensorFlow version (or github SHA if from source): 1.11.0 (latest from pip)
- Keras version: 2.2.4 (latest from pip)

Hello,
I am trying to accelerate a CNN model in order to run it in python and on my raspberry pi. I've tried using tensorflow lite as a solution to this but I still have no success in converting my keras model to a lite one and use it afterwards. I've tried tflite_convert on the command line but I get the error quoted below. I've also tried using the TocoConverter inside python but I get the same error. Could you help me?

**Provide the text output from tflite_convert**

```
raise TypeError('Keyword argument not understood:' , kwarg)
TypeError: ('Keyword argument not understood:' , u'output_padding')
```
"
24073,TPU runs as slow as CPU when using keras_to_tpu_model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): [run in colab](https://colab.research.google.com)
- TensorFlow installed from (source or binary):[Pre-installed in colab](https://colab.research.google.com)
- TensorFlow version (use command below):[1.12.0 Pre-installed in colab](https://colab.research.google.com)
- Python version:3.6.7 (default, Oct 22 2018, 11:32:17) [GCC 8.2.0]
- CUDA/cuDNN version:
- GPU model and memory: TPU


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
`v1.12.0-0-ga6d8ffae09 1.12.0`
**Describe the current behavior**
I use tf.contrib.tpu.keras_to_tpu_model to make my code be able to run on TPU,but it took 170 hours to finish an epoch while CPU took the same time and GPU took only 40 hours per epoch.I tried to adjust batch size but nothing changed.And I've tested the input function may take up 20% of the run time when running on GPU, so I think it's maybe not the main reason.
**Describe the expected behavior**
Run faster than GPU

**Code to reproduce the issue**
Here is my code:[https://github.com/WangHexie/DHNE/blob/master/src/hypergraph_embedding.py](https://github.com/WangHexie/DHNE/blob/master/src/hypergraph_embedding.py)
Run on colab:         
 1. TPU:[https://colab.research.google.com/gist/WangHexie/30c385509f9cd93be747f04c39f039a4/tpu-error.ipynb](https://colab.research.google.com/gist/WangHexie/30c385509f9cd93be747f04c39f039a4/tpu-error.ipynb)       
 2. GPU[https://colab.research.google.com/gist/WangHexie/5bfac53bf92ef0ad527f15ddbf8705e1/-gpu-ipynb.ipynb](https://colab.research.google.com/gist/WangHexie/5bfac53bf92ef0ad527f15ddbf8705e1/-gpu-ipynb.ipynb)

"
24072,Bad broadcasting when multiplying sparse and dense tensors,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): b'v1.12.0-rc2-3-ga6d8ffae09' 1.12.0
- Python version: 3.6.7 Anaconda 64bit
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**

When multiplying (`*` operator) a sparse tensor `S` with rank `N` with a dense tensor `D` with rank `M`, where `N > M` and `S` and `D` have different but broadcast-compatible shapes, then TensorFlow attempts to perform the multiplication producing and incorrect sparse tensor as a result with the shape of `S`.

**Describe the expected behavior**

TensorFlow should refuse to perform a sparse-dense multiplication that requires broadcasting. This is the case when both tensors have the same rank or the rank of the sparse tensor is less than the rank of the dense tensor, but not when the rank of the sparse tensor is greater.

**Code to reproduce the issue**

```py
import tensorflow as tf

with tf.Graph().as_default(), tf.Session() as sess:
    a = tf.reshape(tf.range(12), [3, 4, 1])
    b = tf.SparseTensor([[0, 0, 1, 0], [0, 0, 3, 0]], [10, 20], [1, 1, 4, 2])
    c = a * b
    print(c.shape)
    # (1, 1, 4, 2)
    print(sess.run(tf.sparse.to_dense(c)))
    # [[[[ 0  0]
    #    [10  0]
    #    [ 0  0]
    #    [60  0]]]]
```

**Other info / logs**
NA"
24070,How to restructure a tensor according to one dimension.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Centos 7
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:no
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.7.0
- **Python version**:3.5.5
- **Bazel version (if compiling from source)**:no
- **GCC/Compiler version (if compiling from source)**:4.9.3
- **CUDA/cuDNN version**:no
- **GPU model and memory**:2 same GTX Titan X (Pascal), 12GB
- **Exact command to reproduce**:


### Describe the problem

How to restructure a tensor according to one dimension specified by user?

The paper ""An End-to-End Deep Learning Architecture for Graph Classification"" published in AAAI-18([PDF](http://www.cse.wustl.edu/~muhan/papers/AAAI_2018_DGCNN.pdf)) proposed a new novel neural network architecture for `graph` classification and a new layer called ""SortPooling"" as a key block of the network. The author of this paper implement the network by `torch`, i want to reproduce by tensorflow. One of the key operation is to restructure a tensor by the descend value according to some one dimension.

e.g.

assume the original tensor `a` is 
```python
[[93, 69, 38, 46]
[54, 5, 1, 13]
[96, 45, 52, 50]]
```
and we want to restructure the tensor above according to the `a[:, 3]`, so output:
```python
[54, 5, 1, 13]
[[93, 69, 38, 46]
[96, 45, 52, 50]]
```
Then remove the last row(if two line already enough), so the final result i want is:
```python
[[54, 5, 1, 13]
[93, 69, 38, 46]]
```

How to achieve this by tensorflow? Any funciton exist? the `tf.nn.top_k` seem not enough for this.

### Source code / logs
no
"
24069,TensorFlow Architecture low level intro link broken,"Link:
https://www.tensorflow.org/guide/extend/architecture

In the above link, an introduction link is provided. Its broken.
https://www.tensorflow.org/guide/guide/low_level_intro

"
24067,max_pool_with_argmax: resulting indices dependent on data type,"Hello!  

When testing code using the function tf.nn.max_pool_with_argmax, I found that:
- when the input tensor is tf.constant: the reported indices are correct  
- when the input tensor is tf.Variable: the reported indices are calculated per batch-dimension (*wrong*)

### Example code:
```
import numpy as np
import tensorflow as tf 

# Create a tensor with KNOWN max-pooling output:
A = np.zeros((4, 4), dtype = np.float32)
A[0,0] = 1.1
A[1,2] = 1.2
A[2,1] = 1.4
A[3,3] = 1.3

B = np.zeros((4, 4), dtype = np.float32)
B[1,0] = 1.9
B[2,1] = 1.8
B[0,2] = 1.7
B[3,2] = 1.6

total_stack = np.zeros((3, 4, 4, 2), dtype = np.float32 )
total_stack[0,:,:,0] = A 
total_stack[0,:,:,1] = B 
total_stack[1,:,:,0] = A
total_stack[1,:,:,1] = B
total_stack[2,:,:,0] = A
total_stack[2,:,:,1] = B 

# Wrap all in a two tensors with different data types:
test_constant = tf.constant( total_stack, dtype = tf.float32, name = 'constant'  )
test_variable = tf.Variable( total_stack, name = 'variable')

constant_values, constant_indices = tf.nn.max_pool_with_argmax( test_constant, [1, 2, 2, 1], [1, 2, 2, 1], padding = 'SAME')
variable_values, variable_indices = tf.nn.max_pool_with_argmax( test_variable, [1, 2, 2, 1], [1, 2, 2, 1], padding = 'SAME')

# Setup a session and initialize variables
model = tf.global_variables_initializer()
session = tf.InteractiveSession()
session.run( model )     

# Print the results on screen
print(""Constant -> CORRECT"")
print(session.run( constant_indices ))
print(""Variable -> WRONG"")
print(session.run( variable_indices ))
```

The output is: 
- In case of data type is tf.constant, the behavior is as expected:
```
[[[[ 0  9]
   [12  5]]

  [[18 19]
   [30 29]]]


 [[[32 41]
   [44 37]]

  [[50 51]
   [62 61]]]


 [[[64 73]
   [76 69]]

  [[82 83]
   [94 93]]]]
```
Note that the indices are unique and go from 0 to 94 to span all dimensions.
- In case the data type is tf.Variable, the results are *wrong*: indices are repeated per batch dimension: 
```
[[[[ 0  9]
   [12  5]]

  [[18 19]
   [30 29]]]


 [[[ 0  9]
   [12  5]]

  [[18 19]
   [30 29]]]


 [[[ 0  9]
   [12  5]]

  [[18 19]
   [30 29]]]]
```
Note that the sequence of indices are repeated 3 times, as the batch dimension is 3. This is *NOT* the expected behavior of the function.

#### A fix would be highly appreciated....

Kind regards,

Tokin256


| Configuration | Version |
|----------------|----------|
| Platform         |  Windows 7 (64) |
| Python version | 3.6.4 (x64) |
| IPython version | 6.2.1 |
| TensorFlow | 1.10.0 (GPU) |
| CUDA | V9.0.176|
| GPU |  Nvidia M2000M GPU (2GB)|

"
24066,Android demo build fails with bazel,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
source
- TensorFlow version:
1.12
- Python version:
2.7.15
- Installed using virtualenv? pip? conda?:
pip
- Bazel version (if compiling from source):
0.15.1
- GCC/Compiler version (if compiling from source):
5.5
- CUDA/cuDNN version:
9.2 / 7.1.3
- GPU model and memory:
Nvidia GTX 1080


**Describe the problem**
When trying to build android demo with bazel following the  [README]( https://github.com/tensorflow/tensorflow/tree/r1.12/tensorflow/examples/android) 
Build command with bazel fails.
Thank you for your help. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
(cuda_env)  jean@pc:~/tensorflow$ bazel build //tensorflow/examples/android:tensorflow_demo
(full details of output in logs)
 ...
fatal error: too many errors emitted, stopping now [-ferror-limit=]
11 warnings and 20 errors generated.
Target //tensorflow/examples/android:tensorflow_demo failed to build
INFO: Elapsed time: 78.831s, Critical Path: 18.32s
INFO: 304 processes: 303 local, 1 worker.
FAILED: Build did NOT complete successfully


**Any other info / logs**
[log.txt](https://github.com/tensorflow/tensorflow/files/2632785/log.txt)


"
24065,"InvalidArgumentError (see above for traceback): Got 88 frames, but animated gifs can only be decoded by tf.image.decode_gif or tf.image.decode_image","I am trying to train a new model. Here is my code!

python /Users/tianchuangxin1/hub/examples/image_retraining/retrain.py --image_dir /Users/tianchuangxin1/tensorflow_image ----model_dir=/Users/tianchuangxin1/tensorflow_image --how_many_training_steps=6000 --flip_left_right Ture --testing_percentage=10 --train_batch_size=32 --validation_batch_size=-1 --random_brightness=30 --random_scale=30 --eval_step_interval=200

Then , get the error:


INFO:tensorflow:Looking for images in 'apple'
INFO:tensorflow:Looking for images in 'computer'
INFO:tensorflow:Looking for images in 'pants'
INFO:tensorflow:Looking for images in 'shoes'
INFO:tensorflow:Looking for images in 'watch'
INFO:tensorflow:Using /var/folders/85/qbnsnmfn5vn0r3x3nvr09w790pxfq6/T/tfhub_modules to cache modules.
2018-11-30 16:31:19.221881: W tensorflow/core/graph/graph_constructor.cc:1265] Importing a graph with a lower producer version 26 into an existing graph with producer version 27. Shape inference will have run different parts of the graph with different producer versions.
INFO:tensorflow:Saver not created because there are no variables in the graph to restore
2018-11-30 16:31:23.032847: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
WARNING:tensorflow:From /Users/tianchuangxin1/hub/examples/image_retraining/retrain.py:582: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.gfile.GFile.
Traceback (most recent call last):
  File ""/Users/tianchuangxin1/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    return fn(*args)
  File ""/Users/tianchuangxin1/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/Users/tianchuangxin1/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Got 88 frames, but animated gifs can only be decoded by tf.image.decode_gif or tf.image.decode_image
	 [[{{node DecodeJpeg_1}} = DecodeJpeg[acceptable_fraction=1, channels=3, dct_method="""", fancy_upscaling=true, ratio=1, try_recover_truncated=false, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_DistortJPGInput_0_0)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/tianchuangxin1/hub/examples/image_retraining/retrain.py"", line 1315, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/Users/tianchuangxin1/anaconda/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/Users/tianchuangxin1/hub/examples/image_retraining/retrain.py"", line 1057, in main
    distorted_image_tensor, resized_image_tensor, bottleneck_tensor)
  File ""/Users/tianchuangxin1/hub/examples/image_retraining/retrain.py"", line 587, in get_random_distorted_bottlenecks
    {input_jpeg_tensor: jpeg_data})
  File ""/Users/tianchuangxin1/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/Users/tianchuangxin1/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/Users/tianchuangxin1/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/Users/tianchuangxin1/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Got 88 frames, but animated gifs can only be decoded by tf.image.decode_gif or tf.image.decode_image
	 [[node DecodeJpeg_1 (defined at /Users/tianchuangxin1/hub/examples/image_retraining/retrain.py:671)  = DecodeJpeg[acceptable_fraction=1, channels=3, dct_method="""", fancy_upscaling=true, ratio=1, try_recover_truncated=false, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_DistortJPGInput_0_0)]]

Caused by op 'DecodeJpeg_1', defined at:
  File ""/Users/tianchuangxin1/hub/examples/image_retraining/retrain.py"", line 1315, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/Users/tianchuangxin1/anaconda/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/Users/tianchuangxin1/hub/examples/image_retraining/retrain.py"", line 1024, in main
    FLAGS.random_brightness, module_spec)
  File ""/Users/tianchuangxin1/hub/examples/image_retraining/retrain.py"", line 671, in add_input_distortions
    decoded_image = tf.image.decode_jpeg(jpeg_data, channels=input_depth)
  File ""/Users/tianchuangxin1/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/gen_image_ops.py"", line 954, in decode_jpeg
    name=name)
  File ""/Users/tianchuangxin1/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/Users/tianchuangxin1/anaconda/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/Users/tianchuangxin1/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3274, in create_op
    op_def=op_def)
  File ""/Users/tianchuangxin1/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1770, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): Got 88 frames, but animated gifs can only be decoded by tf.image.decode_gif or tf.image.decode_image
	 [[node DecodeJpeg_1 (defined at /Users/tianchuangxin1/hub/examples/image_retraining/retrain.py:671)  = DecodeJpeg[acceptable_fraction=1, channels=3, dct_method="""", fancy_upscaling=true, ratio=1, try_recover_truncated=false, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_DistortJPGInput_0_0)]]






"
24064,can't use tensorflow-gpu=1.12,"I use anaconda and I both have cuda8 and cuda10 with the right cudnn. 
I tried 3.6.2 and 3.6.6. None of them works. 
Not only in the windows desk top. The tensorflow can't work on  linux cluster.

(py36) C:\Users\GUANGYUAN>python
Python 3.6.2 |Continuum Analytics, Inc.| (default, Jul 20 2017, 12:30:02) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""C:\Users\GUANGYUAN\AppData\Local\conda\conda\envs\py36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\GUANGYUAN\AppData\Local\conda\conda\envs\py36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\GUANGYUAN\AppData\Local\conda\conda\envs\py36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\GUANGYUAN\AppData\Local\conda\conda\envs\py36\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\GUANGYUAN\AppData\Local\conda\conda\envs\py36\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\GUANGYUAN\AppData\Local\conda\conda\envs\py36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\GUANGYUAN\AppData\Local\conda\conda\envs\py36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\GUANGYUAN\AppData\Local\conda\conda\envs\py36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\GUANGYUAN\AppData\Local\conda\conda\envs\py36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\GUANGYUAN\AppData\Local\conda\conda\envs\py36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\GUANGYUAN\AppData\Local\conda\conda\envs\py36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\GUANGYUAN\AppData\Local\conda\conda\envs\py36\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\GUANGYUAN\AppData\Local\conda\conda\envs\py36\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
24063,cann't import tensorflow-gpu 1.12 on windows,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
24062,conv/weights/Assign: Input tensor Cannot convert a tensor of type float32 to an input of type float32_ref,"When freezing a model, I keep encounter the problem represented in the title.

After training's done, I use the following code to freeze the checkpoint to a pb file:
```
import os, argparse

import tensorflow as tf

# The original freeze_graph function
# from tensorflow.python.tools.freeze_graph import freeze_graph 

dir = os.path.dirname(os.path.realpath(__file__))

def freeze_graph(model_dir, output_node_names):
    """"""Extract the sub graph defined by the output nodes and convert 
    all its variables into constant 
    Args:
        model_dir: the root folder containing the checkpoint state file
        output_node_names: a string, containing all the output node's names, 
                            comma separated
    """"""
    if not tf.gfile.Exists(model_dir):
        raise AssertionError(
            ""Export directory doesn't exists. Please specify an export ""
            ""directory: %s"" % model_dir)

    if not output_node_names:
        print(""You need to supply the name of a node to --output_node_names."")
        return -1

    # We retrieve our checkpoint fullpath
    checkpoint = tf.train.get_checkpoint_state(model_dir)
    input_checkpoint = checkpoint.model_checkpoint_path
    
    # We precise the file fullname of our freezed graph
    absolute_model_dir = ""/"".join(input_checkpoint.split('/')[:-1])
    output_graph = absolute_model_dir + ""/frozen_model.pb""

    # We clear devices to allow TensorFlow to control on which device it will load operations
    clear_devices = True

    # We start a session using a temporary fresh Graph
    with tf.Session(graph=tf.Graph()) as sess:
        # We import the meta graph in the current default Graph
        saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)

        # We restore the weights
        saver.restore(sess, input_checkpoint)

        # We use a built-in TF helper to export variables to constants
        output_graph_def = tf.graph_util.convert_variables_to_constants(
            sess, # The session is used to retrieve the weights
            tf.get_default_graph().as_graph_def(), # The graph_def is used to retrieve the nodes 
            output_node_names.split("","") # The output node names are used to select the usefull nodes
        ) 

        # Finally we serialize and dump the output graph to the filesystem
        with tf.gfile.GFile(output_graph, ""wb"") as f:
            f.write(output_graph_def.SerializeToString())
        print(""%d ops in the final graph."" % len(output_graph_def.node))

    return output_graph_def

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument(""--model_dir"", type=str, default="""", help=""Model folder to export"")
    parser.add_argument(""--output_node_names"", type=str, default="""", help=""The name of the output nodes, comma separated."")
    args = parser.parse_args()

    freeze_graph(args.model_dir, args.output_node_names)
```
And it looks fine:
```
Converted 603 variables to const ops.
7851 ops in the final graph.
```
plus, the generated pb file is larger than the ckpt file but smaller than the size of meta plus ckpt together.
Then, I tried to load the pb to inspect the ops:
```
import argparse
import tensorflow as tf

def load_graph(frozen_graph_filename):
    # We load the protobuf file from the disk and parse it to retrieve the
    # unserialized graph def
    with tf.gfile.GFile(frozen_graph_filename, 'rb') as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())

    # Then, we import the graph_def into a new Graph and return it
    with tf.Graph().as_default() as graph:
        # The name var will prefix every op/nodes in your graph
        # Since we load everything in a new graph, this is not needed
        tf.import_graph_def(graph_def, name='prefix')
    return graph

if __name__ == '__main__':
    # Let's allow the user to pass the filename as an argument
    parser = argparse.ArgumentParser()
    parser.add_argument('--frozen_model_filename', default='../experiments/ft-ht/frozen_model.pb', type=str, help='Frozen model file to import')
    args = parser.parse_args()

    # We use our ""load_graph"" function
    graph = load_graph(args.frozen_model_filename)

    # We can verify that we can access the list of operations in the graph
    for op in graph.get_operations():
        print(op.name)
```
And the code throws the error:
```
Traceback (most recent call last):
  File ""infer_pb.py"", line 25, in <module>
    graph = load_graph(args.frozen_model_filename)
  File ""infer_pb.py"", line 15, in load_graph
    tf.import_graph_def(graph_def, name='prefix')
  File ""/path/to/lib/python2.7/site-packages/tensorflow/python/framework/importer.py"", line 406, in import_graph_def
    node, 'Input tensor %r %s' % (input_name, te)))
ValueError: graph_def is invalid at node u'resnet_v1_50/conv1/weights/Assign': Input tensor 'resnet_v1_50/conv1/weights:0' Cannot convert a tensor of type float32 to an input of type float32_ref.
```

Anyone could shed me some light? Thanks!"
24061,Investigate precomputing reciprocals for division operations in XLA,"Today we optimize this elementwise division by a scalar:

```
HloModule DivReciprocal

ENTRY main {
  x = f32[2000,200]{1,0} parameter(0)
  y = f32[] parameter(1)
  y.broadcast = f32[2000,200]{1,0} broadcast(y), dimensions={}
  ROOT div = f32[2000,200]{1,0} divide(x, y.broadcast)
}
```

into the following:

```
HloModule DivReciprocal

fused_computation {
  param_0 = f32[2000,200]{1,0} parameter(0)
  param_1.1 = f32[] parameter(1)
  y.broadcast.1 = f32[2000,200]{1,0} broadcast(param_1.1), dimensions={}
  ROOT div.1 = f32[2000,200]{1,0} divide(param_0, y.broadcast.1)
}

ENTRY main {
  x = f32[2000,200]{1,0} parameter(0)
  y = f32[] parameter(1)
  ROOT fusion = f32[2000,200]{1,0} fusion(x, y), kind=kLoop, calls=fused_computation
}
```

`fusion` is ultimately lowered into the following PTX (via LLVM):

```
//
// Generated by LLVM NVPTX Back-End
//

.version 6.0
.target sm_60
.address_size 64

        // .globl       fusion

.visible .entry fusion(
        .param .u64 fusion_param_0,
        .param .u64 fusion_param_1,
        .param .u64 fusion_param_2
)
.reqntid 1024, 1, 1
{
        .reg .pred      %p<2>;
        .reg .f32       %f<11>;
        .reg .b32       %r<6>;
        .reg .b64       %rd<10>;

        mov.u32         %r2, %ctaid.x;
        mov.u32         %r3, %tid.x;
        shl.b32         %r4, %r2, 10;
        or.b32          %r5, %r4, %r3;
        setp.lt.u32     %p1, %r5, 100000;
        @%p1 bra        LBB0_2;
        bra.uni         LBB0_1;
LBB0_2:
        ld.param.u64    %rd4, [fusion_param_0];
        ld.param.u64    %rd5, [fusion_param_2];
        cvta.to.global.u64      %rd1, %rd5;
        ld.param.u64    %rd6, [fusion_param_1];
        cvta.to.global.u64      %rd2, %rd6;
        cvta.to.global.u64      %rd3, %rd4;
        shl.b32         %r1, %r5, 2;
        mul.wide.u32    %rd7, %r1, 4;
        add.s64         %rd8, %rd2, %rd7;
        ld.global.nc.v4.f32     {%f1, %f2, %f3, %f4}, [%rd8];
        ld.global.nc.f32        %f5, [%rd1];
        rcp.approx.f32  %f6, %f5;
        mul.f32         %f7, %f1, %f6;
        add.s64         %rd9, %rd3, %rd7;
        mul.f32         %f8, %f2, %f6;
        mul.f32         %f9, %f3, %f6;
        mul.f32         %f10, %f4, %f6;
        st.global.v4.f32        [%rd9], {%f7, %f8, %f9, %f10};
LBB0_1:
        ret;

}
```

It is conceivable that in some cases computing the reciprocal for `x` once, before we launch the kernel for `fusion`, can be profitable.  We should investigate if (and when) precomputing the reciprocal helps, and if it does, implement it as an optimization in XLA."
24060,Cpu tensorflow ImportError ,"Dear All ,

I am very new to Python and datascience , I have installed tensorflow cpu from anaconda navigator , but when I am importing tensorflow its showing the following error. also find the system information and conda information below 


**ERROR :** 
ImportError: Traceback (most recent call last):
  File ""C:\Users\Aashay.bane\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Aashay.bane\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Aashay.bane\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Aashay.bane\AppData\Local\Continuum\anaconda3\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Aashay.bane\AppData\Local\Continuum\anaconda3\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
************************************************************************************************************
**!conda info**

     active environment : base
    active env location : C:\Users\Aashay.bane\AppData\Local\Continuum\anaconda3
            shell level : 1
       user config file : C:\Users\Aashay.bane\.condarc
 populated config files : C:\Users\Aashay.bane\.condarc
          conda version : 4.5.11
    conda-build version : 3.16.3
         python version : 3.6.7.final.0
       base environment : C:\Users\Aashay.bane\AppData\Local\Continuum\anaconda3  (writable)
           channel URLs : https://repo.anaconda.com/pkgs/main/win-64
                          https://repo.anaconda.com/pkgs/main/noarch
                          https://repo.anaconda.com/pkgs/free/win-64
                          https://repo.anaconda.com/pkgs/free/noarch
                          https://repo.anaconda.com/pkgs/r/win-64
                          https://repo.anaconda.com/pkgs/r/noarch
                          https://repo.anaconda.com/pkgs/pro/win-64
                          https://repo.anaconda.com/pkgs/pro/noarch
                          https://repo.anaconda.com/pkgs/msys2/win-64
                          https://repo.anaconda.com/pkgs/msys2/noarch
          package cache : C:\Users\Aashay.bane\AppData\Local\Continuum\anaconda3\pkgs
                          C:\Users\Aashay.bane\AppData\Local\conda\conda\pkgs
       envs directories : C:\Users\Aashay.bane\AppData\Local\Continuum\anaconda3\envs
                          C:\Users\Aashay.bane\AppData\Local\conda\conda\envs
                          C:\Users\Aashay.bane\.conda\envs
               platform : win-64
             user-agent : conda/4.5.11 requests/2.20.1 CPython/3.6.7 Windows/8.1 Windows/6.3.9600
          administrator : False
             netrc file : None
           offline mode : False
***********************************************************************************************************
!conda list --show-channel-urls
# packages in environment at C:\Users\Aashay.bane\AppData\Local\Continuum\anaconda3:
#
# Name                    Version                   Build  Channel
_ipyw_jlab_nb_ext_conf    0.1.0                    py36_0    defaults
absl-py                   0.6.1                    py36_0    defaults
alabaster                 0.7.12                   py36_0    defaults
anaconda                  5.3.0                    py37_0    defaults
anaconda-client           1.7.2                    py36_0    defaults
anaconda-navigator        1.9.2                    py36_0    defaults
anaconda-project          0.8.2                    py36_0    defaults
appdirs                   1.4.3            py36h28b3542_0    defaults
asn1crypto                0.24.0                   py36_0    defaults
astor                     0.7.1                    py36_0    defaults
astroid                   2.1.0                    py36_0    defaults
astropy                   3.0.5            py36he774522_0    defaults
atomicwrites              1.2.1                    py36_0    defaults
attrs                     18.2.0           py36h28b3542_0    defaults
automat                   0.7.0                    py36_0    defaults
babel                     2.6.0                    py36_0    defaults
backcall                  0.1.0                    py36_0    defaults
backports                 1.0                      py36_1    defaults
backports.os              0.1.1                    py36_0    defaults
backports.shutil_get_terminal_size 1.0.0                    py36_2    defaults
beautifulsoup4            4.6.3                    py36_0    defaults
bitarray                  0.8.3            py36hfa6e2cd_0    defaults
bkcharts                  0.2              py36h7e685f7_0    defaults
blas                      1.0                         mkl    defaults
blaze                     0.11.3                   py36_0    defaults
bleach                    3.0.2                    py36_0    defaults
blosc                     1.14.4               he51fdeb_0    defaults
bokeh                     1.0.2                    py36_0    defaults
boto                      2.49.0                   py36_0    defaults
bottleneck                1.2.1            py36h452e1ab_1    defaults
bzip2                     1.0.6                hfa6e2cd_5    defaults
ca-certificates           2018.03.07                    0    defaults
certifi                   2018.10.15               py36_0    defaults
cffi                      1.11.5           py36h74b6da3_1    defaults
chardet                   3.0.4                    py36_1    defaults
click                     7.0                      py36_0    defaults
cloudpickle               0.6.1                    py36_0    defaults
clyent                    1.2.2                    py36_1    defaults
colorama                  0.4.0                    py36_0    defaults
comtypes                  1.1.7                    py36_0    defaults
conda                     4.5.11                   py36_0    defaults
conda-build               3.16.3                   py36_0    defaults
conda-env                 2.6.0                h36134e3_1    defaults
console_shortcut          0.1.1                         3    defaults
constantly                15.1.0           py36h28b3542_0    defaults
contextlib2               0.5.5            py36he5d52c0_0    defaults
cryptography              2.3.1            py36h74b6da3_0    defaults
cudatoolkit               9.0                           1    defaults
cudnn                     7.1.4                 cuda9.0_0    defaults
curl                      7.61.0               h7602738_0    defaults
cycler                    0.10.0           py36h009560c_0    defaults
cython                    0.29             py36ha925a31_0    defaults
cytoolz                   0.9.0.1          py36hfa6e2cd_1    defaults
dask                      1.0.0                    py36_0    defaults
dask-core                 1.0.0                    py36_0    defaults
datashape                 0.5.4                    py36_1    defaults
decorator                 4.3.0                    py36_0    defaults
defusedxml                0.5.0                    py36_1    defaults
distributed               1.25.0                   py36_0    defaults
docutils                  0.14             py36h6012d8f_0    defaults
entrypoints               0.2.3                    py36_2    defaults
et_xmlfile                1.0.1            py36h3d2d736_0    defaults
fastcache                 1.0.2            py36hfa6e2cd_2    defaults
filelock                  3.0.10                   py36_0    defaults
flask                     1.0.2                    py36_1    defaults
flask-cors                3.0.7                    py36_0    defaults
freetype                  2.9.1                ha9979f8_1    defaults
gast                      0.2.0                    py36_0    defaults
get_terminal_size         1.0.0                h38e98db_0    defaults
gevent                    1.3.7            py36he774522_1    defaults
glob2                     0.6                      py36_1    defaults
greenlet                  0.4.15           py36hfa6e2cd_0    defaults
grpcio                    1.14.1           py36h5c4b210_0    defaults
h5py                      2.8.0            py36h3bdd7fb_2    defaults
hdf5                      1.10.2               hac2f561_1    defaults
heapdict                  1.0.0                    py36_2    defaults
html5lib                  1.0.1                    py36_0    defaults
hyperlink                 18.0.0                   py36_0    defaults
icc_rt                    2017.0.4             h97af966_0    defaults
icu                       58.2                 ha66f8fd_1    defaults
idna                      2.7                      py36_0    defaults
imageio                   2.4.1                    py36_0    defaults
imagesize                 1.1.0                    py36_0    defaults
importlib_metadata        0.6                      py36_0    defaults
incremental               17.5.0                   py36_0    defaults
intel-openmp              2019.0                      118    defaults
ipykernel                 5.1.0            py36h39e3cac_0    defaults
ipython                   7.1.1            py36h39e3cac_0    defaults
ipython_genutils          0.2.0            py36h3c5d0ee_0    defaults
ipywidgets                7.4.2                    py36_0    defaults
isort                     4.3.4                    py36_0    defaults
itsdangerous              1.1.0                    py36_0    defaults
jdcal                     1.4                      py36_0    defaults
jedi                      0.13.1                   py36_0    defaults
jinja2                    2.10                     py36_0    defaults
jpeg                      9b                   hb83a4c4_2    defaults
jsonschema                2.6.0            py36h7636477_0    defaults
jupyter                   1.0.0                    py36_7    defaults
jupyter_client            5.2.3                    py36_0    defaults
jupyter_console           6.0.0                    py36_0    defaults
jupyter_core              4.4.0                    py36_0    defaults
jupyterlab                0.35.3                   py36_0    defaults
jupyterlab_launcher       0.13.1                   py36_0    defaults
jupyterlab_server         0.2.0                    py36_0    defaults
keras-applications        1.0.6                    py36_0    defaults
keras-preprocessing       1.0.5                    py36_0    defaults
keyring                   16.1.0                   py36_0    defaults
kiwisolver                1.0.1            py36h6538335_0    defaults
lazy-object-proxy         1.3.1            py36hfa6e2cd_2    defaults
libarchive                3.3.3                h798a506_1    defaults
libcurl                   7.61.0               h7602738_0    defaults
libiconv                  1.15                 h1df5818_7    defaults
libpng                    1.6.35               h2a8f88b_0    defaults
libprotobuf               3.6.1                h7bd577a_0    defaults
libsodium                 1.0.16               h9d3ae62_0    defaults
libssh2                   1.8.0                hd619d38_4    defaults
libtiff                   4.0.9                h36446d0_2    defaults
libxml2                   2.9.8                hadb2253_1    defaults
libxslt                   1.1.32               hf6f1972_0    defaults
llvmlite                  0.26.0           py36ha925a31_0    defaults
locket                    0.2.0            py36hfed976d_1    defaults
lxml                      4.2.5            py36hef2cd61_0    defaults
lz4-c                     1.8.1.2              h2fa13f4_0    defaults
lzo                       2.10                 h6df0209_2    defaults
m2w64-gcc-libgfortran     5.3.0                         6    defaults
m2w64-gcc-libs            5.3.0                         7    defaults
m2w64-gcc-libs-core       5.3.0                         7    defaults
m2w64-gmp                 6.1.0                         2    defaults
m2w64-libwinpthread-git   5.0.0.4634.697f757               2    defaults
Markdown                  3.0.1                     <pip>
markupsafe                1.1.0            py36he774522_0    defaults
matplotlib                3.0.1            py36hc8f65d3_0    defaults
mccabe                    0.6.1                    py36_1    defaults
menuinst                  1.4.14           py36hfa6e2cd_0    defaults
mistune                   0.8.4            py36he774522_0    defaults
mkl                       2018.0.3                      1    defaults
mkl-service               1.1.2            py36hb217b18_5    defaults
mkl_fft                   1.0.6            py36hdbbee80_0    defaults
mkl_random                1.0.1            py36h77b88f5_1    defaults
more-itertools            4.3.0                    py36_0    defaults
mpmath                    1.0.0                    py36_2    defaults
msgpack-python            0.5.6            py36he980bc4_1    defaults
msys2-conda-epoch         20160418                      1    defaults
multipledispatch          0.6.0                    py36_0    defaults
navigator-updater         0.2.1                    py36_0    defaults
nbconvert                 5.4.0                    py36_1    defaults
nbformat                  4.4.0            py36h3a5bc1b_0    defaults
networkx                  2.2                      py36_1    defaults
nltk                      3.3.0                    py36_0    defaults
nose                      1.3.7                    py36_2    defaults
notebook                  5.7.2                    py36_0    defaults
numba                     0.41.0           py36hf9181ef_0    defaults
numexpr                   2.6.8            py36h9ef55f4_0    defaults
numpy                     1.15.4           py36ha559c80_0    defaults
numpy-base                1.15.4           py36h8128ebf_0    defaults
numpydoc                  0.8.0                    py36_0    defaults
odo                       0.5.1            py36h7560279_0    defaults
olefile                   0.46                     py36_0    defaults
openpyxl                  2.5.9                    py36_0    defaults
openssl                   1.0.2p               hfa6e2cd_0    defaults
packaging                 18.0                     py36_0    defaults
pandas                    0.23.4           py36h830ac7b_0    defaults
pandoc                    1.19.2.1             hb2460c7_1    defaults
pandocfilters             1.4.2                    py36_1    defaults
parso                     0.3.1                    py36_0    defaults
partd                     0.3.9                    py36_0    defaults
path.py                   11.5.0                   py36_0    defaults
pathlib2                  2.3.2                    py36_0    defaults
patsy                     0.5.1                    py36_0    defaults
pep8                      1.7.1                    py36_0    defaults
pickleshare               0.7.5                    py36_0    defaults
pillow                    5.3.0            py36hdc69c19_0    defaults
pip                       18.1                     py36_0    defaults
pkginfo                   1.4.2                    py36_1    defaults
pluggy                    0.8.0                    py36_0    defaults
ply                       3.11                     py36_0    defaults
prometheus_client         0.4.2                    py36_0    defaults
prompt_toolkit            2.0.7                    py36_0    defaults
protobuf                  3.6.1            py36h33f27b4_0    defaults
psutil                    5.4.8            py36he774522_0    defaults
py                        1.7.0                    py36_0    defaults
pyasn1                    0.4.4            py36h28b3542_0    defaults
pyasn1-modules            0.2.2                    py36_0    defaults
pycodestyle               2.4.0                    py36_0    defaults
pycosat                   0.6.3            py36hfa6e2cd_0    defaults
pycparser                 2.19                     py36_0    defaults
pycrypto                  2.6.1            py36hfa6e2cd_9    defaults
pycurl                    7.43.0.2         py36h74b6da3_0    defaults
pyflakes                  2.0.0                    py36_0    defaults
pygments                  2.2.0            py36hb010967_0    defaults
pyhamcrest                1.9.0                    py36_2    defaults
pylint                    2.1.1                    py36_0    defaults
pyodbc                    4.0.24           py36h6538335_0    defaults
pyopenssl                 18.0.0                   py36_0    defaults
pyparsing                 2.3.0                    py36_0    defaults
pyqt                      5.9.2            py36h6538335_2    defaults
pysocks                   1.6.8                    py36_0    defaults
pytables                  3.4.4            py36he6f6034_0    defaults
pytest                    4.0.0                    py36_0    defaults
pytest-arraydiff          0.2              py36h39e3cac_0    defaults
pytest-astropy            0.4.0                    py36_0    defaults
pytest-doctestplus        0.2.0                    py36_0    defaults
pytest-openfiles          0.3.0                    py36_0    defaults
pytest-remotedata         0.3.1                    py36_0    defaults
python                    3.6.7                h33f27b4_1    defaults
python-dateutil           2.7.5                    py36_0    defaults
python-libarchive-c       2.8                      py36_6    defaults
pytz                      2018.7                   py36_0    defaults
pywavelets                1.0.1            py36h8c2d366_0    defaults
pywin32                   223              py36hfa6e2cd_1    defaults
pywinpty                  0.5.4                    py36_0    defaults
pyyaml                    3.13             py36hfa6e2cd_0    defaults
pyzmq                     17.1.2           py36hfa6e2cd_0    defaults
qt                        5.9.6            vc14h1e9a669_2  [vc14]  defaults
qtawesome                 0.5.3                    py36_0    defaults
qtconsole                 4.4.2                    py36_0    defaults
qtpy                      1.5.2                    py36_0    defaults
requests                  2.20.1                   py36_0    defaults
rope                      0.11.0                   py36_0    defaults
ruamel_yaml               0.15.46          py36hfa6e2cd_0    defaults
scikit-image              0.14.0           py36h6538335_1    defaults
scikit-learn              0.20.1           py36hb854c30_0    defaults
scipy                     1.1.0            py36h4f6bf74_1    defaults
seaborn                   0.9.0                    py36_0    defaults
send2trash                1.5.0                    py36_0    defaults
service_identity          17.0.0           py36h28b3542_0    defaults
setuptools                40.6.2                   py36_0    defaults
simplegeneric             0.8.1                    py36_2    defaults
singledispatch            3.4.0.3          py36h17d0c80_0    defaults
sip                       4.19.8           py36h6538335_0    defaults
six                       1.11.0                   py36_1    defaults
snappy                    1.1.7                h777316e_3    defaults
snowballstemmer           1.2.1            py36h763602f_0    defaults
sortedcollections         1.0.1                    py36_0    defaults
sortedcontainers          2.0.5                    py36_0    defaults
sphinx                    1.8.2                    py36_0    defaults
sphinxcontrib             1.0                      py36_1    defaults
sphinxcontrib-websupport  1.1.0                    py36_1    defaults
spyder                    3.3.2                    py36_0    defaults
spyder-kernels            0.3.0                    py36_0    defaults
sqlalchemy                1.2.14           py36he774522_0    defaults
sqlite                    3.24.0               h7602738_0    defaults
statsmodels               0.9.0            py36h452e1ab_0    defaults
sympy                     1.3                      py36_0    defaults
tblib                     1.3.2            py36h30f5020_0    defaults
tensorboard               1.12.0                    <pip>
tensorflow-base           1.12.0          gpu_py36h6e53903_0    defaults
termcolor                 1.1.0                    py36_1    defaults
terminado                 0.8.1                    py36_1    defaults
testpath                  0.4.2                    py36_0    defaults
textblob                  0.15.1                     py_0    conda-forge/label/gcc7
tk                        8.6.8                hfa6e2cd_0    defaults
toolz                     0.9.0                    py36_0    defaults
tornado                   5.1.1            py36hfa6e2cd_0    defaults
tqdm                      4.28.1           py36h28b3542_0    defaults
traitlets                 4.3.2            py36h096827d_0    defaults
twisted                   18.9.0           py36he774522_0    defaults
typed-ast                 1.1.0            py36hfa6e2cd_0    defaults
unicodecsv                0.14.1           py36h6450c06_0    defaults
urllib3                   1.23                     py36_0    defaults
vc                        14.1                 h0510ff6_4    defaults
vs2015_runtime            14.15.26706          h3a45250_0    defaults
wcwidth                   0.1.7            py36h3d5aa90_0    defaults
webencodings              0.5.1                    py36_1    defaults
werkzeug                  0.14.1                   py36_0    defaults
wheel                     0.32.3                   py36_0    defaults
widgetsnbextension        3.4.2                    py36_0    defaults
win_inet_pton             1.0.1                    py36_1    defaults
win_unicode_console       0.5              py36hcdbd4b5_0    defaults
wincertstore              0.2              py36h7fe50ca_0    defaults
winpty                    0.4.3                         4    defaults
wrapt                     1.10.11          py36hfa6e2cd_2    defaults
xlrd                      1.1.0                    py36_1    defaults
xlsxwriter                1.1.2                    py36_0    defaults
xlwings                   0.14.1                   py36_0    defaults
xlwt                      1.3.0            py36h1a4751e_0    defaults
xz                        5.2.4                h2fa13f4_4    defaults
yaml                      0.1.7                hc54c509_2    defaults
zeromq                    4.2.5                he025d50_1    defaults
zict                      0.1.3                    py36_0    defaults
zlib                      1.2.11               h8395fce_2    defaults
zope                      1.0                      py36_1    defaults
zope.interface            4.6.0            py36he774522_0    defaults
zstd                      1.3.3                hfe6a214_0    defaults

***********************************************************************************************************
SYSTEM INFO
Windows 8.1 pro

processor : intel(R) core(TM)-i5-4590cpu @3.30GHz  3.30GHz
RAM : 8gb
SYSTEM TYPE: 64-bit OS, X64-based processor "
24059,The performance of transpose_conv op in TensorFlow Lite,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
**Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
**Linux Ubuntu 14.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
**Meizu 16th**
- TensorFlow installed from (source or binary):
**benchmark_model is built form source**
- TensorFlow version (use command below):
**master**
- Python version:
**Python 3.5.3**
- Bazel version (if compiling from source):
**0.16.1**
- GCC/Compiler version (if compiling from source):
**4.9.4**
- CUDA/cuDNN version:
**No**
- GPU model and memory:
**N/A**

**Describe the current behavior**
I wanted to deploy a trained model on Android devices with TensorFlow Lite, but it was quite slow.
Then, I profiled the model(.tflite) with the benchmark_model tool, and found that the transpose_conv op took too much time. The summary by node type shown below:
![screenshot from 2018-11-30 11 11 48](https://user-images.githubusercontent.com/21071150/49267677-8f39c600-f496-11e8-9e91-1da5dd9f4603.png)

When I profiled the same model(.pb) used to convert to .tflite, I found that the transpose_conv achieving fast inference speed. The summary by node type shown below:
![screenshot from 2018-11-30 11 04 33](https://user-images.githubusercontent.com/21071150/49269045-d9727580-f49d-11e8-8d4c-84e965350420.png)

It seems like the transpose_conv op in TensorFlow Lite is much slower than that in TensorFlow Mobile?
"
24053,Why not using TensorFixedSize on most const TTypes?,"Hi im not too sure if this is a bug but why in TTypes.h, most of the tensor types that are 'Const' are not using TensorFixedSize? only a few are.

Apologies if this was not a bug."
24050,Feature Request: Consolidate tf.keras get_updates_for with existing tensorflow update_ops,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): TF 1.12
- Are you willing to contribute it (Yes/No): Potentially (not sure how complicated this is)



**Describe the feature and the current behavior/state.**
Currently, if `tf.keras` layers have update_ops (notably BatchNormalization), they are not added to `tf.GraphKeys.UPDATE_OPS`. Instead, we have to directly access them via `tf.keras.Model.get_updates_for`. However, for normal tf.layers implementations like https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization, these ops are directly added to `tf.GraphKeys.UPDATE_OPS` and this is cleanly documented. 

Is there a reason this can't be done with `tf.keras` models too? Or at least be much better documented?

**Will this change the current api? How?**
No, we can keep the current API. But, this will allow users migrating from older implementations to tf.keras based models to maintain their update_ops control dependencies.

**Who will benefit with this feature?**
Developers will face less technical load in migrating to `tf.keras`, especially since this was not clearly documented in the `tf.keras.layers.BatchNormalization` page.

**Any Other info.**
"
24047,Major memory leak after calling `tf.estimator.DNNClassifier.train` with certain `tf.train` optimizers.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Reproduced on OS X and Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: not sure.
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.11 and 1.12 tested
- Python version: 3.6.4
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: (tested on CPU)
- GPU model and memory: (tested on CPU)


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
When using the pre-canned `DNNClassifier` together with `tf.train.RMSPropOptimizer`, I noticed that a `tf.Graph()` instance is left allocated in memory _after_ an `estimator.train()` call terminates. When running `estimator.train()` in a loop N times, there will be N `tf.Graph()` instances allocated in memory. Therefore, calling `estimator.train()` in a loop will lead to unbounded memory growth, and eventually, an OOM error.

To track down the root cause of this behaviour, I have used the [`objgraph`](https://mg.pov.lt/objgraph/) dependency together with the [`iris premade estimator`](https://github.com/tensorflow/models/blob/master/samples/core/get_started/premade_estimator.py) example. The script is attached in the code section. After 5 iterations of `train()`, the following is a visualization of all the `tf.Graph` instances that exist in memory (a total of 5):

![iter-4](https://user-images.githubusercontent.com/2341691/49243825-2cc3c580-f3c3-11e8-8909-6ef4d41a1f41.png)

You can clearly see that the root cause is a `_slots` attribute on `tf.train.RMSPropOptimizer` that saves a reference to some variable associated with the graph every time `train()` runs. 

I have noticed this behaviour with `tf.train.AdamOptimizer`, `tf.train.RMSPropOptimizer`, and `tf.train.AdadeltaOptimizer`. I would be willing to bet that most provided optimizers that save some state in the optimization procedure run into this same issue. I have observed that this issue does _not_ occur with optimizers that _do not_ save state (see below).

I would also be willing to bet that this issue is not specific to `DNNClassifier` and occurs with many different pre-canned and custom estimators. However, I have not yet investigated reproducing this issue on other types of estimators. 

**Describe the expected behavior**
The expected behaviour is that the `tf.Graph()` instance used by an invocation of `train()` is garbage collected by the Python GC after `train()` terminates. I've observed this expected behaviour when using `tf.train.GradientDescentOptimizer` and `tf.train.AdagradOptimizer`. 

**Code to reproduce the issue**
[memory-leak-example.zip](https://github.com/tensorflow/tensorflow/files/2630449/memory-leak-example.zip)

The above code is a modified version of the [`iris premade estimator`](https://github.com/tensorflow/models/blob/master/samples/core/get_started/premade_estimator.py) example to loop over `train()` multiple times. It requires the [`objgraph`](https://mg.pov.lt/objgraph/) to show the peak memory growth every iteration and generate an visualization of in-memory `tf.Graph` instances. 

Here is an example log provided of an example invocation: `python premade_estimator.py --optimizer=rmsprop`

[rmsprop.log](https://github.com/tensorflow/tensorflow/files/2630465/rmsprop.log)

Note the object growth result after the 5th iteration. As can be seen, 5 `tf.Graph` instances remain in memory, along with all the `Tensor`, `TensorShape`, `Dimension`, and other objects that go along with it.
```
Object growth after iteration 4
tuple                         82229    +14171
list                          21693     +2862
dict                          24251     +2475
TensorShape                    2806      +561
Tensor                         2775      +555
Operation                      2745      +549
_InputList                     2735      +547
Dimension                      2455      +491
cell                           6487      +312
function                      31199      +218
weakref                        5083       +25
Variable                         95       +19
SaveSliceInfo                    90       +18
set                            1409        +8
CondContext                      30        +6
PartitionedVariable              30        +6
type                           3077        +5
builtin_function_or_method     3143        +4
OrderedDict                      48        +1
Condition                        10        +1
deque                            11        +1
_local                            8        +1
Graph                             5        +1
GroupLock                         5        +1
TraceableStack                    5        +1
ScopedTFGraph                     5        +1
_VariableScopeStore               5        +1
_VariableStore                    5        +1
Saver                             5        +1
TFShouldUseWarningWrapper         5        +1
BulkSaverBuilder                  5        +1
VariableScope                     5        +1
Graph written to /var/folders/rc/_2lrng15575d3bsd9t75br840000gn/T/objgraph-sejl5tm3.dot (276 nodes)
Image generated as /tmp/iter-4.png
```

"
24046,Error compiling iterator_ops.cc when building from source with MSVC 2017,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 
- TensorFlow installed from (source or binary): Source
- TensorFlow version: latest from git - 1.12.0
- Python version: 2.7
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version: N/A
- GPU model and memory: NVIDIA GeForce 940MX 4096mb



**When building tensorflow from source using bazel on windows 10 I encountered an error in iterator_ops.cc.**
To the best of my ability I think the error that stopped the build was 
`""binary '*': no operator found which takes a left-hand operand of type 'const tensorflow::OpInputList::Iterator' (or there is no acceptable conversion)""`
But I'm not familliar with tensorflow or the building process being used.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build :loader (loader being a barebones .cc file with tensorflow as a dependancy)

**Any other info / logs**
The relevant build logs: (somewhat long error message)
https://pastebin.com/akqRxB0P"
24045,Docker container support for RTX 2070,"**System information**
- OS Platform and Distribution : I've tried two with the same result: Linux Ubuntu 16.04 & Linux Ubuntu 18.04
- TensorFlow installed from docker hub :  tensorflow/tensorflow:latest-gpu-py3
- TensorFlow version: 1.12
- Python version: 3
- Installed using docker : tensorflow/tensorflow:latest-gpu-py3
- CUDA/cuDNN version: 
nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2017 NVIDIA Corporation
Built on Fri_Sep__1_21:08:03_CDT_2017
Cuda compilation tools, **release 9.0, V9.0.176**

- GPU model and memory: dual RTX 2070 8gb

I'm trying to set up TF through a docker image and I'm having problems getting TF working with the GPUs.  I get a segfault shortly after TF allocates memory space on the GPUs.  I'm thinking the drivers are the issue at this point.

I would like to clarify the pre-requisites for the host machine to be capable of running the TF docker image.  The info on the [site](https://www.tensorflow.org/install/docker) seems pretty straight forward, but when you look at the detail, it becomes confusing (to me at least) and slightly contradictory.  There appears to be a discrepancy between what is stated in instructions, and where you end up if you follow the links in the instructions.  

The [pre-requisites](https://www.tensorflow.org/install/docker) for the docker image state _only the NVIDIA GPU driver is required on the host machine (the NVIDIA CUDA Toolkit does not need to be installed)_.

However, if you follow the links, you will end up on a page which instructions you to install the CUDA toolkit and CUDA, by implication making these a pre-requisite for using the TR docker container.

The hyperlink for 'NVIDIA GPU driver' links to an [NVIDIA FAQ](https://github.com/NVIDIA/nvidia-docker/wiki/Frequently-Asked-Questions#how-do-i-install-the-nvidia-driver) which recommends you use your package manager to install the NVidia drivers (which is what I have done, using the PPA repository ([Instructions here](http://www.linuxandubuntu.com/home/how-to-install-latest-nvidia-drivers-in-linux)).  However, if you click the link behind _package manager_, you end up [here](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#package-manager-installation) 
 an NVidia site which details how to install CUDA, not the NVidia graphics drivers! Also, if you check the pre-requisites of the CUDA installation guide you will see you need to install the CUDA toolkit, which seems to contradict the statement at the start:

_only the NVIDIA GPU driver is required on the host machine (the NVIDIA CUDA Toolkit does not need to be installed)_

So do we need to install CUDA toolkit and CUDA and graphics drivers on the host?

Confusingly, there also seems to be something called a 'CUDA driver' which I assume is something different from the NVidia driver.  The [NVidia instructions ](https://github.com/NVIDIA/nvidia-docker/wiki/Frequently-Asked-Questions#how-do-i-install-the-nvidia-driver) are about installing the 'CUDA driver' on the host machine, not the NVidia driver??

_The recommended way is to use your package manager and install the cuda-drivers package (or equivalent)_

Please can you advise how I should set up the host machine to make it work with the TF docker images?

Thanks!"
24044,Secondary algorithm not provided error when not using cuDNN autotune,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.12
- Python version: 3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 9.0 / 7.1.4
- GPU model and memory: Nvidia Tesla K80, Quadro M1200

**Describe the current behavior**
I get the following error message when I set TF_CUDNN_USE_AUTOTUNE=0:
 2018-11-22 10:10:01.227780: E tensorflow/stream_executor/cuda/cuda_dnn.cc:82] The primary convolution algorithm failed memory allocation, while a secondary algorithm is not provided.

**Describe the expected behavior**
I would expect a secondary algorithm that does not require any workspace to kick in should the scratch allocation for the primary algorithm fail even when I am not using autotune.

**Other info / logs**
I traced back the error message to when it was [first introduced](https://github.com/tensorflow/tensorflow/blob/466eb299f0ce20cf929b9e06d3d3c16959360c59/tensorflow/stream_executor/cuda/cuda_dnn.cc#L706) in the code base to revise the logic.

It seems to me that years of refactoring have disabled the [fallback mechanism](https://github.com/tensorflow/tensorflow/blob/466eb299f0ce20cf929b9e06d3d3c16959360c59/tensorflow/stream_executor/cuda/cuda_dnn.cc#L792) when both the primary and secondary algorithms are the default ones.
"
24043,Pruning: Multi-GPU support,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): modified models/official/resnet/resnet_model.py to utilize Pruning's masked layers.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v1.10.0
- Python version: 3.5.2
- Bazel version (if compiling from source): 16.1
- GCC/Compiler version (if compiling from source): gcc
- CUDA/cuDNN version: 8/6
- GPU model and memory: x2 nvidia titan Xp

**Describe the current behavior**
Multi-GPU training session of modified tensorflow/models for pruning, the session won't boot.
The same modified Resnet model _does_ execute iff num_gpus=1.

The error is:
ValueError: You must specify an aggregation method to update a MirroredVariable in Tower Context.
Raised from tensorflow/contrib/distribute/python/values.py"", line 336

[out_batch64.log](https://github.com/tensorflow/tensorflow/files/2629288/out_batch64.log)"
24042,ModelCheckpoint TypeError: get_config() missing 1 required positional argument: 'self',"[comment]
Epoch 00001: loss improved from inf to 103.67410, saving model to /tmp/weights.hdf5
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-12-a8915ea6c3f5> in <module>()
     27 
     28 #validation_data=(X_test,y_test),
---> 29 model.fit(X_train,y_train,epochs=2000,batch_size=128,callbacks=[checkpointer])
     30 
     31 return_eval = model.evaluate(X_test,y_test,batch_size=128)


- [error ] as  error show:

/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/utils/generic_utils.py in serialize_keras_object(instance)
    134     return {
    135         'class_name': instance.__class__.__name__,
--> 136         'config': instance.get_config()
    137     }
    138   if hasattr(instance, '__name__'):

TypeError: get_config() missing 1 required positional argument: 'self'


- [ code]  as I use code below:

checkpointer = ModelCheckpoint(filepath=""/tmp/weights.hdf5"", verbose=1, monitor='loss',save_best_only=True)

model.compile(#metrics=[mean_pred],
    loss=keras.losses.mean_squared_error,    
    optimizer=keras.optimizers.Adam(lr=0.001))

#validation_data=(X_test,y_test),
model.fit(X_train,y_train,epochs=2000,batch_size=128,callbacks=[checkpointer])

also with validation_data & val_loss has issue, plz help to check it. thanks~"
24041,Tensorboard embedding is not visible when filename contains 'plugins',"Tensorboard embedding is not visible when filename contains 'plugins'.
While it works well after changing 'plugins' to other names any.
And, btw, graph is showing well even with 'plugins' file name."
24040,TensorForest: the base random seed takes no effects,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary, pip install tensorflow==1.12.0
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6.4
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: No usage
- GPU model and memory: No usage

**Describe the current behavior**

I found that setting the same base_random_seed for TensorForest generate different classification performance.

**Describe the expected behavior**

Setting the **same** base_random_seed for TensorForest should generate the **same** classification performance so that we can reproduce every experiment.

**Code to reproduce the issue**

```
import numpy as np
import tensorflow as tf
import sklearn.datasets
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

X, y = sklearn.datasets.load_digits(return_X_y=True)
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
x_train = x_train.astype(np.float32)
x_test = x_test.astype(np.float32)

params = tf.contrib.tensor_forest.python.tensor_forest.ForestHParams(
num_classes=10, num_features=64, regression=False,
num_trees=50, max_nodes=1000, base_random_seed=2)

classifier = tf.contrib.tensor_forest.client.random_forest.TensorForestEstimator(params)
classifier.fit(x=x_train, y=y_train)
y_out = classifier.predict(x=x_test)

yyy = [x for x in y_out]

y_hat = np.array([x['classes'] for x in yyy])
print(accuracy_score(y_test, y_pred=y_hat))
# first run:  0.9629629629629629
# second run: 0.968013468013468
# third run:  0.9646464646464646
```

The base random seed is non-zero, it's supposed to produce the same result.
Thus, I wonder where else does the code have randomness?
@yupbank "
24035,how to take out the output of a layer,"I am using the code from here:
https://www.tensorflow.org/tutorials/estimators/cnn
how to take out the output of pool2_flat?"
24034,the performance of tensorflow distributed ,"I have trained a speech recognition network using tensorflow both on multi-gpu and distributed. there is some question that i have been puzzled a long time. that's my [code,](https://github.com/dingevin/distributed-training) when i'm training i faced two questions:
1. **the accuracy of distributed not equal to multi-gpu**. in multi-gpu version, i use 2 gpu and training 8000 steps, the accuracy can reach 32%; however the accuracy only can reach 28% at distributed version(i have 2 ps 2 worker, and each worker use 2 gpu), each worker training 4000 steps, and the learning rate was same. it seems the parameters not share.
multi-gpu: (when training 4000 steps loss is reduce to about 5.0)
```
INFO:tensorflow:Step #4015: lr: 0.000100, time 2018-11-28 16:35:16, accuracy 17.12786%, cross entropy 5.230135(147.0 examples/sec; 0.218 sec/batch)
INFO:tensorflow:Step #4016: lr: 0.000100, time 2018-11-28 16:35:17, accuracy 16.93807%, cross entropy 5.064457(151.4 examples/sec; 0.211 sec/batch)
INFO:tensorflow:Step #4017: lr: 0.000100, time 2018-11-28 16:35:17, accuracy 18.51658%, cross entropy 5.146353(149.4 examples/sec; 0.214 sec/batch)
INFO:tensorflow:Step #4018: lr: 0.000100, time 2018-11-28 16:35:17, accuracy 18.98760%, cross entropy 4.905624(145.8 examples/sec; 0.219 sec/batch)
INFO:tensorflow:Step #4019: lr: 0.000100, time 2018-11-28 16:35:17, accuracy 19.00991%, cross entropy 4.929277(139.1 examples/sec; 0.230 sec/batch)
INFO:tensorflow:Step #4020: lr: 0.000100, time 2018-11-28 16:35:17, accuracy 15.84382%, cross entropy 5.140810(139.6 examples/sec; 0.229 sec/batch)
INFO:tensorflow:Step #4021: lr: 0.000100, time 2018-11-28 16:35:18, accuracy 20.37208%, cross entropy 4.693164(143.1 examples/sec; 0.224 sec/batch)
```
distributed ( training 2000 steps the loss was reduce to 5.5 equal to 2000 steps at multi-gpu )
```
INFO:tensorflow:Step #2018: learning_rate:0.0001, accuracy 17.56266%, cross entropy 5.382931(15.6 examples/sec; 2.050 sec/batch)
INFO:tensorflow:Step #2019: learning_rate:0.0001, accuracy 16.61609%, cross entropy 5.386977(21.3 examples/sec; 1.505 sec/batch)
INFO:tensorflow:Step #2020: learning_rate:0.0001, accuracy 11.97795%, cross entropy 6.026215(29.3 examples/sec; 1.093 sec/batch)
INFO:tensorflow:Step #2021: learning_rate:0.0001, accuracy 14.76162%, cross entropy 5.657806(32.7 examples/sec; 0.977 sec/batch)
INFO:tensorflow:Step #2022: learning_rate:0.0001, accuracy 15.25826%, cross entropy 5.588447(29.4 examples/sec; 1.087 sec/batch)
INFO:tensorflow:Step #2023: learning_rate:0.0001, accuracy 13.63419%, cross entropy 5.882462(28.7 examples/sec; 1.117 sec/batch)
INFO:tensorflow:Step #2024: learning_rate:0.0001, accuracy 15.66866%, cross entropy 5.585463(37.7 examples/sec; 0.849 sec/batch)
```
2. **the speed of distributed version was so slow**. the bandwidth is 125Mb/s, is not the bottleneck.
i execute the command as follow on every worker:
```
CUDA_VISIBLE_DEVICES='' nohup python distributed_train.py --ps_hosts=gpu42-hca:2220, gpu47-hca:2221 --worker_hosts=gpu42-hca:3330,gpu47-hca:3331 --job_name=ps --task_index=0 &
CUDA_VISIBLE_DEVICES='0,1' nohup python distributed_train.py --ps_hosts=gpu42-hca:2220,gpu47-hca:2221 --worker_hosts=gpu42-hca:3330,gpu47-hca:3331 --job_name=worker --task_index=0 --gpu_num=2 --learning_rate=0.0001--how_many_training_steps=4000 --save_step_interval=5000 &
```
and another worker:
```
CUDA_VISIBLE_DEVICES='' nohup python distributed_train.py --ps_hosts=gpu42-hca:2220, gpu47-hca:2221 --worker_hosts=gpu42-hca:3330,gpu47-hca:3331 --job_name=ps --task_index=1 &
CUDA_VISIBLE_DEVICES='0,1' nohup python distributed_train.py --ps_hosts=gpu42-hca:2220,gpu47-hca:2221 --worker_hosts=gpu42-hca:3330,gpu47-hca:3331 --job_name=worker --task_index=1 --gpu_num=2 --learning_rate=0.0001--how_many_training_steps=4000 --save_step_interval=5000 &
```

was my code wrong? or i misunderstand distributed? can somebody give me some advice. thanks.

"
24033,"[TF1.12][graph_transforms] The INT8 performance test of graph_transforms for inceptionv3 model, GPU utilization extremely low.","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source 
- TensorFlow version (use command below): 1.12
- Python version: 2.7
- Bazel version (if compiling from source): 0.15
- GCC/Compiler version (if compiling from source): 4.9.3
- CUDA/cuDNN version: 9.0/7.0.5
- GPU model and memory: TitanXP

**Describe the current behavior**
I prepared an inception v3 model and try to quantify it by the tool ""graph_transforms"".
This is my conversion command:
`bazel-bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=frozen_graph.pb --out_graph=out_graph.pb --inputs='Placeholder:0' --outputs='InceptionV3/Logits/SpatialSqueeze:0' --transforms='add_default_attributes strip_unused_nodes(type=float, shape=""16,299,299,3"") remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms quantize_weights quantize_nodes strip_unused_nodes sort_by_execution_order'`

Then I evaluated the accuracy loss by test dataset which is consists of ~80k images. The result shows the accuracy decreased from 70.1% to 64.03%.

Then I try to evaluate the GPU performance by repeatedly run a batch of images, and the GPU utilization is extremely low and the speed is very slow. 
```
$ nvidia-smi dmon -i 0
# gpu   pwr  temp    sm   mem   enc   dec  mclk  pclk
# Idx     W     C     %     %     %     %   MHz   MHz
    0    59    35     2     0     0     0  5508  1430
    0    59    35     0     0     0     0  5508  1430
    0    59    36     2     0     0     0  5508  1430
    0    59    36     0     0     0     0  5508  1430
    0    59    36     2     0     0     0  5508  1430
    0    59    36     2     0     0     0  5508  1430
    0    59    36     0     0     0     0  5508  1430
    0    59    36     3     0     0     0  5508  1430
    0    59    36     0     0     0     0  5508  1430
    0    59    36     2     0     0     0  5508  1430
    0    59    36     0     0     0     0  5508  1430
    0    60    36     2     0     0     0  5508  1430
    0    59    36     4     0     0     0  5508  1430
    0    59    36     0     0     0     0  5508  1430
    0    60    36     3     0     0     0  5508  1430
    0    59    37     0     0     0     0  5508  1430
    0    59    37     2     0     0     0  5508  1430
    0    59    37     2     0     0     0  5508  1430
    0    59    36     2     0     0     0  5508  1430
    0    59    37     3     0     0     0  5508  1430
    0    59    37     0     0     0     0  5508  1430
    0    59    37     3     0     0     0  5508  1430
    0    59    37     0     0     0     0  5508  1430
    0    59    37     2     0     0     0  5508  1430
    0    59    37     4     0     0     0  5508  1430
```

There shall exists some bugs in this tool, which is related to the GPU utilization.

Any idea will be welcome."
24032,TFTRT and `tf.keras.application.ResNet50` INT8 and FP32 not working,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
**Ubuntu 18.04**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **tf-nightly (1.13.0-dev20181127).**
- Python version: **3.5**
- Bazel version (if compiling from source): **N/A**
- GCC/Compiler version (if compiling from source):  **N/A**
 CUDA/cuDNN version: **9.0 / 7.3**
- GPU model and memory: **GTX 1080 Ti (11GB)**

**Describe the current behavior**

Conversion to FP32 using TensorRT fails on `tf.keras.applications.InceptionResNetV2`, `tf.keras.applications.InceptionV3` with:

```
Engine my_trt_op_0 creation for segment 0, composed of 2049 nodes failed:
Invalid argument: Validation failed for TensorRTInputPH_0 and input slot 0:
Input tensor with shape [?,?,?,3] has an unknown non-batch dimemension at dim 1.
```

With `tf.keras.applications.VGG19`, conversion to FP32 fails with:

```
python: helpers.cpp:56: nvinfer1::DimsCHW nvinfer1::getCHW(const nvinfer1::Dims&): Assertion `d.nbDims >= 3' failed.
``` 

For `tf.keras.applications.ResNet50`, conversion to FP32 succeed but with INT8, the error is similar to `VGG19` with FP32:

```
python: helpers.cpp:56: nvinfer1::DimsCHW nvinfer1::getCHW(const nvinfer1::Dims&): Assertion `d.nbDims >= 3' failed.```
```

**Describe the expected behavior**

Given that it is possible to convert TensorFlow models such as Inception to TensorRT as evidenced by https://github.com/NVIDIA-AI-IOT/tf_trt_models, the conversion should similarly work for models in `tf.keras.applications.*`.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
import tensorflow as tf
import tensorflow.contrib.tensorrt as trt
import utils
import time

from imutils import paths
from tensorflow.python.client import session as csess
from tensorflow.python.framework import importer as importer
from tensorflow.python.framework import ops as ops
from tensorflow.core.protobuf import config_pb2 as cpb2
from tensorflow.python.platform import gfile


print(tf.__version__)

gpu_memory = 10000000000
trt_frac = 0.5

# Create a session first...
sess = tf.Session(config=tf.ConfigProto(
    gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=trt_frac)))

tf.keras.backend.set_session(sess)

input_tensor = tf.placeholder(tf.float32, shape=(None, 224, 224, 3), name='image_tensor')

tf.keras.backend.set_learning_phase(0)

# NOTE: ResNet50 doesn't work with INT8
# python: helpers.cpp:56: nvinfer1::DimsCHW nvinfer1::getCHW(const nvinfer1::Dims&): Assertion `d.nbDims >= 3' failed.
model = tf.keras.applications.ResNet50(weights='imagenet')

# NOTE: MobileNetV2 doesn't work with INT8
# python: helpers.cpp:56: nvinfer1::DimsCHW nvinfer1::getCHW(const nvinfer1::Dims&): Assertion `d.nbDims >= 3' failed.
# model = tf.keras.applications.MobileNetV2(weights='imagenet')

# NOTE: InceptionResNetV2 and InceptionV3 doesn't work:
# Engine my_trt_op_0 creation for segment 0, composed of 2049 nodes failed:
# Invalid argument: Validation failed for TensorRTInputPH_0 and input slot 0:
# Input tensor with shape [?,?,?,3] has an unknown non-batch dimemension at dim 1.
# model = tf.keras.applications.InceptionResNetV2(weights='imagenet')
# model = tf.keras.applications.InceptionV3(weights='imagenet')

# NOTE: VGG19 doesn't work
# python: helpers.cpp:56: nvinfer1::DimsCHW nvinfer1::getCHW(const nvinfer1::Dims&): Assertion `d.nbDims >= 3' failed.
# model = tf.keras.applications.VGG19(weights='imagenet')

model_input = model.input.name.strip(':0')
model_output = model.output.name.strip(':0')

print(model_input, model_output)

graph = tf.get_default_graph().as_graph_def()

# freeze graph and remove nodes used for training
frozen_graph = tf.graph_util.convert_variables_to_constants(sess, graph, [model_output])
frozen_graph = tf.graph_util.remove_training_nodes(frozen_graph)

nodes = [node.name for node in frozen_graph.node]
print(nodes)

# Make this the default graph now
tf.import_graph_def(frozen_graph, name='')

max_workspace_size_bytes = int(trt_frac * gpu_memory)

precision_mode = 'INT8' # FP32 | INT8

trt_graph = trt.create_inference_graph(
    input_graph_def=frozen_graph,
    outputs=[model_output],
    max_batch_size=1,
    max_workspace_size_bytes=max_workspace_size_bytes,
    precision_mode=precision_mode,
    minimum_segment_size=3,
    maximum_cached_engines=3,
    is_dynamic_op=False
)

trt_engine_opts = len([1 for n in trt_graph.node if str(n.op) == 'TRTEngineOp'])
print('TRT Engine Op: {}'.format(trt_engine_opts))
assert trt_engine_opts > 0, 'No TRT Engine Ops!'

if precision_mode == 'INT8':
    with gfile.GFile(""frozen/frozen_graph_INT8CALIB.pb"", 'wb') as f:
        f.write(trt_graph.SerializeToString())
else:
    with gfile.GFile(""frozen/frozen_graph_FP32.pb"", 'wb') as f:
        f.write(trt_graph.SerializeToString())


if precision_mode == 'INT8':
    gpu_options = cpb2.GPUOptions(per_process_gpu_memory_fraction=trt_frac)
    sess_config = cpb2.ConfigProto(gpu_options=gpu_options)

    ops.reset_default_graph()
    g1 = ops.Graph()
    with g1.as_default():
        inp, out = importer.import_graph_def(
            graph_def=trt_graph,
            return_elements=[model_input, model_output]
        )
        inp = inp.outputs[0]
        out = out.outputs[0]
    with csess.Session(
            config=sess_config, graph=g1) as sess:
        for file_name in list(paths.list_images('samples'))[0:10]:
            input = utils.read_tensor_from_image_file(file_name,
                                                      input_height=224,
                                                      input_width=224)
            start_time = time.time()
            val = sess.run(out, {inp: input})
            stop_time = time.time()
            print('Inference time is {}'.format(stop_time - start_time))
            print('Running {}'.format(file_name))

    trt_graph = trt.calib_graph_to_infer_graph(trt_graph)

    with gfile.GFile(""frozen/frozen_graph_INT8.pb"", 'wb') as f:
        f.write(trt_graph.SerializeToString())

    ops.reset_default_graph()
    g2 = ops.Graph()
    with g2.as_default():
        # Run the inference

        inp, out = importer.import_graph_def(
            graph_def=trt_graph,
            return_elements=[model_input, model_output]
        )

        inp = inp.outputs[0]
        out = out.outputs[0]

    with csess.Session(
            config=sess_config, graph=g2) as sess:
        for file_name in list(paths.list_images('samples'))[0:10]:
            input = utils.read_tensor_from_image_file(file_name,
                                                      input_height=224,
                                                      input_width=224)
            start_time = time.time()
            val = sess.run(out, {inp: input})
            stop_time = time.time()
            print('Inference time is {}'.format(stop_time - start_time))
            print('Running {}'.format(file_name))
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

I have also tried messing around with every combination of `max_batch_size`, `minimum_segment_size`, `maximum_cached_engines` but no joy.

"
24031,ImportError: module could not be found - Windows 10,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10.0.17134.407
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Binary
- TensorFlow version: 1.12.0
- Python version: 3.6.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 10.0.130 / 7.4.1.5 for Win 10
- GPU model and memory: Quadro M520



**Describe the problem**
tensorflow fails on import with ""module could not be found"".  Other issue says to open a new issue due to differing config (https://github.com/tensorflow/tensorflow/issues/22794)

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. python
2. import tensorflow as tf




**Any other info / logs**
Python 3.6.7 (v3.6.7:6ec5cf24b7, Oct 20 2018, 13:35:33) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\marktayl\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\marktayl\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\marktayl\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\marktayl\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\marktayl\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\marktayl\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\marktayl\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\marktayl\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\marktayl\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\marktayl\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\marktayl\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\marktayl\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\marktayl\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
24030,Windows Bazel build fails when using TF_SYSTEM_LIBS,"**System information**
- OS Platform and Distribution: Windows 10 Pro
- TensorFlow installed from: source
- TensorFlow version: 1.11
- Python version: 3.5.5
- Installed using: conda
- Bazel version: 0.15.1
- GCC/Compiler version: Visual Studio 14.0
- CUDA/cuDNN version: 9.0
- GPU model and memory: NVIDIA GeForce GTX 960

**Describe the problem**

I can build  Tensorflow 1.11 normally using Bazel on Windows, but if I set any TF_SYSTEM_LIBS, then Bazel can't find the necessary headers.

Via setting some options and messing with bazel's `copt` commandline argument and the CC_OPT_FLAGS environment variable, I was once able to get bazel to find zlib.h, but now I can't figure out what I did to make that happen. When that happened, I got a different error, something about zlib.h being an undeclared dependency.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

steps that I am running in Git Bash:
```
export PYTHON_BIN_PATH=C:/Users/nstier/AppData/Local/Continuum/anaconda3/python.exe
export PYTHON_LIB_PATH=C:/Users/nstier/AppData/Local/Continuum/anaconda3/Lib

export TF_CUDA_VERSION=9.0
export TF_CUDNN_VERSION=7
export CUDA_TOOLKIT_PATH=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0""
export CUDNN_INSTALL_PATH=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0""

export GCC_HOST_COMPILER_PATH=/usr/bin/gcc
export TF_CUDA_CLANG=0

# zlib/ is symlinked into the tensorflow directory to avoid the error ""include path references outside of execution root...."" 
export CC_OPT_FLAGS=""/arch:AVX /I/c/Users/nstier/tensorflow/ExternalPackages/zlib-1.2.11-t1-VC15.8.1/include""

export TF_CUDA_COMPUTE_CAPABILITIES=6.1

export TF_NEED_CUDA=1
export TF_NEED_JEMALLOC=1
export TF_ENABLE_XLA=0
export TF_NEED_OPENCL=0
export TF_NEED_OPENCL_SYCL=0
export TF_NEED_TENSORRT=0
export TF_NEED_NGRAPH=0

export TF_NEED_GCP=0
export TF_NEED_AWS=0

export TF_NEED_KAFKA=0
export TF_NEED_HDFS=0
export TF_NEED_GDR=0
export TF_NEED_VERBS=0
export TF_NEED_MPI=0

export TF_SET_ANDROID_WORKSPACE=0
export TMP=/tmp
export TF_SYSTEM_LIBS=zlib_archive
export TF_OVERRIDE_EIGEN_STRONG_INLINE=1

bazel build \
  --config=opt \
  --config=cuda \
  --config=monolithic \
  --define=no_tensorflow_py_deps=true \
  --copt=/I/c/Users/nstier/tensorflow/ExternalPackages/zlib-1.2.11-t1-VC15.8.1/include \
  --cxxopt=/I/c/Users/nstier/tensorflow/ExternalPackages/zlib-1.2.11-t1-VC15.8.1/include \
   //tensorflow:libtensorflow_cc.so \
  //tensorflow/tools/pip_package:build_pip_package
```

**Any other info / logs**

build log:
<pre>
Starting local Bazel server and connecting to it...
.............
WARNING: The following configs were expanded more than once: [cuda, monolithic]. For repeatable flags, repeats are counted twice anday lead to unexpected behavior.
Loading:
Loading: 0 packages loaded
Loading: 0 packages loaded
    currently loading: tensorflow/tools/pip_package ... (2 packages)
DEBUG: C:/users/nstier/_bazel_nstier/t2qf7f76/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest Visual C++ installed.
DEBUG: C:/users/nstier/_bazel_nstier/t2qf7f76/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variables, eg. VS140COMNTOOLS
DEBUG: C:/users/nstier/_bazel_nstier/t2qf7f76/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: Visual C++ build tools found at C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\
DEBUG: C:/users/nstier/_bazel_nstier/t2qf7f76/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest Visual C++ installed.
DEBUG: C:/users/nstier/_bazel_nstier/t2qf7f76/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variables, eg. VS140COMNTOOLS
DEBUG: C:/users/nstier/_bazel_nstier/t2qf7f76/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: Visual C++ build tools found at C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\
Analyzing: 2 targets (2 packages loaded)
Analyzing: 2 targets (150 packages loaded)
WARNING: C:/users/nstier/tensorflow/tensorflow/core/BUILD:2464:1: in includes attribute of cc_library rule //tensorflow/core:framewo_internal_headers_lib: '../../external/com_google_absl' resolves to 'external/com_google_absl' not below the relative path of its paage 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the eor might have been caused by the macro implementation in C:/users/nstier/tensorflow/tensorflow/tensorflow.bzl:1373:20
WARNING: C:/users/nstier/tensorflow/tensorflow/core/BUILD:2549:1: in includes attribute of cc_library rule //tensorflow/core:framewo_headers_lib: '../../external/com_google_absl' resolves to 'external/com_google_absl' not below the relative path of its package 'teorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error mighhave been caused by the macro implementation in C:/users/nstier/tensorflow/tensorflow/tensorflow.bzl:1373:20
WARNING: C:/users/nstier/_bazel_nstier/t2qf7f76/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopbplease do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on  appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused  the macro implementation in C:/users/nstier/_bazel_nstier/t2qf7f76/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: C:/users/nstier/_bazel_nstier/t2qf7f76/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopbplease do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on  appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused  the macro implementation in C:/users/nstier/_bazel_nstier/t2qf7f76/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: C:/users/nstier/_bazel_nstier/t2qf7f76/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopbplease do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on  appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused  the macro implementation in C:/users/nstier/_bazel_nstier/t2qf7f76/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: C:/users/nstier/tensorflow/tensorflow/core/BUILD:2563:1: in includes attribute of cc_library rule //tensorflow/core:stream_ecutor_headers_lib: '../../external/com_google_absl' resolves to 'external/com_google_absl' not below the relative path of its packa 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the erromight have been caused by the macro implementation in C:/users/nstier/tensorflow/tensorflow/tensorflow.bzl:1373:20
WARNING: C:/users/nstier/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. itch to SavedModel immediately.
WARNING: C:/users/nstier/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switcho SavedModel immediately.
WARNING: C:/users/nstier/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:354:1: in py_library rule //tensorflow/conib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated rget '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https:/ithub.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will bremoved by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: C:/users/nstier/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:73:1: in py_library rul//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/teseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': Tensorow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in .contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.dtributions` to `tfp.distributions`.
WARNING: C:/users/nstier/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:230:1: in py_library ru //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseri/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distbutions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecatedopies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all uge of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: C:/users/nstier/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_p target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py'TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaing in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.ctrib.distributions` to `tfp.distributions`.
WARNING: C:/users/nstier/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distribions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated coes remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usagof `tf.contrib.distributions` to `tfp.distributions`.
WARNING: C:/users/nstier/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tenrflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributio has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distrutions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` totfp.distributions`.
INFO: Analysed 2 targets (297 packages loaded).
INFO: Found 2 targets...
Building: no action
[1 / 8] [-----] BazelWorkspaceStatusAction stable-status.txt
<b>ERROR: C:/users/nstier/_bazel_nstier/t2qf7f76/external/grpc/BUILD:1443:1: C++ compilation of rule '@grpc//:grpc_transport_chttp2' faed (Exit 2): msvc_wrapper_for_nvcc.bat failed: error executing command
  cd C:/users/nstier/_bazel_nstier/t2qf7f76/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0
    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VATLMFC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\ilude\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.17134\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\winrt;
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LMFC\LIB\amd64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17134.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\l\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17134.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\WINDOWS\Microsoft.NET\Framework64\v4.0.30319;C:\Proam Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\Tools;C:\Programiles (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Windows Kits\10\bin\x86;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.\bin\NETFX 4.6.1 Tools\x64\;;C:\WINDOWS\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/nstier/AppData/Local/Continuum/anaconda3/python.exe
    SET PYTHON_LIB_PATH=C:/Users/nstier/AppData/Local/Continuum/anaconda3/Lib
    SET TEMP=C:\Users\nstier\AppData\Local\Temp
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1
    SET TF_CUDA_VERSION=9.0
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_SYSTEM_LIBS=zlib_archive
    SET TMP=C:\Users\nstier\AppData\Local\Temp
  external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.bat /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /DRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351wd4291 /wd4250 /wd4996 /Iexternal/grpc /Ibazel-out/x64_windows-opt/genfiles/external/grpc /Iexternal/bazel_tools /Ibazel-out/x64_winws-opt/genfiles/external/bazel_tools /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Iexternal/gc/include /Ibazel-out/x64_windows-opt/genfiles/external/grpc/include /Ibazel-out/x64_windows-opt/bin/external/grpc/include /DGRPC_AR=0 /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX /I/c/Users/nstier/tensorflow/ExternalPackages/zlib-1.2.11-t1-VC15.8.1/include /I/c/Uss/nstier/tensorflow/ExternalPackages/zlib-1.2.11-t1-VC15.8.1/include /I/c/Users/nstier/tensorflow/ExternalPackages/zlib-1.2.11-t1-VC.8.1/include /Fobazel-out/x64_windows-opt/bin/external/grpc/_objs/grpc_transport_chttp2/hpack_parser.o /c external/grpc/src/core/extransport/chttp2/transport/hpack_parser.cc
external/grpc\src/core/lib/compression/stream_compression.h(27): fatal error C1083: Cannot open include file: 'zlib.h': No such file or directory </b>
INFO: Elapsed time: 33.850s, Critical Path: 3.71s
INFO: 0 processes.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
</pre>
Any ideas?

This line from the logs looks interesting: ""set INCLUDE=..."". Not sure how to get the directories that I want into that list. And if I manage to do that, I might end up with the ""undeclared dependency error.""
"
24029,Inference Accuracy Regression for Resnet50 and Densenet,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NA
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Centos 7.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): master branch, commit 0c1eb886 onward
- Python version: 2.7
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source): 6.3.0
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**
Inference accuracy for Resnet50 and Densenet has dropped to zero on CPU. This commit https://github.com/tensorflow/tensorflow/commit/0c1eb8861624d6d17c797b70d25330711df5eb2f was found to be causing the regression.

**Describe the expected behavior**
Resnet50 inference accuracy should look something close to (Top1, Top5) = (0.7315, 0.9109).
"
24028,TensorFlow estimator train_and_evaluate loss is None after step 0 and model does not train,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 14.04.5 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No
- TensorFlow installed from (source or binary):
Binary
- TensorFlow version (use command below):
'1.4.0'
- Python version:
Python 2.7.12
- Bazel version (if compiling from source):
- CUDA/cuDNN version:
Using CPU
**Describe the current behavior**
I've built a tensorflow custom estimator using Keras layers, and it worked fine initially when I used `train_and_evaluate`, but I'm seeing now that when I am using `train_and_evaluate`, it just checkpoints at step 0, the loss being `None` and moves to the evaluate phase. I'm not sure why this is happening, and any suggestions about what to look for would be great
**Describe the expected behavior**
Normal behavior as expected from the `train_and_evaluate` function
**Code to reproduce the issue**
```
# I've wrapped the estimator train_and_evaluate function inside this function:
def train_and_evaluate():    

#     classifier = tf.estimator.Estimator(model_fn = dnn_custom_estimator_v2,
#                                         config = run_config,
#                                         params=hparams)

    classifier = tf.estimator.Estimator(model_fn = dnn_custom_estimator_v2,
                                     params=hparams,
                                     config=run_config)

    train_spec = tf.estimator.TrainSpec(
        input_fn = lambda: csv_input_fn(
            TRAIN_DATA_FILES_PATTERN,
            mode = tf.estimator.ModeKeys.TRAIN,
            num_epochs=1000
        ),
        max_steps=6000,
        hooks=None
    )

    eval_spec = tf.estimator.EvalSpec(
        input_fn = lambda: csv_input_fn(
            TEST_DATA_FILES_PATTERN,
            mode=tf.estimator.ModeKeys.EVAL,
            num_epochs=1  
        ),
        exporters=[tf.estimator.LatestExporter(
            name=""predict"", # the name of the folder in which the model will be exported to under export
            serving_input_receiver_fn=json_serving_input_fn,
            exports_to_keep=1,
            as_text=False)],
        steps=None
    )
    
    tf.estimator.train_and_evaluate(
        estimator=classifier,
        train_spec=train_spec, 
        eval_spec=eval_spec
    )
```
```
# The estimator function:
he_init = tf.keras.initializers.he_normal()

def build_dense_layer(X, n_units=32, activation=tf.keras.activations.relu, initialization=he_init,
                          batch_normalization=False, kernel_regularizer=None, training=False, name=None):
    layer = tf.keras.layers.Dense(n_units,
                              activation=activation,
                              kernel_initializer = he_init,
                              kernel_regularizer = kernel_regularizer,
                              name=name)(X)
    if batch_normalization:
        bn = tf.keras.layers.BatchNormalization(momentum=0.90)
        layer = bn(layer, training=training)

    return layer



def dnn_custom_estimator_v2(features, labels, mode, params):
    in_training = mode == tf.estimator.ModeKeys.TRAIN
    # Returns the feature columns after transforming them
    continuos_feature_cols, dense_vector_feature_cols, embedding_feature_cols, weight_col = get_feature_columns()
    
    ### Build Vector Network
    vec_features = {}
    for feature_name in [some col names]:
        vec_features[feature_name] = features[feature_name]

    vec_net_ip = tf.feature_column.input_layer(vec_features, dense_vector_feature_cols)
    ## Build embedding Network
    embedding_features = {}
    for feature_name in [Column names]:
        embedding_features[feature_name] = features[feature_name]
    weight_col = features['sample_weight']
    embedding_net_ip = tf.feature_column.input_layer(embedding_features, feature_columns = embedding_feature_cols)
    ## Continous feature Network
    continous_cols = [Some column names]

    continous_features = {}
    for feature_name in continous_cols:
        if feature_name != 'sample_weight':
            continous_features[feature_name] = features[feature_name]

    continous_net_ip = tf.feature_column.input_layer(continous_features, feature_columns = continuos_feature_cols)

    ## Merge continous, embedding and vec layers together
    merged_layer = tf.keras.layers.concatenate([vec_net_ip, embedding_net_ip, continous_net_ip])

    ## OP deep dense layer
    output_hidden_1 = build_dense_layer(merged_layer,
                                        n_units=128, training=in_training, 
                                        batch_normalization = False, 
                                        activation = tf.keras.activations.relu, name = 'output_hidden_1')
    output_hidden_2 = build_dense_layer(output_hidden_1, n_units=64, 
                        training=in_training, batch_normalization = False,
                        activation = tf.keras.activations.relu, 
                        name = 'output_hidden_2')
    
    output_hidden_3 = build_dense_layer(output_hidden_2, n_units=32, 
                                    training=in_training, 
                                    batch_normalization = False,
                                activation = tf.keras.activations.relu, name = 'output_hidden_3')
    output_layer_size = len(TARGET_LABELS)

    logits = build_dense_layer(output_hidden_3,
                             n_units=output_layer_size, 
                            activation=None, name='prob_output')
    output = tf.squeeze(logits)

    # Provide an estimator spec for `ModeKeys.PREDICT`.
    if mode == tf.estimator.ModeKeys.PREDICT:
        probabilities = tf.nn.softmax(logits)
        predicted_indices = tf.argmax(probabilities, 1)
        # Convert predicted_indices back into strings
        predictions = {
            'classes': tf.gather(TARGET_LABELS, predicted_indices),
            'scores': probabilities
        }
        export_outputs = {
            'prediction': tf.estimator.export.PredictOutput(predictions)
        }

        # Provide an estimator spec for `ModeKeys.PREDICT` modes.
        return tf.estimator.EstimatorSpec(mode,
                                          predictions=predictions,
                                          export_outputs=export_outputs)

    # Calculate loss using softmax cross entropy
    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(
            logits=logits, labels=labels)
    
    loss = tf.reduce_mean(weight_col*losses)
    
    tf.summary.scalar('loss', loss)

    if mode == tf.estimator.ModeKeys.TRAIN:
        # Create Optimiser
        optimizer = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.9, beta2=0.999,  epsilon=1e-8)

        # Create training operation
        train_op = optimizer.minimize(
            loss=loss, global_step=tf.train.get_global_step())

        # Provide an estimator spec for `ModeKeys.TRAIN` modes.
        return tf.estimator.EstimatorSpec(mode=mode,
                                          loss=loss,
                                          train_op=train_op)


    if mode == tf.estimator.ModeKeys.EVAL:
        probabilities = tf.nn.softmax(logits)
        predicted_indices = tf.argmax(probabilities, 1)

        # Return accuracy and area under ROC curve metrics
        labels_one_hot = tf.one_hot(
            labels,
            depth=len(TARGET_LABELS),
            on_value=True,
            off_value=False,
            dtype=tf.bool
        )

        eval_metric_ops = {
            'accuracy': tf.metrics.accuracy(labels, predicted_indices),
            'auroc': tf.metrics.auc(labels_one_hot, probabilities),
            'precision': tf.metrics.precision(labels, predicted_indices),
            'recall': tf.metrics.recall(labels, predicted_indices)
        }

        # Provide an estimator spec for `ModeKeys.EVAL` modes.
        return tf.estimator.EstimatorSpec(mode,
                                          loss=loss,
                                          eval_metric_ops=eval_metric_ops)
```



**Other info / logs**
The output of `train_and_evaluate` as defined above is this:
```
INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_tf_random_seed': 19830610, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f86f040dc50>, '_model_dir': ' /whereever/model/needs/to/be/saved/model_v2', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_save_summary_steps': 100, '_num_ps_replicas': 0}
INFO:tensorflow:Running training and evaluation locally (non-distributed).
INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.

* Output from  data input_fn:
================
Input file(s):  /whereever/train/is/data/train_equal_downsample.csv
Batch size: 1024
Epoch Count: 1000
Mode: train
Thread Count: 32
Shuffle: True
================

('target_dtype', <tf.Tensor 'DecodeCSV:9' shape=(?,) dtype=int32>)
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Saving checkpoints for 0 into  /whereever/model/needs/to/be/saved//model.ckpt.
INFO:tensorflow:Loss for final step: None.

* data input_fn:
================
Input file(s):  /whereever/test/is/test_v2.csv
Batch size: 1024
Epoch Count: 1
Mode: eval
Thread Count: 32
Shuffle: False
================

('target_dtype', <tf.Tensor 'DecodeCSV:9' shape=(?,) dtype=int32>)
INFO:tensorflow:Starting evaluation at 2018-11-28-21:52:28
INFO:tensorflow:Restoring parameters from /wherever/model/is/model_v2/model.ckpt-0
INFO:tensorflow:Finished evaluation at 2018-11-28-22:14:58
INFO:tensorflow:Saving dict for global step 0: accuracy = 0.91067266, auroc = 0.94267476, global_step = 0, loss = 0.5579337, precision = 0.14235616, recall = 0.002937618
INFO:tensorflow:Restoring parameters from /wherever/model/is/model_v2/model.ckpt-0
INFO:tensorflow:Assets added to graph.
INFO:tensorflow:No assets to write.
INFO:tensorflow:SavedModel written to: /wherever/model/is/export/predict/temp-1543443300/saved_model.pb

* data input_fn:
================
Input file(s): /wherever/this/file/is/train_equal_downsample.csv
Batch size: 1024
Epoch Count: 1000
Mode: train
Thread Count: 32
Shuffle: True
================

('target_dtype', <tf.Tensor 'DecodeCSV:9' shape=(?,) dtype=int32>)
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Restoring parameters from /whereever/model/needs/to/be/saved/model.ckpt-0
INFO:tensorflow:Saving checkpoints for 0 into  /whereever/model/needs/to/be/saved//model.ckpt.
INFO:tensorflow:Loss for final step: None.
WARNING:tensorflow:No new checkpoint ready for evaluation. Skip the current evaluation pass as evaluation results are expected to be same for the same checkpoint.


RuntimeErrorTraceback (most recent call last)
<ipython-input-52-858f040bdf6d> in <module>()
      6 if(not os.path.exists(output_dir)):
      7     os.makedirs(output_dir)
----> 8 train_and_evaluate()

<ipython-input-51-6feac2ec48d2> in train_and_evaluate()
     36         estimator=classifier,
     37         train_spec=train_spec,
---> 38         eval_spec=eval_spec
     39     )

/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.pyc in train_and_evaluate(estimator, train_spec, eval_spec)
    428       config.task_type != run_config_lib.TaskType.EVALUATOR):
    429     logging.info('Running training and evaluation locally (non-distributed).')
--> 430     executor.run_local()
    431     return
    432 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.pyc in run_local(self)
    618       if not metrics:
    619         #  This is unexpected. Training should always end with a new checkpoint.
--> 620         raise RuntimeError('There was no new checkpoint after the training.')
    621 
    622       if _should_stop_local_train(metrics[ops.GraphKeys.GLOBAL_STEP]):

RuntimeError: There was no new checkpoint after the training.


```"
24027,Duplicated variable names with inner tf.keras.layers.Dense layers in eager mode,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL7 3.10.0-862.11.6.el7.x86_64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): conda
- TensorFlow version (use command below): 1.11.0
- Python version: Python 3.6.6 :: Anaconda, Inc.
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: -
- GPU model and memory: -

**Describe the current behavior**

When creating a custom `tf.keras.Model` with custom `build()` function, the variable name scopes seem not to propagate correctly in `tf.keras.Dense` layers.

**Describe the expected behavior**

Variables of inner `tf.keras.Dense` layers should have different names, such as:
`dense/kernel:0`
`dense/bias:0`
`dense_1/kernel:0`
`dense_1/bias:0`

Right now they have the same name:
`kernel:0`
`bias:0`
`kernel:0`
`bias:0`

This is problematic for saving / loading parameters.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import tensorflow as tf
import os

os.environ['CUDA_VISIBLE_DEVICES'] = '-1'

tf.enable_eager_execution()
tf.executing_eagerly()


class MyModel(tf.keras.Model):
    def __init__(self):
        super().__init__(name='mymodel')
        
        self.f = tf.keras.layers.Dense(units=10)
        self.g = tf.keras.layers.Dense(units=10)

#         Workaround
#         self.f = tf.keras.Sequential([tf.keras.layers.Dense(units=10)])
#         self.g = tf.keras.Sequential([tf.keras.layers.Dense(units=10)])

    def build(self, input_shapes):
        self.f.build(input_shapes[0])
        self.g.build(input_shapes[1])
        self.built = True
    
    def call(self, x, y):
        return self.f(x) + self.g(y)


model = MyModel()
model.build([(None, 5), (None, 3)])

for v in model.variables:
    print(v.name)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24026,Abs operator not implemented?,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora release 28
- TensorFlow installed from (source or binary): binary (conda)
- TensorFlow version (or github SHA if from source): '1.13.0-dev20181127'


**Provide the text output from tflite_convert**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONV_2D, DEPTHWISE_CONV_2D, L2_NORMALIZATION, MUL, RELU, SQUEEZE, SUB. Here is a list of operators for which you will need custom implementations: Abs.
```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24025,Does XLA GPU ptxas recompile on each session.run()?,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 1.12
- Doc Link: https://www.tensorflow.org/xla/tfcompile

**Describe the documentation issue**

I'm seeing undocumented behavior where NVPTXCompiler::CompilePtxOrGetCachedResult(const string& ptx) is being called with new ptx code on each invocation of session.run().

1. Is it intended behavior that XLA JIT is invoked with each session.run() call? I'm currently calling session.run() multiple times by invoking q.dequeue() repeatedly.
2. Currently it takes longer to compile the ptxas code than to actually execute the rest of the code.
3. Should we restructure our code so we minimize the number of session.run() calls to make JIT behave nicely?

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
24024,AttributeError: 'IndexedSlices' object has no attribute '_copy',"Hello,
This is puzzling, as this only happens with `tensorflow-gpu` version 1.12.0 -- `tensorflow` version 1.12.0 works flawlessly (both on GNU/Linux and OSX).

```
Traceback (most recent call last):
  File ""./bin/moe-cli.py"", line 476, in <module>
    main(sys.argv[1:])
  File ""./bin/moe-cli.py"", line 372, in main
    gradients = tape.gradient(loss, trainable_variables)
  File ""/home/testing/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py"", line 901, in gradient
    output_gradients=output_gradients)
  File ""/home/testing/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py"", line 64, in imperative_grad
    output_gradients)
  File ""/home/testing/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 858, in grad_fun
    return [dresult._copy(device_name=self_device)]
AttributeError: 'IndexedSlices' object has no attribute '_copy'
```
"
24023,ValueError: tf.enable_eager_execution must be called at program startup.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
NA
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
1.12.0
- Python version:
3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
9/7
- GPU model and memory:
NVIDIA GTX1080, 8GB

**Describe the current behavior**
Eager execution isn't working on a jupyter notebook with GPU. Have tried restarting the kernel multiple times. I get a ValueError: tf.enable_eager_execution must be called at program startup.

**Describe the expected behavior**
Eager Execution should work. Why isn't it working?

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
`import tensorflow as tf
`tf.enable_eager_execution()

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-1-e43ec884c055> in <module>()
      5 import numpy as np
      6 import tensorflow as tf
----> 7 tf.enable_eager_execution()
      8 from keras import layers
      9 from keras import models

~\AppData\Local\conda\conda\envs\DLCdependencies\lib\site-packages\tensorflow\python\framework\ops.py in enable_eager_execution(config, device_policy, execution_mode)
   5421         device_policy=device_policy,
   5422         execution_mode=execution_mode,
-> 5423         server_def=None)
   5424 
   5425 

~\AppData\Local\conda\conda\envs\DLCdependencies\lib\site-packages\tensorflow\python\framework\ops.py in enable_eager_execution_internal(config, device_policy, execution_mode, server_def)
   5465     if graph_mode_has_been_used:
   5466       raise ValueError(
-> 5467           ""tf.enable_eager_execution must be called at program startup."")
   5468   context.default_execution_mode = context.EAGER_MODE
   5469   # pylint: disable=protected-access

ValueError: tf.enable_eager_execution must be called at program startup."
24022,Offset when rotating image and points by the same random angle,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 14.04, Os X 10.14
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): conda and pip
- TensorFlow version (use command below): 1.8
- Python version: 2.7, 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


**Describe the current behavior**

I have an image and 3 points. I want to rotate the image and the points together. To this end, I rotate the image by some angle a and the points by the same angle. When a is fixed to a python scalar (say pi/3), the rotation works fine (cf. image below, the blue dots are on the dark squares).

![rotation](https://user-images.githubusercontent.com/16196950/49168292-d5304600-f33f-11e8-87c5-c1b73b84a053.jpg)


When the angle is randomly chosen with `angle = tf.random_uniform([])`, there is an offset between the rotated image and the rotated points.

![rotation](https://user-images.githubusercontent.com/16196950/49168588-7c14e200-f340-11e8-8a21-9060bea077bf.jpg)


**Code to reproduce the issue**

```
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# create toy image
square = np.zeros((1, 800, 800, 3))
square[:, 100:400, 100:400] = 1
square[:, 140:180, 140:180] = 0
square[:, 240:280, 240:280] = 0
square[:, 280:320, 280:320] = 0
kp = np.array([[160, 160], [260, 260], [300, 300]])
kp = np.expand_dims(kp, axis=0)

def _rotate(image, keypoints, angle, keypoints_num):
    image = tf.contrib.image.rotate(image, angle)
    cos, sin = tf.cos(angle), tf.sin(angle)
    x0, y0  = .5, .5
    rot_mat = tf.Variable([[cos, -sin], [sin, cos]], trainable=False)
    keypoints -= (x0, y0)
    keypoints = tf.reshape(keypoints, shape=[-1, 2])
    keypoints = tf.matmul(keypoints, rot_mat)
    keypoints = tf.reshape(keypoints, shape=[-1, keypoints_num, 2])
    keypoints += (x0, y0)
    return image, keypoints


image = tf.placeholder(tf.float32, [None, 800, 800, 3])
keypoints = tf.placeholder(tf.float32, [None, 3, 2])

angle = np.pi / 3 # fix angle, works fine
#angle = tf.random_uniform([]) # random angle, does not work
image_r, keypoints_r = _rotate(image, keypoints / 800, angle, 3)
keypoints_r *= 800

sess = tf.Session()
sess.run(tf.initialize_all_variables())

imr, kr = sess.run([image_r, keypoints_r], feed_dict={image: square, keypoints:kp})

# displaying output
plt.imshow(imr[0])
plt.scatter(*zip(*kr[0]))
plt.savefig('rotation.jpg')
```"
24020,[ppc64le] Getting error installing a custom tensorflow wheel on Power8 with NVIDIA P100 with anaconda3,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
LSB Version:	:core-4.1-noarch:core-4.1-ppc64le
Distributor ID:	RedHatEnterpriseServer
Description:	Red Hat Enterprise Linux Server release 7.4 (Maipo)
Release:	7.4
Codename:	Maipo

- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.8
- Python version: 3.6.5
- Installed using virtualenv? pip? conda?: conda (anaconda3 5.2)
- Bazel version (if compiling from source): 0.10.0
- GCC/Compiler version (if compiling from source): 4.8.5
- CUDA/cuDNN version: 8.0/ 7.0.5
- GPU model and memory: NVIDIA Tesla P100/ 16 GB



**Describe the problem**
Getting the following error while building a tensorflow wheel on power8 system.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
CUDA_VER=8.0
CUDNN_VER=7.0.5
NCCL_VER=2.3.7-1
BAZEL_VER=0.10.0
TF_VER=1.8.0
CONDA_VER=5.2.0
PY_VER=3.6

#module load cuda/8.0.61-1
CUDA_DIR=$(pwd)/8.0.61-1

#install pre-built anaconda
wget https://repo.anaconda.com/archive/Anaconda3-$CONDA_VER-Linux-ppc64le.sh -O anaconda3.sh
bash ./anaconda3.sh -b -p anaconda3
export PATH=$(pwd)/anaconda3/bin:$PATH
pip install --upgrade pip
pip install keras_applications==1.0.6 --no-deps
pip install keras_preprocessing==1.0.4 --no-deps
#install bazel 
wget https://github.com/bazelbuild/bazel/releases/download/$BAZEL_VER/bazel-$BAZEL_VER-dist.zip
unzip bazel-$BAZEL_VER-dist.zip -d bazel
cd bazel
./compile.sh
export PATH=$(pwd)/output:$PATH
cd ../

#workaround for nccl2 
git clone https://github.com/NVIDIA/nccl
cd nccl
git checkout v$NCCL_VER
make -j160 src.build CUDA_HOME=$CUDA_DIR
make pkg.txz.build CUDA_HOME=$CUDA_DIR
tar -xf  build/pkg/txz/* -C ..
cd ..
ln -s nccl_$NCCL_VER* nccl2
cd nccl_$NCCL_VER*
ln -s LICENSE.txt NCCL-SLA.txt
cd ..

CUDNN_DIR=$CUDA_DIR

#setup env var
export PYTHON_BIN_PATH=$(pwd)/anaconda3/bin/python3
export PYTHON_LIB_PATH=$(pwd)/anaconda3/lib/python$PY_VER/site-packages
export TF_NEED_MKL=0
export CC_OPT_FLAGS=""-march=native""
export TF_NEED_JEMALLOC=0
export TF_NEED_GCP=0
export TF_NEED_HDFS=0
export TF_ENABLE_XLA=0
export TF_NEED_OPENCL=0
export TF_NEED_CUDA=1
export TF_CUDA_CLANG=0
export TF_CUDA_VERSION=$CUDA_VER
export CUDA_TOOLKIT_PATH=$CUDA_DIR
export TF_CUDNN_VERSION=$CUDNN_VER
export CUDNN_INSTALL_PATH=$CUDNN_DIR
export TF_CUDA_COMPUTE_CAPABILITIES=""6.0""
export TF_NEED_VERBS=0
export TF_NEED_AWS=0
export TF_NEED_NGRAPH=0
export TF_NEED_S3=0
export TF_NEED_GDR=0
export TF_NEED_OPENCL_SYCL=0
export GCC_HOST_COMPILER_PATH=/usr/bin/gcc
export TF_NEED_MPI=0
export TF_NEED_KAFKA=0
export TF_NEED_ROCM=0
export TF_NEED_IGNITE=0
export TF_NEED_TENSORRT=0
export TF_SET_ANDROID_WORKSPACE=0
export TF_NCCL_VERSION=$(echo $NCCL_VER | cut -d. -f1,2)
export NCCL_INSTALL_PATH=""$(pwd)/nccl2""
export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:$CUDA_DIR/lib64/stubs

#build tensorflow
git clone --recursive https://github.com/tensorflow/tensorflow
cd tensorflow
git checkout v$TF_VER
./configure
bazel build -c opt --config=cuda tensorflow/tools/pip_package:build_pip_package
bazel-bin/tensorflow/tools/pip_package/build_pip_package ../tensorflow_pkg

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
ERROR: /autofs/nccs-svm1_home1/shubhankar/summitdev/tensorflow/tensorflow/core/kernels/BUILD:1201:1: output 'tensorflow/core/kernels/_objs/gather_functor_gpu/tensorflow/core/kernels/gather_functor_gpu.cu.pic.o' was not created
INFO: From Compiling tensorflow/core/kernels/quantization_utils.cc:
In file included from tensorflow/core/kernels/quantization_utils.cc:16:0:
./tensorflow/core/kernels/quantization_utils.h:35:0: warning: ""GEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK"" redefined [enabled by default]
 #define GEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK
 ^
<command-line>:0:0: note: this is the location of the previous definition
In file included from tensorflow/core/kernels/quantization_utils.cc:16:0:
./tensorflow/core/kernels/quantization_utils.h: In function 'void tensorflow::RequantizeManyInNewRangeReference(const qint32*, tensorflow::int64, float, float, float, float, tensorflow::quint8*)':
./tensorflow/core/kernels/quantization_utils.h:269:34: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (size_t index = 0; index < count; ++index) {
                                  ^
ERROR: /autofs/nccs-svm1_home1/shubhankar/summitdev/tensorflow/tensorflow/core/kernels/BUILD:1201:1: not all outputs were created or valid
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 221.932s, Critical Path: 161.23s
FAILED: Build did NOT complete successfully
```"
24013,MirroredStrategy not work with estimator,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): Docker
- TensorFlow version (use command below): 1.12.0-rc0
- Python version: 3.5.2
- CUDA/cuDNN version: V9.0.176
- GPU model and memory: TITAN Xp

**Describe the current behavior**
My code looks like
```python
dist_strategy = tf.contrib.distribute.MirroredStrategy()

run_config = tf.estimator.RunConfig(
    train_distribute=dist_strategy,
    log_step_count_steps=10)

estimator = tf.estimator.Estimator(
    model_fn,
    model_dir=params.ckpt_dir,
    params=params,
    config=run_config)

estimator.train(train_input_fn, max_steps=steps)
```

**Describe the expected behavior**
Successfully train.

**Other info / logs**
```log
Traceback (most recent call last):
  File ""main.py"", line 109, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""main.py"", line 75, in main
    estimator.train(train_input_fn, max_steps=steps)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 354, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1181, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1287, in _train_model_distributed
    grouped_estimator_spec = self._train_distribution.call_for_each_tower(
AttributeError: 'MirroredStrategy' object has no attribute 'call_for_each_tower'
```
"
24012,java.lang.RuntimeException: Native TF methods not found; check that the correct native libraries are present in the APK.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA
- TensorFlow installed from (source or binary): source
- TensorFlow version:1.6
- Python version:3.6
- Installed using virtualenv? pip? conda?:pip
- Bazel version (if compiling from source): 0.16.1
- GCC/Compiler version (if compiling from source): - 
- CUDA/cuDNN version: - 
- GPU model and memory: - 



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
 
bazel build -c opt //tensorflow/examples/android:libtensorflow_demo.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0""


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24009,Keras feature DropConnect,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>

**System information**
- TensorFlow version (you are using):1.1.0 (Keras version: 2.2.2)
- Are you willing to contribute it (Yes/No):Yes

**Describe the feature and the current behavior/state.**
I would like to know if you would accept a pull request in ``DropConnect,i.e., https://cs.nyu.edu/~wanli/dropc/, which is a generalization of Hintons Dropout. 
In experiments, we may sometimes want to drop edges(weights) instead of neurons, I think it is very convenient to have the general version for regularizing layers. 
If so, I am willing to work on this. I would add a flag to the operator so that users can decide it to run. This changes should not be difficult to make.

**Will this change the current api? How?**
No. Probably based on current api. 

**Who will benefit with this feature?**
Users build their own architectures/algorithms.  
**Any Other info.**
"
24006,The tensorflow/tensorflow:custom-op is missing from DockerHub,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): docker pull tensorflow/tensorflow:custom-op
- TensorFlow version: n/a
- Python version: n/a
- Installed using virtualenv? pip? conda?: `docker pull`
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a



**Describe the problem**

While trying to pull `tensorflow/tensorflow:custom-op` to build custom ops, I noticed that `custom-op` tag is missing. (The tag was still there around 11/20/2018)

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
ubuntu@ubuntu:~/tensorflow_io$ docker pull tensorflow/tensorflow:custom-op
Error response from daemon: manifest for tensorflow/tensorflow:custom-op not found
```
**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached."
24005,tf.ConditionalAccumulator does not behave as expected,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.12.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):  binary
- TensorFlow version (use command below): 1.10 cpu
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 
- GPU model and memory:

**Describe the current behavior**
Result of the accumulator.take_grad() not predictable if apply more gradients than num_required.
**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
NUM_REQUIRED = 3

a = np.arange(12).reshape((3, 4)).astype(np.float32)
dataset = tf.data.Dataset.from_tensor_slices((a, a))
dataset = dataset.repeat(1000).batch(1)
iterator = dataset.make_one_shot_iterator()
x, y = iterator.get_next()

predict = tf.layers.dense(x, 4)
loss = tf.nn.l2_loss(predict - y)

optimizer = tf.train.AdamOptimizer(learning_rate=0.01)
trainable_vars = tf.trainable_variables()[1]

grads_vars = optimizer.compute_gradients(loss, var_list=[trainable_vars])
grads = [grad_var[0] for grad_var in grads_vars]
vars = [grad_var[1] for grad_var in grads_vars]
shapes = [grad_var[1].get_shape() for grad_var in grads_vars]

local_step = tf.get_variable('accu_local_step', dtype=tf.int32, initializer=0, trainable=False)
accumulators = [tf.ConditionalAccumulator(tf.float32, shape=shapes[i]) for i in range(len(vars))]
with tf.control_dependencies([tf.assign_add(local_step, 1)]):
    apply_ops = [accumulators[i].apply_grad(grads[i], local_step=local_step) for i in range(len(vars))]
    apply_ops = tf.group(*apply_ops)
grads_mean = [accumulators[i].take_grad(num_required=NUM_REQUIRED) for i in range(len(vars))]
num_acc = [accumulators[i].num_accumulated() for i in range(len(vars))]

# #
sess_local = tf.train.MonitoredSession()
for i in range(100):
    # print(i)
    if (i + 1) % (NUM_REQUIRED + 1) != 0:
        local_step_np, num_acc_np, grad_tmp, _ = sess_local.run([local_step, num_acc, grads, apply_ops])
        # sess_local.run(reset_op)
        print('local_step_np is: {}, num_ncc_np is: {}'.format(local_step_np, num_acc_np))
        print(grad_tmp)
    else:
        # grad_tmp, _ = sess_local.run([grads, apply_ops])
        # print(grad_tmp)
        grad_tmp, num_acc_np, _, grads_mean_np = sess_local.run([grads, num_acc, apply_ops, grads_mean])
        print('num_ncc_np is: {}'.format(num_acc_np))
        print(grad_tmp)
    # num_acc_np = sess_local.run(num_acc)
        print('grads_mean_np:', grads_mean_np)
```

**Other info / logs**
Runing the above code gives result:
local_step_np is: 1, num_ncc_np is: [0]
[array([ 1.9753755,  1.9916878,  0.9389882, -5.003906 ], dtype=float32)]
local_step_np is: 2, num_ncc_np is: [1]
[array([  3.120553 ,   1.0842252,   3.9395618, -11.656693 ], dtype=float32)]
local_step_np is: 3, num_ncc_np is: [2]
[array([  4.26573   ,   0.17676353,   6.940136  , -18.30948   ],
      dtype=float32)]
num_ncc_np is: [0]
[array([ 1.9753755,  1.9916878,  0.9389882, -5.003906 ], dtype=float32)]
grads_mean_np: [array([  3.1205528,   1.0842255,   3.9395618, -11.6566925], dtype=float32)]
local_step_np is: 5, num_ncc_np is: [1]
[array([  3.120553 ,   1.0842252,   3.9395618, -11.656693 ], dtype=float32)]
local_step_np is: 6, num_ncc_np is: [2]
[array([  4.26573   ,   0.17676353,   6.940136  , -18.30948   ],
      dtype=float32)]
local_step_np is: 7, num_ncc_np is: [3]
[array([ 1.9753755,  1.9916878,  0.9389882, -5.003906 ], dtype=float32)]
num_ncc_np is: [4]
[array([  3.120553 ,   1.0842252,   3.9395618, -11.656693 ], dtype=float32)]
grads_mean_np: [array([ 2.8342583,  1.3110911,  3.1894183, -9.993496 ], dtype=float32)]
local_step_np is: 9, num_ncc_np is: [1]
[array([  4.26573   ,   0.17676353,   6.940136  , -18.30948   ],
      dtype=float32)]
local_step_np is: 10, num_ncc_np is: [2]
[array([ 1.9753755,  1.9916878,  0.9389882, -5.003906 ], dtype=float32)]
local_step_np is: 11, num_ncc_np is: [3]
[array([  3.120553 ,   1.0842252,   3.9395618, -11.656693 ], dtype=float32)]
num_ncc_np is: [0]
[array([  4.26573   ,   0.17676353,   6.940136  , -18.30948   ],
      dtype=float32)]
grads_mean_np: [array([  3.1205528,   1.0842254,   3.9395618, -11.656693 ], dtype=float32)]
local_step_np is: 13, num_ncc_np is: [1]
[array([ 1.9753755,  1.9916878,  0.9389882, -5.003906 ], dtype=float32)]
local_step_np is: 14, num_ncc_np is: [2]

grads_mean_np is not the mean of previously applied grad. 

If we just set run 'apply_ops' 'NUM_REQUIRED' times before run 'grads_mean'(take_grads()), ConditionalAccumulator seems to work properly, but when we run different number of 'apply_ops' before(as in the example, we run 'NUM_REQUIRED' + 1 times), the computed grads_mean(averaged grads) becomes mysterious. 
"
24003,Unable to train a LinearClassifier with categorical columns and CollectiveAllReduce,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0
- Python version: 3.6.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

I'm trying to train a simple model LinearClassifier (tf.estimator.LinearClassifier) with different distribution strategies. 
I've successfully managed to train a model with parameter servers with numeric columns (tf.feature_column.numeric_column) and categorical columns (tf.feature_column.categorical_column_with_*). 
I've also successfully managed to train a model with CollectiveAllReduce and only numeric columns. 
But unfortunately, I'm getting the following error whith the same model (with CollectiveAllReduce) but with one categorical column in place of a numeric column:

> ValueError: `IndexSlices` is not supported for Collective All-Reduce.

See below for the all traceback and logs.

Here is a part of the code I am running:

```
estimator = tf.estimator.LinearClassifier(
        feature_columns=[
            tf.feature_column.categorical_column_with_hash_bucket(""partnerid"", 13, dtype=tf.int64),
            tf.feature_column.numeric_column(""campaignid"", dtype=tf.int64)
        ],
        model_dir=""my_path"",
        n_classes=2,
        optimizer=""Adam"",
        config=tf.estimator.RunConfig(
            experimental_distribute=tf.contrib.distribute.DistributeConfig(
                train_distribute=tf.contrib.distribute.CollectiveAllReduceStrategy(),
                remote_cluster=cluster_spec
            )
        )
    )

    tf.estimator.train_and_evaluate(
        estimator,
        tf.estimator.TrainSpec(
            input_fn_train,
            max_steps=training_steps
        ),
        tf.estimator.EvalSpec(
            input_fn_test,
            steps=evaluation_steps,
            start_delay_secs=0,
            throttle_secs=evaluation_throttle_secs
        )
    )
```
Is it a known issue ? Are there current limitations with CollectiveAllReduce ?

```
INFO:tensorflow:Waiting for worker:0/init
tensorflow - Waiting for worker:0/init
INFO:tensorflow:Waiting for worker:1/init
tensorflow - Waiting for worker:1/init
cluster_spec: ClusterSpec({'worker': ['10.188.17.14:42897', '10.188.50.21:48063']})
INFO:tensorflow:CollectiveAllReduceStrategy with local_devices = ['/device:CPU:0']
tensorflow - CollectiveAllReduceStrategy with local_devices = ['/device:CPU:0']
INFO:tensorflow:Initializing RunConfig with distribution strategies.
tensorflow - Initializing RunConfig with distribution strategies.
INFO:tensorflow:RunConfig initialized for Distribute Coordinator with STANDALONE_CLIENT mode
tensorflow - RunConfig initialized for Distribute Coordinator with STANDALONE_CLIENT mode
INFO:tensorflow:Using config: {'_model_dir': 'hdfs://root/user/username/model_dir', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.contrib.distribute.python.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x7f1c4a7bef60>, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': DistributeConfig(train_distribute=<tensorflow.contrib.distribute.python.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x7f1c4a7bef60>, eval_distribute=None, remote_cluster=<tensorflow.python.training.server_lib.ClusterSpec object at 0x7f1c4a7d3208>), '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f1c4a7d3208>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': 'standalone_client'}
tensorflow - Using config: {'_model_dir': 'hdfs://root/user/username/model_dir', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.contrib.distribute.python.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x7f1c4a7bef60>, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': DistributeConfig(train_distribute=<tensorflow.contrib.distribute.python.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x7f1c4a7bef60>, eval_distribute=None, remote_cluster=<tensorflow.python.training.server_lib.ClusterSpec object at 0x7f1c4a7d3208>), '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f1c4a7d3208>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': 'standalone_client'}
INFO:tensorflow:Running `train_and_evaluate` with Distribute Coordinator.
tensorflow - Running `train_and_evaluate` with Distribute Coordinator.
INFO:tensorflow:Running Distribute Coordinator with mode = 'standalone_client', cluster_spec = {'worker': ['10.188.17.14:42897', '10.188.50.21:48063']}, task_type = None, task_id = None, environment = None, rpc_layer = 'grpc'
tensorflow - Running Distribute Coordinator with mode = 'standalone_client', cluster_spec = {'worker': ['10.188.17.14:42897', '10.188.50.21:48063']}, task_type = None, task_id = None, environment = None, rpc_layer = 'grpc'
WARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.
tensorflow - `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.
INFO:tensorflow:Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'worker': ['10.188.17.14:42897', '10.188.50.21:48063']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ['/job:worker/task:0']
tensorflow - Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'worker': ['10.188.17.14:42897', '10.188.50.21:48063']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ['/job:worker/task:0']
INFO:tensorflow:Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'worker': ['10.188.17.14:42897', '10.188.50.21:48063']}, task_type = 'worker', task_id = 1, num_workers = 2, local_devices = ['/job:worker/task:1']
tensorflow - Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'worker': ['10.188.17.14:42897', '10.188.50.21:48063']}, task_type = 'worker', task_id = 1, num_workers = 2, local_devices = ['/job:worker/task:1']
INFO:tensorflow:Updated config: {'_model_dir': 'hdfs://root/user/username/model_dir', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.contrib.distribute.python.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x7f1c4a7d3710>, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': DistributeConfig(train_distribute=<tensorflow.contrib.distribute.python.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x7f1c4a7601d0>, eval_distribute=None, remote_cluster=<tensorflow.python.training.server_lib.ClusterSpec object at 0x7f1c4a760438>), '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f1c4a7603c8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.188.17.14:42897', '_evaluation_master': 'grpc://10.188.17.14:42897', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 2, '_distribute_coordinator_mode': 'standalone_client'}
tensorflow - Updated config: {'_model_dir': 'hdfs://root/user/username/model_dir', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.contrib.distribute.python.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x7f1c4a7d3710>, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': DistributeConfig(train_distribute=<tensorflow.contrib.distribute.python.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x7f1c4a7601d0>, eval_distribute=None, remote_cluster=<tensorflow.python.training.server_lib.ClusterSpec object at 0x7f1c4a760438>), '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f1c4a7603c8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.188.17.14:42897', '_evaluation_master': 'grpc://10.188.17.14:42897', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 2, '_distribute_coordinator_mode': 'standalone_client'}INFO:tensorflow:Updated config: {'_model_dir': 'hdfs://root/user/username/model_dir', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.contrib.distribute.python.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x7f1c4a7d3cc0>, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': DistributeConfig(train_distribute=<tensorflow.contrib.distribute.python.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x7f1c4a760630>, eval_distribute=None, remote_cluster=<tensorflow.python.training.server_lib.ClusterSpec object at 0x7f1c4a760898>), '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f1c4a760828>, '_task_type': 'worker', '_task_id': 1, '_global_id_in_cluster': 1, '_master': 'grpc://10.188.50.21:48063', '_evaluation_master': 'grpc://10.188.50.21:48063', '_is_chief': False, '_num_ps_replicas': 0, '_num_worker_replicas': 2, '_distribute_coordinator_mode': 'standalone_client'}

tensorflow - Updated config: {'_model_dir': 'hdfs://root/user/username/model_dir', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.contrib.distribute.python.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x7f1c4a7d3cc0>, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': DistributeConfig(train_distribute=<tensorflow.contrib.distribute.python.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x7f1c4a760630>, eval_distribute=None, remote_cluster=<tensorflow.python.training.server_lib.ClusterSpec object at 0x7f1c4a760898>), '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f1c4a760828>, '_task_type': 'worker', '_task_id': 1, '_global_id_in_cluster': 1, '_master': 'grpc://10.188.50.21:48063', '_evaluation_master': 'grpc://10.188.50.21:48063', '_is_chief': False, '_num_ps_replicas': 0, '_num_worker_replicas': 2, '_distribute_coordinator_mode': 'standalone_client'}
2018-11-27 13:25:56.606318: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
INFO:tensorflow:Calling model_fn.
tensorflow - Calling model_fn.
WARNING:tensorflow:Partitioned variables are disabled when using DistributionStrategy.
tensorflow - Partitioned variables are disabled when using DistributionStrategy.
INFO:tensorflow:Calling model_fn.
tensorflow - Calling model_fn.
DEBUG:tensorflow:Transforming feature_column _NumericColumn(key='campaignid', shape=(1,), default_value=None, dtype=tf.int64, normalizer_fn=None).
tensorflow - Transforming feature_column _NumericColumn(key='campaignid', shape=(1,), default_value=None, dtype=tf.int64, normalizer_fn=None).
DEBUG:tensorflow:Transforming feature_column _NumericColumn(key='campaignid', shape=(1,), default_value=None, dtype=tf.int64, normalizer_fn=None).
tensorflow - Transforming feature_column _NumericColumn(key='campaignid', shape=(1,), default_value=None, dtype=tf.int64, normalizer_fn=None).
DEBUG:tensorflow:Transforming feature_column _HashedCategoricalColumn(key='partnerid', hash_bucket_size=13, dtype=tf.int64).
tensorflow - Transforming feature_column _HashedCategoricalColumn(key='partnerid', hash_bucket_size=13, dtype=tf.int64).
DEBUG:tensorflow:Transforming feature_column _HashedCategoricalColumn(key='partnerid', hash_bucket_size=13, dtype=tf.int64).
tensorflow - Transforming feature_column _HashedCategoricalColumn(key='partnerid', hash_bucket_size=13, dtype=tf.int64).
INFO:tensorflow:Error reported to Coordinator: `IndexSlices` is not supported for Collective All-Reduce.
Traceback (most recent call last):
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 177, in _call_for_each_tower
    **merge_kwargs)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 661, in _distributed_apply
    variable_scope.VariableAggregation.SUM, grads_and_vars)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/python/training/distribute.py"", line 776, in batch_reduce
    return self._batch_reduce(aggregation, value_destination_pairs)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 628, in _batch_reduce
    value_destination_pairs)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py"", line 243, in batch_reduce
    return self._batch_reduce(aggregation, value_destination_pairs)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py"", line 829, in _batch_reduce
    ""`IndexSlices` is not supported for Collective All-Reduce."")
ValueError: `IndexSlices` is not supported for Collective All-Reduce.
tensorflow - Error reported to Coordinator: `IndexSlices` is not supported for Collective All-Reduce.
Traceback (most recent call last):
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 177, in _call_for_each_tower
    **merge_kwargs)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 661, in _distributed_apply
    variable_scope.VariableAggregation.SUM, grads_and_vars)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/python/training/distribute.py"", line 776, in batch_reduce
    return self._batch_reduce(aggregation, value_destination_pairs)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 628, in _batch_reduce
    value_destination_pairs)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py"", line 243, in batch_reduce
    return self._batch_reduce(aggregation, value_destination_pairs)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py"", line 829, in _batch_reduce
    ""`IndexSlices` is not supported for Collective All-Reduce."")
ValueError: `IndexSlices` is not supported for Collective All-Reduce.
Exception in thread Thread-5:
Traceback (most recent call last):
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_coordinator.py"", line 344, in _run_single_worker
    worker_fn(strategy)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/python/distribute/estimator_training.py"", line 246, in _worker_fn
    hooks=hooks)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1205, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1316, in _train_model_distributed
    self.config)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/python/training/distribute.py"", line 721, in call_for_each_tower
    return self._call_for_each_tower(fn, *args, **kwargs)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 556, in _call_for_each_tower
    return _call_for_each_tower(self, fn, *args, **kwargs)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 183, in _call_for_each_tower
    coord.join(threads)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 177, in _call_for_each_tower
    **merge_kwargs)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 661, in _distributed_apply
    variable_scope.VariableAggregation.SUM, grads_and_vars)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/python/training/distribute.py"", line 776, in batch_reduce
    return self._batch_reduce(aggregation, value_destination_pairs)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 628, in _batch_reduce
    value_destination_pairs)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py"", line 243, in batch_reduce
    return self._batch_reduce(aggregation, value_destination_pairs)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py"", line 829, in _batch_reduce
    ""`IndexSlices` is not supported for Collective All-Reduce."")
ValueError: `IndexSlices` is not supported for Collective All-Reduce.

INFO:tensorflow:Error reported to Coordinator: `IndexSlices` is not supported for Collective All-Reduce.
Traceback (most recent call last):
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 177, in _call_for_each_tower
    **merge_kwargs)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 661, in _distributed_apply
    variable_scope.VariableAggregation.SUM, grads_and_vars)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/python/training/distribute.py"", line 776, in batch_reduce
    return self._batch_reduce(aggregation, value_destination_pairs)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 628, in _batch_reduce
    value_destination_pairs)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py"", line 243, in batch_reduce
    return self._batch_reduce(aggregation, value_destination_pairs)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py"", line 829, in _batch_reduce
    ""`IndexSlices` is not supported for Collective All-Reduce."")
ValueError: `IndexSlices` is not supported for Collective All-Reduce.
tensorflow - Error reported to Coordinator: `IndexSlices` is not supported for Collective All-Reduce.
Traceback (most recent call last):
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 177, in _call_for_each_tower
    **merge_kwargs)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 661, in _distributed_apply
    variable_scope.VariableAggregation.SUM, grads_and_vars)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/python/training/distribute.py"", line 776, in batch_reduce
    return self._batch_reduce(aggregation, value_destination_pairs)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 628, in _batch_reduce
    value_destination_pairs)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py"", line 243, in batch_reduce
    return self._batch_reduce(aggregation, value_destination_pairs)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py"", line 829, in _batch_reduce
    ""`IndexSlices` is not supported for Collective All-Reduce."")
ValueError: `IndexSlices` is not supported for Collective All-Reduce.
Exception in thread Thread-4:
Traceback (most recent call last):
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_coordinator.py"", line 344, in _run_single_worker
    worker_fn(strategy)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/python/distribute/estimator_training.py"", line 246, in _worker_fn
    hooks=hooks)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1205, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1316, in _train_model_distributed
    self.config)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/python/training/distribute.py"", line 721, in call_for_each_tower
    return self._call_for_each_tower(fn, *args, **kwargs)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 556, in _call_for_each_tower
    return _call_for_each_tower(self, fn, *args, **kwargs)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 183, in _call_for_each_tower
    coord.join(threads)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 177, in _call_for_each_tower
    **merge_kwargs)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 661, in _distributed_apply
    variable_scope.VariableAggregation.SUM, grads_and_vars)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/python/training/distribute.py"", line 776, in batch_reduce
    return self._batch_reduce(aggregation, value_destination_pairs)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 628, in _batch_reduce
    value_destination_pairs)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py"", line 243, in batch_reduce
    return self._batch_reduce(aggregation, value_destination_pairs)
  File ""/home/username/miniconda3/envs/explorer2/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py"", line 829, in _batch_reduce
    ""`IndexSlices` is not supported for Collective All-Reduce."")
ValueError: `IndexSlices` is not supported for Collective All-Reduce.
```"
24002,Segfault unless .so is compiled in debug mode,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): v1.11.0-0-gc19e29306c 1.11.0
- Python version: 3.5.2
- Bazel version (if compiling from source): 0.19.0
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
We compile a .so file, loosely based on the docs in https://www.tensorflow.org/xla/tfcompile
We can then compile a binary using that .so
However, at runtime it hits a segfault.
_However_, if we build the .so with `--compilation_mode=dbg` the binary works as expected.

**Describe the expected behavior**
The binary that used the .so works without segfault whether or not the .so is compiled in debug mode or not.

**Code to reproduce the issue**
BUILD looks like this:
```
load(""//tensorflow/compiler/aot:tfcompile.bzl"", ""tf_library"")
load(""//tensorflow:tensorflow.bzl"", ""tf_copts"")

# Use the tf_library macro to compile your graph into executable code.
tf_library(
    name = ""punctuation_decode"",
    cpp_class = ""TensorflowXLA::PunctuationDecode"",
    graph = ""final.pb"",
    config = ""punctuation_decode.config.pbtxt"",
    tfcompile_flags = ""--target_features=+avx""
)

native.cc_binary(
  name = ""libpunctuation.so"",
  srcs = [""punctuation_decode_tfcompile_function.o"", ""punctuation_decode_tfcompile_metadata.o"", ""punctuation_decode.h""],
  linkshared=1,
  linkstatic=1,
  linkopts=[""-fPIC""],
  deps = [
          # TODO(cwhipkey): only depend on kernel code that the model actually needed.
          # ""//tensorflow/compiler/tf2xla/kernels:gather_op_kernel_float_int32"",
          # ""//tensorflow/compiler/tf2xla/kernels:gather_op_kernel_float_int64"",
          ""//tensorflow/compiler/tf2xla/kernels:index_ops_kernel_argmax_float_1d"",
          ""//tensorflow/compiler/tf2xla/kernels:index_ops_kernel_argmax_float_2d"",
          # ""//tensorflow/compiler/aot:runtime"",
          # ""//tensorflow/compiler/tf2xla:xla_local_runtime_context"",
          ""//tensorflow/compiler/xla/service/cpu:runtime_conv2d"",
          ""//tensorflow/compiler/xla/service/cpu:runtime_matmul"",
          ""//tensorflow/compiler/xla/service/cpu:runtime_single_threaded_conv2d"",
          ""//tensorflow/compiler/xla/service/cpu:runtime_single_threaded_matmul"",
          ""//tensorflow/compiler/xla:executable_run_options"",
          ""//third_party/eigen3"",
          ""//tensorflow/core:framework_lite"",
          ],
  copts=tf_copts()
)
```
punctuation_decode.config.pbtxt looks like:
```
feed {
  id { node_name: ""Model/Input/word"" }
  shape {
    dim { size: 1 }
    dim { size: 13 }     # 1 + max_forward + max_backward
  }
}

feed {
  id { node_name: ""Model/Input/punctuation"" }
  shape {
    dim { size: 1 }
    dim { size: 1 }
  }
}

feed {
  id { node_name: ""Model/Input/capitalisation"" }
  shape {
    dim { size: 1 }
    dim { size: 1 }
  }
}

feed {
  id { node_name: ""Model/Transformer/input_cache"" }
  shape {
    dim { size: 9 }     # num_hidden_layers - 1
    dim { size: 1 }     # fixed (batch_size of 1 at test time)
    dim { size: 8 }     # max_backward
    dim { size: 128 }    # layer_size
  }
}


fetch {
  id { node_name: ""Model/Transformer/output_cache"" }
}

fetch {
  id { node_name: ""Model/Output/Delay_0/cap/predicted"" }
}

fetch {
  id { node_name: ""Model/Output/Delay_0/punc/predicted"" }
}

fetch {
  id { node_name: ""Model/Output/Delay_1/cap/predicted"" }
}

fetch {
  id { node_name: ""Model/Output/Delay_1/punc/predicted"" }
}

fetch {
  id { node_name: ""Model/Output/Delay_2/cap/predicted"" }
}

fetch {
  id { node_name: ""Model/Output/Delay_2/punc/predicted"" }
}

fetch {
  id { node_name: ""Model/Output/Delay_3/cap/predicted"" }
}

fetch {
  id { node_name: ""Model/Output/Delay_3/punc/predicted"" }
}

fetch {
  id { node_name: ""Model/Output/Delay_4/cap/predicted"" }
}

fetch {
  id { node_name: ""Model/Output/Delay_4/punc/predicted"" }
}
```
Then in our binary cpp the code is called like this (simplified, parts removed for clarity):
```
const int MAX_FORWARD=4;
const int MAX_BACKWARD=8;
const int LAYER_SIZE=128;
const int NUM_HIDDEN_LAYERS_MINUS_ONE=9;
...
    PunctuationDecode punctuator;
    Eigen::ThreadPool tp(1);
    Eigen::ThreadPoolDevice device(&tp, tp.NumThreads());
    punctuator.set_thread_pool(&device);

...
    vector<int> word_idxs;
    word_idxs = initialise_word_idxs(eos_idx, max_backward); // (this populates word_idxs with a long vector of ints)
    vector<int> prev_cap;
    prev_cap.push_back(0);
    vector<int> prev_punc;
    prev_punc.push_back(0);
    float cache[NUM_HIDDEN_LAYERS_MINUS_ONE][1][MAX_BACKWARD][LAYER_SIZE] = {0.0};

    // now iterate through, one buffer to be decoded at a time
   int width=MAX_FORWARD + MAX_BACKWARD + 1;
    for(int i=0;i<num_entries;i++) {
        // get correct part of input feed of word indexes
        vector<int> decode_buffer(word_idxs.begin() + i, word_idxs.begin() + i + width);

        // feed the correct values to the TF method, run it
        punctuator.set_arg0_data(decode_buffer.data());
        punctuator.set_arg1_data(prev_punc.data());
        punctuator.set_arg2_data(prev_cap.data());
        punctuator.set_arg3_data(cache);
        punctuator.Run();
        // here it falls over if the .so is not compiled in debug mode
    }
```
The object works if compiled like: `bazel build -c opt //bazel_punctuation:libpunctuation.so --compilation_mode=dbg` but fails if compiled like: `bazel build -c opt //bazel_punctuation:libpunctuation.so`

**Other info / logs**
Full Valgrind output:
```
valgrind ./decode /cantab/exp0/inbetweeners/punctuation/20180903iwslt/data/preprocessed/test.nopunc /cantab/exp0/inbetweeners/punctuation/2
0180903iwslt/export/default_build
==9068== Memcheck, a memory error detector
==9068== Copyright (C) 2002-2015, and GNU GPL'd, by Julian Seward et al.
==9068== Using Valgrind-3.11.0 and LibVEX; rerun with -h for copyright info
==9068== Command: ./decode /cantab/exp0/inbetweeners/punctuation/20180903iwslt/data/preprocessed/test.nopunc /cantab/exp0/inbetweeners/punctuation/20180903iwslt/export/default_build
==9068== 
before main loop
setting feeds
setting prev punc feeds
setting prev cap feeds
setting cache feeds
running punctuator
==9068== Invalid read of size 4
==9068==    at 0x5009807: void Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>::evalProduct<true, true, false, 0>(float*) const (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/generated/libpunctuation.so)
==9068==    by 0x5015C94: Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 2, 0, long>, 16, Eigen::MakePointer>, Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eige$::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice, true>::run(Eigen::TensorAs$ignOp<Eigen::TensorMap<Eigen::Tensor<float, 2, 0, long>, 16, Eigen::MakePointer>, Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointe$> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const> const> const&, Eigen::ThreadPoolDevice const&) (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/generate$/libpunctuation.so)
==9068==    by 0x502F1CA: __xla_cpu_runtime_EigenMatMulF32 (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/generated/libpunctuation.so)
==9068==    by 0x4EFCC37: __bazel_punctuation__punctuation_decode (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/generated/libpunctuation.so)
==9068==    by 0x413311: tensorflow::XlaCompiledCpuFunction::Run() (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/decode)
==9068==    by 0x403548: greedy_decode(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >, std:$__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/decode)
==9068==    by 0x403F18: decode_from_file(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) (in /home/exp0/exp/inbetweeners/$unctuation/20180903iwslt/export/punctuation/decode/decode)
==9068==    by 0x4040B6: main (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/decode)
==9068==  Address 0x8 is not stack'd, malloc'd or (recently) free'd
==9068== 
==9068== 
==9068== Process terminating with default action of signal 11 (SIGSEGV)
==9068==  Access not within mapped region at address 0x8
==9068==    at 0x5009807: void Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<E$gen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>::evalProduct<true, true, false, 0>(float*) const (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/ge$erated/libpunctuation.so)
==9068==    by 0x5015C94: Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 2, 0, long>, 16, Eigen::MakePointer>, Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice, true>::run(Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 2, 0, long>, 16, Eigen::MakePointer>, Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const> const> const&, Eigen::ThreadPoolDevice const&) (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/generated/libpunctuation.so)
==9068==    by 0x502F1CA: __xla_cpu_runtime_EigenMatMulF32 (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/generated/libpunctuation.so)
==9068==    by 0x4EFCC37: __bazel_punctuation__punctuation_decode (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/generated/libpunctuation.so)
==9068==    by 0x413311: tensorflow::XlaCompiledCpuFunction::Run() (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/decode)
==9068==    by 0x403548: greedy_decode(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/decode)
==9068==    by 0x403F18: decode_from_file(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/decode)
==9068==    by 0x4040B6: main (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/decode)
==9068==  Address 0x8 is not stack'd, malloc'd or (recently) free'd
==9068== 
==9068== 
==9068== Process terminating with default action of signal 11 (SIGSEGV)
==9068==  Access not within mapped region at address 0x8
==9068==    at 0x5009807: void Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>::evalProduct<true, true, false, 0>(float*) const (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/generated/libpunctuation.so)
==9068==    by 0x5015C94: Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 2, 0, long>, 16, Eigen::MakePointer>, Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice, true>::run(Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 2, 0, long>, 16, Eigen::MakePointer>, Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const> const> const&, Eigen::ThreadPoolDevice const&) (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/generated/libpunctuation.so)
==9068==    by 0x502F1CA: __xla_cpu_runtime_EigenMatMulF32 (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/generated/libpunctuation.so)
==9068==    by 0x4EFCC37: __bazel_punctuation__punctuation_decode (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/generated/libpunctuation.so)
==9068==    by 0x413311: tensorflow::XlaCompiledCpuFunction::Run() (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/decode)
==9068==    by 0x403548: greedy_decode(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/decode)
==9068==    by 0x403F18: decode_from_file(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/decode)
==9068==    by 0x4040B6: main (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/decode)
==9068==  If you believe this happened as a result of a stack
==9068==  overflow in your program's main thread (unlikely but
==9068==  possible), you can try to increase the size of the
==9068==  main thread stack using the --main-stacksize= flag.
==9068==  The main thread stack size used in this run was 8388608.
==9068== 
==9068== HEAP SUMMARY:
==9068==     in use at exit: 7,997,706 bytes in 96,216 blocks
==9068==   total heap usage: 631,034,209 allocs, 630,937,993 frees, 41,411,688,816 bytes allocated
==9068== 
==9068== LEAK SUMMARY:
==9068==    definitely lost: 0 bytes in 0 blocks
==9068==    indirectly lost: 0 bytes in 0 blocks
==9068==      possibly lost: 0 bytes in 0 blocks
==9068==    still reachable: 7,997,706 bytes in 96,216 blocks
==9068==         suppressed: 0 bytes in 0 blocks
==9068== Rerun with --leak-check=full to see details of leaked memory
==9068== 
==9068== For counts of detected and suppressed errors, rerun with: -v
==9068== ERROR SUMMARY: 1 errors from 1 contexts (suppressed: 0 from 0)
Segmentation fault (core dumped)
```
GDB falls over like this:
```
(gdb) s
tensorflow::XlaCompiledCpuFunction::Run (this=0x7fffffff45f0) at /cantab/exp0/inbetweeners/punctuation/20180903iwslt/export/punctuation/tensorflow/tensorflow/compiler/tf2xla/xla_compiled_cpu_function.cc:52
52        raw_function_(buffer_table_[result_index_], &run_options_, nullptr,
(gdb) s
53                      buffer_table_, profile_counters_);
(gdb) s
52        raw_function_(buffer_table_[result_index_], &run_options_, nullptr,
(gdb) s
53                      buffer_table_, profile_counters_);
(gdb) s

Thread 1 ""decode"" received signal SIGSEGV, Segmentation fault.
__GI___libc_free (mem=0x10011) at malloc.c:2951
2951    malloc.c: No such file or directory.
```
"
24001,AttributeError: module 'tensorflow._api.v1.lite' has no attribute 'OpsSet',"

### System information
- **OS Platform and Distribution (centos7.3)**:
- **TensorFlow installed from  binary:from binary
- **TensorFlow version **:1.12
- **Python version**:3.6
- **CUDA/cuDNN version**:9.1
- **GPU model and memory**:K80


### Describe the problem
when i follow the tensorflow lite tutorial. 

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/using_select_tf_ops.md
it gets this error.
### Source code / logs
import tensorflow as tf
import sys
import os

os.environ[""CUDA_VISIBLE_DEVICES""] = ""1""
converter = tf.lite.TFLiteConverter.from_saved_model('model',input_shapes={""input"":[1]})
converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,
                         tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model = converter.convert()
converter.post_training_quantize = True
tflite_model_quantize = converter.convert()
open(""converted_model_quantize_save.tflite"", ""wb"").write(tflite_model_quantize)



INFO:tensorflow:Restoring parameters from model/variables/variables
INFO:tensorflow:The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'serving_default'}
INFO:tensorflow:input tensors info: 
INFO:tensorflow:Tensor's key in saved_model's tensor_map: input
INFO:tensorflow: tensor name: input_6:0, shape: (1), type: DT_FLOAT
INFO:tensorflow:output tensors info: 
INFO:tensorflow:Tensor's key in saved_model's tensor_map: out
INFO:tensorflow: tensor name: out_6:0, shape: (1), type: DT_FLOAT
INFO:tensorflow:Restoring parameters from model/variables/variables
INFO:tensorflow:Froze 1 variables.
INFO:tensorflow:Converted 1 variables to const ops.

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-10-2cf72d9b817e> in <module>()
      7 
      8 converter = tf.lite.TFLiteConverter.from_saved_model('model',input_shapes={""input"":[1]})
----> 9 converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,
     10                          tf.lite.OpsSet.SELECT_TF_OPS]
     11 tflite_model = converter.convert()

AttributeError: module 'tensorflow._api.v1.lite' has no attribute 'OpsSet'



"
24000,"i changed the ""libtensorflow_demo.so"" file. Even then am facing the same issue","E/AndroidRuntime: FATAL EXCEPTION: IntentService[Prediction Service]
    Process: com.cognizant.oralclassifier, PID: 25426
    java.lang.RuntimeException: Native TF methods not found; check that the correct native libraries are present in the APK.
        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.prepareNativeRuntime(TensorFlowInferenceInterface.java:544)
        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:60)
        at com.cognizant.oralclassifier.analyze.PredictionService.onHandleIntent(PredictionService.java:66)
        at android.app.IntentService$ServiceHandler.handleMessage(IntentService.java:76)
        at android.os.Handler.dispatchMessage(Handler.java:106)
        at android.os.Looper.loop(Looper.java:193)
        at android.os.HandlerThread.run(HandlerThread.java:65)

_Originally posted by @sthuthyraj in https://github.com/tensorflow/tensorflow/issue_comments#issuecomment-442025141_"
23999,libiomp5 multiple initialization error message when dataset size is large enough,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **OS X El Capitan 10.11.6 (15G22010)**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**
- TensorFlow installed from (source or binary): **binary (anaconda)**
output from `conda list tensorflow`: 
```
tensorflow                1.12.0          mkl_py36h2b2bbaf_0  
tensorflow-base           1.12.0          mkl_py36h70e0e9a_0 
```
- TensorFlow version (use command below): **b'unknown' 1.12.0**
- Python version:
**Python 3.6.6 |Anaconda custom (64-bit)| (default, Jun 28 2018, 11:07:29) **
[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin**
- Bazel version (if compiling from source): **N/A**
- GCC/Compiler version (if compiling from source): **N/A**
- CUDA/cuDNN version: **N/A**
- GPU model and memory: **N/A**


**Describe the current behavior**
I get the following error message only under **specific circumstances** (see code below):

> OMP: Error #15: Initializing libiomp5.dylib, but found libiomp5.dylib already initialized.
> OMP: Hint: This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it candegrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/.
> Abort trap: 6

**Describe the expected behavior**
No error message


**Code to reproduce the issue**
```python
import tensorflow as tf
import numpy as np

def get_dense(input, widths, activations):
    assert len(widths) == len(activations)
    output = input
    for i, (w, a) in enumerate(zip(widths, activations)):
        output = tf.layers.dense(output, units=w, activation=a)
    return output

nn = lambda input, depth, width: get_dense(
        input,
        [width      for _ in range(depth - 1)] + [3   ],
        [tf.nn.relu for _ in range(depth - 1)] + [None]
    )

tf.reset_default_graph()

# Works fine when N = 8192,
# but breaks with N = 8193:
N = 8193
n = 100

X = np.random.normal(loc=0.0, scale=1., size=(N, 1)).astype(np.float32)
nx = X.shape[1]
Y = X**2

XY = np.concatenate([X, Y], axis=1)

ds = tf.data.Dataset.from_tensor_slices(XY).batch(n).make_one_shot_iterator().get_next()

pred = nn(ds[:,:nx], 2, 10)
loss = tf.reduce_mean((pred - ds[:,nx:])**2)
op = tf.train.RMSPropOptimizer(0.0005).minimize(loss)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    try:
        for i in range(20):
            print(i)
            _, l1 = sess.run([op, loss])
            print(l1)
    except tf.errors.OutOfRangeError:
        pass

print(""Done"")
```

**Other info / logs**
I would consider this an anaconda problem if only it happened at all times. However, what I find weird, and why I suspect it might be a tensorflow issue, is that it only occurs when input dataset size is large enough. In this example the error pops up when N is 8193 or higher, no error when N is 8192 or lower. This ""threshold"" value is different in my original project where I first faced the problem - there it happened when dataset array length was 100001 (100k+1) or higher, while working fine with 100000 (100k) long dataset. Apologies if this is irrelevant to tensorflow and indeed an anaconda issue.

P.S.
Adding KMP_DUPLICATE_LIB_OK=TRUE does silence the error."
23997,Tensor.eval() hangs with Tensorflow 1.10+,"**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.10.0-0-g656e7a2b34 1.10.0
- Python version: 3.6


**Describe the current behavior**
Call to `Tensor.eval()` within a custom gradient function hangs forever (see minimal example below). This seems to happen with all Tensorflow releases >=1.10.0 while it does not happen with <=1.9.0.

**Describe the expected behavior**
The tensor evaluates in the session.

**Code to reproduce the issue**
```
import numpy as np
import tensorflow as tf

SESSION = None

@tf.RegisterGradient(""CustomGrad"")
def _relu_grad(unused_op, grad):
    print (""Grad override fun"")
    # ISSUE HERE: The following line never returns in 1.10+
    print (tf.ones((1,1)).eval(session=SESSION))
    return grad

with tf.Session() as sess:
    SESSION = sess
    g = tf.get_default_graph()
    with g.gradient_override_map({""Relu"": ""CustomGrad""}):
        weights = tf.random_normal((2,2)) 
        result = tf.nn.relu(weights)
        grad = tf.gradients(result, weights)[0]
        print (grad.eval())
```
Output with 1.9.0:
```
Grad override fun
[[1.]]
[[1. 1.]
 [1. 1.]]

```
Output with 1.10.0:
```
Grad override fun
--> the programs does not terminate and hangs forever here
```"
23995,"dataset.output_shapes is (TensorShape([]), TensorShape([]))","
I want to set steps per epoch according to the shape of the dataset, but when I print dataset.output_shapes, I find the result is None, like this:(TensorShape([]), TensorShape([]))."
23992,Graph Transform Tool fails to optimize away useless Switch node,"**System information**
- TensorFlow version (you are using): v1.12.0
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**
The Graph Transform Tool apparently fails to optimize away Switch nodes that have constant pred input, as demonstrated in the test case. The test program shows a GraphDef that includes an unnecessary Switch node that could have been optimized away.

When a Switch node has pred input that is constant, it should be possible to optimize the node away but currently this optimization does not happen. Consequently, when using the Graph Transform Tool to optimize a GraphDef for a network that is built to be used for training, eval and inference, that has a bool placeholder to differentiate the use case, nodes needed only for training remain even when the placeholder node has been converted to a constant False node and 'fold_constants' is specified to the Graph Transform Tool. Together with 'strip_unused_nodes', the desired optimization would result in other nodes also being removed that are used/available only during training.

This optimization could be provided as a new transform or as part of 'fold_constants'.

Test:
```
#!/usr/bin/env python3

import tensorflow as tf
from tensorflow.tools.graph_transforms import TransformGraph

with tf.Graph().as_default() as g:
    pred = tf.constant(False, dtype=tf.bool, shape=(), name='pred')

    def one(): return tf.constant(1, dtype=tf.int32, shape=(), name='one')

    def zero(): return tf.constant(0, dtype=tf.int32, shape=(), name='zero')

    r = tf.cond(pred, one, zero)

gd = g.as_graph_def()
transforms = ['strip_unused_nodes', 'fold_constants', 'strip_unused_nodes']
gd = TransformGraph(gd, [], [r.name], transforms)
print(gd)
```

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
Especially users who build one graph for both training and inference and differentiate the use case using a boolean placeholder that controls the behavior of the batch norm or dropout in the graph. This feature would help them get cleaner and smaller graphs for inference.

**Any Other info.**
"
23991,Improve SEO score on index page,"I'd like to update the home page on suggestion SEO improvements.  I'm having difficulty finding the index file.  

The first issues I'd to correct is related to links not having a descriptive text.  "
23990,Does GanEstimator support multi GPU ?,"Does GanEstimator support multi gpu?
How could I using multi GPU by ganEstimator?

Thanks."
23987,LARSOptimizer does not initialize _learning_rate_tensor and _momentum_tensor,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from: binary
- TensorFlow version: tags/1.12
- Python version: 2.7
- cpu mode


**Describe the current behavior**
'LARSOptimizer' object has no attribute '_learning_rate_tensor'

**Code to reproduce the issue**
\tensorflow\tensorflow\contrib\opt\python\training\lars_optimizer_test.py
"
23986,train.init_from_checkpoint does not support mirrorredStrategy and CollectiveAllReduceStrategy,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
   We are changing some code in https://github.com/google-research/bert run_classifier.py to make it can be run on machine with multiple GPUs

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
  Distributor ID:	Ubuntu
  Description:	Ubuntu 16.04.5 LTS
  Release:	16.04
  Codename:	xenial

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
  NA

- TensorFlow installed from (source or binary):
  binary

- TensorFlow version (use command below):
  v1.11.0-0-gc19e29306c. and v1.12.0-0-ga6d8ffae09
- Python version:
  2.7.12

- Bazel version (if compiling from source):
  NA

- GCC/Compiler version (if compiling from source):
  NA

- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
In Bert's source code, it will call tf.train.init_from_checkpoint(init_checkpoint, assignment_map) in the model_fn

we change some of the Bert's source code to make it can run on mirroredStrategy mode, but we met a failure with the call stack as below:

Traceback (most recent call last):
  File ""run_classifier_collect.py"", line 859, in <module>
    tf.app.run()
  File ""/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""run_classifier_collect.py"", line 815, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
  File ""/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 462, in train_and_evaluate
    estimator, train_spec, eval_spec, _TrainingExecutor)
  File ""/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/python/distribute/estimator_training.py"", line 279, in train_and_evaluate
    session_config=run_config.session_config)
  File ""/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/python/distribute/distribute_coordinator.py"", line 792, in run_distribute_coordinator
    task_id, session_config, rpc_layer)
  File ""/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/python/distribute/distribute_coordinator.py"", line 344, in _run_single_worker
    worker_fn(strategy)
  File ""/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/python/distribute/estimator_training.py"", line 246, in _worker_fn
    hooks=hooks)
  File ""/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py"", line 2409, in train
    rendezvous.raise_errors()
  File ""/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/contrib/tpu/python/tpu/error_handling.py"", line 128, in raise_errors
    six.reraise(typ, value, traceback)
  File ""/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py"", line 2403, in train
    saving_listeners=saving_listeners
  File ""/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1205, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1316, in _train_model_distributed
    self.config)
  File ""/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/python/training/distribute.py"", line 721, in call_for_each_tower
    return self._call_for_each_tower(fn, *args, **kwargs)
  File ""/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 556, in _call_for_each_tower
    return _call_for_each_tower(self, fn, *args, **kwargs)
  File ""/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 183, in _call_for_each_tower
    coord.join(threads)
  File ""/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 177, in _call_for_each_tower
    **merge_kwargs)
  File ""/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/python/training/checkpoint_utils.py"", line 211, in _init_from_checkpoint
    var = _collect_partitioned_variable(current_var_or_name, store_vars)
  File ""/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/python/training/checkpoint_utils.py"", line 365, in _collect_partitioned_variable
    if name + ""/part_0"" in all_vars:
TypeError: unsupported operand type(s) for +: 'PerDevice' and 'str'



we can see  the code in init_from_checkpoint:
  if distribution_strategy_context.get_cross_tower_context():
    _init_from_checkpoint(None, ckpt_dir_or_file, assignment_map)
  else:
    distribution_strategy_context.get_tower_context().merge_call(
        _init_from_checkpoint, ckpt_dir_or_file, assignment_map)

seems that above code goes into the merge_call version

I think init_from_checkpoint is indent to support cross_tower scenario, and seems that above code goes into the merge_call version in BERT's scenario

but the function _init_from_checkpoint does not properly handle the parameter passed across tower?
 

**Describe the expected behavior**
init_from_checkpoint can be called successfully


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
1. Modified BERT's code to make it use MirrorredStrategy
2. Modified BERT's optimization.py to make it can be run in distribution environment

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23985,What is the encoding of a .tflite file?,
23984,tensorflow  - gpu issue,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

System information

OS : Windows 10
TensorFlow installed from (source or binary): from pip
TensorFlow version: 1.11.0
Python version: 3.6
Installed using virtualenv? pip? conda?: conda
Bazel version (if compiling from source): No
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 10
GPU model and memory: Nvidia 1050Ti


**Describe the problem**
Recently I tried updating my tensor flow and later on, my tensorflow-gpu stopped working.
Now I have downgraded to 1.11.0 but then still my tensorflow-gpu is not working.

nvidia-smi is working fine. 
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 416.34       Driver Version: 416.34       CUDA Version: 10.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 105... WDDM  | 00000000:01:00.0 Off |                  N/A |
| N/A   48C    P8    N/A /  N/A |     78MiB /  4096MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|==================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+


tf.test.is_gpu_available(
cuda_only=False,
min_cuda_compute_capability=None
)

I ran this, its coming false

then , 
I ran this sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
Device mapping: no known devices.
2018-11-26 17:27:10.277030: I tensorflow/core/common_runtime/direct_session.cc:291] Device mapping:

it's coming empty, I read lot of threads usually people had error with cuda, by installing that it worked for lot of people.

But then in my case things were working fine earlier, now it got messed up and cuda is installed properly.

why should I do?

"
23983,Can't import tensorflow,"**System information**
- OS Platform and Distribution: Windows
- TensorFlow installed from (source or binary): Anaconda
- TensorFlow version: Cannot check as tensorflow cannot be imported
- Python version: Python 3.5.5 :: Anaconda, Inc.
- Installed using virtualenv: conda
- GCC/Compiler version (if compiling from source): (tdm64-1) 5.1.0

**Problem**

I had a working version of tensorflow-gpu. I decided to upgrade it since I had an older version. However, after upgrading it, whenever I try to import it, I now get an error.

I ran the command:

```
conda install -c anaconda tensorflow-gpu 
```
After that it installed and everything went fine. But now, I cannot import tensorflow anymore. The stack trace is shown below.

**Logs**
>Traceback (most recent call last):
  File ""C:\Users\Shoaib\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\p
ython\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Shoaib\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\p
ython\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Shoaib\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\p
ython\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript
ion)
  File ""C:\Users\Shoaib\Anaconda3\envs\tensorflow\lib\imp.py"", line 243, in load
_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Shoaib\Anaconda3\envs\tensorflow\lib\imp.py"", line 343, in load
_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified procedure could not be found.

> During handling of the above exception, another exception occurred:

> Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Shoaib\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\_
_init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-im
port
  File ""C:\Users\Shoaib\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\p
ython\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Shoaib\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\p
ython\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Shoaib\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\p
ython\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Shoaib\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\p
ython\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Shoaib\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\p
ython\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript
ion)
  File ""C:\Users\Shoaib\Anaconda3\envs\tensorflow\lib\imp.py"", line 243, in load
_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Shoaib\Anaconda3\envs\tensorflow\lib\imp.py"", line 343, in load
_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified procedure could not be found.


> Failed to load the native TensorFlow runtime.

> See https://www.tensorflow.org/install/errors

> for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

Could anyone please tell me what could be causing this? Any help would be appreciated.
Thank you so much."
23979,Save/Load problem with keras.layers.ReLU,"Running this example
```
x = tf.keras.layers.Input([1])
y = tf.keras.layers.ReLU()(x)
mdl = tf.keras.models.Model(x, y)
tf.keras.models.save_model(mdl, ""./keras_model_tmp.hdf5"", overwrite=True)
tf.keras.models.load_model(""./keras_model_tmp.hdf5"")
```
returns trackback
```
/data/virtualenv/venv2/lib/python2.7/site-packages/tensorflow/python/keras/engine/saving.pyc in load_model(filepath, custom_objects, compile)
    228       raise ValueError('No model found in config file.')
    229     model_config = json.loads(model_config.decode('utf-8'))
--> 230     model = model_from_config(model_config, custom_objects=custom_objects)
    231 
    232     # set weights

/data/virtualenv/venv2/lib/python2.7/site-packages/tensorflow/python/keras/engine/saving.pyc in model_from_config(config, custom_objects)
    308                     '`Sequential.from_config(config)`?')
    309   from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top
--> 310   return deserialize(config, custom_objects=custom_objects)
    311 
    312 

/data/virtualenv/venv2/lib/python2.7/site-packages/tensorflow/python/keras/layers/serialization.pyc in deserialize(config, custom_objects)
     62       module_objects=globs,
     63       custom_objects=custom_objects,
---> 64       printable_module_name='layer')

/data/virtualenv/venv2/lib/python2.7/site-packages/tensorflow/python/keras/utils/generic_utils.pyc in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)
    171             custom_objects=dict(
    172                 list(_GLOBAL_CUSTOM_OBJECTS.items()) +
--> 173                 list(custom_objects.items())))
    174       with CustomObjectScope(custom_objects):
    175         return cls.from_config(config['config'])

/data/virtualenv/venv2/lib/python2.7/site-packages/tensorflow/python/keras/engine/network.pyc in from_config(cls, config, custom_objects)
   1291     for layer_data in config['layers']:
   1292       process_layer(layer_data)
-> 1293     # Then we process nodes in order of layer depth.
   1294     # Nodes that cannot yet be processed (if the inbound node
   1295     # does not yet exist) are re-enqueued, and the process

/data/virtualenv/venv2/lib/python2.7/site-packages/tensorflow/python/keras/engine/network.pyc in process_layer(layer_data)
   1276       from tensorflow.python.keras.layers import deserialize as deserialize_layer  # pylint: disable=g-import-not-at-top
   1277 
-> 1278       layer = deserialize_layer(layer_data, custom_objects=custom_objects)
   1279       created_layers[layer_name] = layer
   1280 

/data/virtualenv/venv2/lib/python2.7/site-packages/tensorflow/python/keras/layers/serialization.pyc in deserialize(config, custom_objects)
     62       module_objects=globs,
     63       custom_objects=custom_objects,
---> 64       printable_module_name='layer')

/data/virtualenv/venv2/lib/python2.7/site-packages/tensorflow/python/keras/utils/generic_utils.pyc in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)
    173                 list(custom_objects.items())))
    174       with CustomObjectScope(custom_objects):
--> 175         return cls.from_config(config['config'])
    176     else:
    177       # Then `cls` may be a function returning a class.

/data/virtualenv/venv2/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.pyc in from_config(cls, config)
   1604         A layer instance.
   1605     """"""
-> 1606     return cls(**config)
   1607 
   1608 

/data/virtualenv/venv2/lib/python2.7/site-packages/tensorflow/python/keras/layers/advanced_activations.py in __init__(self, max_value, negative_slope, threshold, **kwargs)
    319     self.max_value = max_value
    320     self.negative_slope = K.cast_to_floatx(negative_slope)
--> 321     self.threshold = K.cast_to_floatx(threshold)
    322 
    323   def call(self, inputs):

/data/virtualenv/venv2/lib/python2.7/site-packages/tensorflow/python/keras/backend.pyc in cast_to_floatx(x)
    231   ```
    232   """"""
--> 233   return np.asarray(x, dtype=_FLOATX)
    234 
    235 

/data/virtualenv/venv2/lib/python2.7/site-packages/numpy/core/numeric.pyc in asarray(a, dtype, order)
    499 
    500     """"""
--> 501     return array(a, dtype, copy=False, order=order)
    502 
    503 

TypeError: float() argument must be a string or a number
```
I temporally fixed it in `tensorflow/python/keras/layers/advanced_activations.py` by changing `get_config`
from
```
  def get_config(self):
    config = {
        'max_value': self.max_value,
        'negative_slope': self.negative_slope,
        'threshold': self.threshold
    }
    base_config = super(ReLU, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))
```
to 
```
  def get_config(self):
    config = {
        'max_value': self.max_value,
        'negative_slope': float(self.negative_slope),
        'threshold': float(self.threshold)
    }
    base_config = super(ReLU, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))
```
However, the problem might be somewhere in `deserialize_layer`, as for a some reason `negative_slope` is a dictionary, not a `numpy.ndarray` "
23977,pyhton code with tensorflow code throws execption in xampp server.,"I have a python code in server root folder which has a tensorflow code, when compiled using exec() php command in php, it is throwing exception.

But when executed normally through console it excutes.

I have attached text file which has the exception thrown while executing.
Currently I'm using postman to test the php file.

[output.txt](https://github.com/tensorflow/tensorflow/files/2615237/output.txt)
 "
23976,ImportError: cannot import name cloud,"**System information**
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04, 18.04 s390x
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
TensorFlow installed from (source or binary): source
TensorFlow version: v1.12.0
Python version: 2.7.x
Installed using virtualenv? pip? conda?: Building from source
Bazel version (if compiling from source): v0.15.0
GCC/Compiler version (if compiling from source): 7.3.0 (Ubuntu 18.04)
CUDA/cuDNN version: NA
GPU model and memory: NA  


**Describe the problem**

We have build TensorFlow v1.12.0 from source on Ubuntu 16.04, 18.04 on s390x platform. Observed test failures with an error: ""ImportError: cannot import name cloud""


"
23974,TypeError: Type already registered for SparseTensorValue when running 2nd script ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/a
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/a
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.12
- Python version: 3.6.5
- Bazel version (if compiling from source): 0.17.2
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version: 9.0/7.0
- GPU model and memory: GTX 1080 TI (11 GB)

**Describe the current behavior**
When I run a certain script in Spyder at the 1st time, everything flows smoothly. However, when I continue to run another script, following errors occur (which didn't happen at previously built tf 1.10):

> File ""/home/haohua/tf_env/lib/python3.6/site-packages/tensorflow/init.py"", line 24, in from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import

> File ""/home/haohua/tf_env/lib/python3.6/site-packages/tensorflow/python/init.py"", line 70, in from tensorflow.python.framework.framework_lib import * # pylint: disable=redefined-builtin

> File ""/home/haohua/tf_env/lib/python3.6/site-packages/tensorflow/python/framework/framework_lib.py"", line 30, in from tensorflow.python.framework.sparse_tensor import SparseTensor

> File ""/home/haohua/tf_env/lib/python3.6/site-packages/tensorflow/python/framework/sparse_tensor.py"", line 248, in pywrap_tensorflow.RegisterType(""SparseTensorValue"", SparseTensorValue)

> TypeError: Type already registered for SparseTensorValue

My temporary solution is restart the kernel before continuing to run the next script.

But restarting kernel at every single step of running a script (from the 2nd run) is not comfortable. Thus, I would like to ask for a critical solution for such kind of issue.

P/S: [https://stackoverflow.com/questions/53477005/tensorflow-r1-12-typeerror-type-already-registered-for-sparsetensorvalue-when](url) is my question on Stackoverflow. However, I think it is still proper to put this issue here."
23973,Update ExternalOptimizerInterface for use with Eager Execution,"**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No):
I suppose yes...although i'm not confident i can produce a general code framework for this in any reasonable time.


**Describe the feature and the current behavior/state.**
Currently the external scipy optimizer interface is coded to work with delayed [graph model] execution only.  Would like it updated for eager inclusion.

**Will this change the current api? How?**
Just need an option for eager.

**Who will benefit with this feature?**
Everyone that wants to user eager execution and the scipy optimizer in tensorflow
"
23972,ERROR: undeclared inclusion(s) in rule '//tensorflow/python/eager:pywrap_tfe_lib',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.12.0
- Python version: 3.7.0
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.18
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: 10.0
- GPU model and memory: GTX 1080 and 8GB



**Describe the problem**

ERROR: /home/ylzhao/project/tensorflow/tensorflow/python/eager/BUILD:10:1: undeclared inclusion(s) in rule '//tensorflow/python/eager:pywrap_tfe_lib':
this rule is missing dependency declarations for the following files included by 'tensorflow/python/eager/pywrap_tensor.cc':
  'bazel-out/k8-opt/genfiles/external/local_config_python/python_include/numpy/arrayobject.h'
  'bazel-out/k8-opt/genfiles/external/local_config_python/python_include/numpy/ndarrayobject.h'
  'bazel-out/k8-opt/genfiles/external/local_config_python/python_include/numpy/ndarraytypes.h'
  'bazel-out/k8-opt/genfiles/external/local_config_python/python_include/numpy/npy_common.h'
  'bazel-out/k8-opt/genfiles/external/local_config_python/python_include/numpy/numpyconfig.h'
  'bazel-out/k8-opt/genfiles/external/local_config_python/python_include/numpy/_numpyconfig.h'
  'bazel-out/k8-opt/genfiles/external/local_config_python/python_include/numpy/npy_endian.h'
  'bazel-out/k8-opt/genfiles/external/local_config_python/python_include/numpy/npy_cpu.h'
  'bazel-out/k8-opt/genfiles/external/local_config_python/python_include/numpy/utils.h'
  'bazel-out/k8-opt/genfiles/external/local_config_python/python_include/numpy/_neighborhood_iterator_imp.h'
  'bazel-out/k8-opt/genfiles/external/local_config_python/python_include/numpy/__multiarray_api.h'
  'bazel-out/k8-opt/genfiles/external/local_config_python/python_include/numpy/npy_interrupt.h'
  'bazel-out/k8-opt/genfiles/external/local_config_python/python_include/numpy/ufuncobject.h'
  'bazel-out/k8-opt/genfiles/external/local_config_python/python_include/numpy/npy_math.h'
  'bazel-out/k8-opt/genfiles/external/local_config_python/python_include/numpy/npy_common.h'
  'bazel-out/k8-opt/genfiles/external/local_config_python/python_include/numpy/arrayobject.h'
  'bazel-out/k8-opt/genfiles/external/local_config_python/python_include/numpy/__ufunc_api.h'
Target //tensorflow/tools/pip_package:build_pip_package failed to build


**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23971,Aborted (core dumped),"

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):       UBUNTU 18.04 LTS

- TensorFlow installed from (source or binary): 
      https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.12.0-cp36-cp36m-linux_x86_64.whl

- Python version: 3.6
- Installed using virtualenv? pip? conda?:   conda


2018-11-26 10:47:27.577247: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use SSE4.2 instructions, but these aren't available on your machine.


cannot import tensorflow installation is successful but its unable to run. need some help!!
"
23970,yolo-v3 has correctness issue at tf-nightly,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):NA
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos 7.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):  tf-nightly==1.13.0.dev20181115
- Python version: 3.4
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**
 Start from tf-nightly==1.13.0.dev20181115, yolo-v3(https://github.com/mystic123/tensorflow-yolo-v3) meet correctness issue, the TF output did not consistent with the previous version
**Describe the expected behavior**
With same command, tf-nightly==1.13.0.dev20181114 can get the right result and plot the bounding box.
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
https://github.com/mystic123/tensorflow-yolo-v3#how-to-run-the-demo
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
23969,inactive tensor board at safari,"when I want to access my tensor board summary with commands ... 
and listening on localhost:6060
The same address would be inactive on safari , but ok on chrome browser.
I wonder if it's general problem... 
![2018-11-26 12 57 10](https://user-images.githubusercontent.com/42016485/48994110-c81b2780-f184-11e8-8801-702fd48fc351.png)

I uploaded screenshots ( the left one is chrome, and the active one is on safari



This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in 



TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).`
If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
23968,[I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ReadVariableOp,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

System information
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
1.12.0
- Python version:
Python 3.6.5 :: Anaconda, Inc.
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

--------------------------------------
I am trying to convert a graph from .pb to .lite format using toco, but I get this error:
2018-11-26 03:31:18.511856: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ReadVariableOp

I think ReadVariableOp is the basic op, Lite shoud support it.

---------------------------------------"
23967,[ppc64le] Error installing custom build tensorflow 1.12 wheel with python 3.6,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 

LSB Version:	:core-4.1-noarch:core-4.1-ppc64le
Distributor ID:	RedHatEnterpriseServer
Description:	Red Hat Enterprise Linux Server release 7.5 (Maipo)
Release:	7.5
Codename:	Maipo

- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.12
- Python version: 3.6.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source): 4.8.5
- CUDA/cuDNN version: 9.2/7.1
- GPU model and memory: NVIDIA Pascal V100 16GB



**Describe the problem**
Trying to install a custom tensorflow 1.12 wheel on ppc64le IBM Power9 system with NVIDIA Pascal V100 but the installation breaks while installing h5py with the error shown below. 


**Provide the exact sequence of commands / steps that you executed before running into the problem**

Build tensorflow wheel from source using standard procedure described: [here](https://developer.ibm.com/tutorials/install-tensorflow-on-power/) (though modified as required) and install using pip. 

**Any other info / logs**
```
gcc -pthread -B /ccs/home/shubhankar/miniconda3/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -DH5_USE_16_API -I./h5py -I/tmp/pip-install-d__8pdpg/h5py/lzf -I/opt/local/include -I/usr/local/include -I/ccs/home/shubhankar/miniconda3/lib/python3.6/site-packages/numpy/core/include -I/ccs/home/shubhankar/miniconda3/include/python3.6m -c /tmp/pip-install-d__8pdpg/h5py/h5py/defs.c -o build/temp.linux-ppc64le-3.6/tmp/pip-install-d__8pdpg/h5py/h5py/defs.o
    In file included from /ccs/home/shubhankar/miniconda3/lib/python3.6/site-packages/numpy/core/include/numpy/ndarraytypes.h:1821:0,
                     from /ccs/home/shubhankar/miniconda3/lib/python3.6/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                     from /ccs/home/shubhankar/miniconda3/lib/python3.6/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                     from /tmp/pip-install-d__8pdpg/h5py/h5py/api_compat.h:26,
                     from /tmp/pip-install-d__8pdpg/h5py/h5py/defs.c:657:
    /ccs/home/shubhankar/miniconda3/lib/python3.6/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning ""Using deprecated NumPy API, disable it by "" ""#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION"" [-Wcpp]
     #warning ""Using deprecated NumPy API, disable it by "" \
      ^
    In file included from /tmp/pip-install-d__8pdpg/h5py/h5py/defs.c:657:0:
    /tmp/pip-install-d__8pdpg/h5py/h5py/api_compat.h:27:18: fatal error: hdf5.h: No such file or directory
     #include ""hdf5.h""
                      ^
    compilation terminated.
    error: command 'gcc' failed with exit status 1

    ----------------------------------------
  Rolling back uninstall of h5py
Command ""/ccs/home/shubhankar/miniconda3/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/pip-install-d__8pdpg/h5py/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/pip-record-5_3ofwib/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /tmp/pip-install-d__8pdpg/h5py/
```
It breaks at the installation of h5py for some reason. It all starts with a warning message. I am not sure if it is related to it.

```
Cythonizing /tmp/pip-install-d__8pdpg/h5py/h5py/h5z.pyx
    /tmp/pip-install-d__8pdpg/h5py/.eggs/Cython-0.29.1-py3.6-linux-ppc64le.egg/Cython/Compiler/Main.py:367: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /tmp/pip-install-d__8pdpg/h5py/h5py/h5z.pxd
```
And finally

```
Rolling back uninstall of h5py
Command ""/ccs/home/shubhankar/miniconda3/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/pip-install-d__8pdpg/h5py/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/pip-record-5_3ofwib/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /tmp/pip-install-d__8pdpg/h5py/
```

EDIT: Using miniconda for package management."
23966,"tensorflow lite, interpreter.get_tensor(output_details[o_index]['index'])) IndexError: list index out of range","------------------------

### System information
- **Linux raspberrypi 4.14.71-v7+**:
- **raspberrypi 3b**:
- **TensorFlow installed from binary**:
- **TensorFlow version : 1.11.0**:
- **Python version : 3.5.3**:

### Describe the problem
I'm trying the objection_detection for tflite. 

And I'm following [this guide](https://github.com/freedomtan/tensorflow/blob/deeplab_tflite_python/tensorflow/contrib/lite/examples/python/object_detection_ssd_coco.md). @freedomtan

When I run `python3 /home/pi/tensorflow/tensorflow/lite/examples/python/object_detection.py --graph /home/pi/tmp/mobilenet_v1_0.5_192.tflite   --image /home/pi/tmp/image2.jpg  --show_image True`,

it comes error that

```
time spent: 135.03503799438477
Traceback (most recent call last):
  File ""/home/pi/tensorflow/tensorflow/lite/examples/python/object_detection.py"", line 182, in <module>
    interpreter.get_tensor(output_details[o_index]['index']))
IndexError: list index out of range
```

### Source code / logs
```
pi@raspberrypi:~/tmp $ ls -l
total 7588
-rw-r--r-- 1 pi pi   76932 Nov 25 08:55 box_priors.txt
-rw-r--r-- 1 pi pi     665 Nov 25 08:54 coco_labels_list.txt
-rw-r--r-- 1 pi pi  940650 Nov 25 08:56 grace_hopper.bmp
-rw-r--r-- 1 pi pi 1415684 Nov 26 02:41 image2.jpg
-rw-r----- 1 pi pi 5319064 Nov 25 09:03 mobilenet_v1_0.5_192.tflite
```
* **object_detection.py**
```
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import math
import time
from heapq import heappush, nlargest

import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.patches as patches

from tensorflow.contrib.lite.python import interpreter as interpreter_wrapper

NUM_RESULTS = 1917
NUM_CLASSES = 91

X_SCALE = 10.0
Y_SCALE = 10.0
H_SCALE = 5.0
W_SCALE = 5.0

def load_box_priors(filename):
  with open(filename) as f:
    count = 0
    for line in f:
      row = line.strip().split(' ')
      box_priors.append(row)
      #print(box_priors[count][0])
      count = count + 1
      if count == 4:
        return

def load_labels(filename):
  my_labels = []
  input_file = open(filename, 'r')
  for l in input_file:
    my_labels.append(l.strip())
  return my_labels

def decode_center_size_boxes(locations):
  """"""calculate real sizes of boxes""""""
  for i in range(0, NUM_RESULTS):
    ycenter = locations[i][0] / Y_SCALE * np.float(box_priors[2][i]) \
            + np.float(box_priors[0][i])
    xcenter = locations[i][1] / X_SCALE * np.float(box_priors[3][i]) \
            + np.float(box_priors[1][i])
    h = math.exp(locations[i][2] / H_SCALE) * np.float(box_priors[2][i])
    w = math.exp(locations[i][3] / W_SCALE) * np.float(box_priors[3][i])

    ymin = ycenter - h / 2.0
    xmin = xcenter - w / 2.0
    ymax = ycenter + h / 2.0
    xmax = xcenter + w / 2.0

    locations[i][0] = ymin
    locations[i][1] = xmin
    locations[i][2] = ymax
    locations[i][3] = xmax
  return locations

def iou(box_a, box_b):
  x_a = max(box_a[0], box_b[0])
  y_a = max(box_a[1], box_b[1])
  x_b = min(box_a[2], box_b[2])
  y_b = min(box_a[3], box_b[3])

  intersection_area = (x_b - x_a + 1) * (y_b - y_a + 1)

  box_a_area = (box_a[2] - box_a[0] + 1) * (box_a[3] - box_a[1] + 1)
  box_b_area = (box_b[2] - box_b[0] + 1) * (box_b[3] - box_b[1] + 1)

  iou = intersection_area / float(box_a_area + box_b_area - intersection_area)
  return iou

def nms(p, iou_threshold, max_boxes):
  sorted_p = sorted(p, reverse=True)
  selected_predictions = []
  for a in sorted_p:
    if len(selected_predictions) > max_boxes:
      break
    should_select = True
    for b in selected_predictions:
      if iou(a[3], b[3]) > iou_threshold:
        should_select = False
        break
    if should_select:
      selected_predictions.append(a)

  return selected_predictions

if __name__ == ""__main__"":
  file_name = ""/home/pi/tmp/image2.jpg""
  model_file = ""/home/pi/tmp/mobilenet_ssd.tflite""
  label_file = ""/home/pi/tmp/coco_labels_list.txt""
  box_prior_file = ""/home/pi/tmp/box_priors.txt""
  input_mean = 127.5
  input_std = 127.5
  min_score = 20.0
  max_boxes = 10
  floating_model = False
  show_image = False
  alt_output_order = False

  parser = argparse.ArgumentParser()
  parser.add_argument(""--image"", help=""image to be classified"")
  parser.add_argument(""--graph"", help="".tflite model to be executed"")
  parser.add_argument(""--labels"", help=""name of file containing labels"")
  parser.add_argument(""--input_mean"", help=""input_mean"")
  parser.add_argument(""--input_std"", help=""input standard deviation"")
  parser.add_argument(""--min_score"", help=""show only > min_score"")
  parser.add_argument(""--max_boxes"", help=""max boxes to show"")
  parser.add_argument(""--show_image"", help=""show image"")
  parser.add_argument(""--alt_output_order"", help=""alternative output index"")
  args = parser.parse_args()

  if args.graph:
    model_file = args.graph
  if args.image:
    file_name = args.image
  if args.labels:
    label_file = args.labels
  if args.input_mean:
    input_mean = float(args.input_mean)
  if args.input_std:
    input_std = float(args.input_std)
  if args.min_score:
    min_score = float(args.min_score)
  if args.max_boxes:
    max_boxes = int(args.max_boxes)
  if args.show_image:
    show_image = args.show_image
  if args.alt_output_order:
    alt_output_order = args.alt_output_order

  interpreter = interpreter_wrapper.Interpreter(model_path=model_file)
  interpreter.allocate_tensors()

  input_details = interpreter.get_input_details()
  output_details = interpreter.get_output_details()
  #print(input_details)
  #print(output_details)

  # check the type of the input tensor
  if input_details[0]['dtype'] == type(np.float32(1.0)):
    floating_model = True

  # NxHxWxC, H:1, W:2
  height = input_details[0]['shape'][1]
  width = input_details[0]['shape'][2]
  img = Image.open(file_name)
  img = img.resize((width, height))

  # add N dim
  input_data = np.expand_dims(img, axis=0)

  if floating_model:
    input_data = (np.float32(input_data) - input_mean) / input_std

  interpreter.set_tensor(input_details[0]['index'], input_data)

  start_time = time.time()
  interpreter.invoke()
  finish_time = time.time()
  print(""time spent:"", ((finish_time - start_time) * 1000))

  box_priors = []
  load_box_priors(box_prior_file)
  labels = load_labels(label_file)

  p_index = 0
  o_index = 1
  if alt_output_order:
    p_index = 1
    o_index = 0

  predictions = np.squeeze( \
                  interpreter.get_tensor(output_details[p_index]['index']))
  output_classes = np.squeeze( \
                     interpreter.get_tensor(output_details[o_index]['index']))
  if not floating_model:
    p_scale, p_mean = output_details[p_index]['quantization']
    o_scale, o_mean = output_details[o_index]['quantization']

    predictions = (predictions - p_mean * 1.0) * p_scale
    output_classes = (output_classes - o_mean * 1.0) * o_scale

  decode_center_size_boxes(predictions)

  pruned_predictions = [[],]
  for c in range(1, NUM_CLASSES):
    pruned_predictions.append([])
    for r in range(0, NUM_RESULTS):
      score = 1. / (1. + math.exp(-output_classes[r][c]))
      if score > 0.01:
        rect = (predictions[r][1] * width, predictions[r][0] * width, \
                predictions[r][3] * width, predictions[r][2] * width)

        pruned_predictions[c].append((output_classes[r][c], r, labels[c], rect))

  final_predictions = []
  for c in range(1, NUM_CLASSES):
    predictions_for_class = pruned_predictions[c]
    suppressed_predictions = nms(predictions_for_class, 0.5, max_boxes)
    final_predictions = final_predictions +  suppressed_predictions

  if show_image:
    fig, ax = plt.subplots(1)

  final_predictions = sorted(final_predictions, reverse=True)[:max_boxes]
  for e in final_predictions:
    score = 100. / (1. + math.exp(-e[0]))
    score_string = '{0:2.0f}%'.format(score)
    print(score_string, e[2], e[3])
    if score < min_score:
      break
    left, top, right, bottom = e[3]
    rect = patches.Rectangle((left, top), (right - left), (bottom - top), \
             linewidth=1, edgecolor='r', facecolor='none')

    if show_image:
      # Add the patch to the Axes
      ax.add_patch(rect)
      ax.text(left, top, e[2]+': '+score_string, fontsize=6,
              bbox=dict(facecolor='y', edgecolor='y', alpha=0.5))

  if show_image:
    ax.imshow(img)
    plt.title(model_file)
    plt.show()
```

"
23965,add metrics to Estimator ( not canned estimators),"I just changed my code from `tf.contrib.learn.Estimator` to `tf.estimator.Estimator` in TensorFlow version 1.11.0. In previous version there was an option to use other metrics like accuracy while evaluating the estimator by passing `metrics` to `evaluate` function. In current version if there is a canned estimator, we can define the metrics with `tf.contrib.estimator.add_metrics`, but when I define my estimator and want to use `add_metrics` it says there is no such wrapper. The error is as follow:
`raise TypeError('{!r} is not a Python function'.format(func))
TypeError: <method-wrapper '__call__' of type object at 0x7fff9cb37530> is not a Python function`

I don't know whether it's because I'm not using canned estimator or something else is wrong with my code. I want to know how I can add metrics to estimator which is not a canned estimator."
23964,Math ops with Tensorflow Lite C++ and Android NDK,"I have successfully installed Tensorflow Lite (C++/so) into an Android product (NDK). I'm looking for the tensorflow math operations, do they exist in tensorflow lite? Or is the sole purpose of tensorflow lite to run inference on models? I also can't seem to find array_ops.h in the tensorflow repository, is this cross complied from some other language?

TLDR: Does tensorflow lite lack math ops and in order to use them you need the full tensorflow?"
23963,fatal error LNK1120: 4 unresolved externals when using bazel to build transform-graph,"**System information**
- OS Platform and Distribution: Windows 10
- TensorFlow installed from: source
- TensorFlow version: 1.12.0
- Python version: 3.5
- Bazel version (if compiling from source): 0.19.2
- CUDA/cuDNN version: No
- GPU model and memory: No

**Problem**
I have a build problem when using bazel to build transform-graph. The error is `fatal error LNK1120: 4 unresolved externals`

**Commands executed before running into the problem**
I've already done `set BAZEL_VC=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC`
then `bazel build tensorflow/tools/graph_transforms:transform_graph --local_resources 2048,.5,1.0`

**Full error log:**
 
ERROR: C:/users/administrator/desktop/tensorflow-build/tensorflow/tensorflow/tools/graph_transforms/BUILD:219:1: Linking of rule '//tensorflow/tools/graph_transforms:transform_graph' failed (Exit 1120): link.exe failed: error executing command
  cd C:/users/administrator/_bazel_administrator/x7v2faqn/execroot/org_tensorflow
  SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\LIB\amd64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\WINDOWS\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\Tools;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Team Tools\Performance Tools;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Windows Kits\10\bin\x86;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;;C:\WINDOWS\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Anaconda/envs/tensor/python.exe
    SET PYTHON_LIB_PATH=C:/Anaconda/envs/tensor/lib/site-packages
    SET TEMP=C:\Users\ADMINI~1\AppData\Local\Temp
    SET TF_DOWNLOAD_CLANG=0
    SET TF_NEED_CUDA=0
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
    SET TMP=C:\Users\ADMINI~1\AppData\Local\Temp
  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/link.exe /nologo /OUT:bazel-out/x64_windows-opt/bin/tensorflow/tools/graph_transforms/transform_graph.exe /SUBSYSTEM:CONSOLE -DEFAULTLIB:advapi32.lib /MACHINE:X64 @bazel-out/x64_windows-opt/bin/tensorflow/tools/graph_transforms/transform_graph.exe-2.params /OPT:ICF /OPT:REF
LINK : warning LNK4044: unrecognized option '/lpthread'; ignored
   Creating library bazel-out/x64_windows-opt/bin/tensorflow/tools/graph_transforms/transform_graph.lib and object bazel-out/x64_windows-opt/bin/tensorflow/tools/graph_transforms/transform_graph.exp
batch_kernels.lo.lib(batch_kernels.obj) : warning LNK4217: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported in function ""void __cdecl tensorflow::`dynamic initializer for 'registrar__body__0__object''(void)"" (??__Eregistrar__body__0__object@tensorflow@@YAXXZ)
captured_function.lib(captured_function.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported
arithmetic_optimizer.lib(arithmetic_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported
pin_to_host_optimizer.lib(pin_to_host_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported
arithmetic_optimizer.lib(arithmetic_optimizer.obj) : warning LNK4217: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported in function ""private: bool __cdecl tensorflow::grappler::`anonymous namespace'::ReorderCastAndTranspose::NodeIsOnCpuOrGpu(class tensorflow::NodeDef const *)const "" (?NodeIsOnCpuOrGpu@ReorderCastAndTranspose@?A0x2ef8a95a@grappler@tensorflow@@AEBA_NPEBVNodeDef@4@@Z)
layout_optimizer.lib(layout_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported
pin_to_host_optimizer.lib(pin_to_host_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported
unicode_script_op.lo.lib(unicode_script_op.obj) : error LNK2019: unresolved external symbol ""__declspec(dllimport) public: virtual __cdecl icu_62::ErrorCode::~ErrorCode(void)"" (__imp_??1ErrorCode@icu_62@@UEAA@XZ) referenced in function ""public: virtual void __cdecl tensorflow::UnicodeScriptOp::Compute(class tensorflow::OpKernelContext *)"" (?Compute@UnicodeScriptOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
unicode_script_op.lo.lib(unicode_script_op.obj) : error LNK2019: unresolved external symbol ""__declspec(dllimport) public: signed char __cdecl icu_62::ErrorCode::isSuccess(void)const "" (__imp_?isSuccess@ErrorCode@icu_62@@QEBACXZ) referenced in function ""public: virtual void __cdecl tensorflow::UnicodeScriptOp::Compute(class tensorflow::OpKernelContext *)"" (?Compute@UnicodeScriptOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
unicode_script_op.lo.lib(unicode_script_op.obj) : error LNK2019: unresolved external symbol ""__declspec(dllimport) public: enum UErrorCode __cdecl icu_62::ErrorCode::reset(void)"" (__imp_?reset@ErrorCode@icu_62@@QEAA?AW4UErrorCode@@XZ) referenced in function ""public: virtual void __cdecl tensorflow::UnicodeScriptOp::Compute(class tensorflow::OpKernelContext *)"" (?Compute@UnicodeScriptOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
unicode_script_op.lo.lib(unicode_script_op.obj) : error LNK2019: unresolved external symbol ""__declspec(dllimport) const icu_62::ErrorCode::`vftable'"" (__imp_??_7ErrorCode@icu_62@@6B@) referenced in function ""public: virtual void __cdecl tensorflow::UnicodeScriptOp::Compute(class tensorflow::OpKernelContext *)"" (?Compute@UnicodeScriptOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
bazel-out/x64_windows-opt/bin/tensorflow/tools/graph_transforms/transform_graph.exe : fatal error LNK1120: 4 unresolved externals
Target //tensorflow/tools/graph_transforms:transform_graph failed to build
INFO: Elapsed time: 569.319s, Critical Path: 237.59s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]
INFO: 0 processes.
FAILED: Build did NOT complete successfully

"
