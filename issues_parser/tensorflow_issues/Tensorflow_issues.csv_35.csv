Issue Number,Issue Title,Issue Body
33380,get_config missing from AdditiveAttention,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0.0
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
Model containing AdditiveAttention cannot be saved due to missing get_config

**Describe the expected behavior**
AdditiveAttention has get_config and can be saved

**Code to reproduce the issue**
```
import tensorflow as tf
max_tokens=6
dimension = 2

# Variable-length int sequences.
query_input = tf.keras.Input(shape=(None,), dtype='int32')
value_input = tf.keras.Input(shape=(None,), dtype='int32')

# Embedding lookup.
token_embedding = tf.keras.layers.Embedding(max_tokens, dimension)
# Query embeddings of shape [batch_size, Tq, dimension].
query_embeddings = token_embedding(query_input)
# Value embeddings of shape [batch_size, Tv, dimension].
value_embeddings = token_embedding(query_input)

# CNN layer.
cnn_layer = tf.keras.layers.Conv1D(
    filters=100,
    kernel_size=4,
    # Use 'same' padding so outputs have the same shape as inputs.
    padding='same')
# Query encoding of shape [batch_size, Tq, filters].
query_seq_encoding = cnn_layer(query_embeddings)
# Value encoding of shape [batch_size, Tv, filters].
value_seq_encoding = cnn_layer(value_embeddings)

# Query-value attention of shape [batch_size, Tq, filters].
query_value_attention_seq = tf.keras.layers.AdditiveAttention()(
    [query_seq_encoding, value_seq_encoding])

# Reduce over the sequence axis to produce encodings of shape
# [batch_size, filters].
query_encoding = tf.keras.layers.GlobalAveragePooling1D()(
    query_seq_encoding)
query_value_attention = tf.keras.layers.GlobalAveragePooling1D()(
    query_value_attention_seq)

# Concatenate query and document encodings to produce a DNN input layer.
input_layer = tf.keras.layers.Concatenate()(
    [query_encoding, query_value_attention])


model = tf.keras.Model(inputs=(query_input,value_input),outputs=input_layer)
model.save('test.h5')
#NotImplementedError: Layers with arguments in `__init__` must override `get_config`.
```

**Other info / logs**
Code based on https://www.tensorflow.org/api_docs/python/tf/keras/layers/AdditiveAttention

"
33379,Tensorflow 1.13.1 cannot detect my GPUs,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):

```python
tf.test.gpu_device_name()
```
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): conda 
- TensorFlow version (use command below): 1.13.1
- Python version: 3.7.4
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:
|   0  Quadro RTX 6000     On   | 00000000:1A:00.0 Off |                  Off |
| 33%   27C    P8    14W / 260W |      0MiB / 24219MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Quadro RTX 6000     On   | 00000000:1B:00.0 Off |                  Off |
| 33%   24C    P8    16W / 260W |      0MiB / 24220MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Quadro RTX 6000     On   | 00000000:60:00.0 Off |                  Off |
| 34%   26C    P8     8W / 260W |      0MiB / 24220MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Quadro RTX 6000     On   | 00000000:61:00.0 Off |                  Off |
| 55%   74C    P2   103W / 260W |  17128MiB / 24220MiB |     58%      Default |
+-------------------------------+----------------------+----------------------+
|   4  Quadro RTX 6000     On   | 00000000:B1:00.0 Off |                  Off |
| 56%   76C    P2   257W / 260W |  18782MiB / 24220MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   5  Quadro RTX 6000     On   | 00000000:B2:00.0 Off |                  Off |
| 55%   75C    P2   258W / 260W |  17128MiB / 24220MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   6  Quadro RTX 6000     On   | 00000000:DA:00.0 Off |                  Off |
| 45%   68C    P2   240W / 260W |  19408MiB / 24220MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   7  Quadro RTX 6000     On   | 00000000:DB:00.0 Off |                  Off |
| 33%   42C    P2    62W / 260W |   5968MiB / 24220MiB |     11%      Default |

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
```
2019-10-15 10:29:55.961860: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2019-10-15 10:29:55.997917: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2100000000 Hz
2019-10-15 10:29:56.003864: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55c322117980 executing computations on platform Host. Devices:
2019-10-15 10:29:56.003932: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
```
**Describe the expected behavior**
When I use TF2.0 in another conda environment, it gives me:

```
2019-10-15 10:31:36.984764: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2019-10-15 10:31:37.046278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2100000000 Hz
2019-10-15 10:31:37.052904: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561b33274680 executing computations on platform Host. Devices:
2019-10-15 10:31:37.052993: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-10-15 10:31:37.055456: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-10-15 10:31:39.414744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
name: Quadro RTX 6000 major: 7 minor: 5 memoryClockRate(GHz): 1.77
pciBusID: 0000:1a:00.0
2019-10-15 10:31:39.416616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties:
name: Quadro RTX 6000 major: 7 minor: 5 memoryClockRate(GHz): 1.77
pciBusID: 0000:1b:00.0
2019-10-15 10:31:39.418257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties:
name: Quadro RTX 6000 major: 7 minor: 5 memoryClockRate(GHz): 1.77
pciBusID: 0000:60:00.0
2019-10-15 10:31:39.419261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties:
name: Quadro RTX 6000 major: 7 minor: 5 memoryClockRate(GHz): 1.77
pciBusID: 0000:61:00.0
2019-10-15 10:31:39.420278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties:
name: Quadro RTX 6000 major: 7 minor: 5 memoryClockRate(GHz): 1.77
pciBusID: 0000:b1:00.0
2019-10-15 10:31:39.421270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties:
name: Quadro RTX 6000 major: 7 minor: 5 memoryClockRate(GHz): 1.77
pciBusID: 0000:b2:00.0
2019-10-15 10:31:39.422273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties:
name: Quadro RTX 6000 major: 7 minor: 5 memoryClockRate(GHz): 1.77
pciBusID: 0000:da:00.0
2019-10-15 10:31:39.423781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 7 with properties:
name: Quadro RTX 6000 major: 7 minor: 5 memoryClockRate(GHz): 1.77
pciBusID: 0000:db:00.0
2019-10-15 10:31:39.424026: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-10-15 10:31:39.425587: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2019-10-15 10:31:39.427313: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10
2019-10-15 10:31:39.427584: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10
2019-10-15 10:31:39.429138: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10
2019-10-15 10:31:39.429937: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10
2019-10-15 10:31:39.433385: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-10-15 10:31:39.455050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2019-10-15 10:31:39.455089: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-10-15 10:31:39.468145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-10-15 10:31:39.468164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 4 5 6 7
2019-10-15 10:31:39.468172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y Y Y Y Y Y Y
2019-10-15 10:31:39.468179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N Y Y Y Y Y Y
2019-10-15 10:31:39.468185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   Y Y N Y Y Y Y Y
2019-10-15 10:31:39.468192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   Y Y Y N Y Y Y Y
2019-10-15 10:31:39.468198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 4:   Y Y Y Y N Y Y Y
2019-10-15 10:31:39.468206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 5:   Y Y Y Y Y N Y Y
2019-10-15 10:31:39.468213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 6:   Y Y Y Y Y Y N Y
2019-10-15 10:31:39.468223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 7:   Y Y Y Y Y Y Y N
2019-10-15 10:31:39.482310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 22843 MB memory) -> physical GPU (device: 0, name: Quadro RTX 6000, pci bus id: 0000:1a:00.0, compute capability: 7.5)
2019-10-15 10:31:39.486638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:1 with 22845 MB memory) -> physical GPU (device: 1, name: Quadro RTX 6000, pci bus id: 0000:1b:00.0, compute capability: 7.5)
2019-10-15 10:31:39.490720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:2 with 22845 MB memory) -> physical GPU (device: 2, name: Quadro RTX 6000, pci bus id: 0000:60:00.0, compute capability: 7.5)
2019-10-15 10:31:39.493272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:3 with 4348 MB memory) -> physical GPU (device: 3, name: Quadro RTX 6000, pci bus id: 0000:61:00.0, compute capability: 7.5)
2019-10-15 10:31:39.496165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:4 with 4974 MB memory) -> physical GPU (device: 4, name: Quadro RTX 6000, pci bus id: 0000:b1:00.0, compute capability: 7.5)
2019-10-15 10:31:39.498872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:5 with 4348 MB memory) -> physical GPU (device: 5, name: Quadro RTX 6000, pci bus id: 0000:b2:00.0, compute capability: 7.5)
2019-10-15 10:31:39.501375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:6 with 4348 MB memory) -> physical GPU (device: 6, name: Quadro RTX 6000, pci bus id: 0000:da:00.0, compute capability: 7.5)
2019-10-15 10:31:39.504819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:7 with 17184 MB memory) -> physical GPU (device: 7, name: Quadro RTX 6000, pci bus id: 0000:db:00.0, compute capability: 7.5)
2019-10-15 10:31:39.509288: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561b387dc0c0 executing computations on platform CUDA. Devices:
2019-10-15 10:31:39.509305: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Quadro RTX 6000, Compute Capability 7.5
2019-10-15 10:31:39.509311: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): Quadro RTX 6000, Compute Capability 7.5
2019-10-15 10:31:39.509317: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (2): Quadro RTX 6000, Compute Capability 7.5
2019-10-15 10:31:39.509322: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (3): Quadro RTX 6000, Compute Capability 7.5
2019-10-15 10:31:39.509328: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (4): Quadro RTX 6000, Compute Capability 7.5
2019-10-15 10:31:39.509334: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (5): Quadro RTX 6000, Compute Capability 7.5
2019-10-15 10:31:39.509339: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (6): Quadro RTX 6000, Compute Capability 7.5
2019-10-15 10:31:39.509344: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (7): Quadro RTX 6000, Compute Capability 7.5
'/device:GPU:0'
```

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
I need to work on TF 1.13.1 because some specific code requires this version.
"
33378,"Problem with Keras, Tensorflow","
**System information**
- Windows 10
- TensorFlow installed from anaconda
- Python version: 3.7
- Installed using conda
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: Nvidia GeForce GTX 260, Graphics memory 3GB

Code:

from keras.models import Sequential

Error:

Using TensorFlow backend.
ERROR:root:Internal Python error in the inspect module.
Below is the traceback from this internal error.

Traceback:

Traceback (most recent call last):
  File ""C:\Users\chris\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\chris\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\chris\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\chris\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\chris\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\chris\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3326, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-1-9c5e0a19b646>"", line 1, in <module>
    from keras.models import Sequential
  File ""C:\Users\chris\Anaconda3\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils
  File ""C:\Users\chris\Anaconda3\lib\site-packages\keras\utils\__init__.py"", line 6, in <module>
    from . import conv_utils
  File ""C:\Users\chris\Anaconda3\lib\site-packages\keras\utils\conv_utils.py"", line 9, in <module>
    from .. import backend as K
  File ""C:\Users\chris\Anaconda3\lib\site-packages\keras\backend\__init__.py"", line 1, in <module>
    from .load_backend import epsilon
  File ""C:\Users\chris\Anaconda3\lib\site-packages\keras\backend\load_backend.py"", line 90, in <module>
    from .tensorflow_backend import *
  File ""C:\Users\chris\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf
  File ""C:\Users\chris\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 98, in <module>
    from tensorflow_core import *
  File ""C:\Users\chris\Anaconda3\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\chris\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\chris\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\chris\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\chris\Anaconda3\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\chris\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\chris\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\chris\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\chris\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\chris\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\chris\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
"
33376,importing tensorflow inside a function/object causes a memory leak,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX 10.15
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): pip install tensorflow==1.14
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6.8
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: -
- GPU model and memory: -

**Describe the current behavior**
When importing tensorflow from a function or object, the `import` statement somehow keeps a reference to the function and increasing it's reference count. The full `import` stacktrace is never freed, making it impossible for the object (and anything referenced from that object or function) to be freed from memory.

**Describe the expected behavior**
It should be possible to free the function calling `import tensorflow`. This is not an issue with any other imports (like `import logger`).

**Code to reproduce the issue**
```
import gc


class TFImporter:
    def __init__(self, name):
        self._name = name
        print(f""TFImporter init {self._name}"")

    def get_tf(self):
        print(f""import tensorflow {self._name}"")
        import tensorflow
        print(tensorflow.version.VERSION)

    def get_other_module(self):
        print(f""import logging {self._name}"")
        import logging
        logging.info(""Message"")

    def __del__(self):
        print(f""TFImporter delete {self._name}"")


def main():
    importer1 = TFImporter(1)
    importer1.get_other_module()
    del importer1
    print(""importer1 deleted"")

    importer2 = TFImporter(2)
    importer2.get_tf()
    del importer2
    print(""importer2 deleted"")

    importer3 = TFImporter(3)
    importer3.get_tf()
    del importer3
    print(""importer3 deleted"")

    print(f""Garbage collection: {gc.collect()}"")

    print(f""Waiting for input:"")
    input()


main()
```

this outputs:

```
/Users/jan/miniconda/envs/foo/bin/python /Users/jan/code/tensorflow_error.py
TFImporter init 1
import logging 1
TFImporter delete 1
importer1 deleted
TFImporter init 2
import tensorflow 2
1.14.0
importer2 deleted
TFImporter init 3
import tensorflow 3
1.14.0
TFImporter delete 3
importer3 deleted
Garbage collection: 22
Waiting for input:
foo
TFImporter delete 2

Process finished with exit code 0
```

So `importer2` is only freed after the python application finishes. Neither `gc.collect` nor deleting the object causes it to be released in python.

This is not an issue in this toy example, but `importer2` could have a reference to a large number of other objects that take considerable space in memory in reality.

Also, this only happens for the first `import`. `importer3` can be freed without issues.

**Other info / logs**
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/3729501/tf_env.txt)
"
33375,TF2.0 ptxas ignores PATH,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux x86_64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.0
- Python version: 3.7.4
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version: 10.0/7.6
- GPU model and memory: GeForce RTX 2080 Ti

**Describe the problem**

I am receiving an error
`E tensorflow/stream_executor/cuda/ptxas_utils.cc:110] You are using ptxas 8.x, but TF requires ptxas 9.x (and strongly prefers >= 9.2.88).  Compilation of XLA kernels below will likely fail.`
suggesting that tensorflow uses ptxas version that ignores the PATH variable.

PATH, LD_LIBRARY_PATH, CUDA_HOME all points to the CUDA 10.0 directory
LD_LIBRARY_PATH also points to the CUDNN 7.6 directory

typing `ptxas --version` gives me:
`Cuda compilation tools, release 10.0, V10.0.145`

How is it possible that tensorflow launches ptxas of version 8 (probably from /usr/local/cuda/bin/) when all the important environment variables points to ptxas of version 10?

**Provide the exact sequence of commands / steps that you executed before running into the problem**

The bug can be spotted for example when using tensorflow-addons and image rotate function.

```
import numpy as np
import tensorflow as tf
import tensorflow_addons as tfa

class Model:
    def __init__(self):
        tf.keras.backend.clear_session()
        
        inp = tf.keras.layers.Input((32,32,3), name='input_image')
        net = tf.keras.layers.Conv2D(3, 3, padding=""same"")(inp)

        self.model  = tf.keras.Model(inputs=inp, outputs=net)
        self.optimizer = tf.keras.optimizers.Adam()
        self.loss_obj = tf.keras.losses.MeanSquaredError()
        
    @tf.function
    def train_step(self):        
        angles_rad = tf.random.uniform((), 0, 3.14)
        images = tf.random.uniform((2,32,32,3))

        with tf.GradientTape() as tape:
            features = self.model(images, training=True)

            #angles_rad = tf.constant([0.5,0.4])
            rot_features = tfa.image.rotate(features, angles_rad, interpolation='NEAREST', name=""rotate_features"")
            
            loss = self.loss_obj(images, rot_features)

        variables = self.model.trainable_variables
        gradients = tape.gradient(loss, variables)
        self.optimizer.apply_gradients(zip(gradients, variables))
        return loss
    
    def train(self, nr_epochs):
        for epoch in range(nr_epochs):
            loss = self.train_step()
            print(""Train Epoch {}: {}"".format(epoch, loss))

trainer = Model()
trainer.train(5000)
```

**Any other info / logs**
Full log of the code above:

2019-10-15 15:23:07.768703: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2019-10-15 15:23:07.780261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:3b:00.0
2019-10-15 15:23:07.780892: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-10-15 15:23:07.782852: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-10-15 15:23:07.784779: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-10-15 15:23:07.785515: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-10-15 15:23:07.787842: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-10-15 15:23:07.790057: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-10-15 15:23:07.794875: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-10-15 15:23:07.796010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-10-15 15:23:07.796248: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-10-15 15:23:07.804370: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3200000000 Hz
2019-10-15 15:23:07.806385: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562d5b167670 executing computations on platform Host. Devices:
2019-10-15 15:23:07.806411: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2019-10-15 15:23:07.919879: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562d5b1ca830 executing computations on platform CUDA. Devices:
2019-10-15 15:23:07.919925: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5
2019-10-15 15:23:07.920755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:3b:00.0
2019-10-15 15:23:07.920810: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-10-15 15:23:07.920825: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-10-15 15:23:07.920837: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-10-15 15:23:07.920855: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-10-15 15:23:07.920867: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-10-15 15:23:07.920878: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-10-15 15:23:07.920894: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-10-15 15:23:07.921926: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-10-15 15:23:07.921954: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-10-15 15:23:07.922826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-10-15 15:23:07.922841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2019-10-15 15:23:07.922853: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2019-10-15 15:23:07.923954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10312 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:3b:00.0, compute capability: 7.5)
2019-10-15 15:23:10.869688: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-10-15 15:23:12.622865: E tensorflow/stream_executor/cuda/ptxas_utils.cc:110] You are using ptxas 8.x, but TF requires ptxas 9.x (and strongly prefers >= 9.2.88).  Compilation of XLA kernels below will likely fail.

You do not need to update CUDA; cherry-picking the ptxas binary is sufficient.
2019-10-15 15:23:12.700955: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Internal: ptxas exited with non-zero error code 65280, output: ptxas fatal   : Value 'sm_75' is not defined for option 'gpu-name'

Relying on driver to perform ptx compilation. This message will be only logged once.
2019-10-15 15:23:15.104683: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x562d5b1cdff0
2019-10-15 15:23:15.104819: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-10-15 15:23:15.436019: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-10-15 15:23:15.727354: F tensorflow/core/kernels/cuda_solvers.cc:99] Check failed: cublasCreate(&cublas_handle) == CUBLAS_STATUS_SUCCESS Failed to create cuBlas instance.
Aborted (core dumped)
"
33374,Python3.8 support,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version: 2
- Python version: 3.8
- Installed using virtualenv? pip? conda?: venv
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA



**Describe the problem**
 cant install tensorflow
**Provide the exact sequence of commands / steps that you executed before running into the problem**

install python 3.8
create a new venv
`venv> pip install tensorflow`
fails with `ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensorflow`


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


EDIT: I guess you don't provide binaries for python 3.8 (https://pypi.org/project/tensorflow/#files)
is there another issue I can subscribe for about support for python 3.8? (couldn't find one, so keeping this open for now)"
33373,tf.reduce_mean gives incorrect results on CPU,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0
- Python version: 3.7.4

**Current Behavior**

The following script:

```python
import tensorflow as tf
import numpy as np

(x_train, _), (_, _) = tf.keras.datasets.cifar10.load_data()
x_train = x_train.astype('float32')

tf_mean = tf.reduce_mean(x_train, axis=[0, 1, 2], keepdims=False)
np_mean = np.mean(x_train, (0,1,2))
print('channel means:')
print('  tf:', tf_mean)
print('  np:', np_mean)
```

prints:

```
channel means:
  tf: tf.Tensor([83.88608 83.88608 83.88608], shape=(3,), dtype=float32)
  np: [125.3069  122.95015 113.866  ]
```

Note: the numpy results are the correct channel-wise means for CIFAR10.

**Expected Behavior**
Tensorflow and numpy should give at least vaguely similar results."
33371,How to make a custom Model with a stateful LSTM,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): The Decoder part in the official example: https://www.tensorflow.org/tutorials/text/nmt_with_attention
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0.0
- Python version: 3.6.9
- CUDA/cuDNN version: 10.0
- GPU model and memory: GTX 1080Ti, 12 GB

**Describe the current behavior**
ValueError: If a RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: 
- If using a Sequential model, specify the batch size by passing a `batch_input_shape` argument to your first layer.
- If using the functional API, specify the batch size by passing a `batch_shape` argument to your Input layer.

**Describe the expected behavior**
I want to make a custom decoder layer with a stateful LSTM and a pre-net which is a simple Dense, but I do not want to use Sequential or functional API, because I want to control with reset_states() when the LSTM states will be reset. It always shows the error above. How should I give the input shape in this custom layer?

**Code to reproduce the issue**
```python
class Decoder(Model):
    def __init__(self, dim):
        super(DecoderCell, self).__init__()
        self.input = Sequential([
            Input(batch_shape=(32, 1, num_mels)),
            Dense(128, activation='relu')
        ])

        self.rnn = LSTM(256, stateful=True)

    def call(self, x):
        x = self.input(x)

        x = self.rnn(x)

        return x
```"
33370,Compiling Tensorflow Lite on Arm,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.10.0
- Bazel version (if compiling from source): 0.18.1
- GCC/Compiler version (if compiling from source): 4.8.5
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


I'm using ./tensorflow/contrib/lite/build_imx6_lib.sh to build tensorflow-lite, 
modified a little from build_rpi_lib.sh: CC_PREFIX=arm-poky-linux-gnueabi- make -j 3 -f tensorflow/contrib/lite/Makefile TARGET=RPI TARGET_ARCH=armv7-a


**Any other info / logs**
And I got following problem:
./tensorflow/contrib/lite/build_iMx6_lib.sh 
+ set -e
+++ dirname ./tensorflow/contrib/lite/build_iMx6_lib.sh
++ cd ./tensorflow/contrib/lite
++ pwd
+ SCRIPT_DIR=/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite
+ cd /home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/../../..
+ CC_PREFIX=arm-poky-linux-gnueabi-
+ make -j 3 -f tensorflow/contrib/lite/Makefile TARGET=imx6 TARGET_ARCH=armv7-a
tensorflow/contrib/lite/Makefile:25: ""CROSS :imx6 HOST_ARCH: TARGET_ARCH:armv7-a TARGET_TOOLCHAIN_PREFIX:""
/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/gen/bin/imx6_armv7-a/benchmark_model
g++ -pthread -fPIC -O3 -DNDEBUG --std=c++11 -I. -I/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/../../../ -I/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/downloads/ -I/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/downloads/eigen -I/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/downloads/gemmlowp -I/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/downloads/neon_2_sse -I/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/downloads/farmhash/src -I/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/downloads/flatbuffers/include -I/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/gen/obj/ -I/usr/include -c tensorflow/contrib/lite/kernels/activations.cc -o /home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/gen/obj/imx6_armv7-a/tensorflow/contrib/lite/kernels/activations.o
g++ -pthread -fPIC -O3 -DNDEBUG --std=c++11 -I. -I/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/../../../ -I/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/downloads/ -I/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/downloads/eigen -I/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/downloads/gemmlowp -I/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/downloads/neon_2_sse -I/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/downloads/farmhash/src -I/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/downloads/flatbuffers/include -I/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/gen/obj/ -I/usr/include -c tensorflow/contrib/lite/kernels/add.cc -o /home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/gen/obj/imx6_armv7-a/tensorflow/contrib/lite/kernels/add.o
g++ -pthread -fPIC -O3 -DNDEBUG --std=c++11 -I. -I/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/../../../ -I/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/downloads/ -I/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/downloads/eigen -I/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/downloads/gemmlowp -I/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/downloads/neon_2_sse -I/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/downloads/farmhash/src -I/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/downloads/flatbuffers/include -I/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/gen/obj/ -I/usr/include -c tensorflow/contrib/lite/kernels/arg_min_max.cc -o /home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/gen/obj/imx6_armv7-a/tensorflow/contrib/lite/kernels/arg_min_max.o
In file included from ./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:34:0,
                 from tensorflow/contrib/lite/kernels/activations.cc:25:
./tensorflow/contrib/lite/kernels/internal/reference/reference_ops.h:2247:5: error: ‘Eigen’ was not declared in this scope
     Eigen::Map<const Eigen::Matrix<typename std::remove_const<Scalar>::type,
     ^
./tensorflow/contrib/lite/kernels/internal/reference/reference_ops.h:2249:75: error: wrong number of template arguments (2, should be 3)
     Eigen::Map<Eigen::Matrix<std::Scalar, Eigen::Dynamic, Eigen::Dynamic>>>::type;
                                                                           ^
In file included from /usr/include/c++/4.8/bits/move.h:57:0,
                 from /usr/include/c++/4.8/bits/stl_pair.h:59,
                 from /usr/include/c++/4.8/bits/stl_algobase.h:64,
                 from /usr/include/c++/4.8/bits/char_traits.h:39,
                 from /usr/include/c++/4.8/ios:40,
                 from /usr/include/c++/4.8/ostream:38,
                 from /usr/include/c++/4.8/iostream:39,
                 from tensorflow/contrib/lite/kernels/activations.cc:20:
/usr/include/c++/4.8/type_traits:77:12: error: provided for ‘template<bool <anonymous>, class, class> struct std::conditional’
     struct conditional;
            ^
In file included from ./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:34:0,
                 from tensorflow/contrib/lite/kernels/activations.cc:25:
./tensorflow/contrib/lite/kernels/internal/reference/reference_ops.h:2249:76: error: expected identifier before ‘::’ token
     Eigen::Map<Eigen::Matrix<std::Scalar, Eigen::Dynamic, Eigen::Dynamic>>>::type;
                                                                            ^
./tensorflow/contrib/lite/kernels/internal/reference/reference_ops.h:2252:1: error: ‘MatrixMap’ does not name a type
 MatrixMap<Scalar> MapAsMatrixWithFirstDimAsRows(Scalar* data,
 ^
./tensorflow/contrib/lite/kernels/internal/reference/reference_ops.h:2263:1: error: ‘MatrixMap’ does not name a type
 MatrixMap<Scalar> MapAsMatrixWithLastDimAsCols(Scalar* data,
 ^
In file included from ./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:34:0,
                 from tensorflow/contrib/lite/kernels/add.cc:17:
./tensorflow/contrib/lite/kernels/internal/reference/reference_ops.h:2247:5: error: ‘Eigen’ was not declared in this scope
     Eigen::Map<const Eigen::Matrix<typename std::remove_const<Scalar>::type,
     ^
./tensorflow/contrib/lite/kernels/internal/reference/reference_ops.h:2249:75: error: wrong number of template arguments (2, should be 3)
     Eigen::Map<Eigen::Matrix<std::Scalar, Eigen::Dynamic, Eigen::Dynamic>>>::type;
                                                                           ^
In file included from /usr/include/c++/4.8/bits/move.h:57:0,
                 from /usr/include/c++/4.8/bits/stl_pair.h:59,
                 from /usr/include/c++/4.8/utility:70,
                 from /usr/include/c++/4.8/algorithm:60,
                 from ./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:21,
                 from tensorflow/contrib/lite/kernels/add.cc:17:
/usr/include/c++/4.8/type_traits:77:12: error: provided for ‘template<bool <anonymous>, class, class> struct std::conditional’
     struct conditional;
            ^
In file included from ./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:34:0,
                 from tensorflow/contrib/lite/kernels/add.cc:17:
./tensorflow/contrib/lite/kernels/internal/reference/reference_ops.h:2249:76: error: expected identifier before ‘::’ token
     Eigen::Map<Eigen::Matrix<std::Scalar, Eigen::Dynamic, Eigen::Dynamic>>>::type;
                                                                            ^
./tensorflow/contrib/lite/kernels/internal/reference/reference_ops.h:2252:1: error: ‘MatrixMap’ does not name a type
 MatrixMap<Scalar> MapAsMatrixWithFirstDimAsRows(Scalar* data,
 ^
./tensorflow/contrib/lite/kernels/internal/reference/reference_ops.h:2263:1: error: ‘MatrixMap’ does not name a type
 MatrixMap<Scalar> MapAsMatrixWithLastDimAsCols(Scalar* data,
 ^
In file included from tensorflow/contrib/lite/kernels/activations.cc:25:0:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:86:5: error: ‘Eigen’ was not declared in this scope
     Eigen::Map<const Eigen::Matrix<typename std::remove_const<Scalar>::type,
     ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:88:57: error: wrong number of template arguments (2, should be 3)
     Eigen::Map<Eigen::Matrix<Scalar, Eigen::Dynamic, 1>>>::type;
                                                         ^
In file included from /usr/include/c++/4.8/bits/move.h:57:0,
                 from /usr/include/c++/4.8/bits/stl_pair.h:59,
                 from /usr/include/c++/4.8/bits/stl_algobase.h:64,
                 from /usr/include/c++/4.8/bits/char_traits.h:39,
                 from /usr/include/c++/4.8/ios:40,
                 from /usr/include/c++/4.8/ostream:38,
                 from /usr/include/c++/4.8/iostream:39,
                 from tensorflow/contrib/lite/kernels/activations.cc:20:
/usr/include/c++/4.8/type_traits:77:12: error: provided for ‘template<bool <anonymous>, class, class> struct std::conditional’
     struct conditional;
            ^
In file included from tensorflow/contrib/lite/kernels/activations.cc:25:0:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:88:58: error: expected identifier before ‘::’ token
     Eigen::Map<Eigen::Matrix<Scalar, Eigen::Dynamic, 1>>>::type;
                                                          ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:91:1: error: ‘VectorMap’ does not name a type
 VectorMap<Scalar> MapAsVector(Scalar* data, const RuntimeShape& shape) {
 ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:97:1: error: ‘VectorMap’ does not name a type
 VectorMap<Scalar> MapAsVector(Scalar* data, const Dims<N>& dims) {
 ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:108:5: error: ‘Eigen’ was not declared in this scope
     Eigen::Map<const Eigen::Matrix<typename std::remove_const<Scalar>::type,
     ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:110:70: error: wrong number of template arguments (2, should be 3)
     Eigen::Map<Eigen::Matrix<Scalar, Eigen::Dynamic, Eigen::Dynamic>>>::type;
                                                                      ^
In file included from /usr/include/c++/4.8/bits/move.h:57:0,
                 from /usr/include/c++/4.8/bits/stl_pair.h:59,
                 from /usr/include/c++/4.8/bits/stl_algobase.h:64,
                 from /usr/include/c++/4.8/bits/char_traits.h:39,
                 from /usr/include/c++/4.8/ios:40,
                 from /usr/include/c++/4.8/ostream:38,
                 from /usr/include/c++/4.8/iostream:39,
                 from tensorflow/contrib/lite/kernels/activations.cc:20:
/usr/include/c++/4.8/type_traits:77:12: error: provided for ‘template<bool <anonymous>, class, class> struct std::conditional’
     struct conditional;
            ^
In file included from tensorflow/contrib/lite/kernels/activations.cc:25:0:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:110:71: error: expected identifier before ‘::’ token
     Eigen::Map<Eigen::Matrix<Scalar, Eigen::Dynamic, Eigen::Dynamic>>>::type;
                                                                       ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:113:1: error: ‘MatrixMap’ does not name a type
 MatrixMap<Scalar> MapAsMatrixWithLastDimAsRows(Scalar* data,
 ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:122:1: error: ‘MatrixMap’ does not name a type
 MatrixMap<Scalar> MapAsMatrixWithFirstDimAsCols(Scalar* data,
 ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:130:1: error: ‘MatrixMap’ does not name a type
 MatrixMap<Scalar> MapAsMatrixWithFirstDimAsRows(Scalar* data,
 ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:141:1: error: ‘MatrixMap’ does not name a type
 MatrixMap<Scalar> MapAsMatrixWithLastDimAsCols(Scalar* data,
 ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:154:5: error: ‘Eigen’ was not declared in this scope
     Eigen::Map<const Eigen::Array<typename std::remove_const<Scalar>::type,
     ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:156:69: error: wrong number of template arguments (2, should be 3)
     Eigen::Map<Eigen::Array<Scalar, Eigen::Dynamic, Eigen::Dynamic>>>::type;
                                                                     ^
In file included from /usr/include/c++/4.8/bits/move.h:57:0,
                 from /usr/include/c++/4.8/bits/stl_pair.h:59,
                 from /usr/include/c++/4.8/bits/stl_algobase.h:64,
                 from /usr/include/c++/4.8/bits/char_traits.h:39,
                 from /usr/include/c++/4.8/ios:40,
                 from /usr/include/c++/4.8/ostream:38,
                 from /usr/include/c++/4.8/iostream:39,
                 from tensorflow/contrib/lite/kernels/activations.cc:20:
/usr/include/c++/4.8/type_traits:77:12: error: provided for ‘template<bool <anonymous>, class, class> struct std::conditional’
     struct conditional;
            ^
In file included from tensorflow/contrib/lite/kernels/activations.cc:25:0:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:156:70: error: expected identifier before ‘::’ token
     Eigen::Map<Eigen::Array<Scalar, Eigen::Dynamic, Eigen::Dynamic>>>::type;
                                                                      ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:159:1: error: ‘ArrayMap’ does not name a type
 ArrayMap<Scalar> MapAsArrayWithFirstDimAsRows(Scalar* data,
 ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:172:1: error: ‘MatrixMap’ does not name a type
 MatrixMap<Scalar> MapAsMatrixWithGivenNumberOfRows(Scalar* data,
 ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:408:17: error: ‘Eigen’ does not name a type
 void Gemm(const Eigen::MatrixBase<Lhs>& lhs, const Eigen::MatrixBase<Rhs>& rhs,
                 ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:408:34: error: expected unqualified-id before ‘<’ token
 void Gemm(const Eigen::MatrixBase<Lhs>& lhs, const Eigen::MatrixBase<Rhs>& rhs,
                                  ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:408:34: error: expected ‘)’ before ‘<’ token
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:408:34: error: expected initializer before ‘<’ token
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::FullyConnected(const float*, const tflite::Dims<4>&, const float*, const tflite::Dims<4>&, const float*, const tflite::Dims<4>&, float, float, float*, const tflite::Dims<4>&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:932:74: error: ‘MapAsMatrixWithGivenNumberOfRows’ was not declared in this scope
       MapAsMatrixWithGivenNumberOfRows(input_data, input_dims, input_rows);
                                                                          ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:934:63: error: ‘MapAsMatrixWithFirstDimAsRows’ was not declared in this scope
       MapAsMatrixWithFirstDimAsRows(weights_data, weights_dims);
                                                               ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:938:75: error: ‘Gemm’ was not declared in this scope
   Gemm(filter_matrix_map.transpose(), input_matrix_map, &output_matrix_map);
                                                                           ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:938:75: note: suggested alternative:
In file included from ./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:31:0,
                 from tensorflow/contrib/lite/kernels/activations.cc:25:
/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/downloads/gemmlowp/public/gemmlowp.h:74:6: note:   ‘gemmlowp::Gemm’
 void Gemm(GemmContextType* context,
      ^
In file included from tensorflow/contrib/lite/kernels/add.cc:17:0:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:86:5: error: ‘Eigen’ was not declared in this scope
     Eigen::Map<const Eigen::Matrix<typename std::remove_const<Scalar>::type,
     ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:88:57: error: wrong number of template arguments (2, should be 3)
     Eigen::Map<Eigen::Matrix<Scalar, Eigen::Dynamic, 1>>>::type;
                                                         ^
In file included from /usr/include/c++/4.8/bits/move.h:57:0,
                 from /usr/include/c++/4.8/bits/stl_pair.h:59,
                 from /usr/include/c++/4.8/utility:70,
                 from /usr/include/c++/4.8/algorithm:60,
                 from ./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:21,
                 from tensorflow/contrib/lite/kernels/add.cc:17:
/usr/include/c++/4.8/type_traits:77:12: error: provided for ‘template<bool <anonymous>, class, class> struct std::conditional’
     struct conditional;
            ^
In file included from tensorflow/contrib/lite/kernels/add.cc:17:0:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:88:58: error: expected identifier before ‘::’ token
     Eigen::Map<Eigen::Matrix<Scalar, Eigen::Dynamic, 1>>>::type;
                                                          ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:91:1: error: ‘VectorMap’ does not name a type
 VectorMap<Scalar> MapAsVector(Scalar* data, const RuntimeShape& shape) {
 ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:97:1: error: ‘VectorMap’ does not name a type
 VectorMap<Scalar> MapAsVector(Scalar* data, const Dims<N>& dims) {
 ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:108:5: error: ‘Eigen’ was not declared in this scope
     Eigen::Map<const Eigen::Matrix<typename std::remove_const<Scalar>::type,
     ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:110:70: error: wrong number of template arguments (2, should be 3)
     Eigen::Map<Eigen::Matrix<Scalar, Eigen::Dynamic, Eigen::Dynamic>>>::type;
                                                                      ^
In file included from /usr/include/c++/4.8/bits/move.h:57:0,
                 from /usr/include/c++/4.8/bits/stl_pair.h:59,
                 from /usr/include/c++/4.8/utility:70,
                 from /usr/include/c++/4.8/algorithm:60,
                 from ./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:21,
                 from tensorflow/contrib/lite/kernels/add.cc:17:
/usr/include/c++/4.8/type_traits:77:12: error: provided for ‘template<bool <anonymous>, class, class> struct std::conditional’
     struct conditional;
            ^
In file included from tensorflow/contrib/lite/kernels/add.cc:17:0:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:110:71: error: expected identifier before ‘::’ token
     Eigen::Map<Eigen::Matrix<Scalar, Eigen::Dynamic, Eigen::Dynamic>>>::type;
                                                                       ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:113:1: error: ‘MatrixMap’ does not name a type
 MatrixMap<Scalar> MapAsMatrixWithLastDimAsRows(Scalar* data,
 ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:122:1: error: ‘MatrixMap’ does not name a type
 MatrixMap<Scalar> MapAsMatrixWithFirstDimAsCols(Scalar* data,
 ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:130:1: error: ‘MatrixMap’ does not name a type
 MatrixMap<Scalar> MapAsMatrixWithFirstDimAsRows(Scalar* data,
 ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:141:1: error: ‘MatrixMap’ does not name a type
 MatrixMap<Scalar> MapAsMatrixWithLastDimAsCols(Scalar* data,
 ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:154:5: error: ‘Eigen’ was not declared in this scope
     Eigen::Map<const Eigen::Array<typename std::remove_const<Scalar>::type,
     ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:156:69: error: wrong number of template arguments (2, should be 3)
     Eigen::Map<Eigen::Array<Scalar, Eigen::Dynamic, Eigen::Dynamic>>>::type;
                                                                     ^
In file included from /usr/include/c++/4.8/bits/move.h:57:0,
                 from /usr/include/c++/4.8/bits/stl_pair.h:59,
                 from /usr/include/c++/4.8/utility:70,
                 from /usr/include/c++/4.8/algorithm:60,
                 from ./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:21,
                 from tensorflow/contrib/lite/kernels/add.cc:17:
/usr/include/c++/4.8/type_traits:77:12: error: provided for ‘template<bool <anonymous>, class, class> struct std::conditional’
     struct conditional;
            ^
In file included from tensorflow/contrib/lite/kernels/add.cc:17:0:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:156:70: error: expected identifier before ‘::’ token
     Eigen::Map<Eigen::Array<Scalar, Eigen::Dynamic, Eigen::Dynamic>>>::type;
                                                                      ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:159:1: error: ‘ArrayMap’ does not name a type
 ArrayMap<Scalar> MapAsArrayWithFirstDimAsRows(Scalar* data,
 ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:172:1: error: ‘MatrixMap’ does not name a type
 MatrixMap<Scalar> MapAsMatrixWithGivenNumberOfRows(Scalar* data,
 ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:408:17: error: ‘Eigen’ does not name a type
 void Gemm(const Eigen::MatrixBase<Lhs>& lhs, const Eigen::MatrixBase<Rhs>& rhs,
                 ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:408:34: error: expected unqualified-id before ‘<’ token
 void Gemm(const Eigen::MatrixBase<Lhs>& lhs, const Eigen::MatrixBase<Rhs>& rhs,
                                  ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:408:34: error: expected ‘)’ before ‘<’ token
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:408:34: error: expected initializer before ‘<’ token
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::FullyConnected(const float*, const tflite::Dims<4>&, const float*, const tflite::Dims<4>&, const float*, const tflite::Dims<4>&, float, float, float*, const tflite::Dims<4>&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:932:74: error: ‘MapAsMatrixWithGivenNumberOfRows’ was not declared in this scope
       MapAsMatrixWithGivenNumberOfRows(input_data, input_dims, input_rows);
                                                                          ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:934:63: error: ‘MapAsMatrixWithFirstDimAsRows’ was not declared in this scope
       MapAsMatrixWithFirstDimAsRows(weights_data, weights_dims);
                                                               ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:938:75: error: ‘Gemm’ was not declared in this scope
   Gemm(filter_matrix_map.transpose(), input_matrix_map, &output_matrix_map);
                                                                           ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:938:75: note: suggested alternative:
In file included from ./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:31:0,
                 from tensorflow/contrib/lite/kernels/add.cc:17:
/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/downloads/gemmlowp/public/gemmlowp.h:74:6: note:   ‘gemmlowp::Gemm’
 void Gemm(GemmContextType* context,
      ^
In file included from ./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:34:0,
                 from tensorflow/contrib/lite/kernels/arg_min_max.cc:17:
./tensorflow/contrib/lite/kernels/internal/reference/reference_ops.h:2247:5: error: ‘Eigen’ was not declared in this scope
     Eigen::Map<const Eigen::Matrix<typename std::remove_const<Scalar>::type,
     ^
./tensorflow/contrib/lite/kernels/internal/reference/reference_ops.h:2249:75: error: wrong number of template arguments (2, should be 3)
     Eigen::Map<Eigen::Matrix<std::Scalar, Eigen::Dynamic, Eigen::Dynamic>>>::type;
                                                                           ^
In file included from /usr/include/c++/4.8/bits/move.h:57:0,
                 from /usr/include/c++/4.8/bits/stl_pair.h:59,
                 from /usr/include/c++/4.8/utility:70,
                 from /usr/include/c++/4.8/algorithm:60,
                 from ./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:21,
                 from tensorflow/contrib/lite/kernels/arg_min_max.cc:17:
/usr/include/c++/4.8/type_traits:77:12: error: provided for ‘template<bool <anonymous>, class, class> struct std::conditional’
     struct conditional;
            ^
In file included from ./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:34:0,
                 from tensorflow/contrib/lite/kernels/arg_min_max.cc:17:
./tensorflow/contrib/lite/kernels/internal/reference/reference_ops.h:2249:76: error: expected identifier before ‘::’ token
     Eigen::Map<Eigen::Matrix<std::Scalar, Eigen::Dynamic, Eigen::Dynamic>>>::type;
                                                                            ^
./tensorflow/contrib/lite/kernels/internal/reference/reference_ops.h:2252:1: error: ‘MatrixMap’ does not name a type
 MatrixMap<Scalar> MapAsMatrixWithFirstDimAsRows(Scalar* data,
 ^
./tensorflow/contrib/lite/kernels/internal/reference/reference_ops.h:2263:1: error: ‘MatrixMap’ does not name a type
 MatrixMap<Scalar> MapAsMatrixWithLastDimAsCols(Scalar* data,
 ^
In file included from tensorflow/contrib/lite/kernels/activations.cc:25:0:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::Conv(const float*, const tflite::Dims<4>&, const float*, const tflite::Dims<4>&, const float*, const tflite::Dims<4>&, int, int, int, int, int, int, float, float, float*, const tflite::Dims<4>&, float*, const tflite::Dims<4>&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:1990:70: error: ‘MapAsMatrixWithFirstDimAsRows’ was not declared in this scope
       MapAsMatrixWithFirstDimAsRows(gemm_input_data, *gemm_input_dims);
                                                                      ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:1992:60: error: ‘MapAsMatrixWithLastDimAsCols’ was not declared in this scope
       MapAsMatrixWithLastDimAsCols(filter_data, filter_dims);
                                                            ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:1996:76: error: ‘Gemm’ was not declared in this scope
   Gemm(filter_matrix_map.transpose(), im2col_matrix_map, &output_matrix_map);
                                                                            ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:1996:76: note: suggested alternative:
In file included from ./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:31:0,
                 from tensorflow/contrib/lite/kernels/activations.cc:25:
/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/downloads/gemmlowp/public/gemmlowp.h:74:6: note:   ‘gemmlowp::Gemm’
 void Gemm(GemmContextType* context,
      ^
In file included from tensorflow/contrib/lite/kernels/add.cc:17:0:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::Conv(const float*, const tflite::Dims<4>&, const float*, const tflite::Dims<4>&, const float*, const tflite::Dims<4>&, int, int, int, int, int, int, float, float, float*, const tflite::Dims<4>&, float*, const tflite::Dims<4>&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:1990:70: error: ‘MapAsMatrixWithFirstDimAsRows’ was not declared in this scope
       MapAsMatrixWithFirstDimAsRows(gemm_input_data, *gemm_input_dims);
                                                                      ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:1992:60: error: ‘MapAsMatrixWithLastDimAsCols’ was not declared in this scope
       MapAsMatrixWithLastDimAsCols(filter_data, filter_dims);
                                                            ^
In file included from tensorflow/contrib/lite/kernels/activations.cc:25:0:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::ConvAsGemm(const float*, const tflite::Dims<4>&, const float*, const tflite::Dims<4>&, const float*, const tflite::Dims<4>&, float*, const tflite::Dims<4>&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:2230:59: error: there are no arguments to ‘MapAsMatrixWithFirstDimAsRows’ that depend on a template parameter, so a declaration of ‘MapAsMatrixWithFirstDimAsRows’ must be available [-fpermissive]
       MapAsMatrixWithFirstDimAsRows(input_data, input_dims);
                                                           ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:2230:59: note: (if you use ‘-fpermissive’, G++ will accept your code, but allowing the use of an undeclared name is deprecated)
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:1996:76: error: ‘Gemm’ was not declared in this scope
   Gemm(filter_matrix_map.transpose(), im2col_matrix_map, &output_matrix_map);
                                                                            ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:1996:76: note: suggested alternative:
In file included from ./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:31:0,
                 from tensorflow/contrib/lite/kernels/add.cc:17:
/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/downloads/gemmlowp/public/gemmlowp.h:74:6: note:   ‘gemmlowp::Gemm’
 void Gemm(GemmContextType* context,
      ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:2232:60: error: there are no arguments to ‘MapAsMatrixWithLastDimAsCols’ that depend on a template parameter, so a declaration of ‘MapAsMatrixWithLastDimAsCols’ must be available [-fpermissive]
       MapAsMatrixWithLastDimAsCols(filter_data, filter_dims);
                                                            ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:2234:61: error: there are no arguments to ‘MapAsMatrixWithFirstDimAsRows’ that depend on a template parameter, so a declaration of ‘MapAsMatrixWithFirstDimAsRows’ must be available [-fpermissive]
       MapAsMatrixWithFirstDimAsRows(output_data, output_dims);
                                                             ^
In file included from tensorflow/contrib/lite/kernels/arg_min_max.cc:17:0:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:86:5: error: ‘Eigen’ was not declared in this scope
     Eigen::Map<const Eigen::Matrix<typename std::remove_const<Scalar>::type,
     ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:88:57: error: wrong number of template arguments (2, should be 3)
     Eigen::Map<Eigen::Matrix<Scalar, Eigen::Dynamic, 1>>>::type;
                                                         ^
In file included from /usr/include/c++/4.8/bits/move.h:57:0,
                 from /usr/include/c++/4.8/bits/stl_pair.h:59,
                 from /usr/include/c++/4.8/utility:70,
                 from /usr/include/c++/4.8/algorithm:60,
                 from ./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:21,
                 from tensorflow/contrib/lite/kernels/arg_min_max.cc:17:
/usr/include/c++/4.8/type_traits:77:12: error: provided for ‘template<bool <anonymous>, class, class> struct std::conditional’
     struct conditional;
            ^
In file included from tensorflow/contrib/lite/kernels/arg_min_max.cc:17:0:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:88:58: error: expected identifier before ‘::’ token
     Eigen::Map<Eigen::Matrix<Scalar, Eigen::Dynamic, 1>>>::type;
                                                          ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:91:1: error: ‘VectorMap’ does not name a type
 VectorMap<Scalar> MapAsVector(Scalar* data, const RuntimeShape& shape) {
 ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:97:1: error: ‘VectorMap’ does not name a type
 VectorMap<Scalar> MapAsVector(Scalar* data, const Dims<N>& dims) {
 ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:108:5: error: ‘Eigen’ was not declared in this scope
     Eigen::Map<const Eigen::Matrix<typename std::remove_const<Scalar>::type,
     ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:110:70: error: wrong number of template arguments (2, should be 3)
     Eigen::Map<Eigen::Matrix<Scalar, Eigen::Dynamic, Eigen::Dynamic>>>::type;
                                                                      ^
In file included from /usr/include/c++/4.8/bits/move.h:57:0,
                 from /usr/include/c++/4.8/bits/stl_pair.h:59,
                 from /usr/include/c++/4.8/utility:70,
                 from /usr/include/c++/4.8/algorithm:60,
                 from ./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:21,
                 from tensorflow/contrib/lite/kernels/arg_min_max.cc:17:
/usr/include/c++/4.8/type_traits:77:12: error: provided for ‘template<bool <anonymous>, class, class> struct std::conditional’
     struct conditional;
            ^
In file included from tensorflow/contrib/lite/kernels/arg_min_max.cc:17:0:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:110:71: error: expected identifier before ‘::’ token
     Eigen::Map<Eigen::Matrix<Scalar, Eigen::Dynamic, Eigen::Dynamic>>>::type;
                                                                       ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:113:1: error: ‘MatrixMap’ does not name a type
 MatrixMap<Scalar> MapAsMatrixWithLastDimAsRows(Scalar* data,
 ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:122:1: error: ‘MatrixMap’ does not name a type
 MatrixMap<Scalar> MapAsMatrixWithFirstDimAsCols(Scalar* data,
 ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:130:1: error: ‘MatrixMap’ does not name a type
 MatrixMap<Scalar> MapAsMatrixWithFirstDimAsRows(Scalar* data,
 ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:141:1: error: ‘MatrixMap’ does not name a type
 MatrixMap<Scalar> MapAsMatrixWithLastDimAsCols(Scalar* data,
 ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:154:5: error: ‘Eigen’ was not declared in this scope
     Eigen::Map<const Eigen::Array<typename std::remove_const<Scalar>::type,
     ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:156:69: error: wrong number of template arguments (2, should be 3)
     Eigen::Map<Eigen::Array<Scalar, Eigen::Dynamic, Eigen::Dynamic>>>::type;
                                                                     ^
In file included from /usr/include/c++/4.8/bits/move.h:57:0,
                 from /usr/include/c++/4.8/bits/stl_pair.h:59,
                 from /usr/include/c++/4.8/utility:70,
                 from /usr/include/c++/4.8/algorithm:60,
                 from ./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:21,
                 from tensorflow/contrib/lite/kernels/arg_min_max.cc:17:
/usr/include/c++/4.8/type_traits:77:12: error: provided for ‘template<bool <anonymous>, class, class> struct std::conditional’
     struct conditional;
            ^
In file included from tensorflow/contrib/lite/kernels/arg_min_max.cc:17:0:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:156:70: error: expected identifier before ‘::’ token
     Eigen::Map<Eigen::Array<Scalar, Eigen::Dynamic, Eigen::Dynamic>>>::type;
                                                                      ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:159:1: error: ‘ArrayMap’ does not name a type
 ArrayMap<Scalar> MapAsArrayWithFirstDimAsRows(Scalar* data,
 ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:172:1: error: ‘MatrixMap’ does not name a type
 MatrixMap<Scalar> MapAsMatrixWithGivenNumberOfRows(Scalar* data,
 ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::Relu(const float*, const tflite::RuntimeShape&, float*, const tflite::RuntimeShape&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:2369:57: error: ‘MapAsVector’ was not declared in this scope
   const auto input = MapAsVector(input_data, input_shape);
                                                         ^
In file included from tensorflow/contrib/lite/kernels/add.cc:17:0:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::ConvAsGemm(const float*, const tflite::Dims<4>&, const float*, const tflite::Dims<4>&, const float*, const tflite::Dims<4>&, float*, const tflite::Dims<4>&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:2230:59: error: there are no arguments to ‘MapAsMatrixWithFirstDimAsRows’ that depend on a template parameter, so a declaration of ‘MapAsMatrixWithFirstDimAsRows’ must be available [-fpermissive]
       MapAsMatrixWithFirstDimAsRows(input_data, input_dims);
                                                           ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:2230:59: note: (if you use ‘-fpermissive’, G++ will accept your code, but allowing the use of an undeclared name is deprecated)
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:2232:60: error: there are no arguments to ‘MapAsMatrixWithLastDimAsCols’ that depend on a template parameter, so a declaration of ‘MapAsMatrixWithLastDimAsCols’ must be available [-fpermissive]
       MapAsMatrixWithLastDimAsCols(filter_data, filter_dims);
                                                            ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:2234:61: error: there are no arguments to ‘MapAsMatrixWithFirstDimAsRows’ that depend on a template parameter, so a declaration of ‘MapAsMatrixWithFirstDimAsRows’ must be available [-fpermissive]
       MapAsMatrixWithFirstDimAsRows(output_data, output_dims);
                                                             ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:408:17: error: ‘Eigen’ does not name a type
 void Gemm(const Eigen::MatrixBase<Lhs>& lhs, const Eigen::MatrixBase<Rhs>& rhs,
                 ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:408:34: error: expected unqualified-id before ‘<’ token
 void Gemm(const Eigen::MatrixBase<Lhs>& lhs, const Eigen::MatrixBase<Rhs>& rhs,
                                  ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:408:34: error: expected ‘)’ before ‘<’ token
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:408:34: error: expected initializer before ‘<’ token
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::FullyConnected(const float*, const tflite::Dims<4>&, const float*, const tflite::Dims<4>&, const float*, const tflite::Dims<4>&, float, float, float*, const tflite::Dims<4>&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:932:74: error: ‘MapAsMatrixWithGivenNumberOfRows’ was not declared in this scope
       MapAsMatrixWithGivenNumberOfRows(input_data, input_dims, input_rows);
                                                                          ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:934:63: error: ‘MapAsMatrixWithFirstDimAsRows’ was not declared in this scope
       MapAsMatrixWithFirstDimAsRows(weights_data, weights_dims);
                                                               ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:938:75: error: ‘Gemm’ was not declared in this scope
   Gemm(filter_matrix_map.transpose(), input_matrix_map, &output_matrix_map);
                                                                           ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:938:75: note: suggested alternative:
In file included from ./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:31:0,
                 from tensorflow/contrib/lite/kernels/arg_min_max.cc:17:
/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/downloads/gemmlowp/public/gemmlowp.h:74:6: note:   ‘gemmlowp::Gemm’
 void Gemm(GemmContextType* context,
      ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::Relu(const float*, const tflite::RuntimeShape&, float*, const tflite::RuntimeShape&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:2369:57: error: ‘MapAsVector’ was not declared in this scope
   const auto input = MapAsVector(input_data, input_shape);
                                                         ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::Add(const int32*, const tflite::Dims<4>&, const int32*, const tflite::Dims<4>&, int32*, const tflite::Dims<4>&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:2755:57: error: there are no arguments to ‘MapAsVector’ that depend on a template parameter, so a declaration of ‘MapAsVector’ must be available [-fpermissive]
   auto input1_map = MapAsVector(input1_data, input1_dims);
                                                         ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:2756:57: error: there are no arguments to ‘MapAsVector’ that depend on a template parameter, so a declaration of ‘MapAsVector’ must be available [-fpermissive]
   auto input2_map = MapAsVector(input2_data, input2_dims);
                                                         ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:2757:57: error: there are no arguments to ‘MapAsVector’ that depend on a template parameter, so a declaration of ‘MapAsVector’ must be available [-fpermissive]
   auto output_map = MapAsVector(output_data, output_dims);
                                                         ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::Add(const int32*, const tflite::Dims<4>&, const int32*, const tflite::Dims<4>&, int32*, const tflite::Dims<4>&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:2755:57: error: there are no arguments to ‘MapAsVector’ that depend on a template parameter, so a declaration of ‘MapAsVector’ must be available [-fpermissive]
   auto input1_map = MapAsVector(input1_data, input1_dims);
                                                         ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:2756:57: error: there are no arguments to ‘MapAsVector’ that depend on a template parameter, so a declaration of ‘MapAsVector’ must be available [-fpermissive]
   auto input2_map = MapAsVector(input2_data, input2_dims);
                                                         ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:2757:57: error: there are no arguments to ‘MapAsVector’ that depend on a template parameter, so a declaration of ‘MapAsVector’ must be available [-fpermissive]
   auto output_map = MapAsVector(output_data, output_dims);
                                                         ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::Mul(const int32*, const tflite::Dims<4>&, const int32*, const tflite::Dims<4>&, int32*, const tflite::Dims<4>&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3078:57: error: there are no arguments to ‘MapAsVector’ that depend on a template parameter, so a declaration of ‘MapAsVector’ must be available [-fpermissive]
   auto input1_map = MapAsVector(input1_data, input1_dims);
                                                         ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3079:57: error: there are no arguments to ‘MapAsVector’ that depend on a template parameter, so a declaration of ‘MapAsVector’ must be available [-fpermissive]
   auto input2_map = MapAsVector(input2_data, input2_dims);
                                                         ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3080:57: error: there are no arguments to ‘MapAsVector’ that depend on a template parameter, so a declaration of ‘MapAsVector’ must be available [-fpermissive]
   auto output_map = MapAsVector(output_data, output_dims);
                                                         ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::Mul(const int32*, const tflite::Dims<4>&, const int32*, const tflite::Dims<4>&, int32*, const tflite::Dims<4>&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3078:57: error: there are no arguments to ‘MapAsVector’ that depend on a template parameter, so a declaration of ‘MapAsVector’ must be available [-fpermissive]
   auto input1_map = MapAsVector(input1_data, input1_dims);
                                                         ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3079:57: error: there are no arguments to ‘MapAsVector’ that depend on a template parameter, so a declaration of ‘MapAsVector’ must be available [-fpermissive]
   auto input2_map = MapAsVector(input2_data, input2_dims);
                                                         ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3080:57: error: there are no arguments to ‘MapAsVector’ that depend on a template parameter, so a declaration of ‘MapAsVector’ must be available [-fpermissive]
   auto output_map = MapAsVector(output_data, output_dims);
                                                         ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::LstmCell(const float*, const tflite::Dims<4>&, const float*, const tflite::Dims<4>&, const float*, const tflite::Dims<4>&, const float*, const tflite::Dims<4>&, const float*, const tflite::Dims<4>&, float*, const tflite::Dims<4>&, float*, const tflite::Dims<4>&, float*, const tflite::Dims<4>&, float*, const tflite::Dims<4>&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3481:3: error: ‘ArrayMap’ was not declared in this scope
   ArrayMap<float> activ_temp_map =
   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3481:12: error: expected primary-expression before ‘float’
   ArrayMap<float> activ_temp_map =
            ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3481:12: error: expected ‘;’ before ‘float’
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3483:24: error: ‘activ_temp_map’ was not declared in this scope
   auto input_gate_sm = activ_temp_map.block(0 * output_depth, 0, output_depth,
                        ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3491:12: error: expected primary-expression before ‘const’
   ArrayMap<const float> prev_state_map =
            ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3491:12: error: expected ‘;’ before ‘const’
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::LstmCell(const float*, const tflite::Dims<4>&, const float*, const tflite::Dims<4>&, const float*, const tflite::Dims<4>&, const float*, const tflite::Dims<4>&, const float*, const tflite::Dims<4>&, float*, const tflite::Dims<4>&, float*, const tflite::Dims<4>&, float*, const tflite::Dims<4>&, float*, const tflite::Dims<4>&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3481:3: error: ‘ArrayMap’ was not declared in this scope
   ArrayMap<float> activ_temp_map =
   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3493:12: error: expected primary-expression before ‘float’
   ArrayMap<float> output_state_map =
            ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3493:12: error: expected ‘;’ before ‘float’
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3481:12: error: expected primary-expression before ‘float’
   ArrayMap<float> activ_temp_map =
            ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3481:12: error: expected ‘;’ before ‘float’
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3483:24: error: ‘activ_temp_map’ was not declared in this scope
   auto input_gate_sm = activ_temp_map.block(0 * output_depth, 0, output_depth,
                        ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3495:12: error: expected primary-expression before ‘float’
   ArrayMap<float> output_activ_map =
            ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3495:12: error: expected ‘;’ before ‘float’
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3491:12: error: expected primary-expression before ‘const’
   ArrayMap<const float> prev_state_map =
            ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3491:12: error: expected ‘;’ before ‘const’
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3500:3: error: ‘output_state_map’ was not declared in this scope
   output_state_map =
   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3493:12: error: expected primary-expression before ‘float’
   ArrayMap<float> output_state_map =
            ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3493:12: error: expected ‘;’ before ‘float’
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3495:12: error: expected primary-expression before ‘float’
   ArrayMap<float> output_activ_map =
            ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3495:12: error: expected ‘;’ before ‘float’
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3501:31: error: ‘Eigen’ has not been declared
       input_gate_sm.unaryExpr(Eigen::internal::scalar_sigmoid_op<float>()) *
                               ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3500:3: error: ‘output_state_map’ was not declared in this scope
   output_state_map =
   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3501:66: error: expected primary-expression before ‘float’
       input_gate_sm.unaryExpr(Eigen::internal::scalar_sigmoid_op<float>()) *
                                                                  ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3501:31: error: ‘Eigen’ has not been declared
       input_gate_sm.unaryExpr(Eigen::internal::scalar_sigmoid_op<float>()) *
                               ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3501:66: error: expected primary-expression before ‘float’
       input_gate_sm.unaryExpr(Eigen::internal::scalar_sigmoid_op<float>()) *
                                                                  ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3503:32: error: ‘Eigen’ has not been declared
       forget_gate_sm.unaryExpr(Eigen::internal::scalar_sigmoid_op<float>()) *
                                ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3503:32: error: ‘Eigen’ has not been declared
       forget_gate_sm.unaryExpr(Eigen::internal::scalar_sigmoid_op<float>()) *
                                ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3503:67: error: expected primary-expression before ‘float’
       forget_gate_sm.unaryExpr(Eigen::internal::scalar_sigmoid_op<float>()) *
                                                                   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3503:67: error: expected primary-expression before ‘float’
       forget_gate_sm.unaryExpr(Eigen::internal::scalar_sigmoid_op<float>()) *
                                                                   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3504:11: error: ‘prev_state_map’ was not declared in this scope
           prev_state_map;
           ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3504:11: error: ‘prev_state_map’ was not declared in this scope
           prev_state_map;
           ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3505:3: error: ‘output_activ_map’ was not declared in this scope
   output_activ_map =
   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3505:3: error: ‘output_activ_map’ was not declared in this scope
   output_activ_map =
   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3506:32: error: ‘Eigen’ has not been declared
       output_gate_sm.unaryExpr(Eigen::internal::scalar_sigmoid_op<float>()) *
                                ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3506:32: error: ‘Eigen’ has not been declared
       output_gate_sm.unaryExpr(Eigen::internal::scalar_sigmoid_op<float>()) *
                                ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3506:67: error: expected primary-expression before ‘float’
       output_gate_sm.unaryExpr(Eigen::internal::scalar_sigmoid_op<float>()) *
                                                                   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3506:67: error: expected primary-expression before ‘float’
       output_gate_sm.unaryExpr(Eigen::internal::scalar_sigmoid_op<float>()) *
                                                                   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::AveragePool(const tflite::PoolParams&, const tflite::RuntimeShape&, const float*, const tflite::RuntimeShape&, float*)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3805:75: error: ‘MapAsMatrixWithLastDimAsRows’ was not declared in this scope
   const auto in_mat = MapAsMatrixWithLastDimAsRows(input_data, input_shape);
                                                                           ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::AveragePool(const tflite::PoolParams&, const tflite::RuntimeShape&, const float*, const tflite::RuntimeShape&, float*)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3805:75: error: ‘MapAsMatrixWithLastDimAsRows’ was not declared in this scope
   const auto in_mat = MapAsMatrixWithLastDimAsRows(input_data, input_shape);
                                                                           ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3808:3: error: ‘Eigen’ has not been declared
   Eigen::VectorXf out_count(out_mat.cols());
   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3808:3: error: ‘Eigen’ has not been declared
   Eigen::VectorXf out_count(out_mat.cols());
   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3808:19: error: expected ‘;’ before ‘out_count’
   Eigen::VectorXf out_count(out_mat.cols());
                   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3808:19: error: expected ‘;’ before ‘out_count’
   Eigen::VectorXf out_count(out_mat.cols());
                   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3809:3: error: ‘out_count’ was not declared in this scope
   out_count.setZero();
   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3809:3: error: ‘out_count’ was not declared in this scope
   out_count.setZero();
   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::MaxPool(const tflite::PoolParams&, const tflite::RuntimeShape&, const float*, const tflite::RuntimeShape&, float*)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3980:75: error: ‘MapAsMatrixWithLastDimAsRows’ was not declared in this scope
   const auto in_mat = MapAsMatrixWithLastDimAsRows(input_data, input_shape);
                                                                           ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::MaxPool(const tflite::PoolParams&, const tflite::RuntimeShape&, const float*, const tflite::RuntimeShape&, float*)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3980:75: error: ‘MapAsMatrixWithLastDimAsRows’ was not declared in this scope
   const auto in_mat = MapAsMatrixWithLastDimAsRows(input_data, input_shape);
                                                                           ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::L2Pool(const tflite::PoolParams&, const tflite::RuntimeShape&, const float*, const tflite::RuntimeShape&, float*)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4128:75: error: ‘MapAsMatrixWithLastDimAsRows’ was not declared in this scope
   const auto in_mat = MapAsMatrixWithLastDimAsRows(input_data, input_shape);
                                                                           ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::L2Pool(const tflite::PoolParams&, const tflite::RuntimeShape&, const float*, const tflite::RuntimeShape&, float*)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4128:75: error: ‘MapAsMatrixWithLastDimAsRows’ was not declared in this scope
   const auto in_mat = MapAsMatrixWithLastDimAsRows(input_data, input_shape);
                                                                           ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4130:3: error: ‘Eigen’ has not been declared
   Eigen::VectorXf in_square(in_mat.rows());
   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4130:3: error: ‘Eigen’ has not been declared
   Eigen::VectorXf in_square(in_mat.rows());
   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4130:19: error: expected ‘;’ before ‘in_square’
   Eigen::VectorXf in_square(in_mat.rows());
                   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4130:19: error: expected ‘;’ before ‘in_square’
   Eigen::VectorXf in_square(in_mat.rows());
                   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4131:3: error: ‘Eigen’ has not been declared
   Eigen::VectorXf out_count(out_mat.cols());
   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4131:3: error: ‘Eigen’ has not been declared
   Eigen::VectorXf out_count(out_mat.cols());
   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4131:19: error: expected ‘;’ before ‘out_count’
   Eigen::VectorXf out_count(out_mat.cols());
                   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4131:19: error: expected ‘;’ before ‘out_count’
   Eigen::VectorXf out_count(out_mat.cols());
                   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4132:3: error: ‘out_count’ was not declared in this scope
   out_count.setZero();
   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4132:3: error: ‘out_count’ was not declared in this scope
   out_count.setZero();
   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4154:9: error: ‘in_square’ was not declared in this scope
         in_square =
         ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4154:9: error: ‘in_square’ was not declared in this scope
         in_square =
         ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::LocalResponseNormalization(const float*, const tflite::Dims<4>&, int, float, float, float, float*, const tflite::Dims<4>&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4188:76: error: ‘MapAsMatrixWithFirstDimAsRows’ was not declared in this scope
   const auto data_in = MapAsMatrixWithFirstDimAsRows(input_data, input_dims);
                                                                            ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::LocalResponseNormalization(const float*, const tflite::Dims<4>&, int, float, float, float, float*, const tflite::Dims<4>&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4188:76: error: ‘MapAsMatrixWithFirstDimAsRows’ was not declared in this scope
   const auto data_in = MapAsMatrixWithFirstDimAsRows(input_data, input_dims);
                                                                            ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4196:3: error: ‘Eigen’ has not been declared
   Eigen::VectorXf padded_square(data_in.rows() + double_range);
   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4196:3: error: ‘Eigen’ has not been declared
   Eigen::VectorXf padded_square(data_in.rows() + double_range);
   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4196:19: error: expected ‘;’ before ‘padded_square’
   Eigen::VectorXf padded_square(data_in.rows() + double_range);
                   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4196:19: error: expected ‘;’ before ‘padded_square’
   Eigen::VectorXf padded_square(data_in.rows() + double_range);
                   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4197:3: error: ‘padded_square’ was not declared in this scope
   padded_square.setZero();
   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4197:3: error: ‘padded_square’ was not declared in this scope
   padded_square.setZero();
   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::Softmax(const float*, const tflite::RuntimeShape&, float, float*, const tflite::RuntimeShape&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4231:75: error: ‘MapAsMatrixWithLastDimAsRows’ was not declared in this scope
   const auto in_mat = MapAsMatrixWithLastDimAsRows(input_data, input_shape);
                                                                           ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::Softmax(const float*, const tflite::RuntimeShape&, float, float*, const tflite::RuntimeShape&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4231:75: error: ‘MapAsMatrixWithLastDimAsRows’ was not declared in this scope
   const auto in_mat = MapAsMatrixWithLastDimAsRows(input_data, input_shape);
                                                                           ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4239:3: error: ‘Eigen’ has not been declared
   Eigen::Array<float, 1, Eigen::Dynamic> scale =
   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4239:3: error: ‘Eigen’ has not been declared
   Eigen::Array<float, 1, Eigen::Dynamic> scale =
   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4239:16: error: expected primary-expression before ‘float’
   Eigen::Array<float, 1, Eigen::Dynamic> scale =
                ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4239:16: error: expected ‘;’ before ‘float’
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4239:16: error: expected primary-expression before ‘float’
   Eigen::Array<float, 1, Eigen::Dynamic> scale =
                ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4239:16: error: expected ‘;’ before ‘float’
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4241:32: error: ‘scale’ was not declared in this scope
   out_mat.array().rowwise() *= scale;
                                ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4241:32: error: ‘scale’ was not declared in this scope
   out_mat.array().rowwise() *= scale;
                                ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::Logistic(const float*, const tflite::RuntimeShape&, float*, const tflite::RuntimeShape&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4693:55: error: ‘MapAsVector’ was not declared in this scope
   auto input_map = MapAsVector(input_data, input_shape);
                                                       ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::Logistic(const float*, const tflite::RuntimeShape&, float*, const tflite::RuntimeShape&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4693:55: error: ‘MapAsVector’ was not declared in this scope
   auto input_map = MapAsVector(input_data, input_shape);
                                                       ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4696:35: error: ‘Eigen’ has not been declared
       input_map.array().unaryExpr(Eigen::internal::scalar_sigmoid_op<float>());
                                   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4696:35: error: ‘Eigen’ has not been declared
       input_map.array().unaryExpr(Eigen::internal::scalar_sigmoid_op<float>());
                                   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4696:70: error: expected primary-expression before ‘float’
       input_map.array().unaryExpr(Eigen::internal::scalar_sigmoid_op<float>());
                                                                      ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4696:70: error: expected primary-expression before ‘float’
       input_map.array().unaryExpr(Eigen::internal::scalar_sigmoid_op<float>());
                                                                      ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::Tanh(const float*, const tflite::RuntimeShape&, float*, const tflite::RuntimeShape&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4899:55: error: ‘MapAsVector’ was not declared in this scope
   auto input_map = MapAsVector(input_data, input_shape);
                                                       ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::Tanh(const float*, const tflite::RuntimeShape&, float*, const tflite::RuntimeShape&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4899:55: error: ‘MapAsVector’ was not declared in this scope
   auto input_map = MapAsVector(input_data, input_shape);
                                                       ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::Floor(const float*, const tflite::Dims<4>&, float*, const tflite::Dims<4>&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:5163:54: error: ‘MapAsVector’ was not declared in this scope
   auto input_map = MapAsVector(input_data, input_dims);
                                                      ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::Floor(const float*, const tflite::Dims<4>&, float*, const tflite::Dims<4>&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:5163:54: error: ‘MapAsVector’ was not declared in this scope
   auto input_map = MapAsVector(input_data, input_dims);
                                                      ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:5165:24: error: ‘Eigen’ has not been declared
   output_map.array() = Eigen::floor(input_map.array());
                        ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:5165:24: error: ‘Eigen’ has not been declared
   output_map.array() = Eigen::floor(input_map.array());
                        ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::TransposeConv(const float*, const tflite::Dims<4>&, const float*, const tflite::Dims<4>&, int, int, int, int, float*, const tflite::Dims<4>&, float*, const tflite::Dims<4>&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:6047:61: error: ‘MapAsMatrixWithFirstDimAsRows’ was not declared in this scope
       MapAsMatrixWithFirstDimAsRows(im2col_data, im2col_dims);
                                                             ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::TransposeConv(const float*, const tflite::Dims<4>&, const float*, const tflite::Dims<4>&, int, int, int, int, float*, const tflite::Dims<4>&, float*, const tflite::Dims<4>&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:6047:61: error: ‘MapAsMatrixWithFirstDimAsRows’ was not declared in this scope
       MapAsMatrixWithFirstDimAsRows(im2col_data, im2col_dims);
                                                             ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:6049:60: error: ‘MapAsMatrixWithLastDimAsCols’ was not declared in this scope
       MapAsMatrixWithLastDimAsCols(filter_data, filter_dims);
                                                            ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:6049:60: error: ‘MapAsMatrixWithLastDimAsCols’ was not declared in this scope
       MapAsMatrixWithLastDimAsCols(filter_data, filter_dims);
                                                            ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:6053:76: error: ‘Gemm’ was not declared in this scope
   Gemm(filter_matrix_map.transpose(), im2col_matrix_map, &output_matrix_map);
                                                                            ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:6053:76: note: suggested alternative:
In file included from ./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:31:0,
                 from tensorflow/contrib/lite/kernels/activations.cc:25:
/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/downloads/gemmlowp/public/gemmlowp.h:74:6: note:   ‘gemmlowp::Gemm’
 void Gemm(GemmContextType* context,
      ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:6053:76: error: ‘Gemm’ was not declared in this scope
   Gemm(filter_matrix_map.transpose(), im2col_matrix_map, &output_matrix_map);
                                                                            ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:6053:76: note: suggested alternative:
In file included from ./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:31:0,
                 from tensorflow/contrib/lite/kernels/add.cc:17:
/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/downloads/gemmlowp/public/gemmlowp.h:74:6: note:   ‘gemmlowp::Gemm’
 void Gemm(GemmContextType* context,
      ^
In file included from tensorflow/contrib/lite/kernels/arg_min_max.cc:17:0:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::Conv(const float*, const tflite::Dims<4>&, const float*, const tflite::Dims<4>&, const float*, const tflite::Dims<4>&, int, int, int, int, int, int, float, float, float*, const tflite::Dims<4>&, float*, const tflite::Dims<4>&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:1990:70: error: ‘MapAsMatrixWithFirstDimAsRows’ was not declared in this scope
       MapAsMatrixWithFirstDimAsRows(gemm_input_data, *gemm_input_dims);
                                                                      ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:1992:60: error: ‘MapAsMatrixWithLastDimAsCols’ was not declared in this scope
       MapAsMatrixWithLastDimAsCols(filter_data, filter_dims);
                                                            ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:1996:76: error: ‘Gemm’ was not declared in this scope
   Gemm(filter_matrix_map.transpose(), im2col_matrix_map, &output_matrix_map);
                                                                            ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:1996:76: note: suggested alternative:
In file included from ./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:31:0,
                 from tensorflow/contrib/lite/kernels/arg_min_max.cc:17:
/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/downloads/gemmlowp/public/gemmlowp.h:74:6: note:   ‘gemmlowp::Gemm’
 void Gemm(GemmContextType* context,
      ^
In file included from tensorflow/contrib/lite/kernels/arg_min_max.cc:17:0:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::ConvAsGemm(const float*, const tflite::Dims<4>&, const float*, const tflite::Dims<4>&, const float*, const tflite::Dims<4>&, float*, const tflite::Dims<4>&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:2230:59: error: there are no arguments to ‘MapAsMatrixWithFirstDimAsRows’ that depend on a template parameter, so a declaration of ‘MapAsMatrixWithFirstDimAsRows’ must be available [-fpermissive]
       MapAsMatrixWithFirstDimAsRows(input_data, input_dims);
                                                           ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:2230:59: note: (if you use ‘-fpermissive’, G++ will accept your code, but allowing the use of an undeclared name is deprecated)
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:2232:60: error: there are no arguments to ‘MapAsMatrixWithLastDimAsCols’ that depend on a template parameter, so a declaration of ‘MapAsMatrixWithLastDimAsCols’ must be available [-fpermissive]
       MapAsMatrixWithLastDimAsCols(filter_data, filter_dims);
                                                            ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:2234:61: error: there are no arguments to ‘MapAsMatrixWithFirstDimAsRows’ that depend on a template parameter, so a declaration of ‘MapAsMatrixWithFirstDimAsRows’ must be available [-fpermissive]
       MapAsMatrixWithFirstDimAsRows(output_data, output_dims);
                                                             ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::Relu(const float*, const tflite::RuntimeShape&, float*, const tflite::RuntimeShape&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:2369:57: error: ‘MapAsVector’ was not declared in this scope
   const auto input = MapAsVector(input_data, input_shape);
                                                         ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::Add(const int32*, const tflite::Dims<4>&, const int32*, const tflite::Dims<4>&, int32*, const tflite::Dims<4>&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:2755:57: error: there are no arguments to ‘MapAsVector’ that depend on a template parameter, so a declaration of ‘MapAsVector’ must be available [-fpermissive]
   auto input1_map = MapAsVector(input1_data, input1_dims);
                                                         ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:2756:57: error: there are no arguments to ‘MapAsVector’ that depend on a template parameter, so a declaration of ‘MapAsVector’ must be available [-fpermissive]
   auto input2_map = MapAsVector(input2_data, input2_dims);
                                                         ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:2757:57: error: there are no arguments to ‘MapAsVector’ that depend on a template parameter, so a declaration of ‘MapAsVector’ must be available [-fpermissive]
   auto output_map = MapAsVector(output_data, output_dims);
                                                         ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::Mul(const int32*, const tflite::Dims<4>&, const int32*, const tflite::Dims<4>&, int32*, const tflite::Dims<4>&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3078:57: error: there are no arguments to ‘MapAsVector’ that depend on a template parameter, so a declaration of ‘MapAsVector’ must be available [-fpermissive]
   auto input1_map = MapAsVector(input1_data, input1_dims);
                                                         ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3079:57: error: there are no arguments to ‘MapAsVector’ that depend on a template parameter, so a declaration of ‘MapAsVector’ must be available [-fpermissive]
   auto input2_map = MapAsVector(input2_data, input2_dims);
                                                         ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3080:57: error: there are no arguments to ‘MapAsVector’ that depend on a template parameter, so a declaration of ‘MapAsVector’ must be available [-fpermissive]
   auto output_map = MapAsVector(output_data, output_dims);
                                                         ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::LstmCell(const float*, const tflite::Dims<4>&, const float*, const tflite::Dims<4>&, const float*, const tflite::Dims<4>&, const float*, const tflite::Dims<4>&, const float*, const tflite::Dims<4>&, float*, const tflite::Dims<4>&, float*, const tflite::Dims<4>&, float*, const tflite::Dims<4>&, float*, const tflite::Dims<4>&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3481:3: error: ‘ArrayMap’ was not declared in this scope
   ArrayMap<float> activ_temp_map =
   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3481:12: error: expected primary-expression before ‘float’
   ArrayMap<float> activ_temp_map =
            ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3481:12: error: expected ‘;’ before ‘float’
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3483:24: error: ‘activ_temp_map’ was not declared in this scope
   auto input_gate_sm = activ_temp_map.block(0 * output_depth, 0, output_depth,
                        ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3491:12: error: expected primary-expression before ‘const’
   ArrayMap<const float> prev_state_map =
            ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3491:12: error: expected ‘;’ before ‘const’
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3493:12: error: expected primary-expression before ‘float’
   ArrayMap<float> output_state_map =
            ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3493:12: error: expected ‘;’ before ‘float’
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3495:12: error: expected primary-expression before ‘float’
   ArrayMap<float> output_activ_map =
            ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3495:12: error: expected ‘;’ before ‘float’
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3500:3: error: ‘output_state_map’ was not declared in this scope
   output_state_map =
   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3501:31: error: ‘Eigen’ has not been declared
       input_gate_sm.unaryExpr(Eigen::internal::scalar_sigmoid_op<float>()) *
                               ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3501:66: error: expected primary-expression before ‘float’
       input_gate_sm.unaryExpr(Eigen::internal::scalar_sigmoid_op<float>()) *
                                                                  ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3503:32: error: ‘Eigen’ has not been declared
       forget_gate_sm.unaryExpr(Eigen::internal::scalar_sigmoid_op<float>()) *
                                ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3503:67: error: expected primary-expression before ‘float’
       forget_gate_sm.unaryExpr(Eigen::internal::scalar_sigmoid_op<float>()) *
                                                                   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3504:11: error: ‘prev_state_map’ was not declared in this scope
           prev_state_map;
           ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3505:3: error: ‘output_activ_map’ was not declared in this scope
   output_activ_map =
   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3506:32: error: ‘Eigen’ has not been declared
       output_gate_sm.unaryExpr(Eigen::internal::scalar_sigmoid_op<float>()) *
                                ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3506:67: error: expected primary-expression before ‘float’
       output_gate_sm.unaryExpr(Eigen::internal::scalar_sigmoid_op<float>()) *
                                                                   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::AveragePool(const tflite::PoolParams&, const tflite::RuntimeShape&, const float*, const tflite::RuntimeShape&, float*)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3805:75: error: ‘MapAsMatrixWithLastDimAsRows’ was not declared in this scope
   const auto in_mat = MapAsMatrixWithLastDimAsRows(input_data, input_shape);
                                                                           ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3808:3: error: ‘Eigen’ has not been declared
   Eigen::VectorXf out_count(out_mat.cols());
   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3808:19: error: expected ‘;’ before ‘out_count’
   Eigen::VectorXf out_count(out_mat.cols());
                   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3809:3: error: ‘out_count’ was not declared in this scope
   out_count.setZero();
   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::MaxPool(const tflite::PoolParams&, const tflite::RuntimeShape&, const float*, const tflite::RuntimeShape&, float*)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:3980:75: error: ‘MapAsMatrixWithLastDimAsRows’ was not declared in this scope
   const auto in_mat = MapAsMatrixWithLastDimAsRows(input_data, input_shape);
                                                                           ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::L2Pool(const tflite::PoolParams&, const tflite::RuntimeShape&, const float*, const tflite::RuntimeShape&, float*)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4128:75: error: ‘MapAsMatrixWithLastDimAsRows’ was not declared in this scope
   const auto in_mat = MapAsMatrixWithLastDimAsRows(input_data, input_shape);
                                                                           ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4130:3: error: ‘Eigen’ has not been declared
   Eigen::VectorXf in_square(in_mat.rows());
   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4130:19: error: expected ‘;’ before ‘in_square’
   Eigen::VectorXf in_square(in_mat.rows());
                   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4131:3: error: ‘Eigen’ has not been declared
   Eigen::VectorXf out_count(out_mat.cols());
   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4131:19: error: expected ‘;’ before ‘out_count’
   Eigen::VectorXf out_count(out_mat.cols());
                   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4132:3: error: ‘out_count’ was not declared in this scope
   out_count.setZero();
   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4154:9: error: ‘in_square’ was not declared in this scope
         in_square =
         ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::LocalResponseNormalization(const float*, const tflite::Dims<4>&, int, float, float, float, float*, const tflite::Dims<4>&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4188:76: error: ‘MapAsMatrixWithFirstDimAsRows’ was not declared in this scope
   const auto data_in = MapAsMatrixWithFirstDimAsRows(input_data, input_dims);
                                                                            ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4196:3: error: ‘Eigen’ has not been declared
   Eigen::VectorXf padded_square(data_in.rows() + double_range);
   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4196:19: error: expected ‘;’ before ‘padded_square’
   Eigen::VectorXf padded_square(data_in.rows() + double_range);
                   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4197:3: error: ‘padded_square’ was not declared in this scope
   padded_square.setZero();
   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::Softmax(const float*, const tflite::RuntimeShape&, float, float*, const tflite::RuntimeShape&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4231:75: error: ‘MapAsMatrixWithLastDimAsRows’ was not declared in this scope
   const auto in_mat = MapAsMatrixWithLastDimAsRows(input_data, input_shape);
                                                                           ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4239:3: error: ‘Eigen’ has not been declared
   Eigen::Array<float, 1, Eigen::Dynamic> scale =
   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4239:16: error: expected primary-expression before ‘float’
   Eigen::Array<float, 1, Eigen::Dynamic> scale =
                ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4239:16: error: expected ‘;’ before ‘float’
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4241:32: error: ‘scale’ was not declared in this scope
   out_mat.array().rowwise() *= scale;
                                ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::Logistic(const float*, const tflite::RuntimeShape&, float*, const tflite::RuntimeShape&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4693:55: error: ‘MapAsVector’ was not declared in this scope
   auto input_map = MapAsVector(input_data, input_shape);
                                                       ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4696:35: error: ‘Eigen’ has not been declared
       input_map.array().unaryExpr(Eigen::internal::scalar_sigmoid_op<float>());
                                   ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4696:70: error: expected primary-expression before ‘float’
       input_map.array().unaryExpr(Eigen::internal::scalar_sigmoid_op<float>());
                                                                      ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::Tanh(const float*, const tflite::RuntimeShape&, float*, const tflite::RuntimeShape&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4899:55: error: ‘MapAsVector’ was not declared in this scope
   auto input_map = MapAsVector(input_data, input_shape);
                                                       ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::Floor(const float*, const tflite::Dims<4>&, float*, const tflite::Dims<4>&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:5163:54: error: ‘MapAsVector’ was not declared in this scope
   auto input_map = MapAsVector(input_data, input_dims);
                                                      ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:5165:24: error: ‘Eigen’ has not been declared
   output_map.array() = Eigen::floor(input_map.array());
                        ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function ‘void tflite::optimized_ops::TransposeConv(const float*, const tflite::Dims<4>&, const float*, const tflite::Dims<4>&, int, int, int, int, float*, const tflite::Dims<4>&, float*, const tflite::Dims<4>&)’:
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:6047:61: error: ‘MapAsMatrixWithFirstDimAsRows’ was not declared in this scope
       MapAsMatrixWithFirstDimAsRows(im2col_data, im2col_dims);
                                                             ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:6049:60: error: ‘MapAsMatrixWithLastDimAsCols’ was not declared in this scope
       MapAsMatrixWithLastDimAsCols(filter_data, filter_dims);
                                                            ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:6053:76: error: ‘Gemm’ was not declared in this scope
   Gemm(filter_matrix_map.transpose(), im2col_matrix_map, &output_matrix_map);
                                                                            ^
./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:6053:76: note: suggested alternative:
In file included from ./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:31:0,
                 from tensorflow/contrib/lite/kernels/arg_min_max.cc:17:
/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/downloads/gemmlowp/public/gemmlowp.h:74:6: note:   ‘gemmlowp::Gemm’
 void Gemm(GemmContextType* context,
      ^
tensorflow/contrib/lite/Makefile:221: recipe for target '/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/gen/obj/imx6_armv7-a/tensorflow/contrib/lite/kernels/add.o' failed
make: *** [/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/gen/obj/imx6_armv7-a/tensorflow/contrib/lite/kernels/add.o] Error 1
make: *** Waiting for unfinished jobs....
tensorflow/contrib/lite/Makefile:221: recipe for target '/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/gen/obj/imx6_armv7-a/tensorflow/contrib/lite/kernels/activations.o' failed
make: *** [/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/gen/obj/imx6_armv7-a/tensorflow/contrib/lite/kernels/activations.o] Error 1
tensorflow/contrib/lite/Makefile:221: recipe for target '/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/gen/obj/imx6_armv7-a/tensorflow/contrib/lite/kernels/arg_min_max.o' failed
make: *** [/home/lyra/tf2arm/tfr1.10/tensorflow/contrib/lite/gen/obj/imx6_armv7-a/tensorflow/contrib/lite/kernels/arg_min_max.o] Error 1

Anyone has any idea how to deal with it? 

"
33369,Dose the data division for multi-gpu destruct the accuracy?,"Data division for multi-gpu after tf1.11 seems to have the same shape of data across all gpus, which will result in a bigger variance of loss between different steps. For example, for the transformer model, when in one step, all the gpus are feeded with the data with length near 8, but in the next step, all the gpus are feeded with the data with length near 32, which will result in a bigger variance of loss.

In practice, I got a lower accuracy trained with tf2.0 than tf1.11.

@guptapriya @yuefengz @zongweiz @sgpyc"
33365,No float64 support with batch normalization in Tensorflow 2.0?,"Stock Ubuntu 19.04 with Cuda 10.0, Tensorflow 2.0.0 installed via pip3, Python 3.7.3, GTX1060.

I have a float64 valued dataset with a simple conv2d network that includes tf.keras.layers.BatchNormalization() which is where the error is being thrown I think.

The first set of issues:
```
WARNING:tensorflow:Layer conv2d is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. 
```
After setting tf.keras.backend.set_floatx('float64'), next set of errors:
```
Traceback (most recent call last):
  File ""/home/aj/ga.py"", line 183, in <module>
    encoder = make_encoder_model(z_dim)
  File ""/home/aj/ga.py"", line 138, in make_encoder_model
    x = tf.keras.layers.BatchNormalization()(x)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 842, in __call__
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/layers/normalization.py"", line 659, in call
    outputs = self._fused_batch_norm(inputs, training=training)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/layers/normalization.py"", line 517, in _fused_batch_norm
    training, _fused_batch_norm_training, _fused_batch_norm_inference)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/utils/tf_utils.py"", line 59, in smart_cond
    pred, true_fn=true_fn, false_fn=false_fn, name=name)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/smart_cond.py"", line 59, in smart_cond
    name=name)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/control_flow_ops.py"", line 1174, in cond
    return cond_v2.cond_v2(pred, true_fn, false_fn, name)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/cond_v2.py"", line 84, in cond_v2
    op_return_value=pred)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/func_graph.py"", line 915, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/layers/normalization.py"", line 503, in _fused_batch_norm_training
    data_format=self._data_format)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/nn_impl.py"", line 1509, in fused_batch_norm
    name=name)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/gen_nn_ops.py"", line 4620, in fused_batch_norm_v3
    name=name)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/op_def_library.py"", line 631, in _apply_op_helper
    param_name=input_name)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/op_def_library.py"", line 60, in _SatisfiesTypeConstraint
    "", "".join(dtypes.as_dtype(x).name for x in allowed_list)))
TypeError: Value passed to parameter 'x' has DataType float64 not in list of allowed values: float16, bfloat16, float32
```
So is there perhaps another (hopefully drop in) method of batch normalization that supports float64? I don't want to go hacking at allowed_list and all that."
33363,An unbelievable error in TensorFlow2.0 tutorials/load_data/images !!!!!!!!!!!!!!!,"when i run the [tutorial](https://tensorflow.google.cn/tutorials/load_data/images?hl=en) in tensorflow2.0 , it run error in 
``` python
for image, label in labeled_ds.take(1):
    print(""Image shape: "", image.numpy().shape)
    print(""Label: "", label.numpy())
```
the information is:
```python
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-28-8c161867f891> in <module>
----> 1 for image, label in labeled_ds.take(1):
      2     print(""Image shape: "", image.numpy().shape)
      3     print(""Label: "", label.numpy())

c:\users\30660\anaconda3\envs\pytorch\lib\site-packages\tensorflow_core\python\data\ops\iterator_ops.py in __next__(self)
    620 
    621   def __next__(self):  # For Python 3 compatibility
--> 622     return self.next()
    623 
    624   def _next_internal(self):

c:\users\30660\anaconda3\envs\pytorch\lib\site-packages\tensorflow_core\python\data\ops\iterator_ops.py in next(self)
    664     """"""Returns a nested structure of `Tensor`s containing the next element.""""""
    665     try:
--> 666       return self._next_internal()
    667     except errors.OutOfRangeError:
    668       raise StopIteration

c:\users\30660\anaconda3\envs\pytorch\lib\site-packages\tensorflow_core\python\data\ops\iterator_ops.py in _next_internal(self)
    649             self._iterator_resource,
    650             output_types=self._flat_output_types,
--> 651             output_shapes=self._flat_output_shapes)
    652 
    653       try:

c:\users\30660\anaconda3\envs\pytorch\lib\site-packages\tensorflow_core\python\ops\gen_dataset_ops.py in iterator_get_next_sync(iterator, output_types, output_shapes, name)
   2671       else:
   2672         message = e.message
-> 2673       _six.raise_from(_core._status_to_exception(e.code, message), None)
   2674   # Add nodes to the TensorFlow graph.
   2675   if not isinstance(output_types, (list, tuple)):

c:\users\30660\anaconda3\envs\pytorch\lib\site-packages\six.py in raise_from(value, from_value)

InvalidArgumentError: {{function_node __inference_Dataset_map_process_path_205}} slice index -1 of dimension 0 out of bounds.
	 [[{{node strided_slice}}]] [Op:IteratorGetNextSync]
```

And i don`t know why?
I try download form the [github web](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/load_data/images.ipynb)

It also run error on the same place, i don't know why and i don't know how to fix it.

I want to Submit the issue on the doce, but it can't submit issue, so i come here.

Does the function changed?

Anybody can help me, i am so sad (>_<)..........."
33361,INFO level logs not coming in MirroredStrategy(),"Hi. I am on TensorFlow 2.0 and on my local Jupyter Notebook environment. I am following [this example](https://www.tensorflow.org/tutorials/distribute/keras). When I am running the accompanying notebook Colab, the INFO level logs are getting displayed. But when I am running local, the logs are not there (although locally I am running a slightly different code) and here it is:

```python
import tensorflow as tf

strategy = tf.distribute.MirroredStrategy()

# Call the distribution scope context manager
with strategy.scope():
    # Define a model to fit the above data
    model = tf.keras.Sequential([
        tf.keras.layers.Dropout(rate=0.2, input_shape=X.shape[1:]),
        tf.keras.layers.Dense(units=64, activation='relu'),
        tf.keras.layers.Dropout(rate=0.2),
        tf.keras.layers.Dense(units=1, activation='sigmoid')
    ])
    
    # Compile the model
    model.compile(loss='binary_crossentropy',
                optimizer='adam',
                metrics=['accuracy'])
```

Any hints as to why the logs are not coming? "
33360,TensorFlow source cannot be compiled with -c dbg,"When compiling the TensorFlow sources (at 776b99925c1fe0d045b363f7437c21a5a9ce2357) with:
bazel build -c dbg -j 24 --config=opt //tensorflow/tools/pip_package:build_pip_package

it produces many errors like:
```
external/arm_neon_2_x86_sse/NEON_2_SSE.h:2296:30: error: selector must be an integer constant in the range 0..7
     #define _MM_INSERT_EPI16 _mm_insert_epi16
                              ^
external/arm_neon_2_x86_sse/NEON_2_SSE.h:9369:40: note: in expansion of macro '_MM_INSERT_EPI16'
 #define vld1q_lane_s16(ptr, vec, lane) _MM_INSERT_EPI16(vec, *(ptr), lane)
                                        ^~~~~~~~~~~~~~~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:12090:12: note: in expansion of macro 'vld1q_lane_s16'
     return vld1q_lane_s16(&val, vec,  lane);
```
This started when the TF build moved to C++14 compilation. It works fine with C++11.
I distilled it down to the following piece of code:
```
typedef short __v8hi __attribute__ ((__vector_size__ (16)));
extern inline __attribute__((__gnu_inline__, __always_inline__, __artificial__))
__v8hi vsetq_lane_s16(__v8hi x,const int l)
{ return __builtin_ia32_vec_set_v8hi(x,0,(l)); }
void ff(__v8hi x) { vsetq_lane_s16(x,0);}```

When compiling this with
g++ -std=c++11 x.cc -c
it compiles just fine. When compiling with
g++ -std=c++14 x.cc -c
it produces:
x.cc: In function ‘void ff(__v8hi)’:
x.cc:6:47: error: selector must be an integer constant in the range 0..7
     return __builtin_ia32_vec_set_v8hi(x,0,(l));
```

some details:
- removing the ""const"" from the formal argument makes C++11 fail as well
- removing the () around ""l"" makes c++14 pass
- int(l) in stead of (l) make c++14 pass, but (int)(l) still fails

Tried gcc 7.4.0 with glibc 2.27 on ubuntu 18.04
and gcc 5.4.0 with glibc 2.23 on ubuntu 16.04"
33359,ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos 7
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.6
- Python version: 2.7
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: 9.0/7
- GPU model and memory: GeForce GTX TITAN and Tesla K40c



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

when ""import tensorflow"", will show ""ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory"" 

But /usr/local/cuda-9.0 is installed, and CUDA_HOME points to this path. 

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```

>>> ""libcublas.so.9.0"" in os.listdir(""/usr/local/cuda-9.0/lib64"")
True
>>> os.environ[""CUDA_HOME""]
'/usr/local/cuda-9.0'
>>> import tensorflow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/project/activelearning/shalijiang/anaconda2/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/project/activelearning/shalijiang/anaconda2/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/project/activelearning/shalijiang/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/project/activelearning/shalijiang/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/project/activelearning/shalijiang/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/project/activelearning/shalijiang/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```
"
33358,Cannot train canned estimators in multiple estimator.train() calls when using tf.keras.optimizers or tf.optimizers,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- Platform: Code run in google Colab
- Python version: Python 3
- Tensorflow version: v2.0.0-rc1-51-g2646d23 2.0.0-rc2

**Describe the current behavior**
When training a canned estimator with multiple tf.train calls while using any tf.keras.optimizer the optimizer raises an exception. 

**Describe the expected behavior**
Repeated tf.train calls train for the given amount of steps.

**Code to reproduce the issue**
Lightly edited example using canned estimators:
https://gist.github.com/JoshEZiegler/2a923a707d831ca7efd33dbfbf9779c9

**Other info / logs**
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-17-75a55ecc34ba> in <module>()
      5 classifier.train(
      6     input_fn=lambda: input_fn(train, train_y, training=True),
----> 7     steps=500)

7 frames
/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py in iterations(self, variable)
    660   def iterations(self, variable):
    661     if self._iterations is not None:
--> 662       raise RuntimeError(""Cannot set `iterations` to a new Variable after ""
    663                          ""the Optimizer weights have been created"")
    664     self._iterations = variable

RuntimeError: Cannot set `iterations` to a new Variable after the Optimizer weights have been created
"
33357,Error when loading model with tf.keras.layers.Input() layer,"On TensorFlow 1.14 (OS Ubuntu 16.04), when I call tf.keras.models.load_model() to load an entire tf.Keras model (following https://www.tensorflow.org/tutorials/keras/save_and_load) if the model contains tf.keras.layers.Input(), the function fails with:

```
(...)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/hdf5_format.py
in load_weights_from_hdf5_group(f, layers)
    735                      'containing ' + str(len(layer_names)) +
    736                      ' layers into a model with ' + str(len(filtered_layers)) +
--> 737                      ' layers.')
    738 
    739   # We batch weight value assignments in a single backend call

ValueError: You are trying to load a weight file containing 6 layers into a model with
0 layers.
```

The minimal code to reproduce is:

```
import tensorflow as tf
from tensorflow.keras.utils import to_categorical, HDF5Matrix

(Xtr, Ytr), (Xva, Yva) = tf.keras.datasets.cifar10.load_data()
Xtr, Ytr, Xva, Yva, nc = Xtr[:1000], Ytr[:1000], Xva[:100], Yva[:100], 10
Xtr, Xva = Xtr.astype('float32') / 255, Xva.astype('float32') / 255
Ytr, Yva, ins = to_categorical(Ytr, nc), to_categorical(Yva, nc), Xtr.shape[1:]

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(ins)) # (1)
model.add(tf.keras.layers.Conv2D(8, (3, 3))) # (2)
#model.add(tf.keras.layers.Conv2D(8, (3, 3), input_shape=ins)) # (3)
model.add(tf.keras.layers.Activation('relu'))
model.add(tf.keras.layers.Conv2D(8, (3, 3), activation='relu'))
model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))
model.add(tf.keras.layers.Dropout(0.25))
model.add(tf.keras.layers.Conv2D(16, (3, 3), activation='relu'))
model.add(tf.keras.layers.Conv2D(16, (3, 3), activation='relu'))
model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))
model.add(tf.keras.layers.Dropout(0.25))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(32, activation='relu'))
model.add(tf.keras.layers.Dropout(0.5))
model.add(tf.keras.layers.Dense(nc, activation='softmax'))
opt = tf.keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)
model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc'])

model.fit(x=Xtr, y=Ytr, batch_size=32, epochs=1, validation_data=(Xva, Yva))
model.save('model.hdf5')
del model
model = tf.keras.models.load_model('model.hdf5')
```

If I replace (1) and (2) by (3) the load_model() function works.
"
33356,'tensorflow.python.keras.api._v1.keras.losses' has no attribute 'Reduction',"I am using Huber loss implementation in tf.keras in tensorflow 1.14.0 as follows:

```
huber_keras_loss = tf.keras.losses.Huber(
        delta=delta,
        reduction=tf.keras.losses.Reduction.SUM,
        name='huber_loss'
    )
```

I am getting the error AttributeError: module 'tensorflow.python.keras.api._v1.keras.losses' has no attribute 'Reduction'

I have tried using tf.losses.Reduction, tf.compat.v2.losses.Reduction nothing seems to work.

Did tensorflow remove Reduction from tf.keras.losses, it is strange if they did so because their documentation still shows: https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/keras/losses/Huber#args"
33355,Error when Saving Whole Model Keras Model with LSTM / Embedding with Checkpoints ,"When running the model with the model checkpoint callback and adding `save_weights_only=True`, it works. 

Install:
pip install tensorflow==2.0.0

Model:
```
    model = keras.Sequential([
        keras.layers.Embedding(vocab_size, embedding_dim,
                               batch_input_shape=[batch_size, None]),
        keras.layers.GRU(rnn_units,
                         return_sequences=True,
                         stateful=True,
                         recurrent_initializer='glorot_uniform'),
        keras.layers.Dense(vocab_size)
    ])
    def loss(labels, logits):
        return keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)

    optimizer = keras.optimizers.Adam(lr=learning_rate)
    model.compile(optimizer=optimizer, loss=loss)
```
Callback:
```
keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix)
```
Error
```
Traceback (most recent call last):
  File ""/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/text_generator_tf2/trainer/task.py"", line 112, in <module>
    train_and_evaluate(args)
  File ""/text_generator_tf2/trainer/task.py"", line 96, in train_and_evaluate
    history = training_model.fit(dataset, steps_per_epoch=2, epochs=args.num_epochs, callbacks=callbacks)
  File ""/text_generator_tf2/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py"", line 728, in fit
    use_multiprocessing=use_multiprocessing)
  File ""/text_generator_tf2/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 372, in fit
    prefix='val_')
  File ""/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py"", line 119, in __exit__
    next(self.gen)
  File ""/text_generator_tf2/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 685, in on_epoch
    self.callbacks.on_epoch_end(epoch, epoch_logs)
  File ""/text_generator_tf2/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py"", line 298, in on_epoch_end
    callback.on_epoch_end(epoch, logs)
  File text_generator_tf2/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py"", line 965, in on_epoch_end
    self._save_model(epoch=epoch, logs=logs)
  File ""/text_generator_tf2/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py"", line 1012, in _save_model
    self.model.save(filepath, overwrite=True)
  File ""/text_generator_tf2/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py"", line 975, in save
    signatures, options)
  File ""/text_generator_tf2/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py"", line 115, in save_model
    signatures, options)
  File ""/text_generator_tf2/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save.py"", line 74, in save
    save_lib.save(model, filepath, signatures, options)
  File ""/text_generator_tf2/venv/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py"", line 893, in save
    meta_graph_def, saveable_view, signatures, options.namespace_whitelist)
  File ""/text_generator_tf2/venv/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py"", line 593, in _fill_meta_graph_def
    signatures = _generate_signatures(signature_functions, resource_map)
  File ""/text_generator_tf2/venv/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py"", line 465, in _generate_signatures
    function, mapped_inputs, resource_map)
  File ""/text_generator_tf2/venv/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py"", line 417, in _call_function_with_mapped_captures
    function.graph.captures, resource_map)
  File ""/text_generator_tf2/venv/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py"", line 339, in _map_captures_to_created_tensors
    .format(interior))
AssertionError: Tried to export a function which references untracked object Tensor(""StatefulPartitionedCall/args_2:0"", shape=(), dtype=resource).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.
```

"
33354,fix documentation for tf.batch_to_space,"https://www.tensorflow.org/api_docs/python/tf/batch_to_space

the documentation describing the steps is merged together and not clear.

Is the link to the source code correct?
yes

### Submit a pull request?
yes submitted a fix here: https://github.com/tensorflow/tensorflow/pull/33351



"
33352,No module named 'tensorflow.tools.graph_transforms' in TF2.0,"This functionality seems still [included](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#strip_unused_nodes) in TensorFlow 2.0, however, it raises an error when calling.

```
% ipython
Python 3.7.3 (v3.7.3:ef4ec6ed12, Mar 25 2019, 16:52:21) 
Type 'copyright', 'credits' or 'license' for more information
IPython 7.8.0 -- An enhanced Interactive Python. Type '?' for help.

In [1]: from tensorflow.tools.graph_transforms import TransformGraph                                                 
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-1-1fd86d9792e0> in <module>
----> 1 from tensorflow.tools.graph_transforms import TransformGraph

ModuleNotFoundError: No module named 'tensorflow.tools.graph_transforms'
```

TensorFlow version:

```
% pip show tensorflow
Name: tensorflow
Version: 2.0.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author: Google Inc.
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: /Volumes/data/venv-tf2/lib/python3.7/site-packages
Requires: gast, six, tensorboard, grpcio, termcolor, tensorflow-estimator, protobuf, keras-preprocessing, keras-applications, numpy, opt-einsum, absl-py, wrapt, wheel, astor, google-pasta
Required-by: tfcoreml
```

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary pip install
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7.3
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A
"
33348,Variable creation fails in TF-Nightly,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below):  > 2.1.0-dev20191012
- Python version: 3.6

**Describe the current behavior**
Variable creation currently fails with:
```
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/variables.py"", line 261, in __call__
    return cls._variable_v2_call(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/variables.py"", line 255, in _variable_v2_call
    shape=shape)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/variables.py"", line 236, in <lambda>
    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/variable_scope.py"", line 2645, in default_variable_creator_v2
    shape=shape)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/variables.py"", line 263, in __call__
    return super(VariableMetaclass, cls).__call__(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py"", line 1410, in __init__
    distribute_strategy=distribute_strategy)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py"", line 1556, in _init_from_args
    graph_mode=self._in_graph_mode)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py"", line 231, in eager_safe_variable_handle
    shape, dtype, shared_name, name, graph_mode, initial_value)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py"", line 167, in _variable_handle_from_shape_and_dtype
    handle_data.shape_and_type.append(
AttributeError: 'google.protobuf.pyext._message.RepeatedCompositeCo' object has no attribute 'append'
```
**Code to reproduce the issue**
From example: https://www.tensorflow.org/guide/variable#create_a_variable
```
import tensorflow as tf
var = tf.Variable(tf.zeros([1., 2., 3.]))
```

**Other info / logs**
I believe this is occured after aa25ad70c021968fb3a4a93ee814ca2fa699b32b 

cc @mrry 
"
33347,tf.data pipeline on TPU slows when upgrading from v1.12 -> v1.14,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 & 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.12.0 & 1.14.0
- Python version: 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A (TPU)
- GPU model and memory: N/A (TPU)

**Describe the current behavior**

Training a custom estimator on TPU with a `tf.data` pipeline that reads from TFRecords performs well on v1.12. Profiling with the TPU profiler shows that very little of the step time is spent waiting for input.

After upgrading to v1.14, training is slowed down, and the TPU profiler reports that much more of the step time is spent waiting for input (~67.1%). The global steps/sec falls from ~10 steps/sec to ~6 steps/sec.

Fixing deprecation warnings (e.g. not using `.make_one_shot_iterator().get_next()`) doesn't seem to help.

Our pipeline appears to follow the [`tf.data` performance guidelines](https://www.tensorflow.org/guide/data_performance) except for not using `tf.data.experimental.AUTOTUNE` -- using this parameter doesn't recover performance either.

**Describe the expected behavior**

I would not expect upgrading the library to slow down data infeed.

**Code to reproduce the issue**

Our `tf.data` pipeline looks roughly like:
```python
dataset = tf.data.TFRecordDataset(
    tf.data.Dataset.list_files(pattern, shuffle=True),
    buffer_size=(32 * 1024 * 1024),
    num_parallel_reads=8,
)
dataset = dataset.shuffle(shuffle_buffer_size) \
                 .repeat() \
                 .map(parse_fn, num_parallel_calls=8) \
                 .batch(batch_size, drop_remainder=True) \
                 .prefetch(4)
```
Tuning parameters after upgrading doesn't recover any performance, nor does using `tf.data.experimental.AUTOTUNE`.

**Other info / logs**

Here are the overview pages from the TPU profiler on TensorBoard:

TPU profiler v1.12:
![v1-12](https://user-images.githubusercontent.com/3229244/66775978-2a9f1400-ee7a-11e9-9a7c-66af8da8829d.png)

TPU profiler v1.14:
![v1-14](https://user-images.githubusercontent.com/3229244/66775984-2ecb3180-ee7a-11e9-8f77-435ce9b0236e.png)

Let me know if there's any other information that I can provide!"
33346,Update Build instructions for Docker 19.03,"## Description of issue (what needs changing):

Docker Build from Source documentation seems to be out of date for docker 19.03.
For example there is no --runtime=nvidia flag available any more."
33345,keras optimizer `apply_gradients` arg `grads_and_vars` has wrong type in documentation,"## URL(s) with the issue:

https://github.com/tensorflow/tensorflow/blob/516c98da7b7d8526c153827c426c675a4ece9543/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L414

## Description of issue (what needs changing):

`grads_and_vars` is documented as list but is actually passed as `zip object`.

### Clear description

This can be problematic when writing custom optimizers that iterate over the `grads_and_vars` multiple times, as in that case a `zip object` will not give the intended behaviour.

"
33344,Bugs in tf.data.experimental.make_csv_dataset,"I believe these are two bugs in tf.data.experimental.make_csv_dataset. I am using the examples from https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_csv_dataset.

- OS Platform and Distribution: Ubuntu 19.04
- TensorFlow installed from (source or binary): binary via pip3
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0
- Python version: 3.7.3
- CUDA/cuDNN version: CUDA 10.0
- GPU model and memory: GTX1060 / 6GB

**Describe the current behavior**

I have a CSV dataset consisting of 50K lines, and with each line having ~6500 integer valued columns such as this:

```
1,9720798423,89,3,7537296578,98,2...8470364829,91,0
2,7984338623,32,1,4023987716,98,2...3876678829,91,3
```
Based on the length of each line and size of the dataset I can't store it in memory, so I am attempting to stream the CVS from disk using this function:

```
BATCHSIZE = 5
SELECT_COLUMNS = ['index', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k',...'aaab', 'aaac', 'aaad']
DEFAULTS = [tf.int32, tf.int32, tf.int32, ... tf.int32]

train_dataset = tf.data.experimental.make_csv_dataset(
    sys.argv[1],
    batch_size = BATCHSIZE,
    column_names = SELECT_COLUMNS,
    column_defaults = DEFAULTS,
    label_name = LABEL_COLUMN,
    field_delim=',',
    use_quote_delim=True,
    na_value='',
    header=False,
    num_epochs=1,
    shuffle=True,
    shuffle_buffer_size=500,
    shuffle_seed=None,
    num_parallel_reads=1,
    sloppy=False,
    num_rows_for_inference=1000,
    compression_type=None,
    ignore_errors=False
)
```
Here is the error I am getting:
```

2019-10-14 13:06:40.698090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5183 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:03:00.0, compute capability: 6.1)
WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/data/experimental/ops/readers.py:521: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
2019-10-14 13:07:08.579307: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at iterator_ops.cc:929 : Invalid argument: Field 5 in record is not a valid int32: 7537296578
Traceback (most recent call last):
  File ""/home/gp/dm/aae.py"", line 77, in <module>
    for element in train_dataset:
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 622, in __next__
    return self.next()
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 666, in next
    return self._next_internal()
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 651, in _next_internal
    output_shapes=self._flat_output_shapes)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/gen_dataset_ops.py"", line 2673, in iterator_get_next_sync
    _six.raise_from(_core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: Field 5 in record is not a valid int32: 7537296578 [Op:IteratorGetNextSync]


```
I have inspected the records for control characters and whitespace, everything appears to be fine and I can copy and paste that integer value into an interactive Python terminal and assign it to an int32.

Also, it would appear prefetch_buffer_size=dataset_ops.AUTOTUNE is not a valid option:

```
/usr/bin/python3.7 /home/aj/ga.py dataset.csv
Loading data...
Traceback (most recent call last):
  File ""/home/aj/ga.py"", line 68, in <module>
    prefetch_buffer_size=dataset_ops.AUTOTUNE,
NameError: name 'dataset_ops' is not defined
```

**Describe the expected behavior**

For each each column in the CSV to be assigned an int32 during each row fetch, and for the prefetch_buffer_size option to be implemented according to the most recent documentation."
33341,TF lite implementation for RandomStandardNormal,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): Docker image tf-nightly-gpu-py3
- TensorFlow version (or github SHA if from source): 3.6.8


**Provide the text output from tflite_convert**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, EXP, FULLY_CONNECTED, LEAKY_RELU, LOG, MUL. Here is a list of operators for which you will need custom implementations: RandomStandardNormal.
```

VAE model was taken from your custom layers and models site ([here](https://www.tensorflow.org/guide/keras/custom_layers_and_models#building_models)). The line of code producing this exception is
`epsilon = tf.keras.backend.random_normal(shape=(batch, dim))`.

It might be a good idea to add a tflite implementation for this function so that people can try a conversion with your existing examples.
"
33340,Significant prediction slowdown after model.compile(),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): `pip install tensorflow`
- TensorFlow version: 2.0.0
- Python version: 3.7
- CUDA/cuDNN version: CUDA=10.0, cuDNN=7.6.4
- GPU model and memory: GTX 1060 6GB


**Describe the current behavior**
The prediction speed is slowed down a lot after `model.compile()` call.

**Describe the expected behavior**
Speed should not be affected. Predict function is used by users assuming that it will work fast because we use it all the time in production. It should not cause surprise to users.

**Code to reproduce the issue**
https://nbviewer.jupyter.org/github/off99555/TensorFlowExperiments/blob/master/test-prediction-speed-after-compile.ipynb?flush_cache=true

![image](https://user-images.githubusercontent.com/15215732/66762282-e3dc0900-eecf-11e9-8d93-82c8bcc5325b.png)
"
33339,embedding_column converts from variable-length input feature does not work with Distributed Keras MultiWorkerMirroredStrategy,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): ('v2.0.0-rc2-26-g64c3d382ca', '2.0.0')
- Python version: 2.7.13
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CPU only
- GPU model and memory: CPU only

**Describe the current behavior**

`embedding_column` converts from `variable-length input feature` does not work with Distributed Keras `MultiWorkerMirroredStrategy`. 
**IF:**
① **With local training(non-distributed), everything is fine . ✔️**
② **Convert variable-length input feature to indicator_column everything is ok. ✔️**
③ **embedding_column converts from fixed-length input feature works pretty good. ✔️**

Maybe this problem is caused by SparseTensor ?

**Code to reproduce the issue**

```
import tensorflow as tf
import os
import json

os.environ['TF_CONFIG'] = json.dumps({
    'cluster': {
        'worker': [""localhost:12345"", ""localhost:23456""]
    },
    'task': {'type': 'worker', 'index': 1}
})

# generate fake data
def serialize_example(value):
    feature = {
      'color': tf.train.Feature(bytes_list=tf.train.BytesList(value=value)),
    }
    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))
    return example_proto.SerializeToString()

tfrecord_writer = tf.io.TFRecordWriter('./color.tfrecord')
for each in [['G', 'R'], ['B'], ['B', 'G'], ['R']]:
    tfrecord_writer.write(serialize_example(each))
tfrecord_writer.close()

# build feature column
color_column = tf.feature_column.categorical_column_with_vocabulary_list('color', ['R', 'G', 'B'], dtype=tf.string)
color_embeding = tf.feature_column.embedding_column(color_column, 4) # tf.feature_column.indicator_column(color_column)

inputs = {}
inputs['color'] = tf.keras.layers.Input(name='color', shape=(None, ), sparse=True, dtype='string')

# build model
with tf.distribute.experimental.MultiWorkerMirroredStrategy().scope():
    dense = tf.keras.layers.DenseFeatures([color_embeding])(inputs)
    output = tf.keras.layers.Dense(1, activation='sigmoid')(dense)
    model = tf.keras.Model(inputs, output)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# build dataset
def parse(example_proto):
    feature_description = {
        'color': tf.io.VarLenFeature(tf.string)
    }
    parsed_features = tf.io.parse_single_example(example_proto, feature_description)
    return parsed_features, True
    
dataset = tf.data.TFRecordDataset('./color.tfrecord').map(parse).repeat().batch(1)

model.fit(dataset, epochs=3, steps_per_epoch=1)
```

**Other info / logs**
```
2019-10-14 22:35:48.491329: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-10-14 22:35:48.504154: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fd065e01d50 executing computations on platform Host. Devices:
2019-10-14 22:35:48.504171: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2019-10-14 22:35:48.506239: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:258] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12345, 1 -> localhost:23456}
2019-10-14 22:35:48.506975: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:23456
INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:1/device:CPU:0', '/job:worker/replica:0/task:1/device:XLA_CPU:0']
INFO:tensorflow:Multi-worker CollectiveAllReduceStrategy with cluster_spec = {u'worker': [u'localhost:12345', u'localhost:23456']}, task_type = 'worker', task_id = 1, num_workers = 2, local_devices = ('/job:worker/task:1',), communication = CollectiveCommunication.AUTO
INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_workers = 2
INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_workers = 2
INFO:tensorflow:Running Distribute Coordinator with mode = 'independent_worker', cluster_spec = {u'worker': [u'localhost:12345', u'localhost:23456']}, task_type = u'worker', task_id = 1, environment = None, rpc_layer = 'grpc'
WARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an ""evaluator"" task exists in the cluster.
WARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.
INFO:tensorflow:Multi-worker CollectiveAllReduceStrategy with cluster_spec = {u'worker': [u'localhost:12345', u'localhost:23456']}, task_type = u'worker', task_id = 1, num_workers = 2, local_devices = (u'/job:worker/task:1',), communication = CollectiveCommunication.AUTO
INFO:tensorflow:Multi-worker CollectiveAllReduceStrategy with cluster_spec = {u'worker': [u'localhost:12345', u'localhost:23456']}, task_type = u'worker', task_id = 1, num_workers = 2, local_devices = (u'/job:worker/task:1',), communication = CollectiveCommunication.AUTO
WARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.
2019-10-14 22:36:00.226012: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:400] Cannot find shardable dataset, adding a shard node at the end of the dataset instead. This may have performance implications.
Train for 1 steps
Epoch 1/3
INFO:tensorflow:Collective batch_all_reduce: 2 all-reduces, num_workers = 2
INFO:tensorflow:Collective batch_all_reduce for IndexedSlices: 1 all-reduces, num_workers = 2
INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_workers = 2
INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_workers = 2
INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_workers = 2
INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_workers = 2
INFO:tensorflow:Collective batch_all_reduce: 2 all-reduces, num_workers = 2
INFO:tensorflow:Collective batch_all_reduce for IndexedSlices: 1 all-reduces, num_workers = 2
INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_workers = 2
INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_workers = 2
INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_workers = 2
INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_workers = 2
2019-10-14 22:36:01.352450: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Internal: Inconsistent output shapes, got [4], but expected is [2].
	 [[{{node Adam/allreduce_1/CollectiveGather_1}}]]
Additional GRPC error information:
{""created"":""@1571063761.352390000"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""Inconsistent output shapes, got [4], but expected is [2].\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]"",""grpc_status"":13}
2019-10-14 22:36:01.352478: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Inconsistent output shapes, got [4], but expected is [2].
	 [[{{node Adam/allreduce_1/CollectiveGather_1}}]]
Additional GRPC error information:
{""created"":""@1571063761.352390000"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""Inconsistent output shapes, got [4], but expected is [2].\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]"",""grpc_status"":13}
2019-10-14 22:36:01.352638: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingGather with Internal: Inconsistent output shapes, got [4], but expected is [2].
	 [[{{node Adam/allreduce_1/CollectiveGather_1}}]]
Additional GRPC error information:
{""created"":""@1571063761.352390000"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""Inconsistent output shapes, got [4], but expected is [2].\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]"",""grpc_status"":13}
2019-10-14 22:36:01.352657: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Inconsistent output shapes, got [4], but expected is [2].
	 [[{{node Adam/allreduce_1/CollectiveGather_1}}]]
Additional GRPC error information:
{""created"":""@1571063761.352390000"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""Inconsistent output shapes, got [4], but expected is [2].\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]"",""grpc_status"":13}
2019-10-14 22:36:01.353012: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingGather with Internal: Inconsistent output shapes, got [4], but expected is [2].
	 [[{{node Adam/allreduce_1/CollectiveGather_1}}]]
Additional GRPC error information:
{""created"":""@1571063761.352390000"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""Inconsistent output shapes, got [4], but expected is [2].\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]"",""grpc_status"":13}
2019-10-14 22:36:01.353029: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Inconsistent output shapes, got [4], but expected is [2].
	 [[{{node Adam/allreduce_1/CollectiveGather_1}}]]
Additional GRPC error information:
{""created"":""@1571063761.352390000"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""Inconsistent output shapes, got [4], but expected is [2].\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]"",""grpc_status"":13}
2019-10-14 22:36:01.353213: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Internal: Inconsistent output shapes, got [4], but expected is [2].
	 [[{{node Adam/allreduce_1/CollectiveGather_1}}]]
Additional GRPC error information:
{""created"":""@1571063761.352390000"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""Inconsistent output shapes, got [4], but expected is [2].\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]"",""grpc_status"":13}
2019-10-14 22:36:01.353225: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Inconsistent output shapes, got [4], but expected is [2].
	 [[{{node Adam/allreduce_1/CollectiveGather_1}}]]
Additional GRPC error information:
{""created"":""@1571063761.352390000"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""Inconsistent output shapes, got [4], but expected is [2].\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]"",""grpc_status"":13}
2019-10-14 22:36:01.353325: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:234 : Internal: Inconsistent output shapes, got [4], but expected is [2].
	 [[{{node Adam/allreduce_1/CollectiveGather_1}}]]
Additional GRPC error information:
{""created"":""@1571063761.352390000"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""Inconsistent output shapes, got [4], but expected is [2].\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]"",""grpc_status"":13}
2019-10-14 22:36:01.353338: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Cancelled: [_Derived_]Cancelled
Additional GRPC error information:
{""created"":""@1571063761.352818000"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""Cancelled"",""grpc_status"":1}
2019-10-14 22:36:01.353352: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Cancelled: [_Derived_]Cancelled
Additional GRPC error information:
{""created"":""@1571063761.352818000"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""Cancelled"",""grpc_status"":1}
2019-10-14 22:36:01.353370: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:125 : Internal: Inconsistent output shapes, got [4], but expected is [2].
	 [[{{node Adam/allreduce_1/CollectiveGather_1}}]]
Additional GRPC error information:
{""created"":""@1571063761.352390000"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""Inconsistent output shapes, got [4], but expected is [2].\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]"",""grpc_status"":13}
2019-10-14 22:36:01.353359: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Inconsistent output shapes, got [4], but expected is [2].
	 [[{{node Adam/allreduce_1/CollectiveGather_1}}]]
Additional GRPC error information:
{""created"":""@1571063761.352390000"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""Inconsistent output shapes, got [4], but expected is [2].\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]"",""grpc_status"":13}
	 [[metrics/accuracy/div_no_nan/allreduce_1/CollectiveReduce]]
2019-10-14 22:36:01.353469: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Cancelled: [_Derived_]Cancelled
Additional GRPC error information:
{""created"":""@1571063761.352917000"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""Cancelled"",""grpc_status"":1}
2019-10-14 22:36:01.353451: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:234 : Cancelled: [_Derived_]Cancelled
Additional GRPC error information:
{""created"":""@1571063761.352818000"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""Cancelled"",""grpc_status"":1}
2019-10-14 22:36:01.353491: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Cancelled: [_Derived_]Cancelled
Additional GRPC error information:
{""created"":""@1571063761.352917000"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""Cancelled"",""grpc_status"":1}
2019-10-14 22:36:01.353512: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:234 : Internal: Inconsistent output shapes, got [4], but expected is [2].
	 [[{{node Adam/allreduce_1/CollectiveGather_1}}]]
Additional GRPC error information:
{""created"":""@1571063761.352390000"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""Inconsistent output shapes, got [4], but expected is [2].\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]"",""grpc_status"":13}
2019-10-14 22:36:01.353630: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:234 : Cancelled: [_Derived_]Cancelled
Additional GRPC error information:
{""created"":""@1571063761.352917000"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""Cancelled"",""grpc_status"":1}
2019-10-14 22:36:01.353698: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:125 : Internal: Inconsistent output shapes, got [4], but expected is [2].
	 [[{{node Adam/allreduce_1/CollectiveGather_1}}]]
Additional GRPC error information:
{""created"":""@1571063761.352390000"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""Inconsistent output shapes, got [4], but expected is [2].\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]"",""grpc_status"":13}
1/1 [==============================] - 1s 1s/step
---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
<ipython-input-1-b83aa9a3e019> in <module>()
     43
     44 dataset = tf.data.TFRecordDataset('./color.tfrecord').map(parse).repeat().batch(1)
---> 45 model.fit(dataset, epochs=3, steps_per_epoch=1)

/Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training.pyc in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    726         max_queue_size=max_queue_size,
    727         workers=workers,
--> 728         use_multiprocessing=use_multiprocessing)
    729
    730   def evaluate(self,

/Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.pyc in fit(self, *args, **kwargs)
    787   def fit(self, *args, **kwargs):
    788     return train_with_multi_worker(self._single_worker_loop.fit)(
--> 789         *args, **kwargs)
    790
    791   def evaluate(self, *args, **kwargs):

/Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.pyc in wrapper(model, **kwargs)
    774         _worker_fn,
    775         model._distribution_strategy,
--> 776         mode=dc.CoordinatorMode.INDEPENDENT_WORKER)
    777
    778   return wrapper

/Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.pyc in run_distribute_coordinator(worker_fn, strategy, eval_fn, eval_strategy, mode, cluster_spec, task_type, task_id, session_config, rpc_layer)
    851         # All jobs run `worker_fn` if between-graph.
    852         return _run_single_worker(worker_fn, strategy, cluster_spec, task_type,
--> 853                                   task_id, session_config, rpc_layer)
    854       else:
    855         # Only one node runs `worker_fn` if in-graph.

/Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.pyc in _run_single_worker(worker_fn, strategy, cluster_spec, task_type, task_id, session_config, rpc_layer, worker_barrier, coord)
    358         return worker_fn(strategy)
    359     else:
--> 360       return worker_fn(strategy)
    361
    362

/Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.pyc in _worker_fn(_)
    769       filtered_callbacks = dist_utils.filter_distributed_callbacks(callbacks)
    770       kwargs['callbacks'] = filtered_callbacks
--> 771       return method(model, **kwargs)
    772
    773     return dc.run_distribute_coordinator(

/Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training_v2.pyc in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
    322                 mode=ModeKeys.TRAIN,
    323                 training_context=training_context,
--> 324                 total_epochs=epochs)
    325             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)
    326

/Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training_v2.pyc in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)
    121         step=step, mode=mode, size=current_batch_size) as batch_logs:
    122       try:
--> 123         batch_outs = execution_function(iterator)
    124       except (StopIteration, errors.OutOfRangeError):
    125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?

/Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.pyc in execution_function(input_fn)
     84     # `numpy` translates Tensors to values in Eager mode.
     85     return nest.map_structure(_non_none_constant_value,
---> 86                               distributed_function(input_fn))
     87
     88   return execution_function

/Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/eager/def_function.pyc in __call__(self, *args, **kwds)
    455
    456     tracing_count = self._get_tracing_count()
--> 457     result = self._call(*args, **kwds)
    458     if tracing_count == self._get_tracing_count():
    459       self._call_counter.called_without_tracing()

/Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/eager/def_function.pyc in _call(self, *args, **kwds)
    518         # Lifting succeeded, so variables are initialized and we can run the
    519         # stateless function.
--> 520         return self._stateless_fn(*args, **kwds)
    521     else:
    522       canon_args, canon_kwds = \

/Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/eager/function.pyc in __call__(self, *args, **kwargs)
   1821     """"""Calls a graph function specialized to the inputs.""""""
   1822     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
-> 1823     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   1824
   1825   @property

/Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/eager/function.pyc in _filtered_call(self, args, kwargs)
   1139          if isinstance(t, (ops.Tensor,
   1140                            resource_variable_ops.BaseResourceVariable))),
-> 1141         self.captured_inputs)
   1142
   1143   def _call_flat(self, args, captured_inputs, cancellation_manager=None):

/Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/eager/function.pyc in _call_flat(self, args, captured_inputs, cancellation_manager)
   1222     if executing_eagerly:
   1223       flat_outputs = forward_function.call(
-> 1224           ctx, args, cancellation_manager=cancellation_manager)
   1225     else:
   1226       gradient_name = self._delayed_rewrite_functions.register()

/Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/eager/function.pyc in call(self, ctx, args, cancellation_manager)
    509               inputs=args,
    510               attrs=(""executor_type"", executor_type, ""config_proto"", config),
--> 511               ctx=ctx)
    512         else:
    513           outputs = execute.execute_with_cancellation(

/Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/eager/execute.pyc in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     65     else:
     66       message = e.message
---> 67     six.raise_from(core._status_to_exception(e.code, message), None)
     68   except TypeError as e:
     69     keras_symbolic_tensors = [

/Users/felix/Envs/tf2/lib/python2.7/site-packages/six.pyc in raise_from(value, from_value)
    735 else:
    736     def raise_from(value, from_value):
--> 737         raise value
    738
    739

InternalError:  Inconsistent output shapes, got [4], but expected is [2].
	 [[node Adam/allreduce_1/CollectiveGather_1 (defined at /Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]
Additional GRPC error information:
{""created"":""@1571063761.352390000"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""Inconsistent output shapes, got [4], but expected is [2].\n\t [[node Adam/allreduce_1/CollectiveGather_1 (defined at /Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]"",""grpc_status"":13}
	 [[metrics/accuracy/div_no_nan/allreduce_1/CollectiveReduce]] [Op:__inference_distributed_function_1149]

Function call stack:
distributed_function
```

This problem has been bothering me for a week, any help will be appreciated"
33338,//tensorflow/python/autograph/pyct/static_analysis/activity_py3_test.py fails with Python2.7,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 19.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): v2.0.0
- Python version: Python 2.7.15+
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 8.3.0-6ubuntu1) 8.3.0
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**
Looks like //tensorflow/python/autograph/pyct/static_analysis:activity_py3_test is newly added in TF 2.0. Following command throws this error 
```
bazel --host_jvm_args=""-Xms1024m"" --host_jvm_args=""-Xmx2048m"" test --define=tensorflow_mkldnn_contraction_kernel=0 --host_javabase=""@local_jdk//:jdk"" //tensorflow/python/autograph/pyct/static_analysis:activity_py3_test
```
Error:
```
File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/tensorflow/python/autograph/pyct/static_analysis/activity_py3_test.runfiles/org_tensorflow/tensorflow/python/autograph/pyct/static_analysis/activity_py3_test.py"", line 39
    nonlocal nonlocal_a
                      ^
SyntaxError: invalid syntax
```
I found [here](https://stackoverflow.com/questions/14264313/syntax-error-on-nonlocal-statement-in-python/14264325#14264325) that this issue gets resolved by upgrading Python2 to Python3. Tried the same and test passes. 

Is Python3.x recommended for building Tensorflow 2.0? I didn't see such update in Release notes for Tensorflow 2.x. Also as per [this update on Python](https://pip.pypa.io/en/latest/development/release-process/#python-2-support) Python2.7 will be deprecated in few months, are we planning to completely shift to Python 3.x?

Please guide us on this. Thanks."
33337,Dimension reasoning,"**System information**
- TensorFlow version (you are using): Tensorflow2.0
- Are you willing to contribute it (Yes/No):

**Describe the feature and the current behavior/state.**
**Will this change the current api? How?**
No
**Who will benefit with this feature?**
No
**Any Other info.**


`    
```
def roi_align_one_layer(feat, bbox, s, align_c):

            s_y, s_x = s
            num_bbox = tf.shape(bbox)[0]
            feat_shape = tf.shape(feat)
```
# tensorflow know c_i here, for example, [None, None, 256]
            h_i, w_i, c_i = feat_shape[0], feat_shape[1], feat_shape[2]

            h_f, w_f = tf.cast(h_i, tf.float32), tf.cast(w_i, tf.float32)
            bbox_y1 = bbox[:, 0:1]
            bbox_x1 = bbox[:, 1:2]
            bbox_y2 = bbox[:, 2:3]
            bbox_x2 = bbox[:, 3:4]
            bbox_h = bbox_y2 - bbox_y1
            bbox_w = bbox_x2 - bbox_x1
            if align_c:
                off_y = 0. if s_y > 1 else bbox_h / 2.
                off_x = 0. if s_x > 1 else bbox_w / 2.
                grid_y = tf.linspace(0.0, 1.0, s_y)
                grid_x = tf.linspace(0.0, 1.0, s_x)
            else:
                off_y = bbox_h / (2. * s_y)
                off_x = bbox_w / (2. * s_x)
                grid_y = tf.linspace(0.0, 1.0, s_y + 1)[:-1]
                grid_x = tf.linspace(0.0, 1.0, s_x + 1)[:-1]
            grid_y = tf.expand_dims(tf.matmul(bbox_h, tf.expand_dims(grid_y, axis=0)) + off_y + bbox_y1, axis=-1)
            grid_x = tf.expand_dims(tf.matmul(bbox_w, tf.expand_dims(grid_x, axis=0)) + off_x + bbox_x1, axis=-1)

            grid_y = tf.where(grid_y < 0., 0., grid_y)
            grid_x = tf.where(grid_x < 0., 0., grid_x)
            grid_y = tf.where(grid_y > h_f-1., h_f-1., grid_y)
            grid_x = tf.where(grid_x > w_f-1., w_f-1., grid_x)
            grid_y = tf.tile(grid_y, [1, 1, s_y])
            grid_x = tf.tile(grid_x, [1, 1, s_x])
            grid_y = tf.reshape(grid_y, [num_bbox, -1])
            grid_x = tf.reshape(grid_x, [num_bbox, -1])
            grid_y1 = tf.math.floor(grid_y)
            grid_y2 = tf.math.floor(grid_y+1.)
            grid_x1 = tf.math.floor(grid_x)
            grid_x2 = tf.math.floor(grid_x+1.)
            wey1 = tf.expand_dims(grid_y - grid_y1, axis=-1)
            wey2 = tf.expand_dims(grid_y2 - grid_y, axis=-1)
            wex1 = tf.expand_dims(grid_x - grid_x1, axis=-1)
            wex2 = tf.expand_dims(grid_x2 - grid_x, axis=-1)
            grid_y2 = tf.where(grid_y2 > h_f - 1., h_f - 1., grid_y2)
            grid_x2 = tf.where(grid_x2 > w_f - 1., w_f - 1., grid_x2)
            grid_y1 = tf.cast(grid_y1, tf.int32)
            grid_y2 = tf.cast(grid_y2, tf.int32)
            grid_x1 = tf.cast(grid_x1, tf.int32)
            grid_x2 = tf.cast(grid_x2, tf.int32)
            grid_11 = grid_y1 * w_i + grid_x1
            grid_12 = grid_y1 * w_i + grid_x2
            grid_21 = grid_y2 * w_i + grid_x1
            grid_22 = grid_y2 * w_i + grid_x2
            feat = tf.reshape(feat, [h_i*w_i, c_i])
            feat_11 = tf.gather(feat, grid_11)
            feat_12 = tf.gather(feat, grid_12)
            feat_21 = tf.gather(feat, grid_21)
            feat_22 = tf.gather(feat, grid_22)
            feat_bilinear = wey2 * (feat_11 * wex2 + feat_12 * wex1) + wey1 * (feat_21 * wex2 + feat_22 * wex1)

# tensorflow don't know c_i here,   [None, 14, 14, None]
            feat_bilinear = tf.reshape(feat_bilinear, [num_bbox, s_y, s_x, c_i])

            return feat_bilinear
`

the data_format is the 'channels_last'

when enable eager or disable_enger

when I use `y = roi_align_one_layer(feat, bbox, s, align_c)` the tensorflow2.0 know the number of channels of `feat` but don't know the number of channels of `y`,
when I use `tf.keras.layers.Conv()(y)`, tensorflow2.0 raise error that tensorflow2.0 don't know the number of input channels

how to let tensorflow  know it automatically

 
"
33336,tf.sparse.sparse_dense_matmul can't broadcast as tf.linalg.matmul does.,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.1.0
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**

tf.sparse.sparse_dense_matmul can't broadcast on the leading dimensions. the following code is an example.

```python
#!/usr/bin/python3

import numpy as np;
import tensorflow as tf;

def main():

    a = tf.sparse.SparseTensor(
        indices = [[0, 0, 1], [0, 0, 2], [0, 1, 2], [0, 1, 3], [0, 2, 1], [0, 2, 3]],
        values = [1., 1., 1., 1., 1., 1.],
        dense_shape = [1, 3, 4]
    ); # a.shape = (1,3,4)
    b = tf.constant(np.random.normal(size = (4, 4, 5)), dtype = tf.float32);
    c = tf.linalg.matmul(tf.sparse.to_dense(a),b); # will be succeed
    c = tf.sparse.sparse_dense_matmul(a,b); # will be fail

if __name__ == ""__main__"":

    main();
```

**Will this change the current api? How?**

no api need to be changed.

**Who will benefit with this feature?**

user handling sparse matrices will be benefit from the change.

**Any Other info.**
"
33335,tf.contrib.image.transform in TF2.0,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.0
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
In TF 1.X there were nice methods [tf.contrib.image.transform()](https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/contrib/image/transform) and [tf.contrib.image.compose_transforms()](https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/contrib/image/compose_transforms) that allowed to perform affine transforms with an image. 

In TF 2.0 whole tf.contrib.* is dropped.

For instance, I was using those in my custom augmentation mechanisms to add rotation and shear transforms (along with all zooms, shifts, flips, etc). And in TF 2.0 I see no way to perform those.

I tried to implement such a functionality with numpy+opencv instead of TF. However when I tried injecting such a functionality to tf.dataset.map() pipeline tensors seemed not to have .nupmy() method yet.

I also found tf.keras.preprocessing.image.apply_affine_transform() which does perform affine transform on image, but according to [docs](https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/keras/preprocessing/image/apply_affine_transform) it works with numpy array only.

**Will this change the current api? How?**
I guess that would be nice to port those functions form 1.14 to 2.X to: tf.image.compose_transforms() and tf.image.transform() or something. Should not affect existing API I guess.

**Who will benefit with this feature?**
Anybody who likes to perform affine transforms on image say in data augmentation step of model training.

**Any Other info.**
If I miss something and mentioned functionality can be implemented with existing API, please let me know."
33333,Potential error in Codelab : Learning Tensorflow 2 : Computer Vision ,"So I was following this codelab : https://codelabs.developers.google.com/codelabs/tensorflow-lab2-computervision/index.html?index=..%2F..index#4

On slide 5 the optimizer is set to be `tf.train.AdamOptimizer()`
and it returns an error of
 `module 'tensorflow_core._api.v2.train' has no attribute 'AdamOptimizer'`
I guess it should be 
`tf.optimizers.Adam()`
?"
33332,How can I change variable on the layer ( fine tuning ),"
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 x64
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:1.13.1
- **Python version**:3.6.4
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:9.0/7.4
- **GPU model and memory**:Nvidia Geforce 840m
- **Exact command to reproduce**: N/A

### Describe the problem
I have a pre-trained model that detects the whole face with 68 key points, I would like to use this pre-trained model to train my dataset for detecting only the eye region ( 40 points ).
So I need to change the value of units on the logits layer from 136(68 * 2) to 80(40 * 2).
I see an answer on Stackoverflow that I should use the fine-tuning, how can I use this technique to change some variables on the frozen model (.pb) ?

![mdlrp](https://user-images.githubusercontent.com/19480228/66752207-2f22f700-ee91-11e9-802c-3a40c540078d.PNG)

Hope I get an answer.
"
33328,use tf.layers.dense as a layer ups bug,"hi,dear all,
I just wanna use the tf's dense,
and when I set tf.layers.dense as a keras layer,it ups a bug,
how to solve it ?

codes:
```
def dense(inputs):
    return tf.layers.dense(inputs,units=513,activation='relu')

x_mixed=tf.keras.Input(shape=(5, 256),dtype=tf.float32, name='input')
fc_out=tf.keras.layers.Lambda(dense)(x_mixed)

model=tf.keras.Model(inputs=x_mixed,outputs=fc_out)
model.compile(loss=""mse"",metrics=['mae'],optimizer='adam')
model.summary()

X=np.random.randn(1000,5,256)
y=np.random.randn(1000,5,513)
model.fit(X,y,batch_size=16,epochs=5)
```

bug：
`ValueError: Tensor conversion requested dtype float32_ref for Tensor with dtype float32: 'Tensor(""Adam/dense/kernel/m/Initializer/zeros:0"", shape=(256, 513), dtype=float32)'`
Thx

"
33327,modulenotfounderror no module named '_pywrap_tensorflow_internal',"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.



I am using Windows 10, CPU: Intel(R) Core(TM) 2 Duo CPU T6600 @ 2.2GHz (2 CPUs) ~2.2GHz. RAM: 4GB. Video card: ATI Mobility Radeon HD 3400 Series. 


I installed Python 3.6 and Tensorflow==1.10.0. When I do import tensorflow, I get this error.

`modulenotfounderror no module named '_pywrap_tensorflow_internal'
`

I can install whichever Python/Tensorflow version you want. I just want to use Tensorflow. I saw similar issues, but none of them seems to be solving my issue. "
33326,[TF 2.0 API Docs] tf.GradientTape,"## URL(s) with the issue:

https://www.tensorflow.org/api_docs/python/tf/GradientTape

## Description of issue (what needs changing):

### Clear description
Class members and Return values in ""gradient"" function not consistent with actual values in code

### Correct links
Yes
### Parameters defined
Yes
### Returns defined
Needs to be modified
gradient () :
1. Argument - unconnected_gradients: a value which can either hold 'none' or 'zero'

  Should be NONE or ZERO

2. Return values:  

In addition to the mentioned return values, If none of the provided elements in ""sources"" argument are being watched, the function will return None


### Raises listed and defined
Yes

### Usage example
Yes
### Submit a pull request?
Yes"
33323,ValueError when using AdamOptimizer,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7
- CUDA/cuDNN version: CUDA10.0 and cnDNN7.6

**Describe the current behavior**
I instantiate the Adam in two ways, one works but the another report the ValueError.

**Describe the expected behavior**
There should be no difference between two ways to instantiate the optimizers.

**Code to reproduce the issue**
```python
from tensorflow import keras
from tensorflow.python.keras import layers, optimizers
import tensorflow as tf

# model
inp = layers.Input((None, None, 3))
x = layers.Conv2D(32, 3, padding=""same"")(inp)
x = layers.Conv2D(3, 3, padding=""same"")(inp)
model = keras.Model(inp, x)

# compile
# opt = keras.optimizers.Adam() # This method works
opt = optimizers.Adam()  # This method doesn't work
model.compile(optimizer=opt, loss=""mse"")

# data
x = tf.ones((16, 48, 48, 3))
y = tf.zeros((16, 48, 48, 3))

# train
model.fit(x, y, batch_size=1)

```

**Other info / logs**

```
Traceback (most recent call last):
  File ""/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py"", line 527, in _apply_op_helper
    preferred_dtype=default_dtype)
  File ""/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 1296, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py"", line 286, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py"", line 227, in constant
    allow_broadcast=True)
  File ""/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py"", line 265, in _constant_impl
    allow_broadcast=allow_broadcast))
  File ""/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py"", line 437, in make_tensor_proto
    raise ValueError(""None values not supported."")
ValueError: None values not supported.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py"", line 541, in _apply_op_helper
    values, as_ref=input_arg.is_ref).dtype.name
  File ""/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 1296, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py"", line 286, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py"", line 227, in constant
    allow_broadcast=True)
  File ""/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py"", line 265, in _constant_impl
    allow_broadcast=allow_broadcast))
  File ""/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py"", line 437, in make_tensor_proto
    raise ValueError(""None values not supported."")
ValueError: None values not supported.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""demo.py"", line 21, in <module>
    model.fit(x, y, batch_size=1)
  File ""/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py"", line 728, in fit
    use_multiprocessing=use_multiprocessing)
  File ""/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py"", line 674, in fit
    steps_name='steps_per_epoch')
  File ""/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py"", line 189, in model_iteration
    f = _make_execution_function(model, mode)
  File ""/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py"", line 565, in _make_execution_function
    return model._make_execution_function(mode)
  File ""/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py"", line 2184, in _make_execution_function
    self._make_train_function()
  File ""/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py"", line 2116, in _make_train_function
    params=self._collected_trainable_weights, loss=self.total_loss)
  File ""/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizers.py"", line 476, in get_updates
    grads = self.get_gradients(loss, params)
  File ""/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizers.py"", line 92, in get_gradients
    if None in grads:
  File ""/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py"", line 1336, in tensor_equals
    return gen_math_ops.equal(self, other)
  File ""/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_math_ops.py"", line 3627, in equal
    name=name)
  File ""/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py"", line 545, in _apply_op_helper
    (input_name, err))
ValueError: Tried to convert 'y' to a tensor and failed. Error: None values not supported.
```
"
33322,Can distributed strategy solve VRAM runout isuue?,"I have a complex function with tens of millions parameters and two hundreds of variables. I need to calculate the gradient of this function,but my Nvidia graphic card has not enough VRAM. I tried with two cards but it cannot solve the issue.
Two cards only accelerate the calculating speed, cannot solve memory problem, because every video card needs to save the same calculation graph, right?

Anyone can help me?"
33321,"""Skipping rendezvous re-initialization"" when calling collective_ops in a distributed cluster","We meet a [problem](https://stackoverflow.com/questions/58161090/when-does-skipping-rendezvous-re-initialization-emerge-in-calling-collective) which continuously logs `re-initialization` within the same MonitoredSession on the leader worker. Furthermore, there seems to be an accuracy losing during the gradient descent of the parabolic toy model. We have no idea how to prevent the logging of `re-initialization`. All hints or explanations are welcome.

It seems the [`BaseRemoteRendezvous::Initialize`](https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/core/distributed_runtime/base_rendezvous_mgr.cc#L152) method is called at every train step. The log of the leader worker is attached below.
```console
task 0: before syncing ...
{'u': 0.1, 'loss': -0.19, 'global_step': 0}
task 0: running init_ops ...
{'u': 0.1, 'loss': -0.19, 'global_step': 0}
task 0: obtaining gstep ...
train one step at step 0.
2019-09-30 10:15:56.954439: I tensorflow/core/distributed_runtime/base_rendezvous_mgr.cc:159] Skipping rendezvous re-initialization.
{'u': 0.1, 'loss': -0.19, 'global_step': 1}
train one step at step 1.
2019-09-30 10:15:56.959006: I tensorflow/core/distributed_runtime/base_rendezvous_mgr.cc:159] Skipping rendezvous re-initialization.
{'u': 0.28, 'loss': -0.4816, 'global_step': 2}
train one step at step 2.
2019-09-30 10:15:56.962205: I tensorflow/core/distributed_runtime/base_rendezvous_mgr.cc:159] Skipping rendezvous re-initialization.
{'u': 0.42396685, 'loss': -0.66818583, 'global_step': 3}
train one step at step 3.
2019-09-30 10:15:56.965719: I tensorflow/core/distributed_runtime/base_rendezvous_mgr.cc:159] Skipping rendezvous re-initialization.
{'u': 0.53912044, 'loss': -0.78759, 'global_step': 4}
train one step at step 4.
2019-09-30 10:15:56.969335: I tensorflow/core/distributed_runtime/base_rendezvous_mgr.cc:159] Skipping rendezvous re-initialization.
{'u': 0.6312327, 'loss': -0.8640107, 'global_step': 5}
{'u': 0.70491827, 'loss': -0.9129268, 'global_step': 5}
task 0: final syncing ...
task 0: finalized
```
A related issue #32810 also has the similar `re-initialization` at first. But the logging disappears with the improved code snippet, which patches the `send_tensor`, `recv_tensor` and `ChiefSessionCreator` on both workers."
33320,tf.distribute.MirroredStrategy read data from tfrecord slower!!!!,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):YES
- OS Platform and Distribution : Redhat 7.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below): tensorflow 2.0.0b1
- Python version: 3.5.4
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.0
- GPU model and memory: CRNN for OCR

**Describe the current behavior**
Using tf.distribute.MirroredStrategy parallel training, it was found that reading tfrecord was abnormally slow and the final training speed was very slow.


**Describe the expected behavior**

**Code to reproduce the issue**
# tfrecord reader
        eos_idx = self.charset.get_eosid()
        padded_shapes = ([], [self.norm_h, None, 3], [], [None], [], [], [])
        padding_values = ('', 0.0, '', eos_idx, 0, '', 0)
        fileset = tf.data.Dataset.list_files(self.file_list)
        dataset = fileset.apply(
                    tf.data.experimental.parallel_interleave(
                        lambda filename: tf.data.TFRecordDataset(
                            filename, num_parallel_reads=self.num_parallel),
                        cycle_length=32))
        dataset = dataset.map(map_func=self.parse_example, num_parallel_calls=self.num_parallel)
        dataset = dataset.filter(self.filter)
        dataset = dataset.apply(tf.data.experimental.ignore_errors()).shuffle(5000)
        dataset = dataset.padded_batch(self.batch_size, padded_shapes, padding_values)

        if repeat != 0:
            dataset = dataset.repeat(repeat)
        dataset = dataset.prefetch(buffer_size=6000) 

# tf.function code
    @tf.function
    def distributed_train_step(dataset_inputs):
        per_replica_losses = mirrored_strategy.experimental_run_v2(train_step_multi, args=(dataset_inputs,))
        return mirrored_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)
    @tf.function
    def distributed_test_step(dataset_inputs):
        return mirrored_strategy.experimental_run_v2(test_step_multi, args=(dataset_inputs,))

# The following code will have an apparent idle state of about 1s after each iteration.
    with mirrored_strategy.scope():
        iter_count = 0
        for epoch in range(train_epoch):
            logger.info(""run epoch {}"".format(epoch))
            for batch, data in enumerate(train_dataset):
                try:
                    distributed_train_step(data)
                    if iter_count % show_interval == 0:
                        print(""train_loss:"", train_loss.result().numpy())
                        train_loss.reset_states()
                except Exception as e:
                    print(""Exception:"", str(e))


# The following code is much faster than the above code, gpu will not be obviously idle, but the usage rate of gpu cannot be maintained more than 90%.
    with mirrored_strategy.scope():
        iter_count = 0
        for epoch in range(train_epoch):
            logger.info(""run epoch {}"".format(epoch))
            data = None
            for batch, data1 in enumerate(train_dataset):
               data  = data1
               break
            for i in range(999999999):
                try:
                    distributed_train_step(data)
                    if iter_count % show_interval == 0:
                        print(""train_loss:"", train_loss.result().numpy())
                        train_loss.reset_states()
                except Exception as e:
                    print(""Exception:"", str(e))

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
33319,Unable to run tensorflow-gpu 2.0.0 on RStudio after update,"The following error pops:

`> tensorflow::tf_config()`

_tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library '**cudart64_100.dll**'; dlerror: **cudart64_100.dll** not found_

- Have CUDA v.10.1 installed.  Folder CUDA\ v10.1\bin shows **cudart64_101.dll** not cudart64_100.dll
- Have CUDNN v.7.6.4.38 installed
- Environment variables point to CUDA\v10.1\bin and CUDA\v10.1\libnvvp
- Have created CUDA_HOME, CUDA_PATH and CUDA_PATH_V10_1 - all pointing to the same folder, CUDA\v10.1
- Anaconda3 contains 3 environments: _base, r-reticulate and tensorflow_, all having python 3.7.4 (do I really need one python for each environment?)
- Anaconda3 environment _tensorflow_ shows tensorflow-gpu 1.14.0 as I have installed tensorflow 2.0.0 from RStudio:

```
> require(keras)
> install_keras(tensorflow = 'gpu')
```

which installed tensorflow 2.0.0.

This was verified via conda

`> pip list | grep tensorflow`

_tensorflow 1.14.0_                     ## installed via Anaconda3 Navigator
_tensorflow-gpu 2.0.0_                ## version 1.14.0 shown in Anaconda3 Navigator
_tensorflow-estimator 2.0.0_       ## version 1.14.0 shown in Anaconda3 Navigator

- I work with

OS Windows 10
GPU Nvidia GeForce RTX 2080

I should mention that keras with tensorflow-gpu v 1.14.0 backend worked on RStudio before my tensorflow update to 2.0.0.

Please advise, thank you!
"
33317,no attribute 'ConfigProto' & no attribute '_MTCNN__session',"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pycharm addons
- TensorFlow version: 2.0.0
- Python version:3.7.2
- Installed using virtualenv? pip? conda?:pip


Rookie from China, not a native speaker. Working on tutorials at 'https://machinelearningmastery.com/how-to-perform-face-recognition-with-vggface2-convolutional-neural-network-in-keras/'. 
Problem is that I couldn't install the previous tensorflow version in >Project Interpreter>package of Pycharm no matter how I tried to install the other packages related with tensorflow. So that I installed version 2.0.0 which I've known that version 2.0.0 will not work as expected as is mentioned in that tutorial. 
Keyed those first part '**How to Detect Faces for Face Recognition**' codes. Here are the Erorrs:
AttributeError: module 'tensorflow' has no attribute 'ConfigProto'
Exception ignored in: <function MTCNN.__del__ at 0x00000199DEC60B70>
Traceback (most recent call last):
  File ""C:\PycharmProjects\VGG\venv\lib\site-packages\mtcnn\mtcnn.py"", line 616, in __del__
    self.__session.close()
AttributeError: 'MTCNN' object has no attribute '_MTCNN__session'

Then I found ways to solve this problem but couldn't quite understand which push me to post this issue of course the first time posting things.

Any suggestions and help are warmly welcomed. Thanks a lot.

Kym
"
33316,With lazy_import tensorflow cannot be imported,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 (and gLinux)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): pip binary
- TensorFlow version: 1.15.0.{rc1,rc2,rc3}
- Python version: python 3.6.8
- Installed using virtualenv? pip? conda?: anaconda
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: -
- GPU model and memory: -


**Describe the problem**

The following works correctly:

```bash 
$ python -c 'import tensorflow as tf; print(tf.__version__)'
1.15.0-rc3
```

In an interactive shell:

```python
import tensorflow as tf
```

I get the following error: `ValueError: operator __getitem__ cannot be overwritten again on class <class 'tensorflow.python.framework.ops.Tensor'>`.

**This happens when [`lazy_import`](https://github.com/mnmelo/lazy_import) is installed.** I think tensorflow's internal lazy_import has a conflict with some external library. TF 1.14 has no problem.

Though lazy_import is a third-party library, I think what it is doing is pretty non-hacky. Since this bug has appeared since 1.15.dev, we will need to have this fixed in the release version (given that 1.15 will be the last 1.x release).

Traceback:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/$HOME/.miniconda3/envs/ace-base/lib/python3.6/site-packages/tensorflow/__init__.py"", line 99, in <module>
    from tensorflow_core import *
  File ""/$HOME/.miniconda3/envs/ace-base/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 34, in <module>
    from tensorflow._api.v1 import autograph
  File ""/$HOME/.miniconda3/envs/ace-base/lib/python3.6/site-packages/tensorflow_core/_api/v1/autograph/__init__.py"", line 22, in <module>
    from tensorflow._api.v1.autograph import experimental
  File ""/$HOME/.miniconda3/envs/ace-base/lib/python3.6/site-packages/tensorflow_core/_api/v1/autograph/experimental/__init__.py"", line 10, in <module>
    from tensorflow.python.autograph.core.converter import Feature
  File ""/$HOME/.miniconda3/envs/ace-base/lib/python3.6/site-packages/tensorflow_core/python/autograph/__init__.py"", line 35, in <module>
    from tensorflow.python.autograph import operators
  File ""/$HOME/.miniconda3/envs/ace-base/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/__init__.py"", line 40, in <module>
    from tensorflow.python.autograph.operators.control_flow import for_stmt
  File ""/$HOME/.miniconda3/envs/ace-base/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/control_flow.py"", line 65, in <module>
    from tensorflow.python.autograph.operators import py_builtins
  File ""/$HOME/.miniconda3/envs/ace-base/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/py_builtins.py"", line 28, in <module>
    from tensorflow.python.autograph.utils import py_func
  File ""/$HOME/.miniconda3/envs/ace-base/lib/python3.6/site-packages/tensorflow_core/python/autograph/utils/__init__.py"", line 21, in <module>
    from tensorflow.python.autograph.utils.context_managers import control_dependency_on_returns
  File ""/$HOME/.miniconda3/envs/ace-base/lib/python3.6/site-packages/tensorflow_core/python/autograph/utils/context_managers.py"", line 24, in <module>
    from tensorflow.python.ops import tensor_array_ops
  File ""/$HOME/.miniconda3/envs/ace-base/lib/python3.6/site-packages/tensorflow_core/python/ops/tensor_array_ops.py"", line 35, in <module>
    from tensorflow.python.ops import array_ops
  File ""/$HOME/.miniconda3/envs/ace-base/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py"", line 1045, in <module>
    ops.Tensor._override_operator(""__getitem__"", _slice_helper)
  File ""/$HOME/.miniconda3/envs/ace-base/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 745, in _override_operator
    _override_helper(Tensor, operator, func)
  File ""/$HOME/.miniconda3/envs/ace-base/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 177, in _override_helper
    (operator, clazz_object))
ValueError: operator __getitem__ cannot be overwritten again on class <class 'tensorflow.python.framework.ops.Tensor'>.
```"
33315,Training slows down with repeated calls to Model.fit(),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Version 1903
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca 2.0.0
- Python version: 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)]
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
Calls to `tf.keras.Model.fit()` run slower and slower the more there have been. The sample code below reports about 200 us/sample at the start but after running for 20 minutes report about 400 us per sample. This continues to increase with no apparent upper limit. It remains slow even when training subsequent models but resets to fast after restarting the program.

**Describe the expected behavior**
No long term upward trend in training time.

**Code to reproduce the issue**

    import numpy
    import tensorflow as tf
    input = tf.keras.Input(shape=(1600,))
    z = tf.keras.layers.Dense(units=200, activation='relu')(input)
    z = tf.keras.layers.Dense(units=1)(z)
    model = tf.keras.Model(inputs=input, outputs=z)
    model.compile(loss='mse')
    x = numpy.full((100,1600), fill_value=2.34, dtype=numpy.float32)
    y = numpy.full((100,1), fill_value=1.23, dtype=numpy.float32)
    while True:
      tf.keras.backend.clear_session()
      model.fit(x=x, y=y)


**Other info / logs**
"
33311,tf.print usage throws SyntaxError: invalid syntax,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): n/a
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0
- Python version: 2.7 (Ubuntu 18.04 system default + latest pip from get-pip.py)
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

Wile playing with `tf.print` as described in docs, the following error happens:
```python
$ python
Python 2.7.15+ (default, Oct  7 2019, 17:39:04) 
[GCC 7.4.0] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> import sys
>>> tensor = tf.range(10) 
>>> tf.print(tensor, output_stream=sys.stderr)
  File ""<stdin>"", line 1
    tf.print(tensor, output_stream=sys.stderr)
           ^
SyntaxError: invalid syntax
>>>
```
**Describe the expected behavior**

`tf.print` should print out values as was specified in https://www.tensorflow.org/api_docs/python/tf/print


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import tensorflow as tf
import sys
tensor = tf.range(10) 
tf.print(tensor, output_stream=sys.stderr)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
33309,tf_upgrade_v2 is not reporting issues related to invalid imports in TensorFlow 2,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Catalina
- TensorFlow installed from (source or binary): binary (pip install tensorflow)
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7.4
- CUDA/cuDNN version: None
- GPU model and memory: Intel Iris Pro 1536 MB

**Describe the current behavior**

I am trying to use the script `tf_upgrade_v2` as follows 

```
tf_upgrade_v2 --infile bayesian_nn.py --outfile bayesian_nn2.py
```
The new file `bayesian_nn2.py` is created and the `report.txt` file is also generated, but `report.txt` does not report any issues, while in `bayesian_nn2.py` there is still code that cannot be run in TF2. For example, `report.txt` does not report that I have the import `from tensorflow.contrib.learn.python.learn.datasets import mnist`, which is no longer valid in TF2.

More specifically, this is my `report.txt`

```
TensorFlow 2.0 Upgrade Script
-----------------------------
Converted 1 files
Detected 0 issues that require attention
--------------------------------------------------------------------------------
================================================================================
Detailed log follows:

================================================================================
--------------------------------------------------------------------------------
Processing file 'bayesian_nn.py'
 outputting to 'bayesian_nn2.py'
--------------------------------------------------------------------------------
```

I am trying to port the following module https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/bayesian_neural_network.py to TF2.

According to the documentation, the upgrade script should report these issues: https://www.tensorflow.org/guide/upgrade, but it does not."
33308,iterating over 'tf.Tensor' is not allowed AutoGraph did not convert this function. ,"Hi, I am trying to convert below function to tensorflow graph using tf.function decorator.

However, I got an error message. 

My question is which part should I change the code below to use tf.function?

thanks. 

```python
@tf.function 
def band2bin(bands):
        tensor_list = [tf.tile(tf.expand_dims(bands[:, b, :], axis=1), [1, hp.band[b, 1]-hp.band[b, 0]+1, 1]) for b in tf.range(bands.shape[1])]
        bins = tf.concat(tensor_list, axis=1)
    return bins
```
error message: 
OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.


"
33307,No OpKernel was registered to support Op 'TPUReplicateMetadata' used by node TPUReplicateMetadata,"When I run the following .ipynb:

https://colab.research.google.com/drive/1DpUCBm58fruGNRtQL_DiSVbT90spdZgm

I got:
No OpKernel was registered to support Op 'TPUReplicateMetadata' used by node TPUReplicateMetadata (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) with these attrs: [num_cores_per_replica=1, use_tpu=true, num_replicas=8, computation_shape=[], host_compute_core=[], device_assignment=[], _tpu_replicate=""cluster"", padding_map=[], topology="""", step_marker_location=""STEP_MARK_AT_ENTRY"", allow_soft_placement=false]
Registered devices: [CPU, XLA_CPU]"
33306,"when run example: speech_commands and try to run freeze.py, Tensor name ""final_fc_bias"" not found in checkpoint files /tmp/speech_commands_train/conv.ckpt-2000.index","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
no

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
MacOS Sierra version 10.12.6

- TensorFlow installed from (source or binary):
binary

- TensorFlow version (use command below):
2.0 beta

- Python version:
2.7

- Run tf_env_collect.sh
== check python ===================================================
python version: 2.7.10
python branch: 
python build version: ('default', 'Feb  7 2017 00:08:15')
python compiler version: GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.34)
python implementation: CPython


== check os platform ===============================================
os: Darwin
os kernel version: Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64
os release version: 16.7.0
os platform: Darwin-16.7.0-x86_64-i386-64bit
linux distribution: ('', '', '')
linux os distribution: ('', '', '')
mac version: ('10.12.6', ('', '', ''), 'x86_64')
uname: ('Darwin', 's-macbook-air', '16.7.0', 'Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64', 'x86_64', 'i386')
architecture: ('64bit', '')
machine: x86_64


== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 9.0.0 (clang-900.0.38)
Target: x86_64-apple-darwin16.7.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== check pips ===================================================
numpy                                  1.8.0rc1

== check for virtualenv =========================================
False

== tensorflow import ============================================
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: No module named tensorflow

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf_env_collect.sh: line 147: nvidia-smi: command not found

== cuda libs  ===================================================

== tensorflow installed from info ==================

== python version  ==============================================
(major, minor, micro, releaselevel, serial)
(2, 7, 10, 'final', 0)

== bazel version  ===============================================
Build label: 0.29.1
Build time: Tue Sep 10 13:47:42 2019 (1568123262)
Build timestamp: 1568123262
Build timestamp as int: 1568123262

== check python ===================================================
python version: 2.7.10
python branch: 
python build version: ('default', 'Feb  7 2017 00:08:15')
python compiler version: GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.34)
python implementation: CPython


== check os platform ===============================================
os: Darwin
os kernel version: Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64
os release version: 16.7.0
os platform: Darwin-16.7.0-x86_64-i386-64bit
linux distribution: ('', '', '')
linux os distribution: ('', '', '')
mac version: ('10.12.6', ('', '', ''), 'x86_64')
uname: ('Darwin', 's-macbook-air', '16.7.0', 'Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64', 'x86_64', 'i386')
architecture: ('64bit', '')
machine: x86_64


== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 9.0.0 (clang-900.0.38)
Target: x86_64-apple-darwin16.7.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== check pips ===================================================
numpy                                  1.16.5              
protobuf                               3.9.1               
tensorflow                             2.0.0b0             

== check for virtualenv =========================================
True

== tensorflow import ============================================
tf.version.VERSION = 2.0.0-beta0
tf.version.GIT_VERSION = v1.12.1-3259-gf59745a381
tf.version.COMPILER_VERSION = 4.2.1 Compatible Apple LLVM 10.0.0 (clang-1000.11.45.5)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf_env_collect.sh: line 147: nvidia-smi: command not found

== cuda libs  ===================================================

== tensorflow installed from info ==================
Name: tensorflow
Version: 2.0.0b0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: /Users/liupai/tensorflow/lib/python2.7/site-packages
Required-by: 

== python version  ==============================================
(major, minor, micro, releaselevel, serial)
(2, 7, 10, 'final', 0)

== bazel version  ===============================================
Build label: 0.29.1
Build time: Tue Sep 10 13:47:42 2019 (1568123262)
Build timestamp: 1568123262
Build timestamp as int: 1568123262


**Describe the current behavior**
I am following the tutorials:
examples/speech_commands

python tensorflow/examples/speech_commands/train.py 
--how_many_training_steps=1500,500  
--data_dir=""speech_commands_v0.02"" 
--data_url=


python tensorflow/examples/speech_commands/freeze.py 
--start_checkpoint=/tmp/speech_commands_train/conv.ckpt-2000.index 
--output_file=/tmp/my_frozen_graph.pb

it failed with:
(tensorflow) s-macbook-air:tensorflow liupai$ python tensorflow/examples/speech_commands/freeze.py --start_checkpoint=/tmp/speech_commands_train/conv.ckpt-200.index --output_file=/tmp/my_frozen_graph.pb
2019-10-13 18:55:56.670314: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
WARNING:tensorflow:From /Users/liupai/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
W1013 18:55:56.782098 140736832123840 deprecation.py:323] From /Users/liupai/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
INFO:tensorflow:Restoring parameters from /tmp/speech_commands_train/conv.ckpt-200.index
I1013 18:55:56.784265 140736832123840 saver.py:1280] Restoring parameters from /tmp/speech_commands_train/conv.ckpt-200.index
Traceback (most recent call last):
  File ""tensorflow/examples/speech_commands/freeze.py"", line 250, in <module>
    tf.compat.v1.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/Users/liupai/tensorflow/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/Users/liupai/tensorflow/lib/python2.7/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/Users/liupai/tensorflow/lib/python2.7/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""tensorflow/examples/speech_commands/freeze.py"", line 176, in main
    models.load_variables_from_checkpoint(sess, FLAGS.start_checkpoint)
  File ""/Users/liupai/tensorflow/tensorflow/tensorflow/examples/speech_commands/models.py"", line 101, in load_variables_from_checkpoint
    saver.restore(sess, start_checkpoint)
  File ""/Users/liupai/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1302, in restore
    err, ""a Variable name or other graph key that is missing"")
tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

Tensor name ""final_fc_bias"" not found in checkpoint files /tmp/speech_commands_train/conv.ckpt-200.index
	 [[node save/RestoreV2 (defined at /Users/liupai/tensorflow/tensorflow/tensorflow/examples/speech_commands/models.py:100) ]]

Original stack trace for u'save/RestoreV2':
  File ""tensorflow/examples/speech_commands/freeze.py"", line 250, in <module>
    tf.compat.v1.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/Users/liupai/tensorflow/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/Users/liupai/tensorflow/lib/python2.7/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/Users/liupai/tensorflow/lib/python2.7/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""tensorflow/examples/speech_commands/freeze.py"", line 176, in main
    models.load_variables_from_checkpoint(sess, FLAGS.start_checkpoint)
  File ""/Users/liupai/tensorflow/tensorflow/tensorflow/examples/speech_commands/models.py"", line 100, in load_variables_from_checkpoint
    saver = tf.compat.v1.train.Saver(tf.compat.v1.global_variables())
  File ""/Users/liupai/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 825, in __init__
    self.build()
  File ""/Users/liupai/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 837, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/Users/liupai/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 875, in _build
    build_restore=build_restore)
  File ""/Users/liupai/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 508, in _build_internal
    restore_sequentially, reshape)
  File ""/Users/liupai/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 328, in _AddRestoreOps
    restore_sequentially)
  File ""/Users/liupai/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 575, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""/Users/liupai/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1696, in restore_v2
    name=name)
  File ""/Users/liupai/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/Users/liupai/tensorflow/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/Users/liupai/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3296, in create_op
    op_def=op_def)
  File ""/Users/liupai/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1692, in __init__
    self._traceback = tf_stack.extract_stack()

**Describe the expected behavior**
when run freeze.py, it should success according to the tutorials.

"
33305,layer reuse,"```

def head(self, input, num_anchors, name, flatten=False):

    out_channels = (self.num_classes + 4) * num_anchors
    conv = layers.Conv2D(256, 3, 1, 'same', activation='relu', name=name+'_conv1')(input)
    conv = layers.Conv2D(256, 3, 1, 'same', activation='relu', name=name+'_conv2')(conv)
    conv = layers.Conv2D(256, 3, 1, 'same', activation='relu', name=name+'_conv3')(conv)
    out = layers.Conv2D(out_channels, 3, 1, 'same', name=name+'output')(conv)
    return out

```

I want to know how to reuse these layers as tf.variable_scope(scope resue=tf.AUTO_REUSE) in tensorflow1

In tensorflow1

```
with tf.variable_scope('a', resue=tf.AUTO_REUSE) as scope:

         all layers here could be auto reuse

```
"
33304,tf.feature_column.indicator_column OOM,"
**Describe the current behavior**
i'm building a CTR model， use tf.feature_column.indicator_column  and tf.feature_column.categorical_column_with_hash_bucket  to make my  categorical feature turn into a multi-hot feature  .  But OOM happens , the error show that API indicator_column generate many one-hot feature , this cause the OOM.   
it seems produce many one-hot [batch_size ,categorical list len, bucket_size] then reduce_sum to multi-hot ,this will cost many memory ,how can i solve this problem?  
"
33303,Convert to tflite will change output order for models with multiple outputs,"Hello everyone, 

Currently I have a model with one input and multiple outputs. I train my model with tensorflow eager execution mode and saved trained model into keras format. after that I converted model to tflite format to use in mobile devices. The issue is when I converted my model to tflite the output order will changed. I tested same image with keras model and tflite model and the output order is different.
 
**System information**
- OS Platform and Distribution (Mac OS Mojave)
- TensorFlow version : 2.0.0
- Number of outputs: 16
- Base Model: MobilenetV1(0.25)


Here is my outputs for both keras and tflite :

tflite :6157037319861035

keras :6219861085157097

and here is a picture of output layer of my model :


![Screen Shot 2019-10-13 at 3 06 29 PM](https://user-images.githubusercontent.com/28183901/66715108-1c92ba00-edcc-11e9-8f26-eaa147e49e76.png)


"
33302,Tflite coverter error. tensorflow/lite/toco/tooling_util.cc:935,"<em>I'm trying to convert a TensorFlow model to tflite model from a frozen graph.
I' getting an error saying I should make a bug report to tflite team.

**you can download the graph from [HERE.**](https://drive.google.com/file/d/1LqyZWJsZadOHCxY8kG8JAskMoRgOWzSH/view?usp=sharing) 

any help would be much appreciated. </em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7

**Describe the current behavior**
tensorflow/lite/toco/tooling_util.cc:935] Check failed: GetOpWithOutput(model, output_array) Specified output array ""sample_sequence/while/Exit_3"" is not produced by any op in this graph. Is it a typo? This should not happen. If you trigger this error please send a bug report (with code to reporduce this error), to the TensorFlow Lite team.
Aborted (core dumped)
**Describe the expected behavior**

**Code to reproduce the issue**
**import tensorflow as tf
import sys
from tensorflow.python.platform import gfile

from tensorflow.core.protobuf import saved_model_pb2
from tensorflow.python.util import compat

graph_def_file = ""frozen_355.pb""
input_arrays = [""sample_sequence/model/Shape""]
output_arrays = [""sample_sequence/while/Exit_3""]

converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays)
tflite_model = converter.convert()
open(""converted_model_0.tflite"", ""wb"").write(tflite_model)**


**Other info / logs**
ConverterError                            Traceback (most recent call last)
<ipython-input-4-e27c2f171b9b> in <module>()
      4 
      5 converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays)
----> 6 tflite_model = converter.convert()
      7 open(""content/drive/My Drive/converted_model_0.tflite"", ""wb"").write(tflite_model)

2 frames
/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    198       stdout = _try_convert_to_unicode(stdout)
    199       stderr = _try_convert_to_unicode(stderr)
--> 200       raise ConverterError(""See console for info.\n%s\n%s\n"" % (stdout, stderr))
    201   finally:
    202     # Must manually cleanup files.

ConverterError: See console for info.
2019-10-13 11:18:54.608948: F tensorflow/lite/toco/tooling_util.cc:935] Check failed: GetOpWithOutput(model, output_array) Specified output array ""sample_sequence/while/Exit_3"" is not produced by any op in this graph. Is it a typo? This should not happen. If you trigger this error please send a bug report (with code to reporduce this error), to the TensorFlow Lite team.
Aborted (core dumped)

"
33301,tf.io.gfile.GFile.read() dose not return but terminates my ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS: Windows 10 18362
- TensorFlow installed from (source or binary): sourece
- TensorFlow version (use command below):  v2.0.0-rc2-26-g64c3d382ca 2.0.0
- Python version: python 3.68

**Describe the current behavior**
The func read() does not work (it return noting) and terminates my program. 
**Describe the expected behavior**
In the doc, read() shoud return the contents of the file as string
**Code to reproduce the issue**
`import tensorflow as tf `
`image = tf.io.gfile.GFile(r""..\datasets\icdar2015\img_1.jpg"", 'r')`
`image.read()`

My TF2.0 version below has the same problem with the beyond.
`image_data = tf.io.gfile.GFile(os.path.abspath(path), 'rb').read()`

My TF1.x version below run well 
`image_data = tf.gfile.FastGFile(os.path.abspath(path), 'rb').read()`
"
33299,"[tf2] isinstance(numeric_column, FeatureColumn)  == false","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): tf2
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
diffrent call code,  make `transformation_cache.get(self, state_manager)`  use different FeatureColumn, result in exception.

```python
transformation_cache.get(self, state_manager):
    if not isinstance(key, FeatureColumn):
      raise TypeError('""key"" must be either a ""str"" or ""FeatureColumn"". '
                      'Provided: {}'.format(key))
```

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```python
import tensorflow as tf
from tensorflow import feature_column
import tensorflow.keras as keras
from tensorflow.python.feature_column.feature_column import _LazyBuilder
from tensorflow_core.python.feature_column.feature_column_v2 import _StateManagerImpl, FeatureTransformationCache



def tnumeric():
    price = {'price': [[1.], [2.], [3.], [4.]]} 
    builder = _LazyBuilder(price)

    def transform_fn(x):
        return x + 2

    price_column = feature_column.numeric_column('price', normalizer_fn=transform_fn)

    from tensorflow.python.feature_column.feature_column_v2 import FeatureColumn
    print(isinstance(price_column, FeatureColumn))  # true
    from tensorflow_core.python.feature_column.feature_column_v2 import FeatureColumn
    print(isinstance(price_column, FeatureColumn))   # false

    feature_layer = keras.layers.DenseFeatures([price_column])
    a = feature_layer(price)  # use tensorflow,  true

    sm = _StateManagerImpl(feature_layer, feature_layer.trainable)
    ff=  FeatureTransformationCache(price)
    dt = price_column.get_dense_tensor(ff, sm) # use tensorflow_core, false

tnumeric()
```




**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
Traceback (most recent call last):
  File ""/Applications/PyCharm 2019.3 EAP.app/Contents/helpers/pydev/pydevd.py"", line 1415, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""/Applications/PyCharm 2019.3 EAP.app/Contents/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/Users/lqk/project/PycharmProjects/TF2/numeric.py"", line 49, in <module>
  File ""/Users/lqk/project/PycharmProjects/TF2/numeric.py"", line 35, in tnumeric
    
  File ""/Users/lqk/anaconda2/envs/p37t2/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py"", line 2845, in get_dense_tensor
    return transformation_cache.get(self, state_manager)
  File ""/Users/lqk/anaconda2/envs/p37t2/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py"", line 2604, in get
    'Provided: {}'.format(key))
TypeError: ""key"" must be either a ""str"" or ""FeatureColumn"". Provided: NumericColumn(key='price', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=<function tnumeric.<locals>.transform_fn at 0x11aa0f170>)

```
"
33298,Not able to install tensorflow,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Windows 7):

- TensorFlow installed from (source or binary):
- TensorFlow version:1.13.1
- Python version:3.7.3
- Installed using virtualenv? pip? conda?: pip

- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

When i install tensorflow and i import tensorflow ,i get the folowing error:
Traceback (most recent call last):
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed with error code -1073741795

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\site-packages\tensorflow\__init__.py"", line 98, in <module>
    from tensorflow_core import *
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed with error code -1073741795


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
33297,Surprising random seed behavior when using @tf.function,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOSX 10.14.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca 2.0.0
- Python version: 3.7.4
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

Random seeds work in surprising ways in TF 2.0 when using `@tf.function`. In particular, the value of the global random seed is only taken into account when a function is traced, not when it is called. This is surprising and different from TF 1.x behavior.

**Describe the expected behavior**
I expect the value of the global random seed to be taken into account every time a pseudo-random number is generated.

**Code to reproduce the issue**

```python
@tf.function
def rnd():
    return tf.random.uniform(shape=[])

tf.random.set_seed(42)
print(rnd()) # The rnd() function's seed is generated randomly now, based on
print(rnd()) # the current random seed (which is 42).
print()

tf.random.set_seed(43) # resets the random sequence but ignores this seed!
print(rnd())
print(rnd())
print()

tf.random.set_seed(42) # resets the random sequence but ignores this seed!
print(rnd())
print(rnd())
print()
```

The output value is:

```
tf.Tensor(0.63789964, shape=(), dtype=float32)
tf.Tensor(0.8774011, shape=(), dtype=float32)

tf.Tensor(0.63789964, shape=(), dtype=float32)
tf.Tensor(0.8774011, shape=(), dtype=float32)

tf.Tensor(0.63789964, shape=(), dtype=float32)
tf.Tensor(0.8774011, shape=(), dtype=float32)
```

Notice that we get the same sequence of random numbers every time, ignoring the value of the global random seed. The only value that matters is the first one (when the function gets traced).

More code and examples of surprising behavior in [this colab](https://colab.research.google.com/drive/1C3LZkt5hfO6T2Uo2xaYVG8hiNQL8c3xu).

**Other info / logs**

Specifically, I would expect the output to look the same as when the function is not decorated with `@tf.function`:

```
tf.Tensor(0.6645621, shape=(), dtype=float32)
tf.Tensor(0.68789124, shape=(), dtype=float32)

tf.Tensor(0.2733041, shape=(), dtype=float32)
tf.Tensor(0.5168259, shape=(), dtype=float32)

tf.Tensor(0.6645621, shape=(), dtype=float32)
tf.Tensor(0.68789124, shape=(), dtype=float32)
```

Note that the second sequence is different, as expected (in fact, the pseudo-random numbers should be identical whether the function is decorated or not, but that's a nice-to-have)."
33294,PackOp race condition / heisenbug,"tensorflow (1.x trunk) has a peculiar kernel: 
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/pack_op.cc#L168-L174
it is registered for the op ""Pack"" as a GPU kernel, but it is implemented on the host (CPU), and it takes its inputs in host memory. (There may be others like it, this is merely the one I noticed.)

Consider what happens when its input comes from the GPU memory. Graph builder will insert a HostRecv op, which copies the data from the GPU to the host. HostRecv is, by default, asynchronous.

After HostRecv, tensorflow will call BaseGPUDevice::Compute() on the PackOp kernel. 
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.cc#L479
Which will immediately proceed calling Compute() on the assumption that it is a GPU kernel that's going to be executed in the same stream (therefore synchronization is unnecessary).
Since HostRecv is asynchronous, the data may or may not be there by the time Pack attempts to read it, resulting in a race condition.

(There was some code in BaseGPUDevice::Compute() that did attempt waiting for inputs up until about 2 weeks ago, but it did not prevent the race condition either.)"
33293,Operators not supported by tensorflow lite.,"Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, ARG_MAX, CAST, CONCATENATION, CONV_2D, DIV, EQUAL, EXP, EXPAND_DIMS, FULLY_CONNECTED, GATHER, GATHER_ND, GREATER, GREATER_EQUAL, LESS, LOG, LOGICAL_AND, LOGISTIC, MAXIMUM, MAX_POOL_2D, MINIMUM, MUL, PACK, PAD, PADV2, RANGE, REDUCE_MAX, RESHAPE, RESIZE_NEAREST_NEIGHBOR, ROUND, SHAPE, SOFTMAX, SPARSE_TO_DENSE, SPLIT, SQRT, SQUEEZE, STRIDED_SLICE, SUB, SUM, TOPK_V2, TRANSPOSE_CONV, UNIQUE, WHERE. Here is a list of operators for which you will need custom implementations: CropAndResize, DenseToDenseSetOperation, Enter, Exit, LoopCond, Merge, NonMaxSuppressionV3, Switch, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3."
33290,Tensorboard Embedding Projector - non-Embedding Layers?,"I noticed at some point, the Tensorboard Callback stopped taking `embedding_layer_names`. Now it simply uses `isinstance(layer, Embedding)`

So my question is, how are we supposed to visualize the embeddings for say, an autoencoder, which would be a `Dense` layer?

The easiest solution would be to create a subclass of `layers.Embedding` that operates something like `tf.identity`, but obviously, this is hacky and not an ideal solution.

I think the ideal solution would be either to create a BaseEmbedding layer that doesn't modify anything, but allows users to insert into a model where they want embeddings visualized.

Or, adding something like `tf.summary.embedding` which seems like it would be fitting and semantic.

Here's the change in question:
https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/keras/callbacks.py#L1474-L1477
"
33289,Cannot import tensorflow.train,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOSX 10.14.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca 2.0.0
- Python version: 3.7.4
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

Importing tensorflow.train raises a `ModuleNotFoundError: No module named 'tensorflow.train'`.

**Describe the expected behavior**

No error.
Note: this used to work fine in tf-nightly-2.0-preview.

**Code to reproduce the issue**

```python
import tensorflow.train
```

**Other info / logs**

Traceback:

```
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-1-0b4f8a92bbad> in <module>
----> 1 import tensorflow.train

ModuleNotFoundError: No module named 'tensorflow.train'
```

**Workaround**

Instead of:

```python
from tensorflow.train import BytesList
```

use:

```python
import tensorflow as tf
BytesList = tf.train.BytesList
```
"
33287,image rotate,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):tensorflow2.0
- Are you willing to contribute it (Yes/No):



**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**

rotate an image with a degree as 
tf.contrib.image.rotate  in tensorflow1"
33285,Failing to download MNIST dataset at load_data(),"**System information**
- OS Platform and Distribution: macOS version10.15
- TensorFlow version: 2.0
- Python version: 3.7
- Installed using: pip install
- Bazel version (if compiling from source): 1.0.0



I have tried the first beginner example:

`from __future__ import absolute_import, division, print_function, unicode_literals

import tensorflow as tf

mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)

model.evaluate(x_test,  y_test, verbose=2)`

But it always seems to break at `(x_train, y_train), (x_test, y_test) = mnist.load_data()`. The programs run fine on Colaboratory but if I try to run in locally on Terminal, it fails. Seems to be a certification issue.

I have download installed everything, even upgraded just to be sure.


Here are the logs that are printed out:

Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py"", line 1317, in do_open
    encode_chunked=req.has_header('Transfer-encoding'))
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py"", line 1229, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py"", line 1275, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py"", line 1224, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py"", line 1016, in _send_output
    self.send(msg)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py"", line 956, in send
    self.connect()
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py"", line 1392, in connect
    server_hostname=server_hostname)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/ssl.py"", line 412, in wrap_socket
    session=session
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/ssl.py"", line 853, in _create
    self.do_handshake()
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/ssl.py"", line 1117, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1056)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/DanialZikri/venv37/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/data_utils.py"", line 251, in get_file
    urlretrieve(origin, fpath, dl_progress)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py"", line 247, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py"", line 222, in urlopen
    return opener.open(url, data, timeout)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py"", line 525, in open
    response = self._open(req, data)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py"", line 543, in _open
    '_open', req)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py"", line 503, in _call_chain
    result = func(*args)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py"", line 1360, in https_open
    context=self._context, check_hostname=self._check_hostname)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py"", line 1319, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1056)>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/DanialZikri/Documents/TensorflowFirst.py"", line 7, in <module>
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
  File ""/Users/DanialZikri/venv37/lib/python3.7/site-packages/tensorflow_core/python/keras/datasets/mnist.py"", line 50, in load_data
    '731c5ac602752760c8e48fbffcf8c3b850d9dc2a2aedcf2cc48468fc17b673d1')
  File ""/Users/DanialZikri/venv37/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/data_utils.py"", line 255, in get_file
    raise Exception(error_msg.format(origin, e.errno, e.reason))
Exception: URL fetch failure on https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz: None -- [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1056)
"
33283,pickling showing error : <<TypeError: can't pickle _thread._local objects>> working with tensorflow2.0 while it works with tensorflow 1.14,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 9.11 (stretch)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): via pip
- TensorFlow version (use command below): 2.0
- Python version: Python 3.7.3
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behaviour**
Currently, if I pickle in 2.0, it gives error TypeError: can't pickle _thread._local objects whereas in tensorflow version 1.14, it works fine

**Describe the expected behavior
[tp.txt](https://github.com/tensorflow/tensorflow/files/3720599/tp.txt)
**
pickling should work in in tf 2.0

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
in the attached file, run get_model method for tf2.0  and get_model_prev to run for tf1.14.
You will see that pickle model is generated for 1.14 but not for 2.0



**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
33281,Catnot catch `InvalidArgumentError` exception,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): - 
- TensorFlow version (use command below): 1.15 and 2.0
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`
v2.0.0-rc1-51-g2646d23 2.0.0-rc2

**Describe the current behavior**
Exception fails without being able to catch with 
`except tf.errors.InvalidArgumentError as exception:`

**Describe the expected behavior**
I expect, to be able to handle the mentioned example

**Code to reproduce the issue**
https://colab.research.google.com/drive/1IEg-KbY08dnCNnZSiuGb0-jToUDtQ_Vi#scrollTo=Xll7CUHiYn1B

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
33280,tensorflow 1.14 eigen using failed,"tensorflow version:1.14
tensorflow lib compile ok, but i use c api to embed into my app, make build error below:

In file included from _external/usr/local/include/tensorflow/unsupported/Eigen/CXX11/Tensor:120:0,
                 from _external/usr/local/include/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from _external/usr/local/include/tensorflow/tensorflow/core/framework/tensor.h:21,
                 from _external/usr/local/include/tensorflow/tensorflow/core/public/session.h:24,
                 from _external/usr/local/include/tensorflow/tensorflow/cc/saved_model/loader.h:26,
                 from ./apps/frame/util/tensorflow_util.h:7,
                 from build/release64/apps/frame/data_loader/tensorflow_loader.h:5,
                 from build/release64/apps/frame/data_loader/ServiceDataLoaderFactory.cpp:5:
_external/usr/local/include/tensorflow/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h: In static member function 'static void Eigen::internal::TensorBlockIO<Scalar, StorageIndex, NumDims, Layout, BlockRead>::Copy(const Block&, StorageIndex, const Dimensions&, const Dimensions&, const Scalar*, Scalar*)':
_external/usr/local/include/tensorflow/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h:393:63: error: the value of 'j' is not usable in a constant expression
         if (++block_iter_state[j].count < block_iter_state[j].size) {
                                                               ^
_external/usr/local/include/tensorflow/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h:392:16: note: 'int j' is not const
       for (int j = 0; j < num_squeezed_dims; ++j) {
                ^
_external/usr/local/include/tensorflow/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h:393:35: error: parse error in template argument list
         if (++block_iter_state[j].count < block_iter_state[j].size) {
                                   ^
_external/usr/local/include/tensorflow/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h: In static member function 'static void Eigen::internal::TensorBlockCwiseUnaryIO<UnaryFunctor, StorageIndex, OutputScalar, NumDims, Layout>::Run(const UnaryFunctor&, const Dimensions&, const Dimensions&, OutputScalar*, Eigen::array<StorageIndex, NumDims>&, const InputScalar*)':
_external/usr/local/include/tensorflow/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h:604:21: error: parse error in template argument list
         if (++state.count < state.size) {
                     ^
_external/usr/local/include/tensorflow/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h: In static member function 'static void Eigen::internal::TensorBlockCwiseBinaryIO<BinaryFunctor, StorageIndex, OutputScalar, NumDims, Layout>::Run(const BinaryFunctor&, const Dimensions&, const Dimensions&, OutputScalar*, Eigen::array<StorageIndex, NumDims>&, const LeftScalar*, Eigen::array<StorageIndex, NumDims>&, const RightScalar*)':
_external/usr/local/include/tensorflow/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h:758:21: error: parse error in template argument list
         if (++state.count < state.size) {

I try to replace it with 3.3.7 eigen, but tensorflow code is confict with 3.3.7 eigen.
Please help me!"
33279,TensorFlow import error,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
pip3 install --user tensorflow-gpu
- TensorFlow version:2.0
- Python version:3.7.3 64bit
- Installed using virtualenv? pip? conda?:pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.0,7.4.1
- GPU model and memory:



**Describe the problem**

Error after installing when importing tensorflow


C:\Users\Dr Edd>python -c ""import tensorflow as tf""
Traceback (most recent call last):
  File ""C:\Users\Dr Edd\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Dr Edd\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Dr Edd\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Dr Edd\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Dr Edd\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed with error code -1073741795

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\Dr Edd\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 98, in <module>
    from tensorflow_core import *
  File ""C:\Users\Dr Edd\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\Dr Edd\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\Dr Edd\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\Dr Edd\AppData\Local\Programs\Python\Python37\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\Dr Edd\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Dr Edd\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Dr Edd\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Dr Edd\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Dr Edd\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Dr Edd\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Dr Edd\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed with error code -1073741795


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
pip3 install --user tensorflow-gpu
python -c ""import tensorflow as tf""

**Any other info / logs**
"
33277,INT8 calibration error using TrtGraphConverterV2 in TensorFlow2.0,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install tensorflow-gpu
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda10.0
- GPU model and memory: TITAN V, 12GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Using TensorRT version in: 5.1.5
When using TrtGraphConverterV2 to convert a saved model and trying to calibrate with INT8 support, it will raise the error shown as follow.
Should be mentioned that, the FP16 and FP32 conversation is work fine but only the INT8 is failed.
```
2019-10-12 06:33:06.015951: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.5
2019-10-12 06:33:06.142792: E tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:838] Calibration failed: Invalid argument: Pad only supports explicit padding on 4 dimensional tensor, at StatefulPartitionedCall/resnet50/conv1_pad/Pad
2019-10-12 06:33:06.142978: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Failed to feed calibration data
         [[{{node TRTEngineOp_0}}]]
Traceback (most recent call last):
  File ""calibration.py"", line 16, in <module>
    converter.convert(calibration_input_fn=input_fn)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compiler/tensorrt/trt_convert.py"", line 984, in convert
    self._converted_func(*map(ops.convert_to_tensor, inp))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py"", line 1081, in __call__
    return self._call_impl(args, kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py"", line 1121, in _call_impl
    return self._call_flat(args, self.captured_inputs, cancellation_manager)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py"", line 1224, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py"", line 511, in call
    ctx=ctx)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py"", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError:  Failed to feed calibration data
         [[node TRTEngineOp_0 (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_pruned_79677]

Function call stack:
pruned

terminate called without an active exception
```
**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```python
from tensorflow.python.compiler.tensorrt import trt_convert as trt
import tensorflow as tf

model = tf.keras.applications.ResNet50(weights=None)
out = model(tf.random.normal((2, 224, 224, 3)))
tf.saved_model.save(model, './saved_model/')
print(model.input_names, model.output_names)

params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(precision_mode='INT8',
                                                    use_calibration=True)
converter = trt.TrtGraphConverterV2(input_saved_model_dir='./saved_model/', conversion_params=params)

def input_fn():
    for i in range(10):
        yield tf.random.normal((1, 224, 224, 3))
converter.convert(calibration_input_fn=input_fn)  #  raise error here
converter.save('./model.trt')
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
33275,Installation to shared (global) python env via Conda.,"I understand that Conda can be used to install older versions of Tensorflow, but it appears difficult with TF 2.0.  Most articles refer to creating the environment via Conda, but then installing via pip after the env is activated.  Given tales of problems when mixing install methods, it seems safer to find an installation method using Conda directly.  Looking for methods or projected timeline when this will be available.
"
33274,overlap backward and optimization in TensorFlow 2.0,"**System information**
- TensorFlow version (you are using): 2.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**

A typical training process in TensorFlow 2.0 could be as following:

```python
def train(model, dataset, optimizer):
  for x, y in dataset:
    with tf.GradientTape() as tape:
      prediction = model(x)
      loss = loss_fn(prediction, y)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
```
First, it does backward computation and gets all gradients. Then, the gradients will be updated in local, or send to a remote node to do optimization. The backward computation and gradient optimization are sequential.

In a distributed training scenario, sending gradients will be time-consuming. It's better to overlap communication and computation. Once one layer completes its backward computation, it could start sending its gradients out, and do optimization.

I am wondering if there is any approach to achieve such a goal.

Thanks!

**Will this change the current api? How?**

Or could we expose a callback interface, to let users insert customized processing logic after a gradient is calculated?

There are different cases:

- Case 1, optimization in the remote parameter server

```python
gradients = tape.gradient(loss, model.trainable_variables, send_callback)
```

- Case 2, gradients need to be averaged by allreduce operation across nodes.

```python
gradients = tape.gradient(loss, model.trainable_variables, allreduce_callback)
optimizer.apply_gradients(zip(gradients, model.trainable_variables))
```

- Case 3, gradients are applied in local

```python
gradients = tape.gradient(loss, model.trainable_variables, None)
optimizer.apply_gradients(zip(gradients, model.trainable_variables))
```

**Who will benefit with this feature?**

**Any Other info.**
"
33273,Loading ModelCheckpoint OSError -- 'Permission denied',"**System information**
- OS Platform and Distribution Win10
- TensorFlow version: 2.0
- Python version: 3.7.3
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: 25.21.14.1771
- GPU model and memory: GeForce RTX 2060 6GB

I have a model that has a ModelCheckpoint.
```
filepath = r""my_filepath""
checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=filepath, mode='max', monitor='val_accuracy', verbose=2, save_best_only=True)
callbacks_list = [checkpoint]
model.fit(train_dataset, validation_data=y_test_dataset, validation_steps=BATCH_SIZE, callbacks=callbacks_list, epochs=5, verbose=2, steps_per_epoch=(X_train_deleted_nans.shape[0]//BATCH_SIZE))
```
The model is saved to a directory similar to the tutorial [here](https://www.tensorflow.org/tutorials/keras/save_and_load#checkpoint_callback_usage).

I make another model exactly like the first one and when I 
`new_model.load_weights(filepath)`

I get:

`OSError: Unable to open file (unable to open file: name = 'my_filepath', errno = 13, error message = 'Permission denied', flags = 0, o_flags = 0)`"
33269,TFTRT : Dynamic batch size creates  new TRT nodes for each batch size.,"

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.14
- Python version:2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.1
- GPU model and memory:T4

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
It seems that  when the batch size is dynamic, every new batch size in the input creates a slew of TRTEngine_op nodes.  Seems to be a very crude and non-scalable way to handle dynamic batching. 

**Describe the expected behavior**
One node for all batch sizes.

**Code to reproduce the issue**
Any network, just feed  dynamic batch sizes. 

**Other info / logs**
N/A
"
33267,An application using tensorflow_cc.dll crashes whenever the Tensorflow API is invoked.,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Windows 10
- TensorFlow installed from: source
- TensorFlow version: tensorflow v2.0.0 commit 64c3d382cadf7bbe8e7e99884bede8284ff67f56 (HEAD, tag: v2.0.0, origin/r2.0)
- Python version: 3.6.9
- Bazel version: 0.24.1
- Compiler version: Visual Studio, 2017 14.16.27023
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

A tensorflow dynamic link library is built using 

`bazel build --config=opt //tensorflow:tensorflow_cc.dll`

with the default configuration (no GPU), except that /arch:AVX2 is used instead of /arch:AVX. The patch 

	diff --git a/tensorflow/tools/def_file_filter/def_file_filter.py.tpl b/tensorflow/tools/def_file_filter/def_file_filter.py.tpl
	index 329a9bb94e..82146ec933 100644
	--- a/tensorflow/tools/def_file_filter/def_file_filter.py.tpl
	+++ b/tensorflow/tools/def_file_filter/def_file_filter.py.tpl
	@@ -154,6 +154,9 @@ def main():
		   else:
			 def_fp.write(""\t"" + decorated + "" DATA\n"")
		   taken.add(decorated)
	+    def_fp.write(""\t??0SessionOptions@tensorflow@@QEAA@XZ\n"")
	+    def_fp.write(""\t?LoadSavedModel@tensorflow@@YA?AVStatus@1@AEBUSessionOptions@1@AEBVRunOptions@1@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBV?$unordered_set@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@U?$hash@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@U?$equal_to@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@6@QEAUSavedModelBundle@1@@Z\n"")
	+
		 def_fp.close()

	   exit_code = proc.wait()

is applied to the source before the build in order to account for missing symbols (The need for this patch should perhaps be filed in a separate bug-report).

**Describe the current behavior**
After much work I was able to build `tensorflow_cc.dll` and linking it to a custom application without errors. However, the application using the `.dll` crashes whenever `LOG(INFO/WARNING/ERROR)` is called.
After inspecting the source code, I see that the macro `LOG(ERROR)` expands to a constructor of the class `tensorflow::internal::LogMessage` which inherits from `std::basic_ostringstream<char>`.
A heap error occurs when the destructor of the base class deletes its internal buffer.

The error also seems to occur when calling any function in the tensorflow api which in turn calls the log function, effectively rendering the `.dll` useless.

**Describe the expected behavior**
The application should not crash when calling (almost) any function in the tensorflow api.

I am no expert on dynamic libraries on Windows, but I have read that the `.dll` and the `.exe` have separate heaps. One might speculate that the error could have something to do with this (but in that case I do not understand how). Has anyone actually managed to use `tensorflow_cc.dll` (on Windows) at all? I don't have the same issue on Linux.

A possible workaround might be to build a static `tensorflow_cc.lib`. Is this possible? For my purposes, it does not matter much if I use a dynamically or a statically linked library.

**Code to reproduce the issue**
```c++
#define NOMINMAX
#define COMPILER_MSVC
#include ""tensorflow/core/platform/logging.h""

int main(int argc, char* argv[]) {
	LOG(ERROR) << ""Test"";
}
```

**Other info / logs**
Here is the stack trace produced by the debugger when the crash occurs.

	ntdll.dll!RtlpBreakPointHeap()
	ntdll.dll!RtlpValidateHeapEntry()
	ntdll.dll!RtlValidateHeap()
	KernelBase.dll!HeapValidate()
	ucrtbased.dll!_CrtIsValidHeapPointer(const void * block) Line 1407
		at minkernel\crts\ucrt\src\appcrt\heap\debug_heap.cpp(1407)
	ucrtbased.dll!free_dbg_nolock(void * const block, const int block_use) Line 904
		at minkernel\crts\ucrt\src\appcrt\heap\debug_heap.cpp(904)
	ucrtbased.dll!_free_dbg(void * block, int block_use) Line 1030
		at minkernel\crts\ucrt\src\appcrt\heap\debug_heap.cpp(1030)
	ucrtbased.dll!free(void * block) Line 32
		at minkernel\crts\ucrt\src\appcrt\heap\free.cpp(32)
	msvcp140d.dll!std::_Crt_new_delete::operator delete(void * _Ptr) Line 73
		at d:\agent\_work\3\s\src\vctools\crt\crtw32\stdhpp\xlocale(73)
	msvcp140d.dll!std::locale::`scalar deleting destructor'(unsigned int)
	msvcp140d.dll!std::ios_base::_Ios_base_dtor(std::ios_base * _This) Line 44
		at d:\agent\_work\3\s\src\vctools\crt\crtw32\stdcpp\ios.cpp(44)
	msvcp140d.dll!std::ios_base::~ios_base() Line 478
		at d:\agent\_work\3\s\src\vctools\crt\crtw32\stdhpp\xiosbase(478)
	msvcp140d.dll!std::basic_ios<char,std::char_traits<char> >::~basic_ios<char,std::char_traits<char> >() Line 34
		at d:\agent\_work\3\s\src\vctools\crt\crtw32\stdhpp\ios(34)
	main.exe!tensorflow::internal::LogMessage::`vbase destructor'()
	main.exe!main(int argc, char * * argv) Line 7
		at (PATH_TO_PROJECT)\main.cpp(7)
"
33266,Installation of tensorflow 2.0 cpu on windows 10 not working,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64 bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): pip install tensorflow
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7.4
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Installation of tensorflow 2.0 cpu on windows 10 not working. I'm unable to import tensorflow.
**Describe the expected behavior**
Run the command ""python -c ""import tensorflow as tf"" within conda CLI without error.
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
1) 
#from conda CLI base create new environment
conda create --name tensorflow2_test
2)
#activate environment
conda activate tensorflow2_test
3)
#install pip
conda install pip
4)
#install environment tensorflow 2.0
pip install tensorflow
5)
#verify install: 
python -c ""import tensorflow as tf""

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

error trace:
------------
```
(tensorflow2_test) C:\~\install environment\production>python -c ""import tensorflow as tf""
Traceback (most recent call last):
  File ""C:\Anaconda3\envs\tensorflow2_test\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Anaconda3\envs\tensorflow2_test\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Anaconda3\envs\tensorflow2_test\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Anaconda3\envs\tensorflow2_test\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Anaconda3\envs\tensorflow2_test\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Anaconda3\envs\tensorflow2_test\lib\site-packages\tensorflow\__init__.py"", line 98, in <module>
    from tensorflow_core import *
  File ""C:\Anaconda3\envs\tensorflow2_test\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Anaconda3\envs\tensorflow2_test\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Anaconda3\envs\tensorflow2_test\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Anaconda3\envs\tensorflow2_test\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Anaconda3\envs\tensorflow2_test\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Anaconda3\envs\tensorflow2_test\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Anaconda3\envs\tensorflow2_test\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Anaconda3\envs\tensorflow2_test\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Anaconda3\envs\tensorflow2_test\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Anaconda3\envs\tensorflow2_test\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Anaconda3\envs\tensorflow2_test\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

(tensorflow2_test) C:\~\install environment\production>
```
package list:
-------------
(tensorflow2_test) C:\~\install environment\production>conda list
# packages in environment at C:\Anaconda3\envs\tensorflow2_test:
#
# Name                    Version                   Build  Channel
absl-py                   0.8.1                    pypi_0    pypi
astor                     0.8.0                    pypi_0    pypi
ca-certificates           2019.8.28                     0
certifi                   2019.9.11                py37_0
gast                      0.2.2                    pypi_0    pypi
google-pasta              0.1.7                    pypi_0    pypi
grpcio                    1.24.1                   pypi_0    pypi
h5py                      2.10.0                   pypi_0    pypi
keras-applications        1.0.8                    pypi_0    pypi
keras-preprocessing       1.1.0                    pypi_0    pypi
markdown                  3.1.1                    pypi_0    pypi
numpy                     1.17.2                   pypi_0    pypi
openssl                   1.1.1d               he774522_2
opt-einsum                3.1.0                    pypi_0    pypi
pip                       19.2.3                   py37_0
protobuf                  3.10.0                   pypi_0    pypi
python                    3.7.4                h5263a28_0
setuptools                41.4.0                   py37_0
six                       1.12.0                   pypi_0    pypi
sqlite                    3.30.0               he774522_0
tensorboard               2.0.0                    pypi_0    pypi
tensorflow                2.0.0                    pypi_0    pypi
tensorflow-estimator      2.0.0                    pypi_0    pypi
termcolor                 1.1.0                    pypi_0    pypi
vc                        14.1                 h0510ff6_4
vs2015_runtime            14.16.27012          hf0eaf9b_0
werkzeug                  0.16.0                   pypi_0    pypi
wheel                     0.33.6                   py37_0
wincertstore              0.2                      py37_0
wrapt                     1.11.2                   pypi_0    pypi

(tensorflow2_test) C:\~\install environment\production>
"
33265,Build of Tensorflow 2.0 from source fails on Windows “Could not find bazel-bin”,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0
- Python version: 3.7.4
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): Visual Studio 2017
- CUDA/cuDNN version: None (CPU-only)
- GPU model and memory: None (CPU-only)



**Describe the problem**
_(Also posted on Stackoverflow [here](https://stackoverflow.com/questions/58344865/build-of-tensorflow-2-0-from-source-fails-on-windows-could-not-find-bazel-bin) as I'm not sure the correct place to post)_

I'm following the guide [Build from source on Windows](https://www.tensorflow.org/install/source_windows#build_the_package) in order to get AVX2 support of Tensorflow 2.0. I successfully ran the bazel build command, but haven't been able to run build_pip_package successfully.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

I followed the instructions in the Build From Source guide for the most part. I cloned the r2.0 branch of tensorflow from git and put it in c:\tmp\tensorflow. I installed the python and the tensorflow package dependencies. I installed bazel, msys2, and Visual Studio 2017 (with build tools). I configured the system build (cpu-only with AVX2 support). I then successfully ran:

```
cd c:\tmp\tensorflow
set BAZEL_VS=C:\Program Files (x86)\Microsoft Visual Studio\2017\Community
set BAZEL_VC=C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --define=no_tensorflow_py_deps=true
```

The next step in the instructions ""Build the package"" says to run
```
bazel-bin\tensorflow\tools\pip_package\build_pip_package C:/tmp/tensorflow_pkg
```

However, the folder C:\tmp\tensorflow\bazel-bin is empty. When I look at the log (shown below), I can see that there's a file build_pip_package.exe at C:/users/john.doe/_bazel_john.doe/3ttaaxce/execroot/org_tensorflow/bazel-out/x64_windows-opt/bin/tensorflow/tools/pip_package/build_pip_package.exe. I tried running the following:
```
C:/users/john.doe/_bazel_john.doe/3ttaaxce/execroot/org_tensorflow/bazel-out/x64_windows-opt/bin/tensorflow/tools/pip_package/build_pip_package C:/tmp/tensorflow_pkg
```
But this results in the error:
>Fri Oct 11 08:30:40 PDT 2019 : === Preparing sources in dir: /tmp/tmp.B207TraE5w
>
>Could not find bazel-bin. Did you run from the root of the build tree?

Should there be a bazel-bin folder with the implied substructure somewhere (bazel-bin\tensorflow\tools\pip_package\build_pip_package)?

**Any other info / logs**

Pasting the last few lines of build output:
```
INFO: From Linking tensorflow/lite/experimental/microfrontend/python/ops/_audio_microfrontend_op.so:
   Creating library bazel-out/x64_windows-opt/bin/tensorflow/lite/experimental/microfrontend/python/ops/python/ops/_audio_microfrontend_op.so.if.lib and object bazel-out/x64_windows-opt/bin/tensorflow/lite/experimental/microfrontend/python/ops/python/ops/_audio_microfrontend_op.so.if.exp
INFO: From Linking tensorflow/compiler/tf2xla/ops/_xla_ops.so:
   Creating library bazel-out/x64_windows-opt/bin/tensorflow/compiler/tf2xla/ops/_xla_ops.so.if.lib and object bazel-out/x64_windows-opt/bin/tensorflow/compiler/tf2xla/ops/_xla_ops.so.if.exp
Target //tensorflow/tools/pip_package:build_pip_package up-to-date:
  C:/users/john.doe/_bazel_john.doe/3ttaaxce/execroot/org_tensorflow/bazel-out/x64_windows-opt/bin/tensorflow/tools/pip_package/build_pip_package
  C:/users/john.doe/_bazel_john.doe/3ttaaxce/execroot/org_tensorflow/bazel-out/x64_windows-opt/bin/tensorflow/tools/pip_package/build_pip_package.exe
INFO: Elapsed time: 32810.307s, Critical Path: 25586.71s
INFO: 8885 processes: 8885 local.
INFO: Build completed successfully, 11312 total actions
```
"
33264,Difference between Tensor and EagerTensor?,"[Relevant issue](https://github.com/tensorflow/tensorflow/issues/33227) w/ full code; excerpt:

```python
var_delta = m_t / (K.sqrt(v_t) + epsilon_t)
var_update = state_ops.assign_sub(var, lr_t * var_delta, use_locking=self._use_locking)
```
Above fails to update `var`, but using `math_ops.sqrt` fixes it. To differentiate, below are some `print` statements & their outputs. Code also included demonstrating introspection limitation.

It appears that `K.sqrt` yields a Keras `Tensor`, whereas `math_ops.sqrt` yields a `tf.Tensor`, with former not showing its value even though eager is on. However, calling `K.eval` on `K.sqrt` shows the same value, so it isn't 'lost' or different. 

All considered: why does `state_ops.assign_sub` work with an `EagerTensor` but fail with a `Tensor`? How are they different? -- (TF2, Keras 2.3.0)

<hr>

```python
X1 = math_ops.sqrt(v_t)
X2 = K.sqrt(v_t)
Y1 = m_t / (math_ops.sqrt(v_t) + epsilon_t)
Y2 = m_t / (K.sqrt(v_t) + epsilon_t)

print(X1);       print(X2);       print()
print(Y1);       print(Y2);       print()
print(type(X1)); print(type(X2)); print()
print(type(Y1)); print(type(Y2)); print()
print(""type(ref) ="", type(var))
```
```python
tf.Tensor(
[[0.0002048  0.00189643]
 [0.00204984 0.00426161]
 [0.0013686  0.0048186 ]
 [0.00296201 0.00318883]], shape=(4, 2), dtype=float32)
Tensor(""Sqrt:0"", shape=(4, 2), dtype=float32, device=/job:localhost/
replica:0/task:0/device:CPU:0)

tf.Tensor(
[[ 3.1607556 -3.1621318]
 [-3.162145  -3.1622243]
 [ 3.1620677  3.162233 ]
 [-3.1621926 -3.1621997]], shape=(4, 2), dtype=float32)
Tensor(""truediv:0"", shape=(4, 2), dtype=float32, device=/job:localhost/
replica:0/task:0/device:CPU:0)

<class 'tensorflow.python.framework.ops.EagerTensor'>
<class 'tensorflow.python.framework.ops.Tensor'>

<class 'tensorflow.python.framework.ops.EagerTensor'>
<class 'tensorflow.python.framework.ops.Tensor'>

type(ref) = <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>
```

<hr>

**UPDATE**: The _type_ information may be more relevant; (`tf.`) `Tensor` vs. (Keras) `EagerTensor`. However, one lacks reasonable introspection:

```python
from tensorflow.python.framework.ops import Tensor, EagerTensor
print(Tensor.__doc__) # OK
print(EagerTensor.__doc__) # returns None

from inspect import getsource
print(getsource(EagerTensor)) # OSError: could not find class definition
```
Looking into the parent module, apparently I'll need to look into the C API to find out:

```python
EagerTensor = c_api.TFE_Py_InitEagerTensor(_EagerTensorBase)
```"
33261,Can't save a Model with a TimeDistributed layer wrapping another Model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution: Linux Ubuntu 18.04.2
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.0.0
- Python version: 3.7.3
- CUDA/cuDNN version: CUDA 10.1 cuDNN 7.5.1
- GPU model and memory: TITAN X

**Describe the current behavior**

I get a `ValueError` when trying to save a `tf.keras.Model` with a `tf.keras.layers.TimeDistributed` layer wrapping another `tf.keras.Model` that has convolutional layers. I am using `tf.keras.Model.save` with the default `save_format` (SavedModel). See below for examples.

There is no error when saving with `save_format='h5'`.

**Describe the expected behavior**

Successfully saving a SavedModel with the `tf.keras.layers.TimeDistributed` layer.

**Code to reproduce the issue**

1.  Wrapping a 1-layer convolutional NN with `tf.keras.layers.TimeDistributed`:

    ```python
    import tensorflow as tf

    input_shape = (100, 100, 3)

    embedding_model = tf.keras.Sequential([
        tf.keras.layers.Input(input_shape),
        tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=1),
    ])

    input_sequence = tf.keras.layers.Input((None,) + input_shape)
    sequence_embedding = tf.keras.layers.TimeDistributed(embedding_model)
    outputs = sequence_embedding(input_sequence)

    model = tf.keras.Model(inputs=input_sequence, outputs=outputs)

    model.save('model1')
    ```

    Error:

    ```
    ValueError: Input 0 of layer conv2d is incompatible with the layer: expected ndim=4, found ndim=5. Full shape received: [None, None, 100, 100, 3]
    ```

2.  Wrapping a pre-trained `tf.keras.applications` model (closer to my actual use case):

    ```python
    import tensorflow as tf

    input_shape = (224, 224, 3)

    mobilenet = tf.keras.applications.MobileNet(
        input_shape=input_shape,
        include_top=False,
        weights='imagenet',
        pooling='avg',
    )

    input_sequence = tf.keras.layers.Input((None,) + input_shape)
    sequence_embedding = tf.keras.layers.TimeDistributed(mobilenet)
    outputs = sequence_embedding(input_sequence)

    model = tf.keras.Model(inputs=input_sequence, outputs=outputs)

    model.save('model2')
    ```

    Error:

    ```
    ValueError: Input 0 of layer conv1_pad is incompatible with the layer: expected ndim=4, found ndim=5. Full shape received: [None, None, 224, 224, 3]
    ```

3.  Saving as an HDF5 file instead:

    ```python
    import tensorflow as tf

    input_shape = (224, 224, 3)

    mobilenet = tf.keras.applications.MobileNet(
        input_shape=input_shape,
        include_top=False,
        weights='imagenet',
        pooling='avg',
    )

    input_sequence = tf.keras.layers.Input((None,) + input_shape)
    sequence_embedding = tf.keras.layers.TimeDistributed(mobilenet)
    outputs = sequence_embedding(input_sequence)

    model = tf.keras.Model(inputs=input_sequence, outputs=outputs)

    model.save('model3.h5', save_format='h5')
    ```

    This works without errors.

4.  Saving to the SavedModel format works with just dense layers:

    ```python
    import tensorflow as tf

    input_shape = (100,)

    embedding_model = tf.keras.Sequential([
        tf.keras.layers.Input(input_shape),
        tf.keras.layers.Dense(units=10)
    ])

    input_sequence = tf.keras.layers.Input((None,) + input_shape)
    sequence_embedding = tf.keras.layers.TimeDistributed(embedding_model)
    outputs = sequence_embedding(input_sequence)

    model = tf.keras.Model(inputs=input_sequence, outputs=outputs)

    model.save('model4')
    ```

    This works without errors.
"
33260,Keras does not verify supports_masking,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OS X Mojave
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): ""pip install""
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca 2.0.0
- Python version: 3.7.4
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
I add an Embedding layer with mask_zero=True. Then I add another layer, either Flatten or GlobalAvgPool1D or GlobalMaxPool1D.
Of these, only GlobalAvgPool1D supports masking.
But when I compile and fit the model, no error is raised. (I believe in the past, prior to TF 2.0, an error would be raised.)

**Describe the expected behavior**
An error should be raised when using a layer that doesn't support masking on top of a layer that performs masking.

**Code to reproduce the issue**
Code for repro: https://github.com/asadovsky/nn/blob/master/text_classification.py
When setting ""arch"" to ""max_pool"" or ""flatten"", the train_and_evaluate_model(hp) function should raise an error. But instead, no error is raised."
33258,keras.layers.LSTM does not work with model.evaluate after training,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9.11 (GCE DeepLearning image)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0
- Python version: 3.7.3 (anaconda3)
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: V10.0.130 + 7.6.4.38
- GPU model and memory: V100 16GB 

**Describe the current behavior**
model.evaluate raises this error after training
```Traceback (most recent call last):
  File ""lstm.py"", line 76, in <module>
    model.evaluate([left, right], labels)
  File ""/home/zhaohaozeng/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py"", line 833, in evaluate
    use_multiprocessing=use_multiprocessing)
  File ""/home/zhaohaozeng/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 456, in evaluate
import typing
    sample_weight=sample_weight, steps=steps, callbacks=callbacks, **kwargs)
  File ""/home/zhaohaozeng/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 444, in _model_iteration
    total_epochs=1)
  File ""/home/zhaohaozeng/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 123, in run_one_epoch
    batch_outs = execution_function(iterator)
  File ""/home/zhaohaozeng/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 86, in execution_function
    distributed_function(input_fn))
  File ""/home/zhaohaozeng/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 457, in __call__
    result = self._call(*args, **kwds)
  File ""/home/zhaohaozeng/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 526, in _call
    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
  File ""/home/zhaohaozeng/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 1141, in _filtered_call
    self.captured_inputs)
  File ""/home/zhaohaozeng/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 1224, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager)
  File ""/home/zhaohaozeng/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 511, in call
    ctx=ctx)
  File ""/home/zhaohaozeng/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py"", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.NotFoundError: 2 root error(s) found.
  (0) Not found:  Resource AnonymousIterator/AnonymousIterator1/N10tensorflow4data16IteratorResourceE does not exist.
	 [[node IteratorGetNext (defined at /home/zhaohaozeng/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]
	 [[IteratorGetNext/_45]]
  (1) Not found:  Resource AnonymousIterator/AnonymousIterator1/N10tensorflow4data16IteratorResourceE does not exist.
	 [[node IteratorGetNext (defined at /home/zhaohaozeng/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]
0 successful operations.
0 derived errors ignored. [Op:__inference_distributed_function_11390]

Function call stack:
distributed_function -> distributed_function
```


**Describe the expected behavior**
`model.evaluate` should not raise this error after training.

**Code to reproduce the issue**
Code and log:  https://gist.github.com/matthew-z/e5848d545b60792dd84bfb9470ea541f

I tested with this machine on GCE:
* n1-standard-4 
* 1 x NVIDIA Tesla V100
* image: c1-deeplearning-common-cu100-20191003 (Google Deep Learning VM, common-dl-gpu)
* installed anaconda3, CUDNN 7.6.4.38 (got some errors with the original 7.4 CUDNN in the image), tensorflow-gpu 2.0

**Other info / logs**
The problem may not happen with CPU."
33255,tcmalloc: large alloc on  Colab and Tensorflow killed on local machine due to over consumption of RAM,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Conda
- TensorFlow version (use command below): tensorflow-gpu version 1.9.0
- Python version: 3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version:  V10.1.243
- GPU model and memory: Quadro RTX 5000; and 16 GB RAM

**Describe the current behavior**

The tensorflow API always tries to consume the maximum RAM even when I have a GPU and the kernel gets killed while training my deep learning algorithm. I referred online on multiple sources ([1](https://stackoverflow.com/questions/45077571/tensorflow-training-killed-by-system), [2](https://stackoverflow.com/questions/42205205/tensorflow-python-script-getting-killed), [3](https://stackoverflow.com/questions/49442670/my-process-being-killed-the-moment-it-start-training-tensorflow-object-detectio), [4](https://stackoverflow.com/questions/45150773/tensorflow-object-detection-training-killed-resource-starvation), [5](https://github.com/tensorflow/models/issues/3497), [6](https://github.com/tensorflow/tensorflow/issues/29365)) and tried the following things :

1.  Reduce the batch size
2. Change the optimizer from adam to momentum

However, none of these suggestions helped to solve the problem.

**Describe the expected behavior**

Be able to train without over consumption of memory and not cause the tensorflow kernel to get killed

**Code to reproduce the issue**

Provide a reproducible test case that is the bare minimum necessary to generate the problem.

I ran the following code in an ipython notebook in both my local machine (local GPU) and Google Colab :
```
!git clone https://github.com/charlesq34/pointnet.git
cd pointnet/sem_seg/
!sh download_data.sh
!python train.py --log_dir log6 --test_area 6
```
**Other info / logs**

The error log is very long and hence I am attaching it in a separate text file here : 
[ERROR_LOG.txt](https://github.com/tensorflow/tensorflow/files/3718556/ERROR_LOG.txt)

"
33254,Share memory between numpy and tensorflow doesn't work,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Python
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0.0
- Python version: 3.6.8

**Describe the current behavior**
As described in introduction, tf will try to share memory between tf and numpy when possible. However I couldn't figure out how to do this.

```python
a = tf.constant([3, 4]).cpu()
b = tf.numpy()
b[0] = 1
print(a)
# [3, 4]
print(b)
# [1, 4]
```


**Describe the expected behavior**
```python
a = tf.constant([3, 4]).cpu()
b = tf.numpy()
b[0] = 1
print(a)
# [1, 4]
print(b)
# [1, 4]
```

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
33253,sqlite dataset fails to raise a StopIteration: incorrect result,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): issue is reproducible with both
- TensorFlow version (use command below): Confirmed on 2.0 and 1.14 
- Python version: python2

**Describe the current behavior**
After creating a `tf.data.experimental.SqlDataset()` and performing a map and batch operaion, the dataset fails to raise a StopIteration after going through the entire database, and begins to repeat / recycle values incorrectly. 

**Describe the expected behavior**
The dataset stops after returning all the records in the sqlite database.

**Code to reproduce the issue**
```python
# --- Create a dummy sqlite database ---
import os
import sqlite3
pth = ""/tmp/bug-report.sqlite""
if os.path.exists(pth): os.unlink(pth)

query = 'SELECT * FROM data'

con = sqlite3.connect(pth)
c = con.cursor()
c.execute('CREATE TABLE data (col1 Int)')

for i in range(3):
  c.execute('INSERT INTO data VALUES (' + str(i) + ')')

con.commit()

# print the db, just to show what's in there
c.execute(query)
print ""Actual query results: "", c.fetchall()
con.close()

# --- create a tf sqlite dataset ---
import tensorflow as tf
print tf.version.VERSION

ds = tf.data.experimental.SqlDataset('sqlite', pth, query, (tf.int32))
ds = ds.map(lambda x: tf.identity(x))


# this is supposed to terminate after only two batchs since the sqlite db only
# has 2 entries, but it goes forever
print ""Batch size of 2:""
i = 0
for e in ds.batch(2):
  print e
  
  i += 1
  if i > 2: print ""  Should have stopped by now""
  if i > 10: print ""    breaking early""; break
  
# if batch size is larger than the db size, it also fails to stop
print ""Batch size of 4:""
i = 0
for e in ds.batch(4):
  print e
  
  i += 1
  if i > 1: print ""  Should have stopped by now""
  if i > 10: print ""    breaking early""; break

# if batch size is exactly a multiple of the sqlite db size, then it does
# raise a StopIteration correctly
print ""Batch size of 3:""
for e in ds.batch(3):
  print e
```

![image](https://user-images.githubusercontent.com/8462255/66663537-ecef8080-ec18-11e9-84fd-afd02c8a66ab.png)


**Other info / logs**
This has been reproduced on tensorflow versions 2.0, 1.14 and 1.15
"
33252,Variable name misspelt in example," Loading is spelled loadimg in one example(pose2seg)  And set to False. But loading is expected.  A text search will get you there. Although its in cluster_pose.py.  not a biggy, i know, still, one of those things that may create a problem if someone assumes its set"
33251,How to confiture TF2.0 compilation with Intel MPI ?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: none
- TensorFlow installed from (source or binary): source (configuration process pbm)
- TensorFlow version: 2.0
- Python version: 3.6.9
- Installed using virtualenv? pip? conda?: not concerned
- Bazel version (if compiling from source): 0.29.1
- GCC/Compiler version (if compiling from source): 4.8.5
- CUDA/cuDNN version: 10.0 / 7.6.4 
- GPU model and memory: Tesla K80 / 256Gb

**Describe the problem**
problem in ./configure when asking for MPI Toolkit path
I found description howto for openmpi (https://github.com/robsanpam/TensorFlow_From_Sources), but I want to use intel mpi.
what I need to indicate in path for intel mpi (intel cluster edition 2019) ?

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I input : /opt/intel/compilers_and_libraries/linux/mpi/intel64

**Any other info / logs**
Invalid path to the MPI Toolkit. /opt/intel/compilers_and_libraries/linux/mpi/intel64 or True or False or False cannot be found
"
33250,ValueError: Tensor Tensor by tf.keras.backend.function,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave
- TensorFlow installed from (source or binary): pip install tensorflow
- TensorFlow version: Tensorflow 2.0.0
- Python version: Python 3.7.2
- Installed using virtualenv? pip? conda?: Anaconda

**Describe the problem**
> ValueError: Tensor Tensor(""Mean:0"", shape=(512,), dtype=float32) is not an element of this graph.

I'm run this line of code show above error message, `iterate = tf.keras.backend.function([model.input], [pooled_grads, last_conv_layer.output[0]])`


**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions
import numpy as np
import tensorflow as tf

tf.compat.v1.disable_eager_execution()
tf.keras.backend.clear_session()

img_path = '/Users/Klaus/downloads/flower_photos/daisy/5547758_eea9edfd54_n.jpg'

img = image.load_img(img_path, target_size=(224, 224))
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)
x = preprocess_input(x)

# Bug: loading multiple models is by manually specifying different graph and session for each loaded model
graph = tf.Graph()
with graph.as_default():
       session = tf.compat.v1.Session()
       with session.as_default():
          ## your load model code
          model = VGG16(weights='imagenet')
# save each graph and session value to some variable for later prediction use.
graph_var = graph
session_var = session
with graph_var.as_default():
     with session_var.as_default():
            preds = model.predict(x)

print('Predicted:', decode_predictions(preds, top=3)[0])

np.argmax(preds[0])

flower_output = model.output[:, 309]
last_conv_layer = model.get_layer('block5_conv3')

grads = tf.keras.backend.gradients(flower_output, last_conv_layer.output)[0]
pooled_grads = tf.keras.backend.mean(grads, axis=(0, 1, 2))

iterate = tf.keras.backend.function([model.input], [pooled_grads, last_conv_layer.output[0]])
```

**Whole error mesage**

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-360-5ea87dd3ba6a> in <module>
----> 1 iterate = tf.keras.backend.function([model.input], [pooled_grads, last_conv_layer.output[0]])

~/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/keras/backend.py in function(inputs, outputs, updates, name, **kwargs)
   3780                'backend') % key
   3781         raise ValueError(msg)
-> 3782   return GraphExecutionFunction(inputs, outputs, updates=updates, **kwargs)
   3783 
   3784 

~/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/keras/backend.py in __init__(self, inputs, outputs, updates, name, **session_kwargs)
   3432     # dependencies in call.
   3433     # Index 0 = total loss or model output for `predict`.
-> 3434     with ops.control_dependencies([self.outputs[0]]):
   3435       updates_ops = []
   3436       for update in updates:

~/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/framework/ops.py in control_dependencies(control_inputs)
   5255     return NullContextmanager()
   5256   else:
-> 5257     return get_default_graph().control_dependencies(control_inputs)
   5258 
   5259 

~/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/framework/ops.py in control_dependencies(self, control_inputs)
   4689           (hasattr(c, ""_handle"") and hasattr(c, ""op""))):
   4690         c = c.op
-> 4691       c = self.as_graph_element(c)
   4692       if isinstance(c, Tensor):
   4693         c = c.op

~/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/framework/ops.py in as_graph_element(self, obj, allow_tensor, allow_operation)
   3608 
   3609     with self._lock:
-> 3610       return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
   3611 
   3612   def _as_graph_element_locked(self, obj, allow_tensor, allow_operation):

~/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/framework/ops.py in _as_graph_element_locked(self, obj, allow_tensor, allow_operation)
   3687       # Actually obj is just the object it's referring to.
   3688       if obj.graph is not self:
-> 3689         raise ValueError(""Tensor %s is not an element of this graph."" % obj)
   3690       return obj
   3691     elif isinstance(obj, Operation) and allow_operation:

ValueError: Tensor Tensor(""Mean:0"", shape=(512,), dtype=float32) is not an element of this graph.
```"
33248,Dimensional Reasoning,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): Tensorflow2.0
- Are you willing to contribute it (Yes/No):



**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**
No
**Who will benefit with this feature?**
No
**Any Other info.**


`    

def roi_align_one_layer(feat, bbox, s, align_c):

            s_y, s_x = s
            num_bbox = tf.shape(bbox)[0]
            feat_shape = tf.shape(feat)

# tensorflow know c_i here
            h_i, w_i, c_i = feat_shape[0], feat_shape[1], feat_shape[2]


            h_f, w_f = tf.cast(h_i, tf.float32), tf.cast(w_i, tf.float32)
            bbox_y1 = bbox[:, 0:1]
            bbox_x1 = bbox[:, 1:2]
            bbox_y2 = bbox[:, 2:3]
            bbox_x2 = bbox[:, 3:4]
            bbox_h = bbox_y2 - bbox_y1
            bbox_w = bbox_x2 - bbox_x1
            if align_c:
                off_y = 0. if s_y > 1 else bbox_h / 2.
                off_x = 0. if s_x > 1 else bbox_w / 2.
                grid_y = tf.linspace(0.0, 1.0, s_y)
                grid_x = tf.linspace(0.0, 1.0, s_x)
            else:
                off_y = bbox_h / (2. * s_y)
                off_x = bbox_w / (2. * s_x)
                grid_y = tf.linspace(0.0, 1.0, s_y + 1)[:-1]
                grid_x = tf.linspace(0.0, 1.0, s_x + 1)[:-1]
            grid_y = tf.expand_dims(tf.matmul(bbox_h, tf.expand_dims(grid_y, axis=0)) + off_y + bbox_y1, axis=-1)
            grid_x = tf.expand_dims(tf.matmul(bbox_w, tf.expand_dims(grid_x, axis=0)) + off_x + bbox_x1, axis=-1)

            grid_y = tf.where(grid_y < 0., 0., grid_y)
            grid_x = tf.where(grid_x < 0., 0., grid_x)
            grid_y = tf.where(grid_y > h_f-1., h_f-1., grid_y)
            grid_x = tf.where(grid_x > w_f-1., w_f-1., grid_x)
            grid_y = tf.tile(grid_y, [1, 1, s_y])
            grid_x = tf.tile(grid_x, [1, 1, s_x])
            grid_y = tf.reshape(grid_y, [num_bbox, -1])
            grid_x = tf.reshape(grid_x, [num_bbox, -1])
            grid_y1 = tf.math.floor(grid_y)
            grid_y2 = tf.math.floor(grid_y+1.)
            grid_x1 = tf.math.floor(grid_x)
            grid_x2 = tf.math.floor(grid_x+1.)
            wey1 = tf.expand_dims(grid_y - grid_y1, axis=-1)
            wey2 = tf.expand_dims(grid_y2 - grid_y, axis=-1)
            wex1 = tf.expand_dims(grid_x - grid_x1, axis=-1)
            wex2 = tf.expand_dims(grid_x2 - grid_x, axis=-1)
            grid_y2 = tf.where(grid_y2 > h_f - 1., h_f - 1., grid_y2)
            grid_x2 = tf.where(grid_x2 > w_f - 1., w_f - 1., grid_x2)
            grid_y1 = tf.cast(grid_y1, tf.int32)
            grid_y2 = tf.cast(grid_y2, tf.int32)
            grid_x1 = tf.cast(grid_x1, tf.int32)
            grid_x2 = tf.cast(grid_x2, tf.int32)
            grid_11 = grid_y1 * w_i + grid_x1
            grid_12 = grid_y1 * w_i + grid_x2
            grid_21 = grid_y2 * w_i + grid_x1
            grid_22 = grid_y2 * w_i + grid_x2
            feat = tf.reshape(feat, [h_i*w_i, c_i])
            feat_11 = tf.gather(feat, grid_11)
            feat_12 = tf.gather(feat, grid_12)
            feat_21 = tf.gather(feat, grid_21)
            feat_22 = tf.gather(feat, grid_22)
            feat_bilinear = wey2 * (feat_11 * wex2 + feat_12 * wex1) + wey1 * (feat_21 * wex2 + feat_22 * wex1)

# tensorflow don't know c_i here
            feat_bilinear = tf.reshape(feat_bilinear, [num_bbox, s_y, s_x, c_i])


            return feat_bilinear
`

the data_format is the 'channels_last'

when I use `y = roi_align_one_layer(feat, bbox, s, align_c)` the tensorflow2.0 know the number of channels of `feat` but don't know the number of channels of `y`,
when I use `tf.keras.layers.Conv()(y)`, tensorflow2.0 raise error that tensorflow2.0 don't know the number of input channels"
33247,Saving GRU with dropout to SavedModel fails,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Pip
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0
- Python version: python 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
When Model containing GRU layer, dropout is set and activation='relu', the model is not savable.

Error:
Attempted to save a function b'__inference_GRU_layer_call_fn_8041' which references a symbolic Tensor Tensor(""dropout/mul_1:0"", shape=(None, 3), dtype=float32) that is not a simple constant. This is not supported.

**Describe the expected behavior**
Model gets saved.

**Code to reproduce the issue**
```
from tensorflow import keras
from tensorflow.keras import layers

inputs = keras.Input(shape=(784,3), name='digits')
x = layers.GRU(64, activation='relu', name='GRU',dropout=0.1)(inputs)
x = layers.Dense(64, activation='relu', name='dense')(x)
outputs = layers.Dense(10, activation='softmax', name='predictions')(x)

model = keras.Model(inputs=inputs, outputs=outputs, name='3_layer')
model.summary()
model.save('model',save_format='tf')
```
Based on: https://www.tensorflow.org/guide/keras/save_and_serialize
"
33246,Better  Dimension reasoning,"`    

def roi_align_one_layer(feat, bbox, s, align_c):

            s_y, s_x = s
            num_bbox = tf.shape(bbox)[0]
            feat_shape = tf.shape(feat)
            h_i, w_i, c_i = feat_shape[0], feat_shape[1], feat_shape[2]
            h_f, w_f = tf.cast(h_i, tf.float32), tf.cast(w_i, tf.float32)
            bbox_y1 = bbox[:, 0:1]
            bbox_x1 = bbox[:, 1:2]
            bbox_y2 = bbox[:, 2:3]
            bbox_x2 = bbox[:, 3:4]
            bbox_h = bbox_y2 - bbox_y1
            bbox_w = bbox_x2 - bbox_x1
            if align_c:
                off_y = 0. if s_y > 1 else bbox_h / 2.
                off_x = 0. if s_x > 1 else bbox_w / 2.
                grid_y = tf.linspace(0.0, 1.0, s_y)
                grid_x = tf.linspace(0.0, 1.0, s_x)
            else:
                off_y = bbox_h / (2. * s_y)
                off_x = bbox_w / (2. * s_x)
                grid_y = tf.linspace(0.0, 1.0, s_y + 1)[:-1]
                grid_x = tf.linspace(0.0, 1.0, s_x + 1)[:-1]
            grid_y = tf.expand_dims(tf.matmul(bbox_h, tf.expand_dims(grid_y, axis=0)) + off_y + bbox_y1, axis=-1)
            grid_x = tf.expand_dims(tf.matmul(bbox_w, tf.expand_dims(grid_x, axis=0)) + off_x + bbox_x1, axis=-1)

            grid_y = tf.where(grid_y < 0., 0., grid_y)
            grid_x = tf.where(grid_x < 0., 0., grid_x)
            grid_y = tf.where(grid_y > h_f-1., h_f-1., grid_y)
            grid_x = tf.where(grid_x > w_f-1., w_f-1., grid_x)
            grid_y = tf.tile(grid_y, [1, 1, s_y])
            grid_x = tf.tile(grid_x, [1, 1, s_x])

            grid_y = tf.reshape(grid_y, [num_bbox, -1])
            grid_x = tf.reshape(grid_x, [num_bbox, -1])
            grid_y1 = tf.math.floor(grid_y)
            grid_y2 = tf.math.floor(grid_y+1.)
            grid_x1 = tf.math.floor(grid_x)
            grid_x2 = tf.math.floor(grid_x+1.)
            wey1 = tf.expand_dims(grid_y - grid_y1, axis=-1)
            wey2 = tf.expand_dims(grid_y2 - grid_y, axis=-1)
            wex1 = tf.expand_dims(grid_x - grid_x1, axis=-1)
            wex2 = tf.expand_dims(grid_x2 - grid_x, axis=-1)
            grid_y2 = tf.where(grid_y2 > h_f - 1., h_f - 1., grid_y2)
            grid_x2 = tf.where(grid_x2 > w_f - 1., w_f - 1., grid_x2)
            grid_y1 = tf.cast(grid_y1, tf.int32)
            grid_y2 = tf.cast(grid_y2, tf.int32)
            grid_x1 = tf.cast(grid_x1, tf.int32)
            grid_x2 = tf.cast(grid_x2, tf.int32)
            grid_11 = grid_y1 * w_i + grid_x1
            grid_12 = grid_y1 * w_i + grid_x2
            grid_21 = grid_y2 * w_i + grid_x1
            grid_22 = grid_y2 * w_i + grid_x2

            feat = tf.reshape(feat, [h_i*w_i, c_i])
            feat_11 = tf.gather(feat, grid_11)
            feat_12 = tf.gather(feat, grid_12)
            feat_21 = tf.gather(feat, grid_21)
            feat_22 = tf.gather(feat, grid_22)

            feat_bilinear = wey2 * (feat_11 * wex2 + feat_12 * wex1) + wey1 * (feat_21 * wex2 + feat_22 * wex1)
            feat_bilinear = tf.reshape(feat_bilinear, [num_bbox, s_y, s_x, c_i])
            return feat_bilinear
`

the input the 'channels_last'

when I use `y = roi_align_one_layer(feat, bbox, s, align_c)` the tensorflow2.0 know the number of channels of feat but don't know the number of channels of y,
when I use `tf.keras.layers.Conv()(y)`, tensorflow2.0 raise error that tensorflow2.0 don't know the number of input channels"
33245,Cannot use dict base datasets with keras.Model.fit.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0
- Python version: 3.6.4
- CUDA/cuDNN version: 10.1
- GPU model and memory: 7.6.2

In order to make both dataset and keras model have good structures, I create a dataset and a vanilla model like this.
```python
# dataset is something like <BatchDataset shapes: ({input: (None, 100)}, {output: (None, 10)}), types: ({input: tf.float32}, {output: tf.float32})>

# subscale model is something like
class VanillaModel(Model):

  def __init__(self, num_units, **kwargs):
    super(VanillaModel, self).__init__(**kwargs)
    self.num_units = num_units

    # One linear projection layer.
    self.dense_proj = tf.keras.layers.Dense(num_units, activation='relu')

  def call(self, features):
    """"""Forward pass.""""""
    output = self.dense_proj(features['input'])
    return {
        'output': output
    }
```

When I use the dict based dataset (from tfds) with keras.Model.fit, the first call will cause expection as 
```python
# Compile model using dict with same keys.
model.compile('adam', {'output': 'mse'})

# The errors
 File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_utils.py"", line 1248, in cast_if_floating_dtype_and_mismatch
    if target.dtype != out.dtype:
AttributeError: 'str' object has no attribute 'dtype'
```

I checked the code and found that, when dict is passed, iterating through `zip(targets, outputs)` will just get the keys of the dict, so the string keys have no dtyle.
(https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training_utils.py#L1246)
![image](https://user-images.githubusercontent.com/11533479/66652073-59916c00-ec67-11e9-8974-eda50bf98e18.png)

So how can I use dict based dataset and model with keras.Model.fit?
"
33244,Model has not been built yet,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.0.0-beta1
- Python version:3.7..3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): [GCC 7.3.0] :: Anaconda, Inc. on linux
- CUDA/cuDNN version:
- GPU model and memory: VGA compatible controller: NVIDIA Corporation GP102 [GeForce GTX 1080 Ti] (rev a1)

I have downloaded the official example of Convolutional Variational Autoencoder from this [link](https://www.tensorflow.org/tutorials/generative/cvae). 

If one starts to execute this file everything looks fine but the problem arises when one wants to save the weight or call the summary() method. (i.e.) model.summary(). This produces the following error:

ValueError: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.

PS, I faced the same issue in  my costume code as well. since the file is large I prevent to provide it here but you can find it in this [link](https://github.com/Sorooshi/ML4DC/blob/development/my_ope.ipynb). 


Any Idea what's going on?! And how to overcome this challenge.

Thanks in advance. 

#tensorflow "
33242,CTC Beam Search Decoder in TensorFlow android,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod

## Description of issue (what needs changing):
How to decode output float array into string using CTC beam search decoder in android. I'm not using tflite as of now. 

### Clear description


For example, why should someone use this method? How is it useful?

### Correct links

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined
Float Array
Are return values defined? 
Yes

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example?

See the API guide: https://www.tensorflow.org/community/contribute/docs_ref
on how to write testable usage examples.

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
33241,some Ops tensorflow could but tflite,"hi,
I have some Ops which are valid in Tensorflow,but not valid in Tensorflow Lite,and if I want use the tflite file,how to solve the problem ?thx
`2019-10-11 17:30:39.943242: W tensorflow/lite/toco/tflite/operator.cc:2707] Op TensorListReserve is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.
2019-10-11 17:30:39.943256: W tensorflow/lite/toco/tflite/operator.cc:2707] Op TensorListFromTensor is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.
2019-10-11 17:30:39.943266: W tensorflow/lite/toco/tflite/operator.cc:2707] Op While is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.
2019-10-11 17:30:39.943275: W tensorflow/lite/toco/tflite/operator.cc:2707] Op TensorListStack is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.
2019-10-11 17:30:39.943359: W tensorflow/lite/toco/tflite/operator.cc:2707] Op TensorListReserve is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.
2019-10-11 17:30:39.943373: W tensorflow/lite/toco/tflite/operator.cc:2707] Op TensorListFromTensor is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.
2019-10-11 17:30:39.943382: W tensorflow/lite/toco/tflite/operator.cc:2707] Op While is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.
2019-10-11 17:30:39.943392: W tensorflow/lite/toco/tflite/operator.cc:2707] Op TensorListStack is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.
2019-10-11 17:30:39.943422: W tensorflow/lite/toco/tflite/operator.cc:2707] Op TensorListReserve is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.
2019-10-11 17:30:39.943456: I tensorflow/lite/toco/tflite/operator.cc:2054] Writing flex op: TensorListReserve
2019-10-11 17:30:39.943471: W tensorflow/lite/toco/tflite/operator.cc:2707] Op TensorListFromTensor is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.
2019-10-11 17:30:39.943483: I tensorflow/lite/toco/tflite/operator.cc:2054] Writing flex op: TensorListFromTensor
2019-10-11 17:30:39.943500: W tensorflow/lite/toco/tflite/operator.cc:2707] Op While is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.
2019-10-11 17:30:39.943566: I tensorflow/lite/toco/tflite/operator.cc:2054] Writing flex op: While
2019-10-11 17:30:39.943591: W tensorflow/lite/toco/tflite/operator.cc:2707] Op TensorListStack is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.
2019-10-11 17:30:39.943606: I tensorflow/lite/toco/tflite/operator.cc:2054] Writing flex op: TensorListStack`

I just want to use the tflite file in cellphone ,what should I do ?
Thanks a lot


"
33239,need some Ops in RNN,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source):
- TensorFlow version (github):




```
bazel-bin/tensorflow/lite/toco/toco \
--input_file=../mydlstm.pb --output_file=../mydlstmfloat.tflite \
--output_format=TFLITE --input_shapes=1,5,513 --input_arrays=x_mixed \
--output_arrays=y_out1,y_out2 --inference_type=FLOAT \
--inference_input_type=FLOAT
```
and the error is here

'''
2019-10-11 17:02:20.357549: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListReserve
2019-10-11 17:02:20.357612: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-10-11 17:02:20.357628: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListFromTensor
2019-10-11 17:02:20.357638: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-10-11 17:02:20.360078: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: While
2019-10-11 17:02:20.360126: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-10-11 17:02:20.360134: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2019-10-11 17:02:20.360162: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListStack

'''

model is here
[mydlstm.zip](https://github.com/tensorflow/tensorflow/files/3716604/mydlstm.zip)

"
33238,redzone_checker appears a lot while profiling tf-2.0,"I have downloaded and built tf-2.0.0 from source. As I profile the label_image example with nvprof, I see that `redzone_checker` kernel is the most time consuming kernel (25%). Is that normal? I remember that this kernel wasn't time consuming in previous versions.

Also I didn't find any useful information about what does this kernel doing in a typical run. The same happens with multibox_detector example with a large percent of 87%. I don't think that is normal.

![Screenshot from 2019-10-11 12-22-49](https://user-images.githubusercontent.com/11626212/66638471-de599700-ec21-11e9-8c9c-c7f82257c86b.png)

"
33236,How to load a keras model saved by tf 1.14 with tf 1.15?,"I run tensorflow on colab.research.google.com.
Some days before, when the tensorflow version was 1.14,
I saved a keras model to ""drive/Colab/mo_big7_12345789bcdfghijklm"" . it is a file, not a path.
```
Model = tf.keras.models.Model(...)
Model.save(...)
```
Now, the tensorflow upgraded to v1.15. when loading the file by
`tf.keras.models.load_model( 'drive/Colab/mo_big7_12345789bcdfghijklm' )`
it raises: OSError: SavedModel file does not exist at: drive/Colab/mo_big7_12345789bcdfghijklm/{saved_model.pbtxt|saved_model.pb}
I havn't seen the .pb or .pbtxt files before. how to load it?"
33233,[tflite2.0] tf.lite.Interpreter fails with 'post-training quantization' (tf.matmul).,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from source
- TensorFlow version (use command below): ('v2.0.0-0-g64c3d38', '2.0.0')
- Python version: 2.7.12
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
- CUDA/cuDNN version: 10.0 / 7.5
- GPU model and memory: GeForce GTX TITAN

**Describe the current behavior**
'Post-training integer quantization' fails with tf.matmul() in a certain condition.
(https://www.tensorflow.org/lite/performance/post_training_integer_quant)

Following code is a testing code for converting a dummy tensorflow graph with quantization. This code fails if shape_b[-1] > 15 for me. When I analyzed the visualization result ((bazel run //tensorflow/lite/tools:visualize model.tflite visualized_model.html)), I think it is a result of graph transformation issue.

Suppose that shape_b = (None, 16, 4, 5). Then each tf.matmul op is replaced by 16 FULLY_CONNECTED ops in the tflite model. And each FULLY_CONNECTED op has its own bias tensor, with shape=[5], as third input. In this case, tflite_convert works in both cases that quantization=True or quantization=False.

When Suppose that shape_b = (None, 16, 4, 16). When quantization=True, the interpreter fails at the allocation step with following output:
```
INFO: Initialized TensorFlow Lite runtime.
Traceback (most recent call last):
  File ""tflite_matmul_v2.py"", line 89, in <module>
    main()
  File ""tflite_matmul_v2.py"", line 67, in main
    interpreter.allocate_tensors()
  File ""/home/hh1208-kang/venv_py2_tf_nightly/local/lib/python2.7/site-packages/tensorflow_core/lite/python/interpreter.py"", line 244, in allocate_tensors
    return self._interpreter.AllocateTensors()
  File ""/home/hh1208-kang/venv_py2_tf_nightly/local/lib/python2.7/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py"", line 106, in AllocateTensors
    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)
RuntimeError: tensorflow/lite/kernels/kernel_util.cc:119 std::abs(input_product_scale - bias_scale) <= 1e-6 * std::min(input_product_scale, bias_scale) was not true.Node number 282 (FULLY_CONNECTED) failed to prepare.
```
But the code still works if quantization=False. When I check the visualization result, I noticed that the bias tensors for FULLY_CONNECTED ops are merged into a single bias tensor, with shape [16]. It seems that tflite_convert try to remove redundant zero bias tensors when it becomes large.

However that makes problem in the quantization. Because in that case, the quantization parameters of the bias tensor is shared by all the FULLY_CONNECTED ops. Thus the check std::abs(input_product_scale - bias_scale) <= 1e-6 * std::min(input_product_scale, bias_scale) fails, since 'input_product_scale' may be different in some FULLY_CONNECTED op, while 'bias_scale' are the same.

How can I suppress the transformation behavior? I think somehow if I could prevent the transformation that merges all the bias tensors, the quantization success.

**Code to reproduce the issue**
```
import numpy as np
import tensorflow as tf

def test_tflite_model(tflite_filename, examples):
    print(""Loading TFLite interpreter for %s..."" % tflite_filename)
    interpreter = tf.lite.Interpreter(model_path=tflite_filename)
    interpreter.allocate_tensors()
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    print(""input details: %s"" % input_details)
    print(""output details: %s"" % output_details)

    for i, input_tensor in enumerate(input_details):
        interpreter.set_tensor(input_tensor['index'], examples[i])
    interpreter.invoke()
    model_output = []
    for i, output_tensor in enumerate(output_details):
        model_output.append(interpreter.get_tensor(output_tensor['index']))
    return model_output

def main():
    nsamples=1
    quantization=True
    tflite_filename = ""matmul_model_v2.tflite""
    shape_a = (None, 16, 3, 4)
    shape_b = (None, 16, 4, 15) # quantization fails if shape_b[-1] >= 16

    @tf.function(input_signature=[
    	tf.TensorSpec(shape=shape_a, dtype=tf.float32, name='a'), 
        tf.TensorSpec(shape=shape_b, dtype=tf.float32, name='b')])
    def model(a,b):
	c = tf.matmul(a,b) #[16, 3, 5]

	def submodule(c,b):
		nb = tf.transpose(b, [0, 1, 3, 2])
		#nb = tf.nn.softmax(nb)
		c = tf.matmul(c,nb) # [16,3,4]
		c = tf.matmul(c,b)  # [16,3,5]
		return c
	c = submodule(c,b)
	return c


    def _representative_dataset_gen():
	for i in range(50):
		a_ = np.random.random_sample((1,)+shape_a[1:]).astype(np.float32)
	        b_ = np.random.random_sample((1,)+shape_b[1:]).astype(np.float32)
		yield [a_,b_]

    # tflite_convert
    cfunc = model.get_concrete_function()
    converter = tf.lite.TFLiteConverter.from_concrete_functions([cfunc])
    if (quantization):
	converter.representative_dataset = _representative_dataset_gen # with quantization
	converter.optimizations = [tf.lite.Optimize.DEFAULT]
    tflite_model = converter.convert()
    open(tflite_filename, ""wb"").write(tflite_model)

    # run tensorflow model (not converted)
    np.random.seed(1234)
    a_ = np.random.random_sample((nsamples,)+shape_a[1:]).astype(np.float32)
    b_ = np.random.random_sample((nsamples,)+shape_b[1:]).astype(np.float32)
    session_output = model(a_,b_)

    # load and run tflite model (converted_)
    interpreter = tf.lite.Interpreter(model_path=tflite_filename)
    interpreter.allocate_tensors()
    output_details = interpreter.get_output_details()
    output_shape = interpreter.get_tensor(output_details[0]['index']).shape
    tflite_output = np.zeros((nsamples,)+output_shape[1:], np.float32)
    for n in range(nsamples):
	tflite_output[n,:] = test_tflite_model(tflite_filename, 
		[np.expand_dims(a_[n,:],0), np.expand_dims(b_[n,:],0)])[0]

    print(""Input example:"")
    print(a_)
    print(a_.shape)
    print(b_)
    print(b_.shape)
    print(""Session output:"")
    print(session_output)
    print(session_output.shape)
    print(""TFLite output:"")
    print(tflite_output)
    print(tflite_output.shape)
    print(np.allclose(session_output, tflite_output))

if __name__ == '__main__':
    main()
```
Note that this code is inspired by https://github.com/tensorflow/tensorflow/issues/27640.


p.s. (2019.10.17)
I corrected the post since I made a mistake. The problem is not about converting, but about the interpreter, especially in the allocation. I'm sorry.
"
33232,Tensor not eagerly evaluated when using train_on_batch,"Dear all, 

I am using the train_on_batch() function to train my model and which to redirect the outputs in a log file. I am using the following configuration TensorFlow 2.0.0.rc2, Kera 2.3.0 and Python 3.7.3. For
the moment, I'm just training my model with few batches within a loop

```python
    print(""{}"".format(model.metrics_names))
    for batch_i in range(batch_count):
        x_i, y_i = next(training_generator)
        history_i = model.train_on_batch(x_i, y_i, reset_metrics=True)
        print(""{}: {}"".format(batch_i, history_i))
        print(model.outputs[0])
```
My output is supposed to be a 5x3 tensor. However, the tensor is not evaluated and I've got the following output:
```
0: [1.9101071, 1.0, 0.0, 10.0, 0.0, 5.0]
Tensor(""fc3/sigmoid/Identity:0"", shape=(None, 3), dtype=float32)
...
```
When I'm using ```model.outputs[0].numpy()```, I've got an attribute error:
```
AttributeError: 'Tensor' object has no attribute 'numpy'
```
With TensorFlow v1, I would have said that the session is not correctly initialize. WIth V2, I'm a bit puzzled. How eager evaluation is supposed to work with train_on_batch? 

Nick,"
33231,java.lang.RuntimeException: java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find custom op for name 'TFLite_Detection_PostProcess' with version 1 ,"I used tensorflow lite android object detection demo in my android project , but it crashs , I set ndk {
            abiFilters 'armeabi-v7a'
        } 
       in build.gradle, it still crashs
     Error Log ：
 java.lang.RuntimeException: java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find custom op for name 'TFLite_Detection_PostProcess' with version 1
    Registration failed. at com.tfdetection.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:125)
  crash code ：d.tfLite = new Interpreter(loadModelFile(assetManager, modelFilename));"
33230,Embedding visualization in tf.keras / TensorFlow 2.0,"I'm trying to visualize some embeddings in Tensorboard. In pure keras this was possible, for example:
- https://keras.io/examples/tensorboard_embeddings_mnist/

There was already a regression in `tf.keras`, breaking this functionality, see [this bug](https://github.com/keras-team/keras/issues/12808).

I thought I'd try out tensorflow 2.0 and it seems that the functionality is removed altogether. I now get the message:

> WARNING:tensorflow:`embeddings_layer_names` is not supported in TensorFlow 2.0. Instead, all `Embedding` layers will be visualized.

I generally don't care about visualizing my `Embedding` layers. Instead, I'd like to visualize the output of my bottleneck layer, which is typically just a `Dense` layer.

As I'm writing this, I do realize that the way tensorboard/projector works is that is extracts the embedding `Variable` (weights) from a checkpoint. In the case of an `Embedding` layer this is very natural. The interesting case, I would argue, is the visualization of **activations**, i.e. a generic `Tensor`.

Is this supported by `tf.keras` or should I write a custom callback class to do this?

Thanks!
"
33227,Custom OptimizerV2 frozen,"I seek to implement [AdamW](https://github.com/OverLordGoldDragon/keras-adamw) for `OptimizerV2`, using syntax from [Keras](https://github.com/CyberZHG/keras-radam/blob/5bda32f5e26c6dc8140f7c6c95c13e9128341282/keras_radam/optimizer_v2.py) and [TF2](https://github.com/taki0112/RAdam-Tensorflow) implementations of RAdam - but am getting nowhere.  Below is a minimal reproducible example for a rewritten portion of [Adam](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/optimizer_v2/adam.py#L32); the complete version is just this revision, with `_prepare_local` deleted entirely (see [gist](https://gist.github.com/OverLordGoldDragon/aad492db1ff6f20a7983e600611799f7)). `print` statements for introspection. Observations:

 - Loss doesn't change at all (tested for 20 iters)
 - Weights don't change at all via `state_ops.assign_sub` (or via `.assign` w/ `math_ops.sub`)
 - `var_t` _does_ change, showing that subtracted value isn't negligibly small - but the change isn't applied to `var`
 - `var_t` changes slightly across iterations, possibly per GPU parallelism randomness (values do vary within 1%, not ""almost zero"")

<hr>

If linked implementations are correct, why is mine failing?

<hr>

```python
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
from adam_v2 import Adam  # custom local module w/ described changes
reset_seeds()  # see below

ipt   = Input(shape=(4,))
x     = Dense(2, activation='relu', name='dense_1')(ipt)
out   = Dense(1, activation='sigmoid')(x)
model = Model(ipt, out)
model.compile(Adam(lr=1e-3), 'binary_crossentropy')

X = np.random.randn(32, 4)
Y = np.random.randint(0, 2, (32, 1))
print(model.train_on_batch(X,Y))
```
```python
<tf.Variable 'dense_1/kernel:0' shape=(4, 2) dtype=float32, numpy=
array([[-0.73267365,  0.13570619],
       [-0.00733662, -0.9662695 ],
       [ 0.00908351,  0.4974625 ],
       [-0.73932624, -0.32877016]], dtype=float32)>
      [[-0.7358344   0.13886832]
       [-0.00417447 -0.9631073 ]
       [ 0.00592144  0.49430028]
       [-0.73616403 -0.32560796]]
<tf.Variable 'dense_1/kernel:0' shape=(4, 2) dtype=float32, numpy=
array([[-0.73267365,  0.13570619],
       [-0.00733662, -0.9662695 ],
       [ 0.00908351,  0.4974625 ],
       [-0.73932624, -0.32877016]], dtype=float32)>

0.8960571  # loss
```

<hr>

```python
def _resource_apply_dense(self, grad, var):
    var_dtype = var.dtype.base_dtype
    lr_t = self._decayed_lr(var_dtype)
    m = self.get_slot(var, 'm')
    v = self.get_slot(var, 'v')
    beta_1_t = array_ops.identity(self._get_hyper('beta_1', var_dtype))
    beta_2_t = array_ops.identity(self._get_hyper('beta_2', var_dtype))
    epsilon_t = ops.convert_to_tensor(self.epsilon, var_dtype)
    
    m_t = state_ops.assign(m,
    		   beta_1_t * m + (1.0 - beta_1_t) * grad,
    		   use_locking=self._use_locking)
    v_t = state_ops.assign(v,
    		   beta_2_t * v + (1.0 - beta_2_t) * math_ops.square(grad),
    		   use_locking=self._use_locking)
    if self.amsgrad:
    	vhat = self.get_slot(var, 'vhat')
    	vhat_t = state_ops.assign(vhat, math_ops.maximum(vhat, v_t),
                                  use_locking=self._use_locking)
    	var_delta = m_t / (K.sqrt(vhat_t) + epsilon_t)
    else:
    	var_delta = m_t / (K.sqrt(v_t) + epsilon_t)
    var_t = math_ops.sub(var, lr_t * var_delta)

    if 'dense_1/kernel' in var.name:
    	print(var)
    	print(K.eval(var_t))
    var_update = state_ops.assign_sub(var, lr_t * var_delta,
    			              use_locking=self._use_locking)
    if 'dense_1/kernel' in var.name:
    	print(var)
    	
    updates = [var_update, m_t, v_t]
    if self.amsgrad:
    	updates.append(vhat_t)
    return control_flow_ops.group(*updates)
```
```python
import numpy as np
import random
import tensorflow as tf

def reset_seeds():
    np.random.seed(1)
    random.seed(2)
    if tf.__version__[0] == '2':
        tf.random.set_seed(3)
    else:
        tf.set_random_seed(3)
```"
33226,python3-pip missing from container image,"# Issue
Python 2.7 is being deprecated in 2020, Python 3.6+ is included but is lacking pip3.  
Some [Python examples](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/python) don't work without having to manually install it.

## Fix
Install python3-pip as a dependency"
33223,tf.summary.image log spam,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0, 7.6.2
- GPU model and memory: Tesla v100 16GiB (AWS EC2 p3.2x instance).

**Describe the current behavior**

I'm running TF in a lazy mode by calling tf.compat.v1.disable_v2_behavior().

I have code that calls tf.summary.image in a trivial way:
  tf.summary.image(""name"", sometensor)

TF shows a following error message. TF continues functioning properly, but these messages are eyesore.

```
ERROR:tensorflow:==================================                                                                                                                                           
Object was never used (type <class 'tensorflow.python.framework.ops.Operation'>):                                                                                                             
<tf.Operation 'channel_multiplicity/assert_non_negative/assert_less_equal/Assert/Assert' type=Assert>                                                                                         
If you want to mark it as used call its ""mark_used()"" method.                                                                                                                                 
It was originally created here:                                                                                                                                                               
  File ""/home/ubuntu/.local/lib/python3.7/site-packages/tensorboard/plugins/image/summary_v2.py"", line 72, in image                                                                           
    tf.debugging.assert_non_negative(max_outputs)  File ""/home/ubuntu/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/check_ops.py"", line 302, in assert_non_negative_v2        
    name=name)  File ""/home/ubuntu/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/check_ops.py"", line 345, in assert_non_negative                                              
    return assert_less_equal(zero, x, data=data, summarize=summarize)  File ""/home/ubuntu/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/check_ops.py"", line 952, in assert_le\
ss_equal                                                                                                                                                                                      
    return control_flow_ops.Assert(condition, data, summarize=summarize)  File ""/home/ubuntu/.local/lib/python3.7/site-packages/tensorflow_core/python/util/tf_should_use.py"", line 198, in w\
rapped                                                                                                                                                                                        
    return _add_should_use_warning(fn(*args, **kwargs))                                                                                                                                       
==================================                           
```

**Describe the expected behavior**

These error messages shouldn't be shown.

**Code to reproduce the issue**
I can create a repro if needed.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
33222,TF 2.0: Can't authenticate with google storage in colab,"I'm trying to use the TPU in colab so I have to authenticate to my google storage account to feed the data (as I understood from past tutorials on using TPUs on colab). When I'm trying to authenticate I get the following error:

```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-2-1f759c1655bd> in <module>()
      1 from google.colab import auth
----> 2 auth.authenticate_user()

/usr/local/lib/python3.6/dist-packages/google/colab/auth.py in authenticate_user(clear_output)
    154       with tf.compat.v1.Session('grpc://{}'.format(colab_tpu_addr)) as sess:
    155         with open(_get_adc_path()) as auth_info:
--> 156           tf.contrib.cloud.configure_gcs(
    157               sess, credentials=_json.load(auth_info))
    158   if _check_adc():

AttributeError: module 'tensorflow' has no attribute 'contrib'
```

The code I run is:

```
!pip install tensorflow-gpu
from google.colab import auth
auth.authenticate_user()
```
The following link contains the code to reproduce the error https://colab.research.google.com/drive/1LQ_SuPoetIUBTBhohVbFJnatkyKNb1G9
"
33219,regularization parameter being ignored in tf.keras dense layer initialization,"An example of this issue can easily be seen on the TF 2.0 colab advanced quickstart tutorial: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/quickstart/advanced.ipynb#scrollTo=i-2pkctU_Ci7

Changing the code block from

```
`class MyModel(Model):
  def __init__(self):
    super(MyModel, self).__init__()
    self.conv1 = Conv2D(32, 3, activation='relu')
    self.flatten = Flatten()
    self.d1 = Dense(128, activation='relu')
    self.d2 = Dense(10, activation='softmax')

  def call(self, x):
    x = self.conv1(x)
    x = self.flatten(x)
    x = self.d1(x)
    return self.d2(x)
model = MyModel()`
```

To 

```
`class MyModel(Model):
  def __init__(self):
    super(MyModel, self).__init__()
    self.conv1 = Conv2D(32, 3, activation='relu',activity_regularizer=tf.keras.regularizers.l2(100000.))
    self.flatten = Flatten()
    self.d1 = Dense(128, activation='relu',activity_regularizer=tf.keras.regularizers.l2(100000.))
    self.d2 = Dense(10, activation='softmax',activity_regularizer=tf.keras.regularizers.l2(100000.))
  def call(self, x):
    x = self.conv1(x)
    x = self.flatten(x)
    x = self.d1(x)
    return self.d2(x)
model = MyModel()`
```
    
has no effect, despite the regularization parameter being inordinately large (we should expect learning to completely stop in this case, as the penalization should force outputs to 0). 
This is also true for kernel_regularizer.
Manually adding tf.keras.layers.ActivityRegularization also has no effect. 

"
33218,TF 2.0: Functions and packages autocomplete broken in PyCharm,"from tensorflow.**_keras_** import **_callbacks_**
from tensorflow._**keras.optimizers**_ import **_Adam_**

The packages in bold are just a few examples that are not recognized in PyCharm. The issue was first spotted and reported in rc release. At that time we've been told to wait for the release of 2.0 because the issue is a bit more complex. The 2.0 is now here and as far as I am concerned the issue still persists. There are any configurations to be made in order to work or this problem is still an unresolved issue?

PyCharm gives the following complaint: 
```Cannot find reference 'keras' in '__init__.py'```"
33217,TF2.0: TypeError when intializting Variable using constant_initializer,"

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04**
- TensorFlow installed from (source or binary): **binary (pip)**
- TensorFlow version (use command below): **2.0.0**
- Python version: **3.7**
- CUDA/cuDNN version: **10.2**
- GPU model and memory: **GTX 1060, 6GB**

**Describe the current behavior**

I get an a `TypeError` execption when I try to initialize `tf.Variable` using `constant_initializer`.
Error: `TypeError: __call__() missing 1 required positional argument: 'shape'`

**Code to reproduce the issue**
```
import tensorflow as tf
import numpy as np

input_init = tf.constant_initializer(np.random.uniform(low=-1, high=1, size=(300, 300)))
input_weights = tf.Variable(input_init, shape=(300, 300))
```


**Other info / logs (Traceback)**

```
Traceback (most recent call last):
  File ""/home/ibrahimsharaf/workspace/GenSen/GenSen/garab.py"", line 38, in <module>
    input_weights = tf.Variable(input_init, shape=(300, 300))
  File ""/home/ibrahimsharaf/miniconda3/envs/gensen-tf/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py"", line 260, in __call__
    return cls._variable_v2_call(*args, **kwargs)
  File ""/home/ibrahimsharaf/miniconda3/envs/gensen-tf/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py"", line 254, in _variable_v2_call
    shape=shape)
  File ""/home/ibrahimsharaf/miniconda3/envs/gensen-tf/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py"", line 235, in <lambda>
    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)
  File ""/home/ibrahimsharaf/miniconda3/envs/gensen-tf/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py"", line 2556, in default_variable_creator_v2
    shape=shape)
  File ""/home/ibrahimsharaf/miniconda3/envs/gensen-tf/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py"", line 262, in __call__
    return super(VariableMetaclass, cls).__call__(*args, **kwargs)
  File ""/home/ibrahimsharaf/miniconda3/envs/gensen-tf/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py"", line 1406, in __init__
    distribute_strategy=distribute_strategy)
  File ""/home/ibrahimsharaf/miniconda3/envs/gensen-tf/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py"", line 1537, in _init_from_args
    initial_value() if init_from_fn else initial_value,
TypeError: __call__() missing 1 required positional argument: 'shape'
```
"
33216,[TF 2.0.0] Training keras Model on tf.data.Dataset causes small bug in logging,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **-**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **2.0.0**
- Python version: **3.6.x**
- CUDA/cuDNN version: **10.0/7.6.1**
- GPU model and memory: 


---
**Describe the current behavior**

When fitting (.fit) a keras Model on a tf.data.Dataset, the dataset size is not inferred. Because of this, when setting `verbose=1`, during the first epoch the log becomes `current_step/Unknown`. Also the following is thrown (though it does not cause crashing):
```
[[{{node IteratorGetNext}}]]
	 [[IteratorGetNext/_2]]
2019-10-10 18:41:50.728985: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext}}]]
```
**Describe the expected behavior**

I would expect to see the number of samples/batches/etc.

**Code to reproduce the issue**

I created a small Colab notebook to demonstrate the issue: https://colab.research.google.com/drive/1-S787cE6BWhXJ_0BeAb6EGq4GaXAFwmu
I recommend downloading the .py file and running it in command line (so colab logging doesn't interfere), because after the epoch is done, the correct batch number is found. The problem is during the epoch.
 
**Other info / logs**

I found that the dataset size inferring is actually run, but the returned value is not stored or used anywhere, it is only to throw a warning if the initialization made by the user is faulty in some way.
I am referring to this line: [https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/engine/training_v2.py#L247](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/engine/training_v2.py#L247)
The above problem would be eliminated with something like this (or the like):
```
steps_per_epoch = training_utils.infer_steps_for_dataset(training_dataset, steps_per_epoch, 
                                                                                 steps_name='steps_per_epoch',epochs=0) 
                                                                            if steps_per_epoch is None else steps_per_epoch
```
"
33215,[TF2]'Tensor' object has no attribute '_keras_history',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.0
- Python version: 3.7


### Problem
in TF2.0 use Keras to save model.h5 ,then load model.h5.

I have saved model.h5 from official.nlp.bert_models.py by use model.save(""model.h5"") .
everything is ok , but when I load the model.h5 there have some problems.
```python
model =tf.keras.models.load_model('./my_model1.h5', custom_objects={'BertModel':bert_modeling.BertModel})

```
emmmmm, how to solve this problem????
I am not sure this is BUG, but in this link someone say this maybe is a bug.
https://github.com/tensorflow/models/issues/7643

### Traceback
Traceback (most recent call last):
File ""/Users/lollipop/Documents/tf2g/false_news/test.py"", line 30, in
model =tf.keras.models.load_model('./my_model1.h5', custom_objects={'BertModel':bert_modeling.BertModel})
File ""/Users/lollipop/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py"", line 146, in load_model
return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)
File ""/Users/lollipop/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py"", line 168, in load_model_from_hdf5
custom_objects=custom_objects)
File ""/Users/lollipop/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/model_config.py"", line 55, in model_from_config
return deserialize(config, custom_objects=custom_objects)
File ""/Users/lollipop/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/serialization.py"", line 102, in deserialize
printable_module_name='layer')
File ""/Users/lollipop/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/generic_utils.py"", line 191, in deserialize_keras_object
list(custom_objects.items())))
File ""/Users/lollipop/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py"", line 906, in from_config
config, custom_objects)
File ""/Users/lollipop/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py"", line 1852, in reconstruct_from_config
process_node(layer, node_data)
File ""/Users/lollipop/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py"", line 1802, in process_node
output_index = nest.flatten(output_tensors)[0]._keras_history.node_index
AttributeError: 'Tensor' object has no attribute '_keras_history'

Process finished with exit code 1"
33214,Unable to use the save model for prediction,
33212,"""module 'tensorflow' has no attribute <anything>"" if tensorflow_core imported first","**System information**
- OS Platform and Distribution: Linux Manjaro 18
- TensorFlow installed from: pip install tensorflow-gpu==2.0.0
- TensorFlow version: 2.0.0
- Python version: 3.6

**Describe the current behavior**
If I import something from **tensorflow_core** before importing **tensorflow**, the **tensorflow** doens't work properly.

**Describe the expected behavior**
Being able to import from tensorflow_core (eg: from tensorflow_core.python.keras.engine.training import Model) with no problem.

**Code to reproduce the issue**


```
from tensorflow_core.python.keras.engine.training import Model
import tensorflow as tf

print(tf.__version__)
```
AttributeError: module 'tensorflow' has no attribute '__version__'

The following works:

```
import tensorflow as tf
from tensorflow_core.python.keras.engine.training import Model

print(tf.__version__)
```"
33211,Fail build librensorflow_cc.so with cuda support,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Debian 10
- TensorFlow version: 1.15.0
- Python version: 3
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): nvcc (gcc-7)
- CUDA/cuDNN version: 10.1
- GPU model and memory: GeForce RTX 2070 SUPER, 8192 MB



**Describe the problem**
When I try compile tensorflow I get error: `nvcc fatal   : Unknown option 'MD'`. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
While configuring I allow only: `python3`, compilation with `cuda` and cuda compiler as `nvcc` (with complete path to it). After it I try run `bazel` for build `libtensorflow_cc.so` but, almost immediately I get the error: `nvcc fatal   : Unknown option 'MD'`


**Any other info / logs**
```bash
bazel build --config=cuda //tensorflow:libtensorflow_cc.so --verbose_failures

...

INFO: Analyzed target //tensorflow:libtensorflow_cc.so (1 packages loaded, 2 targets configured).
INFO: Found 1 target...
WARNING: failed to create one or more convenience symlinks for prefix 'bazel-':
  cannot create symbolic link bazel-bin -> /home/levkovitch/.cache/bazel/_bazel_levkovitch/81f9c3cb5093a9779b51fe05f62c9c35/execroot/org_tensorflow/bazel-out/k8-opt/bin:  /tmp/tensorflow_dir/bazel-bin (Permission denied)
  cannot create symbolic link bazel-testlogs -> /home/levkovitch/.cache/bazel/_bazel_levkovitch/81f9c3cb5093a9779b51fe05f62c9c35/execroot/org_tensorflow/bazel-out/k8-opt/testlogs:  /tmp/tensorflow_dir/bazel-testlogs (Permission denied)
  cannot create symbolic link bazel-genfiles -> /home/levkovitch/.cache/bazel/_bazel_levkovitch/81f9c3cb5093a9779b51fe05f62c9c35/execroot/org_tensorflow/bazel-out/k8-opt/bin:  /tmp/tensorflow_dir/bazel-genfiles (Permission denied)
  cannot create symbolic link bazel-out -> /home/levkovitch/.cache/bazel/_bazel_levkovitch/81f9c3cb5093a9779b51fe05f62c9c35/execroot/org_tensorflow/bazel-out:  /tmp/tensorflow_dir/bazel-out (Permission denied)
  cannot create symbolic link bazel-tensorflow_dir -> /home/levkovitch/.cache/bazel/_bazel_levkovitch/81f9c3cb5093a9779b51fe05f62c9c35/execroot/org_tensorflow:  /tmp/tensorflow_dir/bazel-tensorflow_dir (Permission denied)
ERROR: /home/levkovitch/.cache/bazel/_bazel_levkovitch/81f9c3cb5093a9779b51fe05f62c9c35/external/com_google_protobuf/BUILD:294:1: C++ compilation of rule '@com_google_protobuf//:protoc_lib' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/levkovitch/.cache/bazel/_bazel_levkovitch/81f9c3cb5093a9779b51fe05f62c9c35/execroot/org_tensorflow && \
  exec env - \
    PATH=/tmp/bazel_dir/bin:/usr/sbin:/usr/local/sbin:/usr/local/cuda-10.1/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games \
    PWD=/proc/self/cwd \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/code_generator.d '-frandom-seed=bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/code_generator.o' -iquote external/com_google_protobuf -iquote bazel-out/host/bin/external/com_google_protobuf -iquote external/zlib_archive -iquote bazel-out/host/bin/external/zlib_archive -isystem external/com_google_protobuf/src -isystem bazel-out/host/bin/external/com_google_protobuf/src -isystem external/zlib_archive -isystem bazel-out/host/bin/external/zlib_archive '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIE -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 -g0 -DHAVE_PTHREAD -DHAVE_ZLIB -Woverloaded-virtual -Wno-sign-compare -Wno-unused-function -Wno-write-strings -c external/com_google_protobuf/src/google/protobuf/compiler/code_generator.cc -o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/code_generator.o)
Execution platform: @bazel_tools//platforms:host_platform
nvcc fatal   : Unknown option 'MD'
Target //tensorflow:libtensorflow_cc.so failed to build
INFO: Elapsed time: 0.421s, Critical Path: 0.04s
INFO: 2 processes: 2 local.
FAILED: Build did NOT complete successfully
```

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
33210,Error using TensorBoard callback with histogram_freq > 0,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: 
- TensorFlow installed from (source or binary): installed with pip 
- TensorFlow version (use command below): 1.14.0
- Python version: Python 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda_10.1.243_426.00_win10, cudnn-10.1-windows10-x64-v7.6.3.30
- GPU model and memory: GeForce GTX 1060, 6GB dedicated

**Describe the current behavior**
The code in the section bellow results in an error on this line:
https://github.com/tensorflow/tensorflow/blob/025365a736ed10185c1a68c2c896cf5c54b9f69f/tensorflow/python/keras/callbacks_v1.py#L385

The test_function does not have fetch_callbacks defined. The error I get is:
```Python
tensorflow_gpu\lib\site-packages\tensorflow\python\keras\callbacks_v1.py"", line 386, in on_epoch_begin
    self.merged] = self._fetch_callback
AttributeError: 'Function' object has no attribute 'fetch_callbacks'
```
**Describe the expected behavior**
There should be no error. The code works only when ```histogram_freq=0```
**Code to reproduce the issue**
```python 
import numpy as np
import tensorflow as tf
from keras.layers import Input, Dense
from keras.models import Model

from keras.optimizers import SGD

num_features = 100
train_x = np.random.rand(40, num_features)
train_y = np.random.randint(2, size=40)

# The input layer
input_layer = Input(shape=(num_features,), name=""Input"")
output = Dense(10, activation='sigmoid', name=""Hidden_1"")(input_layer)
output = Dense(1, activation='sigmoid', name=""Output"")(output)
model = Model(inputs=input_layer, outputs=output)

sgd = SGD(lr=0.01, decay=1e-4, momentum=0.9, nesterov=True)
model.compile(loss='binary_crossentropy',
                     optimizer=sgd,
                      metrics=['accuracy'])

tensorboard_callback = tf.keras.callbacks.TensorBoard(
            log_dir=os.path.join(""out_dir"", datetime.now().strftime(""%Y%m%d-%H%M%S"")),
            histogram_freq=2, write_graph=True, write_images=True)
my_callbacks = [tensorboard_callback]

model.fit(x=train_x, y=train_y,
                  validation_split=.2,
                  epochs=5,
                  callbacks=my_callbacks)
```
**Other info / logs**
model summary 
```
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Input (InputLayer)           (None, 100)               0         
_________________________________________________________________
Hidden_1 (Dense)             (None, 10)                1010      
_________________________________________________________________
Output (Dense)               (None, 1)                 11        
=================================================================
Total params: 1,021
Trainable params: 1,021
Non-trainable params: 0
_________________________________________________________________
```"
33209,KMP_AFFINITY problem when doing import from keras (backend: TF) and using sklearn,"Have a problem when doing import from keras (backend: TensorFlow) and using sklearn.preprocessing.StandardScaler
details:
Windows10
TensorFlow 1.14 (working with CPU)
keras 2.2.4
pycharm 2019.2.2 (community edition)


To reproduce:
Run(python)-
`from sklearn.preprocessing import StandardScaler`
`import numpy as np`
`from keras.models import Sequential`
`A = np.random.rand(4001,2)`
`scaler = StandardScaler()`
`scaler.fit(X=A)`
`B = scaler.transform(A)`

Resuls:
1. if `from keras.models import Sequential` removed works fine.
2. if the sized of `A` reduce (for 4000 x 2 as example) works fine.
3. else the last step `B = scaler.transform(A)` resulted with-
> Using TensorFlow backend.
> OMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.
> OMP: Info #210: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info
> OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-7
> OMP: Info #156: KMP_AFFINITY: 8 available OS procs
> OMP: Info #157: KMP_AFFINITY: Uniform topology
> OMP: Info #179: KMP_AFFINITY: 1 packages x 4 cores/pkg x 2 threads/core (4 total cores)
> OMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:
> OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 
> OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 0 thread 1 
> OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 1 thread 0 
> OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 1 thread 1 
> OMP: Info #171: KMP_AFFINITY: OS proc 4 maps to package 0 core 2 thread 0 
> OMP: Info #171: KMP_AFFINITY: OS proc 5 maps to package 0 core 2 thread 1 
> OMP: Info #171: KMP_AFFINITY: OS proc 6 maps to package 0 core 3 thread 0 
> OMP: Info #171: KMP_AFFINITY: OS proc 7 maps to package 0 core 3 thread 1 
> OMP: Info #250: KMP_AFFINITY: pid 17364 tid 16076 thread 0 bound to OS proc set 0
"
33208,implementation of VariableOp,"I have question of implementation of VariableOp.  I see it find op_kernel's name when every Compute.
Why do not save var as member variable in object?"
33207,compile tensorflow r1.14 with tensorrt error,"

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos 7.3
- TensorFlow installed from (source or binary):source
- TensorFlow version:1.14
- Python version:3.6
- Bazel version (if compiling from source):0.24.1
- GCC/Compiler version (if compiling from source):6.5
- CUDA/cuDNN version:10.0/7.6.1
- GPU model and memory:tesla V100



**Describe the problem**
Tensorrt 5.1.5
nccl 2.4.8
when i compile tensorflow use  ./configure ,the log is bellow:

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]:


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]:


Please specify the TensorRT version you want to use. [Leave empty to  default to TensorRT 5


Please specify the locally installed NCCL version you want to use. [Leave empty to use http


Please specify the comma-separated list of base paths to look for CUDA libraries and header


Could not find any NvInferVersion.h matching version '5' in any subdirectory:
        ''
        'include'
        'include/cuda'
        'include/*-linux-gnu'
        'extras/CUPTI/include'
        'include/cuda/CUPTI'
of:
        '/lib64'
        '/usr'
        '/usr/lib64/dyninst'
        '/usr/local/cuda'
        '/usr/local/cuda-10.0/targets/x86_64-linux/lib'
Asking for detailed CUDA configuration...



are some versions wrong？ 
"
33206,Importerror when installing costum tf whl in conda environment,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave 10.14.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0.0
- Python version: 3.7
- Installed using virtualenv? pip? conda?: pip & conda
- Bazel version (if compiling from source): 0.26.0
- GCC/Compiler version (if compiling from source): Apple clang version 11.0.0 (clang-1100.0.33.8)
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**
I cannot import tensorflow module in a conda virtual environment (Python 3.7) after installing a costum whl-package with pip of tensorflow 2.0.0.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Followed the building from source steps [here](https://www.tensorflow.org/install/source):
1. Created a new conda python 3.7 environment and installed: pip six numpy wheel setuptools mock 'future>=0.17.1' keras_applications keras_preprocessing
2. Ran configure and set python paths to my new conda env
3. Build pip-package with some CPU flags found [here](https://software.intel.com/en-us/articles/intel-optimization-for-tensorflow-installation-guide):
```shell 
bazel --output_user_root=/Codes/bazel/TensorFlow_2.0.0_pip_package build --jobs=8 --config=mkl -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mavx512f --copt=-mavx512pf --copt=-mavx512cd --copt=-mavx512er --verbose_failures //tensorflow/tools/pip_package:build_pip_package
```

4. Create whl-package
```shell
bazel-bin\tensorflow\tools\pip_package\build_pip_package .
```

5. Installed package in conda environment using pip:
```
conda activate TensorFlow_2.0.0
pip install --upgrade Codes/tensorflow/tensorflow-2.0.0-cp37-cp37m-macosx_10_9_x86_64.whl
```
6. Ran simple import command to test installation:
```
python -c ""import tensorflow as tf""
```
And here occurs the error, that it cannot find a module named tensorflow"
33205,tf.GradientTape() can't train custom subclassing model.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.0
- Python version: 3.7.4
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0, 7.6
- GPU model and memory: 8G

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I tried to make my custom model inherited from tf.keras.Model and I want to training this model by using tf.GradienTape(). 

But it can't train my model. Loss keeps same values for training and isn't reduced.
However, model.complie() and model.fit() with same criterion and optimizer work well for my custom model.
I don't understand why does this situation happen. Which part is problem?

**Code to reproduce the issue**
```
import tensorflow as tf
cfg_imagenet = [64, (128,2), 128, (256,2), 256, (512,2), 512, 512, 512, 512, 512, (1024,2), 1024]
cfg_cifar = [64, 128, 128, 256, 256, 512, 512, 512, 512, 512, 512, 1024, 1024]

class MobileNet(tf.keras.Model):
    def __init__(self, data_format='channels_last'):
        super(MobileNet, self).__init__()
        self.channel_axis = 1 if data_format == 'channels_last' else -1
        self.initializer = tf.keras.initializers.he_normal()

        self.features = self._make_layers()
        self.classifier = tf.keras.layers.Dense(10, kernel_initializer='he_normal')

    def _make_layers(self):
        layers = []
        layers.append(tf.keras.layers.Conv2D(32, kernel_size=3, strides=2, padding='same', name='conv1',
                      kernel_initializer=self.initializer))
        layers.append(tf.keras.layers.BatchNormalization(axis=self.channel_axis, name='conv1_bn'))
        layers.append(tf.keras.layers.ReLU(name='conv1_relu'))
        for idx, x in enumerate(cfg_imagenet):
            i = idx + 1
            filters = x if isinstance(x, int) else x[0]
            strides = 1 if isinstance(x, int) else x[1]
            layers.append(tf.keras.layers.DepthwiseConv2D((3, 3), strides, padding='same', kernel_initializer=self.initializer, name='block{}_dw'.format(i)))
            layers.append(tf.keras.layers.BatchNormalization(axis=self.channel_axis, name='block{}_bn1'.format(i)))
            layers.append((tf.keras.layers.ReLU(name='block{}_relu1'.format(i))))
            layers.append(tf.keras.layers.Conv2D(filters, 1, padding='same', activation='relu', kernel_initializer=self.initializer, name='block{}_pw'.format(i)))
            layers.append(tf.keras.layers.BatchNormalization(axis=self.channel_axis, name='block{}_bn2'.format(i)))
            layers.append((tf.keras.layers.ReLU(name='block{}_relu2'.format(i))))
        layers.append(tf.keras.layers.GlobalAveragePooling2D())
        return layers

    def call(self, input_tensor):
        x = input_tensor
        for layer in self.features:
            x = layer(x)
        x = self.classifier(x)
        return x

criterion = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

def loss_func(model, x, y):
    y_ = model(x)
    return criterion(y, y_)

def grad(model, inputs, targets):
    with tf.GradientTape() as tape:
        loss_value = loss_func(model, inputs, targets)
    return loss_value, tape.gradient(loss_value, model.trainable_variables)

train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))
test_dataset = tf.data.Dataset.from_tensor_slices((test_data, test_labels))
for epoch in range(10):
    train_acc = tf.keras.metrics.SparseCategoricalAccuracy(name='train_acc')
    for x, y in train_dataset:
        loss_value, grads = grad(model, x, y)
        optimizer.apply_gradients(grads_and_vars=zip(grads, model.trainable_variables))
        train_acc(y, model(x))
        print(loss_value)
        print(train_acc.result())
    print(epoch, ""complete"")
```


"
33204,TypeError: can't pickle _thread.RLock objects,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.0.0-beta1
- Python version: Python 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA Version 10.1 
- GPU model and memory: GeForce GTX 1080 Ti

**Describe the current behavior**
I'm using GridSearchCV from sklearn
When I start the training process finishes with `TypeError: can't pickle _thread.RLock objects`

**Describe the expected behavior**
No error, working `grid.fit(train_X, train_Y)`

**Code to reproduce the issue**
```
import tensorflow
from tensorflow.compat.v2.keras.layers import *
from tensorflow.compat.v2.keras.models import Model
from tensorflow.compat.v2.keras.optimizers import *
from tensorflow.compat.v2.keras.preprocessing import image
from tensorflow.compat.v2.keras.wrappers.scikit_learn import KerasClassifier
import numpy as np
from sklearn.model_selection import GridSearchCV

def loadImages(path):
    data = []
    for img_name in os.listdir(path):
        img = image.load_img(path + img_name, target_size=(512, 512, 1))
        img = image.img_to_array(img)
        img = img / 255
        data.append(img)
    return np.array(data)

if __name__ == ""__main__"":
    batch_size = 4
    train_frame_path = '/data/segmentation/train_frames/'
    train_mask_path = '/data/segmentation/train_masks/'
[segmentation_training.txt]

    train_X = loadImages(train_frame_path)
    train_Y = loadImages(train_mask_path)

    m = unet()
    model = KerasClassifier(build_fn=m, epochs=25, batch_size=batch_size, verbose=0)

    optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']
    param_grid = dict(optimizer=optimizer)
    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=3)
    grid_result = grid.fit(train_X, train_Y)
```

Here there is the full code [segmentation_training.txt](https://github.com/tensorflow/tensorflow/files/3712133/segmentation_training.txt)

**Other info / logs**
```


Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/vlongoba/singularity/testForRadio/jackzen.py"", line 120, in gridSearch
    grid_result = grid.fit(train_X, train_Y)
  File ""/home/vlongoba/singularity/.local/lib/python3.6/site-packages/sklearn/model_selection/_search.py"", line 633, in fit
    base_estimator = clone(self.estimator)
  File ""/home/vlongoba/singularity/.local/lib/python3.6/site-packages/sklearn/base.py"", line 64, in clone
    new_object_params[name] = clone(param, safe=False)
  File ""/home/vlongoba/singularity/.local/lib/python3.6/site-packages/sklearn/base.py"", line 55, in clone
    return copy.deepcopy(estimator)
  File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""/usr/lib/python3.6/copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""/usr/lib/python3.6/copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""/usr/lib/python3.6/copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""/usr/lib/python3.6/copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""/usr/lib/python3.6/copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""/usr/lib/python3.6/copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""/usr/lib/python3.6/copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""/usr/lib/python3.6/copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""/usr/lib/python3.6/copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""/usr/lib/python3.6/copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""/usr/lib/python3.6/copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""/usr/lib/python3.6/copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""/usr/lib/python3.6/copy.py"", line 169, in deepcopy
    rv = reductor(4)
TypeError: can't pickle _thread.RLock objects

```"
33203,Memory leak,"- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.0.0
- Python version:3.7
- CUDA/cuDNN version: 10.0/7.4
- GPU model and memory: 1060 6GB

Tell me how to run the code on TF 2.0?
The problem arises only in the second era, as if the memory of the GPU is not cleared.
The first version helps
`tf.keras.backend.get_session().run(tf.global_variables_initializer())`
Code
https://github.com/sgenza/keras_crnn
"
33202,[TF-1.0] RecursionError: maximum recursion depth exceeded,"### System information
- **Have I written custom code**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04.2 LTS (Docker)
- **TensorFlow installed from (source or binary)**: Binary, conda
- **TensorFlow version (use command below)**: unknown 1.14.0
- **Python version**: Python 3.7.3
- **CUDA/cuDNN version**: CUDA=10.0, CUDNN=7.4.1.5-1
- **GPU model and memory**: Quadro RTX 6000 major: 7 minor: 5 memoryClockRate(GHz): 1.77

### Describe the problem
The code below produces a `RecursionError` in TF-1.0 presumably because of the large Dataset. The error does not occur for much smaller values in the `n_files` variable. The error does also not occur in TF-2.0!

**Describe the expected behavior**
No error, working `model.fit()`

**Code to reproduce the issue**
```
import numpy as np
import tensorflow as tf

# TF-2.0
# gpus = tf.config.experimental.list_physical_devices('GPU')
# for gpu in gpus:
#     tf.config.experimental.set_memory_growth(gpu, True)
# #tf.debugging.set_log_device_placement(True)

# TF-1.0
tf.compat.v1.enable_eager_execution()
config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth = True
#config.log_device_placement = True
sess = tf.compat.v1.Session(config=config)
tf.compat.v1.keras.backend.set_session(sess)

assert tf.executing_eagerly()

batch_size = 256
num_tsteps = 144
num_features = 130
num_units = 88

n_files = 3320
#n_files = 10
num_epochs = 1000

seq_len_max_trunc = batch_size * num_tsteps
flen = 3728

X = np.random.rand(flen + 1, num_features + 2)
n_label0 = int((flen + 1) * 0.2)
Y = np.concatenate((
    np.zeros((n_label0, 1)), # label 0
    np.ones((flen - n_label0 + 1, 1)), # label 1
), axis=0)
ds_out = tf.data.Dataset.from_tensor_slices((X, Y))
ds_ser = ds_out.map(lambda *x: 
   tf.reshape(tf.py_function(lambda *v: 
       tf.train.Example(features=tf.train.Features(feature={
           ""features"": tf.train.Feature(float_list=tf.train.FloatList(value=v[0].numpy())),
           ""label"": tf.train.Feature(float_list=tf.train.FloatList(value=v[1].numpy())),
       })).SerializeToString(), x, tf.string
   ), ()), num_parallel_calls=tf.data.experimental.AUTOTUNE
)
writer = tf.data.experimental.TFRecordWriter(""temp.tfrecord"")
writer.write(ds_ser)

files = [""temp.tfrecord""] * n_files

model = tf.keras.Sequential([
    tf.keras.layers.InputLayer(input_shape=(num_tsteps, num_features), batch_size=batch_size),
    #tf.keras.layers.Masking(mask_value=0.0, input_shape=(num_tsteps, num_features)),
    tf.keras.layers.LSTM(num_units,  batch_input_shape=(batch_size, num_tsteps, num_features), return_sequences=True, stateful=False),
    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1)),
    tf.keras.layers.Activation('sigmoid'),
])
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])


def _prep_ds_file(file):
    _ds = tf.data.TFRecordDataset(file)
    _ds = _ds.map(lambda x: tf.io.parse_single_example(x, {
        ""features"": tf.io.FixedLenFeature([132], tf.float32),
        ""label"": tf.io.FixedLenFeature([1], tf.float32),
    }), num_parallel_calls=tf.data.experimental.AUTOTUNE)
        
    _ds = _ds.flat_map(lambda v: tf.data.Dataset.from_tensors((v[""features""][2:], v[""label""])))

    _trunc = min(seq_len_max_trunc, ((flen + 1) // num_tsteps) * num_tsteps)
    _ds = _ds.take(_trunc)

    _c_pad = (batch_size - ((flen + 1) // num_tsteps)) * num_tsteps
    if _c_pad >= 0:
        assert _c_pad + ((flen + 1) // num_tsteps * num_tsteps) == seq_len_max_trunc
        _ds_pad = tf.data.Dataset.from_tensors((
            tf.constant(0.0, shape=[num_features,]),
            tf.constant(0.0, shape=[1,])))
        _ds_pad = _ds_pad.repeat(_c_pad)
        _ds = _ds.concatenate(_ds_pad) # pad to correct size

    _ds = _ds.window(size=num_tsteps, shift=None, stride=1, drop_remainder=True)
    _ds = _ds.flat_map(lambda x, y: tf.data.Dataset.zip((x.batch(num_tsteps), y.batch(num_tsteps))))

    _ds = _ds.batch(batch_size, drop_remainder=True)
    
    return _ds


ds_fs = tf.data.Dataset.list_files(files, shuffle=True, seed=1)
fs_train = ds_fs.take(int(n_files * 0.7))
fs_val = ds_fs.skip(int(n_files * 0.7)).take(int(n_files * 0.1))

ds_train = [_prep_ds_file(f) for f in fs_train.take(1)][0]
for f in fs_train.skip(1):
    ds_train = ds_train.concatenate(_prep_ds_file(f))
ds_train = ds_train.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)

ds_val = [_prep_ds_file(f) for f in fs_val.take(1)][0]
for f in fs_val.skip(1):
    ds_val = ds_val.concatenate(_prep_ds_file(f))
ds_val = ds_val.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)

cbs = [
    tf.keras.callbacks.EarlyStopping(monitor=""val_loss"", patience=10, restore_best_weights=True),
]
model.fit(ds_train, epochs=num_epochs, verbose=1, shuffle=False,
          validation_data=ds_val, validation_steps=None, callbacks=cbs)
```

**Other info / logs**
```
WARNING: Logging before flag parsing goes to stderr.
W1010 10:35:45.397222 140253093480256 deprecation.py:323] From /ws/miniconda3/lib/python3.7/site-packages/tensorflow/python/data/util/random_seed.py:58: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where


RecursionErrorTraceback (most recent call last)
<ipython-input-1-c18884a831b5> in <module>
    109 ]
    110 model.fit(ds_train, epochs=num_epochs, verbose=1, shuffle=False,
--> 111           validation_data=ds_val, validation_steps=None, callbacks=cbs)

/ws/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    692           workers=0,
    693           shuffle=shuffle,
--> 694           initial_epoch=initial_epoch)
    695 
    696     # Case 3: Symbolic tensors or Numpy array-like.

/ws/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1431         shuffle=shuffle,
   1432         initial_epoch=initial_epoch,
-> 1433         steps_name='steps_per_epoch')
   1434 
   1435   def evaluate_generator(self,

/ws/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)
    142       batch_size=batch_size,
    143       epochs=epochs - initial_epoch,
--> 144       shuffle=shuffle)
    145 
    146   do_validation = validation_data is not None

/ws/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py in convert_to_generator_like(data, batch_size, steps_per_epoch, epochs, shuffle)
    475     return data, steps_per_epoch
    476   if isinstance(data, dataset_ops.DatasetV2):
--> 477     return dataset_ops.make_one_shot_iterator(data), steps_per_epoch
    478 
    479   # Create generator from NumPy or EagerTensor Input.

/ws/miniconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py in make_one_shot_iterator(dataset)
   1940     # Call the defined `_make_one_shot_iterator()` if there is one, because some
   1941     # datasets (e.g. for prefetching) override its behavior.
-> 1942     return dataset._make_one_shot_iterator()  # pylint: disable=protected-access
   1943   except AttributeError:
   1944     return DatasetV1Adapter(dataset)._make_one_shot_iterator()  # pylint: disable=protected-access

/ws/miniconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py in _make_one_shot_iterator(self)
   1525   def _make_one_shot_iterator(self):  # pylint: disable=missing-docstring
   1526     if context.executing_eagerly():
-> 1527       return iterator_ops.IteratorV2(self)
   1528 
   1529     _ensure_same_dataset_graph(self)

/ws/miniconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py in __init__(self, dataset)
    562     with ops.device(""/cpu:0""):
    563       # pylint: disable=protected-access
--> 564       dataset = dataset._apply_options()
    565       ds_variant = dataset._variant_tensor
    566       self._structure = dataset._element_structure

/ws/miniconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py in _apply_options(self)
    230 
    231     dataset = self
--> 232     options = self.options()
    233     if options.experimental_threading is not None:
    234       t_options = options.experimental_threading

/ws/miniconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py in options(self)
   1888 
   1889   def options(self):
-> 1890     return self._dataset.options()
   1891 
   1892   @property

/ws/miniconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py in options(self)
    221     options = Options()
    222     for input_dataset in self._inputs():
--> 223       input_options = input_dataset.options()
    224       if input_options is not None:
    225         options = options.merge(input_options)

... last 2 frames repeated, from the frame below ...

/ws/miniconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py in options(self)
   1888 
   1889   def options(self):
-> 1890     return self._dataset.options()
   1891 
   1892   @property

RecursionError: maximum recursion depth exceeded
```"
33201,Tensorflow 2.0 SavedModel format with GPU acceleration slowdown,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): tensorflow==2.0.0-rc1 (2.0.0rc0-gpu-py3 docker image)
- Python version: 3.5.0
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: -
- GPU model and memory: TeslaV100 

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I have a SavedModel trained with Tensorflow 2.0 which I use to make an inference as follows: 

```
 model= tf.saved_model.load(export_dir = 'path/to/savedmodel/1')
 infer = model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]
 outputs = infer(img_tensor)

```
The model predicts fast on a CPU but when using a GPU device, it scales linearly and GPU does not offer any advantage. When logging the device placement, I see that GPU is used.

To compare, I have the same model trained in Tensorflow 1.13 saved in frozen graph format. I use the model as follows:
```

   graph = load_graph('path/to/graph.pb')
   labels = load_labels('path/to/labels.txt')

    input_name = 'import/Placeholder'
    output_name = 'import/final_result'

    input_operation = graph.get_operation_by_name(input_name)
    output_operation = graph.get_operation_by_name(output_name)

    # initialize a tensorflow session
    with tf.compat.v1.Session(graph=graph) as sess:
        results = sess.run(
            output_operation.outputs[0],
            {input_operation.outputs[0]: img_array})

        sess.close()

def load_graph(model_file):
    graph = tf.Graph()
    graph_def = tf.compat.v1.GraphDef()

    with open(model_file, ""rb"") as file:
        graph_def.ParseFromString(file.read())

    with graph.as_default():
        tf.import_graph_def(graph_def)

    return graph

```

The timing of this model on GPU in the same environment is much faster than SavedModel format. I think this is a performance issue.

Here are the timing details:
TF2.0 SavedModel
Batch of 64 images CPU: 23 seconds
Batch of 64 images GPU: 14 seconds

TF13.0 Frozen Graph:
Batch of 64 images CPU: 25 seconds
Batch of 64 images GPU: 5 seconds

**Describe the expected behavior**
My expected behaviour is that Tensorflow 2.0 version with SavedModel would use GPU acceleration. Maybe I am missing on how to import the SavedModel in python. Please let me know what could be the route cause of it.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
33200,"CancelledError: RecvAsync is cancelled during ""fit"" (module 'gast' has no attribute 'Num')","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0.0 (gpu)
- Python version: 3.7.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: v10.1
- GPU model and memory: GTX 970 4GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
During ""fit"" of the first batch it gives a CancelledError

**Describe the expected behavior**
To ""fit"" without errors

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
data: English: (4549, 15)x5687; Afrikaans: (4549, 15)x5982 --- (vsize, timesteps)xsamples
z_train = OneHotEncoder(sparse=False, n_values=af_vsize).fit_transform(y_train[:,1:]).reshape(-1, af_timesteps-1, af_vsize)

model:
encoder_inputs = Input(batch_size=32, shape=(en_timesteps,), name='encoder_inputs')
encoder_embeddings = Embedding(en_vsize, 128)(encoder_inputs)
decoder_inputs = Input(batch_size=32, shape=(af_timesteps-1,), name='decoder_inputs')
decoder_embeddings = Embedding(af_vsize, 128, mask_zero=True)(decoder_inputs)
encoder_out, encoder_state = GRU(128, return_sequences=True, return_state=True, name='encoder_gru')(encoder_embeddings)
decoder_out, decoder_state = GRU(128, return_sequences=True, return_state=True, name='decoder_gru')(decoder_embeddings, initial_state=encoder_state)
attn_out = Attention(name='attention_layer')([decoder_out, encoder_out])
decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_out, attn_out])
dense = Dense(af_vsize, activation='softmax', name='softmax_layer')
dense_time = TimeDistributed(dense, name='time_distributed_layer')
decoder_pred = dense_time(decoder_concat_input)
model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

The error occurs when i run:
h = model.fit([X_train, y_train[:,:-1]], z_train, validation_data=([X_test, y_test[:,:-1]], z_test), epochs=1)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Train on 4094 samples, validate on 455 samples
WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002112B520B70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'
WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002112B520B70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'
  32/4094 [..............................] - ETA: 9:06
---------------------------------------------------------------------------
CancelledError                            Traceback (most recent call last)
<ipython-input-17-592a9f974384> in <module>
      1 for i in range(50):
----> 2     h = model.fit([X_train, y_train[:,:-1]], z_train, validation_data=([X_test, y_test[:,:-1]], z_test), epochs=1)
      3     history.append(h.history)

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    726         max_queue_size=max_queue_size,
    727         workers=workers,
--> 728         use_multiprocessing=use_multiprocessing)
    729 
    730   def evaluate(self,

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\keras\engine\training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
    322                 mode=ModeKeys.TRAIN,
    323                 training_context=training_context,
--> 324                 total_epochs=epochs)
    325             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)
    326 

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\keras\engine\training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)
    121         step=step, mode=mode, size=current_batch_size) as batch_logs:
    122       try:
--> 123         batch_outs = execution_function(iterator)
    124       except (StopIteration, errors.OutOfRangeError):
    125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py in execution_function(input_fn)
     84     # `numpy` translates Tensors to values in Eager mode.
     85     return nest.map_structure(_non_none_constant_value,
---> 86                               distributed_function(input_fn))
     87 
     88   return execution_function

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\eager\def_function.py in __call__(self, *args, **kwds)
    455 
    456     tracing_count = self._get_tracing_count()
--> 457     result = self._call(*args, **kwds)
    458     if tracing_count == self._get_tracing_count():
    459       self._call_counter.called_without_tracing()

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\eager\def_function.py in _call(self, *args, **kwds)
    518         # Lifting succeeded, so variables are initialized and we can run the
    519         # stateless function.
--> 520         return self._stateless_fn(*args, **kwds)
    521     else:
    522       canon_args, canon_kwds = \

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\eager\function.py in __call__(self, *args, **kwargs)
   1821     """"""Calls a graph function specialized to the inputs.""""""
   1822     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
-> 1823     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   1824 
   1825   @property

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\eager\function.py in _filtered_call(self, args, kwargs)
   1139          if isinstance(t, (ops.Tensor,
   1140                            resource_variable_ops.BaseResourceVariable))),
-> 1141         self.captured_inputs)
   1142 
   1143   def _call_flat(self, args, captured_inputs, cancellation_manager=None):

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\eager\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1222     if executing_eagerly:
   1223       flat_outputs = forward_function.call(
-> 1224           ctx, args, cancellation_manager=cancellation_manager)
   1225     else:
   1226       gradient_name = self._delayed_rewrite_functions.register()

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\eager\function.py in call(self, ctx, args, cancellation_manager)
    509               inputs=args,
    510               attrs=(""executor_type"", executor_type, ""config_proto"", config),
--> 511               ctx=ctx)
    512         else:
    513           outputs = execute.execute_with_cancellation(

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\eager\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     65     else:
     66       message = e.message
---> 67     six.raise_from(core._status_to_exception(e.code, message), None)
     68   except TypeError as e:
     69     keras_symbolic_tensors = [

C:\ProgramData\Anaconda3\lib\site-packages\six.py in raise_from(value, from_value)

CancelledError:  [_Derived_]RecvAsync is cancelled.
	 [[{{node metrics/accuracy/broadcast_weights/assert_broadcastable/AssertGuard/else/_36/Assert/data_4/_92}}]]
	 [[loss/time_distributed_layer_loss/weighted_loss/broadcast_weights/assert_broadcastable/is_valid_shape/else/_1/has_valid_nonscalar_shape/then/_106/has_invalid_dims/concat/_58]] [Op:__inference_distributed_function_13318]

Function call stack:
distributed_function"
33199,use tf.saved_model.simple_save get {ValueError}At least two variables have the same name: InceptionResnetV1/Block8/Branch_0/Conv2d_1x1/BatchNorm/beta/ExponentialMovingAverage,"use tf.saved_model.simple_save get {ValueError}At least two variables have the same name: InceptionResnetV1/Block8/Branch_0/Conv2d_1x1/BatchNorm/beta/ExponentialMovingAverage;
tf.saved_model.simple_save(sess,
                                          './models/test_pd3',
                                        inputs={""input"": x},
                                          outputs={""output"": output})


What I don't understand is use graph_util.convert_variables_to_constants saved Success"
33198,"Tensorflow 2.0, feature_column and input_layer","Documentation for tf.feature_column like for example:
https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file

uses input_layer, which is not available in v2
```
columns = [embedding_column(states, 3),...]
features = tf.io.parse_example(..., features=make_parse_example_spec(columns))
dense_tensor = input_layer(features, columns)
```
"
33197,About a function ： tf.train.slice_input_producer(),"when I use the function ：input_queue = tf.train.slice_input_producer([image, label]).
A error come to me:AttributeError: module 'tensorflow_core._api.v2.train' has no attribute 'slice_input_producer'

I want to know how can I solve the problem.

is there anyone who can help me. 
"
33194,Why my tensorflow-gpu runs only on  GPU?,"Not work TF 2.0
```
config = tf.compat.v1.ConfigProto(
        device_count = {'GPU': 0}
    )
sess = tf.compat.v1.Session(config=config)
tf.compat.v1.keras.backend.set_session(sess)
```"
33193,AutoGraph returns an empty array when I use a for-loop and TensorArray,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.6
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0
- Python version: 3.6.x
- Bazel version (if compiling from source): - 
- GCC/Compiler version (if compiling from source): - 
- CUDA/cuDNN version: - 
- GPU model and memory: -

Hello! Here is a sniped of code:
```python
a = np.array([1, 2, 3], np.int32)

@tf.function  # works without the decorator
def foo(a):
  b = tf.TensorArray(tf.string, 4)
  b.write(0, ""test"")
  for i in tf.range(3):
    if a[i] == 2:
      b.write(i, ""fuzz"")
    elif a[i] == 3:
      b.write(i, ""buzz"")
  return b.stack()

print(foo(a))
```
With the decorator, the AutoGraph returns an empty array:
`tf.Tensor([b'' b'' b'' b''], shape=(4,), dtype=string)`

However, it shoud return:
`tf.Tensor([b'test' b'fuzz' b'buzz' b''], shape=(4,), dtype=string)`
"
33192,Incorrect result when subtracting 1 from exponential of Variable ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0
- Python version: 3.7.3
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: Intel Iris Plus Graphics 640 1536 MB

**Describe the current behavior**
With the following setup
```
session = tf.Session()
variable = tf.Variable(4j)
exp = tf.exp(variable)
session.run(tf.global_variables_initializer())
```
the following code
```
print(session.run(exp-1))
print(session.run(exp)-1)
```
produces two different outputs, specifically:
```
(2.897081749192498+1.3258713481172573j)
(-1.6536436208636118-0.7568024953079282j)
```

**Describe the expected behavior**
That code should print identical arrays (the latter is correct, the former is not). 

**Code to reproduce the issue**
```
session = tf.Session()
variable = tf.Variable(4j)
exp = tf.exp(variable)
session.run(tf.global_variables_initializer())

print(session.run(exp-1))
print(session.run(exp)-1)
```
Also see Colab notebook here: https://colab.research.google.com/drive/1KT2gfuWeezhWOr3zfyHhk9HvCH7JcShm

**Other info / logs**
I have no idea what's happening, but some observations:
 - there needs to be a Variable involved. If I use a constant instead of a Variable, all works as expected.
 - the exponent value is important. If I change the 4's to 1's then all works as expected, for example.
 - the TF exponential is important. If instead I calculate the exponential with numpy and then set the Variable to that exponential, subtracting 1 works correctly.
 - specifically subtracting 1 seems important. If instead I subtract 2, or 0.1, or 1j, all works as expected."
33191,_resampler_ops.so not found,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Win10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.10.0
- Python version:3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
in my Win10 OS,i create a virtualevn with pycharm, one of is tensorflow1.10.0, and another is tensorflow2.0.0,when tf2.0 works, that run tf1.10 project,it broken
bug info:
tensorflow.python.framework.errors_impl.NotFoundError: E:\python\lib\site-packages\tensorflow\contrib\resampler\python\ops\_resampler_ops.so not found 
base issues 20320,i remove this file, it works,but there is 10+ files need to remove,
https://github.com/tensorflow/tensorflow/issues/20320

and next i run this project ,it happen again

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
33190,A bug in 'MoveBinaryOperatorBeforeReshape' graph_transformations  during tflite_convert.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from source
- TensorFlow version (use command below): ('v2.0.0-0-g64c3d38', '2.0.0')
- Python version: 2.7.12
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
- CUDA/cuDNN version: 10.0 / 7.5
- GPU model and memory: GeForce GTX TITAN

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
The outputs of the tflite model (*.tflite), converted by the tflite_convert, are totally different from the original pre-trained saved_model.

When I tries to fix that, I finally got the right outputs by suppressing 'MoveBinaryOperatorBeforeReshape' of the graph_transformations (tensorflow/lite/toco/graph_transformations).  To do this, I removed the operations from parts of the source codes (tensorflow/lite/toco/BUILD, tensorflow/lite/toco/toco_tooling.cc, ,tensorflow/lite/toco/graph_transformations/graph_transformations.h) and built a package from it.

I compared the two versions of the tflite models, by using the visualization tool (bazel run //tensorflow/lite/tools:visualize model.tflite visualized_model.html). I found out that when the MoveBinaryOperatorBeforeReshape is turned on, the tflite_convert removes a specific ADD op and its one constant input tensor from my graph. Then it was the very reason I got the different output. 

This seems a bug to me so that I want to report this.

Since my graph is too big, and I cannot share my codes, models nor figures, I want to share the responsible part of the visualization results. I strongly recommend to draw graphs for better understanding.

Consider following as tables,  columns are seperated by '|'.

***Original Graph (which gives wrong output)***
Operations
```
index | inputs | outputs | builtin_options | opcode_index
69 | [175, 43, 4154] | [185] | {u'fused_activation_function':   u'NONE', u'stride_h': 1, u'padding': u'VALID', u'stride_w': 1,   u'dilation_h_factor': 1, u'dilation_w_factor': 1} | CONV_2D (2)
100 | [185, 186] | [190] | {u'new_shape': [1, 600, 512]} | RESHAPE (5)
126 | [190, 1518] | [1517] | {u'keep_dims': True} | MEAN (10)
127 | [190, 1517] | [1529] | {u'fused_activation_function':   u'NONE'} | SUB (11)
128 | [190, 1517] | [1528] | {u'fused_activation_function':   u'NONE'} | SUB (11)
341 | [1539, 190] | [1516] | {u'fused_activation_function':   u'NONE'} | ADD (0)
```
Tensors
```
index | name | type | shape | buffer | quantization
43 | Identity_16 | FLOAT32 | [512, 1, 1, 240] | 2051 | {u'quantized_dimension': 0,   u'details_type': 0}
175 | body/model/parallel_0/body/ExpandDims_2 | FLOAT32 | [1, 600, 1, 240] | 1816 | {u'quantized_dimension': 0,   u'details_type': 0}
4154 | body/model/parallel_0/body/prepare_transform_before_single/Conv2D_bias | FLOAT32 | [512] | 2053 | {u'quantized_dimension': 0,   u'details_type': 0}
185 | body/model/parallel_0/body/Squeeze | FLOAT32 | [1, 600, 1, 512] | 2338 | {u'quantized_dimension': 0,   u'details_type': 0}
186 | body/model/parallel_0/body/Squeeze_shape | INT32 | [3] | 2110 | {u'quantized_dimension': 0,   u'details_type': 0}
190 | body/model/parallel_0/body/add_1 | FLOAT32 | [1, 600, 512] | 1098 | {u'quantized_dimension': 0,   u'details_type': 0}
```



***Fixed Graph ('MoveBinaryOperatorBeforeReshape' suppressed and gives the right outputs)***
Operations
```
index | inputs | outputs | builtin_options | opcode_index
69 | [175, 43, 4156] | [4155] | {u'fused_activation_function':   u'NONE', u'stride_h': 1, u'padding': u'VALID', u'stride_w': 1,   u'dilation_h_factor': 1, u'dilation_w_factor': 1} | CONV_2D (2)
98 | [4155, 186] | [185] | {u'new_shape': [1, 600, 512]} | RESHAPE (5)
101 | [190, 185] | [191] | {u'fused_activation_function':   u'NONE'} | ADD (0)
127 | [191, 1519] | [1518] | {u'keep_dims': True} | MEAN (10)
128 | [191, 1518] | [1530] | {u'fused_activation_function':   u'NONE'} | SUB (11)
129 | [191, 1518] | [1529] | {u'fused_activation_function':   u'NONE'} | SUB (11)
342 | [1540, 191] | [1517] | {u'fused_activation_function':   u'NONE'} | ADD (0)
```
Tensors 
```
index | name | type | shape | buffer | quantization
43 | Identity_16 | FLOAT32 | [512, 1, 1, 240] | 2055 | {u'quantized_dimension': 0,   u'details_type': 0}
175 | body/model/parallel_0/body/ExpandDims_2 | FLOAT32 | [1, 600, 1, 240] | 1819 | {u'quantized_dimension': 0,   u'details_type': 0}
4156 | body/model/parallel_0/body/prepare_transform_before_single/Conv2D_bias | FLOAT32 | [512] | 2057 | {u'quantized_dimension': 0,   u'details_type': 0}
4155 | body/model/parallel_0/body/prepare_transform_before_single/BiasAdd | FLOAT32 | [1, 600, 1, 512] | 1547 | {u'quantized_dimension': 0,   u'details_type': 0}
186 | body/model/parallel_0/body/Squeeze_shape | INT32 | [3] | 2114 | {u'quantized_dimension': 0,   u'details_type': 0}
190 | body/model/parallel_0/body/add | FLOAT32 | [1, 600, 512] | 1845 | {u'quantized_dimension': 0,   u'details_type': 0}
185 | body/model/parallel_0/body/Squeeze | FLOAT32 | [1, 600, 512] | 2342 | {u'quantized_dimension': 0,   u'details_type': 0}
191 | body/model/parallel_0/body/add_1 | FLOAT32 | [1, 600, 512] | 1098 | {u'quantized_dimension': 0,   u'details_type': 0}
```

Rest of the operations are the same. Total number of graphs are different only by 1. In the fixed graph, the ADD op 101 has one constant tensor (with none-zero values) and one variable tensor input. It is missing from the original graph, I don't understand why the tflite_convert removes it.


**Code to reproduce the issue**
I could not find reproducing code.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
33189,TFTRT:  Huge events file generated.,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.14
- Python version:2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.1
- GPU model and memory:T4

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Very very large events files  are generated  when TF-TRT is enabled.  These files are so big that Tensorboard is able to handle those.  For example,  in one network, TF produces an event file of 4.6 MB size. The  same network after TF-TRT compile, generates an event file of 1.3 GB !! 


**Describe the expected behavior**

A reasonable size. 

**Code to reproduce the issue**
Not able to provide any code at this point.   However, the sympton is possibly reproducible with any large network such as ResNet1XX . 

**Other info / logs**

"
33188,Duplicated Gradient issue during TF Lite conversion with 1.13.2,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source with tag v1.13.2
- TensorFlow version (use command below): b'v1.13.2-0-g04256c89d8' 1.13.2
- Python version:3.6.5
- Bazel version (if compiling from source): 0.20.0
- GCC/Compiler version (if compiling from source): 4.9.2
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
The following error happened when I was convert my model to tf.lite. It's quite similar to #24525 .
```python
Traceback (most recent call last):
  File ""tfconvert.py"", line 18, in <module>
    tflite_quantized_model = converter.convert()
  File ""/home/user/.local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 455, in convert
    **converter_kwargs)
  File ""/home/user/.local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py"", line 442, in toco_convert_impl
    input_data.SerializeToString())
  File ""/home/user/.local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py"", line 205, in toco_convert_protos
    ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
2019-10-10 10:12:14.730948: F tensorflow/core/framework/function.cc:1626] Check failed: GetOpGradFactory()->insert({op, func}).second Duplicated gradient for MapAccumulate
```

**Code to reproduce the issue**
```python
converter = tf.lite.TFLiteConverter.from_saved_model(
    model_path, input_arrays=input_arrays, output_arrays=output_arrays, input_shapes=input_shapes)
converter.post_training_quantize=True
converter.target_ops = [
    # tf.lite.OpsSet.TFLITE_BUILTINS,
    tf.lite.OpsSet.SELECT_TF_OPS
]
tflite_quantized_model = converter.convert()
open(""quantized_model.tflite"", ""wb"").write(tflite_quantized_model)
```

Any help will be appreciated! Thanks in advance."
33187,"Support ""fetch_skip_sync=false"" in Callable","**System information**
- TensorFlow version (you are using): 1.15.0
- Are you willing to contribute it (Yes/No): yes

**Describe the feature and the current behavior/state.**
As suggested in #5902, feeding / fetching GPU tensors is possible with Callable, however, [`fetch_skip_sync`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/config.proto#L764) must be set to true as the otherwise is not implemented. Based on the comment for `fetch_skip_sync` field, then the user will need to invoke `Device::Sync()`, which can be bad if the device is being shared with other models.

**Will this change the current api? How?**

**Who will benefit with this feature?**
Use case that serves multiple models on the same GPU device.

**Any Other info.**
After some digging around, it seems like Callable just adds a ""RetVal"" op to the original graph and take the original output as its input, which I don't see any data movement. So my other question is whether the `Device::Sync()` is necessary?"
33185,Link to TensorBoard: Graph Visualization is broken,"I am reading about tensorboard here: https://www.tensorflow.org/tensorboard/r1/summaries

On this page the link to *TensorBoard: Graph Visualization* is broken. This is the link: https://www.tensorflow.org/tensorboard/guide/graph_viz"
33184,TF-TRT batchSize > 0 && batchSize <= MAX_BATCH_SIZE,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):1.14.0
- Python version:2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.1
- GPU model and memory:T4

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Seeing the following error.

```
2019-10-09 22:38:11.629630: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:632] Building a new TensorRT engine for XXX input shapes: [[0,28,28,128]]
2019-10-09 22:38:11.629841: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:41] DefaultLogger Parameter check failed at: ../builder/builder.cpp::setMaxBatchSize::113, condition: batchSize > 0 && batchSize <= MAX_BATCH_SIZE
```

**Describe the expected behavior**
Ideally, would not like to see the error.   As currently seeing  the error,  more information along the following line will help a lot determining the root cause.

1.  See more detailed  log on what the offending  layers and tensors are. 
2.  What  exactly is the value that is the issue, in this case batchSize ? 
3. Where  in  code this is  occurring - do not see any file called builder* in TF 1.14 source. 
    If error in happening elsewhere in third-party code ( in this case , may be in NVIDIA  TensorRT closed source )  - making that explicit. 
4. In this case, where is it getting 0 for batch size. 

**Code to reproduce the issue**
It is not possible to provide code for every issue.  Better logging is a much more  efficient way to root cause and resolve issues. 

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
33183,RuntimeError: dictionary changed size during iteration,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.15 Beta (virtual environment)
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0.0
- Python version: 3.7.3
- Installed using virtualenv? pip? conda?: virtualenv pip

**Describe the problem**
I run into this issue where I can't test the most basic Tensorflow program cause it gives me this error at the end:
RuntimeError: dictionary changed size during iteration

I was trying to work on a virtual environment I had already setup but it wasn't working either and it kept giving me that error. So I created a new one from cero and it keeps giving me the error. The tensorflow is properly installed (according to the instructions) but when I try to run the simple program to test it. It gives me the error. I'll add to the log the error and the log of when the Tensorflow is being installed. I hope this is not an unique issue. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
➜  ML python3 -m venv env
➜  ML source env/bin/activate
(env) ➜  ML pip install --upgrade pip
(env) ➜  ML pip install tensorflow
(env) ➜  ML python -c ""import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))""
_and here I get the error._ 
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
**(env) ➜  ML pipip install tensorflow**

Collecting tensorflow
  Using cached https://files.pythonhosted.org/packages/2c/72/6b3264aa2889b7dde7663464b99587d95cd6a5f3b9b30181f14d78a63e64/tensorflow-2.0.0-cp37-cp37m-macosx_10_11_x86_64.whl
Collecting wrapt>=1.11.1 (from tensorflow)
Collecting keras-preprocessing>=1.0.5 (from tensorflow)
  Using cached https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl
Collecting gast==0.2.2 (from tensorflow)
Collecting keras-applications>=1.0.8 (from tensorflow)
  Using cached https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl
Collecting tensorboard<2.1.0,>=2.0.0 (from tensorflow)
  Using cached https://files.pythonhosted.org/packages/9b/a6/e8ffa4e2ddb216449d34cfcb825ebb38206bee5c4553d69e7bc8bc2c5d64/tensorboard-2.0.0-py3-none-any.whl
Collecting tensorflow-estimator<2.1.0,>=2.0.0 (from tensorflow)
  Using cached https://files.pythonhosted.org/packages/95/00/5e6cdf86190a70d7382d320b2b04e4ff0f8191a37d90a422a2f8ff0705bb/tensorflow_estimator-2.0.0-py2.py3-none-any.whl
Collecting numpy<2.0,>=1.16.0 (from tensorflow)
  Using cached https://files.pythonhosted.org/packages/b4/e8/5ececadd9cc220bb783b4ce6ffaa9266925d37ed41237bc23bc530ab4f3d/numpy-1.17.2-cp37-cp37m-macosx_10_6_intel.whl
Collecting termcolor>=1.1.0 (from tensorflow)
Collecting google-pasta>=0.1.6 (from tensorflow)
  Using cached https://files.pythonhosted.org/packages/d0/33/376510eb8d6246f3c30545f416b2263eee461e40940c2a4413c711bdf62d/google_pasta-0.1.7-py3-none-any.whl
Collecting grpcio>=1.8.6 (from tensorflow)
  Using cached https://files.pythonhosted.org/packages/75/07/f1d41d10519ca165b0e078949078f20beb57e7e46dc0f1d56b73bb01270a/grpcio-1.24.1-cp37-cp37m-macosx_10_9_x86_64.whl
Collecting opt-einsum>=2.3.2 (from tensorflow)
Collecting wheel>=0.26 (from tensorflow)
  Using cached https://files.pythonhosted.org/packages/00/83/b4a77d044e78ad1a45610eb88f745be2fd2c6d658f9798a15e384b7d57c9/wheel-0.33.6-py2.py3-none-any.whl
Collecting six>=1.10.0 (from tensorflow)
  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl
Collecting absl-py>=0.7.0 (from tensorflow)
Collecting protobuf>=3.6.1 (from tensorflow)
  Using cached https://files.pythonhosted.org/packages/a5/c6/a8b6a74ab1e165f0aaa673a46f5c895af8780976880c98934ae82060356d/protobuf-3.10.0-cp37-cp37m-macosx_10_9_intel.whl
Collecting astor>=0.6.0 (from tensorflow)
  Using cached https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl
Collecting h5py (from keras-applications>=1.0.8->tensorflow)
  Using cached https://files.pythonhosted.org/packages/1a/8b/4d01ae9a9d50a0bcc7b0b9aae41785d8d9de6fa9bba04dc20b1582181d2d/h5py-2.10.0-cp37-cp37m-macosx_10_6_intel.whl
Collecting werkzeug>=0.11.15 (from tensorboard<2.1.0,>=2.0.0->tensorflow)
  Using cached https://files.pythonhosted.org/packages/ce/42/3aeda98f96e85fd26180534d36570e4d18108d62ae36f87694b476b83d6f/Werkzeug-0.16.0-py2.py3-none-any.whl
Collecting markdown>=2.6.8 (from tensorboard<2.1.0,>=2.0.0->tensorflow)
  Using cached https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl
Collecting setuptools>=41.0.0 (from tensorboard<2.1.0,>=2.0.0->tensorflow)
  Using cached https://files.pythonhosted.org/packages/6a/9a/50fadfd53ec909e4399b67c74cc7f4e883488035cfcdb90b685758fa8b34/setuptools-41.4.0-py2.py3-none-any.whl
Installing collected packages: wrapt, six, numpy, keras-preprocessing, gast, h5py, keras-applications, grpcio, wheel, werkzeug, setuptools, markdown, protobuf, absl-py, tensorboard, tensorflow-estimator, termcolor, google-pasta, opt-einsum, astor, tensorflow
  Found existing installation: setuptools 40.8.0
    Uninstalling setuptools-40.8.0:
      Successfully uninstalled setuptools-40.8.0
Successfully installed absl-py-0.8.1 astor-0.8.0 gast-0.2.2 google-pasta-0.1.7 grpcio-1.24.1 h5py-2.10.0 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.1.1 numpy-1.17.2 opt-einsum-3.1.0 protobuf-3.10.0 setuptools-41.4.0 six-1.12.0 tensorboard-2.0.0 tensorflow-2.0.0 tensorflow-estimator-2.0.0 termcolor-1.1.0 werkzeug-0.16.0 wheel-0.33.6 wrapt-1.11.2
**(env) ➜  ML pypython -c ""import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))""**

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/Users/marialopez/projects/ML/env/lib/python3.7/site-packages/tensorflow/__init__.py"", line 98, in <module>
    from tensorflow_core import *
  File ""/Users/marialopez/projects/ML/env/lib/python3.7/site-packages/tensorflow_core/__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 959, in _find_and_load_unlocked
  File ""/Users/marialopez/projects/ML/env/lib/python3.7/site-packages/tensorflow/__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""/Users/marialopez/projects/ML/env/lib/python3.7/site-packages/tensorflow/__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/Users/marialopez/projects/ML/env/lib/python3.7/site-packages/tensorflow_core/python/__init__.py"", line 52, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""/Users/marialopez/projects/ML/env/lib/python3.7/site-packages/tensorflow_core/core/framework/graph_pb2.py"", line 7, in <module>
    from google.protobuf import descriptor as _descriptor
  File ""/Users/marialopez/projects/ML/env/lib/python3.7/site-packages/google/protobuf/__init__.py"", line 37, in <module>
    __import__('pkg_resources').declare_namespace(__name__)
  File ""/Users/marialopez/projects/ML/env/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 84, in <module>
    __import__('pkg_resources.extern.packaging.requirements')
  File ""/Users/marialopez/projects/ML/env/lib/python3.7/site-packages/pkg_resources/_vendor/packaging/requirements.py"", line 9, in <module>
    from pkg_resources.extern.pyparsing import stringStart, stringEnd, originalTextFor, ParseException
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 668, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 638, in _load_backward_compatible
  File ""/Users/marialopez/projects/ML/env/lib/python3.7/site-packages/pkg_resources/extern/__init__.py"", line 43, in load_module
    __import__(extant)
  File ""/Users/marialopez/projects/ML/env/lib/python3.7/site-packages/pkg_resources/_vendor/pyparsing.py"", line 4756, in <module>
    _escapedPunc = Word( _bslash, r""\[]-*.$+^?()~ "", exact=2 ).setParseAction(lambda s,l,t:t[0][1])
  File ""/Users/marialopez/projects/ML/env/lib/python3.7/site-packages/pkg_resources/_vendor/pyparsing.py"", line 1284, in setParseAction
    self.parseAction = list(map(_trim_arity, list(fns)))
  File ""/Users/marialopez/projects/ML/env/lib/python3.7/site-packages/pkg_resources/_vendor/pyparsing.py"", line 1066, in _trim_arity
    this_line = extract_stack(limit=2)[-1]
  File ""/Users/marialopez/projects/ML/env/lib/python3.7/site-packages/pkg_resources/_vendor/pyparsing.py"", line 1050, in extract_stack
    frame_summary = traceback.extract_stack(limit=-offset+limit-1)[offset]
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/traceback.py"", line 211, in extract_stack
    stack = StackSummary.extract(walk_stack(f), limit=limit)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/traceback.py"", line 363, in extract
    f.line
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/traceback.py"", line 285, in line
    self._line = linecache.getline(self.filename, self.lineno).strip()
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/linecache.py"", line 16, in getline
    lines = getlines(filename, module_globals)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/linecache.py"", line 48, in getlines
    for mod in sys.modules.values():
RuntimeError: dictionary changed size during iteration
```"
33182,tf.compat.v1.keras.backend.get_session gives erroneous deprecation warning,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0
- Python version: 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

Accessing `tf.compat.v1.keras.backend.get_session` gives a deprecation warning telling you to use `tf.compat.v1.keras.backend.get_session`.

**Describe the expected behavior**

Accessing `tf.compat.v1.keras.backend.get_session` should not give a deprecation warning (since we're explicitly accessing the `compat.v1` namespace). Or it should direct the user to the correct function, if `tf.compat.v1.keras.backend.get_session` is not the right access point, rather than telling the user to use the namespace they're already using.

**Code to reproduce the issue**
``` python
import tensorflow as tf

tf.compat.v1.keras.backend.get_session
```

This results in the warning
```
The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.
```
(note that we're accessing `tf.compat.v1.keras.backend.get_session`, not `tf.keras.backend.get_session`).
"
33181,Build TF 2.0 Windows,"` C++ compilation of rule '//tensorflow/core/kernels:eigen_contraction_kernel_with_mkl' failed (Exit 2): python.exe failed: error executing command
`

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- Python version: 3.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): vs 2017
- CUDA/cuDNN version: 7.4
- GPU model and memory: 1060 6GB



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
33180,_list_all_concrete_functions_for_serialization doubles the trace count when called? ,"I'm seeing the new warning ""triggered tf.function retracing. Tracing is expensive and the excessive number of tracings"" and was digging into it. When I printed *everything* that went through the offending function I only saw two tensors by (type, shape). This lead me to notice the following weird event that bumps (doubles) the trace count.

Is there some reason for this? If this is normal, I will try a minimal reproduction of the warning message that seems to not make sense.

```
import numpy as np
import tensorflow as tf

@tf.function
def f(a):
    tf.print(a.shape)
    return a


f(tf.convert_to_tensor(np.random.randn(4, 3).astype(np.float32))); print(f._get_tracing_count())
f(tf.convert_to_tensor(np.random.randn(4, 3).astype(np.float32))); print(f._get_tracing_count())
f(tf.convert_to_tensor(np.random.randn(4, 3).astype(np.float32))); print(f._get_tracing_count())
f(tf.convert_to_tensor(np.random.randn(4, 2).astype(np.float32))); print(f._get_tracing_count())
f(tf.convert_to_tensor(np.random.randn(4, 1).astype(np.float32))); print(f._get_tracing_count())
print(f._list_all_concrete_functions_for_serialization())
print(f._get_tracing_count())
```

Results in 

```
TensorShape([4, 3])
1
TensorShape([4, 3])
1
TensorShape([4, 3])
1
TensorShape([4, 2])
2
TensorShape([4, 1])
3
[<tensorflow.python.eager.function.ConcreteFunction object at 0x7fa32c03b438>, <tensorflow.python.eager.function.ConcreteFunction object at 0x7fa3247f4438>, <tensorflow.python.eager.function.ConcreteFunction object at 0x7fa3247f4f60>]
6
```


**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
33179,Module Not Found in docker build: tensorflow_datasets,"I have been following the Docker instructions on the Tensorflow website (https://www.tensorflow.org/install/docker).
Trying to run the Jupyter notebook provided in the tensorflow docker images, specifically trying to run the tutorial here: https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/text_classification.ipynb

But when I run the code segments in sequence, I get the following error:

```
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-3-7eaaf952f6e2> in <module>
      1 from tensorflow import keras
      2 
----> 3 import tensorflow_datasets as tfds
      4 tfds.disable_progress_bar()
      5 

ModuleNotFoundError: No module named 'tensorflow_datasets'
```

This would seem to be an issue with the Docker image since I would expect that module to be installed by default. 

**System information**
- OS Platform and Distribution: MacOS 10.15
- TensorFlow installed from (source or binary): 
`docker run -it -p 8888:8888 tensorflow/tensorflow:nightly-py3-jupyter`

"
33178,Potential memory leak when using LSTM + TimeDistributed,"------------------------

System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  Yes
- **OS Platform and Distribution**: Windows 10
- **TensorFlow installed from**: binary
- **TensorFlow version**: v2.0.0-rc2-26-g64c3d382ca 2.0.0
- **Python version**: 3.6.9
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**:  10.0 (CUDA) / 7.5 (cuDNN)
- **GPU model and memory**: TITAN RTX (24GB)
- **Exact command to reproduce**: N/A

### Describe the problem
#### Potential memory leak when using LSTM + TimeDistributed
I have a standard time series model that consists 3 layers of convolutional layers feeding into 2 LSTM layers. Up until now, I have had no problems mapping a Dense layer to the last output of the top LSTM and making a prediction etc. However, I want to implement a model where I use a TimeDistributed(Dense(..)) layer on top of the top LSTM and feed back the error signal at each time point. I have implemented this but after only training a few epochs, I get a resource exhausted error.  

It doesn't seem to be affected by how small I make the model, after training for a few epochs. The error I get is: ""ResourceExhaustedError: OOM when allocating tensor with shape[25600,9,11,128]"". This comes after a call to tape.gradients (full error reported in section below).

In my non-TimeDistributed I monitor the number of objects via len(gc.get_objects())) and during training the object count remains the same (as expected), but when I only change the model to handle this TimeDistributed change (i.e. making sure the labels are correctly repeated and making return_sequences=1 for the top-level LSTM) then all of a sudden at each training epoch, thousands of new variables are being added during each epoch of training.

gc objects: 249861
[TRAIN]: End (epoch 0): loss 0.693269372 ; train accuracy 0.5
[TEST]:  End (epoch 0): loss 0.691318274 ; test accuracy 0.500683606
gc objects: 251746 (+ 1885 objects)
[TRAIN]: End (epoch 1): loss 0.691800237 ; train accuracy 0.500202894
[TEST]:  End (epoch 1): loss 0.690349817 ; test accuracy 0.502343774
gc objects: 254144 (+ 2398 objects)
[TRAIN]: End (epoch 2): loss 0.690762699 ; train accuracy 0.500456572
[TEST]:  End (epoch 2): loss 0.689480364 ; test accuracy 0.504296899
gc objects: 254996 (+852 objects)
[TRAIN]: End (epoch 3): loss 0.692312837 ; train accuracy 0.501090705
[TEST]:  End (epoch 3): loss 0.689140499 ; test accuracy 0.505468726
gc objects: 269643 (+ 14647 objects)
[TRAIN]: End (epoch 4): loss 0.688487 ; train accuracy 0.501116097
[TEST]:  End (epoch 4): loss 0.686942577 ; test accuracy 0.508886695
gc objects: 270444 (+ 801 objects)

So in 4 epochs of training, while no other process is running, 20,583 new objects were created and I presume resulted in this resource exhausted error.

I've tried to force the garbage collector to collect any unused variables but the object count increases whether this is included or not. I ran a snapshot comparison from the **tracemalloc** library, which I will include below as it might be helpful (it wasn't to me).

Something is creating variables during every epoch, vastly using up all the memory and not releasing them, leading to this resource exhausted error. This doesn't occur if I don't use TimeDistributed, so I don't think anything about this layer requires the creation of additional memory-hungry variables. It looks more like a leak. 

Do you have any idea of what I could do to alleviate this problem? It seems like a bug fix at a technical level. Maybe there is a technical solution. Please let me know if any further info from my end would be useful in looking at this issue.

### Source code / logs

#### tracemalloc top 10 differences between snapshot calls at adjacent epochs
C:\Users\AXM1390\AppData\Local\Continuum\anaconda3\envs\tf2\lib\site-packages\tensorflow_core\python\eager\execute.py:61: size=111 KiB (+69.9 KiB), count=677 (+426), average=168 B
<string>:14: size=7464 B (-46.9 KiB), count=107 (-749), average=70 B
C:\Users\AXM1390\AppData\Local\Continuum\anaconda3\envs\tf2\lib\tokenize.py:609: size=2944 B (-43.6 KiB), count=46 (-698), average=64 B
C:\Users\AXM1390\AppData\Local\Continuum\anaconda3\envs\tf2\lib\site-packages\tensorflow_core\python\framework\tensor_shape.py:193: size=59.9 KiB (+33.8 KiB), count=1305 (+732), average=47 B
C:\Users\AXM1390\AppData\Local\Continuum\anaconda3\envs\tf2\lib\site-packages\tensorflow_core\python\training\tracking\data_structures.py:768: size=54.0 KiB (+31.3 KiB), count=386 (+219), average=143 B
C:\Users\AXM1390\AppData\Local\Continuum\anaconda3\envs\tf2\lib\site-packages\tensorflow_core\python\framework\tensor_shape.py:718: size=55.7 KiB (+30.8 KiB), count=1018 (+564), average=56 B
C:\Users\AXM1390\AppData\Local\Continuum\anaconda3\envs\tf2\lib\site-packages\tensorflow_core\python\framework\tensor_shape.py:776: size=51.0 KiB (+28.7 KiB), count=1235 (+690), average=42 B
C:\Users\AXM1390\AppData\Local\Continuum\anaconda3\envs\tf2\lib\site-packages\tensorflow_core\python\keras\utils\generic_utils.py:564: size=40.9 KiB (+25.8 KiB), count=675 (+426), average=62 B
C:\Users\AXM1390\AppData\Local\Continuum\anaconda3\envs\tf2\lib\site-packages\tensorflow_core\python\framework\ops.py:1035: size=39.3 KiB (+23.3 KiB), count=950 (+566), average=42 B
C:\Users\AXM1390\AppData\Local\Continuum\anaconda3\envs\tf2\lib\site-packages\tensorflow_core\python\training\tracking\data_structures.py:809: size=27.1 KiB (+15.9 KiB), count=3 (+0), average=9248 B

#### full error
ResourceExhaustedError                    Traceback (most recent call last)
<ipython-input-14-422df5497d33> in <module>
----> 1 best_val, best_epoch, tmp_history = run_model_once(0, 25, epochs=50)

<ipython-input-12-61614e3bb222> in run_model_once(start, end, epochs)
     36         printed_cm = False
     37 
---> 38         train_loss, val_loss, acc_metric, val_acc_metric = train(RCNN_model, optimizer, train_ds, test_ds, cm)
     39         tf.print(f'[TRAIN]: End (epoch {i}): loss', train_loss, '; train accuracy', acc_metric.result())
     40         tf.print(f'[TEST]:  End (epoch {i}): loss', val_loss, '; test accuracy', val_acc_metric.result())

<ipython-input-11-00ed72fa26fb> in train(model, optimizer, train_ds, test_ds, cm)
     60     for x_true, y_true in train_ds:
     61         if TIME_DISTRIBUTED:
---> 62             train_loss = train_one_step_timedistributed(model, optimizer, x_true, y_true, training=True)
     63         else:
     64             train_loss = train_one_step(model, optimizer, x_true, y_true, training=True)

<ipython-input-11-00ed72fa26fb> in train_one_step_timedistributed(model, optimizer, x_true, y_true, training)
     22         print(f'model trainable variables: {len(model.trainable_variables)}')
     23 
---> 24     gradients = tape.gradient(loss_, model.trainable_variables)
     25     optimizer.apply_gradients(zip(gradients, model.trainable_variables))
     26 

~\AppData\Local\Continuum\anaconda3\envs\tf2\lib\site-packages\tensorflow_core\python\eager\backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)
   1012         output_gradients=output_gradients,
   1013         sources_raw=flat_sources_raw,
-> 1014         unconnected_gradients=unconnected_gradients)
   1015 
   1016     if not self._persistent:

~\AppData\Local\Continuum\anaconda3\envs\tf2\lib\site-packages\tensorflow_core\python\eager\imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)
     74       output_gradients,
     75       sources_raw,
---> 76       compat.as_str(unconnected_gradients.value))

~\AppData\Local\Continuum\anaconda3\envs\tf2\lib\site-packages\tensorflow_core\python\eager\backprop.py in _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)
    136     return [None] * num_inputs
    137 
--> 138   return grad_fn(mock_op, *out_grads)
    139 
    140 

~\AppData\Local\Continuum\anaconda3\envs\tf2\lib\site-packages\tensorflow_core\python\ops\math_grad.py in _TanhGrad(op, grad)
    712   with ops.control_dependencies([grad]):
    713     y = math_ops.conj(y)
--> 714     return gen_math_ops.tanh_grad(y, grad)
    715 
    716 

~\AppData\Local\Continuum\anaconda3\envs\tf2\lib\site-packages\tensorflow_core\python\ops\gen_math_ops.py in tanh_grad(y, dy, name)
  11410       else:
  11411         message = e.message
> 11412       _six.raise_from(_core._status_to_exception(e.code, message), None)
  11413   # Add nodes to the TensorFlow graph.
  11414   _, _, _op = _op_def_lib._apply_op_helper(

~\AppData\Local\Continuum\anaconda3\envs\tf2\lib\site-packages\six.py in raise_from(value, from_value)

ResourceExhaustedError: OOM when allocating tensor with shape[25600,9,11,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:TanhGrad]
"
33177,Efficiency of model.fit_generator() are greatly reduced in 2.0.0,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below):  2.0.0
- Python version: 3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0, 7.6
- GPU model and memory: Titan Xp, 12G

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
My code takes 21s/epoch in tf 2.0.0alpha while it takes 76s/epoch in tf 2.0.0. I found that the problem is model.fit_generator(). When I replace it with model.fit(), the efficiency of the code is exactly the same in both versions. But I need to use ImageDataGenerator.

**Describe the expected behavior**
The efficiency of the code should be same.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
(train_images, train_labels, test_images, test_labels) = load_CIFAR('/home/user/Documents/dataset/Cifar-10')
datagen = ImageDataGenerator(horizontal_flip=True,
                                 width_shift_range=0.125,
                                 height_shift_range=0.125,
                                 fill_mode='constant', cval=0.)
datagen.fit(train_images)
datagenflow = datagen.flow(train_images, train_labels, batch_size=batch_size)
model.fit_generator(datagenflow,
              steps_per_epoch=iterations,
              epochs=epoch_num,
              callbacks=[change_lr],
              validation_data=(test_images, test_labels))

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
33176,"op name couldn't contain '_', '/' ?","Hi :

```
Name of the node - tower_0/Concat/concat/axis
Name of the node - tower_0/Concat/concat
Name of the node - tower_0/MatMul
Name of the node - tower_0/Add
Name of the node - tower_0/MatMul_1
Name of the node - tower_0/Add_1
Name of the node - tower_0/MatMul_2
Name of the node - tower_0/Add_2
Name of the node - tower_0/Sigmoid
Name of the node - tower_0/pre_2/shape
Name of the node - tower_0/pre_2
Name of the node - tower_0/MatMul_3
Name of the node - tower_0/Add_3
Name of the node - tower_0/MatMul_4
Name of the node - tower_0/Add_4
Name of the node - tower_0/MatMul_5
```

The above is part of op names in my model. When I use tensorflow c api:TF_GraphGetOpDef to get op detail, I found a problem: 
The op name which contain '_' or '/' couldn't be found, the message is ：`Op type not registered 'tower_0/Add_4' in binary running on xxx. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.`

The op name which do not contain '_' or '/' could be found.  Why?
"
33175,tf1.12 keras input shape does not match the correct data feed,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): 
- TensorFlow version (use command below): 1.12
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.0.176
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
```
import tensorflow as tf
from tensorflow.python import keras
import numpy as np
r_input = keras.layers.Input(shape=(), dtype=tf.float32, name='input_tensor')
r_output = keras.layers.Lambda(lambda x: x+1.)(r_input)
final_model = keras.models.Model(r_input, r_output)
c_input = np.asarray([1.])
ret = final_model.predict(c_input)
print(ret)
```
Raise `ValueError: Error when checking input: expected input_tensor to have 1 dimensions, but got array with shape (1, 1)`

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

In TF2.0, the code listed above can be run correctly. So I think it is a bug in tf1.12.

How to fix in tf1.12?
Line `r_input = keras.layers.Input(shape=(), dtype=tf.float32, name='input_tensor')`
can be replaced to `r_input = keras.layers.Input(shape=(1, ), dtype=tf.float32, name='input_tensor')` to fix this problem.
"
33174,"TF2.0 OOM error training imagenet with vgg, fine when eager execution off","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.0
- Python version: 3.7
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: V10.0.130
- GPU model and memory: GeForce RTX 2080ti, 10G

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Getting OOM error when training vgg16 on imagenet with batch_size=256. Was able to get away with OOM error with batch_size=8 or turning off eager execution.
**Describe the expected behavior**
The exact same code was running fine with tf1.14 with no memory problem
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
import numpy as np
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam


imagenet_test = 'some image directory'

model = VGG16(weights='imagenet', include_top=True, input_shape=(224, 224, 3))
model.compile('adam', loss='sparse_categorical_crossentropy', metrics=['acc'])

datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
test_generator = datagen.flow_from_directory(directory=imagenet_test,
                                            batch_size=256,
                                            class_mode='sparse',
                                            target_size=(224, 224))
steps = np.ceil(len(test_generator.classes) / 256)

model.evaluate_generator(test_generator, steps, verbose=1)
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[256,64,224,224] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Conv2D]
```"
33173,[tf2.0] tf.signal.fft2d causes an error when using @tf.function,"tf_version: 2.0.0-beta1
```
import numpy as np
import tensorflow as tf
from tensorflow.python.keras.layers import *
from tensorflow.python.keras import backend as K
from tensorflow.python.keras.models import Model

def build_fftNet(img_shape = (224, 512, 1)):
    input_f = Input(shape=(img_shape))
    
    ins = Lambda(lambda x: (K.mean(x, axis=3)+1)*127.5)(input_f)
    ins = Lambda(lambda x: tf.complex(tf.math.real(x), tf.zeros_like(x)))(ins) # float to complex
    
    fft = Lambda(lambda x: tf.signal.fft2d(ins))(ins)
    fftNet = Model([input_f], [fft], name='fft')

    return fftNet

img_shape = (224, 512, 1)
fftNet = build_fftNet(img_shape)

@tf.function
def fft(blur):
    fft_true = fftNet(blur, training=False)
    return fft_true

blur = np.zeros((1, *img_shape), np.float32)
fft(blur)
```
**@tf.function** gives the following error when using **tf.signal.fft2d**:
```
Traceback (most recent call last):

  File ""<ipython-input-122-2efd6297c220>"", line 21, in <module>
    fft(blur)

  File ""C:\Users\user\Anaconda3\envs\tf20\lib\site-packages\tensorflow\python\eager\def_function.py"", line 434, in __call__
    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access

  File ""C:\Users\user\Anaconda3\envs\tf20\lib\site-packages\tensorflow\python\eager\function.py"", line 589, in _filtered_call
    (t for t in nest.flatten((args, kwargs), expand_composites=True)

  File ""C:\Users\user\Anaconda3\envs\tf20\lib\site-packages\tensorflow\python\eager\function.py"", line 671, in _call_flat
    outputs = self._inference_function.call(ctx, args)

  File ""C:\Users\user\Anaconda3\envs\tf20\lib\site-packages\tensorflow\python\eager\function.py"", line 445, in call
    ctx=ctx)

  File ""C:\Users\user\Anaconda3\envs\tf20\lib\site-packages\tensorflow\python\eager\execute.py"", line 70, in quick_execute
    raise core._SymbolicException

_SymbolicException
```"
33172,add expm_multiply functionality,"**System information**
- TensorFlow version (you are using): 2.0.0
- Are you willing to contribute it (Yes/No): Yes, but I need help

**Describe the feature and the current behavior/state.**
It would be great to implement the equivalent of `scipy.sparse.linalg.expm_multiply`, so that one doesn't have to create a whole matrix exponential before multiplying it with a vector, and instead compute the result directly. 

**Will this change the current api? How?**
It will add a new function

**Who will benefit with this feature?**
Those who need to use the matrix exponential on vectors

**Any Other info.**
See [this paper](http://eprints.ma.man.ac.uk/1591/)  and [this paper](http://eprints.ma.man.ac.uk/1451/) for the implementation "
33169,Can we set a proper range of bazel?,"```
[17:01:55] fagn:tensorflow git:(master) $ ./configure
WARNING: Running Bazel server needs to be killed, because the startup options are different.
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.23.0 installed.
Please upgrade your bazel installation to version 0.24.1 or higher to build TensorFlow!
[17:02:35] fan:tensorflow git:(master) $ ./configure
Extracting Bazel installation...
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.29.1 installed.
Please downgrade your bazel installation to version 0.26.1 or lower to build TensorFlow! To downgrade: download the installer for the old version (from https://github.com/bazelbuild/bazel/releases) then run the installer.

```

Messages said higher..... but not upper bounds.... then installed newest version got need lower....

Why one sentence can not said in one line?"
33168,[2.0] Disable usage of GPU using the new config APIs,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.0

**Describe the feature and the current behavior/state.**
For 2.0, there are some new APIs to configure GPU devices (https://www.tensorflow.org/api_docs/python/tf/config/experimental).

However, by using the current APIs, we have to select at least one GPU to be visible [[source code](https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/eager/context.py#L1174-L1202)].

A similar report is [here](https://github.com/tensorflow/tensorflow/issues/26460#issuecomment-515707199).

It would be useful if we can completely disable the usage of GPUs for `tensorflow-gpu` release.
"
33167,Typo,"https://github.com/tensorflow/docs/blame/master/site/en/tutorials/load_data/text.ipynb#L166

Has the word ""labeled"" twice"
33165,TF1.13.1 Bazel Build Error,"
**System information**
- OS Platform and Distribution :Windows 10
- TensorFlow installed from: source
- TensorFlow version: 1.13.1
- Python version: 3.5.2
- Bazel version :  0.20.0
- CUDA/cuDNN version: 10.0/7.6.4

**Describe the problem**

ERROR: Skipping '//tensorflow:libtensorflow_cc.so': error loading package 'tensorflow': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
        File ""C:/d/tensorflow-1.13.1/source/third_party/gpus/cuda_configure.bzl"", line 1556
                _create_local_cuda_repository(repository_ctx)
        File ""C:/d/tensorflow-1.13.1/source/third_party/gpus/cuda_configure.bzl"", line 1395, in _create_local_cuda_repository
                find_cc(repository_ctx)
        File ""C:/d/tensorflow-1.13.1/source/third_party/gpus/cuda_configure.bzl"", line 239, in find_cc
                _get_msvc_compiler(repository_ctx)
        File ""C:/d/tensorflow-1.13.1/source/third_party/gpus/cuda_configure.bzl"", line 156, in _get_msvc_compiler
                find_msvc_tool(repository_ctx, vc_path, ""cl.exe"").replace(""\\"", ""/"")
type 'NoneType' has no method replace(string, string)
WARNING: Target pattern parsing failed.
ERROR: error loading package 'tensorflow': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
        File ""C:/d/tensorflow-1.13.1/source/third_party/gpus/cuda_configure.bzl"", line 1556
                _create_local_cuda_repository(repository_ctx)
        File ""C:/d/tensorflow-1.13.1/source/third_party/gpus/cuda_configure.bzl"", line 1395, in _create_local_cuda_repository
                find_cc(repository_ctx)
        File ""C:/d/tensorflow-1.13.1/source/third_party/gpus/cuda_configure.bzl"", line 239, in find_cc
                _get_msvc_compiler(repository_ctx)
        File ""C:/d/tensorflow-1.13.1/source/third_party/gpus/cuda_configure.bzl"", line 156, in _get_msvc_compiler
                find_msvc_tool(repository_ctx, vc_path, ""cl.exe"").replace(""\\"", ""/"")
type 'NoneType' has no method replace(string, string)



"
33163,"'Can save best model only with val_loss available, skipping.' during tf.keras.callbacks.ModelCheckpoint","My model is built like:
```
Model:  ""sequential""
Layer (type)                 Output Shape              Param #   
=================================================================
dense (Dense)                (None, 1379)              3801903   
_________________________________________________________________
dropout (Dropout)            (None, 1379)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 1379)              1903020   
_________________________________________________________________
dropout_1 (Dropout)          (None, 1379)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 1379)              1903020   
_________________________________________________________________
dropout_2 (Dropout)          (None, 1379)              0         
_________________________________________________________________
dense_3 (Dense)              (None, 1379)              1903020   
_________________________________________________________________
dropout_3 (Dropout)          (None, 1379)              0         
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 1380      
=================================================================
Total params: 9,512,343
Trainable params: 9,512,343
Non-trainable params: 0
```
Here the model is fitted with a callback.
```
checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=filepath, mode='max', monitor='val_acc', verbose=2, save_best_only=True)
callbacks_list = [checkpoint]
model.fit(train_dataset, epochs=1000, callbacks=callbacks_list, verbose=2, steps_per_epoch=(number_of_samples//BATCH_SIZE))

```
This gives me the warning:
```
Epoch 2/1000
W1009 01:11:32.446842 11824 callbacks.py:990] Can save best model only with val_acc available, skipping.
```"
33162,Continue training on SavedModel or load checkpoint from SavedModel,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.14
- Are you willing to contribute it (Yes/No): Yes


**Describe the feature and the current behavior/state.**
In tensorflow 1.14, it's obvious that tf.compat.v1.train.init_from_checkpoint can load ckpt to continue training (or to warm start). However, I couldn't find any corresponding approaches in SavedModel, and tf.estimator.WarmStartSetting only supports ckpt as well. It would be helpful if we could either load checkpoint in SavedModel or warm-start training from SavedModel.

**Will this change the current api? How?**
1. train.init_from_checkpoint could load SavedModel 
2. tf.estimator.WarmStartSettings could load SavedModel

**Who will benefit with this feature?**
Those who store models in SavedModel format and want to keep training (or warm start).

**Any Other info.**
"
33161,TF2.0: Tensorboard The connection was reset,"I have the same problem but w/ TF2.0-GPU, and I am on Chrome/Ubuntu, running it inside a docker container w/ juypter, miniconda, cuda-x, tensorrt. The error I get says: The connection was reset when trying to display it in the notebook, despite everything else working fine. (I'm not sure, but even when I goto localhost:6006 it doesn't load either).

**Error 1:**
![1](https://user-images.githubusercontent.com/5199900/66453018-6f651e00-ea28-11e9-8db4-6961c3cd15cc.png)

**Error 2:**
![2](https://user-images.githubusercontent.com/5199900/66453028-7be97680-ea28-11e9-890a-ec46d30abe9b.png)

**Dockerfile:**
```
FROM nvidia/cuda:10.0-cudnn7-devel-ubuntu18.04

# Core Linux Deps
RUN DEBIAN_FRONTEND=noninteractive apt-get update && apt-get install -y --fix-missing --no-install-recommends apt-utils \
        build-essential \
        curl \
	binutils \
	gdb \
        git \
	freeglut3 \
	freeglut3-dev \
	libxi-dev \
	libxmu-dev \
	gfortran \
        pkg-config \
	python-numpy \
	python-dev \
	python-setuptools \
	libboost-python-dev \
	libboost-thread-dev \
        pbzip2 \
        rsync \
        software-properties-common \
        libblas3 \
        liblapack3 \
        liblapack-dev \
        libblas-dev \
        libboost-all-dev \
        libopenblas-dev \ 
        libtbb2 \
        libtbb-dev \
        libjpeg-dev \
        libpng-dev \
        libtiff-dev \
	libgraphicsmagick1-dev \
        libavresample-dev \
        libavformat-dev \
        libhdf5-dev \
        libpq-dev \
	libgraphicsmagick1-dev \
	libavcodec-dev \
	libgtk2.0-dev \
	liblapack-dev \
        liblapacke-dev \
	libswscale-dev \
	libcanberra-gtk-module \
        libboost-dev \
	libboost-all-dev \
        libeigen3-dev \
	wget \
        vim \
        qt5-default \
        unzip \
	zip \ 
        && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*  && \
    apt-get clean && rm -rf /tmp/* /var/tmp/*


# Install cmake version that supports anaconda python path
RUN wget -O cmake.tar.gz https://github.com/Kitware/CMake/releases/download/v3.15.4/cmake-3.15.4-Linux-x86_64.tar.gz
RUN tar -xvf cmake.tar.gz
WORKDIR /cmake-3.15.4-Linux-x86_64
RUN cp -r bin /usr/
RUN cp -r share /usr/
RUN cp -r doc /usr/share/
RUN cp -r man /usr/share/
WORKDIR /
RUN rm -rf cmake-3.15.4-Linux-x86_64
RUN rm -rf cmake.tar.gz


# Install TensorRT (TPU Access)
RUN apt-get update && \
        apt-get install -y nvinfer-runtime-trt-repo-ubuntu1804-5.0.2-ga-cuda10.0 && \
        apt-get update && \
        apt-get install -y libnvinfer5=5.0.2-1+cuda10.0

RUN file=""$(ls -1 /usr/local/)"" && echo $file


# Fix conda errors per Anaconda team until they can fix
RUN mkdir ~/.conda


# Install Anaconda
RUN wget --quiet https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh && \
/bin/bash Miniconda3-latest-Linux-x86_64.sh -f -b -p /opt/conda && \
rm Miniconda3-latest-Linux-x86_64.sh
ENV PATH /opt/conda/bin:$PATH


# For CUDA profiling, TensorFlow requires CUPTI.
ENV LD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH

ARG PYTHON=python3
ARG PIP=pip3

# See http://bugs.python.org/issue19846
ENV LANG C.UTF-8

RUN apt-get update && apt-get install -y \
    ${PYTHON} \
    ${PYTHON}-pip

RUN ${PIP} --no-cache-dir install --upgrade \
    pip \
    setuptools \
    hdf5storage \
    h5py \
    matplotlib \
    pyinstrument

# Add auto-complete to Juypter
RUN pip install jupyter-tabnine
RUN pip install pydash

RUN conda update -n base -c defaults conda
RUN conda install -c anaconda jupyter 
RUN conda install pytorch torchvision cudatoolkit=10.0 -c pytorch
RUN conda update conda
RUN conda install numba
RUN conda install -c anaconda cupy 
RUN conda install -c anaconda ipykernel 
RUN conda install -c anaconda seaborn 
RUN conda install -c anaconda ipython 


# Some TF tools expect a ""python"" binary
RUN ln -s $(which ${PYTHON}) /usr/local/bin/python 

RUN pip install tf-nightly-gpu-2.0-preview
#RUN pip install tensorflow-gpu

#COPY bashrc /etc/bash.bashrc
#RUN chmod a+rwx /etc/bash.bashrc

RUN ${PIP} --no-cache-dir install jupyter  


WORKDIR /
RUN wget -O opencv.zip https://github.com/opencv/opencv/archive/master.zip
RUN wget -O opencv_contrib.zip https://github.com/opencv/opencv_contrib/archive/master.zip
RUN unzip opencv.zip
RUN unzip opencv_contrib.zip
RUN mv opencv-master opencv
RUN mv opencv_contrib-master opencv_contrib
RUN mkdir /opencv/build
WORKDIR /opencv/build

RUN cmake -DBUILD_TIFF=ON \
		  -DBUILD_opencv_java=OFF \
		  -DWITH_CUDA=ON \
		  -DENABLE_FAST_MATH=1 \
		  -DCUDA_FAST_MATH=1 \
		  -DWITH_CUBLAS=1 \
		  -DENABLE_AVX=ON \
		  -DWITH_OPENGL=ON \
		  -DWITH_OPENCL=OFF \
		  -DWITH_IPP=ON \
		  -DWITH_TBB=ON \
		  -DWITH_EIGEN=ON \
		  -DWITH_V4L=ON \
		#   -DBUILD_TESTS=OFF \
		#   -DBUILD_PERF_TESTS=OFF \
		  -DCMAKE_BUILD_TYPE=RELEASE \
		  -DCMAKE_INSTALL_PREFIX=$(python -c ""import sys; print(sys.prefix)"") \
		  -D PYTHON3_EXECUTABLE=$(which python3) \
                  -D PYTHON_INCLUDE_DIR=$(python3 -c ""from distutils.sysconfig import get_python_inc; print(get_python_inc())"") \
                  -D PYTHON_INCLUDE_DIR2=$(python3 -c ""from os.path import dirname; from distutils.sysconfig import get_config_h_filename; print(dirname(get_config_h_filename()))"") \
                  -D PYTHON_LIBRARY=$(python3 -c ""from distutils.sysconfig import get_config_var;from os.path import dirname,join ; print(join(dirname(get_config_var('LIBPC')),get_config_var('LDLIBRARY')))"") \
                  -D PYTHON3_PACKAGES_PATH=$(python3 -c ""from distutils.sysconfig import get_python_lib; print(get_python_lib())"") \
                  -DOPENCV_ENABLE_NONFREE=ON \
                  -DOPENCV_EXTRA_MODULES_PATH=/opencv_contrib/modules \
                  -DBUILD_EXAMPLES=ON \
                  -D CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-10.0 \
                  -DWITH_QT=ON ..
                 
RUN make -j4 \
        && make install \
	&& rm /opencv.zip \
        && rm /opencv_contrib.zip \
	&& rm -rf /opencv \
        && rm -rf /opencv_contrib


WORKDIR /


# dlib
RUN cd ~ && \
    mkdir -p dlib && \
    git clone -b 'v19.16' --single-branch https://github.com/davisking/dlib.git dlib/ && \
    cd  dlib/ && \
    python3 setup.py install --yes USE_AVX_INSTRUCTIONS --yes DLIB_USE_CUDA --clean


RUN mkdir /.local && chmod a+rwx /.local

WORKDIR /tf
EXPOSE 8888 6006


RUN useradd -ms /bin/bash container_user


RUN ${PYTHON} -m ipykernel.kernelspec


CMD [""bash"", ""-c"", ""source /etc/bash.bashrc && jupyter notebook --notebook-dir=/tf --ip 0.0.0.0 --no-browser --allow-root --NotebookApp.custom_display_url='http://localhost:8888'""]

``` #"
33160,Node 'gradients_boolean_mask_1_gatherv2_grad_shape_boolean_mask_1_reshape' expects to be colocated with unknown node 'boolean_mask_1/Reshape',"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux 7 (Core)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I was getting `ValueError: Node 'gradients_boolean_mask_1_gatherv2_grad_shape_boolean_mask_1_reshape' expects to be colocated with unknown node 'boolean_mask_1/Reshape'

**Describe the expected behavior**

**Code to reproduce the issue**
```
def flow(y_e, y_a, y_t, y_pair, atoms, adjacency_map, coordinates, atom_in_mol,
    bond_in_mol, angle_in_mol, torsion_in_mol, attr_in_mol):

    per_mol_mask = tf.matmul(
        tf.where(
            atom_in_mol,
            tf.ones_like(atom_in_mol, dtype=tf.float32),
            tf.zeros_like(atom_in_mol, dtype=tf.float32)),
        tf.transpose(
            tf.where(
                atom_in_mol,
                tf.ones_like(atom_in_mol, dtype=tf.float32),
                tf.zeros_like(atom_in_mol, dtype=tf.float32))))

    bond_idxs, angle_idxs, torsion_idxs = gin.probabilistic.gn_hyper\
        .get_geometric_idxs(atoms, adjacency_map)

    is_bond = tf.greater(
        adjacency_map,
        tf.constant(0, dtype=tf.float32))

    distance_matrix = gin.deterministic.md.get_distance_matrix(
        coordinates)

    bond_distances = tf.boolean_mask(
        distance_matrix,
        is_bond)

    angle_angles = gin.deterministic.md.get_angles(
        coordinates,
        angle_idxs)

    torsion_dihedrals = gin.deterministic.md.get_dihedrals(
        coordinates,
        torsion_idxs)

    u_bond = tf.math.reduce_sum(
        tf.math.multiply(
            y_e,
            tf.math.pow(
                tf.expand_dims(
                    bond_distances,
                    1),
                tf.range(16, dtype=tf.float32))),
        axis=1)

    u_angle = tf.math.reduce_sum(
        tf.math.multiply(
            y_a,
            tf.math.pow(
                tf.expand_dims(
                    angle_angles,
                    1),
                tf.range(16, dtype=tf.float32))),
        axis=1)

    u_dihedral = tf.math.reduce_sum(
        tf.math.multiply(
            y_t,
            tf.math.pow(
                tf.expand_dims(
                    torsion_dihedrals,
                    1),
                tf.range(16, dtype=tf.float32))),
        axis=1)

    u_pair = tf.reduce_sum(
            tf.multiply(
                y_pair,
                tf.math.pow(
                    tf.expand_dims(
                            tf.where(
                                tf.logical_and(
                                    tf.equal(
                                        tf.eye(
                                            tf.shape(
                                                distance_matrix)[0],
                                            dtype=tf.float32),
                                        tf.constant(0, dtype=tf.float32)),
                                    tf.greater(
                                        distance_matrix,
                                        tf.constant(0, dtype=tf.float32))),
                                tf.pow(
                                    distance_matrix + 1e-2,
                                    -1),
                                distance_matrix),

                        axis=2),
                    tf.range(1, 16, dtype=tf.float32))),
            axis=2)

    u_pair_mask = tf.linalg.band_part(
        tf.nn.relu(
            tf.subtract(
                tf.subtract(
                    per_mol_mask,
                    adjacency_map),
                tf.eye(
                    tf.shape(per_mol_mask)[0]))),
        0, -1)

    u_pair = tf.multiply(
        u_pair_mask,
        u_pair)

    u_bond_tot = tf.matmul(
        tf.transpose(
            tf.where(
                bond_in_mol,
                tf.ones_like(bond_in_mol, dtype=tf.float32),
                tf.zeros_like(bond_in_mol, dtype=tf.float32))),
        tf.expand_dims(
            u_bond,
            axis=1))

    u_angle_tot = tf.matmul(
        tf.transpose(
            tf.where(
                angle_in_mol,
                tf.ones_like(angle_in_mol, dtype=tf.float32),
                tf.zeros_like(angle_in_mol, dtype=tf.float32))),
        tf.expand_dims(
            u_angle,
            axis=1))

    u_dihedral_tot = tf.matmul(
        tf.transpose(
            tf.where(
                torsion_in_mol,
                tf.ones_like(torsion_in_mol, dtype=tf.float32),
                tf.zeros_like(torsion_in_mol, dtype=tf.float32))),
        tf.expand_dims(
            u_dihedral,
            axis=1))

    u_pair_tot = tf.boolean_mask(
        tf.matmul(
            tf.transpose(
                tf.where(
                    atom_in_mol,
                    tf.ones_like(atom_in_mol, dtype=tf.float32),
                    tf.zeros_like(atom_in_mol, dtype=tf.float32))),
            tf.reduce_sum(
                u_pair,
                axis=1,
                keepdims=True)),
        attr_in_mol)

    u_tot = tf.squeeze(
        u_bond_tot + u_angle_tot + u_dihedral_tot + u_pair_tot)

    return u_tot

 with tf.GradientTape(persistent=True) as tape:
                tape.watch(coordinates)
                y_e, y_a, y_t, y_pair, bond_in_mol, angle_in_mol, torsion_in_mol = gn(
                    atoms, adjacency_map, coordinates, atom_in_mol, attr_in_mol)

                u_hat = flow(y_e, y_a, y_t, y_pair, atoms, adjacency_map,
                    coordinates, atom_in_mol, bond_in_mol, angle_in_mol,
                    torsion_in_mol, attr_in_mol)

                jacobian_hat = tape.gradient(
                    u_hat,
                    coordinates)

                jacobian_hat = tf.boolean_mask(
                    jacobian_hat,
                    tf.reduce_any(
                        atom_in_mol,
                        axis=1))

                jacobian = tf.boolean_mask(
                    jacobian,
                    tf.reduce_any(
                        atom_in_mol,
                        axis=1))

                u = tf.boolean_mask(
                    u,
                    attr_in_mol)

                loss = tf.reduce_sum(tf.keras.losses.MSE(u, u_hat)) + tf.reduce_sum(tf.keras.losses.MSE(
                    jacobian, jacobian_hat))

                print(loss)

            variables = gn.variables
            grad = tape.gradient(loss, variables)

```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
33159,AttributeError: module 'gast' has no attribute 'Num',"System Information：
    python == 3.5.4
    tensorflow == 2.0.0-beta0

Description：
When I running the seq2seq with attention ,it report an warning
WARNING:tensorflow:Entity <function train_step at 0x000000002BF42510> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function train_step at 0x000000002BF42510>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method Encoder.call of <__main__.Encoder object at 0x00000000307476A0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Encoder.call of <__main__.Encoder object at 0x00000000307476A0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <function standard_gru at 0x00000000143B1400> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x00000000143B1400>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <function cudnn_gru at 0x00000000143B16A8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x00000000143B16A8>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x00000000307AE160>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <__main__.Decoder object at 0x00000000307AE160>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method Attention.call of <__main__.Attention object at 0x00000000307AA358>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Attention.call of <__main__.Attention object at 0x00000000307AA358>>: AssertionError: Bad argument number for Name: 3, expecting 4
"
33158,Build fails to create GPU target ( nvptx64-nvidia-cuda ) for CUDA10.0/CUDNN7.0/VT100,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Debian GNU/Linux 9 \n \l
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.x
- Python version:  2.7
- Installed using virtualenv? pip? conda?:  pip
- Bazel version (if compiling from source): 26.1
- GCC/Compiler version (if compiling from source): g++ (Debian 6.3.0-18+deb9u1) 6.3.0 20170516
- CUDA/cuDNN version: 10.0/7.0
- GPU model and memory: Tesla V100-SXM2, 16130MiB 



**Describe the problem**

Build Fails

**Provide the exact sequence of commands / steps that you executed before running into the problem**

bazel build //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

ERROR: /home/george/tensorflow/tensorflow/core/kernels/BUILD:4675:1: C++ compilation of rule '//tensorflow/core/kernels:fake_quant_ops_gpu' failed (Exit 1)
error: unable to create target: 'No available targets are compatible with triple ""nvptx64-nvidia-cuda""'
1 error generated when compiling for sm_70.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 4036.064s, Critical Path: 83.02s
INFO: 4647 processes: 4647 local.
FAILED: Build did NOT complete successfully
"
33156,issue with reuse embedding module,"I was using the universal sentence encoder transformer module for some developments and had no issue before. But when I try to reuse the module today, I got an error, wondering if anyone had similar experience and solution with this?

script: embed = hub.Module(""https://tfhub.dev/google/universal-sentence-encoder-large/3"")

error: variable_scope module_20/ was unused but the corresponding name_scope was already taken.

Thanks.

"
33154, Attempt to convert a value (1.0) with an unsupported type (<class 'numpy.float32'>) to a Tensor.,"Im getting this error in this step. Im trying to forecast sales similar to the lstm stock price prediction. I ensured that all the inputs i.e x_t_tf, y_t_tf, a and b are all tensors. Yet, I get this error. I am using tensorflow 2.0.0.

history = model.fit(x_t_tf, y_t_tf, epochs=300, verbose=2, steps_per_epoch=BATCH_SIZE,validation_steps = 3,
                        shuffle=False, validation_data=(a,
                        b), callbacks=[es, mcp])

Please let me know how to resolve this. Im just using a jupyter notebook to run this. "
33153,"tflite_runtime standalone always results in ""ModuleNotFoundError: No module named '_interpreter_wrapper'""","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): openSUSE Leap 15.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): 
- TensorFlow version: tflite_runtime 1.14.0
- Python version: 3.6.5
- Installed using virtualenv? pip? conda?: pip3 (followed the instructions from https://www.tensorflow.org/lite/guide/python)
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: NVIDIA 2080ti 11GB



**Describe the problem**
When trying to install just the TensorFlow Lite interpreter, I'm able to install everything with no errors and I can even import tflite_runtime, but when I put in ""from tflite_runtime.interpreter import Interpreter"", I always run into a ModuleNotFoundError
**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

I also installed Miniconda, installed python 3.7, pip3 installed the correct tflite_runtime for python 3.7, and ran into the exact same ModuleNotFoundError.
"
33152,TPU has XLA compilation issue on TF 1.15rc3,"**System information**
python: `3.7.3`
tensorflow version: `1.15.0rc3`
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/3704020/tf_env.txt)

**Describe the current behavior**

Error message I get is the following:

```
WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From issue.py:19: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.

WARNING:tensorflow:From issue.py:30: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2019-10-08 11:14:04.056451: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2019-10-08 11:14:04.056480: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)
2019-10-08 11:14:04.056497: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cs-6000-devshell-vm-8cf51c20-59fd-4dc8-ad36-465b49377d09): /proc/driver/nvidia/version does not exist
2019-10-08 11:14:04.056770: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-10-08 11:14:04.069196: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz
2019-10-08 11:14:04.070092: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560554da1f40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2019-10-08 11:14:04.070105: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
WARNING:tensorflow:From issue.py:31: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

2019-10-08 11:14:04.154903: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at xla_ops.cc:361 : Invalid argument: Detected unsupported operations when trying to compile graph cluster_3267298482081017018[] on XLA_CPU_JIT: Unique (No registered 'Unique' OpKernel for XLA_CPU_JIT devices compatible with node {{node sgd_momentum/update_embedding/embeddings/Unique}}
	.  Registered:  device='CPU'; T in [DT_BOOL]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_BOOL]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_STRING]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_STRING]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_DOUBLE]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_FLOAT]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_BFLOAT16]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_HALF]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_HALF]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_INT8]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_INT8]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_UINT8]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_UINT8]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_INT16]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_INT16]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_UINT16]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_UINT16]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_INT32]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_INT32]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_INT64]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_INT64]; out_idx in [DT_INT32]
  device='GPU'; T in [DT_INT64]; out_idx in [DT_INT64]
  device='GPU'; T in [DT_INT64]; out_idx in [DT_INT32]
  device='GPU'; T in [DT_INT32]; out_idx in [DT_INT64]
  device='GPU'; T in [DT_INT32]; out_idx in [DT_INT32]
){{node sgd_momentum/update_embedding/embeddings/Unique}}
	This error might be occurring with the use of xla.compile. If it is not necessary that every Op be compiled with XLA, an alternative is to use auto_jit with OptimizerOptions.global_jit_level = ON_2 or the environment variable TF_XLA_FLAGS=""tf_xla_auto_jit=2"" which will attempt to use xla to compile as much of the graph as the compiler is able to.
Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1365, in _do_call
    return fn(*args)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1350, in _run_fn
    target_list, run_metadata)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1443, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Detected unsupported operations when trying to compile graph cluster_3267298482081017018[] on XLA_CPU_JIT: Unique (No registered 'Unique' OpKernel for XLA_CPU_JIT devices compatible with node {{node sgd_momentum/update_embedding/embeddings/Unique}}
	.  Registered:  device='CPU'; T in [DT_BOOL]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_BOOL]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_STRING]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_STRING]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_DOUBLE]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_FLOAT]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_BFLOAT16]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_HALF]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_HALF]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_INT8]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_INT8]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_UINT8]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_UINT8]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_INT16]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_INT16]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_UINT16]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_UINT16]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_INT32]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_INT32]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_INT64]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_INT64]; out_idx in [DT_INT32]
  device='GPU'; T in [DT_INT64]; out_idx in [DT_INT64]
  device='GPU'; T in [DT_INT64]; out_idx in [DT_INT32]
  device='GPU'; T in [DT_INT32]; out_idx in [DT_INT64]
  device='GPU'; T in [DT_INT32]; out_idx in [DT_INT32]
){{node sgd_momentum/update_embedding/embeddings/Unique}}
	This error might be occurring with the use of xla.compile. If it is not necessary that every Op be compiled with XLA, an alternative is to use auto_jit with OptimizerOptions.global_jit_level = ON_2 or the environment variable TF_XLA_FLAGS=""tf_xla_auto_jit=2"" which will attempt to use xla to compile as much of the graph as the compiler is able to.
	 [[cluster]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""issue.py"", line 32, in <module>
    sess.run(loss)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 956, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1359, in _do_run
    run_metadata)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1384, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Detected unsupported operations when trying to compile graph cluster_3267298482081017018[] on XLA_CPU_JIT: Unique (No registered 'Unique' OpKernel for XLA_CPU_JIT devices compatible with node node sgd_momentum/update_embedding/embeddings/Unique (defined at /usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) 
	.  Registered:  device='CPU'; T in [DT_BOOL]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_BOOL]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_STRING]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_STRING]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_DOUBLE]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_FLOAT]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_BFLOAT16]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_HALF]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_HALF]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_INT8]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_INT8]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_UINT8]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_UINT8]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_INT16]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_INT16]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_UINT16]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_UINT16]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_INT32]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_INT32]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_INT64]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_INT64]; out_idx in [DT_INT32]
  device='GPU'; T in [DT_INT64]; out_idx in [DT_INT64]
  device='GPU'; T in [DT_INT64]; out_idx in [DT_INT32]
  device='GPU'; T in [DT_INT32]; out_idx in [DT_INT64]
  device='GPU'; T in [DT_INT32]; out_idx in [DT_INT32]
)node sgd_momentum/update_embedding/embeddings/Unique (defined at /usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) 
	This error might be occurring with the use of xla.compile. If it is not necessary that every Op be compiled with XLA, an alternative is to use auto_jit with OptimizerOptions.global_jit_level = ON_2 or the environment variable TF_XLA_FLAGS=""tf_xla_auto_jit=2"" which will attempt to use xla to compile as much of the graph as the compiler is able to.
	 [[cluster]]
```

**Describe the expected behavior**
If I used tf.train.GradientDescentOptimizer instead it would work, I expect using momentum optimizer to work as well.

**Code to reproduce the issue**
```
import os
import tensorflow as tf
from tensorflow.python.compiler.xla import xla
from tensorflow.keras.layers import Embedding

LEARNING_RATE = 0.1
VOCAB_SIZE = 100
HIDDEN_SIZE = 200
MAX_SEQ_LEN = 150

def my_model(features, labels):
    emb = Embedding(VOCAB_SIZE, HIDDEN_SIZE)(features)
    logits = tf.keras.layers.Dense(units=VOCAB_SIZE)(emb)
    crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(
        labels=labels,
        logits=logits
    )
    loss = tf.reduce_mean(crossent)
    optimizer = tf.train.MomentumOptimizer(
        learning_rate=LEARNING_RATE,
        momentum=0.9,
        name='sgd_momentum')
    train_op = optimizer.minimize(loss)
    return loss

src = tf.constant(0, dtype=tf.int32, shape=[MAX_SEQ_LEN])
tgt = tf.constant(1, dtype=tf.int32, shape=[MAX_SEQ_LEN])
(loss,) = xla.compile(my_model, [src, tgt])

sess = tf.Session()
sess.run(tf.global_variables_initializer())
sess.run(loss)
sess.close()
```

**Other info / logs**

Issue occurs when using keras `Embedding` layer alongside an optimizer that is not sgd.
The issue can be fixed by modifying keras implementation by changing:
```
tf.gather(weights, inputs)
```
to
```
tf.gather(tf.multiply(weights,1), inputs)
```

The issue is in the weights being an IndexedSlice which causes any non-sgd optimizer to use the `unique` operation which is unsupported by XLA. However, if you do `tf.multiply(weights,1)` it makes that expression the IndexedSlice and weights itself doesn't become an IndexedSlice but rather a tensor so the optimizer implementation in tensorflow doesn't call `unique`.
"
33151,Inference On-Cloud for Object Detection,"Hello, 
I built an android app for object detection using tensorflow lite and on-Device inference. I want to make inference on-Cloud and saw that the tensorflow serving client is still unavaliable for android. Do you know a way to use cloud for inference at my object detection mobile app? Thanks an advance.
"
33150,TF2.0: Translation model: Error when restoring the saved model: Unresolved object in checkpoint (root).optimizer.iter: attributes,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOs Mojave 10.14.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): 
- TensorFlow version (use command below):' 2.0.0-beta1'
- Python version: 3.7
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

I am trying to restore the checkpoints and predict on different sentences NMT Attention Model. While restoring the checkpoints and predicting, I am getting gibberish results with warning below:

`Unresolved object in checkpoint (root).optimizer.iter: attributes {
  name: ""VARIABLE_VALUE""
  full_name: ""Adam/iter""
  checkpoint_key: ""optimizer/iter/.ATTRIBUTES/VARIABLE_VALUE""
}`

Below is the additional warnings that I am getting and the results:

```
WARNING: Logging before flag parsing goes to stderr.
W1008 09:57:52.766877 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer.iter
W1008 09:57:52.767037 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer.beta_1
W1008 09:57:52.767082 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer.beta_2
W1008 09:57:52.767120 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer.decay
W1008 09:57:52.767155 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer.learning_rate
W1008 09:57:52.767194 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.embedding.embeddings
W1008 09:57:52.767228 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.gru.state_spec
W1008 09:57:52.767262 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.fc.kernel
W1008 09:57:52.767296 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.fc.bias
W1008 09:57:52.767329 4594230720 util.py:244] Unresolved object in checkpoint: (root).encoder.embedding.embeddings
W1008 09:57:52.767364 4594230720 util.py:244] Unresolved object in checkpoint: (root).encoder.gru.state_spec
W1008 09:57:52.767396 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.gru.cell.kernel
W1008 09:57:52.767429 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.gru.cell.recurrent_kernel
W1008 09:57:52.767461 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.gru.cell.bias
W1008 09:57:52.767493 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.attention.W1.kernel
W1008 09:57:52.767526 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.attention.W1.bias
W1008 09:57:52.767558 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.attention.W2.kernel
W1008 09:57:52.767590 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.attention.W2.bias
W1008 09:57:52.767623 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.attention.V.kernel
W1008 09:57:52.767657 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.attention.V.bias
W1008 09:57:52.767688 4594230720 util.py:244] Unresolved object in checkpoint: (root).encoder.gru.cell.kernel
W1008 09:57:52.767721 4594230720 util.py:244] Unresolved object in checkpoint: (root).encoder.gru.cell.recurrent_kernel
W1008 09:57:52.767755 4594230720 util.py:244] Unresolved object in checkpoint: (root).encoder.gru.cell.bias
W1008 09:57:52.767786 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.embedding.embeddings
W1008 09:57:52.767818 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.fc.kernel
W1008 09:57:52.767851 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.fc.bias
W1008 09:57:52.767884 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).encoder.embedding.embeddings
W1008 09:57:52.767915 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.gru.cell.kernel
W1008 09:57:52.767949 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.gru.cell.recurrent_kernel
W1008 09:57:52.767981 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.gru.cell.bias
W1008 09:57:52.768013 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.attention.W1.kernel
W1008 09:57:52.768044 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.attention.W1.bias
W1008 09:57:52.768077 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.attention.W2.kernel
W1008 09:57:52.768109 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.attention.W2.bias
W1008 09:57:52.768143 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.attention.V.kernel
W1008 09:57:52.768175 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.attention.V.bias
W1008 09:57:52.768207 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).encoder.gru.cell.kernel
W1008 09:57:52.768239 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).encoder.gru.cell.recurrent_kernel
W1008 09:57:52.768271 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).encoder.gru.cell.bias
W1008 09:57:52.768303 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.embedding.embeddings
W1008 09:57:52.768335 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.fc.kernel
W1008 09:57:52.768367 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.fc.bias
W1008 09:57:52.768399 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).encoder.embedding.embeddings
W1008 09:57:52.768431 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.gru.cell.kernel
W1008 09:57:52.768463 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.gru.cell.recurrent_kernel
W1008 09:57:52.768495 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.gru.cell.bias
W1008 09:57:52.768527 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.attention.W1.kernel
W1008 09:57:52.768559 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.attention.W1.bias
W1008 09:57:52.768591 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.attention.W2.kernel
W1008 09:57:52.768623 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.attention.W2.bias
W1008 09:57:52.768654 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.attention.V.kernel
W1008 09:57:52.768686 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.attention.V.bias
W1008 09:57:52.768718 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).encoder.gru.cell.kernel
W1008 09:57:52.768750 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).encoder.gru.cell.recurrent_kernel
W1008 09:57:52.768782 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).encoder.gru.cell.bias
W1008 09:57:52.768816 4594230720 util.py:252] A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.
Input: <start> hola <end>
Predicted translation: ? attack now relax hello 
```
The warning at the very end says:

'A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used...' what does it mean?
**Describe the expected behavior**
Checkpoints should be restored properly
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
checkpoint_dir = './training_checkpoints'
checkpoint_prefix = os.path.join(checkpoint_dir, ""ckpt"")
checkpoint = tf.train.Checkpoint(optimizer=optimizer,
                                 encoder=encoder,
                                 decoder=decoder)
checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
33149,"AutoGraph error with XLA, MKL-DNN, Eager Execution, and custom keras layer","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.6.1810 (Core)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary, using conda
- TensorFlow version (use command below): unknown 1.14.0, with intel MKL
- Python version: 3.7.4
- CUDA/cuDNN version: no gpu
- GPU model and memory: no gpu

python build version: ('default', 'Aug 13 2019 20:35:49')
python compiler version: GCC 7.3.0
python implementation: CPython
os kernel version: #1 SMP Mon Jul 29 17:46:05 UTC 2019
os release version: 3.10.0-957.27.2.el7.x86_64
os platform: Linux-3.10.0-957.27.2.el7.x86_64-x86_64-with-centos-7.6.1810-Core
linux distribution: ('CentOS Linux', '7.6.1810', 'Core')
linux os distribution: ('centos', '7.6.1810', 'Core')
architecture: ('64bit', '')
machine: x86_64
== compiler =====================================================
c++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36)
== check pips ===================================================
numpy                1.16.4   
protobuf             3.9.2    
tensorflow           1.14.0   
tensorflow-estimator 1.14.0   
gast                    0.3.2

**Describe the current behavior**
AutoGraph not working properly with custom Keras Layer and eager execution.
```
2019-10-08 17:22:48.398589: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2019-10-08 17:22:48.404575: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2399845000 Hz
2019-10-08 17:22:48.404752: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56240ed9a1e0 executing computations on platform Host. Devices:
2019-10-08 17:22:48.404799: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-10-08 17:22:48.404944: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
ERROR:tensorflow:Error converting <bound method SegmentedMean.call of <ml_utils.SegmentedMean object at 0x7f9d9e8e6610>>
Traceback (most recent call last):
  File ""/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py"", line 524, in to_graph
    return conversion.convert(entity, program_ctx)
  File ""/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 306, in convert
    entity, program_ctx, free_nonglobal_var_names)
  File ""/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 229, in _convert_with_cache
    entity, program_ctx)
  File ""/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 433, in convert_entity_to_ast
    nodes, name, entity_info = convert_func_to_ast(o, program_ctx)
  File ""/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 624, in convert_func_to_ast
    node = node_to_graph(node, context)
  File ""/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 667, in node_to_graph
    node = converter.apply_(node, context, return_statements)
  File ""/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/core/converter.py"", line 380, in apply_
    node = converter_module.transform(node, context)
  File ""/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/converters/return_statements.py"", line 412, in transform
    node = transformer.visit(node)
  File ""/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/core/converter.py"", line 317, in visit
    return super(Base, self).visit(node)
  File ""/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/transformer.py"", line 480, in visit
    result = super(Base, self).visit(node)
  File ""/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/ast.py"", line 262, in visit
    return visitor(node)
  File ""/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/converters/return_statements.py"", line 363, in visit_FunctionDef
    converted_body = self._visit_statement_block(node, node.body)
  File ""/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/converters/return_statements.py"", line 287, in _visit_statement_block
    nodes = self.visit_block(nodes, after_visit=self._postprocess_statement)
  File ""/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/transformer.py"", line 371, in visit_block
    replacement = self.visit(node)
  File ""/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/core/converter.py"", line 317, in visit
    return super(Base, self).visit(node)
  File ""/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/transformer.py"", line 480, in visit
    result = super(Base, self).visit(node)
  File ""/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/ast.py"", line 262, in visit
    return visitor(node)
  File ""/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/converters/return_statements.py"", line 237, in visit_Return
    retval=retval)
  File ""/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/templates.py"", line 260, in replace
    replacements[k] = _convert_to_ast(replacements[k])
  File ""/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/templates.py"", line 222, in _convert_to_ast
    return gast.Name(id=n, ctx=None, annotation=None)
  File ""/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/gast/gast.py"", line 19, in create_node
    format(Name, nbparam, len(Fields))
AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method SegmentedMean.call of <ml_utils.SegmentedMean object at 0x7f9d9e8e6610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SegmentedMean.call of <ml_utils.SegmentedMean object at 0x7f9d9e8e6610>>: AssertionError: Bad argument number for Name: 3, expecting 4
```

**Describe the expected behavior**
Error should not be raised, code should work.

**Code to reproduce the issue**
```
import pandas as pd
import tensorflow as tf
import numpy as np
from sklearn.preprocessing import MultiLabelBinarizer

class SegmentedMean(tf.keras.layers.Layer):
    def __init__(self, *args, **kwargs):
        super(SegmentedMean, self).__init__(*args, **kwargs)

    def call(self, inputs, **kwargs):
        features, segments = inputs
        return tf.math.segment_mean(features, segments)


df = pd.DataFrame({'list': [
    [[1, 2, 3, 4], [2, 3, 6]],
    [[1, 2], [1, 5, 8]],
    [[6, 7, 8], [2, 4, 10], [1, 6]],
    [[3, 4, 6, 8], [1, 8], [2]],
    [[3, 6, 8]],
],
    'label': [0, 0, 1, 1, 1]})
df['list_len'] = df['list'].apply(len)
batch_size = 8
# list of all unique numbers used
nums_used = list(set([i for j in df['list'].apply(lambda k: [i for j in k for i in j]) for i in j]))
list_enc = MultiLabelBinarizer().fit([[i] for i in nums_used])


def create_batch():
    batch_ub = df.sample(batch_size // 2)  # upper bound, will trim so the batch size is correct after expansion
    batch_end = (batch_ub['list_len'].cumsum() <= batch_size)[::-1].idxmax()
    batch = batch_ub.loc[:batch_end].copy()
    batch['id'] = np.arange(0, len(batch))
    feat_bags = batch['list'].apply(list_enc.transform)
    feats = np.concatenate(feat_bags.values)
    # so I know which data to group together during segmentation
    segments = batch['id'].repeat(batch['list_len']).values
    labels = batch['label'].values.astype(np.int32)
    return (feats, segments), labels


settings = {'k': 40, 'iterations': 10}
feats_len = len(nums_used)
inputs = tf.keras.Input(shape=(feats_len,), name='features')
segments = tf.keras.Input(shape=(), name='segments', dtype=tf.int32)
x = tf.keras.layers.Dense(settings['k'], activation=tf.nn.relu)(inputs)
x = tf.keras.layers.Dense(settings['k'])(x)

x = SegmentedMean()((x, segments))
x = tf.keras.layers.Dense(settings['k'], activation=tf.nn.relu)(x)
logits = tf.keras.layers.Dense(2, name='output_logits')(x)
probs = tf.keras.layers.Softmax()(logits)
model = tf.keras.Model(inputs=(inputs, segments), outputs=(logits, probs), name='mil_model')
optimizer = tf.keras.optimizers.Adam()
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
for step in range(settings['iterations']):
    x_train, y_train = create_batch()
    with tf.GradientTape() as tape:
        logits, probs = model(x_train)
        loss_value = loss(y_train, logits)
    grads = tape.gradient(loss_value, model.trainable_weights)
    optimizer.apply_gradients(zip(grads, model.trainable_weights))
```

**Other info / logs**
Logs are included above."
33148,Masking LSTM: OP_REQUIRES failed at cudnn_rnn_ops.cc:1498 : Unknown: CUDNN_STATUS_BAD_PARAM,"### System information
- **Have I written custom code**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04.2 LTS
- **TensorFlow installed from (source or binary)**: Binary, pip
- **TensorFlow version (use command below)**: v2.0.0-rc2-26-g64c3d38 2.0.0
- **Python version**: Python 3.7.3
- **CUDA/cuDNN version**: CUDA=10.0, CUDNN=7.6.2.24-1
- **GPU model and memory**: Quadro RTX 6000 major: 7 minor: 5 memoryClockRate(GHz): 1.77

### Describe the problem
It seems there is an issue with the CuDNN LSTM implementation when using a `tf.keras.layers.Masking` layer. 

```
batch_size = 256
num_tsteps = 144
num_features = 130
num_units = 88
model = tf.keras.Sequential([
    tf.keras.layers.InputLayer(input_shape=(num_tsteps, num_features), batch_size=batch_size),
    tf.keras.layers.Masking(mask_value=0.0, input_shape=(num_tsteps, num_features)),
    tf.keras.layers.LSTM(num_units,  batch_input_shape=(batch_size, num_tsteps, num_features), return_sequences=True, stateful=False),
    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1)),
    tf.keras.layers.Activation('sigmoid'),
])
```
Similar to #33069 I receive this error during training and I have strictly right-padded data (I am doing trimming and right-padding manually). However, in contrast to this issue, I confirmed that I do not have any inputs containing only zeroes via the following snippet:

```
for i, e in enumerate(ds_train):
    res = []
    f, l = [x.numpy() for x in e]
    for j in range(f.shape[0]):
        if not (f[j] == 0.0).all():
            res.append(1)
        else:
            res.append(0)
    fin = [res[0]]
    for e in res[1:]:
        if e != fin[-1]:
            fin.append(e)
    print(""i {}: {}"".format(i, fin))

# Result:
i 0: [1, 0]
i 1: [1, 0]
i 2: [1, 0]
i 3: [1, 0]
i 4: [1]
i 5: [1, 0]
...
```
If I remove the Masking-layer, the error does not occur. I confirmed this by running a complete epoch (2324 batches), however, the training is probably pretty pointless when including the padded data.

Is there any other pitfall that I am missing that could cause this issue?

### Source code / logs
Python output:
```
Epoch 1/1000
WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: 


CancelledErrorTraceback (most recent call last)
<ipython-input-7-1c503c2dd55c> in <module>
----> 1 m.fit(train=True)

/ws/tf/vol_local/_model_lstm.py in fit(self, train, verbose)
    315             ]
    316             self.model.fit(ds_train, epochs=num_epochs, verbose=verbose, shuffle=False,
--> 317                                 validation_data=ds_val, validation_steps=None, callbacks=cbs)
    318             #self.model.save(sess_hdf5_path)
    319             self.model.save_weights(self.sess_h5_path.as_posix())

/ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    726         max_queue_size=max_queue_size,
    727         workers=workers,
--> 728         use_multiprocessing=use_multiprocessing)
    729 
    730   def evaluate(self,

/ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
    322                 mode=ModeKeys.TRAIN,
    323                 training_context=training_context,
--> 324                 total_epochs=epochs)
    325             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)
    326 

/ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)
    121         step=step, mode=mode, size=current_batch_size) as batch_logs:
    122       try:
--> 123         batch_outs = execution_function(iterator)
    124       except (StopIteration, errors.OutOfRangeError):
    125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?

/ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)
     84     # `numpy` translates Tensors to values in Eager mode.
     85     return nest.map_structure(_non_none_constant_value,
---> 86                               distributed_function(input_fn))
     87 
     88   return execution_function

/ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)
    455 
    456     tracing_count = self._get_tracing_count()
--> 457     result = self._call(*args, **kwds)
    458     if tracing_count == self._get_tracing_count():
    459       self._call_counter.called_without_tracing()

/ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)
    518         # Lifting succeeded, so variables are initialized and we can run the
    519         # stateless function.
--> 520         return self._stateless_fn(*args, **kwds)
    521     else:
    522       canon_args, canon_kwds = \

/ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)
   1821     """"""Calls a graph function specialized to the inputs.""""""
   1822     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
-> 1823     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   1824 
   1825   @property

/ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _filtered_call(self, args, kwargs)
   1139          if isinstance(t, (ops.Tensor,
   1140                            resource_variable_ops.BaseResourceVariable))),
-> 1141         self.captured_inputs)
   1142 
   1143   def _call_flat(self, args, captured_inputs, cancellation_manager=None):

/ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1222     if executing_eagerly:
   1223       flat_outputs = forward_function.call(
-> 1224           ctx, args, cancellation_manager=cancellation_manager)
   1225     else:
   1226       gradient_name = self._delayed_rewrite_functions.register()

/ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in call(self, ctx, args, cancellation_manager)
    509               inputs=args,
    510               attrs=(""executor_type"", executor_type, ""config_proto"", config),
--> 511               ctx=ctx)
    512         else:
    513           outputs = execute.execute_with_cancellation(

/ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     65     else:
     66       message = e.message
---> 67     six.raise_from(core._status_to_exception(e.code, message), None)
     68   except TypeError as e:
     69     keras_symbolic_tensors = [

/ws/miniconda3/lib/python3.7/site-packages/six.py in raise_from(value, from_value)

CancelledError:  [_Derived_]RecvAsync is cancelled.
	 [[{{node metrics/accuracy/broadcast_weights/assert_broadcastable/AssertGuard/else/_36/Assert/data_2/_62}}]]
	 [[loss/activation_loss/weighted_loss/broadcast_weights/assert_broadcastable/is_valid_shape/else/_1/has_valid_nonscalar_shape/then/_106/has_invalid_dims/concat/_28]] [Op:__inference_distributed_function_172102]

Function call stack:
distributed_function
```

Command line log:
```
2019-10-08 14:38:27.367875: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_169668_171093' and '__inference___backward_cudnn_lstm_with_fallback_169668_171093_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_172102' both implement 'lstm_dce676f4-acdd-4bb5-88d9-e8dd57573aba' but their signatures do not match.
2019-10-08 14:38:27.536666: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-10-08 14:38:39.982582: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-10-08 14:38:41.215567: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at cudnn_rnn_ops.cc:1498 : Unknown: CUDNN_STATUS_BAD_PARAM
in tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)'
2019-10-08 14:38:41.215616: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Unknown: CUDNN_STATUS_BAD_PARAM
in tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)'
	 [[{{node cond_64/then/_0/CudnnRNNV3}}]]
2019-10-08 14:38:41.215638: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Cancelled: [_Derived_]RecvAsync is cancelled.
	 [[{{node metrics/accuracy/broadcast_weights/assert_broadcastable/AssertGuard/else/_36/Assert/data_2/_62}}]]
	 [[loss/activation_loss/weighted_loss/broadcast_weights/assert_broadcastable/is_valid_shape/else/_1/has_valid_nonscalar_shape/then/_106/has_invalid_dims/concat/_28]]
2019-10-08 14:38:41.215693: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Cancelled: [_Derived_]RecvAsync is cancelled.
	 [[{{node metrics/accuracy/broadcast_weights/assert_broadcastable/AssertGuard/else/_36/Assert/data_2/_62}}]]
```"
33147,Regularisation losses in nested layers,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Manjaro testing, x86_64
- TensorFlow installed from (source or binary): pypi binary
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0
- Python version: 3.7.4

**Describe the current behavior**
TensorFlow returns duplicated regularisation losses for layers which hold references to regularised variables built in other layers.

**Describe the expected behavior**
Return a single regularisation loss per variable.

**Code to reproduce the issue**
```
import tensorflow as tf


class A(tf.keras.layers.Layer):
    def __init__(self, layer):
        super(A, self).__init__()
        self.layer = layer

    def call(self, inputs):
        return self.layer(inputs)


class B(tf.keras.layers.Layer):
    def __init__(self):
        super(B, self).__init__()
        self.obj = tf.keras.layers.Dense(13, kernel_regularizer=tf.keras.regularizers.l1(5))
        self.layerB = A(self.obj)

    def call(self, inputs):
        return self.layerB(inputs)

model = B()

output = model(tf.ones([5, 10]))
print(len(model.losses))
```
Once we rename self.obj to obj, i.e. not saving it as a class member, we end up with a single regularisation loss.

**Other info / logs**
More info and logs in https://github.com/tensorflow/addons/issues/577
Plus @guillaumekln 's response https://github.com/tensorflow/addons/issues/577#issuecomment-539530375"
33146,tensorflow (cpu only) with mkl is much slower than without mkl! ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Red Hat 4.8.5-4
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.8
- Python version: 2.7
- Bazel version (if compiling from source): 0.10.0
- GCC/Compiler version (if compiling from source): 4.8.5
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I compile tensorflow 1.8 with mkl, and I use c api session to predict wide and deep model.
The Parallel Performance is very slow, avg reponse time is 40 microsecond in 60 qps, cpu load is only 200%

export KMP_BLOCKTIME=0
export KMP_AFFINITY=granularity=fine,verbose,compact,1,0
export OMP_NUM_THREADS=64
export KMP_SETTINGS=1
export MKLDNN_VERBOSE=1

in the opposite， without mkl, The Parallel Performance is ok, avg reponse time is 9 microsecond in 60 qps. I don't know why.Maybe I use wrong!

**Describe the expected behavior**

**Code to reproduce the issue**

    SessionOptions session_options;
    ConfigProto& cp = session_options.config;
    cp.set_intra_op_parallelism_threads(64);

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
33145,tf.keras.layers.LSTM and LSTMCell have different output formats,"**System information**
- TensorFlow version (you are using): 2.0
- Are you willing to contribute it (Yes/No): Yes


**Describe the feature and the current behavior/state.**
The `tf.keras.layers.LSTM` and `tf.keras.layers.LSTMCell` classes currently have different output formats when using the `return_state=True` flag in `LSTM`. 

`LSTMCell` returns `[output, [h, c]]`
`LSTM` returns `[output, h, c]`

This makes it complicated to design a layer wrapper for both types of cells, since the wrapper's call method will have to take into account the cell type.
Are you okay with making `LSTM` adhere to the `LSTMCell` return format ?

**Will this change the current api? How?**
Yes. In `keras.layers.recurrent_v2.py` (Line 983):
```
    if self.return_state:
      #  return [output] + list(states)  # the current format
      return [output] + [states]  # the `LSTMCell` format
```

**Who will benefit with this feature?**
Me
**Any Other info.**
This possibly applies to other cells like GRU."
33144,Tensorflow Lite  fully integer quantization error :Got tensor of type STRING but expected type FLOAT32,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04
- TensorFlow installed from (source or binary):binary(pip)
- TensorFlow version (or github SHA if from source):1.14.0


**Provide the text output from tflite_convert**

```
2019-10-08 20:47:37.470691: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-10-08 20:47:37.491949: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2194920000 Hz
2019-10-08 20:47:37.492291: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5600095b3a90 executing computations on platform Host. Devices:
2019-10-08 20:47:37.492321: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-10-08 20:47:39.487818: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
2019-10-08 20:47:39.487989: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session
2019-10-08 20:47:41.530815: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize
2019-10-08 20:47:41.576395: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 941 nodes (-544), 973 edges (-544), time = 1222.25598ms.
2019-10-08 20:47:41.576429: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 941 nodes (0), 973 edges (0), time = 214.279ms.
INFO: Initialized TensorFlow Lite runtime.
Traceback (most recent call last):
  File ""/home/eagle/PycharmProjects/TFLite/main.py"", line 30, in <module>
    quantized_model = converter.convert()
  File ""/home/eagle/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 908, in convert
    inference_output_type)
  File ""/home/eagle/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 200, in _calibrate_quantize_model
    inference_output_type, allow_float)
  File ""/home/eagle/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/optimize/calibrator.py"", line 75, in calibrate_and_quantize
    self._calibrator.FeedTensor(calibration_sample)
  File ""/home/eagle/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py"", line 112, in FeedTensor
    return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_FeedTensor(self, input_value)
ValueError: Cannot set tensor: Got tensor of type STRING but expected type FLOAT32 for input 0, name: input 
```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Source Code:

import tensorflow as tf
import os

os.environ[""CUDA_VISIBLE_DEVICES""]=""-2""

saved_model_dir = ""./tmp/resnet/resnet_v2_101_299_frozen.pb""

if __name__ == ""__main__"":
    input_arrays = [""input""]
    output_arrays = [""output""]

    converter = tf.lite.TFLiteConverter.from_frozen_graph(
        str(saved_model_dir), input_arrays, output_arrays, input_shapes={""input"": [1, 299, 299, 3]})

    (resnet_train, _), (_, _) = tf.keras.datasets.cifar10.load_data()
    images = tf.cast(resnet_train[0], tf.float32)/255.0
    resnet_ds = tf.data.Dataset.from_tensor_slices(images).batch(1)

    def representative_data_gen():
        for input_value in resnet_ds.take(100):
            yield [input_value]


    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.representative_dataset = representative_data_gen
    converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
    converter.inference_input_type = tf.qint8
    converter.inference_output_type = tf.qint8
    quantized_model = converter.convert()
    open(""./tmp/resnet/resnet_v2_101_299_frozen_weight_quantized.tflite"", ""wb"").write(quantized_model)
"
33143,Tensorflow Keras not allowing different size X and y for fit_generator,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, Professional Edition
- TensorFlow installed from (source or binary): binary, installed using conda
- TensorFlow version (use command below): unknown, 1.14.0
- Python version: 3.7.3
- CUDA/cuDNN version: 10.0, 7.6
- GPU model and memory: T1000, 4GB VRAM

**Describe the current behavior**
When having model with two inputs, with shapes e.g. (486, 3673), (486, ) and one output (87, 1), after calling `model.fit_generator()`, the training crashes:
```
Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\contextlib.py"", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\summary_ops_v2.py"", line 236, in as_default
    yield self
  File ""C:/Projects/iotmap/py/train.py"", line 162, in train_fit_generator
    model.fit_generator(generator, epochs=epochs, workers=4, callbacks=[MyCallback()])
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1433, in fit_generator
    steps_name='steps_per_epoch')
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training_generator.py"", line 264, in model_iteration
    batch_outs = batch_function(*batch_data)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1153, in train_on_batch
    extract_tensors_from_dataset=True)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 2688, in _standardize_user_data
    training_utils.check_array_lengths(x, y, sample_weights)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training_utils.py"", line 483, in check_array_lengths
    'and ' + str(list(set_y)[0]) + ' target samples.')
ValueError: Input arrays should have the same number of samples as target arrays. Found 486 input samples and 87 target samples.
2019-10-08 13:32:22.899390: W tensorflow/python/util/util.cc:280] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
```

**Describe the expected behavior**
Training should proceed without problems, the data is valid. 
When trained during eager execution, using 
```
for step in range(settings['iterations']):
    x_train, y_train = batch_fn()
    with tf.GradientTape() as tape:
        logits, probs = model(x_train)
        loss_value = loss(y_train, logits)
    grads = tape.gradient(loss_value, model.trainable_weights)
    optimizer.apply_gradients(zip(grads, model.trainable_weights))
```
the model is trained without problems, and I expect using  `model.fit_generator()` would allow this behaviour too.

**Code to reproduce the issue**
model is built using
```
import tensorflow as tf

class SegmentedMean(tf.keras.layers.Layer):
    def __init__(self, *args, **kwargs):
        super(SegmentedMean, self).__init__(*args, **kwargs)

    def call(self, inputs, **kwargs):
        features, segments = inputs
        return tf.math.segment_mean(features, segments)

inputs = tf.keras.Input(shape=(feats_len,), name='features')
segments = tf.keras.Input(shape=(), name='segments', dtype=tf.int32)
x = tf.keras.layers.Dense(settings['k'], activation=tf.nn.relu)(inputs)
x = tf.keras.layers.Dense(settings['k'])(x)

x = SegmentedMean()((x, segments))
x = tf.keras.layers.Dense(settings['k'], activation=tf.nn.relu)(x)
logits = tf.keras.layers.Dense(2, name='output_logits')(x)
probs = tf.keras.layers.Softmax()(logits)
model = tf.keras.Model(inputs=(inputs, segments), outputs=(logits, probs), name='mil_model')
optimizer = tf.keras.optimizers.Adam()
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer=optimizer, loss={'output_logits': loss})
model.fit_generator(generator, epochs=5, workers=4)
```

**Other info / logs**
When using segmented aggregations, naturally the size of X and Y is different but after aggregation, dimensions fit, but keras model training don't allow this.
Using those aggregations is motivated by https://arxiv.org/pdf/1609.07257.pdf paper I am trying to use to some problem.
Keras repo allows turning those checks of in [this PR](https://github.com/keras-team/keras/pull/11548), but it is not incorporated into tensorflow version of keras."
33142,TFLiteLSTMCell and TfLiteRNNCell has large error with high hidden unit,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Pixel 3
- TensorFlow version (use command below): 1.14
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Bidirectional RNN with LSTM and RNN perform significantly poorer on Android compared to running on Linux with high `n_hidden` (e.g. >300)
In our testings, we see that the amount of parameter has a positive relationship with error, we suspect it is some sort of rounding error in the Lite implementation.

Also, please fix the inconsistent capitalization of `TFLiteLSTMCell` and `TfliteRNNCell`."
33140,Cannot access RandomFourierFeatures keras layer in TensorFlow 2.0,"### Problem Summary

Hello,

I'm not entirely sure if this is a bug or if I have made an error in installation but I have not been able to access the `tf.keras.layers.kernelized.RandomFourierFeatures` module within the tensorflow 2.0 framework.


### Problem Description

I've seen the `kernelized` and `RandomFourierFeatures` layer within the code-base within the [init file](https://github.com/tensorflow/tensorflow/blob/e62dc433fcce833313b4174e20fc24c418593d27/tensorflow/python/keras/layers/__init__.py), the [actual layer](https://github.com/tensorflow/tensorflow/blob/ddde6c74155df8da7229854d5387f3df64ead88a/tensorflow/python/keras/layers/kernelized.py) and the [test file](https://github.com/tensorflow/tensorflow/blob/b4245c36bc9bcb752c0a2118728098b359fd97e2/tensorflow/python/keras/layers/kernelized_test.py) under the [keras layers](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/keras/layers) directory. However, I cannot call the layer from anywhere within the tensorflow layers frameworks.

**Current Behaviour for importing `Kernelized` layer**

<details>

```python
tf.keras.layers.kernelized
```

```python
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-2-122e54d3d99d> in <module>()
----> 1 tf.keras.layers.kernelized

AttributeError: module 'tensorflow_core.keras.layers' has no attribute 'kernelized'
```
</details>

**Current Behaviour for importing `RandomFourierFeatures` layer**

<details>

```python
tf.keras.layers.RandomFourierFeatures
```

```python
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-3-058ec5bf6f57> in <module>()
----> 1 tf.keras.layers.RandomFourierFeatures

AttributeError: module 'tensorflow_core.keras.layers' has no attribute 'RandomFourierFeatures'
```

</details>

---
### System Information

I am currently using a Google Colab Notebook. [Link to Colab Notebook](https://colab.research.google.com/drive/1lMnpHXnP5BTAWy8vyKtEYgmLbq2Dlw3G).


**TensorFlow Version**

<details>

```python
import tensorflow as tf
tf.__version__
!python --version
```

```python
'2.1.0-dev20191008'
Python 3.6.8
```
</details>

**Colab CPU Info**

<details>

```python
processor	: 0
vendor_id	: GenuineIntel
cpu family	: 6
model		: 63
model name	: Intel(R) Xeon(R) CPU @ 2.30GHz
stepping	: 0
microcode	: 0x1
cpu MHz		: 2300.000
cache size	: 46080 KB
physical id	: 0
siblings	: 2
core id		: 0
cpu cores	: 1
apicid		: 0
initial apicid	: 0
fpu		: yes
fpu_exception	: yes
cpuid level	: 13
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt arat md_clear arch_capabilities
bugs		: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs
bogomips	: 4600.00
clflush size	: 64
cache_alignment	: 64
address sizes	: 46 bits physical, 48 bits virtual
power management:

processor	: 1
vendor_id	: GenuineIntel
cpu family	: 6
model		: 63
model name	: Intel(R) Xeon(R) CPU @ 2.30GHz
stepping	: 0
microcode	: 0x1
cpu MHz		: 2300.000
cache size	: 46080 KB
physical id	: 0
siblings	: 2
core id		: 0
cpu cores	: 1
apicid		: 1
initial apicid	: 1
fpu		: yes
fpu_exception	: yes
cpuid level	: 13
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt arat md_clear arch_capabilities
bugs		: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs
bogomips	: 4600.00
clflush size	: 64
cache_alignment	: 64
address sizes	: 46 bits physical, 48 bits virtual
power management:
```

</details>

**Memory Info**

<details>

```python
MemTotal:       13341992 kB
MemFree:         8670124 kB
MemAvailable:   12753184 kB
Buffers:          115848 kB
Cached:          3865368 kB
SwapCached:            0 kB
Active:          1160124 kB
Inactive:        3077576 kB
Active(anon):     256804 kB
Inactive(anon):      324 kB
Active(file):     903320 kB
Inactive(file):  3077252 kB
Unevictable:           0 kB
Mlocked:               0 kB
SwapTotal:             0 kB
SwapFree:              0 kB
Dirty:               772 kB
Writeback:             0 kB
AnonPages:        256548 kB
Mapped:           167332 kB
Shmem:               656 kB
Slab:             377108 kB
SReclaimable:     355904 kB
SUnreclaim:        21204 kB
KernelStack:        3040 kB
PageTables:         4044 kB
NFS_Unstable:          0 kB
Bounce:                0 kB
WritebackTmp:          0 kB
CommitLimit:     6670996 kB
Committed_AS:    1194088 kB
VmallocTotal:   34359738367 kB
VmallocUsed:           0 kB
VmallocChunk:          0 kB
AnonHugePages:         0 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
DirectMap4k:       59380 kB
DirectMap2M:     4134912 kB
DirectMap1G:    11534336 kB
```
</details>



Thank you.

"
33139,Memory leak when training simple LSTM Network,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No custom code written

> == check python ===================================================
> python version: 3.7.4
> python build version: ('default', 'Aug 13 2019 20:35:49')
> python compiler version: GCC 7.3.0
> python implementation: CPython
> == check os platform ===============================================
> os: Linux
> os kernel version: #31 18.04.1-Ubuntu SMP Thu Sep 12 18:29:21 UTC 2019
> os release version: 5.0.0-29-generic
> os platform: Linux-5.0.0-29-generic-x86_64-with-debian-buster-sid
> linux distribution: ('debian', 'buster/sid', '')
> linux os distribution: ('debian', 'buster/sid', '')
> architecture: ('64bit', '')
> machine: x86_64
> == are we in docker =============================================
> No
> == compiler =====================================================
> c++ (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
> Copyright (C) 2017 Free Software Foundation, Inc.
> This is free software; see the source for copying conditions.  There is NO
> warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
> == check pips ===================================================
> numpy                  1.16.4   
> protobuf               3.8.0    
> tensorflow             1.14.0   
> tensorflow-estimator   1.14.0   
> == check for virtualenv =========================================
> False
> == tensorflow import ============================================
> tf.version.VERSION = 1.14.0
> tf.version.GIT_VERSION = unknown
> tf.version.COMPILER_VERSION = 5.4.0

**Describe the current behavior**
When I run the code below the memory usage increases each epoch until my system is unresponsive.



**Code to reproduce the issue**

```
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Dropout
import numpy as np

num_features = 205
time_lenth = 12
num_of_instances = 5000
trip_sets = np.random.rand(num_of_instances, time_lenth, num_features)
print('num features: ', num_features)
data_len = len(trip_sets)
test_split = np.arange(data_len)
np.random.shuffle(test_split)
new_dataset = np.array(trip_sets)
targets = np.random.rand(num_of_instances, time_lenth, 1)

test_data = new_dataset[test_split[:int(data_len*0.2)]]
y_test_data = targets[test_split[:int(data_len*0.2)]]
train_data = new_dataset[test_split[int(data_len*0.2):]]
y_train_data = targets[test_split[int(data_len*0.2):]]

model = Sequential()
model.add(LSTM(75, return_sequences=True, input_shape=(None, num_features)))
model.add(Dropout(0.3))
model.add(LSTM(75, return_sequences=True))
model.add(Dropout(0.3))
model.add(TimeDistributed(Dense(1)))
# Memory leak also occurs if i use a model.add(Dense(1)) below instead of Time Distributed
# model.add(TimeDistributed(Dense(1)))

adam = tf.keras.optimizers.Adam(lr=0.001)
model.compile(loss='mse', optimizer=adam)
history = model.fit(x=train_data, y=y_train_data, epochs=100, validation_data=(test_data,y_test_data))
```"
33138,keras.LSTM in Tensorflow built with mkl is 5 times slower on CPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): source (with or without mkl)
- TensorFlow version (use command below): 2.0.0
- Python version: 3.6.8
- Bazel version (if compiling from source): 0.20.0
- GCC/Compiler version (if compiling from source):  7.4.0
- CUDA/cuDNN version: 7.6.2

**Describe the current behavior**
keras.LSTM in Tensorflow built with mkl is 5 times slower on CPU

**Other info / logs**
tf.matmul and tf.keras.Conv2D are also tested and mkl version is faster.
"
33136,[tf1.14][TPU] Can not save model trained on TPU using tf.saved_model.save,"I want to deploy the model using TF serving, for which I require to save the model using low level API `tf.saved_model.save()`. I am using [this](https://colab.research.google.com/drive/1C2JLKR3Rn1j2cZ_z_ZrTIc36Ou2s1H0-) exact tutorial on TPU with everything default. In found this tutorial [here](https://cloud.google.com/tpu/docs/colabs). After `model.fit()` if I try to save the model using `tf.saved_model.save()`, it throws the error `AttributeError: 'TPUMirroredVariable' object has no attribute '_constraint'`

However, this works fine with GPU strategies. Please help, or share some info. on how to deploy model which is trained on TPU.

Thank you,
Rishabh Sahrawat"
33135,How to replace Keras' gradients() function with GradientTape in TF2.0?,"With the ""old"" Keras library I created heatmaps for my CNNs using the `keras.backend.gradients()` function, like this:

    # load model and image, then predict the class this image belongs to
    model = load_model(os.path.join(model_folder, ""custom_model.h5""))
    image = image.load_img(image_path)
    img_tensor = image.img_to_array(image)
    img_tensor = np.expand_dims(img_tensor, axis=0)
    img_tensor = preprocess_input(img_tensor)

    preds = model.predict(img_tensor)
    model_prediction = model.output[:, np.argmax(preds[0])]

    # Calculate pooled grads for heatmap
    conv_layer = model.get_layer(""block5_conv3"")  # last conv. layer
    grads = K.gradients(model_prediction, conv_layer.output)[0]
    pooled_grads = K.mean(grads, axis=(0, 1, 2))

    # Get values of pooled grads and model conv. layer output as Numpy arrays
    input_layer = model.get_layer(""model_input"")
    iterate = K.function([input_layer], [pooled_grads, conv_layer.output[0]])
    pooled_grads_value, conv_layer_output_value = iterate([img_tensor])

    # Continue with heatmap generation ...

Now I switched to TF2.0 and it's built-in Keras implementation. Everything works fine, however, using that code I get the following error when calling `K.gradients()`:

    tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead.

I did some research and tried to understand how I can make use of `GradientTape`, but unfortunately I don't know much about TF nor TF2.0 - I always worked with Keras. Can you guys guide me how I can make this gradient calculation work again with my setup?"
33134,[TF 2.0 API Docs] tf.nn.softmax,"## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/nn/softmax

## Description of issue (what needs changing):

### Clear description

Yes

### Correct links

yes

### Parameters defined

No. 
Per the code, the first argument ""logits"" can be of any type that can be passed to ""convert_to_tensor()"", not just a tensor. Therefore the documentation can be modified to include ""Tensor objects, numpy arrays, Python lists, and Python scalars"".

### Returns defined
yes

### Raises listed and defined
yes

### Submit a pull request?
yes
"
33133,[TF2.0] tf.data.Dataset.from_generator raises `InvalidArgumentError` when using flow_from_directory,"Hi,
I have an issue when creating tf.data.Dataset with  tf.keras.preprocessing.image.ImageDataGenerator() and flow_from_directory. Seems like the arguments are not passed in the right format. Or maybe I missed something...

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7
- CUDA/cuDNN version: 10.0
- GPU model and memory: GeForce GTX 1050Ti Max-Q

**Code to reproduce the issue**
```
import tensorflow as tf

flowers = tf.keras.utils.get_file(
    'flower_photos',
    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',
    untar=True)

img_gen = tf.keras.preprocessing.image.ImageDataGenerator()

BATCH_SIZE = 32
IMG_DIM = 224
NB_CLASSES = 5


ds = tf.data.Dataset.from_generator(
    img_gen.flow_from_directory, args=[flowers,(IMG_DIM, IMG_DIM),'rgb','categorical',BATCH_SIZE,False], 
    output_types=(tf.float32, tf.float32), 
    output_shapes = ([BATCH_SIZE,IMG_DIM,IMG_DIM,3],[BATCH_SIZE,NB_CLASSES]))

it = iter(ds)
batch = next(it)
```

**Other info / logs**
```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-7-5cabd7ffcaf2> in <module>
----> 1 batch = next(it)

~/anaconda3/envs/classifier/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py in __next__(self)
    620 
    621   def __next__(self):  # For Python 3 compatibility
--> 622     return self.next()
    623 
    624   def _next_internal(self):

~/anaconda3/envs/classifier/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py in next(self)
    664     """"""Returns a nested structure of `Tensor`s containing the next element.""""""
    665     try:
--> 666       return self._next_internal()
    667     except errors.OutOfRangeError:
    668       raise StopIteration

~/anaconda3/envs/classifier/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py in _next_internal(self)
    649             self._iterator_resource,
    650             output_types=self._flat_output_types,
--> 651             output_shapes=self._flat_output_shapes)
    652 
    653       try:

~/anaconda3/envs/classifier/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py in iterator_get_next_sync(iterator, output_types, output_shapes, name)
   2671       else:
   2672         message = e.message
-> 2673       _six.raise_from(_core._status_to_exception(e.code, message), None)
   2674   # Add nodes to the TensorFlow graph.
   2675   if not isinstance(output_types, (list, tuple)):

~/anaconda3/envs/classifier/lib/python3.7/site-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: ValueError: ('Invalid color mode:', b'rgb', '; expected ""rgb"", ""rgba"", or ""grayscale"".')
Traceback (most recent call last):

  File ""/home/dhassault/anaconda3/envs/classifier/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py"", line 464, in get_iterator
    return self._iterators[iterator_id]

KeyError: 0


During handling of the above exception, another exception occurred:


Traceback (most recent call last):

  File ""/home/dhassault/anaconda3/envs/classifier/lib/python3.7/site-packages/tensorflow_core/python/ops/script_ops.py"", line 221, in __call__
    ret = func(*args)

  File ""/home/dhassault/anaconda3/envs/classifier/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py"", line 585, in generator_py_func
    values = next(generator_state.get_iterator(iterator_id))

  File ""/home/dhassault/anaconda3/envs/classifier/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py"", line 466, in get_iterator
    iterator = iter(self._generator(*self._args.pop(iterator_id)))

  File ""/home/dhassault/anaconda3/envs/classifier/lib/python3.7/site-packages/keras_preprocessing/image/image_data_generator.py"", line 540, in flow_from_directory
    interpolation=interpolation

  File ""/home/dhassault/anaconda3/envs/classifier/lib/python3.7/site-packages/keras_preprocessing/image/directory_iterator.py"", line 93, in __init__
    interpolation)

  File ""/home/dhassault/anaconda3/envs/classifier/lib/python3.7/site-packages/keras_preprocessing/image/iterator.py"", line 176, in set_processing_attrs
    '; expected ""rgb"", ""rgba"", or ""grayscale"".')

ValueError: ('Invalid color mode:', b'rgb', '; expected ""rgb"", ""rgba"", or ""grayscale"".')


	 [[{{node PyFunc}}]] [Op:IteratorGetNextSync]
```

And especially:
```
ValueError: ('Invalid color mode:', b'rgb', '; expected ""rgb"", ""rgba"", or ""grayscale"".')
```"
33132,fully_connect cannot  be quantilized,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):pip
- TensorFlow version (use command below):1.14.0
- Python version:3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10/7.0
- GPU model and memory:32G

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
tflite_convert  the pb ,it said fully_connect  lacking min/max

**Describe the expected behavior**
it should be passed
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
tflite_convert --output_file landmark.tflite --graph_def_file landmark.pb --inference_type QUANTIZED_UINT8 --input_arrays image_batch --input_shapes 1,112,112,3 --output_arrays pfld_inference/output --std_dev_values 255 --mean_values 0
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Array pfld_inference/fc/Tensordot, which is an input to the Reshape operator producing the output array pfld_inference/output, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.
Fatal Python error: Aborted
"
33131,how to assign value to a EagerTensor slice? ----'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment,"as in numpy or pytorch ,we can do someting like this, but how to do it with tf2.0.
the following code will raise exception as :

`'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment`
prediction[:,:,0]=tf.math.sigmoid(prediction[:,:,0])
"
33130,Cannot find NvInfer.h with TensorRT install,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:1.12.0
- Python version:3.5
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):0.15
- GCC/Compiler version (if compiling from source):GCC
- CUDA/cuDNN version:9.0
- GPU model and memory:7.1.4



**Describe the problem**
  When I try to compile Tensorflow I met this issue. To solve this problem I first try to modify the TensorRT install path in perform ./configure, but I find path is not changed, for example:
           default path:  /usr/lib/x86_64-linux-gnu
           I set path is: /usr/local/TensorRT
 This is useless, I don't know why,
So, I second try to copy NvInfer.h to /usr/lib/x86_64-linux-gnu, but this problem stall exist, and I find NvInfer.h is exist in /usr/lib/x86_64-linux-gnu, why cody can't find this file, I feel confused.
 
**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'build_defs.bzl': no such package '@local_config_tensorrt//': Traceback (most recent call last):
        File ""/install_bar/tensorflow-1.12.0/third_party/tensorrt/tensorrt_configure.bzl"", line 171
                _trt_lib_version(repository_ctx, trt_install_path)
        File ""/install_bar/tensorflow-1.12.0/third_party/tensorrt/tensorrt_configure.bzl"", line 81, in _trt_lib_version
                _find_trt_header_dir(repository_ctx, trt_install_path)
        File ""/install_bar/tensorflow-1.12.0/third_party/tensorrt/tensorrt_configure.bzl"", line 67, in _find_trt_header_dir
                auto_configure_fail((""Cannot find NvInfer.h with Ten...))
        File ""/install_bar/tensorflow-1.12.0/third_party/gpus/cuda_configure.bzl"", line 317, in auto_configure_fail
                fail((""\n%sCuda Configuration Error:%...)))

Cuda Configuration Error: Cannot find NvInfer.h with TensorRT install path /usr/lib/x86_64-linux-gnu
WARNING: Target pattern parsing failed.
ERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'build_defs.bzl': no such package '@local_config_tensorrt//': Traceback (most recent call last):
        File ""/install_bar/tensorflow-1.12.0/third_party/tensorrt/tensorrt_configure.bzl"", line 171
                _trt_lib_version(repository_ctx, trt_install_path)
        File ""/install_bar/tensorflow-1.12.0/third_party/tensorrt/tensorrt_configure.bzl"", line 81, in _trt_lib_version
                _find_trt_header_dir(repository_ctx, trt_install_path)
        File ""/install_bar/tensorflow-1.12.0/third_party/tensorrt/tensorrt_configure.bzl"", line 67, in _find_trt_header_dir
                auto_configure_fail((""Cannot find NvInfer.h with Ten...))
        File ""/install_bar/tensorflow-1.12.0/third_party/gpus/cuda_configure.bzl"", line 317, in auto_configure_fail
                fail((""\n%sCuda Configuration Error:%...)))

Cuda Configuration Error: Cannot find NvInfer.h with TensorRT install path /usr/lib/x86_64-linux-gnu
"
33129,[TF2.0] How to get the intermediate layers output of pretrained model?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 5.3.5-arch1-1-ARCH
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 2.0.0
- **Python version**: 3.7.4
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

This is exactly the same issue as https://github.com/tensorflow/models/issues/5236
which has just become more of a real issue with TF 2.0.

In v1, I do this:

```python
outputs = list(map(lambda tname: tf.get_default_graph().get_tensor_by_name(tname), [
    'DCNN/block3_pool/MaxPool:0',
    'DCNN/block4_pool/MaxPool:0',
    'DCNN/block5_pool/MaxPool:0'
]))

with tf.Session() as sess:
    val_outputs = sess.run(outputs)
```

What is the recommended way to do this with **TF 2.0 python API** now that graphs and sessions have been eliminated? Especially **without** access to the code that builds the model, which has been the greatest use case of get_tensor_by_name()? Is there any solution **other than using tf.compat.v1**?

Example:
```python
dcnn = tf.keras.applications.VGG16(include_top=False, weights='imagenet')
# How to access vgg16/block3_pool/MaxPool:0 in TF 2.0 without the knowledge of the model source code?
```

Is there any upgrade instruction? Or plans to accommodate such use cases?
Thanks!"
33128,"How to using batch norm with different T in every batch which input is  [B, T, D] shape tensor ","```
Traceback (most recent call last):
  File ""/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1356, in _do_call
    return fn(*args)
  File ""/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1341, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1429, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument: Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [241,23,1], [batch]: [260,23,1]
         [[{{node MultiDeviceIteratorGetNextFromShard}}]]
         [[RemoteCall]]
         [[IteratorGetNext]]
         [[gradients/speaker_res_net_raw_model_1/model/resnet/resblock-4/bnb_branch2a/FusedBatchNorm_grad/FusedBatchNormGrad/_8463]]
  (1) Invalid argument: Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [241,23,1], [batch]: [260,23,1]
         [[{{node MultiDeviceIteratorGetNextFromShard}}]]
         [[RemoteCall]]
         [[IteratorGetNext]]
0 successful operations.
3 derived errors ignored.
```

```
During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/tmp-data/zhanghui/delta/./delta/main.py"", line 111, in <module>
    app.run(main)
  File ""/home/luban/.local/lib/python3.6/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/home/luban/.local/lib/python3.6/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/tmp-data/zhanghui/delta/./delta/main.py"", line 79, in main
    solver.train_and_eval()
  File ""/tmp-data/zhanghui/delta/delta/utils/solver/estimator_solver.py"", line 445, in train_and_eval
    self.train_and_eval_one_epoch(nn, train_spec, eval_spec)
  File ""/tmp-data/zhanghui/delta/delta/utils/solver/estimator_solver.py"", line 391, in train_and_eval_one_epoch
    nn, train_spec, eval_spec)
  File ""/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py"", line 473, in train_and_evaluate
    return executor.run()
  File ""/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py"", line 613, in run
    return self.run_local()
  File ""/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py"", line 714, in run_local
    saving_listeners=saving_listeners)
  File ""/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 367, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1156, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1219, in _train_model_distributed
    self._config._train_distribute, input_fn, hooks, saving_listeners)
  File ""/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1329, in _actual_train_model_distributed
    saving_listeners)
  File ""/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1484, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 754, in run
    run_metadata=run_metadata)
  File ""/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1252, in run
    run_metadata=run_metadata)
  File ""/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1353, in run
   raise six.reraise(*original_exc_info)
  File ""/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1338, in run
    return self._sess.run(*args, **kwargs)
  File ""/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1411, in run
    run_metadata=run_metadata)
  File ""/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1169, in run
    return self._sess.run(*args, **kwargs)
  File ""/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 950, in run
    run_metadata_ptr)
  File ""/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1173, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1350, in _do_run
    run_metadata)
  File ""/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1370, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument: Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [241,23,1], [batch]: [260,23,1]
         [[{{node MultiDeviceIteratorGetNextFromShard}}]]
         [[RemoteCall]]
         [[IteratorGetNext]]
         [[gradients/speaker_res_net_raw_model_1/model/resnet/resblock-4/bnb_branch2a/FusedBatchNorm_grad/FusedBatchNormGrad/_8463]]
  (1) Invalid argument: Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [241,23,1], [batch]: [260,23,1]
         [[{{node MultiDeviceIteratorGetNextFromShard}}]]
         [[RemoteCall]]
         [[IteratorGetNext]]
0 successful operations.
```"
33127,TensorflowLite Upsampling2D does not change output shape when resize_input_tensor is called,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes (code can be found here https://github.com/benjamintanweihao/YOLOv3)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows, but probably all
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.0
- Python version: 3.7.3
- CUDA/cuDNN version: 10.0
- GPU model and memory: NVIDIA GTX 1070 8GB

**Describe the current behavior**
Upsampling 2D set to twice the size of the input always outputs the size set at convert time, even when resize inputs are called.

**Describe the expected behavior**
Upsampling2D outputs size times the size of the new input

**Code to reproduce the issue**

```
import tensorflow.keras as keras
from tensorflow.lite.python import lite, interpreter

#define model that uses upsampling 2D and depends on it's output changing with the input size
def DemoModel():
  inputs = keras.layers.Input(shape=[None,None, 3], name=""input_1"")
  a = keras.layers.MaxPooling2D((2,2))(inputs)
  b = keras.layers.UpSampling2D(size=(2,2))(a)
  c = keras.layers.Concatenate()([b, inputs])
  return keras.models.Model(inputs=[inputs], outputs=[a, c])

#convert to tflite
model = DemoModel()
model.compile(optimizer=""sgd"", loss=""mean_squared_error"")
model.save(""testModel.h5"")

converter = lite.TFLiteConverter.from_keras_model_file(""testModel.h5"", input_shapes={'input_1': [1,32,64,3]})
tflite_model=converter.convert()


#run the tflite model

inter = interpreter.Interpreter(model_content=tflite_model)
print(inter.get_input_details())
inter.resize_tensor_input(1, [1,16,32,3])
inter.allocate_tensors()
print(inter.get_input_details())


```"
33125,[TF2.0] Keras layers with custom tensors as variables,"(I've asked this question on [StackOverflow](https://stackoverflow.com/questions/58275253/tensorflow-2-0-keras-layers-with-custom-tensors-as-variables), but after looking closer into the source code, I'm not sure if a clean solution exits yet, so re-posting it here.)

In TF 1.x, it was possible to build layers with custom variables. Here's an example:

```python
import numpy as np
import tensorflow as tf

def make_custom_getter(custom_variables):
    def custom_getter(getter, name, **kwargs):
        if name in custom_variables:
            variable = custom_variables[name]
        else:
            variable = getter(name, **kwargs)
        return variable
    return custom_getter

# Make a custom getter for the dense layer variables.
# Note: custom variables can result from arbitrary computation;
#       for the sake of this example, we make them just constant tensors.
custom_variables = {
    ""model/dense/kernel"": tf.constant(
        np.random.rand(784, 64), name=""custom_kernel"", dtype=tf.float32),
    ""model/dense/bias"": tf.constant(
        np.random.rand(64), name=""custom_bias"", dtype=tf.float32),
}
custom_getter = make_custom_getter(custom_variables)

# Compute hiddens using a dense layer with custom variables.
x = tf.random.normal(shape=(1, 784), name=""inputs"")
with tf.variable_scope(""model"", custom_getter=custom_getter):
    Layer = tf.layers.Dense(64)
    hiddens = Layer(x)

print(Layer.variables)
```
The printed variables of the constructed dense layer will be custom tensors we specified in the `custom_variables` dict:
```python
[<tf.Tensor 'custom_kernel:0' shape=(784, 64) dtype=float32>, <tf.Tensor 'custom_bias:0' shape=(64,) dtype=float32>]
```
This allows us to create layers/models that use provided tensors in `custom_variables` directly as their weights, so that we could further differentiate the output of the layers/models with respect to any tensors that `custom_variables` may depend on (particularly useful for implementing functionality in [modulating sub-nets](https://distill.pub/2018/feature-wise-transformations/), [parameter generation](https://arxiv.org/abs/1705.10301), [meta-learning](https://github.com/deepmind/learning-to-learn), etc.).

Variable scopes used to make it easy to nest all off graph-building inside scopes with custom getters and build models that used the provided tensors as their parameters. Since sessions and variable scopes are no longer advisable in TF 2.0 (and all of that low-level stuff is moved to `tf.compat.v1`), what would be **the best practice** to implement the above using Keras and TF 2.0?

**Note:** It looks like `tf.keras.layers.Layer` supports custom getters, but I'm running into issues passing arbitrary tensors instead of variables (e.g., the getter [gets wrapped internally](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/base_layer.py#L412-L416) which I'm not sure how affects non-variable tensors). So, not sure the use-case I outlined above is properly supported. What's the best way to go about it?"
33124,Can a tensorflow-lite model accept a variable-shaped input?,"When converting a saved frozen graph to tensorflow lite I received the following error: `ValueError: None is only supported in the 1st dimension. Tensor 'input' has invalid shape '[1, None, None]'.`

I'm expecting a variable-sized input that can range from e.g. 10x7 to 100x100, but this seems to be actively prevented in the conversion to tensorflow lite.

It seems to be possible to fix the input size and reshape the graph to accommodate the input (e.g. https://stackoverflow.com/a/55732431) but if I had to do it for every call the overhead would probably kill any gain made by switching to tensorfow lite.

Is there a way around that, or as now this functionality is missing from tf-lite?

I'm using tensorflow 1.14 - I'd be happy to switch to tensorflow 2.0 but at least by looking at the source code it seems to have the same problem."
33122,Roll of absl breaks TFLite builds (absl::MakeTaggedSeedSeq),"Build Top of master with 

./tensorflow/lite/tools/make/download_dependencies.sh                                                                                                                                                                  
./tensorflow/lite/tools/make/build_lib.sh    


Fails with:

tensorflow/lite/tools/make/downloads/absl/absl/random/internal/named_generator.cc: In function ‘int main()’:                                                                                                                  │····
tensorflow/lite/tools/make/downloads/absl/absl/random/internal/named_generator.cc:23:25: error: ‘MakeTaggedSeedSeq’ is not a member of ‘absl’                                                                                 │····
   auto seed_seq = absl::MakeTaggedSeedSeq(""TEST_GENERATOR"", std::cerr);                                                                                                                                                      │····
                         ^~~~~~~~~~~~~~~~~                              

This has been fixed in top of master absl by deleting the file. 


**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 19.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source 
- TensorFlow version: master
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


"
33117,Object Detection API Training fails with: Cuda call failed with an illegal memory access on TF1.15rc3,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Using legacy train.py script from Object Detection API
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS 7.6
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below):  v1.15.0-rc2-8-g776451109a 1.15.0-rc3
- Python version: 3.6.8
- Bazel version (if compiling from source): ""https://github.com/bazelbuild/bazel/releases/download/0.24.1/bazel-0.24.1-installer-linux-x86_64.sh""
- GCC/Compiler version (if compiling from source): GCC 6.5.0
- CUDA/cuDNN version: CUDA ""10.0"", cuDNN ""7-7.6.4.38-1.cuda10.0""
- GPU model and memory: ""NVIDIA Tesla K80, 11441MiB""

**Describe the current behavior**
The training from this source compiled binary starts successfully, but dies after some iterations.

**Describe the expected behavior**
The training commences and finishes normally.

**Code to reproduce the issue**
Compile the current r1.15 release candidate and use the train.py script from the OD API to train a model.

**Other info / logs**
The training works with an official pip release of tensorflow 1.14 package, but fails with the compiled new version. Here is the log output of the run: Also start up seems to be incredibly slow (almost 60 seconds for the first iteration)
[tf_err_log.txt](https://github.com/tensorflow/tensorflow/files/3698811/tf_err_log.txt)

"
33113,Warning on why compute devices aren't being used ,"**System information**
- TensorFlow version (you are using): 2.0.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
As of now, if ""tf.debugging.set_log_device_placement(True)"" is set, there is useful feedback on which device is being used for computation. 

It would be helpful however, if feedback was given as to why a TPU, GPU, XLA_GPU or CPU wasn't used. Especially if a lower-power device was chosen instead (GPU instead of TPU, or CPU used instead of GPU for instance). 

Information and hints about CUDA versions, incompatibility and other setup flaws would greatly speed up debugging and getting things going.

**Will this change the current api? How?**
No, it will simply add information to the device placement log.

**Who will benefit with this feature?**
Any users trying to set up their TPU, GPU or cluster.
"
33112,Link broken,The link on the following line is broken: https://github.com/tensorflow/docs/blob/master/site/en/tutorials/load_data/csv.ipynb?short_path=443a7a7#L360
33110,Weight decay in adam optimizer,"- TensorFlow version: 1.14 / 2.*
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**
Could we please get weight decay (L2 penalty) in Adam optimizer? PyTorch has it and it is as easy as adding a `weight_decay` float argument when initializing the optimizer.
Rectified adam would be great too! 

**Who will benefit with this feature?**
Everyone that want to use L2 weight decay. 
"
33108,Tensorflow 2.0 - bazel build from source fails (can't exec cc1plus),"**System information**
System: Gentoo
Tensorflow 2.0, 1.14
Python 3.6
Installed from source
Bazel 0.26.1
GCC 8.3.0, 7.4.0 (both fails, both have c++ installed - /usr/libexec/gcc/x86_64-pc-linux-gnu/8.3.0/cc1plus)
CUDA 10.1
CuDNN 7.6.4


```
# bazel build --config=v2 //tensorflow/tools/pip_package:build_pip_package

...

INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (278 packages loaded, 21924 targets configured).
INFO: Found 1 target...
ERROR: /w/pip/.cache/bazel/_bazel_pip/667008e4969ed3eba4f1f491903cb254/external/com_google_protobuf/BUILD:106:1: C++ compilation of rule '@com_google_protobuf//:protobuf_lite' failed (Exit 1)
gcc: error trying to exec 'cc1plus': execvp: No such file or directory
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.

```


g++ is installed on system...

```
 # g++ --print-prog-name=cc1plus
/usr/libexec/gcc/x86_64-pc-linux-gnu/8.3.0/cc1plus
```

Please help, I get the same error for every other release."
33107,Compilation error - pybind11,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Centos 6.9
- N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0
- Python version: 3.7
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source):  0.25.2
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: 10/7
- GPU model and memory: v100



**Describe the problem**
Compilation fails around tensorflow/python

**Provide the exact sequence of commands / steps that you executed before running into the problem**
./configure 
bazel build --jobs=16 --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
Need to compile for RHEL6 and Centos6 as precompiled packages need glibc > 2.12
Have successfully compiled most previous versions with a little massaging. Attached is the log from where the error occurs.
[tens2.0err.txt](https://github.com/tensorflow/tensorflow/files/3696937/tens2.0err.txt)


"
33105,tf.data.Dataset.interleave does not seem to respect num_parallel_calls argument,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): *Yes*
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): *Linux 5.3.1-arch1-1-ARCH GNU/Linux*
- TensorFlow installed from (source or binary): *binary*
- TensorFlow version (use command below): *v2.0.0-rc2-26-g64c3d38 2.0.0*
- Python version: 3.7.4
- CUDA/cuDNN version: N/A
- GPU model and memory: None

**Describe the current behavior**

While using `tf.data.Dataset.interleave` on a Dataset of 9 elements, with the following arguments:
- cycle_length = 2
- block_length = 1
- num_parallel_calls = 2

6 threads seems to be launched concurrently.

**Describe the expected behavior**

I expect the function to be called concurrently by pair:
```
(call 1, call 2)
(call 3, call 4)
(call 5, call 6)
(call 7, call 8)
(call 9)
```

**Code to reproduce the issue**

```python
import time
import timeit

import tensorflow as tf
from tensorflow.python.data.ops import dataset_ops
from tensorflow_core.python.data.ops.readers import _create_or_validate_filenames_dataset, \
    _create_dataset_reader
from tensorflow.python.framework import tensor_spec

# Clone of TFRecordDataset to add some customization (see creator_fn)
class MyTFRecordDataset(dataset_ops.DatasetV2):
    def _inputs(self):
        return self._impl._inputs()  # pylint: disable=protected-access

    @property
    def element_spec(self):
        return tensor_spec.TensorSpec([], tf.dtypes.string)

    def __init__(self, filenames, compression_type=None, buffer_size=None,
                 num_parallel_reads=None):
        filenames = _create_or_validate_filenames_dataset(filenames)

        self._filenames = filenames
        self._compression_type = compression_type
        self._buffer_size = buffer_size
        self._num_parallel_reads = num_parallel_reads

        def creator_fn(filename):
            tf.print(""Creator_fn"", filename.numpy())
            start = time.perf_counter()
            time.sleep(0.5)
            result = tf.constant([str(x) for x in range(7)])
            tf.print(""Reading time"", time.perf_counter() - start)
            return result

        self._impl = _create_dataset_reader(
            lambda x: tf.data.Dataset.from_tensor_slices(tf.py_function(creator_fn, [x], tf.string)),
            filenames, num_parallel_reads)
        variant_tensor = self._impl._variant_tensor  # pylint: disable=protected-access
        super(MyTFRecordDataset, self).__init__(variant_tensor)

def dataset_interleave_ds():
    ds = tf.data.Dataset.from_tensor_slices(
        [str(x) for x in range(9)]
    ).interleave(
        MyTFRecordDataset,
        cycle_length=2,
        block_length=1,
        num_parallel_calls=2
    )

    return ds

def main():
    ds = dataset_interleave_ds()

    def iterate():
        tf.print(""Iterating"")
        for i, s in ds.enumerate():
            tf.print(""Iteration"", i, s.shape)
            time.sleep(0.0)

    tf.print(timeit.timeit(iterate, number=1))


if __name__ == '__main__':
    main()
```

**Other info / logs**

With `num_parallel_calls` set to `None` I obtain the expected behavior:
```
Iterating
Creator_fn b'0'
Reading time 0.5008154709994415
Iteration 0 TensorShape([])
Creator_fn b'1'
Reading time 0.5007261740011018
Iteration 1 TensorShape([])
```
Two files loaded, iterate over them
```
Iteration 2 TensorShape([])
Iteration 3 TensorShape([])
Iteration 4 TensorShape([])
Iteration 5 TensorShape([])
Iteration 6 TensorShape([])
Iteration 7 TensorShape([])
Iteration 8 TensorShape([])
Iteration 9 TensorShape([])
Iteration 10 TensorShape([])
Iteration 11 TensorShape([])
Iteration 12 TensorShape([])
Iteration 13 TensorShape([])
```
End of sequences. Open new files:
```
Creator_fn b'2'
Reading time 0.500802348000434
Iteration 14 TensorShape([])
Creator_fn b'3'
Reading time 0.5007076660003804
Iteration 15 TensorShape([])
Iteration 16 TensorShape([])
Iteration 17 TensorShape([])
Iteration 18 TensorShape([])
Iteration 19 TensorShape([])
Iteration 20 TensorShape([])
Iteration 21 TensorShape([])
Iteration 22 TensorShape([])
Iteration 23 TensorShape([])
Iteration 24 TensorShape([])
Iteration 25 TensorShape([])
Iteration 26 TensorShape([])
Iteration 27 TensorShape([])
```
And so on:
```
Creator_fn b'4'
Reading time 0.5007231710005726
Iteration 28 TensorShape([])
Creator_fn b'5'
Reading time 0.5007872259993746
[...]
4.591206809000141
```
Total time is ~4.5 seconds, according the 0.5s sleep executed 9 times.

With `num_parallel_calls` set to `2` I expect to divide my execution time by 2, each pair of ""files"" being read in parallel.

```
Iterating
Creator_fn b'0'
Creator_fn b'1'
Creator_fn b'3'
Creator_fn b'5'
Creator_fn b'4'
Creator_fn b'2'
Reading time 0.5008060520012805
Reading time 0.5010762649999378
Reading time 0.501729752000756
Reading time 0.5024584279999544
Reading time 0.503098459001194
Reading time 0.5040528110002924
```
6 Files opened in parallel...
```
Iteration 0 TensorShape([])
[...]
Iteration 14 TensorShape([])
Creator_fn b'7'
Creator_fn b'6'
Iteration 15 TensorShape([])
[...]
Iteration 28 TensorShape([])
Creator_fn b'8'
Iteration 29 TensorShape([])
[...]
Iteration 41 TensorShape([])
Reading time 0.5002393860013399
Reading time 0.5006544690004375
Iteration 42 TensorShape([])
Iteration 43 TensorShape([])
Iteration 44 TensorShape([])
Iteration 45 TensorShape([])
Iteration 46 TensorShape([])
Reading time 0.5014183660005074
Iteration 47 TensorShape([])
[...]
Iteration 62 TensorShape([])
1.047351510000226
```
Execution time divided by 4!

By the way, what is the meaning of `num_parallel_calls` set to 1. 1 parallel call is 2 threads or it should be sequential?"
33104,Prediction on batch values vary with batch size.,"When I predict on batches, e.g. with *predict_on_batch()*, I get different predictions depending on the batch size I am using. The differnce is sometimes as early as on the 4th digit. At first I thought it was due to batch normalization, but then I tried with pretrained VGG16 which I believe do not include batch normalization. This holds on all system settings I have tired:

Ubuntu and Windows
TF 1.1, 1.4 and 2.0
Pre-trained on Imagenet and trained from scratch Inception V3
Pre-trained on Imagenet VGG16 (code bellow)
predict_on_batch(), flow_from_directory(), and flow_from_dataframe()

Since it is constistent on all systems I guess it is not a bug but a known artefact of TF. I have Googled och searched the repository but not found out why this happens.

```{python}
import numpy as np
import tensorflow as tf

model = tf.keras.applications.VGG16(input_shape=(224, 224, 3),
                                    include_top=True,
                                    weights='imagenet')

path = ""path_to_some_image""  # Use any image you got.
img = tf.keras.preprocessing.image.load_img(path, target_size=(224, 224))

batch_size = 32  # Changing this to, say, 3 gives different predictions.
batch_holder = np.zeros((batch_size, 224, 224, 3))
for i in range(batch_size):
    batch_holder[i] = img

predictions = model.predict_on_batch(batch_holder)
print(predictions[:, 0])
```
"
33103,"TPU, model.fit : 2GB of RAM limit","


**Describe the current behavior**

There is an error (""Session is not found""), when trying to use a dataset with the size more than 2GB of RAM.
Error is reproducible for different step size (256,1024,8192).

**Describe the expected behavior**

No error when feeding a dataset which is more than 2GB of RAM.

**Code to reproduce the issue**

To reproduce, just copy the following code to Colab with TPU enabled.
```
import tensorflow as tf
import numpy as np

import distutils
if distutils.version.LooseVersion(tf.__version__) < '1.14':
    raise Exception('This notebook is compatible with TensorFlow 1.14 or higher, for TensorFlow 1.13 or lower please use the previous version at https://github.com/tensorflow/tpu/blob/r1.13/tools/colab/fashion_mnist.ipynb')

import os

resolver = tf.contrib.cluster_resolver.TPUClusterResolver('grpc://' + os.environ['COLAB_TPU_ADDR'])
tf.contrib.distribute.initialize_tpu_system(resolver)
strategy = tf.contrib.distribute.TPUStrategy(resolver)

optimizer = tf.contrib.tpu.CrossShardOptimizer(tf.train.GradientDescentOptimizer(0.01))

with strategy.scope():
  model = tf.keras.applications.VGG16(input_shape=(32,32,3),classes=10, weights=None)#  create_model()
  model.compile(
      optimizer=optimizer,
      loss='mse',
      metrics=['mse'])

# 1024*192 * 32*32*3 * 4bytes = 2 415 919 104 bytes
X = np.zeros((1024*192, 32,32,3),dtype=np.float32)
y = np.ones((1024*192, 10),dtype=np.float32)

model.fit(
    X,y,
    epochs=10,
    steps_per_epoch=1024, #batch size is 192 per TPU
)
```

**Other info / logs**

```
---------------------------------------------------------------------------

AbortedError                              Traceback (most recent call last)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1355     try:
-> 1356       return fn(*args)
   1357     except errors.OpError as e:

13 frames

AbortedError: Session 14acf315e768e91c is not found.


During handling of the above exception, another exception occurred:

AbortedError                              Traceback (most recent call last)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1368           pass
   1369       message = error_interpolation.interpolate(message, self._graph)
-> 1370       raise type(e)(node_def, op, message)
   1371 
   1372   def _extend_graph(self):

AbortedError: Session 14acf315e768e91c is not found.
```

```
print(tf.GIT_VERSION)
v1.14.0-0-g87989f6959
print(tf.VERSION)
1.14.0
```"
33102,How can I update my existing tensorflow to 2.0?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version:1.13.0-rc2 --->2.0
- Python version:3.7.1
- Installed using virtualenv? pip? conda?:pip/conda
- Bazel version (if compiling from source):\
- GCC/Compiler version (if compiling from source):\
- CUDA/cuDNN version:\
- GPU model and memory:\



**Describe the problem**
 
Well, I have installed the tensorflow 1.13 before, and since the newest version 2.0 is open, I wanna to update my old tf to the new one, but I do not know how to do it stably. 
Can you help me?
Very THX! :)

"
33101,TF2.0: tf.print doesn't work in tensorflow keras,"System information

OS Platform and Distribution
Mac os (10.14.6)
TensorFlow installed from (source or binary):
binary
TensorFlow version (use command below):
2.0.0rc1
Python version:
Python 3.6.4

I want to print intermediate tensor in model constructed by tensorflow keras functional api, but i got error.  code bellows

```
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow as tf
import numpy as np

inputs = keras.Input(shape=(128,), name='digits')
x = layers.Dense(64, activation='relu', name='dense_1')(inputs)
x = layers.Dense(64, activation='relu', name='dense_2')(x)
outputs = layers.Dense(10, activation='softmax', name='predictions')(x)
tf.print(""outputs"",outputs)
model = keras.Model(inputs=inputs, outputs=outputs)

model.compile(optimizer=keras.optimizers.RMSprop(),loss=keras.losses.SparseCategoricalCrossentropy())

x_train = np.random.random_sample((100,128))
y_train = np.random.randint(0,10, (100,))
model.fit(x_train, y_train,epochs=10)
```
error informations bellows:

```
_FallbackException                        Traceback (most recent call last)
~/tf2_workspace/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_string_ops.py in string_format(inputs, template, placeholder, summarize, name)
    810         ""template"", template, ""placeholder"", placeholder, ""summarize"",
--> 811         summarize)
    812       return _result

_FallbackException: This function does not handle the case of the path where all inputs are not already EagerTensors.

During handling of the above exception, another exception occurred:

AttributeError                            Traceback (most recent call last)
<ipython-input-106-84d972d9b93b> in <module>
      3 x = layers.Dense(64, activation='relu', name='dense_2')(x)
      4 outputs = layers.Dense(10, activation='softmax', name='predictions')(x)
----> 5 tf.print(""x_train"",outputs)
      6 x = keras.backend.print_tensor(outputs, 'outputs')
      7 # tf.Print(""x_train"",x)

~/tf2_workspace/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/ops/logging_ops.py in print_v2(*inputs, **kwargs)
    371         placeholder=placeholder,
    372         summarize=summarize,
--> 373         name=format_name)
    374 
    375   if compat.forward_compatible(2019, 5, 27):

~/tf2_workspace/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/ops/string_ops.py in string_format(template, inputs, placeholder, summarize, name)
    190                                       placeholder=placeholder,
    191                                       summarize=summarize,
--> 192                                       name=name)
    193 
    194 

~/tf2_workspace/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_string_ops.py in string_format(inputs, template, placeholder, summarize, name)
    815         return string_format_eager_fallback(
    816             inputs, template=template, placeholder=placeholder,
--> 817             summarize=summarize, name=name, ctx=_ctx)
    818       except _core._SymbolicException:
    819         pass  # Add nodes to the TensorFlow graph.

~/tf2_workspace/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_string_ops.py in string_format_eager_fallback(inputs, template, placeholder, summarize, name, ctx)
    869     summarize = 3
    870   summarize = _execute.make_int(summarize, ""summarize"")
--> 871   _attr_T, inputs = _execute.convert_to_mixed_eager_tensors(inputs, _ctx)
    872   _inputs_flat = list(inputs)
    873   _attrs = (""T"", _attr_T, ""template"", template, ""placeholder"", placeholder,

~/tf2_workspace/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py in convert_to_mixed_eager_tensors(values, ctx)
    275 def convert_to_mixed_eager_tensors(values, ctx):
    276   v = [ops.internal_convert_to_tensor(t, ctx=ctx) for t in values]
--> 277   types = [t._datatype_enum() for t in v]  # pylint: disable=protected-access
    278   return types, v
    279 

~/tf2_workspace/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py in <listcomp>(.0)
    275 def convert_to_mixed_eager_tensors(values, ctx):
    276   v = [ops.internal_convert_to_tensor(t, ctx=ctx) for t in values]
--> 277   types = [t._datatype_enum() for t in v]  # pylint: disable=protected-access
    278   return types, v
    279 

AttributeError: 'Tensor' object has no attribute '_datatype_enum'
```

"
33100,"Error saving Keras optimizer, TensorFlow 2.0","[Related](https://github.com/keras-team/keras/issues/13402), minimal reproducible example below. Opened a [PR](https://github.com/tensorflow/tensorflow/pull/33097), but doesn't fix this problem.

<hr>

**DOESN'T WORK:**

```python
from tensorflow.python.keras.layers import Input, Dense
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.optimizers import Nadam # [3]

import numpy as np
ipt = Input(shape=(4,))
out = Dense(1, activation='sigmoid')(ipt)

model = Model(ipt, out)
model.compile(optimizer=Nadam(lr=1e-4), loss='binary_crossentropy')

X = np.random.randn(32,4)
Y = np.random.randint(0,2,(32,1))
model.train_on_batch(X,Y)

modelpath = ""folder/model.h5""
model.save(modelpath)
```

<hr>

**WORKS**:  remove `.python` from above's imports, OR, `model.save(.., include_optimizer=False)`. Also works if `.python` is removed only from import `# [3]`.

<hr>

**Full error trace**:

```python

  File ""<ipython-input-1-fedd4f332165>"", line 23, in <module>
    model.save(modelpath)

  File ""D:\Anaconda\envs\tf2_env\lib\site-packages\tensorflow_core\python\keras\engine\network.py"", line 975, in save
    signatures, options)

  File ""D:\Anaconda\envs\tf2_env\lib\site-packages\tensorflow_core\python\keras\saving\save.py"", line 112, in save_model
    model, filepath, overwrite, include_optimizer)

  File ""D:\Anaconda\envs\tf2_env\lib\site-packages\tensorflow_core\python\keras\saving\hdf5_format.py"", line 115, in save_model_to_hdf5
    save_optimizer_weights_to_hdf5_group(f, model.optimizer)

  File ""D:\Anaconda\envs\tf2_env\lib\site-packages\tensorflow_core\python\keras\saving\hdf5_format.py"", line 587, in save_optimizer_weights_to_hdf5_group
    name, val.shape, dtype=val.dtype)

  File ""D:\Anaconda\envs\tf2_env\lib\site-packages\h5py\_hl\group.py"", line 139, in create_dataset
    self[name] = dset

  File ""D:\Anaconda\envs\tf2_env\lib\site-packages\h5py\_hl\group.py"", line 371, in __setitem__
    h5o.link(obj.id, self.id, name, lcpl=lcpl, lapl=self._lapl)

  File ""h5py\_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper

  File ""h5py\_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper

  File ""h5py\h5o.pyx"", line 202, in h5py.h5o.link

RuntimeError: Unable to create link (name already exists)
```"
33099,possible file name typo in raspberry pi example,"## URL(s) with the issue:
https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/raspberry_pi

## Description of issue (what needs changing):
Near the bottom of the instruction on speeding up inference it reads
```python3 classify_picamera.py \```

Based on the files in that example folder should that instead read
```python3 detect_picamera.py \```

### Clear description
If my assumption above is incorrect, then its unclear where the file ```classify_pycamera.py``` is coming from, and should maybe be explicitly mentioned.

### Submit a pull request?
I didnt plan to since its possibly a simple typo. But I can if you'd like."
33098,TF2.0 gradient result with some minor difference,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- I have written very simple custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Mac OS 10.13.16 and Windows 10):
- No mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source with tag of v2.0.0 ):
- TensorFlow version (use command below):
- Python version: 3.7
- Bazel version (0.26.1 & 0.25.3):
- GCC/Compiler version (Apple LLVM version 9.1.0 (clang-902.0.39.2)):
- CUDA/cuDNN version: CUDA 10.0 and cuDNN 7.4.x
- GPU model and memory: ()

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/3695438/tf_env.txt)

""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
tf.Tensor(31.999998, shape=(), dtype=float32) tf.Tensor(30.0, shape=(), dtype=float32) tf.Tensor(3.0, shape=(), dtype=float32) tf.Tensor(1.0, shape=(), dtype=float32)

**Describe the expected behavior**
As dy/dx = a^2 + b, I expect result of 32.0 for dy_dx

**Code to reproduce the issue**
import tensorflow as tf 


x = tf.constant(3.0)
a = tf.constant(5.0)
b = tf.constant(7.0)
c = tf.constant(9.0)


with tf.GradientTape() as tape:
	tape.watch([x, a, b, c])
	y = a**2 * x + b * x + c


[dy_dx, dy_da, dy_db, dy_dc] = tape.gradient(y, [x, a, b, c])
print(dy_dx, dy_da, dy_db, dy_dc)

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
33096,Reading TF2 summary file with tf.data.TFRecordDataset,"In TF1, i could use `summary_iterator` to read summary files. But now, it will throw a warning

```
WARNING:tensorflow: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.
Instructions for updating:
Use eager execution and: 
`tf.data.TFRecordDataset(path)`
```

So i'm wondering how to use `tf.data.TFRecordDataset(path)` to read tfevent files generated by TF2."
33095,Tensorflow 2.0 stop_gradient cause ValueError has 'None' for gradient for tf.Optimizers.Adam,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
Ubuntu 18.04.2 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): 
pip
- TensorFlow version (use command below): 
v2.0.0-rc2-26-g64c3d38 2.0.0 / 1.14.0
- Python version: 
3.7.4
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 
7.6.0
- GPU model and memory:

**Describe the current behavior**

For Tensorflow 2.0, stop_gradient() doesn't seem to work properly. 

For example, the code below update one model's weight using output from another model. In Tensorflow 2.0, it will try to compute gradient for the stop_gradient() model. 

**Describe the expected behavior**

In Tensorflow 1.14.0 and 1.13.2, it works correctly. The model only updates for model1 without any checking for model2. 

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import numpy as np
import pandas as pd
import tensorflow as tf
import tensorflow_probability as tfp
import sys
import time

from tensorflow.keras.layers import Dense, Input, Add, Lambda
from tensorflow.keras import Model
from tensorflow.keras.models import load_model
from tensorflow.keras import optimizers

from scipy.io import loadmat
from scipy.spatial.distance import cdist

tf.compat.v1.disable_eager_execution()

x = np.random.rand(100).reshape(100, 1).astype(float)
y = np.array(x>0.5).reshape(100, 1).astype(float)

input1 = Input(shape=(1, ), name='input1')
d1 = Dense(12, name='d11')(input1)
d1 = Dense(12, name='d12')(d1)
d1 = Dense(1, name='output1')(d1)

input2 = Input(shape=(1, ), name='input2')
d2 = Dense(12, name = 'd21')(input2)
d2 = Dense(12, name = 'd22')(d2)
d2 = Dense(1, name = 'output2')(d2)

label = Input(shape = (1, ), name = 'label')

model1 = Model(inputs = [input1, label], outputs = d1)
model2 = Model(inputs = input2, outputs = d2)

def cust_loss(xi, yi, yp):
    loss = tf.reduce_mean((yi - yp)**2) + tf.reduce_mean(tf.stop_gradient(model2(xi)))
    
    return loss

model1.add_loss(cust_loss(input1, label, d1))
optimizer = optimizers.Adam(lr = 3e-4)
model1.compile(optimizer, loss = None)

model1.fit({'input1': x, 'label': y}, None, epochs=100)
```

**Other info / logs**
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-2-bb2983e16d6f> in <module>
     26 model1.compile(optimizer, loss = None)
     27 
---> 28 model1.fit({'input1': x, 'label': y}, None, epochs=100)

~/miniconda3/envs/dl_ehr_rl/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    726         max_queue_size=max_queue_size,
    727         workers=workers,
--> 728         use_multiprocessing=use_multiprocessing)
    729 
    730   def evaluate(self,

~/miniconda3/envs/dl_ehr_rl/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
    672         validation_steps=validation_steps,
    673         validation_freq=validation_freq,
--> 674         steps_name='steps_per_epoch')
    675 
    676   def evaluate(self,

~/miniconda3/envs/dl_ehr_rl/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)
    187   # function we recompile the metrics based on the updated
    188   # sample_weight_mode value.
--> 189   f = _make_execution_function(model, mode)
    190 
    191   # Prepare validation data. Hold references to the iterator and the input list

~/miniconda3/envs/dl_ehr_rl/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py in _make_execution_function(model, mode)
    563   if model._distribution_strategy:
    564     return distributed_training_utils._make_execution_function(model, mode)
--> 565   return model._make_execution_function(mode)
    566 
    567 

~/miniconda3/envs/dl_ehr_rl/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _make_execution_function(self, mode)
   2182   def _make_execution_function(self, mode):
   2183     if mode == ModeKeys.TRAIN:
-> 2184       self._make_train_function()
   2185       return self.train_function
   2186     if mode == ModeKeys.TEST:

~/miniconda3/envs/dl_ehr_rl/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _make_train_function(self)
   2114           # Training updates
   2115           updates = self.optimizer.get_updates(
-> 2116               params=self._collected_trainable_weights, loss=self.total_loss)
   2117           # Unconditional updates
   2118           updates += self.get_updates_for(None)

~/miniconda3/envs/dl_ehr_rl/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py in get_updates(self, loss, params)
    498 
    499   def get_updates(self, loss, params):
--> 500     grads = self.get_gradients(loss, params)
    501     grads_and_vars = list(zip(grads, params))
    502     self._assert_valid_dtypes([

~/miniconda3/envs/dl_ehr_rl/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py in get_gradients(self, loss, params)
    396                            ""gradient defined (i.e. are differentiable). ""
    397                            ""Common ops without gradient: ""
--> 398                            ""K.argmax, K.round, K.eval."".format(param))
    399       if hasattr(self, ""clipnorm""):
    400         grads = [clip_ops.clip_by_norm(g, self.clipnorm) for g in grads]

ValueError: Variable <tf.Variable 'd21/kernel:0' shape=(1, 12) dtype=float32> has `None` for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval.
"
33094,Tensorflow 2.0.0 / tf.keras.layers.TimeDistributed layer can't be save to saved Model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution : Colaboratory (GPU Runtime)
- tensorflow version: 2.0.0
- python version: 3.6.8


**Describe the current behavior**
The model defined which has tf.keras.layers.timeDistributed layer cannot be save by model.save() function.

It shows the error below

```
ValueError                                Traceback (most recent call last)
/usr/lib/python3.6/inspect.py in getfullargspec(func)
   1125                                        skip_bound_arg=False,
-> 1126                                        sigcls=Signature)
   1127     except Exception as ex:

41 frames
ValueError: no signature found for builtin <tensorflow.python.keras.saving.saved_model.save_impl.LayerCall object at 0x7f74467284a8>

The above exception was the direct cause of the following exception:

TypeError                                 Traceback (most recent call last)
/usr/lib/python3.6/inspect.py in getfullargspec(func)
   1130         # else. So to be fully backwards compatible, we catch all
   1131         # possible exceptions here, and reraise a TypeError.
-> 1132         raise TypeError('unsupported callable') from ex
   1133 
   1134     args = []

TypeError: unsupported callable
```


**Describe the expected behavior**
In tensorflow 2.0.0 supposed the model.save model default to be saved in SavedModel format. 


**Code to reproduce the issue**
```mirrored_strategy = tf.distribute.MirroredStrategy()

def get_data():
  datasets, ds_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
  mnist_train, mnist_test = datasets['train'], datasets['test']

  BUFFER_SIZE = 10000

  BATCH_SIZE_PER_REPLICA = 64
  BATCH_SIZE = BATCH_SIZE_PER_REPLICA * mirrored_strategy.num_replicas_in_sync

  def scale(image, label):
    image = tf.cast(image, tf.float32)
    image /= 255

    return image, label

  train_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
  eval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)

  return train_dataset, eval_dataset

def get_model():
  with mirrored_strategy.scope():
    model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
        tf.keras.layers.MaxPooling2D(),
        # tf.keras.layers.Flatten(),
        tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(64, activation='softmax')),
        # tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(64, activation='softmax')),
        # tf.keras.layers.Dense(10, activation='softmax')
    ])

    model.compile(loss='sparse_categorical_crossentropy',
                  optimizer=tf.keras.optimizers.Adam(),
                  metrics=['accuracy'])
    return model

model = get_model()
model.save(""test_save"")
```

**Other info / logs**
The not only can be reproduce in Colaboratory, but also in normal Ubuntu machien which installed with tensorflow-gpu 2.0.0"
33092,Prebuilt libtensorflow (C API),"The link in 
https://www.tensorflow.org/install/lang_c

contain the 1.14 library. Are there any prebuilt link for the C API for 2.0 stable?

This also holds true for the install for java page

"
33090,model.evaluate gives unexpected results,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, but very close to example code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab and Ubuntu 18.04
- TensorFlow installed from (source or binary): Source and binary
- TensorFlow version (use command below): `v2.0.0-rc2-26-g64c3d382ca 2.0.0` and `v2.0.0-rc2-26-g64c3d38 2.0.0`
- Python version: 3.6.8 and 3.6.7
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: 10.0, 7.6
- GPU model and memory: K80, V100 16GB

**Describe the current behavior**

- `model.fit()` works and training converges
- validation during training gives effectively random performance
- `model.evaluate()` on both **training** and validation set gives random performance

Training and evaluating on the same **training set** gives random evaluation performance but expected training performance. Hence, there seems to be an issue with the evaluation in TF.

**Describe the expected behavior**

At very least, if I evaluate on the **training set**, I should see loss and accuracy similar to that is output during training.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

Check Colab notebook to reproduce the issue: https://colab.research.google.com/drive/1HBdfWjx3jj65wrP_a0r_Tu80genKBsw2

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Colab notebook should demonstrate everything and aid investigation.

Thanks in advance to anyone who can help!
"
33089,tf silently rounding to integer when converting to PIL format,"**System information**
- Have I written custom code: not really, no.
- OS Platform and Distribution: Linux Ubuntu 18.04.2 LTS (Bionic Beaver)
- TensorFlow installed from (source or binary): pip install (binary?)
- TensorFlow version (use command below): 1.14.0
- Python version: 3.5.2
- Bazel version (if compiling from source): - 
- GCC/Compiler version (if compiling from source): - 
- CUDA/cuDNN version: - (running on cpu only)
- GPU model and memory: - (running on cpu only)

**Describe the current behavior**
When converting to PIL format from array and back, tf silently rounds values to integers. (see https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/array_to_img, https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/img_to_array)

**Describe the expected behavior**
Throw a warning when silently rounding. maybe convert to float instead of integer if input is a float dtype?

**Code to reproduce the issue**
please see the short example test case here: https://github.com/WildTangles/tf-issues/blob/master/tensorflow_bug_conversions/bug_demo.ipynb

In particular, see: 
cell 4 (random noise image, 4800 unique values) vs. 
cell 6 (image after conversion to PIL and back, 256 unique values)

**Other info / logs**
Happy to provide if necessary."
33088,inconsistent behavior of model after creation vs. after loading from save file,"**System information**
- Have I written custom code: yes
- OS Platform and Distribution: Linux Ubuntu 18.04.2 LTS (Bionic Beaver)
- TensorFlow installed from (source or binary): pip install (binary?)
- TensorFlow version (use command below): 1.14.0
- Python version: 3.5.2
- Bazel version (if compiling from source): - 
- GCC/Compiler version (if compiling from source): - 
- CUDA/cuDNN version: - (running on cpu only)
- GPU model and memory: - (running on cpu only)

**Describe the current behavior**
When using lambda layers with parameters generated in a loop, creation-time behavior of model is different from behavior of model re-loaded from a save file.

**Describe the expected behavior**
Consistent behavior at creation-time of model and after re-loading from a save file.

**Code to reproduce the issue**
please see the short example test case here: https://github.com/WildTangles/tf-issues/blob/master/tensorflow_bug_lambdas/bug_demo.ipynb

(alternatively, please see the same notebook rendered on a different site here: https://nbviewer.jupyter.org/github/WildTangles/tf-issues/blob/master/tensorflow_bug_lambdas/bug_demo.ipynb)

In particular, see: 
cell 6 (expected output plotted by hand) vs. 
cell 7 (model behavior right after being created, matches expected) vs.
cell 10 (model behavior after being saved and re-loaded, does not match expected)

cell 4 (model definition, in particular, see first few lines with lambda layer parameters defined in a loop)

**Other info / logs**
Happy to provide if necessary."
33087,InternalError: Failed to call ThenRnnBackward,"**System information**
- Have I written custom code: yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux/google colab
- TensorFlow installed from (source or binary): pip install tensorflow-gpu==2.0.0
- TensorFlow version (use command below): 2.0.0
- Python version: 3
- CUDA/cuDNN version: 10.1
- GPU model and memory: 12gb (Tesla K80)

**Describe the current behavior**
Currently, I have training a simple seq2seq model using tensorflow-2.0.0-beta1.

RC1 and RC2 I had some memory leak issues, so I waited for final release. However, in 2.0.0 version, I got other bug in fit_generator():

``InternalError: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 3, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 162, 1024, 1, 128, 128, 0]  [Op:CudnnRNNBackprop]``

**Describe the expected behavior**

I back again to beta1, cause it's working fine well in this version."
33086,Distributed Keras MultiWorkerMirroredStrategy failed,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): stock example modified
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7

**Describe the current behavior**
Error when using MultiWorkerMirroredStrategy

**Describe the expected behavior**
No Error

**Code to reproduce the issue**
```
# worker.py
import tensorflow as tf
import os
import json

import tensorflow_datasets as tfds
from tensorflow import keras

""""""
Shell 1:

TF_CONFIG='{""cluster"": {""worker"": [""localhost:12345"", ""localhost:56789""]}, ""task"": {""index"": 0, ""type"": ""worker""}}' python worker.py

Shell 2:
TF_CONFIG='{""cluster"": {""worker"": [""localhost:12345"", ""localhost:56789""]}, ""task"": {""index"": 1, ""type"": ""worker""}}' python worker.py
""""""

print('tf {}'.format(tf.__version__))

strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()

tfds.disable_progress_bar()

BUFFER_SIZE = 10000
NUM_WORKERS = 2
GLOBAL_BATCH_SIZE = 64 * NUM_WORKERS

def scale(image, label):
    image = tf.cast(image, tf.float32)
    image /= 255
    return image, label

datasets, info = tfds.load(name='mnist',
                           with_info=True,
                           as_supervised=True)

train_datasets_unbatched = datasets['train'].map(scale).shuffle(BUFFER_SIZE)

train_datasets = train_datasets_unbatched.batch(GLOBAL_BATCH_SIZE)

def build_and_compile_cnn_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(32,3,activation='relu',input_shape=(28,28,1)),
        tf.keras.layers.MaxPooling2D(),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(64,activation='relu'),
        tf.keras.layers.Dense(10,activation='softmax')
    ])
    model.compile(
        loss=tf.keras.losses.sparse_categorical_crossentropy,
        optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
        metrics=['accuracy'])
    return model


with strategy.scope():
    model =  build_and_compile_cnn_model()

    model.fit(x=train_datasets, epochs=3)
```

**Other info / logs**
```
2019-10-06 15:34:50.454016: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-10-06 15:34:50.786372: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fdb6d700010 executing computations on platform Host. Devices:
2019-10-06 15:34:50.786394: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2019-10-06 15:34:50.839300: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:258] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12345, 1 -> localhost:56789}
2019-10-06 15:34:50.840100: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:12345
WARNING: Logging before flag parsing goes to stderr.
W1006 15:34:53.847770 4649092544 distribute_coordinator.py:825] `eval_fn` is not passed in. The `worker_fn` will be used if an ""evaluator"" task exists in the cluster.
W1006 15:34:53.857074 4649092544 distribute_coordinator.py:829] `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.
W1006 15:34:53.870861 4649092544 distributed_training_utils.py:1163] ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.
Epoch 1/3
    469/Unknown - 20s 42ms/step - loss: 2.1887 - accuracy: 0.28872019-10-06 15:35:13.827666: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext}}]]
469/469 [==============================] - 20s 42ms/step - loss: 2.1887 - accuracy: 0.2887
Epoch 2/3
2019-10-06 15:35:15.788116: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-10-06 15:35:15.788143: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-10-06 15:35:15.788208: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:234 : Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-10-06 15:35:15.788223: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
	 [[allreduce_1/CollectiveReduce]]
2019-10-06 15:35:15.789211: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-10-06 15:35:15.789231: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-10-06 15:35:15.789219: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-10-06 15:35:15.789318: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-10-06 15:35:15.789573: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-10-06 15:35:15.789589: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-10-06 15:35:15.789635: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:234 : Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-10-06 15:35:15.790304: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:234 : Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-10-06 15:35:15.790362: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:234 : Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-10-06 15:35:15.797384: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-10-06 15:35:15.797403: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-10-06 15:35:15.797452: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:234 : Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
  1/469 [..............................] - ETA: 14:26Traceback (most recent call last):
  File ""/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 123, in run_one_epoch
    batch_outs = execution_function(iterator)
  File ""/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 86, in execution_function
    distributed_function(input_fn))
  File ""/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 457, in __call__
    result = self._call(*args, **kwds)
  File ""/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 487, in _call
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
  File ""/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 1823, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File ""/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 1141, in _filtered_call
    self.captured_inputs)
  File ""/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 1224, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager)
  File ""/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 511, in call
    ctx=ctx)
  File ""/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py"", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.OutOfRangeError:  [_Derived_]End of sequence
	 [[node IteratorGetNext (defined at /Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]
	 [[allreduce_1/CollectiveReduce]] [Op:__inference_distributed_function_950]

Function call stack:
distributed_function


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""worker.py"", line 59, in <module>
    model.fit(x=train_datasets, epochs=3)
  File ""/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py"", line 728, in fit
    use_multiprocessing=use_multiprocessing)
  File ""/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py"", line 789, in fit
    *args, **kwargs)
  File ""/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py"", line 776, in wrapper
    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)
  File ""/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py"", line 853, in run_distribute_coordinator
    task_id, session_config, rpc_layer)
  File ""/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py"", line 360, in _run_single_worker
    return worker_fn(strategy)
  File ""/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py"", line 771, in _worker_fn
    return method(model, **kwargs)
  File ""/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 324, in fit
    total_epochs=epochs)
  File ""/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 146, in run_one_epoch
    total_epochs * steps_per_epoch))
TypeError: unsupported operand type(s) for *: 'int' and 'NoneType'
2019-10-06 15:35:16.209044: W tensorflow/core/common_runtime/eager/context.cc:290] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.
```
"
33085,Calling tf.keras.models.load_model / model.save twice fails,"Please see [CoLab](https://colab.research.google.com/drive/1dXgiJZDbGHdQFrquvtMBblgz_5O1WSlD) or [Github Gist](https://gist.github.com/srfrnk/713765b44dd6677ba3e26ff4ac46f68b#file-save-load-bug-ipynb) to see live demo.

Steps to reproduce:
1) Create model,compile and fit
2) Save model
3) Load model and fit
4) Save model
5) Load model

Get error: `ValueError: Passing a dictionary input to a Sequential Model which doesn't have FeatureLayer as the first layer is an error.`

Expected: Model that was loaded and saved again should be loaded without error."
33084,how to change version of protobuf when I compile source code of tensorflow1.9 by bazel,"I want to compile source code of tensorflow-1.9 for c library. When I compile the source code of tensorflow1.9 through bazel, it downloaded and used protobuf3.5 during the compilation process. How to modify the code can be downloaded and used protobuf3.4.1 during the compilation process."
33083,hub.module_v2.load() returns a non-callable object,"**System information**
- tf version: 2.0.0 gpu
- tf_hub version: 0.6
- python: 3.7 with Anaconda

```python
model = ""https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/3"" # MobileNetV2
# both lines below cannot work
layer = hub.KerasLayer(model)
layer = hub.KerasLayer(model, tags=""train"")
```
Error messages are long, but those related to the issue are below:
```python
~\anaconda\envs\tf2\lib\site-packages\tensorflow_hub\keras_layer.py in __init__(self, handle, trainable, arguments, **kwargs)
    102       self._func = handle
    103     else:
--> 104       self._func = module_v2.load(handle)
    105       if not callable(self._func):
    106         raise ValueError(""Non-callable result from hub.load('%s')"" %
```
```python
ValueError: Importing a SavedModel with tf.saved_model.load requires a 'tags=' argument if there is more than one MetaGraph. Got 'tags=None', but there are 2 MetaGraphs in the SavedModel with tag sets [[], ['train']]. Pass a 'tags=' argument to load this SavedModel.
```
then I realized if we just directly use `hub.KerasLayer()`, the `'tags='` argument will not be passed to `module_v2.load()`, which causes this error.
So then I modified the codes as below:
```python
model = ""https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/3""
mod = hub.module_v2.load(model,tags=['train']) # this line works fine
layer = hub.KerasLayer(mod) # error here
```
Howerver, this won't work, either. The error messages related to the error are:
```
~\anaconda\envs\tf2\lib\site-packages\tensorflow_hub\keras_layer.py in __init__(self, handle, trainable, arguments, **kwargs)
    102       self._func = handle
    103     else:
--> 104       self._func = module_v2.load(handle)
    105       if not callable(self._func):
    106         raise ValueError(""Non-callable result from hub.load('%s')"" %
```
and
```python
AttributeError: 'AutoTrackable' object has no attribute 'startswith'
```
It seems that it is related to the code in `keras_layer.py` and specificly codes below:
```python
def __init__(self, handle, trainable=False, arguments=None, **kwargs):
    self._handle = handle
    # Resolve the handle to a callable `func`.
    if callable(handle):
      self._func = handle
    else:
      self._func = module_v2.load(handle)
      if not callable(self._func):
           raise ValueError(""Non-callable result from hub.load('%s')"" %
                         str(handle))
```
When I pass the `mod` object to `hub.KerasLayer()`, the code in `if` branch is expected to be executed, but the code in `else` branch is executed, which means the `mod` object returned by  `module_v2.load()` is not `callable`. That doesn't make sense. 
So I wonder maybe it is a bug or there's something wrong with the `MobileNet` model or it just that the model doesn't fit in TF2.0."
33082,tf.data.Dataset.from_generator is not executing eagerly ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2
- Python version: 3

**Describe the current behavior**

`tf.data.Dataset.from_generator` method is not hydrating the dataset eagerly.  When a dataset is created using this method an instance of `DatasetV1Adapter` is returned rather than `DataSet`. 

**Describe the expected behavior**

Should return a `Dataset` [Reference](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/ops/dataset_ops.py#L510)

**Code to reproduce the issue**

[Colab link](https://colab.research.google.com/drive/1jPS1oVrFlWOzJmUFCaDMFwVUVbg9V1NK)

```

# Set up and ensure tf is executing eagerly


import tensorflow as tf

print(tf.__version__)
print(tf.executing_eagerly())

# Define a generator function and create dataset
# you would expect ds have data after from_generator method as it's expected to execute eagerly, but data generation logs are not printed

def gen():
  print('Log: generating data')
  for i in range(0,5):
    yield (i, [1] * i)

ds = tf.data.Dataset.from_generator(gen, (tf.int64, tf.int64), (tf.TensorShape([]), tf.TensorShape([None])))
tf.print(ds)

# Use dataset
# Logs are printed here

for (x,y) in ds:
  tf.print(x, y)

# Use ds one more time
# Now ds is hydrated but generator function is invoked again

x = ds.take(3)
print(x)
for value in x:
  tf.print(value)

```"
33081,why my coco_output.txt (output of evaluation coco object detection with tflite models)is empty!!!!!! ,"

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:samsung s8 pluse
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below):1.15
- Python version:3.6
- Bazel version (if compiling from source):0.26.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:NO CUDA
- GPU model and memory:NO GPU
"
33080,"Until Tensorflow 2.0, there had been an instance norm layer in tf.contrib.layers - please bring it back","<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version: 2.0:
- Are you willing to contribute it: Yes:

There's already the code for this layer in tensorflow 1.14, and (with a bit less parameter choice) in pix2pix example in tensorflow 2.0, It's trivial to add this layer to tensorflow.keras.layers, and I'm willing to do it.


**Describe the feature and the current behavior/state.**
Instance norm was found to be[ more effective](https://arxiv.org/abs/1607.08022) than any other form of normalization for convolutional neural networks with small batches.

It is used in tensorflow's [official example for pix2pix ](https://www.tensorflow.org/tutorials/generative/pix2pix), and was present in tf.contrib.layers in [tensorflow 1.14](https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/contrib/layers/instance_norm).

Instance norm is normalizes each channel according to the channel's spatial mean and standard deviation. 

**Will this change the current api? How?**
Add tensorflow.keras.layers.InstanceNormalization

**Who will benefit with this feature?**
Anyone who's working on a CNN with small batches.

"
33079,"make_csv_dataset with num_epochs=1,2,... (finite) produces (None,) shape tensors","The following CoLab and Github Gist show the issue:
[CoLab](https://colab.research.google.com/gist/srfrnk/5744026f803b57254b5ba6162dd16bf8/csv.ipynb)
[Github Gist](https://gist.github.com/srfrnk/5744026f803b57254b5ba6162dd16bf8#file-csv-ipynb)

It is expected that the same shape will be returned regardless of num_epochs.
Currently if num_epochs=1 the returned tensors all have shape (None,)
Only when num_epochs=None meaning infinite repeat will the correct shape be returned.
"
33078,Issue while compiling with AdamOptimizer,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `model.compile(optimizer=tf.train.AdamOptimizer(),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy']) `
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): Tensorflow -gpu from Anaconda
- TensorFlow version (use command below): 1.14.0
- Python version: 3.7
- CUDA/cuDNN version: 10
- GPU model and memory: Nvidia GeForce 1050Ti 4gb DDR5 VRAM

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
`
WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From convo.py:27: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.
**Describe the expected behavior**
Not expected warnings in the usage of AdamOptimizer. The code executed is working yet error is thrown on every execution
`
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
33077,TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float16 of argument 'x'.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution ( Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.14
- Python version:2.7
- Bazel version (if compiling from source):0.19
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.0  7
- GPU model and memory: V100 32G

You can collect some of this information using our environment capture

NGC images 1904

**Describe the current behavior**
TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float16 of argument 'x'.
**Describe the expected behavior**

**Code to reproduce the issue**

def _train(total_loss, global_step):
  """"""Train CIFAR-10 model.

  Create an optimizer and apply to all trainable variables. Add moving
  average for all trainable variables.

  Args:
    total_loss: Total loss from loss().
    global_step: Integer Variable counting the number of training steps
      processed.
  Returns:
    train_op: op for training.
  """"""
  # Variables that affect learning rate.
  num_batches_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / FLAGS.batch_size
  decay_steps = int(num_batches_per_epoch * NUM_EPOCHS_PER_DECAY)

  # Decay the learning rate exponentially based on the number of steps.
  lr = tf.train.exponential_decay(INITIAL_LEARNING_RATE,
                                  global_step,
                                  decay_steps,
                                  LEARNING_RATE_DECAY_FACTOR,
                                  staircase=True)
  tf.summary.scalar('learning_rate', lr)

  # Generate moving averages of all losses and associated summaries.
  #loss_averages_op = _add_loss_summaries(total_loss)
  loss_averages_op = total_loss

  # Compute gradients.
  with tf.control_dependencies([loss_averages_op]):
    opt = tf.train.GradientDescentOptimizer(lr)
    opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)
    grads = opt.compute_gradients(total_loss)

  # Apply gradients.
  apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)
  #apply_gradient_op=opt.minimize(total_loss)
  #apply_gradient_op = tf.train.GradientDescentOptimizer(lr).minimize(total_loss,global_step=global_step)

  # Add histograms for trainable variables.
  for var in tf.trainable_variables():
    tf.summary.histogram(var.op.name, var)

  # Add histograms for gradients.
  #for grad, var in grads:
  #  if grad is not None:
  #    tf.summary.histogram(var.op.name + '/gradients', grad)

  # Track the moving averages of all trainable variables.
  variable_averages = tf.train.ExponentialMovingAverage(
      MOVING_AVERAGE_DECAY, global_step)
  with tf.control_dependencies([apply_gradient_op]):
    variables_averages_op = variable_averages.apply(tf.trainable_variables())

  return variables_averages_op
**Other info / logs**

Traceback (most recent call last):
  File ""cifar_for_common_cnn.py"", line 566, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""cifar_for_common_cnn.py"", line 562, in main
    train()
  File ""cifar_for_common_cnn.py"", line 495, in train
    train_op = _train(loss, global_step)
  File ""cifar_for_common_cnn.py"", line 421, in _train
    grads = opt.compute_gradients(total_loss)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/experimental/loss_scale_optimizer.py"", line 116, in compute_gradients
    loss = self._scale_loss(loss)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/experimental/loss_scale_optimizer.py"", line 134, in _scale_loss
    return loss * loss_scale
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py"", line 897, in binary_op_wrapper
    return func(x, y, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py"", line 1180, in _mul_dispatch
    return gen_math_ops.mul(x, y, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 6490, in mul
    ""Mul"", x=x, y=y, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 563, in _apply_op_helper
    inferred_from[input_arg.type_attr]))
TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float16 of argument 'x'.
"
33076,Cannot understand class_weight,"**Describe the current behavior**
Hi, how actually does class_weight in model.fit works? Does it multiple to loss function? I cannot find it in source code. If I set class_weight = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 1, 9:0}. The loss is not equal 0.
"
33075,"tf.keras works, tf.python.keras doesn't","**DOESN'T WORK**:
```python
from tensorflow.python.keras.layers import Input, Dense
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.optimizers import Nadam

ipt = Input(shape=(4,))
out = Dense(1, activation='sigmoid')(ipt)

model = Model(ipt, out)
model.compile(optimizer=Nadam(lr=1e-4), loss='binary_crossentropy')

X = np.random.randn(32,4)
Y = np.random.randint(0,2,(32,1))
model.train_on_batch(X,Y)
```
**WORKS**: remove `.python` from above's imports. Above's error trace below.

Keras 2.3.0 and TensorFlow 2.0.0 freshly-installed via Anaconda, older versions uninstalled. Why the difference?

<hr>

```python

  File ""<ipython-input-7-1e86d21d8fc4>"", line 13, in <module>
    model.train_on_batch(X,Y)

  File ""D:\Anaconda\envs\tf2_env\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 1017, in train_on_batch
    self._make_train_function()

  File ""D:\Anaconda\envs\tf2_env\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 2116, in _make_train_function
    params=self._collected_trainable_weights, loss=self.total_loss)

  File ""D:\Anaconda\envs\tf2_env\lib\site-packages\tensorflow_core\python\keras\optimizers.py"", line 653, in get_updates
    grads = self.get_gradients(loss, params)

  File ""D:\Anaconda\envs\tf2_env\lib\site-packages\tensorflow_core\python\keras\optimizers.py"", line 92, in get_gradients
    if None in grads:

  File ""D:\Anaconda\envs\tf2_env\lib\site-packages\tensorflow_core\python\ops\math_ops.py"", line 1336, in tensor_equals
    return gen_math_ops.equal(self, other)

  File ""D:\Anaconda\envs\tf2_env\lib\site-packages\tensorflow_core\python\ops\gen_math_ops.py"", line 3627, in equal
    name=name)

  File ""D:\Anaconda\envs\tf2_env\lib\site-packages\tensorflow_core\python\framework\op_def_library.py"", line 545, in _apply_op_helper
    (input_name, err))

ValueError: Tried to convert 'y' to a tensor and failed. Error: None values not supported.
```
<hr>

**UPDATE:** Debugging the two side-by-side, while both use the [same files](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training.py#L1008), execution diverges fairly quickly:

```python
# .\tensorflow_core\python\keras\engine\training.py

### TF.KERAS
    if self._experimental_run_tf_function: #  TRUE
	
### TF.PYTHON.KERAS
    if self._experimental_run_tf_function: #  FALSE
```
Former proceeds to call `training_v2_utils.train_on_batch(...)` and returns thereafter, latter `self._standardize_user_data(...)` and others before ultimately failing. 

One difference I noted between linked file and mine is, latter's short by ~100 lines - though I installed TF 2 via pip after the file's last update 12 days ago according to Github."
33074,tf.keras accepts incorrect CNN input shapes from tf.data.Dataset when eager execution is disabled,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Ubuntu 16.04
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.14.0
- Python version: 3.7
- Device: CPU

**Describe the current behavior**
I am using a `tf.data.Dataset` to train a `tf.keras` model. My CNN input layer size is (72, 96, 1). I find that if I input a tensor with size (96, 72, 1) (note the reversed dimensions), training completes successfully. However, if I enable eager execution, a `ValueError` is produced as expected.

**Describe the expected behavior**
A `ValueError` should be produced in response to the incorrect input shape, whether or not eager execution is enabled.

**Code to reproduce the issue**
NOTE: With the code below, there is no error message. However, uncomment the `tf.enable_eager_execution()` line and you will get a `ValueError`.

```
#!/usr/bin/env python3
  
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers

# UNCOMMENT THIS LINE AND YOU WILL GET AN ERROR
# tf.enable_eager_execution()

def tf_dataset():

    def data_gen():
        while True:
            yield np.random.rand(72, 96, 1), np.random.rand(18)

    types = (tf.float64, tf.float64)
    shapes = (tf.TensorShape([None, None, None]), tf.TensorShape([None]))
    dataset = tf.data.Dataset.from_generator(data_gen, types, output_shapes=shapes)
    dataset = dataset.batch(16)
    return dataset

dataset = tf_dataset()

model = tf.keras.Sequential()
# Notice how the input dimensions are mismatched
model.add(layers.InputLayer((96, 72, 1)))
model.add(layers.Conv2D(filters=32, kernel_size=4, strides=4, activation='relu'))
model.add(layers.Conv2D(filters=32, kernel_size=4, strides=4, activation='relu'))
model.add(layers.Flatten())
model.add(layers.Dense(18, activation='softmax'))
model.compile(optimizer='adam', loss='categorical_crossentropy')
print(model.summary())
model.fit(dataset, steps_per_epoch=1, epochs=1)
```"
33072,Distributed training Keras ConvLSTM2D layer with stateful=True failing,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1.0-dev20191004
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0.130/7.6
- GPU model and memory: 2x GeForce GTX 1080 Ti, 10481 MB

**Describe the current behavior**
Training Keras model with ConvLSTM2D stateful layer fails to train under distributed scope.
If I set stateful to False it distributed learning works.
Error message:

```
Traceback (most recent call last):
  File ""D:\work\aicomp\challenges\fatigue\ws\stateful_mirrored\src\test.py"", line 17, in <module>
    model.fit(dataset, steps_per_epoch=1, epochs=1)
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 766, in fit
    use_multiprocessing=use_multiprocessing)
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 333, in fit
    total_epochs=epochs)
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 123, in run_one_epoch
    batch_outs = execution_function(iterator)
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py"", line 86, in execution_function
    distributed_function(input_fn))
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\eager\def_function.py"", line 554, in __call__
    result = self._call(*args, **kwds)
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\eager\def_function.py"", line 600, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\eager\def_function.py"", line 493, in _initialize
    *args, **kwds))
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\eager\function.py"", line 2320, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\eager\function.py"", line 2628, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\eager\function.py"", line 2517, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\framework\func_graph.py"", line 943, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\eager\def_function.py"", line 435, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py"", line 73, in distributed_function
    per_replica_function, args=(x, y, sample_weights))
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\distribute\distribute_lib.py"", line 764, in experimental_run_v2
    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\distribute\distribute_lib.py"", line 1820, in call_for_each_replica
    return self._call_for_each_replica(fn, args, kwargs)
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\distribute\mirrored_strategy.py"", line 688, in _call_for_each_replica
    fn, args, kwargs)
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\distribute\mirrored_strategy.py"", line 200, in _call_for_each_replica
    coord.join(threads)
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\training\coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\six.py"", line 693, in reraise
    raise value
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\training\coordinator.py"", line 297, in stop_on_exception
    yield
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\distribute\mirrored_strategy.py"", line 909, in run
    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\autograph\impl\api.py"", line 292, in wrapper
    return func(*args, **kwargs)
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py"", line 264, in train_on_batch
    output_loss_metrics=model._output_loss_metrics)
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\keras\engine\training_eager.py"", line 311, in train_on_batch
    output_loss_metrics=output_loss_metrics))
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\keras\engine\training_eager.py"", line 252, in _process_single_batch
    training=training))
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\keras\engine\training_eager.py"", line 127, in _model_loss
    outs = model(inputs, **kwargs)
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py"", line 759, in __call__
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\keras\engine\network.py"", line 712, in call
    convert_kwargs_to_constants=base_layer_utils.call_context().saving)
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\keras\engine\network.py"", line 868, in _run_internal_graph
    output_tensors = layer(computed_tensors, **kwargs)
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\keras\layers\convolutional_recurrent.py"", line 299, in __call__
    return super(ConvRNN2D, self).__call__(inputs, **kwargs)
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\keras\layers\recurrent.py"", line 631, in __call__
    return super(RNN, self).__call__(inputs, **kwargs)
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py"", line 759, in __call__
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\keras\layers\convolutional_recurrent.py"", line 938, in call
    initial_state=initial_state)
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\keras\layers\convolutional_recurrent.py"", line 397, in call
    updates.append(K.update(self.states[i], states[i]))
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\keras\backend.py"", line 1557, in update
    return state_ops.assign(x, new_x)
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\ops\state_ops.py"", line 228, in assign
    return ref.assign(value, name=name)
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\distribute\values.py"", line 1040, in assign
    return self._assign_func(f=assign_fn, *args, **kwargs)
  File ""D:\work\aicomp\tf2_nightly_venv\lib\site-packages\tensorflow_core\python\distribute\values.py"", line 1020, in _assign_func
    variable_type=""MirroredVariable""))
ValueError: You must specify an aggregation method to update a MirroredVariable in Replica Context. You can do so by passing an explicit value for argument `aggregation` to tf.Variable(..).e.g. `tf.Variable(..., aggregation=tf.VariableAggregation.SUM)``tf.VariableAggregation` lists the possible aggregation methods.This is required because MirroredVariable should always be kept in sync. When updating them or assigning to them in a replica context, we automatically try to aggregate the values before updating the variable. For this aggregation, we need to know the aggregation method. Another alternative is to not try to update such MirroredVariable in replica context, but in cross replica context. You can enter cross replica context by calling `tf.distribute.get_replica_context().merge_call(merge_fn, ..)`.Inside `merge_fn`, you can then update the MirroredVariable using `tf.distribute.StrategyExtended.update()`.
```

**Describe the expected behavior**
Train model distributed without error.

**Code to reproduce the issue**

```
import tensorflow as tf

from tensorflow.keras.layers import Input, ConvLSTM2D
from tensorflow.keras.models import Model

mirrored_strategy = tf.distribute.MirroredStrategy()

with mirrored_strategy.scope():
    model_input = Input(shape=(None, 3, 3, 1), batch_size=1)
    model = ConvLSTM2D(1, kernel_size=3, stateful=True)(model_input)    
    model = Model(inputs=[model_input], outputs=[model])
    model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=tf.keras.optimizers.Adadelta())    
    input_dataset = tf.data.Dataset.from_tensors(tf.random.uniform([1, 3, 3, 1])).batch(1)
    output_dataset = tf.data.Dataset.from_tensors(tf.random.uniform([1, 3, 3, 1])).batch(1)
    dataset = tf.data.Dataset.zip((input_dataset, output_dataset))
    model.fit(dataset, steps_per_epoch=1, epochs=1)
```
"
33071,Explicitly Deleted Function Results in Failed Build with Master Branch on Windows,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution : Windows 10
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 2.0 Nightly
- Python version: 3.7
- Bazel version (if compiling from source): 0.25.2
- GCC/Compiler version (if compiling from source): VS2017
- CUDA/cuDNN version: 10.0

When attempting to build TF 2.0 from the master branch off Github, the build fails with the following error.

```
ERROR: D:/sdks/tensorflow/tensorflow/core/profiler/internal/gpu/BUILD:98:1: C++ compilation of rule '//tensorflow/core/profiler/internal/gpu:cupti_tracer' failed (Exit 2): python.exe failed: error executing command
  cd C:/users/adam/_bazel_adam/fcwavxxi/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\VC\Tools\MSVC\14.13.26128\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\VC\Tools\MSVC\14.13.26128\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\cppwinrt
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\VC\Tools\MSVC\14.13.26128\ATLMFC\lib\x64;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\VC\Tools\MSVC\14.13.26128\lib\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\VC\Tools\MSVC\14.13.26128\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\MSBuild\15.0\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\\x64;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\;C:\Program Files (x86)\Windows Kits\10\bin\10.0.18362.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\\MSBuild\15.0\bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Common7\Tools\;;C:\WINDOWS\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/Adam/Anaconda3/python.exe
    SET PYTHON_LIB_PATH=C:/Users/Adam/Anaconda3/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\Adam\AppData\Local\Temp
    SET TF_CONFIGURE_IOS=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=5.0
    SET TF_NEED_CUDA=1
    SET TMP=C:\Users\Adam\AppData\Local\Temp
  C:/Users/Adam/Anaconda3/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/libjpeg_turbo /Ibazel-out/x64_windows-opt/bin/external/libjpeg_turbo /Iexternal/com_google_protobuf /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_tensorrt /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cupti_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers /Iexternal/local_config_cuda/cuda/cuda/extras/CUPTI/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/extras/CUPTI/include /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/gif/windows /Ibazel-out/x64_windows-opt/bin/external/gif/windows /Iexternal/com_google_protobuf/src /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG -w -DWIN32_LEAN_AND_MEAN -DNOGDI /arch:AVX -std=c++14 -DGOOGLE_CUDA=1 -DTENSORFLOW_MONOLITHIC_BUILD /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COMPILE_LIBRARY -DGOOGLE_CUDA=1 /Fobazel-out/x64_windows-opt/bin/tensorflow/core/profiler/internal/gpu/_objs/cupti_tracer/cupti_tracer.o /c tensorflow/core/profiler/internal/gpu/cupti_tracer.cc
Execution platform: @bazel_tools//platforms:host_platform
cl : Command line warning D9002 : ignoring unknown option '-std=c++14'
external/com_google_absl\absl/meta/type_traits.h(141): error C2280: 'void tensorflow::profiler::AnnotationMap::operator =(const tensorflow::profiler::AnnotationMap &)': attempting to reference a deleted function
.\tensorflow/core/profiler/internal/gpu/cupti_tracer.h(198): note: see declaration of 'tensorflow::profiler::AnnotationMap::operator ='
external/com_google_absl\absl/meta/type_traits.h(121): note: see reference to alias template instantiation 'IsCopyAssignableImpl<tensorflow::profiler::AnnotationMap>' being compiled
external/com_google_absl\absl/meta/type_traits.h(150): note: see reference to class template instantiation 'absl::type_traits_internal::is_detected<absl::type_traits_internal::IsCopyAssignableImpl,T>' being compiled
        with
        [
            T=tensorflow::profiler::AnnotationMap
        ]
external/com_google_absl\absl/meta/type_traits.h(432): note: see reference to class template instantiation 'absl::is_copy_assignable<T>' being compiled
        with
        [
            T=tensorflow::profiler::AnnotationMap
        ]
external/com_google_absl\absl/types/internal/optional.h(173): note: see reference to class template instantiation 'absl::is_trivially_copy_assignable<tensorflow::profiler::AnnotationMap>' being compiled
.\tensorflow/core/profiler/internal/gpu/cupti_tracer.h(262): note: see reference to class template instantiation 'absl::optional<tensorflow::profiler::AnnotationMap>' being compiled
.\tensorflow/core/profiler/internal/gpu/cupti_tracer.h(198): note: 'void tensorflow::profiler::AnnotationMap::operator =(const tensorflow::profiler::AnnotationMap &)': function was explicitly deleted
external/com_google_absl\absl/meta/type_traits.h(121): error C2248: 'tensorflow::profiler::AnnotationMap::operator =': cannot access private member declared in class 'tensorflow::profiler::AnnotationMap'
.\tensorflow/core/profiler/internal/gpu/cupti_tracer.h(198): note: see declaration of 'tensorflow::profiler::AnnotationMap::operator ='
.\tensorflow/core/profiler/internal/gpu/cupti_tracer.h(177): note: see declaration of 'tensorflow::profiler::AnnotationMap'
external/com_google_absl\absl/meta/type_traits.h(144): error C2280: 'void tensorflow::profiler::AnnotationMap::operator =(const tensorflow::profiler::AnnotationMap &)': attempting to reference a deleted function
.\tensorflow/core/profiler/internal/gpu/cupti_tracer.h(198): note: see declaration of 'tensorflow::profiler::AnnotationMap::operator ='
external/com_google_absl\absl/meta/type_traits.h(121): note: see reference to alias template instantiation 'IsMoveAssignableImpl<tensorflow::profiler::AnnotationMap>' being compiled
external/com_google_absl\absl/meta/type_traits.h(155): note: see reference to class template instantiation 'absl::type_traits_internal::is_detected<absl::type_traits_internal::IsMoveAssignableImpl,T>' being compiled
        with
        [
            T=tensorflow::profiler::AnnotationMap
        ]
external/com_google_absl\absl/types/internal/optional.h(328): note: see reference to class template instantiation 'absl::is_move_assignable<T>' being compiled
        with
        [
            T=tensorflow::profiler::AnnotationMap
        ]
external/com_google_absl\absl/types/optional.h(120): note: see reference to class template instantiation 'absl::optional_internal::assign_copy_traits<T>' being compiled
        with
        [
            T=tensorflow::profiler::AnnotationMap
        ]
.\tensorflow/core/profiler/internal/gpu/cupti_tracer.h(198): note: 'void tensorflow::profiler::AnnotationMap::operator =(const tensorflow::profiler::AnnotationMap &)': function was explicitly deleted
```"
33069,TF2.0.0: tensorflow.python.framework.errors_impl.UnknownError: CUDNN_STATUS_BAD_PARAM,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  **yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows Server 2019**
- TensorFlow installed from (source or binary): **pip install tensorflow-gpu**
- TensorFlow version (use command below): **2.0.0**
- Python version:**3.6.9**
- CUDA/cuDNN version:**10.1/7.6.4**
- GPU model and memory: **GTX1080Ti  x4**

**Describe the current behavior**

When I succeeded in debugging on my own host, I occured an error on windows server 2019 for debugging.The error message is below:

```cmd
2019-10-05 21:54:41.858023: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at cudnn_rnn_ops.cc:1498 : Unknown: CUDNN_STATUS_BAD_PARAM
in tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)'
Traceback (most recent call last):
  File ""E:/PythonProjects/Sequence-Labeling/bi-lstm_tf2.py"", line 139, in <module>
    loss_value, grads = gradient_descent(model, train_x, train_y)
  File ""E:/PythonProjects/Sequence-Labeling/bi-lstm_tf2.py"", line 50, in gradient_descent
    y_predict = model(inputs)
  File ""D:\ProgramData\Anaconda3\envs\py36-tf2\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py"", line 891, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""D:\ProgramData\Anaconda3\envs\py36-tf2\lib\site-packages\tensorflow_core\python\keras\engine\sequential.py"", line 256, in call
    return super(Sequential, self).call(inputs, training=training, mask=mask)
  File ""D:\ProgramData\Anaconda3\envs\py36-tf2\lib\site-packages\tensorflow_core\python\keras\engine\network.py"", line 708, in call
    convert_kwargs_to_constants=base_layer_utils.call_context().saving)
  File ""D:\ProgramData\Anaconda3\envs\py36-tf2\lib\site-packages\tensorflow_core\python\keras\engine\network.py"", line 860, in _run_internal_graph
    output_tensors = layer(computed_tensors, **kwargs)
  File ""D:\ProgramData\Anaconda3\envs\py36-tf2\lib\site-packages\tensorflow_core\python\keras\layers\wrappers.py"", line 528, in __call__
    return super(Bidirectional, self).__call__(inputs, **kwargs)
  File ""D:\ProgramData\Anaconda3\envs\py36-tf2\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py"", line 891, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""D:\ProgramData\Anaconda3\envs\py36-tf2\lib\site-packages\tensorflow_core\python\keras\layers\wrappers.py"", line 642, in call
    initial_state=forward_state, **kwargs)
  File ""D:\ProgramData\Anaconda3\envs\py36-tf2\lib\site-packages\tensorflow_core\python\keras\layers\recurrent.py"", line 623, in __call__
    return super(RNN, self).__call__(inputs, **kwargs)
  File ""D:\ProgramData\Anaconda3\envs\py36-tf2\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py"", line 891, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""D:\ProgramData\Anaconda3\envs\py36-tf2\lib\site-packages\tensorflow_core\python\keras\layers\recurrent_v2.py"", line 961, in call
    **cudnn_lstm_kwargs)
  File ""D:\ProgramData\Anaconda3\envs\py36-tf2\lib\site-packages\tensorflow_core\python\keras\layers\recurrent_v2.py"", line 1160, in cudnn_lstm
    rnn_mode='lstm', sequence_lengths=sequence_length)
  File ""D:\ProgramData\Anaconda3\envs\py36-tf2\lib\site-packages\tensorflow_core\python\ops\gen_cudnn_rnn_ops.py"", line 2008, in cudnn_rnnv3
    ctx=_ctx)
  File ""D:\ProgramData\Anaconda3\envs\py36-tf2\lib\site-packages\tensorflow_core\python\ops\gen_cudnn_rnn_ops.py"", line 2114, in cudnn_rnnv3_eager_fallback
    attrs=_attrs, ctx=_ctx, name=name)
  File ""D:\ProgramData\Anaconda3\envs\py36-tf2\lib\site-packages\tensorflow_core\python\eager\execute.py"", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
**tensorflow.python.framework.errors_impl.UnknownError: CUDNN_STATUS_BAD_PARAM**
**in tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)' [Op:CudnnRNNV3]**
```

**Describe the expected behavior**

Code works fine

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
	# Currently, memory growth needs to be the same across GPUs
	try:
		for gpu in gpus:
			tf.config.experimental.set_memory_growth(gpu, True)
	except RuntimeError as e:
		print(e)

def gradient_descent(model, inputs, targets):
	with tf.GradientTape() as tape:
		# compute loss value
		y_predict = model(inputs)
		loss_value = tf.keras.losses.categorical_crossentropy(y_true = targets,
		                                                      y_pred = y_predict)
	return loss_value, tape.gradient(loss, model.trainable_variables)

with tf.device('/device:gpu:0'):
    model = tf.keras.Sequential([
		    tf.keras.Input(shape = (sequence_len, num_feature),
		                   name = 'InputLayer'),
		    tf.keras.layers.Masking(mask_value = 0.,
		                            input_shape = (sequence_len, num_feature)),
		    tf.keras.layers.Bidirectional(
			    tf.keras.layers.LSTM(units = 50, return_sequences = True),
			    name = 'BiLSTM-1'),
		    tf.keras.layers.Dense(units = 3, activation = 'softmax',
		                          name = 'Softmax')])

    train_x, train_y = train_dataset.next_batch()
    loss_value, grads = gradient_descent(model, train_x, train_y)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
33067,tf.gather can't perform real values but op while using sess.run(),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Custom Code 
- OS Platform and Distribution : Linux Ubuntu 16.04
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):tensorflow-gpu 1.12.0
- Python version:3.6
- CUDA/cuDNN version: CUDA 9.0

**Describe the current behavior**
I 'm a beginner of TF.In my code ,I have a `myinput['point']`  matrix(shape:500x3  500 for the number of points,3 for 3-D) which stores the whole point set. I also have a `myinput['neigh']`  matirx(500x10  10 indicates each point have 10 neighbors ) which stores the **idx** of each point's neighs. When I want to get the neighs of each point , I use `tf.gather(myinput['point'],myinput['neigh'])`  .However, when I want to get the neighs of neighs,
I use the code as follow:
```
neigh_neigh_idx=tf.gather(myinput['neigh'],myinput['neigh'])
neigh_neigh=tf.gather(myinput['point'],neigh_neigh_idx)
with tf.Session() as sess:
    n_n_idx=sess.run(neigh_neigh_idx)
```
after evaluate this code, I only get `neigh_neigh_idx` and  `neigh_neigh` in console and I can't find any information but this about n_n_idx.I wander how to get ther real value of  `n_n_idx` instead of some ops.
```
[[node PointNetwork/layer_2/conv2/GatherV2_22 (defined at <string>:1)  = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](PointNetwork/layer_2/conv2/GatherV2_22/params, PointNetwork/layer_2/conv2/GatherV2_22/params, PointNetwork/layer_2/conv2/GatherV2_22/axis)]]
```
THX~

"
33062,Jupyter notebook from `tensorflow/tensorflow:nightly-py3-jupyter` won't authenticate its own token,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
- TensorFlow installed from (source or binary): ` docker pull tensorflow/tensorflow:nightly-py3-jupyter`
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:n/a
- GPU model and memory: n/a

* Tensorflow 1.14 installed on system via `conda install tensorflow`; using docker container because needed a higher version without disturbing the system installation*

**Describe the current behavior**
![Screenshot from 2019-10-05 08-08-27](https://user-images.githubusercontent.com/31768189/66248857-65ea6600-e749-11e9-8d75-22dca6cf6129.png)

The jupyter notebook that comes bundled with `tensorflow/tensorflow:nightly-py3-jupyter` docker image won't authenticate its own token

**Describe the expected behavior**

Jupyter notebook should authenticate its own token.

**Code to reproduce the issue**

``` bash
$ docker pull tensorflow/tensorflow:nightly-py3-jupyter
$ docker run -u $(id -u):$(id -u) -it -p 10001:10001 tensorflow/tensorflow:nightly-py3-jupyter
```

**Other info / logs**

```
________                               _______________                
___  __/__________________________________  ____/__  /________      __
__  /  _  _ \_  __ \_  ___/  __ \_  ___/_  /_   __  /_  __ \_ | /| / /
_  /   /  __/  / / /(__  )/ /_/ /  /   _  __/   _  / / /_/ /_ |/ |/ / 
/_/    \___//_/ /_//____/ \____//_/    /_/      /_/  \____/____/|__/


You are running this container as user with ID 1000 and group 1000,
which should map to the ID and group for your user on the Docker host. Great!

[I 02:35:18.648 NotebookApp] Writing notebook server cookie secret to /.local/share/jupyter/runtime/notebook_cookie_secret
/usr/local/lib/python3.6/dist-packages/IPython/paths.py:68: UserWarning: IPython parent '/' is not a writable location, using a temp directory.
  "" using a temp directory."".format(parent))
[I 02:35:18.817 NotebookApp] Serving notebooks from local directory: /tf
[I 02:35:18.818 NotebookApp] The Jupyter Notebook is running at:
[I 02:35:18.818 NotebookApp] http://82fe909436a2:8888/?token=508ff9f2b673ebcc134e67f6c21f539659cc74b659fceeb1
[I 02:35:18.818 NotebookApp]  or http://127.0.0.1:8888/?token=508ff9f2b673ebcc134e67f6c21f539659cc74b659fceeb1
[I 02:35:18.818 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 02:35:18.822 NotebookApp] 
    
    To access the notebook, open this file in a browser:
        file:///.local/share/jupyter/runtime/nbserver-1-open.html
    Or copy and paste one of these URLs:
        http://82fe909436a2:8888/?token=508ff9f2b673ebcc134e67f6c21f539659cc74b659fceeb1
     or http://127.0.0.1:8888/?token=508ff9f2b673ebcc134e67f6c21f539659cc74b659fceeb1
```
"
33061,image_gradients: 'Tensor' object has no attribute 'get_shape',"Hi all,

While I'm trying to feed my tensor to `image_gradients`, this error arises:

`AttributeError: 'Tensor' object has no attribute 'get_shape'`

The type of this tensor is: `torch.cuda.floattensor`

And its size: `torch.Size([2, 1, 128, 128])`

Any help, please."
33060,Possible corruption in Load or freeze in TF2.0,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary 2.0.0
- TensorFlow version (use command below): 2.0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 7.6.4
- GPU model and memory: Volta/Turing

**Describe the current behavior**
When I run load and freeze in two different python functions, I get a crash that says: 

```AssertionError: Called a function referencing variables which have been deleted. This likely means that function-local variables were created and not referenced elsewhere in the program. This is generally a mistake; consider storing variables in an object attribute on first call.```

But when I run both of load and freeze in the same python function, then it works as expected.

**Describe the expected behavior**
Calling load and freeze in different python functions should work unless there is some hidden assumption in TF 2.0. My guess is that there is a leak or some dependency between the two APIs.

**Code to reproduce the issue**
```
import argparse
import numpy as np
import tensorflow as tf
from tensorflow.python.saved_model import signature_constants
from tensorflow.python.saved_model import tag_constants
from tensorflow.python.framework import convert_to_constants

def get_dataset(batch_size, input_size):
  features = np.random.normal(
      loc=112, scale=70,
      size=(batch_size, input_size, input_size, 3)).astype(np.float32)
  features = np.clip(features, 0.0, 255.0)
  features = tf.convert_to_tensor(value=tf.compat.v1.get_variable(
      ""features"", dtype=tf.float32, initializer=tf.constant(features)))
  dataset = tf.data.Dataset.from_tensor_slices([features])
  dataset = dataset.repeat()
  return dataset


def run_func(saved_model_dir):

  def load_model():
    saved_model_loaded = tf.saved_model.load(
        saved_model_dir, tags=[tag_constants.SERVING])
    graph_func = saved_model_loaded.signatures[
        signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY]
    return graph_func
  graph_func = load_model()

# Replace load_model function and its call with the following to make it work
#  saved_model_loaded = tf.saved_model.load(
#      saved_model_dir, tags=[tag_constants.SERVING])
#  graph_func = saved_model_loaded.signatures[
#      signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY]

  frozen_func = convert_to_constants.convert_variables_to_constants_v2(graph_func)
  def wrap_func(*args, **kwargs):
    return frozen_func(*args, **kwargs)[0]
  inference_graph_func = wrap_func
  dataset = get_dataset(batch_size=8, input_size=224)
  for i, (batch_feats) in enumerate(dataset):
    batch_preds = inference_graph_func(batch_feats).numpy()
    print(batch_preds)


if __name__ == '__main__':
  parser = argparse.ArgumentParser(description='Evaluate model')
  parser.add_argument('--saved_model_dir', type=str, default=None,
                      help='Directory containing a particular saved model.')
  args = parser.parse_args()

  run_func(saved_model_dir=args.saved_model_dir)
```

**Other info / logs**
Command line to run: `python tfv2_load_issue.py --saved_model_dir path_to_saved_model`

```
2019-10-04 20:35:48.913525: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:734] Optimization results for grappler item: graph_to_optimize
2019-10-04 20:35:48.913553: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:736]   function_optimizer: Graph size after: 1314 nodes (1044), 2670 edges (2400), time = 23.171ms.
2019-10-04 20:35:48.913557: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:736]   function_optimizer: function_optimizer did nothing. time = 0.371ms.
Traceback (most recent call last):
  File ""run.py"", line 51, in <module>
    run_func(saved_model_dir=args.saved_model_dir)
  File ""run.py"", line 35, in run_func
    frozen_func = convert_to_constants.convert_variables_to_constants_v2(graph_func)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/convert_to_constants.py"", line 411, in convert_variables_to_constants_v2
    tensor_data = _get_tensor_data(func)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/convert_to_constants.py"", line 182, in _get_tensor_data
    for var in func.graph.variables:
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py"", line 435, in variables
    ""Called a function referencing variables which have been deleted. ""
AssertionError: Called a function referencing variables which have been deleted. This likely means that function-local variables were created and not referenced elsewhere in the program. This is generally a mistake; consider storing variables in an object attribute on first call.
```
"
33059,Size and CombinedNMS Op support request for tflite,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): binary (pip)
- TensorFlow version (or github SHA if from source): 2.0.0


**Provide the text output from tflite_convert**

```
2019-10-04 16:17:28.794407: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2019-10-04 16:17:28.798220: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2019-10-04 16:17:28.798242: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: WD-AI-lab
2019-10-04 16:17:28.798246: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: WD-AI-lab
2019-10-04 16:17:28.798282: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
2019-10-04 16:17:28.798299: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 396.51.0
2019-10-04 16:17:28.798303: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 396.51.0 does not match DSO version 430.26.0 -- cannot find working devices in this configuration
2019-10-04 16:17:28.798415: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-10-04 16:17:28.821484: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 4008000000 Hz
2019-10-04 16:17:28.822257: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5563ee4031d0 executing computations on platform Host. Devices:
2019-10-04 16:17:28.822268: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2019-10-04 16:17:32.842974: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2019-10-04 16:17:32.843029: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2019-10-04 16:17:32.858437: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize
2019-10-04 16:17:32.858454: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.003ms.
2019-10-04 16:17:32.858458: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0ms.
2019-10-04 16:17:35.240170: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2019-10-04 16:17:35.240264: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2019-10-04 16:17:35.995629: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize
2019-10-04 16:17:35.995652: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 1001 nodes (-358), 2553 edges (-358), time = 346.923ms.
2019-10-04 16:17:35.995656: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 1001 nodes (0), 2553 edges (0), time = 114.777ms.
Traceback (most recent call last):
  File ""/home/wd_ai/git-pkgs/yolov3-tf2/convert_tflite.py"", line 29, in <module>
    app.run(main)
  File ""/home/wd_ai/miniconda3/envs/yolov3-tf2-gpu/lib/python3.7/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/home/wd_ai/miniconda3/envs/yolov3-tf2-gpu/lib/python3.7/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/home/wd_ai/git-pkgs/yolov3-tf2/convert_tflite.py"", line 24, in main
    tflite_model = converter.convert()
  File ""/home/wd_ai/miniconda3/envs/yolov3-tf2-gpu/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py"", line 446, in convert
    **converter_kwargs)
  File ""/home/wd_ai/miniconda3/envs/yolov3-tf2-gpu/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py"", line 449, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File ""/home/wd_ai/miniconda3/envs/yolov3-tf2-gpu/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py"", line 200, in toco_convert_protos
    raise ConverterError(""See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: See console for info.
2019-10-04 16:17:37.744297: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size
2019-10-04 16:17:37.744348: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size
2019-10-04 16:17:37.744519: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size
2019-10-04 16:17:37.744542: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size
2019-10-04 16:17:37.744676: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size
2019-10-04 16:17:37.744683: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size
2019-10-04 16:17:37.744759: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: CombinedNonMaxSuppression
2019-10-04 16:17:37.757406: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 743 operators, 1376 arrays (0 quantized)
2019-10-04 16:17:37.767372: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 743 operators, 1376 arrays (0 quantized)
2019-10-04 16:17:38.148616: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 284 operators, 533 arrays (0 quantized)
2019-10-04 16:17:38.152237: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 284 operators, 533 arrays (0 quantized)
2019-10-04 16:17:38.155833: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 284 operators, 533 arrays (0 quantized)
2019-10-04 16:17:38.158553: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 284 operators, 533 arrays (0 quantized)
2019-10-04 16:17:38.164271: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 66560128 bytes, theoretical optimal value: 44408960 bytes.
2019-10-04 16:17:38.165392: E tensorflow/lite/toco/toco_tooling.cc:466] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DIV, EXP, EXPAND_DIMS, FILL, LEAKY_RELU, LOGISTIC, MUL, PACK, PAD, RESHAPE, RESIZE_NEAREST_NEIGHBOR, SHAPE, SPLIT_V, STRIDED_SLICE, SUB. Here is a list of operators for which you will need custom implementations: CombinedNonMaxSuppression, Size.
Traceback (most recent call last):
  File ""/home/wd_ai/miniconda3/envs/yolov3-tf2-gpu/bin/toco_from_protos"", line 10, in <module>
    sys.exit(main())
  File ""/home/wd_ai/miniconda3/envs/yolov3-tf2-gpu/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 89, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/home/wd_ai/miniconda3/envs/yolov3-tf2-gpu/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/wd_ai/miniconda3/envs/yolov3-tf2-gpu/lib/python3.7/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/home/wd_ai/miniconda3/envs/yolov3-tf2-gpu/lib/python3.7/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/home/wd_ai/miniconda3/envs/yolov3-tf2-gpu/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 52, in execute
    enable_mlir_converter)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DIV, EXP, EXPAND_DIMS, FILL, LEAKY_RELU, LOGISTIC, MUL, PACK, PAD, RESHAPE, RESIZE_NEAREST_NEIGHBOR, SHAPE, SPLIT_V, STRIDED_SLICE, SUB. Here is a list of operators for which you will need custom implementations: CombinedNonMaxSuppression, Size.

```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
33057,No stat cache for s3 file system,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.14
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

In gcs file system there's a cache for object `stat` where you don't have to repeatedly send `stat` call to remote server to get stat of the same object. This is a nice performance gain but not available in s3 file system.

As s3 share the same nature as gcs, the same cache can be applied to s3 file system for the performance gain.

**Will this change the current api? How?**

No.

**Who will benefit with this feature?**

User of s3 file system, whose use pattern send rapid api calls.

**Any Other info.**
"
33054,"Build failed, 2.0 for CPU without avx support. ","tensorflow/compiler/xla/client/lib/prng.cc(438): error C2065: 'M_PI': undeclared identifier                             tensorflow/compiler/xla/client/lib/prng.cc(438): error C2672: 'ScalarLike': no matching overloaded function found       tensorflow/compiler/xla/client/lib/prng.cc(438): error C2780: 'xla::XlaOp xla::ScalarLike(xla::XlaOp,T)': expects 2 arguments - 1 provided
.\tensorflow/compiler/xla/client/lib/constants.h(91): note: see declaration of 'xla::ScalarLike'
Target //tensorflow/tools/pip_package:build_pip_package failed to build                                                 INFO: Elapsed time: 4480.154s, Critical Path: 158.59s                                                                   INFO: 800 processes: 800 local.                                                                                         FAILED: Build did NOT complete successfully                 
"
33052,Keras RNN training speed significantly slower with eager execution/control flow v2,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.0.0
- Python version: 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.0/7.6
- GPU model and memory: GTX 980 Ti

**Describe the current behavior**

Enabling eager execution or control flow v2 causes RNN training speed to decrease significantly.

**Describe the expected behavior**

Enabling eager mode or control flow v2 should not affect the training time (or improve it, ideally).

**Code to reproduce the issue**

``` python
import tensorflow as tf
import numpy as np
import timeit

use_eager = False
use_v2 = False

if not use_eager:
    tf.compat.v1.disable_eager_execution()
if not use_v2:
    tf.compat.v1.disable_control_flow_v2()


n_steps = 1000
n_input = 100
n_hidden = 1000
batch_size = 64

inputs = tf.keras.Input((n_steps, n_input))
outputs = tf.keras.layers.SimpleRNN(units=n_hidden, return_sequences=True)(inputs)
outputs = tf.keras.layers.Dense(units=n_input)(outputs)
model = tf.keras.Model(inputs=inputs, outputs=outputs)

model.compile(optimizer=tf.optimizers.SGD(0.1), loss=""mse"")

x = np.ones((batch_size, n_steps, n_input))
y = np.ones((batch_size, n_steps, n_input))

# warmup
model.fit(x, y, epochs=1)

start = timeit.default_timer()
model.fit(x, y, epochs=10)
print(""Execution time:"", timeit.default_timer() - start)

```

**Other info / logs**
On my machine the results look like:
- use_eager=False, use_v2=False: 5.90s
- use_eager=False, use_v2=True: 8.08s
- use_eager=True, use_v2=False: 9.81s
- use_eager=True, use_v2=True: 10.10s

So, overall a >60% increase in training time comparing no eager and no v2 to the current defaults.
"
33051,Build Tensorflow  from sources  for Android x86_64,"

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubunutu
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
Master, 1.14
- TensorFlow version:
- Python version:
2.7
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
25
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:
NDK 17,18

**Describe the problem**
Hi , I am trying to build tensorflow shared library from sources for our device CPU Appolo Lake with Android OS .
**Provide the exact sequence of commands / steps that you executed before running into the problem**
build command 
bazel build  //tensorflow:tensorflow  --cpu=x86_64 --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=""-std=c++11""

output
From Compiling tensorflow/core/kernels/string_split_op.cc:
tensorflow/core/kernels/string_split_op.cc:129:15: warning: 'RemoveLeadingWhitespace' is deprecated: Use absl::StripLeadingAsciiWhitespace instead. [-Wdeprecated-declarations]
    str_util::RemoveLeadingWhitespace(&text);
              ^
./tensorflow/core/lib/strings/str_util.h:55:1: note: 'RemoveLeadingWhitespace' has been explicitly marked deprecated here
ABSL_DEPRECATED(""Use absl::StripLeadingAsciiWhitespace instead."")
^
external/com_google_absl/absl/base/macros.h:148:49: note: expanded from macro 'ABSL_DEPRECATED'
#define ABSL_DEPRECATED(message) __attribute__((deprecated(message)))
                                                ^
tensorflow/core/kernels/string_split_op.cc:133:17: warning: 'RemoveLeadingWhitespace' is deprecated: Use absl::StripLeadingAsciiWhitespace instead. [-Wdeprecated-declarations]
      str_util::RemoveLeadingWhitespace(&text);
                ^
./tensorflow/core/lib/strings/str_util.h:55:1: note: 'RemoveLeadingWhitespace' has been explicitly marked deprecated here
ABSL_DEPRECATED(""Use absl::StripLeadingAsciiWhitespace instead."")
^
external/com_google_absl/absl/base/macros.h:148:49: note: expanded from macro 'ABSL_DEPRECATED'
#define ABSL_DEPRECATED(message) __attribute__((deprecated(message)))
                                                ^
2 warnings generated.
ERROR: /home/dnozik/.cache/bazel/_bazel_dnozik/0d8e3d5d91fa64e88329b83a7165efc6/external/lmdb/BUILD.bazel:8:1: C++ compilation of rule '@lmdb//:lmdb' failed (Exit 1)
external/lmdb/mdb.c:4859:53: error: use of undeclared identifier 'PTHREAD_MUTEX_ROBUST'
                if (!rc) rc = pthread_mutexattr_setrobust(&mattr, PTHREAD_MUTEX_ROBUST);
                                                                  ^
1 error generated.
Target //tensorflow:tensorflow failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1894.656s, Critical Path: 288.31s
INFO: 628 processes: 628 local.
FAILED: Build did NOT complete successfully


Please help

"
33049,Input pipeline guide refactoring,"## URL(s) with the issue:

Better performance with tf.data: https://www.tensorflow.org/guide/data_performance

## Description of issue (what needs changing):

Currently, this guide seems to be the main documentation source for `tf.data` usage.
However, the differents steps shown do not seems to be optimal. For example:
* TFRecordDataset usage do not match the actual API (see #33048)
* Usage of `interleave` with TFRecordDataset is redoundant (see [SO post](https://stackoverflow.com/questions/58014123/how-to-improve-data-input-pipeline-performance)).

### Submit a pull request?

No PR intented for the moment as I think this requires some discussion if I missed something or whatever."
33048,TFRecordDataset constructor does not support globbing pattern,"## URL(s) with the issue:

Better performance with data: https://www.tensorflow.org/guide/data_performance#structure_of_an_input_pipeline

## Description of issue (what needs changing):

The code example provided instantiate a `tf.data.TFRecordDataset` passing a globbing pattern: `""/path/to/dataset/train-*.tfrecord""` while it does not support it.

The example should be updated, relying on [`tf.data.Dataset.list_files`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#list_files).

```diff
- dataset = tf.data.TFRecordDataset(""/path/to/dataset/train-*.tfrecord"")
+ dataset = tf.data.TFRecordDataset(tf.data.Dataset.list_files(""/path/to/dataset/train-*.tfrecord""))
```

### Submit a pull request?

I will submit a pull request soon."
33047,"Kernel restating while importing tensorflow on Old Macs, works OK on new one.","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
33046,Converting Tensor to numpy array,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.14.0
- Are you willing to contribute it (Yes/No): Yes


**Describe the feature and the current behavior/state.**
I already trained a model and I need to load the model in another script (model1) and use it for prediction (df: new data).

I load the model to use it for prediction as follow:
model1 = tf.keras.models.load_model(model_path, custom_objects=None, compile=False)
result = model1(df)

Then how can I extract the numpy array from result Tensor?

if I use ""result.numpy()"" I will get ""AttributeError: 'Normal' object has no attribute 'numpy'""

I need to return numbers to the frontend application.

**Will this change the current api? How?**
I don't know

**Who will benefit with this feature?**
Anyone using TensorFlow for production applications.

**Any Other info.**"
33045,TPU support in tensorflow 2.0 release,"From the following link https://www.tensorflow.org/guide/distributed_training I understand that TPU training is supported in tensorflow 2.0.

I followed the snippet code provided in the same page:

cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(
    tpu=tpu_address)
tf.config.experimental_connect_to_cluster(cluster_resolver)
tf.tpu.experimental.initialize_tpu_system(cluster_resolver)
tpu_strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)

And I got the following error:
InvalidArgumentError: Unable to find a context_id matching the specified one (-7989870214237460624). Perhaps the worker was restarted, or the context was GC'd?
Additional GRPC error information:
{""created"":""@1570180842.964900283"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""Unable to find a context_id matching the specified one (-7989870214237460624). Perhaps the worker was restarted, or the context was GC'd?"",""grpc_status"":3}

Same error was thrown when I didn't provide the tpu_address as it is stated in the above link.

> The TPUClusterResolver instance helps locate the TPUs. In Colab, you don't need to specify any arguments to it.

The test was done in google colab and I selected the TPU accelerator. I installed tensorflow-gpu with !pip install tensorflow-gpu.

If TPU is not yet supported in tf 2.0 when it is planed to be added?
"
33044,Detection and get position in TensorFlow android studio,"
"
33043,Dumping XLA compiled results in specific directory not working,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution : CentOS 7.6 x86_64
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): r1.12.3
- Python version: 3.6.7
- Bazel version (if compiling from source): 0.19.1
- GCC/Compiler version (if compiling from source): gcc 7.4.0
- CUDA/cuDNN version: CUDA 10.1 / cuDNN 7.5
- GPU model and memory: Tesla V100-PCIE-32GB / Titan Xp 12GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
- XLA compiled model files are not saved in the desired directory, even though I declared the xla dump directory in an explicit manner
- According to this link (https://www.tensorflow.org/xla), a compiled model must be saved as `module_XXXX.ptx` or something, which is not visible.
- Meanwhile, I only can see the graph descriptions about the model under /tmp, even though I did already declared a different directory far from /tmp.
```
$ ls /tmp
before_mark_for_compilation_1.pbtxt
before_mark_for_compilation_2.pbtxt
before_mark_for_compilation_3.pbtxt
before_mark_for_compilation_4.pbtxt
before_mark_for_compilation_5.pbtxt
before_mark_for_compilation_6.pbtxt
before_mark_for_compilation.pbtxt
mark_for_compilation_1.pbtxt
mark_for_compilation_2.pbtxt
mark_for_compilation_3.pbtxt
mark_for_compilation_4.pbtxt
mark_for_compilation_5.pbtxt
mark_for_compilation_6.pbtxt
mark_for_compilation.pbtxt
```

**Describe the expected behavior**
```
# XLA compiled module files may be saved in /somewhere/xladump
$ ls /somewhere/xladump
```

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
TF_XLA_FLAGS=""--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit --tf_xla_clustering_debug"" TF_DUMP_GRAPH_PREFIX=/somewhere/xladump XLA_FLAGS=""--dump_hlo_as_text --xla_dump_to=/somewhere/xladump"" PYTHONDONTWRITEBYTECODE=1 time python blahblah.py
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

[Build Flags]
```
- Build with Ignite? Y
- Build with XLA JIT? Y
...
```

[Runtime Log]
```
2019-10-04 15:10:36.868003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-04 15:10:36.868683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.911
pciBusID: 0000:09:00.0
totalMemory: 11.91GiB freeMemory: 11.75GiB
2019-10-04 15:10:36.868705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-10-04 15:10:37.195028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-10-04 15:10:37.195073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0
2019-10-04 15:10:37.195082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N
2019-10-04 15:10:37.195174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11367 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:09:00.0, compute capability: 6.1)
2019-10-04 15:10:37.784171: I tensorflow/compiler/tf2xla/dump_graph.cc:79] Dumped GraphDef to /tmp//before_mark_for_compilation.pbtxt
2019-10-04 15:10:37.794203: I tensorflow/compiler/tf2xla/dump_graph.cc:79] Dumped GraphDef to /tmp//mark_for_compilation.pbtxt
2019-10-04 15:10:37.847682: I tensorflow/compiler/xla/service/service.cc:149] XLA service 0x7f237c001c70 executing computations on platform CUDA. Devices:
2019-10-04 15:10:37.847749: I tensorflow/compiler/xla/service/service.cc:157]   StreamExecutor device (0): TITAN Xp, Compute Capability 6.1
2019-10-04 15:10:37.903711: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:402] *** WARNING *** You are using ptxas 10.0.145, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You do not need to update to CUDA 9.2.88; cherry-picking the ptxas binary is sufficient.
2019-10-04 15:10:40.008334: I tensorflow/compiler/tf2xla/dump_graph.cc:79] Dumped GraphDef to /tmp//before_mark_for_compilation_1.pbtxt
2019-10-04 15:10:40.009382: I tensorflow/compiler/tf2xla/dump_graph.cc:79] Dumped GraphDef to /tmp//mark_for_compilation_1.pbtxt
2019-10-04 15:10:40.548377: I tensorflow/compiler/tf2xla/dump_graph.cc:79] Dumped GraphDef to /tmp//before_mark_for_compilation_2.pbtxt
2019-10-04 15:10:40.549325: I tensorflow/compiler/tf2xla/dump_graph.cc:79] Dumped GraphDef to /tmp//mark_for_compilation_2.pbtxt
2019-10-04 15:10:40.783753: I tensorflow/compiler/tf2xla/dump_graph.cc:79] Dumped GraphDef to /tmp//before_mark_for_compilation_3.pbtxt
2019-10-04 15:10:40.784706: I tensorflow/compiler/tf2xla/dump_graph.cc:79] Dumped GraphDef to /tmp//mark_for_compilation_3.pbtxt
[I] TF_XLA_FLAGS=--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit --tf_xla_clustering_debug
[I] XLA_FLAGS=--dump_hlo_as_text --xla_dump_to=sponge
[I] TF_DUMP_GRAPH_PREFIX=sponge
```"
33042,I have learned the dark secret to compile on Windows,"For all my fellow cybernauts, the dark secrets revealed. To tell it what compiler to use, requires ancient spells of dark magic. You must call into vcvarsall.bat of olde, and then you can tell it the vcvars for which compiler you wish it to choose. Now, go forth and compile."
33041,EdgeTPU library is not updating for coral dev board ,"* Doc you were trying to follow: https://coral.withgoogle.com/news/updates-07-2019/
* Your host OS: Ubuntu 16.04
Python3 version: 3.5.3

# What I ran

I executed the commands as mentioned in the post as mentioned above :

sudo apt-get update

sudo apt-get dist-upgrade



this automatically updates the edgetpu library along with other Mendel system software.

# What the docs said should happen:

As per the post it should have updated the library but after updates were done the edgetpu library version is still been shown as 1.2.0

But our expectations were to be updated to newest 2.11.1 version

# What actually happened

EdgeTPU library version is still been shown as 1.2.0


Kindly help in resolving the issue.
"
33040,Illegal Instruction on importing tensorflow,"Tensorflow version = 1.14
OS : Kali Linux Rolling (2018.2)
Output on cat /proc/cpuinfo : 
processor : 0
vendor_id : GenuineIntel
cpu family : 6
model : 23
model name : Intel(R) Core(TM)2 Duo CPU E7500 @ 2.93GHz
stepping : 10
microcode : 0xa0b
cpu MHz : 2546.724
cache size : 3072 KB
physical id : 0
siblings : 2
core id : 0
cpu cores : 2
apicid : 0
initial apicid : 0
fpu : yes
fpu_exception : yes
cpuid level : 13
wp : yes
flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx lm constant_tsc arch_perfmon pebs bts rep_good nopl cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 xsave lahf_lm pti tpr_shadow vnmi flexpriority dtherm
bugs : cpu_meltdown spectre_v1 spectre_v2
bogomips : 5850.33
clflush size : 64
cache_alignment : 64
address sizes : 36 bits physical, 48 bits virtual
power management:

processor : 1
vendor_id : GenuineIntel
cpu family : 6
model : 23
model name : Intel(R) Core(TM)2 Duo CPU E7500 @ 2.93GHz
stepping : 10
microcode : 0xa0b
cpu MHz : 2231.318
cache size : 3072 KB
physical id : 0
siblings : 2
core id : 1
cpu cores : 2
apicid : 1
initial apicid : 1
fpu : yes
fpu_exception : yes
cpuid level : 13
wp : yes
flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx lm constant_tsc arch_perfmon pebs bts rep_good nopl cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 xsave lahf_lm pti tpr_shadow vnmi flexpriority dtherm
bugs : cpu_meltdown spectre_v1 spectre_v2
bogomips : 5850.33
clflush size : 64
cache_alignment : 64
address sizes : 36 bits physical, 48 bits virtual
power management:

"
33039,TF 2.0:  ValueError: Lengths of branch outputs of cond must match.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.0
- Python version: 3.7
- Bazel version (if compiling from source): /
- GCC/Compiler version (if compiling from source): /
- CUDA/cuDNN version: 10/7.4
- GPU model and memory: gtx1050ti / 12GB

**Describe the current behavior**
I have a error when I use if cond in tf 2.0.
The error code is as follows:

+ Markdown format does not work properly. sry....

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-51-52716284752e> in <module>
      2 multiboxloss = MultiboxLoss(8)
      3 
----> 4 model.compile(optimizer=tf.keras.optimizers.Nadam(lr = 0.001), loss = multiboxloss.comute_loss, metrics=['acc'])

~\Anaconda3\lib\site-packages\tensorflow_core\python\training\tracking\base.py in _method_wrapper(self, *args, **kwargs)
    455     self._self_setattr_tracking = False  # pylint: disable=protected-access
    456     try:
--> 457       result = method(self, *args, **kwargs)
    458     finally:
    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

~\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)
    371 
    372       # Creates the model loss and weighted metrics sub-graphs.
--> 373       self._compile_weights_loss_and_weighted_metrics()
    374 
    375       # Functions for train, test and predict will

~\Anaconda3\lib\site-packages\tensorflow_core\python\training\tracking\base.py in _method_wrapper(self, *args, **kwargs)
    455     self._self_setattr_tracking = False  # pylint: disable=protected-access
    456     try:
--> 457       result = method(self, *args, **kwargs)
    458     finally:
    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

~\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\training.py in _compile_weights_loss_and_weighted_metrics(self, sample_weights)
   1651       #                   loss_weight_2 * output_2_loss_fn(...) +
   1652       #                   layer losses.
-> 1653       self.total_loss = self._prepare_total_loss(masks)
   1654 
   1655   def _prepare_skip_target_masks(self):

~\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\training.py in _prepare_total_loss(self, masks)
   1711 
   1712           if hasattr(loss_fn, 'reduction'):
-> 1713             per_sample_losses = loss_fn.call(y_true, y_pred)
   1714             weighted_losses = losses_utils.compute_weighted_loss(
   1715                 per_sample_losses,

~\Anaconda3\lib\site-packages\tensorflow_core\python\keras\losses.py in call(self, y_true, y_pred)
    219       y_pred, y_true = tf_losses_util.squeeze_or_expand_dimensions(
    220           y_pred, y_true)
--> 221     return self.fn(y_true, y_pred, **self._fn_kwargs)
    222 
    223   def get_config(self):

~\Anaconda3\lib\site-packages\tensorflow_core\python\eager\def_function.py in __call__(self, *args, **kwds)
    455 
    456     tracing_count = self._get_tracing_count()
--> 457     result = self._call(*args, **kwds)
    458     if tracing_count == self._get_tracing_count():
    459       self._call_counter.called_without_tracing()

~\Anaconda3\lib\site-packages\tensorflow_core\python\eager\def_function.py in _call(self, *args, **kwds)
    501       # This is the first call of __call__, so we have to initialize.
    502       initializer_map = object_identity.ObjectIdentityDictionary()
--> 503       self._initialize(args, kwds, add_initializers_to=initializer_map)
    504     finally:
    505       # At this point we know that the initialization is complete (or less

~\Anaconda3\lib\site-packages\tensorflow_core\python\eager\def_function.py in _initialize(self, args, kwds, add_initializers_to)
    406     self._concrete_stateful_fn = (
    407         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--> 408             *args, **kwds))
    409 
    410     def invalid_creator_scope(*unused_args, **unused_kwds):

~\Anaconda3\lib\site-packages\tensorflow_core\python\eager\function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   1846     if self.input_signature:
   1847       args, kwargs = None, None
-> 1848     graph_function, _, _ = self._maybe_define_function(args, kwargs)
   1849     return graph_function
   1850 

~\Anaconda3\lib\site-packages\tensorflow_core\python\eager\function.py in _maybe_define_function(self, args, kwargs)
   2148         graph_function = self._function_cache.primary.get(cache_key, None)
   2149         if graph_function is None:
-> 2150           graph_function = self._create_graph_function(args, kwargs)
   2151           self._function_cache.primary[cache_key] = graph_function
   2152         return graph_function, args, kwargs

~\Anaconda3\lib\site-packages\tensorflow_core\python\eager\function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   2039             arg_names=arg_names,
   2040             override_flat_arg_shapes=override_flat_arg_shapes,
-> 2041             capture_by_value=self._capture_by_value),
   2042         self._function_attributes,
   2043         # Tell the ConcreteFunction to clean up its graph once it goes out of

~\Anaconda3\lib\site-packages\tensorflow_core\python\framework\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    913                                           converted_func)
    914 
--> 915       func_outputs = python_func(*func_args, **func_kwargs)
    916 
    917       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

~\Anaconda3\lib\site-packages\tensorflow_core\python\eager\def_function.py in wrapped_fn(*args, **kwds)
    356         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    357         # the function a weak reference to itself to avoid a reference cycle.
--> 358         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    359     weak_wrapped_fn = weakref.ref(wrapped_fn)
    360 

~\Anaconda3\lib\site-packages\tensorflow_core\python\eager\function.py in bound_method_wrapper(*args, **kwargs)
   2656     # However, the replacer is still responsible for attaching self properly.
   2657     # TODO(mdan): Is it possible to do it here instead?
-> 2658     return wrapped_fn(*args, **kwargs)
   2659   weak_bound_method_wrapper = weakref.ref(bound_method_wrapper)
   2660 

~\Anaconda3\lib\site-packages\tensorflow_core\python\framework\func_graph.py in wrapper(*args, **kwargs)
    903           except Exception as e:  # pylint:disable=broad-except
    904             if hasattr(e, ""ag_error_metadata""):
--> 905               raise e.ag_error_metadata.to_exception(e)
    906             else:
    907               raise

ValueError: in converted code:

    <ipython-input-31-45935cecbe16>:100 comute_loss  *
        pos_list, neg_list, t_gtl, t_gtb = matcher.matching(y_pred[i, :, 4:8], y_pred[i, :, :4], actual_list, i)
    C:\Users\jcy12\Anaconda3\lib\site-packages\tensorflow_core\python\eager\def_function.py:457 __call__
        result = self._call(*args, **kwds)
    <ipython-input-50-83f8d8829ac3>:94 matching  *
        if(jacc_thred == 1):
    C:\Users\jcy12\Anaconda3\lib\site-packages\tensorflow_core\python\autograph\operators\control_flow.py:893 if_stmt
        basic_symbol_names, composite_symbol_names)
    C:\Users\jcy12\Anaconda3\lib\site-packages\tensorflow_core\python\autograph\operators\control_flow.py:931 tf_if_stmt
        error_checking_orelse)
    C:\Users\jcy12\Anaconda3\lib\site-packages\tensorflow_core\python\util\deprecation.py:507 new_func
        return func(*args, **kwargs)
    C:\Users\jcy12\Anaconda3\lib\site-packages\tensorflow_core\python\ops\control_flow_ops.py:1174 cond
        return cond_v2.cond_v2(pred, true_fn, false_fn, name)
    C:\Users\jcy12\Anaconda3\lib\site-packages\tensorflow_core\python\ops\cond_v2.py:101 cond_v2
        name=scope)
    C:\Users\jcy12\Anaconda3\lib\site-packages\tensorflow_core\python\ops\cond_v2.py:216 _build_cond
        _check_same_outputs(_COND, [true_graph, false_graph])
    C:\Users\jcy12\Anaconda3\lib\site-packages\tensorflow_core\python\ops\cond_v2.py:736 _check_same_outputs
        len_b=len(graphs[b].outputs)))

    ValueError: Lengths of branch outputs of cond must match.
    len(graphs[0].outputs): 3
    len(graphs[1].outputs): 2```

---------------------------------------------------------------------------
# **Describe the expected behavior**
I want to make the if statement in my code work. The if statement is as follows:


# compute jaccard for each default box
for gt_list in actual_list:
    gt_label = gt_list[1]
    gt_box = gt_list[0]
            
    for i in range(len(matches)):
         # self.default_boxes[batch_size, i].shape --> (4, )
         jacc = jaccard(gt_box[0], self.default_boxes[batch_size, i])
         jacc_thred = tf.where(0.5 <= jacc, 1, 0)
         if(jacc_thred == 1):
                  matches[i] = 4    # <-- **Error part!**
                  self.pos += 1
                  matched.append(gt_label)
"
33038,TF 2.0: Illegal instruction (core dumped),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.2 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: /
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0
- Python version: 3.7.4
- Installed using virtualenv? pip? conda?: pip (in conda environment)
- Bazel version (if compiling from source): /
- GCC/Compiler version (if compiling from source): /
- CUDA/cuDNN version: 10 / 7
- GPU model and memory: GeForce Titan X, 12GB

**Describe the problem**

After installing TF2 through pip I can't launch it and python exits.

From what I gather [here](https://github.com/tensorflow/tensorflow/issues/30114), my cpu would need AVX instructions and in fact they don't show in my cpuinfo.
However, on the same machine I can smoothly run Tensorflow 1.14 (both cpu and gpu). Any idea on what's going on? Is AVX actually the problem?

Strangely, on [this chart](https://www.intel.com/content/dam/support/us/en/documents/processors/core/intel-core-i7-comparison-chart.pdf) from Intel, it states that my processor (Intel Core i7 CPU 960) supports AVX.

I have also tried building from source but Bazel fails to build the package. I don't know if this could be related.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
$ python
>> import tensorflow as tf
Illegal instruction (core dumped)
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
$ cat /proc/cpuinfo

processor	: 0
vendor_id	: GenuineIntel
cpu family	: 6
model		: 26
model name	: Intel(R) Core(TM) i7 CPU         960  @ 3.20GHz
stepping	: 5
microcode	: 0x11
cpu MHz		: 3373.506
cache size	: 8192 KB
physical id	: 0
siblings	: 8
core id		: 0
cpu cores	: 4
apicid		: 0
initial apicid	: 0
fpu		: yes
fpu_exception	: yes
cpuid level	: 11
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm pti tpr_shadow vnmi flexpriority ept vpid dtherm ida
bugs		: cpu_meltdown spectre_v1 spectre_v2
bogomips	: 6477.12
clflush size	: 64
cache_alignment	: 64
address sizes	: 36 bits physical, 48 bits virtual
power management:

processor	: 1
vendor_id	: GenuineIntel
cpu family	: 6
model		: 26
model name	: Intel(R) Core(TM) i7 CPU         960  @ 3.20GHz
stepping	: 5
microcode	: 0x11
cpu MHz		: 3373.503
cache size	: 8192 KB
physical id	: 0
siblings	: 8
core id		: 1
cpu cores	: 4
apicid		: 2
initial apicid	: 2
fpu		: yes
fpu_exception	: yes
cpuid level	: 11
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm pti tpr_shadow vnmi flexpriority ept vpid dtherm ida
bugs		: cpu_meltdown spectre_v1 spectre_v2
bogomips	: 6477.12
clflush size	: 64
cache_alignment	: 64
address sizes	: 36 bits physical, 48 bits virtual
power management:

processor	: 2
vendor_id	: GenuineIntel
cpu family	: 6
model		: 26
model name	: Intel(R) Core(TM) i7 CPU         960  @ 3.20GHz
stepping	: 5
microcode	: 0x11
cpu MHz		: 3373.504
cache size	: 8192 KB
physical id	: 0
siblings	: 8
core id		: 2
cpu cores	: 4
apicid		: 4
initial apicid	: 4
fpu		: yes
fpu_exception	: yes
cpuid level	: 11
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm pti tpr_shadow vnmi flexpriority ept vpid dtherm ida
bugs		: cpu_meltdown spectre_v1 spectre_v2
bogomips	: 6477.12
clflush size	: 64
cache_alignment	: 64
address sizes	: 36 bits physical, 48 bits virtual
power management:

processor	: 3
vendor_id	: GenuineIntel
cpu family	: 6
model		: 26
model name	: Intel(R) Core(TM) i7 CPU         960  @ 3.20GHz
stepping	: 5
microcode	: 0x11
cpu MHz		: 3373.503
cache size	: 8192 KB
physical id	: 0
siblings	: 8
core id		: 3
cpu cores	: 4
apicid		: 6
initial apicid	: 6
fpu		: yes
fpu_exception	: yes
cpuid level	: 11
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm pti tpr_shadow vnmi flexpriority ept vpid dtherm ida
bugs		: cpu_meltdown spectre_v1 spectre_v2
bogomips	: 6477.12
clflush size	: 64
cache_alignment	: 64
address sizes	: 36 bits physical, 48 bits virtual
power management:

processor	: 4
vendor_id	: GenuineIntel
cpu family	: 6
model		: 26
model name	: Intel(R) Core(TM) i7 CPU         960  @ 3.20GHz
stepping	: 5
microcode	: 0x11
cpu MHz		: 3373.512
cache size	: 8192 KB
physical id	: 0
siblings	: 8
core id		: 0
cpu cores	: 4
apicid		: 1
initial apicid	: 1
fpu		: yes
fpu_exception	: yes
cpuid level	: 11
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm pti tpr_shadow vnmi flexpriority ept vpid dtherm ida
bugs		: cpu_meltdown spectre_v1 spectre_v2
bogomips	: 6477.12
clflush size	: 64
cache_alignment	: 64
address sizes	: 36 bits physical, 48 bits virtual
power management:

processor	: 5
vendor_id	: GenuineIntel
cpu family	: 6
model		: 26
model name	: Intel(R) Core(TM) i7 CPU         960  @ 3.20GHz
stepping	: 5
microcode	: 0x11
cpu MHz		: 3373.506
cache size	: 8192 KB
physical id	: 0
siblings	: 8
core id		: 1
cpu cores	: 4
apicid		: 3
initial apicid	: 3
fpu		: yes
fpu_exception	: yes
cpuid level	: 11
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm pti tpr_shadow vnmi flexpriority ept vpid dtherm ida
bugs		: cpu_meltdown spectre_v1 spectre_v2
bogomips	: 6477.12
clflush size	: 64
cache_alignment	: 64
address sizes	: 36 bits physical, 48 bits virtual
power management:

processor	: 6
vendor_id	: GenuineIntel
cpu family	: 6
model		: 26
model name	: Intel(R) Core(TM) i7 CPU         960  @ 3.20GHz
stepping	: 5
microcode	: 0x11
cpu MHz		: 3373.501
cache size	: 8192 KB
physical id	: 0
siblings	: 8
core id		: 2
cpu cores	: 4
apicid		: 5
initial apicid	: 5
fpu		: yes
fpu_exception	: yes
cpuid level	: 11
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm pti tpr_shadow vnmi flexpriority ept vpid dtherm ida
bugs		: cpu_meltdown spectre_v1 spectre_v2
bogomips	: 6477.12
clflush size	: 64
cache_alignment	: 64
address sizes	: 36 bits physical, 48 bits virtual
power management:

processor	: 7
vendor_id	: GenuineIntel
cpu family	: 6
model		: 26
model name	: Intel(R) Core(TM) i7 CPU         960  @ 3.20GHz
stepping	: 5
microcode	: 0x11
cpu MHz		: 3373.507
cache size	: 8192 KB
physical id	: 0
siblings	: 8
core id		: 3
cpu cores	: 4
apicid		: 7
initial apicid	: 7
fpu		: yes
fpu_exception	: yes
cpuid level	: 11
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm pti tpr_shadow vnmi flexpriority ept vpid dtherm ida
bugs		: cpu_meltdown spectre_v1 spectre_v2
bogomips	: 6477.12
clflush size	: 64
cache_alignment	: 64
address sizes	: 36 bits physical, 48 bits virtual
power management:

```
"
33037,"radon and inverse radon transform, Projection and back_projection function in CT","<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):  tf 1.14
- Are you willing to contribute it (Yes/No):  no
   When training a CT-reconstructed model, the projection and back_projection functions are usually used. However, to my knowledge, there is no api  to implement the above functions.


**Describe the feature and the current behavior/state.**
  Like in the scikit-image package, the projection function is the radon transform (skimage.transform.radon) while the back-projection is the dual operator of projection.

**Will this change the current api? How?**
Will add two maybe three apis.

**Who will benefit with this feature?**
anyone uses these functions to train CT-reconstruction moedl.

**Any Other info.**
"
33036,TF 2.0 How to set a different learning rate for each layer?,"## Description of issue (what needs changing):
Hi, I would like to ask a question about how to set different learning rates for several layers. In my implementation, I would like to set one learning rate for some layers while the other learning rate for some other layers. Is there some example codes? Thanks in advance.
"
33035,tf.saved_model.save() unable to save model with sparse input.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu (Colab)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):  fails on 1.14.0, 1.15.0rc2, 2.0.0
- Python version: python 3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
See this gist for reproduction 
https://colab.research.google.com/gist/yzhuang/3d4c511b247988fdafbd083785d624c1/model-saving-fails-with-sparse-input-tensor.ipynb

If a model has sparse input, they seem to be converted to dense tensors when `saved_model.save()` is called.  This causes model saving to fail.

**Describe the expected behavior**

If a model works during model.fit(), I expect the same model to also work for `saved_model.save()`.

**Code to reproduce the issue**
Colab gist:
https://colab.research.google.com/gist/yzhuang/3d4c511b247988fdafbd083785d624c1/model-saving-fails-with-sparse-input-tensor.ipynb

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
33034,Per-operation random seeds don't work in tensorflow 2.0,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
- TensorFlow installed from (source or binary): anaconda
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0
- Python version: Python 3.7.4

**Describe the current behavior**

Minimal test script:

```python
import tensorflow as tf

x = tf.random.uniform([1], seed=1)
y = tf.random.uniform([1], seed=1)
print(""x == y?"", bool(tf.reduce_all(x == y)))
print(x)
print(y)
```

Output:
```
x == y? False
tf.Tensor([0.2390374], shape=(1,), dtype=float32)
tf.Tensor([0.22267115], shape=(1,), dtype=float32)
```

**Describe the expected behavior**

The script should print `x == y? True`.

According to the [docs](https://www.tensorflow.org/api_docs/python/tf/compat/v1/set_random_seed) on `tf.compat.v1.set_random_seed`:
>  If the graph-level seed is not set, but the operation seed is set: A default graph-level seed and the specified operation seed are used to determine the random sequence.

This does work correctly when not executing eagerly."
33031,Model.evaluate diverging from fit on Unet based model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): 
- TensorFlow version (use command below): 1.14
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: Colab (K80 ?)

**Describe the current behavior**
the model.evaluate is providing totally different results from the model.fit method.

To demonstrate this, i've don't this on colab :
1/ Create a notebook with GPU setup
2/ create a dataset with only one image
3/ feed a training loop (tf.keras.model.fit on 100 epochs) with the image embedded as a tf.Data.Dataset.
As there are only one image, i got a **loss of 0.0571** and a binary **accuracy close to 1**
4/ the, i run the model.evaluate method and i got something totally different :  
**loss of 0.3667** and a binary **accuracy of 0.8373**
5/ a run the fit method again for 1 epoch and got results close to step 3 :  
**loss of 0.0558** and a binary **accuracy close to 1**

**Describe the expected behavior**
The evaluate method must provide results close to the fit method, especially after 100 epochs

**Code to reproduce the issue**
```
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from google.colab import drive

#get a image as input data for model : tensorflow logo
!curl https://avatars0.githubusercontent.com/u/15658638?s=256 --output tensor_logo.png

def train_input_fn():

  def _process_string_image(img_path):
    raw_image = tf.read_file(img_path)
    image_decoded = tf.image.decode_png(raw_image, channels=3)
    image_decoded = tf.cast(image_decoded, tf.float32)/255.
    dummy_labels = tf.round(image_decoded)
    return image_decoded, dummy_labels

  list_files = tf.data.Dataset.from_tensor_slices(['/content/tensor_logo.png'])
  files = list_files.map(_process_string_image)
  files = files.repeat(100)
  files = files.batch(1, drop_remainder=True)
  return files

#basic check to compare train_input_fn_dummy_data, train_input_fn_from_tf_records
#can't be run after TPU initialisation
with tf.Session() as sess:
  batch = train_input_fn().make_one_shot_iterator().get_next()
  item = sess.run(batch)
  print('shape of first item :', item[0].shape, item[1].shape)
  plt.imshow(item[0][0])
  plt.show()
  plt.imshow(item[1][0])

def Unet_Encoder(inp, layer_nb, activation = 'relu', padding = 'same', batchnorm = True, is_pool = True):
    inp = tf.keras.layers.Conv2D(8 * 2**layer_nb, (3, 3), activation=activation, padding=padding, name = f""conv_1_L{layer_nb}"") (inp)
    if batchnorm:
        inp = tf.keras.layers.BatchNormalization(name = f""bn_1_L{layer_nb}"")(inp)
    inp = tf.keras.layers.Conv2D(8 * 2**layer_nb, (3, 3), activation=activation, padding=padding, name = f""conv_2_L{layer_nb}"") (inp)
    if batchnorm:
        inp = tf.keras.layers.BatchNormalization(name = f""bn_2_L{layer_nb}"")(inp)
        
    if is_pool:
        return inp, tf.keras.layers.MaxPooling2D((2, 2), name = f""maxpool_L{layer_nb}"") (inp)
    
    return inp

def Unet_Decoder(inp, attention, layer_nb, activation = 'relu', padding = 'same', batchnorm = True):
    inp = tf.keras.layers.Conv2DTranspose(8 * 2**layer_nb, (2, 2), strides=(2, 2), padding=padding, name = f""convT_L{layer_nb}"") (inp)
    if batchnorm:
        inp = tf.keras.layers.BatchNormalization()(inp)
    inp = tf.keras.layers.concatenate([inp, attention])
    inp = tf.keras.layers.Conv2D(8 * 2**layer_nb, (3, 3), activation=activation, padding=padding, name = f""uconv_1_L{layer_nb}"") (inp)
    if batchnorm:
        inp = tf.keras.layers.BatchNormalization(name = f""ubn_1_L{layer_nb}"")(inp)
    inp = tf.keras.layers.Conv2D(8 * 2**layer_nb, (3, 3), activation=activation, padding=padding, name = f""uconv_2_L{layer_nb}"") (inp)
    if batchnorm:
        inp = tf.keras.layers.BatchNormalization(name = f""ubn_2_L{layer_nb}"")(inp)
    
    return inp

def Build_Unet(input_shape, depth, activation = 'relu', padding = 'same', batchnorm = True):
    inputs = tf.keras.layers.Input(input_shape)
    conv = [None for x in range(depth)]
    
    ###### encoding part ######
    last_pool = inputs
    for i in range(0, depth):
        conv[i], last_pool = Unet_Encoder(last_pool, i, activation, padding, batchnorm)
    
    ###### bottom layer ###### 
    uconv = Unet_Encoder(last_pool, depth, activation, padding, batchnorm, is_pool=False)
    
    ###### encoding part ######
    for i in range(depth-1, -1, -1):
        uconv = Unet_Decoder(uconv, conv[i], i, activation, padding, batchnorm)
    
    outputs = tf.keras.layers.Conv2D(3, (1, 1), activation='sigmoid') (uconv)
    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])

    return model

#build model and compile
model = Build_Unet((None, None, 3), 4)

adam = tf.keras.optimizers.Adam(learning_rate=0.005)
model.compile(adam, loss='binary_crossentropy', metrics=['binary_accuracy'])

model.summary()

# fit the model
model.fit(train_input_fn(), epochs= 100, steps_per_epoch=1)

model.evaluate(train_input_fn(), steps=1)

model.fit(train_input_fn(), epochs=1, steps_per_epoch=1)
```


It seems that the more epochs i run, the more the results are different from train and fit methods.
With a really simple 'dummy' models like this one, i don't have the issue : 
```
inputs = tf.keras.layers.Input(shape=(256, 256, 3))
outputs = tf.keras.layers.Conv2D(6,(2,2),padding='same')(inputs)
outputs = tf.keras.layers.MaxPool2D()(outputs)
outputs = tf.keras.layers.Conv2D(12,(2,2),padding='same')(outputs)
outputs = tf.keras.layers.Conv2DTranspose(6,(4,4),(2,2),padding='same')(outputs)
outputs = tf.keras.layers.Conv2D(3,(2,2),padding='same', activation='sigmoid')(outputs)
```"
33030,Memory leak on TF 2.0 with model.predict or/and model.fit with keras,"**System information**
- OS Platform:

> System Version: macOS 10.14.6 (18G103)
> Kernel Version: Darwin 18.7.0

- TensorFlow installed from binary using `pip install tensorflow`

- Python version:

```python
python -V                                                                                                                                                                                                      
Python 3.7.3
```

- GPU model and memory: No GPU

- TensorFlow version


```python
python -c ""import tensorflow as tf; print(tf.version.VERSION)""                                                                                                                                               
2.0.0
```


**Describe the current behavior**
While running using tensorflow 1.14 or theano backends this code works fine.
After upgraded to tensorflow 2.0.0 it stops working and memory usage increasing without finish the program.

**Describe the expected behavior**
Using theano I get 28 seconds by iteration.
Using tensorflow 2.0.0 I expect same behavior (or better). 


**Code to reproduce the issue**
```python
import gym
import numpy as np
import matplotlib.pylab as plt

import tensorflow as tf
from tensorflow.keras import layers

env = gym.make('NChain-v0')


def q_learning_keras(env, num_episodes=1000):
    # create the keras model
    model = tf.keras.Sequential()
    model.add(layers.InputLayer(batch_input_shape=(1, 5)))
    model.add(layers.Dense(10, activation='sigmoid'))
    model.add(layers.Dense(2, activation='linear'))
    model.compile(loss='mse', optimizer='adam', metrics=['mae'])
    # now execute the q learning
    y = 0.95
    eps = 0.5
    decay_factor = 0.999
    r_avg_list = []
    for i in range(num_episodes):
        s = env.reset()
        eps *= decay_factor
        if i % 100 == 0:
            print(""Episode {} of {}"".format(i + 1, num_episodes))
        done = False
        r_sum = 0
        while not done:
            if np.random.random() < eps:
                a = np.random.randint(0, 2)
            else:
                a = np.argmax(model.predict(np.identity(5)[s:s + 1]))
            new_s, r, done, _ = env.step(a)
            target = r + y * np.max(model.predict(np.identity(5)[new_s:new_s + 1]))
            target_vec = model.predict(np.identity(5)[s:s + 1])[0]
            target_vec[a] = target
            model.fit(np.identity(5)[s:s + 1], target_vec.reshape(-1, 2), epochs=1, verbose=0)
            s = new_s
            r_sum += r
        r_avg_list.append(r_sum / 1000)
    plt.plot(r_avg_list)
    plt.ylabel('Average reward per game')
    plt.xlabel('Number of games')
    plt.show()
    for i in range(5):
        print(""State {} - action {}"".format(i, model.predict(np.identity(5)[i:i + 1])))


if __name__ == ""__main__"":
    q_learning_keras(env)
```
"
33028,tf keras progbar callback is not displayed during keras.fit() when verbose=0.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary (conda install tensorflow-gpu==1.14.0)
- TensorFlow version (use command below): 1.14.0
- Python version: Python 3.7.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10
- GPU model and memory: nvidia 1080, 8GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Attempting to use progbar as a callback to model.fit() fails.
**Describe the expected behavior**
If runing model.fit() with progbar callback and verbosity=0 should be identical to verbosity=1 and no progbar callback
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
current output:
```
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

import tensorflow as tf

print('tf_version:', tf.__version__, 'gpu available:', tf.test.is_gpu_available())
model = tf.keras.applications.ResNet50()


print('compiling model')
model.compile(optimizer='SGD', loss=tf.keras.losses.categorical_crossentropy)

print('running fit function')
x = tf.data.Dataset.from_tensors(tf.zeros([16]+model.input.shape.as_list()[1:]))
y = tf.data.Dataset.from_tensors(tf.zeros([16]+model.output.shape.as_list()[1:]))
print('x:', x, '\ny', y)
model.fit(tf.data.Dataset.zip((x,y)).repeat().shuffle(buffer_size=1),
          steps_per_epoch=10,
          verbose=0,
          callbacks=[tf.keras.callbacks.ProgbarLogger('steps')])
print('done')
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
*running the code above recreates the problem.*
```
tf_version: 1.14.0 gpu available: True
WARNING: Logging before flag parsing goes to stderr.
W1003 10:20:49.769316 140195608012544 deprecation.py:506] From /home/hackerman/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
compiling model
running fit function
x: <DatasetV1Adapter shapes: (16, 224, 224, 3), types: tf.float32> 
y <DatasetV1Adapter shapes: (16, 1000), types: tf.float32>
done
```
*expected output (reproduced by commenting out callback, and setting verbose to 1):*
```
tf_version: 1.14.0 gpu available: True
WARNING: Logging before flag parsing goes to stderr.
W1003 10:21:47.987383 140242665334528 deprecation.py:506] From /home/hackerman/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
compiling model
running fit function
x: <DatasetV1Adapter shapes: (16, 224, 224, 3), types: tf.float32> 
y <DatasetV1Adapter shapes: (16, 1000), types: tf.float32>
10/10 [==============================] - 6s 592ms/step - loss: 0.0000e+00
done
```
*bonus bug (if keeping callback and setting verbosity to 1):*
```
tf_version: 1.14.0 gpu available: True
WARNING: Logging before flag parsing goes to stderr.
W1003 10:23:10.488721 140225721059072 deprecation.py:506] From /home/hackerman/anaconda3/envs/py36/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
compiling model
running fit function
x: <DatasetV1Adapter shapes: (16, 224, 224, 3), types: tf.float32> 
y <DatasetV1Adapter shapes: (16, 1000), types: tf.float32>
10/10 [==============================] - 6s 597ms/step - loss: 0.0000e+00
10/10 [==============================] - 6s 597ms/step - loss: 0.0000e+00
done
```
"
33027,tf.keras.layers.BatchNormalization produces wrong results when original feature scale is moderately small.,"It looks like BatchNormalization fails to scale features up if the original scale is too low ... or I am doing something brain dead here. 

If this is actually a constraint on the scale param, it should be very explicitly documented. 


**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0rc2
- Python version: 3.7.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

```
import tensorflow as tf
import tensorflow_probability as tfp

x = np.random.randn(1000, 1).astype(np.float32)

x = x * 0.001

bn = tf.keras.layers.BatchNormalization()

@tf.function
def _run_one(x):
    bn(x, training=True)

def run(x):
    for i in range(10000):
        _run_one(x)

tf.print(tf.math.reduce_std(bn(x, training=False), axis=0), tf.reduce_mean(bn(x, training=False), axis=0))
run(x)
tf.print(tf.math.reduce_std(bn(x, training=False), axis=0), tf.reduce_mean(bn(x, training=False), axis=0))

[0.000992934452] [-2.47328098e-05]
[0.0313995518] [-2.12341544e-09]
```

**Describe the expected behavior**
```

import tensorflow as tf
import tensorflow_probability as tfp

x = np.random.randn(1000, 1).astype(np.float32)

# x = x * 0.001
x = x * 2

bn = tf.keras.layers.BatchNormalization()

@tf.function
def _run_one(x):
    bn(x, training=True)

def run(x):
    for i in range(10000):
        _run_one(x)

tf.print(tf.math.reduce_std(bn(x, training=False), axis=0), tf.reduce_mean(bn(x, training=False), axis=0))
run(x)
tf.print(tf.math.reduce_std(bn(x, training=False), axis=0), tf.reduce_mean(bn(x, training=False), axis=0))


[2.03433657] [0.100093901]
[0.999882281] [1.81198118e-07]
```

**Code to reproduce the issue**

See above.


**Other info / logs**
"
33026,Why is CUDA 10.1 not supported & strange error message?,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): pip install tensorflow           (2.0 version)
- TensorFlow version (use command below): Tensorflow v2.0.0
- Python version: 3.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.1
- GPU model and memory: Nvidia Quadro M1000M

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

When I try to use Tensorflow 2.0 with CUDA 10.1 I run into some errors. Previously I haven't seen these errors with older versions of keras/tensorflow.

![image](https://user-images.githubusercontent.com/2350015/66144475-32082700-e609-11e9-8fa6-cb88e3692f53.png)

Code:

```
tf.test.is_gpu_available( cuda_only=False, min_cuda_compute_capability=None )
```

Error:
```

Tensorflow v2.0.0
---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
<ipython-input-4-ee93c3bb3dbc> in <module>
      4 print(f'Tensorflow v{tf.__version__}')
      5 
----> 6 tf.test.is_gpu_available( cuda_only=False, min_cuda_compute_capability=None )

C:\ProgramData\Miniconda3\envs\tf2\lib\site-packages\tensorflow_core\python\framework\test_util.py in is_gpu_available(cuda_only, min_cuda_compute_capability)
   1430 
   1431   try:
-> 1432     for local_device in device_lib.list_local_devices():
   1433       if local_device.device_type == ""GPU"":
   1434         if (min_cuda_compute_capability is None or

C:\ProgramData\Miniconda3\envs\tf2\lib\site-packages\tensorflow_core\python\client\device_lib.py in list_local_devices(session_config)
     39   return [
     40       _convert(s)
---> 41       for s in pywrap_tensorflow.list_devices(session_config=session_config)
     42   ]

C:\ProgramData\Miniconda3\envs\tf2\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py in list_devices(session_config)
   2247     return ListDevicesWithSessionConfig(session_config.SerializeToString())
   2248   else:
-> 2249     return ListDevices()
   2250 
   2251 

InternalError: cudaGetDevice() failed. Status: cudaGetErrorString symbol not found.
```

**Describe the expected behavior**

I would expect a minor release (10.0 to 10.1) which has been out for almost 2 years to work correctly.

Furthermore the error doesn't specify anything regarding CUDA 10.1 not being supported. If this would be the case then that's the error I would expect.

**Code to reproduce the issue**
Install the latest tensorflow with `pip install tensorflow` into a Conda environment running python 3.7 with CUDA 10.1 installed. Then run the following command:
```
tf.test.is_gpu_available( cuda_only=False, min_cuda_compute_capability=None )
```

**Other info / logs**
N/A"
33024,Performance: Training is much slower in TF v2.0.0 VS v1.14.0 when using `Tf.Keras` and `model.fit_generator` ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>


**System information**
**NOTE**: I have provided Google Colab' notebooks to reproduce the slowness. 
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): sortof, but is a basically and MNIST example.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab and Windows
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): pip install tensorflow-gpu
- TensorFlow version (use command below): 2.0.0
- Python version: 3
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 10 or Google Colab
- GPU model and memory: 1080 Ti, or Google Colab

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`
It happens on the standard Colab GPU instance

**Describe the current behavior**
**Version 2.0.0** is SLOW compared to the identical code running **v1.14.0**. 
The code I have used to demonstrate it is very simple, and very similar to most existing Keras examples. 
A Larger NN on MNIST is going from `10s` per epoch to `~20s`  and that is a very major slowdown.

**Describe the expected behavior**
A new version should have similar or better performance than the previous version. 
If user error or a new limitation/feature is causing the problem, it should be warned about in Update Notes/Quick Start. This code was perfectly normal in TF 1.X

**Code to reproduce the issue**
See this (GPU) Colab Notebook example with MNIST Data:
https://colab.research.google.com/gist/Raukk/f0927a5e2a357f2d80c9aeef1202e6ee/example_slow_tf2.ipynb

See this (GPU) Colab Notebook example with numpy random for Data:
https://colab.research.google.com/gist/Raukk/518d3d21e08ad02089429529bd6c67d4/simplified_example_slow_tf2.ipynb

See this (GPU) Colab Notebook example using standard Conv2D (not DepthwiseConv2D):
https://colab.research.google.com/gist/Raukk/4f102e192f47a6dc144b890925b652f8/standardconv_example_slow_tf2.ipynb


**Please notify me if you cannot access any of these notebooks, or if they do not run, or don't sufficiently reproduce the issue.**


**Other info / logs**
Each example above starts with a **TLDR;** that gives a very basic summary of results. 


Thank you!"
33023,optimize gfile for sequential reading,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.12 and 2.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
The current gfile only contain one buffer and most of behave is sequential reading, I think it is worthwhile to let gfile to contain double buffers to overlap reading and processing completely. I will make sure there is no performance regression even the handle is used for random access（seek or other interface）. The feature will be very useful especially when use tensorflow in cloud environment and IO has a very long latency.

**Will this change the current api? How?**
I maybe will introduce additonal optional argument to let user to tell if the handle will be used as sequential reading or random reading.

**Who will benefit with this feature?**
anyone who will use the gfile, and most of behave is sequential reading, and the read latency is a little long.

**Any Other info.**
I will prepare the change and send CR"
33022,No module named `tensorflow.data`,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution: Linux Ubuntu 18.04.3 using the official tf docker image (local: macOS High Sierra v10.13.6)
- TensorFlow installed from: Official docker image
- TensorFlow version: v2.0.0-rc2-26-g64c3d38 2.0.0
- Python version: 3.6.8

**Describe the current behavior**
`'import tensorflow.data'` yields `ModuleNotFoundError: No module named 'tensorflow.data'`.

**Describe the expected behavior**
Import the submodule as rc0 did.

**Code to reproduce the issue**
For the release candidate,
`docker run tensorflow/tensorflow:2.0.0rc0-gpu-py3 python -c 'import tensorflow.data'` works fine.
`docker run tensorflow/tensorflow:2.0.0-gpu-py3 python -c 'import tensorflow.data'` yields `ModuleNotFoundError: No module named 'tensorflow.data'`."
33020,Silent end of the estimator's train_and_evaluate method but exit code != 0,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): light adaptation in the estimator paradigm of the TensorFlow 2.0 [BERT pretraining code](https://github.com/tensorflow/models/tree/master/official/nlp)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0
- Python version: 3.6.8
- Bazel version (if compiling from source): None
- GCC/Compiler version (if compiling from source): None
- CUDA/cuDNN version: None
- GPU model and memory: only CPU for now

**Current behavior**
Running a BERT pretraining (but I don't think it is tied to my issue) in the TensorFlow estimator paradigm (using `train_and_evaluate` method), everything sounds to be ok, one checkpoint is written and script terminate without any apparent error. However, regarding at the final exit code, we realize that it is not 0 but the very generic Windows one `-1073741819 (0xC0000005)`.

**Describe the expected behavior**
If every thing really went well, the expected error code is logically zero.

**Other info / logs**
Just before crashing silently, in the cpp logs we have:
```
2019-10-01 13:13:14.853411: I .\tensorflow/core/common_runtime/executor.h:229] ExecutorBarrier finished with bad status: Invalid argument: NewRandomAccessFile failed to Create/Open: 
�
�
	input_ids��
�e��
�������:����g������gg�gf�����'g�1� �l��0g������g�Hg��E��������������u�x����5���������4�����g�x���f

```
Please find attached two log files of my failing scripts: one with only python logs and another one with cpp log level set to maximum verbosity.

[logs.zip](https://github.com/tensorflow/tensorflow/files/3686288/logs.zip)
"
33018,"UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?   'Discrepancy between trainable weights and collected trainable'","Please Check my process because I am trying to solve this issue from very long time. I tried every thing on google but it didn't help.

1) build_generator() and build_discriminator() are 2 functions
2)inputs = Input(shape=(MAX_SEQUENCE_LENGTH,lattice))
dis_model=build_discriminator(inputs)
dis_model.summary()
dis_model.compile(optimizer=Adam(0.0001, beta_1=0.5, beta_2=0.9))
n_disc_trainable = len(dis_model.trainable_weights)
print(""Discriminator"",n_disc_trainable)

3) gen_model=build_generator()
gen_model.trainable = False
gen_model.compile(optimizer=Adam(0.0001, beta_1=0.5, beta_2=0.9),)
n_gen_trainable = len(gen_model.trainable_weights)

4)update_data = np_utils.to_categorical(data[0:10],lattice)
real_samples = Input(shape=update_data.shape[1:])
generator_input_for_discriminator = Input(shape=(seed,))
generated_samples_for_discriminator = gen_model(generator_input_for_discriminator)
discriminator_output_from_generator = dis_model(generated_samples_for_discriminator)
discriminator_output_from_real_samples = dis_model(real_samples)

averaged_samples = RandomWeightedAverage()([real_samples,generated_samples_for_discriminator])
averaged_samples_out = dis_model(averaged_samples)


partial_gp_loss = partial(gradient_penalty_loss,
                          averaged_samples=averaged_samples,
                          gradient_penalty_weight=GRADIENT_PENALTY_WEIGHT)
# Functions need names or Keras will throw an error
partial_gp_loss.__name__ = 'gradient_penalty'

discriminator_model = Model(inputs=[real_samples,
                                    generator_input_for_discriminator],
                            outputs=[discriminator_output_from_real_samples,
                                     discriminator_output_from_generator,
averaged_samples_out])

discriminator_model.compile(optimizer=Adam(0.0001, beta_1=0.5, beta_2=0.9),
                            loss=[Wasserstein_loss,
                                  Wasserstein_loss,
partial_gp_loss],metrics=['accuracy'])
discriminator_model.summary()


5)# PREPARATION FOR GENERATOR MODEL
dis_model.trainable = False
dis_model.compile(optimizer=Adam(0.0001, beta_1=0.5, beta_2=0.9),loss=Wasserstein_loss,metrics=['accuracy'])

gen_model.trainable = True
gen_model.compile(optimizer=Adam(0.0001, beta_1=0.5, beta_2=0.9),loss=Wasserstein_loss,metrics=['accuracy'])

# COMPILE AND INITIALIZE GENERATOR MODEL
generator_input = Input(shape=(seed,))
generator_layers = gen_model(generator_input)
discriminator_layers_for_generator = dis_model(generator_layers)
generator_model = Model(inputs=[generator_input],
                        outputs=[discriminator_layers_for_generator])

generator_model.compile(optimizer=Adam(0.0001, beta_1=0.5, beta_2=0.9),
loss=Wasserstein_loss,metrics=['accuracy'])
generator_model.summary()

print(""BUILD DISCRIMINATOR MODEL TRAINABLE STATUS"",dis_model.trainable)
dis_model.trainable = True
dis_model.compile(optimizer=Adam(0.0001, beta_1=0.5, beta_2=0.9),loss=Wasserstein_loss,metrics=['accuracy'])

positive_y = np.ones((batch_size, 1), dtype=np.float32)
negative_y = -positive_y
dummy_y = np.zeros((batch_size, 1), dtype=np.float32)

6) Then a training loop  ( Train on batch - both discriminator_model and generator_model)

"
33016,Xla and memory allocation,"- TensorFlow version 1.14
- Python version: 2.7

I am defining this kind of graph:

    product_list = [(5, 6), ...] # list of pairs of integers
    data = tf.compat.v1.placeholder(dtype=tf.float64,
                                           name='input_tensor', shape=[1, 1000])
    x_i = tf.gather(data, [i[0] for i in product_list], axis=1)
    x_j = tf.gather(data, [i[1] for i in product_list], axis=1)
    res = x_i * x_j

    res = tf.concat([data, res], axis=-1)

and using `AOT` to compile an `so`.
Then calling the compiled function seems to take a lot of time.
So my question is : is there memory allocation at every call?
If yes, how can I avoid it by telling Tensorflow to pre-allocate the memory (as all the shapes are known in compile time) 
For example are there any commands that result in a new memory allocation at every call? (`tf.stack`, `tf.concat` etc..)

"
33014,tflite.allocate_tensors() fails after changing input size,"(using tensorflow 1.14.0)

I'm trying to use a tflite model to do inference on a batch. I use the following code:

```python
interpreter = tf.lite.Interpreter(model_path=model_path)
input_details = interpreter.get_input_details()
interpreter.resize_tensor_input(input_details[0][""index""], [batch_size, 513, 513, 3])
interpreter.allocate_tensors()
```

The code crashes and gives the following error:
```python
RuntimeError: tensorflow/lite/kernels/reshape.cc:58 num_input_elements != num_output_elements (1579014 != 789507)Node number 0 (RESHAPE) failed to prepare.
````

When looking at the output details, it still has the shape ```[1, 513, 513, output_channels]``` and not ```[batch_size, 513, 513, output_channels]``` as I would expect.

Any ideas?
"
33013,ERROR: Config value android_arm64 is not defined in any .rc file,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source
- TensorFlow version:1.15
- Python version:3.6
- Installed using virtualenv? pip? conda?:virtual.pip
- Bazel version (if compiling from source):0.26
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:





when run this:

bazel build -c opt \
  --config=android_arm64 \
  --cxxopt='--std=c++11' \
  //tensorflow/lite/tools/evaluation/tasks/coco_object_detection:run_eval


get error:

ERROR: Config value android_arm64 is not defined in any .rc file


how can I fix it?

"
33012,tf.function fails with tf.ragged.boolean_mask,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
- OS Platform and Distribution: Windows 10
- TensorFlow version: v2.0.0-rc2-26-g64c3d382ca 2.0.0
- Python version: 3.7.4
- CUDA/cuDNN version: None
- GPU model and memory: NO GPU

**Describe the current behavior**
A function containing `tf.ragged.boolean_mask` and decorated with `tf.function` works on first execution but fails when executed with different inputs. Setting `experimental_relax_shapes=True` does not help.

**Code to reproduce the issue**
```python
import tensorflow as tf

## Define some RaggedTensors
print(""`a2` and `b2` are ragged tensors with batch-length = 2"")
a2 = tf.ragged.constant([[1, 2, 3], [4, 5]], dtype=tf.float32, 
                       ragged_rank=1, inner_shape=tuple())
print(""a2 ="", a2)

b2 = tf.ragged.constant([[1], [2, 3]], dtype=tf.float32, 
                       ragged_rank=1, inner_shape=tuple())
print(""b2 ="", b2)

print(""`a3` and `b3` are ragged tensors with batch-length = 3"")
a3 = tf.ragged.constant([[1, 2, 3], [4, 5], [3]], dtype=tf.float32, 
                       ragged_rank=1, inner_shape=tuple())
print(""a3 ="", a3)

b3 = tf.ragged.constant([[1], [2, 3], [5]], dtype=tf.float32, 
                       ragged_rank=1, inner_shape=tuple())
print(""b3 ="", b3)

## Define a function
print(""We define a function `fun` with `tf.ragged.boolean_mask`."")
def fun(x):
    maximums = tf.reduce_max(x, axis=1)
    mask = maximums > 4
    selection = tf.ragged.boolean_mask(x, mask)
    return tf.reduce_sum(selection)

## Run the function in eager-mode
print(""Running in eager-mode."")
print(""fun(a2) ="", fun(a2))
print(""fun(a3) ="", fun(a3))
print(""fun(b2) ="", fun(b2))
print(""fun(b3) ="", fun(b3))

## Now running the same in graph-mode
fun = tf.function(fun, experimental_relax_shapes=True)

print(""Running in graph-mode."")
print(""fun(a2) ="", fun(a2))
print(""fun(a3) ="", fun(a3))
print(""fun(b2) ="", fun(b2))
print(""fun(b3) ="", fun(b3))
```
**Output of the code**
```
`a2` and `b2` are ragged tensors with batch-length = 2
2019-10-04 10:19:32.835325: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
a2 = <tf.RaggedTensor [[1.0, 2.0, 3.0], [4.0, 5.0]]>
b2 = <tf.RaggedTensor [[1.0], [2.0, 3.0]]>
`a3` and `b3` are ragged tensors with batch-length = 3
a3 = <tf.RaggedTensor [[1.0, 2.0, 3.0], [4.0, 5.0], [3.0]]>
b3 = <tf.RaggedTensor [[1.0], [2.0, 3.0], [5.0]]>
We define a function `fun` with `tf.ragged.boolean_mask`.
Running in eager-mode.
fun(a2) = tf.Tensor(9.0, shape=(), dtype=float32)
fun(a3) = tf.Tensor(9.0, shape=(), dtype=float32)
fun(b2) = tf.Tensor(0.0, shape=(), dtype=float32)
fun(b3) = tf.Tensor(5.0, shape=(), dtype=float32)
Running in graph-mode.
fun(a2) = tf.Tensor(9.0, shape=(), dtype=float32)
fun(a3) = tf.Tensor(9.0, shape=(), dtype=float32)
2019-10-04 10:19:33.187674: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: Input to reshape is a tensor with 3 values, but the requested shape has 5
         [[{{node RaggedMask/RaggedMask/boolean_mask/Reshape}}]]
Traceback (most recent call last):
  File ""Bug in tensorflow tf-function on ragged-boolean_mask.py"", line 43, in <module>
    print(""fun(b2) ="", fun(b2))
  File ""D:\python_projects\workon_hrab2\WPy64-3740\python-3.7.4.amd64\lib\site-packages\tensorflow_core\python\eager\def_function.py"", line 457, in __call__
    result = self._call(*args, **kwds)
  File ""D:\python_projects\workon_hrab2\WPy64-3740\python-3.7.4.amd64\lib\site-packages\tensorflow_core\python\eager\def_function.py"", line 494, in _call
    results = self._stateful_fn(*args, **kwds)
  File ""D:\python_projects\workon_hrab2\WPy64-3740\python-3.7.4.amd64\lib\site-packages\tensorflow_core\python\eager\function.py"", line 1823, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File ""D:\python_projects\workon_hrab2\WPy64-3740\python-3.7.4.amd64\lib\site-packages\tensorflow_core\python\eager\function.py"", line 1141, in _filtered_call
    self.captured_inputs)
  File ""D:\python_projects\workon_hrab2\WPy64-3740\python-3.7.4.amd64\lib\site-packages\tensorflow_core\python\eager\function.py"", line 1224, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager)
  File ""D:\python_projects\workon_hrab2\WPy64-3740\python-3.7.4.amd64\lib\site-packages\tensorflow_core\python\eager\function.py"", line 511, in call
    ctx=ctx)
  File ""D:\python_projects\workon_hrab2\WPy64-3740\python-3.7.4.amd64\lib\site-packages\tensorflow_core\python\eager\execute.py"", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError:  Input to reshape is a tensor with 3 values, but the requested shape has 5
         [[node RaggedMask/RaggedMask/boolean_mask/Reshape (defined at D:\python_projects\workon_hrab2\WPy64-3740\python-3.7.4.amd64\lib\site-packages\tensorflow_core\python\framework\ops.py:1751) ]] [Op:__inference_fun_1126]

Function call stack:
fun

```
"
33011,Using Tflite in Java for getting image as float array,"I have own tranied model and it works right(already implemented on Swift). My inputs for Tflite in Java - two float one-dimensional arrays with size 1*512*512*3 with values from 0.0 to 1.0. They are images and I have to insert them sequentially in Tflite and get new image as one-dimensional float array with the same size. The Tflite docs give only common info about input and output and tell that it should be either ByteBuffer or one-dimensional or multidimensional array of primitive. In examples and internet I have found the task with classification of objects from bitmap or I have found the example with multidimensional array as inputs and outputs, but it seems they are not my cases. The main problem is - I always get bad result because I definitely can’t figure out how to properly configure the inputs and outputs, because he documentation gives very little information and I do not see my case in all examples because I have two float one-dimensional arrays as inputs and float one-dimensional array as output as I said previously.

I try to implement in Java on Android Studio 3.4 on Ubuntu 18.04.

Note: Also I use C++ code in my project. And for set up image from float[] I convert float[] to opentCv Mat, after this I convert Mat into Bitmap. For all this I have method with name:

initializeSecond()
I have checked this method with other images as float[] and it works fine.

So I tried many cases and I will show the main of them:

1)

float masked[] = coreResult.getMasked();
            float inverted[] = coreResult.getInverted();
            float output[]= new float[1*512*512*3];
            Object[] inputs = new Object[]{masked, inverted};
            Map<Integer, Object> outputs = new HashMap();
            outputs.put(0, output);
            interpreter.runForMultipleInputsOutputs(inputs, outputs);
            initializeSecond(coreRequest, output);
2)

float masked[] = coreResult.getMasked();
            float inverted[] = coreResult.getInverted();
            float output[]= new float[1*512*512*3];
            ByteBuffer byteBufferMasked = ByteBuffer.allocateDirect(masked.length*4);
            byteBufferMasked.order(ByteOrder.nativeOrder());
            byteBufferMasked.asFloatBuffer().put(masked);

            ByteBuffer byteBufferInverted = ByteBuffer.allocateDirect(inverted.length*4);
            byteBufferInverted.order(ByteOrder.nativeOrder());
            byteBufferInverted.asFloatBuffer().put(inverted);
            Object[] inputs = new Object[]{byteBufferMasked, byteBufferInverted};
            Map<Integer, Object> outputs = new HashMap();
            outputs.put(0, output);
            interpreter.runForMultipleInputsOutputs(inputs, outputs);
            initializeSecond(coreRequest, output);
3)

float masked[] = coreResult.getMasked();
            float inverted[] = coreResult.getInverted();
            float output[]= new float[1*512*512*3];
            ByteBuffer byteBufferOutput = ByteBuffer.allocateDirect(output.length*4);
            byteBufferOutput.order(ByteOrder.nativeOrder());
            ByteBuffer byteBufferMasked = ByteBuffer.allocateDirect(masked.length*4);
            byteBufferMasked.order(ByteOrder.nativeOrder());
            byteBufferMasked.asFloatBuffer().put(masked);

            ByteBuffer byteBufferInverted = ByteBuffer.allocateDirect(inverted.length*4);
            byteBufferInverted.order(ByteOrder.nativeOrder());
            byteBufferInverted.asFloatBuffer().put(inverted);
            Object[] inputs = new Object[]{byteBufferMasked, byteBufferInverted};
            Map<Integer, Object> outputs = new HashMap();
            outputs.put(0, byteBufferOutput);
            interpreter.runForMultipleInputsOutputs(inputs, outputs);
            output = byteBufferOutput.asFloatBuffer().array();
            initializeSecond(coreRequest, output);
4) I put in code from 3 case

for (int i = 0; i < output.length; i++) {
                output[i] = byteBufferOutput.getFloat(i);
            }
instead

 output = byteBufferOutput.asFloatBuffer().array();
The results of cases:

1) Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/conv.cc:235 input->dims->size != 4 (1 != 4) Node number 2 (CONV_2D) failed to prepare.

2) Cannot copy between a TensorFlowLite tensor with shape [1, 512, 512, 3] and a Java object with shape [786432].

3) java.lang.UnsupportedOperationException. It is the result of byteBufferOutput.asFloatBuffer().array(). It seems I can not get the array by this

4) I have not errors, but it works very slowly and I have the bad result: image with black background and colorful dots"
33009,Memory leak ,"**System information**
**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):** Yes, see below
**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):** Ubuntu 18.04 (also demonstrated in Windows 7)
**- TensorFlow installed from (source or binary):** Binary
**- TensorFlow version (use command below):** v2.0.0-rc2-26-g64c3d38 2.0.0 (problem disappears on v1.14.0-rc1-22-gaf24dc91b5 1.14.0)
**- Python version:** 3.6.8
**- CUDA/cuDNN version:** Cuda v10.0, CuDNN v7.6.2.24
**- GPU model and memory:** Nvidia GeForce 840M, but problem persists in non-GPU version

**Describe the current behavior**
When creating a trivially simple model and then entering a loop that calls predict() with dummy input, memory consumption increases indefinitely over time. On my system, a model with a single hidden layer of only 32 nodes will consume all available system RAM (>10gb) after only 10 minutes. The problem happens on v2.0 (GPU or CPU) of tensorflow, but disappears when running identical code on v1.14.

**Describe the expected behavior**
Expect memory consumption to quickly stabilize but it never does.

**Code to reproduce the issue**

```
from tensorflow.keras import Input, Model
from tensorflow.keras.layers import Dense
import numpy as np

# Build model
In = Input(shape=(10,))
x = Dense(32)(In)
Out = Dense(2)(x)

# Compile
model = Model(inputs=In, outputs=Out)
model.compile(optimizer='adam', loss='mse')        

# Create dummy input data
fake_data = np.random.uniform(low=0, high=1.0, size=(1, 10, ))

while True:
    # Repeatedly predict:
    model.predict(fake_data) # No memory leak if this line is replaced with ""pass""
```"
33007,deepspeech running issue,"![image](https://user-images.githubusercontent.com/36496141/66105607-bd81a980-e570-11e9-8b01-77e08a7f27d4.png)
Facing this issue,any help would be appreciated.
Thanks"
33006,Stop inheriting `tf.keras.Model` to build custom layers in tutorials and guides.,"## URL(s) with the issue:
- https://www.tensorflow.org/tutorials/text/nmt_with_attention
`Encoder`, `BahdanauAttention` and `Decoder` inherit `tf.keras.Model`.
- https://www.tensorflow.org/guide/eager#variables_and_optimizers
Model does not need to be a subclass of `tf.keras.Model` because we don't use `Model`'s utility methods.
- https://www.tensorflow.org/tutorials/customization/custom_layers#models_composing_layers
  > The main class used when creating a layer-like thing which contains other layers is tf.keras.Model. Implementing one is done by inheriting from tf.keras.Model.

  This is not accurate since TF 1.13 (and standalone keras 2.3.0)

There are other tutorials which use `tf.keras.Model` when `tf.keras.layers.Layer` is sufficient.

## Description of issue (what needs changing):

We should stop inheriting `tf.keras.Model` in tutorials when we don't use utility functions of `Model`.

### Clear description

Since tf1.13 and [standalone keras 2.3.0](https://github.com/keras-team/keras/releases/tag/2.3.0), *Layers set as attributes of a Layer are now tracked*.
Also, [Writing custom layers and models with Keras](https://www.tensorflow.org/guide/keras/custom_layers_and_models) says *A Model is just like a Layer, but with added training and serialization utilities.*

We don't need to inherit `Model` unless we use ""utility methods"" of `Model`. I think the idea like *""Always extend Model because Model has more features""* is not correct because utilities of Model work only with special subsets of Layer ([Layers whose call receive only one input](https://github.com/tensorflow/tensorflow/blob/c8ef33dd913463ced8cc347c03945a88b34da7f8/tensorflow/python/keras/engine/training.py#L1461)).

Thus, I think we should stop inheriting `tf.keras.Model` in tutorials when `tf.keras.layers.Layer` is enough.

My original question on stackoverflow as a context of this bug:
https://stackoverflow.com/questions/58118334/when-should-we-inherits-keras-model-instead-of-keras-layers-layer-even-if-we-don
### Submit a pull request?

I'm sending a pull requests to fix them.
"
33005,"Will DepthwiseConv2D support non-square Strides in the future? Ex: `strides=(1,3)`","<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 
2.0.0, 1.14.0
- Are you willing to contribute it (Yes/No): 
No, sorry, that is well outside my expertise. 


**Describe the feature and the current behavior/state.**
Will DepthwiseConv2D support non-square Strides in the future? 
This would make it consistent with the majority of other layers that accept the strides argument and can take non-square strides. Ex: `strides=(1,3)`

Or at minimum, please update the documentation to specify this limitation.
https://www.tensorflow.org/api_docs/python/tf/keras/layers/DepthwiseConv2D
""strides: An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width.""


depthwise_conv_op
""Current implementation only supports equal length strides in the row and column dimensions.""
https://github.com/tensorflow/tensorflow/blob/242a42139513aa5903743c37c0ef15ed00f93fed/tensorflow/core/kernels/depthwise_conv_op.cc#L288


**Will this change the current api? How?**
No, there is already a strides option that should be able to take non-square shaped strides. This would actually make it more consistent with the rest of the API.


**Who will benefit with this feature?**
Bob will.


**Any Other info.**
I'm using the `tf.keras.layers.DepthwiseConv2D`  implementation.

2.0.0:
`tensorflow.python.framework.errors_impl.InvalidArgumentError: Current implementation only supports equal length strides in the row and column dimensions. [Op:DepthwiseConv2dNative] name: model/depthwise_conv2d/depthwise/`

1.14.0:
`tensorflow.python.framework.errors_impl.InvalidArgumentError: Current implementation only supports equal length strides in the row and column dimensions.
         [[{{node depthwise_conv2d/depthwise}}]]`


Simple example:
`tf.keras.layers.DepthwiseConv2D((3, 3), strides=(1,2))`"
33003,Loss of the model ran with TF2.0 is much higher compared to 1.x,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0 GPU
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1
- GPU model and memory: GeForce RTX 2080 Ti

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
After migrating to TF 2.0 I tried to run the exact same model and got much worse results

**Describe the expected behavior**
The results should be the same as in TF1.x
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

Below is the model definition:

```
import os
os.environ['TF_KERAS'] = '1'

from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint
from heatmaps import *
import numpy as np
from keras_radam import RAdam

image_size = 200
## output shape is the same as input
n = 32 * 5
nClasses = 6
nfmp_block1 = 64
nfmp_block2 = 128
batch_size = 64

IMAGE_ORDERING = ""channels_last""
img_input = tf.keras.Input(shape=(image_size, image_size, 3))

# Encoder Block 1
x = Conv2D(nfmp_block1, (3, 3), activation='relu', padding='same', name='block1_conv1', data_format=IMAGE_ORDERING)(
    img_input)
x = Conv2D(nfmp_block1, (3, 3), activation='relu', padding='same', name='block1_conv2', data_format=IMAGE_ORDERING)(x)
block1 = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool', data_format=IMAGE_ORDERING)(x)

# Encoder Block 2
x = Conv2D(nfmp_block2, (3, 3), activation='relu', padding='same', name='block2_conv1', data_format=IMAGE_ORDERING)(
    block1)
x = Conv2D(nfmp_block2, (3, 3), activation='relu', padding='same', name='block2_conv2', data_format=IMAGE_ORDERING)(x)
x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool', data_format=IMAGE_ORDERING)(x)

## bottleneck
o = (Conv2D(n, (int(image_size / 4), int(image_size / 4)),
            activation='relu', padding='same', name=""bottleneck_1"", data_format=IMAGE_ORDERING))(x)
o = (Conv2D(n, (1, 1), activation='relu', padding='same', name=""bottleneck_2"", data_format=IMAGE_ORDERING))(o)

## Decoder Block
## upsampling to bring the feature map size to be the same as the input image i.e., heatmap size
output = Conv2DTranspose(nClasses, kernel_size=(4, 4), strides=(4, 4), use_bias=False, name='upsample_2',
                         data_format=IMAGE_ORDERING)(o)

## Reshaping is necessary to use sample_weight_mode=""temporal"" which assumes 3 dimensional output shape
## See below for the discussion of weights
output = Reshape((image_size * image_size * nClasses, 1))(output)
model = tf.keras.Model(img_input, output)
model.summary()

radam = RAdam(total_steps=10000, warmup_proportion=0.1, min_lr=1e-5)
model.compile(optimizer=radam, loss='mse', sample_weight_mode=""temporal"")

data_folder = 'data'
id2filename, filename2id, annotated_images = dataloader.get_image_annotations(data_folder)
df = dataloader.get_annotation_dataframe(id2filename, annotated_images)
msk = np.random.rand(len(df)) < 0.8
train = df[msk]
test = df[~msk]
encoding = MultiPointHeatmapEncoding(image_size, df, batch_size=64)

model_name = 'stacked_hourglass_tf2'
log_dir = ""logs/{}"".format(model_name)
model_filename = ""saved-models/{}.h5"".format(model_name)

train_gen = encoding.generator(train, batch_size)
test_gen = encoding.generator(test, batch_size, get_weights=True)

steps_per_epoch = len(train) // batch_size
validation_steps = len(test) // batch_size
if validation_steps == 0:
    validation_steps = 1
if steps_per_epoch == 0:
    steps_per_epoch = 1

cb_tensorboard = TensorBoard(log_dir=log_dir)
callback_save_images = CallbackHeatmapOutput(model, get_generator(test_gen), log_dir, encoding)
checkpoint = ModelCheckpoint(model_filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')

history = model.fit_generator(
            get_generator(train_gen),
            validation_data=get_generator(test_gen),
            steps_per_epoch=steps_per_epoch,
            epochs=5000,
            validation_steps=validation_steps,
            verbose=2,
            use_multiprocessing=True,
            callbacks=[checkpoint, callback_save_images, cb_tensorboard]
        )

```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

output log while running the epochs:

> 2019-10-03 00:16:43.397431: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
> 2019-10-03 00:16:45.096352: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
> Relying on driver to perform ptx compilation. This message will be only logged once.
> 2019-10-03 00:17:16.303969: I tensorflow/core/profiler/lib/profiler_session.cc:184] Profiler session started.
> 2019-10-03 00:17:16.304314: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcupti.so.10.0'; dlerror: libcupti.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64
> 2019-10-03 00:17:16.304328: W tensorflow/core/profiler/lib/profiler_session.cc:192] Encountered error while starting profiler: Unavailable: CUPTI error: CUPTI could not be loaded or symbol could not be found.
> 2019-10-03 00:17:17.242794: I tensorflow/core/platform/default/device_tracer.cc:588] Collecting 0 kernel records, 0 memcpy records.
> 2019-10-03 00:17:17.276983: E tensorflow/core/platform/default/device_tracer.cc:70] CUPTI error: CUPTI could not be loaded or symbol could not be found.
"
33002,tensorflow-gpu CUPTI errors,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 18.04):
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.0
- Python version:3
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1
- GPU model and memory: GeForce RTX 2080 Ti 



**Describe the problem**
I upgraded TensorFlow from 1.x to 2.0 and tried to run the same model as I successfully ran with  TF 1.x.

Previously, I already had NVIDIA drivers and CUDA toolkit installed, and therefore I just installed tensorflow-gpu in new virtual environment. Upon running, I got the following output with errors/warnings:

> 2019-10-03 00:16:43.397431: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
> 2019-10-03 00:16:45.096352: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
> Relying on driver to perform ptx compilation. This message will be only logged once.
> 2019-10-03 00:17:16.303969: I tensorflow/core/profiler/lib/profiler_session.cc:184] Profiler session started.
> 2019-10-03 00:17:16.304314: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcupti.so.10.0'; dlerror: libcupti.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64
> 2019-10-03 00:17:16.304328: W tensorflow/core/profiler/lib/profiler_session.cc:192] Encountered error while starting profiler: Unavailable: CUPTI error: CUPTI could not be loaded or symbol could not be found.
> 2019-10-03 00:17:17.242794: I tensorflow/core/platform/default/device_tracer.cc:588] Collecting 0 kernel records, 0 memcpy records.
> 2019-10-03 00:17:17.276983: E tensorflow/core/platform/default/device_tracer.cc:70] CUPTI error: CUPTI could not be loaded or symbol could not be found.

In order to fix these warnings, I ran the NVIDIA/CUDA-related commands, listed on tensorflow-gpu installation page:

# Add NVIDIA package repositories
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.0.130-1_amd64.deb
sudo dpkg -i cuda-repo-ubuntu1804_10.0.130-1_amd64.deb
sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub
sudo apt-get update
wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb
sudo apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb
sudo apt-get update

# Install NVIDIA driver
sudo apt-get install --no-install-recommends nvidia-driver-418
# Reboot. Check that GPUs are visible using the command: nvidia-smi

# Install development and runtime libraries (~4GB)
sudo apt-get install --no-install-recommends \
    cuda-10-0 \
    libcudnn7=7.6.2.24-1+cuda10.0  \
    libcudnn7-dev=7.6.2.24-1+cuda10.0


# Install TensorRT. Requires that libcudnn7 is installed above.
sudo apt-get install -y --no-install-recommends libnvinfer5=5.1.5-1+cuda10.0 \
    libnvinfer-dev=5.1.5-1+cuda10.0

after runnug this, I got the following errir:

> : Version '7.6.2.24-1+cuda10.1' for 'libcudnn7' was not found
> E: Unable to locate package libcudnn7-dev

nvidia-smi outputs the following:

+-----------------------------------------------------------------------------+
| NVIDIA-SMI 430.50       Driver Version: 430.50       CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce RTX 208...  Off  | 00000000:01:00.0  On |                  N/A |
|  0%   40C    P8     4W / 260W |  11012MiB / 11019MiB |      2%      Default |
+-------------------------------+----------------------+----------------------+


**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Actually the biggest issue, that might not be related to this error is that the performance of the exact same model is much worse when trying to execute using TF2.0 and am not sure whether these errors might be the cause, although I doubt it. I guess I should open another issue for this, but am not sure which category is most suitable for this issue.
"
33001,ERROR: C:/tensorflow/tensorflow/compiler/xla/service/gpu/BUILD:1314:1: C++ compilation of rule '//tensorflow/compiler/xla/service/gpu:cudnn_fused_conv_rewriter' failed (Exit 2): python.exe failed: error executing command,"I'm trying to build Tensorflow 2.0 from source on my RTX 2070 machine. I get this error towards the end of the compilation process (https://pastebin.com/zuuQqZTb):

```
ERROR: C:/tensorflow/tensorflow/compiler/xla/service/gpu/BUILD:1314:1: C++ compilation of rule '//tensorflow/compiler/xla/service/gpu:cudnn_fused_conv_rewriter' failed (Exit 2): python.exe failed: error executing command
  cd C:/users/qrabbani/_bazel_qrabbani/xv6zejqw/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.23.28105\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.23.28105\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\cppwinrt
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.23.28105\ATLMFC\lib\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.23.28105\lib\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.23.28105\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\\x64;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\;C:\Program Files (x86)\Windows Kits\10\bin\10.0.18362.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\\MSBuild\Current\Bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\Tools\;;C:\Windows\system32;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/QRabbani/Anaconda3/envs/tf_gpu/python.exe
    SET PYTHON_LIB_PATH=C:/Users/QRabbani/Anaconda3/envs/tf_gpu/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\QRabbani\AppData\Local\Temp
    SET TF_CONFIGURE_IOS=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0
    SET TF_NEED_CUDA=1
    SET TMP=C:\Users\QRabbani\AppData\Local\Temp
  C:/Users/QRabbani/Anaconda3/envs/tf_gpu/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_protobuf /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-opt/bin/external/jpeg /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/local_config_tensorrt /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt /Iexternal/llvm /Ibazel-out/x64_windows-opt/bin/external/llvm /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers /Iexternal/com_google_protobuf/src /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf/src /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/bin/external/gif_archive /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-opt/bin/external/gif_archive/windows /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/llvm/lib/IR /Ibazel-out/x64_windows-opt/bin/external/llvm/lib/IR /Iexternal/llvm/include/llvm/IR /Ibazel-out/x64_windows-opt/bin/external/llvm/include/llvm/IR /Iexternal/llvm/include /Ibazel-out/x64_windows-opt/bin/external/llvm/include /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /DTF_USE_SNAPPY /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_CRT_NONSTDC_NO_DEPRECATE /D_CRT_NONSTDC_NO_WARNINGS /D_SCL_SECURE_NO_DEPRECATE /D_SCL_SECURE_NO_WARNINGS /DUNICODE /D_UNICODE /DLLVM_ENABLE_STATS /D__STDC_LIMIT_MACROS /D__STDC_CONSTANT_MACROS /D__STDC_FORMAT_MACROS /DLLVM_BUILD_GLOBAL_ISEL /showIncludes /MD /O2 /DNDEBUG -w -DWIN32_LEAN_AND_MEAN -DNOGDI /arch:AVX /Fobazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/service/gpu/_objs/cudnn_fused_conv_rewriter/cudnn_fused_conv_rewriter.o /c tensorflow/compiler/xla/service/gpu/cudnn_fused_conv_rewriter.cc
Execution platform: @bazel_tools//platforms:host_platform
.\tensorflow/compiler/xla/service/pattern_matcher.h(499): error C2672: 'xla::match::detail::LayoutPattern<const xla::Layout,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl': no matching overloaded function found
.\tensorflow/compiler/xla/service/pattern_matcher.h(645): note: see reference to class template instantiation 'xla::match::detail::LayoutPattern<const xla::Layout,xla::match::detail::LayoutPatternBaseImpl>' being compiled
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.23.28105\include\xtr1common(159): note: see reference to class template instantiation 'std::integral_constant<bool,false>' being compiled
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.23.28105\include\xtr1common(159): note: see reference to class template instantiation 'std::disjunction<_Traits...>' being compiled
.\tensorflow/compiler/xla/service/pattern_matcher.h(499): error C2893: Failed to specialize function template 'xla::match::detail::LayoutPattern<LayoutType,unknown-type> xla::match::detail::LayoutPattern<LayoutType,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl(NewImpl) const'
        with
        [
            LayoutType=const xla::Layout
        ]
.\tensorflow/compiler/xla/service/pattern_matcher.h(456): note: see declaration of 'xla::match::detail::LayoutPattern<const xla::Layout,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(499): note: With the following template arguments:
.\tensorflow/compiler/xla/service/pattern_matcher.h(499): note: 'NewImpl=xla::match::detail::LayoutPatternEqualImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(505): error C2672: 'xla::match::detail::LayoutPattern<const xla::Layout,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl': no matching overloaded function found
.\tensorflow/compiler/xla/service/pattern_matcher.h(505): error C2893: Failed to specialize function template 'xla::match::detail::LayoutPattern<LayoutType,unknown-type> xla::match::detail::LayoutPattern<LayoutType,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl(NewImpl) const'
        with
        [
            LayoutType=const xla::Layout
        ]
.\tensorflow/compiler/xla/service/pattern_matcher.h(456): note: see declaration of 'xla::match::detail::LayoutPattern<const xla::Layout,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(505): note: With the following template arguments:
.\tensorflow/compiler/xla/service/pattern_matcher.h(505): note: 'NewImpl=xla::match::detail::LayoutPatternFormatImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(511): error C2672: 'xla::match::detail::LayoutPattern<const xla::Layout,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl': no matching overloaded function found
.\tensorflow/compiler/xla/service/pattern_matcher.h(511): error C2893: Failed to specialize function template 'xla::match::detail::LayoutPattern<LayoutType,unknown-type> xla::match::detail::LayoutPattern<LayoutType,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl(NewImpl) const'
        with
        [
            LayoutType=const xla::Layout
        ]
.\tensorflow/compiler/xla/service/pattern_matcher.h(456): note: see declaration of 'xla::match::detail::LayoutPattern<const xla::Layout,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(511): note: With the following template arguments:
.\tensorflow/compiler/xla/service/pattern_matcher.h(511): note: 'NewImpl=xla::match::detail::LayoutPatternFormatImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(499): error C2672: 'xla::match::detail::LayoutPattern<xla::Layout,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl': no matching overloaded function found
.\tensorflow/compiler/xla/service/pattern_matcher.h(655): note: see reference to class template instantiation 'xla::match::detail::LayoutPattern<xla::Layout,xla::match::detail::LayoutPatternBaseImpl>' being compiled
.\tensorflow/compiler/xla/service/pattern_matcher.h(499): error C2893: Failed to specialize function template 'xla::match::detail::LayoutPattern<LayoutType,unknown-type> xla::match::detail::LayoutPattern<LayoutType,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl(NewImpl) const'
        with
        [
            LayoutType=xla::Layout
        ]
.\tensorflow/compiler/xla/service/pattern_matcher.h(456): note: see declaration of 'xla::match::detail::LayoutPattern<xla::Layout,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(499): note: With the following template arguments:
.\tensorflow/compiler/xla/service/pattern_matcher.h(499): note: 'NewImpl=xla::match::detail::LayoutPatternEqualImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(505): error C2672: 'xla::match::detail::LayoutPattern<xla::Layout,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl': no matching overloaded function found
.\tensorflow/compiler/xla/service/pattern_matcher.h(505): error C2893: Failed to specialize function template 'xla::match::detail::LayoutPattern<LayoutType,unknown-type> xla::match::detail::LayoutPattern<LayoutType,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl(NewImpl) const'
        with
        [
            LayoutType=xla::Layout
        ]
.\tensorflow/compiler/xla/service/pattern_matcher.h(456): note: see declaration of 'xla::match::detail::LayoutPattern<xla::Layout,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(505): note: With the following template arguments:
.\tensorflow/compiler/xla/service/pattern_matcher.h(505): note: 'NewImpl=xla::match::detail::LayoutPatternFormatImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(511): error C2672: 'xla::match::detail::LayoutPattern<xla::Layout,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl': no matching overloaded function found
.\tensorflow/compiler/xla/service/pattern_matcher.h(511): error C2893: Failed to specialize function template 'xla::match::detail::LayoutPattern<LayoutType,unknown-type> xla::match::detail::LayoutPattern<LayoutType,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl(NewImpl) const'
        with
        [
            LayoutType=xla::Layout
        ]
.\tensorflow/compiler/xla/service/pattern_matcher.h(456): note: see declaration of 'xla::match::detail::LayoutPattern<xla::Layout,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(511): note: With the following template arguments:
.\tensorflow/compiler/xla/service/pattern_matcher.h(511): note: 'NewImpl=xla::match::detail::LayoutPatternFormatImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(997): error C2672: 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl': no matching overloaded function found
.\tensorflow/compiler/xla/service/pattern_matcher.h(1116): note: see reference to class template instantiation 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>' being compiled
.\tensorflow/compiler/xla/service/pattern_matcher.h(997): error C2893: Failed to specialize function template 'xla::match::detail::ShapePattern<ShapeType,unknown-type> xla::match::detail::ShapePattern<ShapeType,xla::match::detail::ShapePatternBaseImpl>::AppendImpl(NewImpl) const'
        with
        [
            ShapeType=const xla::Shape
        ]
.\tensorflow/compiler/xla/service/pattern_matcher.h(948): note: see declaration of 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(997): note: With the following template arguments:
.\tensorflow/compiler/xla/service/pattern_matcher.h(997): note: 'NewImpl=xla::match::detail::ShapePatternEqualImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1004): error C2672: 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl': no matching overloaded function found
.\tensorflow/compiler/xla/service/pattern_matcher.h(1004): error C2893: Failed to specialize function template 'xla::match::detail::ShapePattern<ShapeType,unknown-type> xla::match::detail::ShapePattern<ShapeType,xla::match::detail::ShapePatternBaseImpl>::AppendImpl(NewImpl) const'
        with
        [
            ShapeType=const xla::Shape
        ]
.\tensorflow/compiler/xla/service/pattern_matcher.h(948): note: see declaration of 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1004): note: With the following template arguments:
.\tensorflow/compiler/xla/service/pattern_matcher.h(1004): note: 'NewImpl=xla::match::detail::ShapePatternCompatibleImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1010): error C2672: 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl': no matching overloaded function found
.\tensorflow/compiler/xla/service/pattern_matcher.h(1010): error C2893: Failed to specialize function template 'xla::match::detail::ShapePattern<ShapeType,unknown-type> xla::match::detail::ShapePattern<ShapeType,xla::match::detail::ShapePatternBaseImpl>::AppendImpl(NewImpl) const'
        with
        [
            ShapeType=const xla::Shape
        ]
.\tensorflow/compiler/xla/service/pattern_matcher.h(948): note: see declaration of 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1010): note: With the following template arguments:
.\tensorflow/compiler/xla/service/pattern_matcher.h(1010): note: 'NewImpl=xla::match::detail::ShapePatternElementTypeImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1016): error C2672: 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl': no matching overloaded function found
.\tensorflow/compiler/xla/service/pattern_matcher.h(1016): error C2893: Failed to specialize function template 'xla::match::detail::ShapePattern<ShapeType,unknown-type> xla::match::detail::ShapePattern<ShapeType,xla::match::detail::ShapePatternBaseImpl>::AppendImpl(NewImpl) const'
        with
        [
            ShapeType=const xla::Shape
        ]
.\tensorflow/compiler/xla/service/pattern_matcher.h(948): note: see declaration of 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1016): note: With the following template arguments:
.\tensorflow/compiler/xla/service/pattern_matcher.h(1016): note: 'NewImpl=xla::match::detail::ShapePatternIsScalarImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1022): error C2672: 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl': no matching overloaded function found
.\tensorflow/compiler/xla/service/pattern_matcher.h(1022): error C2893: Failed to specialize function template 'xla::match::detail::ShapePattern<ShapeType,unknown-type> xla::match::detail::ShapePattern<ShapeType,xla::match::detail::ShapePatternBaseImpl>::AppendImpl(NewImpl) const'
        with
        [
            ShapeType=const xla::Shape
        ]
.\tensorflow/compiler/xla/service/pattern_matcher.h(948): note: see declaration of 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1022): note: With the following template arguments:
.\tensorflow/compiler/xla/service/pattern_matcher.h(1022): note: 'NewImpl=xla::match::detail::ShapePatternIsArrayImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1028): error C2672: 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl': no matching overloaded function found
.\tensorflow/compiler/xla/service/pattern_matcher.h(1028): error C2893: Failed to specialize function template 'xla::match::detail::ShapePattern<ShapeType,unknown-type> xla::match::detail::ShapePattern<ShapeType,xla::match::detail::ShapePatternBaseImpl>::AppendImpl(NewImpl) const'
        with
        [
            ShapeType=const xla::Shape
        ]
.\tensorflow/compiler/xla/service/pattern_matcher.h(948): note: see declaration of 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1028): note: With the following template arguments:
.\tensorflow/compiler/xla/service/pattern_matcher.h(1028): note: 'NewImpl=xla::match::detail::ShapePatternIsTupleImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1033): error C2672: 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl': no matching overloaded function found
.\tensorflow/compiler/xla/service/pattern_matcher.h(1033): error C2893: Failed to specialize function template 'xla::match::detail::ShapePattern<ShapeType,unknown-type> xla::match::detail::ShapePattern<ShapeType,xla::match::detail::ShapePatternBaseImpl>::AppendImpl(NewImpl) const'
        with
        [
            ShapeType=const xla::Shape
        ]
.\tensorflow/compiler/xla/service/pattern_matcher.h(948): note: see declaration of 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1033): note: With the following template arguments:
.\tensorflow/compiler/xla/service/pattern_matcher.h(1033): note: 'NewImpl=xla::match::detail::ShapePatternEffectiveScalarImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1039): error C2672: 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl': no matching overloaded function found
.\tensorflow/compiler/xla/service/pattern_matcher.h(1039): error C2893: Failed to specialize function template 'xla::match::detail::ShapePattern<ShapeType,unknown-type> xla::match::detail::ShapePattern<ShapeType,xla::match::detail::ShapePatternBaseImpl>::AppendImpl(NewImpl) const'
        with
        [
            ShapeType=const xla::Shape
        ]
.\tensorflow/compiler/xla/service/pattern_matcher.h(948): note: see declaration of 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1039): note: With the following template arguments:
.\tensorflow/compiler/xla/service/pattern_matcher.h(1039): note: 'NewImpl=xla::match::detail::ShapePatternRankImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1053): error C2672: 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout': no matching overloaded function found
.\tensorflow/compiler/xla/service/pattern_matcher.h(1053): error C2784: 'unknown-type xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout(const xla::match::detail::LayoutPattern<LayoutType,LayoutImpl> &) const': could not deduce template argument for 'const xla::match::detail::LayoutPattern<LayoutType,LayoutImpl> &' from 'unknown-type'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1046): note: see declaration of 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1058): error C2672: 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout': no matching overloaded function found
.\tensorflow/compiler/xla/service/pattern_matcher.h(1058): error C2784: 'unknown-type xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout(const xla::match::detail::LayoutPattern<LayoutType,LayoutImpl> &) const': could not deduce template argument for 'const xla::match::detail::LayoutPattern<LayoutType,LayoutImpl> &' from 'unknown-type'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1046): note: see declaration of 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1063): error C2672: 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout': no matching overloaded function found
.\tensorflow/compiler/xla/service/pattern_matcher.h(1063): error C2784: 'unknown-type xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout(const xla::match::detail::LayoutPattern<LayoutType,LayoutImpl> &) const': could not deduce template argument for 'const xla::match::detail::LayoutPattern<LayoutType,LayoutImpl> &' from 'unknown-type'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1046): note: see declaration of 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1080): error C2923: 'xla::match::detail::AllOfPattern': 'xla::match::Shape' is not a valid template type argument for parameter 'Item'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1116): note: see declaration of 'xla::match::Shape'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1080): error C3203: 'AllOfPattern': unspecialized class template can't be used as a template argument for template parameter 'Impl', expected a real type
.\tensorflow/compiler/xla/service/pattern_matcher.h(1093): error C2923: 'xla::match::detail::AllOfPattern': 'xla::match::Shape' is not a valid template type argument for parameter 'Item'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1116): note: see declaration of 'xla::match::Shape'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1093): error C3203: 'AllOfPattern': unspecialized class template can't be used as a template argument for template parameter 'Impl', expected a real type
.\tensorflow/compiler/xla/service/pattern_matcher.h(997): error C2672: 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl': no matching overloaded function found
.\tensorflow/compiler/xla/service/pattern_matcher.h(1124): note: see reference to class template instantiation 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>' being compiled
.\tensorflow/compiler/xla/service/pattern_matcher.h(997): error C2893: Failed to specialize function template 'xla::match::detail::ShapePattern<ShapeType,unknown-type> xla::match::detail::ShapePattern<ShapeType,xla::match::detail::ShapePatternBaseImpl>::AppendImpl(NewImpl) const'
        with
        [
            ShapeType=xla::Shape
        ]
.\tensorflow/compiler/xla/service/pattern_matcher.h(948): note: see declaration of 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(997): note: With the following template arguments:
.\tensorflow/compiler/xla/service/pattern_matcher.h(997): note: 'NewImpl=xla::match::detail::ShapePatternEqualImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1004): error C2672: 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl': no matching overloaded function found
.\tensorflow/compiler/xla/service/pattern_matcher.h(1004): error C2893: Failed to specialize function template 'xla::match::detail::ShapePattern<ShapeType,unknown-type> xla::match::detail::ShapePattern<ShapeType,xla::match::detail::ShapePatternBaseImpl>::AppendImpl(NewImpl) const'
        with
        [
            ShapeType=xla::Shape
        ]
.\tensorflow/compiler/xla/service/pattern_matcher.h(948): note: see declaration of 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1004): note: With the following template arguments:
.\tensorflow/compiler/xla/service/pattern_matcher.h(1004): note: 'NewImpl=xla::match::detail::ShapePatternCompatibleImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1010): error C2672: 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl': no matching overloaded function found
.\tensorflow/compiler/xla/service/pattern_matcher.h(1010): error C2893: Failed to specialize function template 'xla::match::detail::ShapePattern<ShapeType,unknown-type> xla::match::detail::ShapePattern<ShapeType,xla::match::detail::ShapePatternBaseImpl>::AppendImpl(NewImpl) const'
        with
        [
            ShapeType=xla::Shape
        ]
.\tensorflow/compiler/xla/service/pattern_matcher.h(948): note: see declaration of 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1010): note: With the following template arguments:
.\tensorflow/compiler/xla/service/pattern_matcher.h(1010): note: 'NewImpl=xla::match::detail::ShapePatternElementTypeImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1016): error C2672: 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl': no matching overloaded function found
.\tensorflow/compiler/xla/service/pattern_matcher.h(1016): error C2893: Failed to specialize function template 'xla::match::detail::ShapePattern<ShapeType,unknown-type> xla::match::detail::ShapePattern<ShapeType,xla::match::detail::ShapePatternBaseImpl>::AppendImpl(NewImpl) const'
        with
        [
            ShapeType=xla::Shape
        ]
.\tensorflow/compiler/xla/service/pattern_matcher.h(948): note: see declaration of 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1016): note: With the following template arguments:
.\tensorflow/compiler/xla/service/pattern_matcher.h(1016): note: 'NewImpl=xla::match::detail::ShapePatternIsScalarImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1022): error C2672: 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl': no matching overloaded function found
.\tensorflow/compiler/xla/service/pattern_matcher.h(1022): error C2893: Failed to specialize function template 'xla::match::detail::ShapePattern<ShapeType,unknown-type> xla::match::detail::ShapePattern<ShapeType,xla::match::detail::ShapePatternBaseImpl>::AppendImpl(NewImpl) const'
        with
        [
            ShapeType=xla::Shape
        ]
.\tensorflow/compiler/xla/service/pattern_matcher.h(948): note: see declaration of 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1022): note: With the following template arguments:
.\tensorflow/compiler/xla/service/pattern_matcher.h(1022): note: 'NewImpl=xla::match::detail::ShapePatternIsArrayImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1028): error C2672: 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl': no matching overloaded function found
.\tensorflow/compiler/xla/service/pattern_matcher.h(1028): error C2893: Failed to specialize function template 'xla::match::detail::ShapePattern<ShapeType,unknown-type> xla::match::detail::ShapePattern<ShapeType,xla::match::detail::ShapePatternBaseImpl>::AppendImpl(NewImpl) const'
        with
        [
            ShapeType=xla::Shape
        ]
.\tensorflow/compiler/xla/service/pattern_matcher.h(948): note: see declaration of 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1028): note: With the following template arguments:
.\tensorflow/compiler/xla/service/pattern_matcher.h(1028): note: 'NewImpl=xla::match::detail::ShapePatternIsTupleImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1033): error C2672: 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl': no matching overloaded function found
.\tensorflow/compiler/xla/service/pattern_matcher.h(1033): error C2893: Failed to specialize function template 'xla::match::detail::ShapePattern<ShapeType,unknown-type> xla::match::detail::ShapePattern<ShapeType,xla::match::detail::ShapePatternBaseImpl>::AppendImpl(NewImpl) const'
        with
        [
            ShapeType=xla::Shape
        ]
.\tensorflow/compiler/xla/service/pattern_matcher.h(948): note: see declaration of 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1033): note: With the following template arguments:
.\tensorflow/compiler/xla/service/pattern_matcher.h(1033): note: 'NewImpl=xla::match::detail::ShapePatternEffectiveScalarImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1039): error C2672: 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl': no matching overloaded function found
.\tensorflow/compiler/xla/service/pattern_matcher.h(1039): error C2893: Failed to specialize function template 'xla::match::detail::ShapePattern<ShapeType,unknown-type> xla::match::detail::ShapePattern<ShapeType,xla::match::detail::ShapePatternBaseImpl>::AppendImpl(NewImpl) const'
        with
        [
            ShapeType=xla::Shape
        ]
.\tensorflow/compiler/xla/service/pattern_matcher.h(948): note: see declaration of 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1039): note: With the following template arguments:
.\tensorflow/compiler/xla/service/pattern_matcher.h(1039): note: 'NewImpl=xla::match::detail::ShapePatternRankImpl'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1053): error C2672: 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout': no matching overloaded function found
.\tensorflow/compiler/xla/service/pattern_matcher.h(1053): error C2784: 'unknown-type xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout(const xla::match::detail::LayoutPattern<LayoutType,LayoutImpl> &) const': could not deduce template argument for 'const xla::match::detail::LayoutPattern<LayoutType,LayoutImpl> &' from 'unknown-type'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1046): note: see declaration of 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1058): error C2672: 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout': no matching overloaded function found
.\tensorflow/compiler/xla/service/pattern_matcher.h(1058): error C2784: 'unknown-type xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout(const xla::match::detail::LayoutPattern<LayoutType,LayoutImpl> &) const': could not deduce template argument for 'const xla::match::detail::LayoutPattern<LayoutType,LayoutImpl> &' from 'unknown-type'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1046): note: see declaration of 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1063): error C2672: 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout': no matching overloaded function found
.\tensorflow/compiler/xla/service/pattern_matcher.h(1063): error C2784: 'unknown-type xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout(const xla::match::detail::LayoutPattern<LayoutType,LayoutImpl> &) const': could not deduce template argument for 'const xla::match::detail::LayoutPattern<LayoutType,LayoutImpl> &' from 'unknown-type'
.\tensorflow/compiler/xla/service/pattern_matcher.h(1046): note: see declaration of 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 2238.604s, Critical Path: 469.42s
INFO: 4001 processes: 4001 local.
FAILED: Build did NOT complete successfully
```

Any idea what's going on and how to fix it?

**Edit:**
Here's the output of ./configure (https://pastebin.com/DhvwwyZM):

```
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.26.1 installed.
Please specify the location of python. [Default is C:\Users\QRabbani\Anaconda3\envs\tf_gpu\python.exe]:


Found possible Python library paths:
  C:\Users\QRabbani\Anaconda3\envs\tf_gpu\lib\site-packages
Please input the desired Python library path to use.  Default is [C:\Users\QRabbani\Anaconda3\envs\tf_gpu\lib\site-packages]

Do you wish to build TensorFlow with XLA JIT support? [y/N]: y
XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Found CUDA 10.1 in:
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include
Found cuDNN 7 in:
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]:


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: y
Eigen strong inline overridden.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=gdr            # Build with GDR support.
        --config=verbs          # Build with libverbs support.
        --config=ngraph         # Build with Intel nGraph support.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v2             # Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=noignite       # Disable Apache Ignite support.
        --config=nokafka        # Disable Apache Kafka support.
        --config=nonccl         # Disable NVIDIA NCCL support.
```

And here's what's in my .bazelrc (https://pastebin.com/h6aLHX0d):

```
# Android configs. Bazel needs to have --cpu and --fat_apk_cpu both set to the
# target CPU to build transient dependencies correctly. See
# https://docs.bazel.build/versions/master/user-manual.html#flag--fat_apk_cpu
build:android --crosstool_top=//external:android/crosstool
build:android --host_crosstool_top=@bazel_tools//tools/cpp:toolchain
build:android_arm --config=android
build:android_arm --cpu=armeabi-v7a
build:android_arm --fat_apk_cpu=armeabi-v7a
build:android_arm64 --config=android
build:android_arm64 --cpu=arm64-v8a
build:android_arm64 --fat_apk_cpu=arm64-v8a
build:android_x86 --config=android
build:android_x86 --cpu=x86
build:android_x86 --fat_apk_cpu=x86
build:android_x86_64 --config=android
build:android_x86_64 --cpu=x86_64
build:android_x86_64 --fat_apk_cpu=x86_64
 
# Sets the default Apple platform to macOS.
build --apple_platform_type=macos
 
# Config to use a mostly-static build and disable modular op registration
# support (this will revert to loading TensorFlow with RTLD_GLOBAL in Python).
# By default, TensorFlow will build with a dependence on
# //tensorflow:libtensorflow_framework.so.
build:monolithic --define framework_shared_object=false
 
# For projects which use TensorFlow as part of a Bazel build process, putting
# nothing in a bazelrc will default to a monolithic build. The following line
# opts in to modular op registration support by default.
build --define framework_shared_object=true
 
# Flags for open source build, always set to be true.
build --define open_source_build=true
test --define open_source_build=true
 
# Please note that MKL on MacOS or windows is still not supported.
# If you would like to use a local MKL instead of downloading, please set the
# environment variable ""TF_MKL_ROOT"" every time before build.
build:mkl --define=build_with_mkl=true --define=enable_mkl=true
build:mkl --define=tensorflow_mkldnn_contraction_kernel=0
build:mkl -c opt
 
# This config option is used to enable MKL-DNN open source library only,
# without depending on MKL binary version.
build:mkl_open_source_only --define=build_with_mkl_dnn_only=true
build:mkl_open_source_only --define=build_with_mkl_dnn_v1_only=true
build:mkl_open_source_only --define=build_with_mkl=true --define=enable_mkl=true
build:mkl_open_source_only --define=tensorflow_mkldnn_contraction_kernel=0
 
build:download_clang --crosstool_top=@local_config_download_clang//:toolchain
build:download_clang --define=using_clang=true
build:download_clang --action_env TF_DOWNLOAD_CLANG=1
# Instruct clang to use LLD for linking.
# This only works with GPU builds currently, since Bazel sets -B/usr/bin in
# auto-generated CPU crosstool, forcing /usr/bin/ld.lld to be preferred over
# the downloaded one.
build:download_clang_use_lld --linkopt='-fuse-ld=lld'
 
# This config refers to building with CUDA available. It does not necessarily
# mean that we build CUDA op kernels.
build:using_cuda --define=using_cuda=true
build:using_cuda --action_env TF_NEED_CUDA=1
build:using_cuda --crosstool_top=@local_config_cuda//crosstool:toolchain
 
# This config refers to building CUDA op kernels with nvcc.
build:cuda --config=using_cuda
build:cuda --define=using_cuda_nvcc=true
 
# This config refers to building CUDA op kernels with clang.
build:cuda_clang --config=using_cuda
build:cuda_clang --define=using_cuda_clang=true
build:cuda_clang --define=using_clang=true
 
build:tensorrt --action_env TF_NEED_TENSORRT=1
 
build:rocm --crosstool_top=@local_config_rocm//crosstool:toolchain
build:rocm --define=using_rocm=true --define=using_rocm_hipcc=true
build:rocm --action_env TF_NEED_ROCM=1
 
build:sycl --crosstool_top=@local_config_sycl//crosstool:toolchain
build:sycl --define=using_sycl=true
build:sycl --action_env TF_NEED_OPENCL_SYCL=1
 
build:sycl_nodouble --config=sycl
build:sycl_nodouble --cxxopt -DTENSORFLOW_SYCL_NO_DOUBLE
 
build:sycl_nodouble --config=sycl
build:sycl_asan --copt -fno-omit-frame-pointer --copt -fsanitize-coverage=3 --copt -DGPR_NO_DIRECT_SYSCALLS --linkopt -fPIC --linkopt -fsanitize=address
 
build:sycl_nodouble --config=sycl
build:sycl_trisycl --define=using_trisycl=true
 
# Options extracted from configure script
build:gdr --define=with_gdr_support=true
build:ngraph --define=with_ngraph_support=true
build:verbs --define=with_verbs_support=true
build:numa --define=with_numa_support=true
 
# Options to disable default on features
build:noaws --define=no_aws_support=true
build:nogcp --define=no_gcp_support=true
build:nohdfs --define=no_hdfs_support=true
build:nokafka --define=no_kafka_support=true
build:noignite --define=no_ignite_support=true
build:nonccl --define=no_nccl_support=true
 
build --define=use_fast_cpp_protos=true
build --define=allow_oversize_protos=true
 
build --spawn_strategy=standalone
build --strategy=Genrule=standalone
build -c opt
 
# Make Bazel print out all options from rc files.
build --announce_rc
 
# Other build flags.
build --define=grpc_no_ares=true
 
# Modular TF build options
build:dynamic_kernels --define=dynamic_loaded_kernels=true
build:dynamic_kernels --copt=-DAUTOLOAD_DYNAMIC_KERNELS
 
# Build TF with C++ 17 features.
build:c++17 --cxxopt=-std=c++1z
build:c++17 --cxxopt=-stdlib=libc++
build:c++1z --config=c++17
 
# Default paths for TF_SYSTEM_LIBS
build --define=PREFIX=/usr
build --define=LIBDIR=$(PREFIX)/lib
build --define=INCLUDEDIR=$(PREFIX)/include
 
# Suppress all warning messages.
build:short_logs --output_filter=DONT_MATCH_ANYTHING
 
# Options when using remote execution
build:rbe --action_env=BAZEL_DO_NOT_DETECT_CPP_TOOLCHAIN=1
build:rbe --auth_enabled=true
build:rbe --auth_scope=https://www.googleapis.com/auth/cloud-source-tools
build:rbe --define=EXECUTOR=remote
build:rbe --flaky_test_attempts=3
build:rbe --jobs=200
build:rbe --remote_accept_cached=true
build:rbe --remote_cache=remotebuildexecution.googleapis.com
build:rbe --remote_executor=remotebuildexecution.googleapis.com
build:rbe --remote_local_fallback=false
build:rbe --remote_timeout=600
build:rbe --spawn_strategy=remote
build:rbe --strategy=Genrule=remote
build:rbe --strategy=Closure=remote
build:rbe --strategy=Javac=remote
build:rbe --strategy=TestRunner=remote
build:rbe --tls_enabled
test:rbe --test_env=USER=anon
 
# Options to build TensorFlow 1.x or 2.x.
build:v1 --define=tf_api_version=1
build:v2 --define=tf_api_version=2
test:v1 --test_env=TF2_BEHAVIOR=0
test:v2 --test_env=TF2_BEHAVIOR=1
build --config=v2
test --config=v2
 
# Default options should come above this line
 
# Options from ./configure
try-import %workspace%/.tf_configure.bazelrc
 
# Put user-specific options in .bazelrc.user
try-import %workspace%/.bazelrc.user
```"
33000,Performance Degradation on MLPerf's GNMT implementation after converting to frozen graph ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): ('v1.14.0-rc1-22-gaf24dc91b5', '1.14.0')
- Python version: 2.7.15+
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): `gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0`
- CUDA/cuDNN version:
- GPU model and memory: NVIDIA Titan V

I am noticing a significant performance degradation in inference time when using the GNMT model provided for the MLPerf v0.5 benchmark with a trained model loaded from a checkpoint vs a trained model represented as a frozen graph. On the first iteration of inference (with batch_size = 6), the checkpoint graph takes ~2.9sec to perform inference while the frozen_graph implementation takes > 40sec. I would expect the frozen_graph to have better performance than the checkpoint model since my impression was that frozen graphs were meant for inference. 

Output from model loaded from checkpoint:
```
# Start decoding
  decoding to output /foobar/g_nmt-out
  infer_mode beam_search, beam_width 10, num translations per input 1.
  total iterations count 1.
infer time: 2.9462 secs
  done, num sentences 6, num translations per input 1, time 2s, Wed Oct  2 16:02:07 2019.
single_worker_inference time: 3.0597 secs
  bleu: 16.8
```

Output from model loaded from a frozen graph:
```
# Start decoding
  decoding to output /foobar/g_nmt-out
  infer_mode beam_search, beam_width 10, num translations per input 1.
  total iterations count 1.
infer time: 40.5799 secs
  done, num sentences 6, num translations per input 1, time 40s, Wed Oct  2 15:40:09 2019.
single_worker_inference time: 40.7543 secs
  bleu: 16.8
```

I've pushed up my own fork of the [mlperf/inference](https://github.com/mlperf/inference) repository with a branch that exposes this performance degradation [found here](https://github.com/brycearden/inference/tree/gnmt-frozen-perf-degradation). The fast / slow behavior can be toggled here, and can be reproduced by running `python run_task.py --run=accuracy`:

https://github.com/brycearden/inference/blob/gnmt-frozen-perf-degradation/v0.5/translation/gnmt/tensorflow/nmt/inference.py#L131-L160

Note that on the second iteration (after the caches warm up), the frozen graph still seems to be ~3x slower than the checkpoint model (7sec vs 2sec)

Any additional insight would be greatly appreciated.
"
32999,Error converting NMT sequence to sequence example to .tflite model,"I am following this example:
https://www.tensorflow.org/tutorials/text/nmt_with_attention

It is working as it should be and saving checkpoints.
I want to now convert this to a TF Lite model following this example:
https://www.tensorflow.org/lite/convert/python_api#converting_a_savedmodel_
or
https://www.tensorflow.org/lite/convert/python_api#converting_a_concrete_function_

Here is what I am running to save and them convert:
```python
tflite_input_tensor = tf.constant(1., shape=[64, 39])
tflite_target_tensor = tf.constant(1., shape=[64, 7])
tflite_enc_hidden_tensor = tf.constant(1., shape=[64, 1024])
export_dir = ""saved_models""
checkpoint.f = train_step
to_save = checkpoint.f.get_concrete_function(tflite_input_tensor, tflite_target_tensor, tflite_enc_hidden_tensor)
tf.saved_model.save(checkpoint, export_dir, to_save)

converter = tf.lite.TFLiteConverter.from_concrete_functions([to_save])
tflite_model = converter.convert()
```
But I am getting this error:
```
~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    845                     outputs = base_layer_utils.mark_as_return(outputs, acd)
    846                 else:
--> 847                   outputs = call_fn(cast_inputs, *args, **kwargs)
    848 
    849             except errors.OperatorNotAllowedInGraphError as e:

~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    290   def wrapper(*args, **kwargs):
    291     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
--> 292       return func(*args, **kwargs)
    293 
    294   if inspect.isfunction(func) or inspect.ismethod(func):

TypeError: call() missing 2 required positional arguments: 'hidden' and 'enc_output'
```

Trained with:
```python
@tf.function
def train_step(inp, targ, enc_hidden):
    loss = 0

    with tf.GradientTape() as tape:
        enc_output, enc_hidden = encoder(inp, enc_hidden)

        dec_hidden = enc_hidden

        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)

        # Teacher forcing - feeding the target as the next input
        for t in range(1, targ.shape[1]):
            # passing enc_output to the decoder
            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)

            loss += loss_function(targ[:, t], predictions)
            
            # using teacher forcing
            dec_input = tf.expand_dims(targ[:, t], 1)

    batch_loss = (loss / int(targ.shape[1]))

    variables = encoder.trainable_variables + decoder.trainable_variables

    gradients = tape.gradient(loss, variables)

    optimizer.apply_gradients(zip(gradients, variables))

    return batch_loss

EPOCHS = 3

for epoch in range(EPOCHS):
    start = time.time()

    enc_hidden = encoder.initialize_hidden_state()
    total_loss = 0

    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):
        batch_loss = train_step(inp, targ, enc_hidden)
        total_loss += batch_loss

        if batch % 100 == 0:
            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,
                                                         batch,
                                                         batch_loss.numpy()))
    # saving (checkpoint) the model every 2 epochs
    if (epoch + 1) % 1 == 0:
        checkpoint.save(file_prefix = checkpoint_prefix)

    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))
    print('Time taken for 1 epoch {} sec\n'.format(time.time() - start))
```


Somehow the parameters for the Decoder call is not being passed in?
```python
class Decoder(tf.keras.Model):
    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):
        super(Decoder, self).__init__()
        self.batch_sz = batch_sz
        self.dec_units = dec_units
        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
        self.rnn = tf.keras.layers.GRU(self.dec_units,
                                       return_sequences=True,
                                       return_state=True,
                                       recurrent_initializer='glorot_uniform',
                                       unroll=True)
        
        self.fc = tf.keras.layers.Dense(vocab_size)

        # used for attention
        self.attention = BahdanauAttention(self.dec_units)

    def call(self, x, hidden, enc_output):
        # enc_output shape == (batch_size, max_length, hidden_size)
        context_vector, attention_weights = self.attention(hidden, enc_output)

        # x shape after passing through embedding == (batch_size, 1, embedding_dim)
        x = self.embedding(x)

        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)
        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)

        # passing the concatenated vector to the GRU
        output, state = self.rnn(x)

        # output shape == (batch_size * 1, hidden_size)
        output = tf.reshape(output, (-1, output.shape[2]))

        # output shape == (batch_size, vocab)
        x = self.fc(output)

        return x, state, attention_weights
```

I understand there may be some trouble converting the GRU layers, but I will tackle that next. This seems to blow up before it even can check if GRU is able to be converted."
32998,Cannot compile TF 2.0.0 with MacOS 10.14.6 and XCode 11.0,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOs 10.14.6
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 2.0.0 and 1.15.0-rc2
- **Python version**: 3.7.4
- **Bazel version (if compiling from source)**: either 2.6.1, 2.6.0, 2.5.3
- **GCC/Compiler version (if compiling from source)**: Apple clang version 11.0.0 (clang-1100.0.33.8)
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**: within the tensorflow 2.0.0 source directory, after standard ./configure (CPU, no GPU) or particular optimization flags besides default:

`bazel build --config=opt --config=v2 --verbose_failures //tensorflow/tools/pip_package:build_pip_package`

### Describe the problem
Compilation stops with following error:
```
ERROR: /Users/feranick/Desktop/tensorflow/tensorflow/core/debug/BUILD:43:1: ProtoCompile tensorflow/core/debug/debug_service.pb.h failed (Segmentation fault): protoc failed: error executing command 
  (cd /private/var/tmp/_bazel_feranick/50b852099a3bf3aaa184abce166f8e34/execroot/org_tensorflow && \
  exec env - \
    PATH=/Users/feranick/Library/Python/3.7/bin/:/opt/local/Library/Frameworks/Python.framework/Versions/3.7/bin:/opt/usr/local/bin/:/Users/feranick/Documents/Work/c/android-sdk-macosx/platform-tools:/opt/local/bin:/opt/local/sbin:/Users/feranick/Library/Python/3.7/bin/:/opt/local/Library/Frameworks/Python.framework/Versions/3.7/bin:/opt/usr/local/bin/:/Users/feranick/Documents/Work/c/android-sdk-macosx/platform-tools:/opt/local/bin:/opt/local/sbin:/Users/feranick/Library/Python/3.7/bin/:/opt/local/Library/Frameworks/Python.framework/Versions/3.7/bin:/opt/usr/local/bin/:/Users/feranick/Documents/Work/c/android-sdk-macosx/platform-tools:/opt/local/bin:/opt/local/sbin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/usr/local/MacGPG2/bin:/opt/X11/bin:/Users/feranick/Documents/Work/c/android-sdk/platform-tools/:/Users/feranick/Documents/Work/c/android-sdk/platform-tools/:/Users/feranick/Documents/Work/c/android-sdk/platform-tools/ \
  bazel-out/host/bin/external/com_google_protobuf/protoc '--cpp_out=bazel-out/host/bin' '--plugin=protoc-gen-grpc=bazel-out/host/bin/external/grpc/grpc_cpp_plugin' '--grpc_out=bazel-out/host/bin' -I. -I. -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src -I. -I. -I. -I. -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src -I. -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src -I. -I. -I. -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src -I. -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src tensorflow/core/debug/debug_service.proto)
Execution platform: @bazel_tools//platforms:host_platform: protoc failed: error executing command 
  (cd /private/var/tmp/_bazel_feranick/50b852099a3bf3aaa184abce166f8e34/execroot/org_tensorflow && \
  exec env - \
    PATH=/Users/feranick/Library/Python/3.7/bin/:/opt/local/Library/Frameworks/Python.framework/Versions/3.7/bin:/opt/usr/local/bin/:/Users/feranick/Documents/Work/c/android-sdk-macosx/platform-tools:/opt/local/bin:/opt/local/sbin:/Users/feranick/Library/Python/3.7/bin/:/opt/local/Library/Frameworks/Python.framework/Versions/3.7/bin:/opt/usr/local/bin/:/Users/feranick/Documents/Work/c/android-sdk-macosx/platform-tools:/opt/local/bin:/opt/local/sbin:/Users/feranick/Library/Python/3.7/bin/:/opt/local/Library/Frameworks/Python.framework/Versions/3.7/bin:/opt/usr/local/bin/:/Users/feranick/Documents/Work/c/android-sdk-macosx/platform-tools:/opt/local/bin:/opt/local/sbin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/usr/local/MacGPG2/bin:/opt/X11/bin:/Users/feranick/Documents/Work/c/android-sdk/platform-tools/:/Users/feranick/Documents/Work/c/android-sdk/platform-tools/:/Users/feranick/Documents/Work/c/android-sdk/platform-tools/ \
  bazel-out/host/bin/external/com_google_protobuf/protoc '--cpp_out=bazel-out/host/bin' '--plugin=protoc-gen-grpc=bazel-out/host/bin/external/grpc/grpc_cpp_plugin' '--grpc_out=bazel-out/host/bin' -I. -I. -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src -I. -I. -I. -I. -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src -I. -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src -I. -I. -I. -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src -I. -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src tensorflow/core/debug/debug_service.proto)
Execution platform: @bazel_tools//platforms:host_platform
bazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.
bazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.
bazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.
bazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.
bazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.
bazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.
bazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.
bazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.
bazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```


"
32997,Cannot link to built tensorflow_cc.lib with Visual C++ -- undefined symbols; r2.0,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
   Windows 10 1903 Build 18362.356
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
  NA
- TensorFlow installed from (source or binary): 
  source
- TensorFlow version: 
  r2.0 as of 29-Sep-2019
- Python version: 
  3.6
- Installed using virtualenv? pip? conda?: 
  No
- Bazel version (if compiling from source): 
  0.29.1
- GCC/Compiler version (if compiling from source): 
  Visual C++ 2017 14.16.27023
- CUDA/cuDNN version: 
  10.1, 7.6.4.38
- GPU model and memory: 
  NVIDIA Quadro P2000, 5GB



**Describe the problem**
First off, I know I am not using the version of Bazel called for in the build instructions. In fact the version of bazel called for is a little ambiguous, but none of the versions called for would work for me. They just did not work with Visual C++ 2017 as I have it installed. But 0.29.1 did work in that it could build ""successfully"".

I am building r2.0 from source to use the C++ API with an external program in a plugin DLL. I built tensorflow_cc.dll and tensorflow_cc.lib using bazel (after much struggle). But when I tried to link to tensorflow_cc.lib (and no other tensorflow objects) I get unresolved symbols from tensorflow packages. With the vanilla r2.0 source, I get unresolved symbols for the following:
```
?_TensorShapeProto_default_instance_@tensorflow@@3VTensorShapeProtoDefaultTypeInternal@1@A

??0SessionOptions@tensorflow@@QEAA@XZ

?LoadSavedModel@tensorflow@@YA?AVStatus@1@AEBUSessionOptions@1@AEBVRunOptions@1@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBV?$unordered_set@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@U?$hash@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@U?$equal_to@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@6@QEAUSavedModelBundle@1@@Z
```


which coresponds to the following when demangled:
```
class tensorflow::TensorShapeProtoDefaultTypeInternal tensorflow::_TensorShapeProto_default_instance_

public: __cdecl tensorflow::SessionOptions::SessionOptions(void) __ptr64

class tensorflow::Status __cdecl tensorflow::LoadSavedModel(struct tensorflow::SessionOptions const & __ptr64,class tensorflow::RunOptions const & __ptr64,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const & __ptr64,class std::unordered_set<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,struct std::hash<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,struct std::equal_to<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > > const & __ptr64,struct tensorflow::SavedModelBundle * __ptr64 const)
```
I read some other github bug reports and discovered that there is a complex method used to export a subset of the global symbols into the .lib file that involves a python program tensorflow/tools/def_file_filter/def_file_filter.py.tpl and someone showed how to put extra symbols in that to add to the set exported. I went ahead and did that:
```
diff --git a/tensorflow/tools/def_file_filter/def_file_filter.py.tpl ...
@@ -154,6 +154,9 @@ def main():
       else:
         def_fp.write(""\t"" + decorated + "" DATA\n"")
       taken.add(decorated)
+    def_fp.write(""\t??0SessionOptions@tensorflow@@QEAA@XZ\n"")
+    def_fp.write(""\t?_TensorShapeProto_default_instance_@tensorflow@@3VTensorShapeProtoDefaultTypeInternal@1@A DATA\n"")
+    def_fp.write(""\t?LoadSavedModel@tensorflow@@YA?AVStatus@1@AEBUSessionOptions@1@AEBVRunOptions@1@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBV?$unordered_set@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@U?$hash@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@U?$equal_to@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@6@QEAUSavedModelBundle@1@@Z\n"")
     def_fp.close()

   exit_code = proc.wait()
```
This fixed 2 of the 3 unresolved, but the following was still undefined:
```
?_TensorShapeProto_default_instance_@tensorflow@@3VTensorShapeProtoDefaultTypeInternal@1@A
```
The only way I could get it to work was to actually link to the object file that defined that global object (tensor_shape.pb.o) and two "".a"" libraries for protobuf (libprotobuf_lite.a & libprotobuf.a):
```
cl.exe -MD ... plugin.obj .wsl.obj/segTask.obj .wsl.obj/tfseg.obj /link /dll ... c:/root/tensorflow/bazel-out/x64_windows-opt/bin/tensorflow/core/_objs/protos_all_proto_cc_impl/tensor_shape.pb.o c:/root/tensorflow/bazel-out/x64_windows-opt/bin/external/com_google_protobuf/libprotobuf.a c:/root/tensorflow/bazel-out/x64_windows-opt/bin/external/com_google_protobuf/libprotobuf_lite.a c:/root/tensorflow/bazel-bin/tensorflow/tensorflow_cc.lib 
```
I am not using any exotic tensorflow calls in my C++ code and it seems that it should just work. There are clearly other similar issues with unresolved symbols with Visual C++ builds and I am hoping the above will help to resolve those. As it is, it seems that anyone trying to deploy an inference engine using C++ on Windows will struggle mightily. I think the first issue that was solved with def_file_filter.py.tpl is simple to solve. But I just could not figure out why the 3rd symbol would never resolve. Clearly, linking in the .o file is totally bogus. I looked at the generated def file tensorflow_filtered_def_file.def and it seemed the symbol was actually in that file already without my changes to def_file_filter.py.tpl, but it still would not work.

(BTW, I also posted a stackoverflow question about this https://stackoverflow.com/questions/58176287/cannot-link-c-using-tensorflow-c-api-tensorflow-cc-lib-on-windows-10-visual/58188482#58188482)

**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build --config=monolithic //tensorflow:tensorflow_cc.dll  --verbose_failures
bazel build --config=monolithic //tensorflow:tensorflow_cc.lib  --verbose_failures

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Logs of the bazel link-stage builds of the DLL and LIB are attached.
[build-dll.txt](https://github.com/tensorflow/tensorflow/files/3682666/build-dll.txt)
[build-lib.txt](https://github.com/tensorflow/tensorflow/files/3682667/build-lib.txt)
"
32996,tf.linalg.inv() on GPU,"Hi all,

It seems that running tf.linalg.inv() on a GPU has no difference in terms of speed with respect to the CPU.

Is there a way to use GPU acceleration to compute the matrix inverse faster?

Cheers,"
32993,Run sample of experts is not work on local,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:no
- TensorFlow installed from (source or binary):pip install tensorflow-gpu
- TensorFlow version:2.0.0
- Python version:Python 3.6.8
- Installed using virtualenv? pip? conda?: pip
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:V10.0.130
- GPU model and memory:GTX1080TI



**Describe the problem**
I can obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"" . But when running sample of experts, is not work on local. ModuleNotFoundError: No module named 'tensorflow.keras'; 'tensorflow' is not a package
source file is following:
```
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from __future__ import unicode_literals

import tensorflow as tf

from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import Flatten
from tensorflow.keras import Model

mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

x_train = x_train[..., tf.newaxis]
x_test = x_test[..., tf.newaxis]

train_ds = tf.data.Dataset.from_tensor_slices(
  (x_train, y_train)).shuffle(10000).batch(32)
test_ds = tf.data.Dataset.from_tensor_slices(
  (x_test, y_test)).batch(32)


class MyModel(Model):
  def __init__(self):
    super(MyModel, self).__init__()
    self.conv1 = Conv2D(32, 3, activation='relu')
    self.flatten = Flatten()
    self.d1 = Dense(128, activation='relu')
    self.d2 = Dense(10, activation='softmax')

  def call(self, x):
    x = self.conv1(x)
    x = self.flatten(x)
    x = self.d1(x)
    return self.d2(x)


model = MyModel()

loss_object = tf.keras.losses.SparseCategoricalCrossentropy()
optimizer = tf.keras.optimizers.Adam()

train_loss = tf.keras.metrics.Mean(name='train_loss')
train_accuracy = tf.keras.metrics.SparseCategoricalCrossentropy()

test_loss = tf.keras.metrics.Mean(name='test_loss')
test_accuracy = tf.keras.metrics.SparseCategoricalCrossentropy(name='test_accuracy')


@tf.function
def train_step(images, labels):
  with tf.GradientTape() as tape:
    predications = model(images)
    loss = loss_object(labels, predications)
  gradients = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))

  train_loss(loss)
  train_accuracy(labels, predications)


@tf.function
def test_step(images, labels):
  predictions = model(images)
  t_loss = loss_object(labels, predictions)

  test_loss(t_loss)
  test_accuracy(labels, predictions)


EPOCHS = 5


for epoch in range(EPOCHS):
    for images, labels in train_ds:
      train_step(images, labels)

    for test_images, test_labels in test_ds:
      test_step(test_images, test_labels)

    template = 'Epoch: {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'
    print (template.format(epoch+1,
                           train_loss.result(),
                           train_accuracy.result()*100,
                           test_loss.result(),
                           test_accuracy.result()*100))

```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
python tensorflow.py (tensorflow.py is source code file)
"
32991,Test //tensorflow/lite/python:tflite_convert_test is broken,"I run Python tests for TFLite converter using this command 
`//tensorflow/lite/python:tflite_convert_test`

and I have error:
`ERROR: missing input file '//tensorflow/lite/python:tflite_convert.par'`

Please take a look and include this test data or maybe I run it wrong way ?
Thanks"
32990,Basic tutorial from Tensorflow 2.0 no longer runs on small machine,"
**System information**
- Tensorflow basic tutorial code from https://www.tensorflow.org/tutorials/quickstart/beginner
- Python version:3.7.4, standard yum repository, compiled by GCC 7.3.1 20180712 (Red Hat 7.3.1-6)] on linux
- Amazon free-tier EC2 node running amazon linux 2. 
- pip installed version of tensorflow version 2.0.0


**Describe the current behavior**
Dies due to memory constraints on Tensorflow 2.0.0 (but not in 1.14.0)

**Describe the expected behavior**
With tensorflow 1.14.0, the tutorial works just fine, but with 2.0.0 it runs out of memory. This is a small machine that Amazon has on it's free tier (t2.micro). It comes with 1 GB of ram, and I'm not expecting it to run anything really large, but it's an ideal machine from a cost perspective to try out tensorflow basics, and it works just fine with tensorflow 1.14 and earlier.

**Code to reproduce the issue**
Just the tensorflow tutorial code directly from the tensorflow website.
[ec2-user@ip-xxx-xxx-xxx-xxx ~]$ cat test_tf.py
#!/usr/bin/env python3
import tensorflow as tf
mnist = tf.keras.datasets.mnist

(x_train, y_train),(x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(512, activation=tf.nn.relu),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)
model.evaluate(x_test, y_test)

**Other info / logs**
Output from test_tf.py:
[ec2-user@ip-xxx-xxx-xxx-xxx ~]$ ./test_tf.py
2019-10-02 13:30:43.682116: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-10-02 13:30:43.775079: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400075000 Hz
2019-10-02 13:30:43.778603: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3798a90 executing computations on platform Host. Devices:
2019-10-02 13:30:43.778635: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2019-10-02 13:30:43.960286: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 376320000 exceeds 10% of system memory.
Segmentation fault
"
32988,The simple model converted by TFLiteConverter not accelerates by Coral USB Accelerator,"**Code to reproduce the issue**
Code to convert the model on Macbook Pro:

```
import tensorflow as tf

inputs = tf.keras.Input(shape=(256, 256, 3), name='model_input')
outputs = tf.keras.layers.Conv2D(filters=32, kernel_size=3)(inputs)
model = tf.keras.Model(inputs, outputs)
converter = tf.lite.TFLiteConverter.from_keras_model(model)

tflite_model = converter.convert()
open(""test_model_tf2.0_by_from_keras_model.tflite"", ""wb+"").write(tflite_model)
```

Code to inference the model on Raspberry Pi:
```
import tqdm

def tensorflow_lite():
    from tflite_runtime.interpreter import Interpreter
    from tflite_runtime.interpreter import load_delegate

    import numpy as np

    interpreter = Interpreter(
            'test_model_tf2.0_by_from_keras_model.tflite',
            experimental_delegates=[load_delegate('libedgetpu.so.1.0')], #with or without it
        )
    interpreter.allocate_tensors()
   
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    _, height, width, _ = interpreter.get_input_details()[0]['shape']

    for _ in tqdm.tqdm(range(100000)):
        image = np.zeros((1, 256, 256, 3,), dtype=np.float32)
        set_input_tensor(interpreter, image)
        interpreter.invoke()
        output = np.squeeze(interpreter.get_tensor(output_details[0]['index']))

def main():
    tensorflow_lite()
if __name__ == '__main__':
    main()
```

**System information**
About machine that model had been converted on:
- Macbook Pro Mid 2014, macOS Mojave 10.14.6
- TensorFlow installed by `sudo pip install -U tensorflow`
- TensorFlow version: v2.0.0-rc2-26-g64c3d382ca 2.0.0

About machine that tflite model had been run on:
- `Raspberry Pi 3B` + `Coral USB Accelerator`
```
$ cat /etc/debian_version
10.1

$ cat /etc/os-release
PRETTY_NAME=""Raspbian GNU/Linux 10 (buster)""
NAME=""Raspbian GNU/Linux""
VERSION_ID=""10""
VERSION=""10 (buster)""
...

$ uname -a
Linux raspberrypi 4.19.75-v7+ #1270 SMP Tue Sep 24 18:45:11 BST 2019 armv7l GNU/Linux
```

To set up Coral USB Accelerator i was guided by [https://coral.withgoogle.com/docs/accelerator/get-started/](url) with `libedgetpu1-std` variant.
Tflite had been installed by:
`pip3 install tflite_runtime-1.14.0-cp37-cp37m-linux_armv7l.whl`

**Current behavior**
The inference code on `Raspberry Pi 3B` + `Coral USB Accelerator` shows  ~12it/s and **no white led signaling on `Coral USB Accelerator` observed.**

**Expected behavior**
I expect at least x5-10 it/s from what i observe now. Also i expect the led signaling on inference because that is what i'm observing while running the `tflite/python/examples/classify_image.py` example.

Also here is my text output for `classify_image.py` example:
```
$ python3 classify_image.py --model models/mobilenet_v2_1.0_224_inat_bird_quant_edgetpu.tflite --labels models/inat_bird_labels.txt --image images/parrot.jpg

Initializing TF Lite interpreter...
INFO: Initialized TensorFlow Lite runtime.
----INFERENCE TIME----
Note: The first inference on Edge TPU is slow because it includes loading the model into Edge TPU memory.
120.6ms
13.5ms
13.5ms
13.8ms
13.6ms
-------RESULTS--------
923 Ara macao (Scarlet Macaw): 0.76562
```

I also tried to convert the model with tf-14.0.0 (on MacOS) and got the same result."
32987,[tf2.0.0] tf.keras.layers.GRU incorrect output of model.fit_generator trying to run Francois Chollet's notebook,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I have slightly modified Francoiis Chollet's Jupyter notebook 6.3 from his book ""Deep Learning with Python"" so that it runs on tensorflow.keras rather than keras (i.e. ""from tensorflow.keras import layers"" instead of ""from keras import layers"" etc)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): built from source
- TensorFlow version (use command below): 2.0.0 (i.e. the release)
- Python version: 3.7 conda
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: 10 / 7.6.4
- GPU model and memory: RTX 2080 Ti and Tesla V100 (tried on both. error occurs on both)

**Describe the current behavior**
Please see attached Jupyter Notebook 
[6.3-advanced-usage-of-recurrent-neural-networks-Copy1.zip](https://github.com/tensorflow/tensorflow/files/3680903/6.3-advanced-usage-of-recurrent-neural-networks-Copy1.zip).  I am going through Francois Chollet's book ""Deep Learning with Python"" and running the code in his Jupyter Notebooks in Tensorflow 2.0.0 adapting the code to ""import tensorflow.keras"" instead of ""import keras"".  Notebook 6.3, (under the heading ""1.6 Using recurrent dropout to fight overfitting"") has a model with a tensorflow.keras.layers.GRU(32, dropout=0.2, recurrent_dropout=0.2, input_shape=(None, float_data.shape[-1])).  The data is read earlier in the notebook from jena_climate_2009_2016.csv. With tensorflow 2.0.0 and tensorflow.keras I get a loss of 20417499919998144512.0000 and val_loss: 1.1059 after the first epoch and similar figures after subsequent epochs.  These figures are simply wrong (see below).  It also runs about 10x slower than its supposed to.  I interrupted the kernel after 4 epochs.  The original notebook (from Francois Chollet) is here: [link to github](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.3-advanced-usage-of-recurrent-neural-networks.ipynb) and includes the correct output.

**Describe the expected behavior**
I ran the same code with keras (not tf.keras) using a tensorflow 1 backend a while ago and it gave correct results.  The loss after 1 or 2 epochs is supposed to be around 0.3  The validation loss is supposed to be a little less than 0.3.  The graph below this code was produced using keras and a tensorflow 1 backend.  It shows the correct output.  Francis Chollet's original notebook (as linked to github above) also shows the correct output.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Download the data as follows:
cd ~
mkdir Datasets
cd ~/Datasets
mkdir jena_climate
cd jena_climate
wget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip
unzip jena_climate_2009_2016.csv.zip

Run jupyter notebook and load the notebook that is attached to this issue in a Python 3.7 environment with tensorflow 2.0.0.
In the notebook replace /home/daniel with /home/(your username).
Run each cell from the beginning of the notebook so you load the data and create the generators before you get to the example under heading 1.6.  Then try to run the example.  You will find that the loss and val_loss are terribly wrong.

EDIT: Since writing this, I have tried to run more code in the notebook.  The code under heading ""1.7 Stacking recurrent layers"" also runs incorrectly in tensorflow 2.0.0 using tensorflow.keras.  The loss produced is ""nan"" (it should be around 0.3).  I think it is the same problem with layers.GRU

The problem does **not** occur with tensorflow.keras in tensorflow 1.1.4.

EDIT: The rest of the remaining code in the notebook runs correctly, e.g. bidirectional GRU runs OK"
32986,Report You must feed a value for placeholder when I do feed the value.,"Tensorflow works in parameter-server mode.
worker1 starts training success, but worker 0 reports an error.

OS: macOS
Python: 3.7.4
Tensorflow: 1.14.0

```
I1002 19:19:12.119244 123145589936128 coordinator.py:219] Error reported to Coordinator: From /job:worker/replica:0/task:0:
You must feed a value for placeholder tensor 'attention/onehot_c1' with dtype int64 and shape [?,?,?]
	 [[node attention/onehot_c1 (defined at /Users/liuda/Tencent/code/nlp/attention/transformer/model.py:38) ]]

Original stack trace for 'attention/onehot_c1':
  File ""./train.py"", line 224, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python3.7/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.7/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""./train.py"", line 77, in main
    train(FLAGS)
  File ""./train.py"", line 135, in train
    model = Model(hidden_units, FLAGS, ""train"")
  File ""/Users/liuda/Tencent/code/nlp/attention/transformer/model.py"", line 38, in __init__
    input_placeholder = tf.placeholder(tf.int64, [None, None, None], name=""onehot_"" + fn)  # batchsize, seqsize, mfea
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py"", line 2143, in placeholder
    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 6262, in placeholder
    ""Placeholder"", dtype=dtype, shape=shape, name=name)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 3616, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 2005, in __init__
    self._traceback = tf_stack.extract_stack()
Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1356, in _do_call
    return fn(*args)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1341, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1429, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:worker/replica:0/task:0:
You must feed a value for placeholder tensor 'attention/onehot_c1' with dtype int64 and shape [?,?,?]
	 [[{{node attention/onehot_c1}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/training/coordinator.py"", line 495, in run
    self.run_loop()
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/training/supervisor.py"", line 1045, in run_loop
    [self._sv.summary_op, self._sv.global_step])
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 950, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1173, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1350, in _do_run
    run_metadata)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1370, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:worker/replica:0/task:0:
You must feed a value for placeholder tensor 'attention/onehot_c1' with dtype int64 and shape [?,?,?]
	 [[node attention/onehot_c1 (defined at /Users/liuda/Tencent/code/nlp/attention/transformer/model.py:38) ]]

Original stack trace for 'attention/onehot_c1':
  File ""./train.py"", line 224, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python3.7/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.7/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""./train.py"", line 77, in main
    train(FLAGS)
  File ""./train.py"", line 135, in train
    model = Model(hidden_units, FLAGS, ""train"")
  File ""/Users/liuda/Tencent/code/nlp/attention/transformer/model.py"", line 38, in __init__
    input_placeholder = tf.placeholder(tf.int64, [None, None, None], name=""onehot_"" + fn)  # batchsize, seqsize, mfea
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py"", line 2143, in placeholder
    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 6262, in placeholder
    ""Placeholder"", dtype=dtype, shape=shape, name=name)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 3616, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 2005, in __init__
    self._traceback = tf_stack.extract_stack()
```
I print the feed_dict, onehot_c1 was generated success as follows

```
attention/onehot_c1:0 <class 'numpy.ndarray'> (10, 15, 5) int64
```

Worker 1 trains normally and gives training info
```
train info 2019-10-02 19:20:16 epo 0 step 931 local_step 931 auc 0.6139 loss 0.0840
```"
32985,Tensorflow C++ LOG problem,"### System information
- **Linux Ubuntu 18.04**:
- **GCC: 7.4.0**
- **no CUDA or cuDNN used**
- **Tensorflow 1.14**

### Steps to reproduce

```c++
#include <memory>

#include ""tensorflow/core/platform/env.h""
#include ""tensorflow/core/platform/types.h""
#include ""tensorflow/core/public/session.h""

int main(int, char**) {
    tensorflow::GraphDef graph;
    tensorflow::SessionOptions options;
    tensorflow::Session* session = nullptr;
    tensorflow::Status status = tensorflow::NewSession(options, &session);
    if (session == nullptr) {
        std::cout << ""nullpt session"" << std::endl;
        std::cout << status << std::endl;
        return -1;
    }
    std::cout << status << std::endl;  // should be OK
    status = session->Create(graph);
    std::cout << status << std::endl;  // should be OK
    status = session->Close();
    std::cout << status << std::endl;  // should be OK
}
```

If I compile this code with the gcc flags `-Werror -O -g -pthread` the  program runs perfectly. 
```
2019-10-02 16:40:34.265739: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2019-10-02 16:40:34.265786: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)
2019-10-02 16:40:34.265803: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (pcen36738): /proc/driver/nvidia/version does not exist
OK
OK
OK
```

But if I use these flags `-Werror -O2 -pthread` the program doesn't create a session and give this error:

```
2019-10-02 16:38:59.090280: E tensorflow/core/common_runtime/session.cc:81] Not found: No session factory registered for the given session options: {target: """" config: } Registered factories are {}.
nullpt session
Not found: No session factory registered for the given session options: {target: """" config: } Registered factories are {}.
```

The problem dissapears when I add this line: `LOG(ERROR) << status;`

```c++
#include <memory>

#include ""tensorflow/core/platform/env.h""
#include ""tensorflow/core/platform/types.h""
#include ""tensorflow/core/public/session.h""

int main(int, char**) {
    tensorflow::GraphDef graph;
    tensorflow::SessionOptions options;
    tensorflow::Session* session = nullptr;
    tensorflow::Status status = tensorflow::NewSession(options, &session);
    if (session == nullptr) {
        LOG(ERROR) << status;
        std::cout << ""nullpt session"" << std::endl;
        std::cout << status << std::endl;
        return -1;
    }
    std::cout << status << std::endl;  // should be OK
    status = session->Create(graph);
    std::cout << status << std::endl;  // should be OK
    status = session->Close();
    std::cout << status << std::endl;  // should be OK
}
```

```
2019-10-02 16:42:26.867554: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2019-10-02 16:42:26.867614: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)
2019-10-02 16:42:26.867654: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (pcen36738): /proc/driver/nvidia/version does not exist
OK
OK
OK
```

So I think there is a problem with the Logger of tensorflow that it makes madatory to declarate it with some optimization flags.

Do you know why does it happen?

Thanks."
32983,BoostedTreesClassifier double free or corruption (!prev),"I am running the BoostedTreesClassifier tutorial (https://www.tensorflow.org/tutorials/estimator/boosted_trees) on tf2.0 

During the training call:

> # Train model.
> linear_est.train(train_input_fn, max_steps=100)
> 

a ""double free or corruption (!prev)"" is logged and the python process crashes

TF 2.0.0 (pip installed)
Ubuntu 19.04
python 3.7
cuda 10.0.130
GPU: RTX 2080 Ti

Other tensorflow components (ie keras) work fine
"
32982,TF2 Stubs for intellisense; current way tf2 imports modules does not support intellisense,"**System information**
- TensorFlow version (you are using): 2.0.0
- Are you willing to contribute it (Yes/No): Happy to help out

**Describe the feature and the current behavior/state.**
Intellisense does not work because of the way tf2 has import statements.

![image](https://user-images.githubusercontent.com/9828683/66023599-f7f83180-e4f1-11e9-83d0-fe931c4486f6.png)


**Will this change the current api? How?**

Add stubs.

**Who will benefit with this feature?**

Vscode users

**Any Other info.**

Discussed here: 
https://github.com/microsoft/python-language-server/issues/818#issuecomment-537143000"
32981,Accessing validation data within a custom callback ??,"**Describe the current behavior**
Hi, how can I access validation data within a custom callback ?

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
from __future__ import absolute_import, division, print_function, unicode_literals

import os
import matplotlib.pyplot as plt
try:
  # %tensorflow_version only exists in Colab.
  %tensorflow_version 2.x
except Exception:
  pass
import tensorflow as tf
import datetime



print(""TensorFlow version: {}"".format(tf.__version__))
print(""Eager execution: {}"".format(tf.executing_eagerly()))

from tensorflow.keras.layers import Dense, Flatten, Conv2D
from tensorflow.keras import Model

mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0


model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation='softmax')
])

class MyCustomCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        print('Hello')
        print(self.validation_data[0].shape)
  
def customLoss3(y_true, y_pred):
    return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred))
  
#model.compile(optimizer='adam', loss=customLoss3, metrics=['accuracy'])

model.compile(optimizer='adam', loss=customLoss3, metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])

model.fit(x_train, y_train, epochs=5, validation_split=0.2, callbacks=[MyCustomCallback()])

model.evaluate(x_test, y_test)

```
**Other info / logs**
TypeError: 'NoneType' object is not subscriptable
"
32980,Why are the Adam implementations in tensorflow.python.keras.optimizers and tensorflow.keras.optimizers different in tensorflow2.0stable? ,"
Why are the Adam implementations in tensorflow.python.keras.optimizers and tensorflow.keras.optimizers different? "
32977,tensorflow2.0 can't find CRF,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):tensorflow2.0
- Are you willing to contribute it (Yes/No):
I don't know how to achieve it well, if need me, I can do as much as I can


**Describe the feature and the current behavior/state.**
No named entity identification module was found

**Will this change the current api? How?**
There is no named entity recognition API

**Who will benefit with this feature?**
A technician who studies named entity recognition and relationship extraction

**Any Other info.**
"
32975,tf.io.gfile.GFile does not work with Python zipfile,"**System information**

-  Have I written custom code: Yes
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from: binary
- TensorFlow version: v1.14.0-rc1-22-gaf24dc9 1.14.0
- Python version: 3.6

**Describe the current behavior**
`tf.io.gfile.GFile` does not work correctly with the Python built in `zipfile` module. When writing to a ZipFile where `file=GFile()` the resulting ZIP file is corrupt.

**Describe the expected behavior**
ZipFiles written with `file=GFile()` should be not be corrupt and equal to those written with `file=<other_type_fd>`.

**Code to reproduce the issue**

```
import tensorflow as tf
import zipfile
import filecmp


normal_fd = open(""normal.zip"", ""wb"")
normal2_fd = open(""normal2.zip"", ""wb"")
open(""gfile.zip"", ""w"").close()  # ""touch"" file so that it exists, issue #32090
gfile_fd = tf.io.gfile.GFile(""gfile.zip"", ""w+b"")  # need +, issue #32122

for fd in (normal_fd, normal2_fd, gfile_fd):
    with zipfile.ZipFile(file=fd, ""w"") as zipfd:
        with zipfd.open(""test.txt"", ""w"") as fid:
            fid.write(""Hello, World!"".encode())
    fd.close()

print(""Normal zips equal?"", filecmp.cmp(""normal.zip"", ""normal2.zip""))
print(""GFile zip equal normal?"", filecmp.cmp(""normal.zip"", ""gfile.zip""))

```"
32972,tf.io.gfile.mkdir does not throw exception if path exists,"**System information**

-  Have I written custom code: Yes
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from: binary
- TensorFlow version: v1.14.0-rc1-22-gaf24dc9 1.14.0
- Python version: 3.6

**Describe the current behavior**
`tf.io.gfile.mkdir()` on Linux does not throw an exception if the path exists. This can cause problems if the path is a file, as you would assume that the path is a directory after calling `mkdir()`. See the code below for an example.

**Describe the expected behavior**
`mkdir()` should throw an exception if the path exists, like the Python built in [`os.mkdir()`](https://docs.python.org/3/library/os.html#os.mkdir) does.

**Code to reproduce the issue**

```
import tensorflow as tf

test_path = ""test""

# Create a file at <test_path>
with open(test_path, ""w"") as f:
    f.write(""Hello, World!"")

# No error when creating dir
tf.io.gfile.mkdir(test_path)
with tf.io.gfile.GFile(test_path + ""/test_file.txt"", ""w"") as f:
    # .write throws exception since the <test_path> directory does not exist
    f.write(""Hello"")
```"
32970,java.nio.BufferOverflowException TensorFlowYoloDetector,"Hello, 
I am using Yolov2-tiny to detect a single class a running this on this example app. Because I only need a single class, I have been experimenting by reducing layers on yolov2-tiny and the filter sizes.  After a second reduction of the model, I have ran into the error ""java.nio.BufferOverflowException"". 

**Full Error**
E/AndroidRuntime: FATAL EXCEPTION: inference
    Process: org.tensorflow.demo, PID: 32033
    java.nio.BufferOverflowException
        at java.nio.HeapFloatBuffer.put(HeapFloatBuffer.java:180)
        at org.tensorflow.Tensor.writeTo(Tensor.java:488)
        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.fetch(TensorFlowInferenceInterface.java:488)
        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.fetch(TensorFlowInferenceInterface.java:442)
        at org.tensorflow.demo.TensorFlowYoloDetector.recognizeImage(TensorFlowYoloDetector.java:168)
        at org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:330)
        at android.os.Handler.handleCallback(Handler.java:873)
        at android.os.Handler.dispatchMessage(Handler.java:99)
        at android.os.Looper.loop(Looper.java:214)
        at android.os.HandlerThread.run(HandlerThread.java:65)
I/Process: Sending signal. PID: 32033 SIG: 9

**Here is my setup** 

- **Hardware**: Samsung S8
- **System**: Windows
-**Models**: Trained inside Linux VM and converted to .pb file using darknet.
- **Preview Size**: Changed from 640x480 to 1920x1080

**Models**

- **Trial 1**
     - **Model**: Yolov2-tiny (no modifications)
     - **Model size**: 43MB
     -**Inference Time**: ~600ms
     -**Status**: Works with no issues

- **Trial 2**
     - **Model**: Yolov2-tiny (Removed a convolutional layer and scaled down filters)
     - **Model size**: 10MB
     -**Inference Time**: ~400ms
     -**Status**: Works with no issues

- **Trial 3**
     - **Model**: Yolov2-tiny (removed 2 convolutional layers and scaled down filters)
     - **Model size**: 2.7MB
     -**Inference Time**: None
     -**Status**: Throws the exception status above
     -**Note**: This version works if I lower the parameter YOLO_BLOCK_SIZE in 
                      DetectorActivitty.java from 32 to 16. The problem with this is that it gives me an 
                      inference time of 600MS and that negates the purpose of scaling down the model

All three of these model work well when used with darknet and were converted using darkflow in the exact same environment and commands. 

As mentioned by  @UsamaIslam in #12649 , he fixed this issue by changing the MAX_RESULTS parameter when using fast rcnn with this example app. I tried doing the same, but it didn't work. I don't fully understand this exception so I need help understanding why this fails with **Trial 3** and not the other two trials. 

I would appreciate any suggestions on how I could make this work. 
"
32967,TensorFlow 2.0 doesn't detect the GPU when rerunning the same program,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): /
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: /
- TensorFlow installed from (source or binary): /
- TensorFlow version (use command below): tensorflow-gpu==2.0.0
- Python version: 3.6.8
- Bazel version (if compiling from source): /
- GCC/Compiler version (if compiling from source): /
- CUDA/cuDNN version: 10.0
- GPU model and memory: GeForce RTX 2080 Ti, ~11GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
When running a tensorflow program for the first time (in a fresh docker instance) it detects the GPU as expected. When running the same program again (after stopping the previous run) it doesn't detect any GPU devices.
This works as expected in tensorflow-gpu==2.0.0rc1 however.

**Describe the expected behavior**
Tensorflow should always detect all attached GPUs, even on subsequent runs.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
import tensorflow as tf
(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()
train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')
train_images = (train_images - 127.5) / 127.5
train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(100).batch(32)
from tensorflow.python.client import device_lib 
print(device_lib.list_local_devices())
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Devices detected by tensorflow on the first run:
```
[name: ""/device:CPU:0""
device_type: ""CPU""
memory_limit: 268435456
locality {
}
incarnation: 2693156956587076928
, name: ""/device:XLA_CPU:0""
device_type: ""XLA_CPU""
memory_limit: 17179869184
locality {
}
incarnation: 7803384591284859902
physical_device_desc: ""device: XLA_CPU device""
, name: ""/device:XLA_GPU:0""
device_type: ""XLA_GPU""
memory_limit: 17179869184
locality {
}
incarnation: 8169967461022885217
physical_device_desc: ""device: XLA_GPU device""
, name: ""/device:GPU:0""
device_type: ""GPU""
memory_limit: 10742700442
locality {
  bus_id: 1
  links {
  }
}
incarnation: 643882925305723598
physical_device_desc: ""device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:0b:00.0, compute capability: 7.5""
]
```

Devices detected by tensorflow on the second run:
```
[name: ""/device:CPU:0""
device_type: ""CPU""
memory_limit: 268435456
locality {
}
incarnation: 13203723707335455513
, name: ""/device:XLA_CPU:0""
device_type: ""XLA_CPU""
memory_limit: 17179869184
locality {
}
incarnation: 6024229839119595332
physical_device_desc: ""device: XLA_CPU device""
]
```
"
32966,DepthwiseConv2D missing dilation_rate argument (& higher performance),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): (yes)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): ongoing, reference github source https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/layers/convolutional.py#L1686-L1877
- TensorFlow version (use command below): TF2 rc
- Python version: (3)
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

Conv, Conv2D, and backend.depthwise_conv2d support dilation_rate, but DepthwiseConv2D does not. 

**Describe the expected behavior**

DepthwiseConv2D needs the dilation_rate argument. It is supported by underlying code and mentioned by the DepthwiseConv2D documentation, but is missing from DepthwiseConv2D. 

**Note that while the DepthwiseConv2D  as below works OK, performance of the forward inference code is significantly less than what it should be. DepthwiseConv2D with dilation is a very useful operation for full-resolution feature matching with a low operation count, so improving the execution code would be valuable.**

**Other info / logs**

Proposed working revised version (sorry, not adept with github).


```

@keras_export('keras.layers.DepthwiseConv2D')
class DepthwiseConv2D(Conv2D):
  """"""Depthwise separable 2D convolution.
  Depthwise Separable convolutions consists in performing
  just the first step in a depthwise spatial convolution
  (which acts on each input channel separately).
  The `depth_multiplier` argument controls how many
  output channels are generated per input channel in the depthwise step.
  Arguments:
    kernel_size: An integer or tuple/list of 2 integers, specifying the
      height and width of the 2D convolution window.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    strides: An integer or tuple/list of 2 integers,
      specifying the strides of the convolution along the height and width.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: one of `'valid'` or `'same'` (case-insensitive).
    depth_multiplier: The number of depthwise convolution output channels
      for each input channel.
      The total number of depthwise convolution output
      channels will be equal to `filters_in * depth_multiplier`.
    data_format: A string,
      one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first`
      corresponds to inputs with shape
      `(batch, channels, height, width)`.
      It defaults to the `image_data_format` value found in your
      Keras config file at `~/.keras/keras.json`.
      If you never set it, then it will be 'channels_last'.
    dilation_rate: an integer or tuple/list of 2 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    activation: Activation function to use.
      If you don't specify anything, no activation is applied
      (ie. 'linear' activation: `a(x) = x`).
    use_bias: Boolean, whether the layer uses a bias vector.
    depthwise_initializer: Initializer for the depthwise kernel matrix.
    bias_initializer: Initializer for the bias vector.
    depthwise_regularizer: Regularizer function applied to
      the depthwise kernel matrix.
    bias_regularizer: Regularizer function applied to the bias vector.
    activity_regularizer: Regularizer function applied to
      the output of the layer (its 'activation').
    depthwise_constraint: Constraint function applied to
      the depthwise kernel matrix.
    bias_constraint: Constraint function applied to the bias vector.
  Input shape:
    4D tensor with shape:
    `[batch, channels, rows, cols]` if data_format='channels_first'
    or 4D tensor with shape:
    `[batch, rows, cols, channels]` if data_format='channels_last'.
  Output shape:
    4D tensor with shape:
    `[batch, filters, new_rows, new_cols]` if data_format='channels_first'
    or 4D tensor with shape:
    `[batch, new_rows, new_cols, filters]` if data_format='channels_last'.
    `rows` and `cols` values might have changed due to padding.
  """"""

  def __init__(self,
               kernel_size,
               strides=(1, 1),
               padding='valid',
               depth_multiplier=1,
               data_format=None,
               activation=None,
               use_bias=True,
               dilation_rate=(1,1),
               depthwise_initializer='glorot_uniform',
               bias_initializer='zeros',
               depthwise_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               depthwise_constraint=None,
               bias_constraint=None,
               **kwargs):
    super(DepthwiseConv2D, self).__init__(
        filters=None,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activation,
        use_bias=use_bias,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        bias_constraint=bias_constraint,
        **kwargs)
    self.depth_multiplier = depth_multiplier
    self.depthwise_initializer = initializers.get(depthwise_initializer)
    self.depthwise_regularizer = regularizers.get(depthwise_regularizer)
    self.depthwise_constraint = constraints.get(depthwise_constraint)
    self.bias_initializer = initializers.get(bias_initializer)

  def build(self, input_shape):
    if len(input_shape) < 4:
      raise ValueError('Inputs to `DepthwiseConv2D` should have rank 4. '
                       'Received input shape:', str(input_shape))
    input_shape = tensor_shape.TensorShape(input_shape)
    if self.data_format == 'channels_first':
      channel_axis = 1
    else:
      channel_axis = 3
    if input_shape.dims[channel_axis].value is None:
      raise ValueError('The channel dimension of the inputs to '
                       '`DepthwiseConv2D` '
                       'should be defined. Found `None`.')
    input_dim = int(input_shape[channel_axis])
    depthwise_kernel_shape = (self.kernel_size[0],
                              self.kernel_size[1],
                              input_dim,
                              self.depth_multiplier)

    self.depthwise_kernel = self.add_weight(
        shape=depthwise_kernel_shape,
        initializer=self.depthwise_initializer,
        name='depthwise_kernel',
        regularizer=self.depthwise_regularizer,
        constraint=self.depthwise_constraint)

    if self.use_bias:
      self.bias = self.add_weight(shape=(input_dim * self.depth_multiplier,),
                                  initializer=self.bias_initializer,
                                  name='bias',
                                  regularizer=self.bias_regularizer,
                                  constraint=self.bias_constraint)
    else:
      self.bias = None
    # Set input spec.
    self.input_spec = InputSpec(ndim=4, axes={channel_axis: input_dim})
    self.built = True

  def call(self, inputs):
    outputs = backend.depthwise_conv2d(
        inputs,
        self.depthwise_kernel,
        strides=self.strides,
        padding=self.padding,
        dilation_rate=self.dilation_rate,
        data_format=self.data_format)

    if self.use_bias:
      outputs = backend.bias_add(
          outputs,
          self.bias,
          data_format=self.data_format)

    if self.activation is not None:
      return self.activation(outputs)

    return outputs

  @tf_utils.shape_type_conversion
  def compute_output_shape(self, input_shape):
    if self.data_format == 'channels_first':
      rows = input_shape[2]
      cols = input_shape[3]
      out_filters = input_shape[1] * self.depth_multiplier
    elif self.data_format == 'channels_last':
      rows = input_shape[1]
      cols = input_shape[2]
      out_filters = input_shape[3] * self.depth_multiplier

    rows = conv_utils.conv_output_length(rows, self.kernel_size[0],
                                         padding=self.padding,
                                         stride=self.strides[0],
                                         dilation=self.dilation_rate[0])
    cols = conv_utils.conv_output_length(cols, self.kernel_size[1],
                                         padding=self.padding,
                                         stride=self.strides[1],
                                         dilation=self.dilation_rate[1])
    if self.data_format == 'channels_first':
      return (input_shape[0], out_filters, rows, cols)
    elif self.data_format == 'channels_last':
      return (input_shape[0], rows, cols, out_filters)

  def get_config(self):
    config = super(DepthwiseConv2D, self).get_config()
    config.pop('filters')
    config.pop('kernel_initializer')
    config.pop('kernel_regularizer')
    config.pop('kernel_constraint')
    config['depth_multiplier'] = self.depth_multiplier
    config['depthwise_initializer'] = initializers.serialize(
        self.depthwise_initializer)
    config['depthwise_regularizer'] = regularizers.serialize(
        self.depthwise_regularizer)
    config['depthwise_constraint'] = constraints.serialize(
        self.depthwise_constraint)
    return config


```"
32965,Freezing a graph using the C API,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).



For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.




Hi everyone,

Is it possible to freeze a graph and save the frozen version using the C API after loading the graph's definition file (.pb) and restoring its variables created in Python ?

A code snippet of the ""freezing"" step would be greatly appreciated!

Thanks :)


"
32964,Automatic gradient for numpy functions/external functions,"I am thinking of the following scenarios and want to see if TensorFlow could do Automatic Differentiation for them:

1. There would be a numpy function that accepts numpy array(s) as input(s) and numpy array(s) as output(s);

2. There would be an external executable that accepts numpy array(s) as input(s) and numpy array(s) as output(s).

I know TensorFlow has `tf.numpy_function` (with the v2 context, and equivalently, it was `tf.py_func` in the v1 context) can ""convert"" a numpy function to a tensor function and inject it into the graph but unfortunately it cannot support Automatic Differentiation; also, `tf.py_function` must accept a tensor function and tensor inputs to inject the function into the graph and fortunately it can support Automatic Differentiation. I cannot think of a way to inject an external executable to be part of the graph and possibly get its differentiation.

If there is no way to achieve scenario 1 and/or 2, is there any workaround?"
32963,tf.io.gfile.mkdir restricts directory mode/(permissions),"**System information**

-  Have I written custom code: Yes
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from: binary
- TensorFlow version: v1.14.0-rc1-22-gaf24dc9 1.14.0
- Python version: 3.6

**Describe the current behavior**
Directories created with `tf.io.gfile.mkdir()` on Linux does not have the w mode bit set for group and other even if allowed by the umask and ACL.

**Describe the expected behavior**
Directories created with `tf.io.gfile.mkdir` should have the maximum permissions allowed by the umask and ACL, which is the way `os.mkdir()` in Python works.
I think this behavior is caused by the fact that [TF always calls mkdir with mode 0755](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/core/platform/posix/posix_file_system.cc#L281) while  [Python calls mkdir with mode 511 (= 777 in octal)](https://github.com/python/cpython/blob/3.7/Modules/clinic/posixmodule.c.h#L1094) if no mode is given .

**Code to reproduce the issue**

```
import tensorflow as tf
import stat
import os

os.umask(0000)

tf_dir = ""test1""
os_dir = ""test2""

tf.io.gfile.mkdir(tf_dir)
tf_mode = os.stat(tf_dir).st_mode

os.mkdir(os_dir)
os_mode = os.stat(os_dir).st_mode

if (tf_mode != os_mode):
    print(""File mode differs:"")
    print(""TF: {}, OS: {}"".format(stat.filemode(tf_mode), stat.filemode(os_mode)))

```"
32960,Keras Nadam optmizer generates error when using MirroredStrategy,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18
- TensorFlow installed from (source or binary): Tensorflow 2
- TensorFlow version (use command below) : Tensorflow 2.0.0
- Python version: Python 3.7
- CUDA/cuDNN version: 10
- GPU model and memory: 2 x Nvidia 1080 TI

**Describe the current behavior**

The training crashes with an error( ValueError: You must specify an aggregation method to update a MirroredVariable in Replica Context.) If the model is compiled with the optimizer Nadam (tf.keras.optimizers.Nadam) along with a MirroredStrategy.

**Describe the expected behavior**
 Expect to be able to train with any optimizer from Keras' options.

"
32959,Tensorflow to CoreML with tf-coreml: `Retval[26]` error,"**System information**
- Have I written custom code: a mix of both
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary): `pip install tensorflow-gpu`
- TensorFlow version: 1.14.0
- Python version: 3.6.6
- CUDA/cuDNN version: 10.1
- GPU model and memory: NVIDIA-SMI 418.87.00 / Driver Version: 418.87.00

**Describe the current behavior**
I have a multi-input network that uses a `tf.bool` `tf.placeholder` to manage how batch normalization is executed in training and validation / testing.
I’ve been trying to convert this trained model to `CoreML` via `tf-coreml` library with no success, with below error:
`tensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[26] does not have value`
I've also encountered: `NotImplementedError: Unsupported Ops of type: Switch,Merg`

I understand that this error states that there is a certain node that’s missing a value so the converter can execute the model. I also understand this error is connected to control flow operations (linked to the batch normalization method creating operations like `Switch` and `Merge`). The [source code](https://github.com/tensorflow/tensorflow/blob/1c7993f7c597745e3639b9918c3b3ae2165d4af2/tensorflow/python/kernel_tests/control_flow_ops_py_test.py#L244) shows this:

```def testSwitchDeadBranch(self):
    with self.cached_session():
      data = constant_op.constant([1, 2, 3, 4, 5, 6], name=""data"")
      ports = ops.convert_to_tensor(True, name=""ports"")
      switch_op = control_flow_ops.switch(data, ports)
      dead_branch = array_ops.identity(switch_op[0])

      with self.assertRaisesWithPredicateMatch(
          errors_impl.InvalidArgumentError,
          lambda e: ""Retval[0] does not have value"" in str(e)):
        self.evaluate(dead_branch)
```
Note that my error is `Retval[26]` (I’ve gotten [24], etc.), not `Retval[0]`. I’m assuming it tests the `Switch` “dead branch”, which should be the non-used branch for inference. The code also does the same with `Merge` “dead branch”.

Is there any detail I’m missing that may be causing this error (not the first error I’ve faced during conversion, of course)? The way the inference is done? The way batch normalization is implemented? The way the model is saved? 

What I’ve done so far:
- I know `tf.layers.batch_normalization` creates operations `Switch` and `Merge`, which are not CoreML compatible
- I’ve tried converting to `Tensorflow Lite` with similar issues
- I’ve follow `Facenet` (this model uses the same `tf.bool` logic for training, validation, testing) conversion process with no success
- I’ve tried the [`GraphTransforms`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md) library
- I’ve tried scripts to remove / modify the control flow 
- I’ve created separate graphs to avoid the extra ops with no success

_Note: I’ve abstracted big part of the code and network to post this issue._

**Code to reproduce the issue**

This is how batch normalization is implemented (within a convolution block):

```
training = tf.placeholder(tf.bool, shape = (), name = 'training')

def conv_layer(input, kernelSize, nFilters, poolSize, stride, input_channels = 1, name = 'conv'):
        with tf.name_scope(name):
        shape = [kernelSize, kernelSize, input_channels, nFilters]
        weights = new_weights(shape = shape)        biases = new_biases(length = nFilters)
        conv = tf.nn.conv2d(input, weights, strides = [1, 2, 2, 1], padding = 'SAME', name = 'convL')
        conv += biases
        pool = tf.reduce_max(conv, reduction_indices=[3], keep_dims=True, name = 'pool') 
       pool = tf.nn.max_pool(conv, ksize = [1, poolSize, poolSize, 1], strides = shape, padding = 'SAME')
        bnorm = tf.layers.batch_normalization(pool, training = training, center = True, scale = True, fused = False, reuse= False)
        act = tf.nn.relu(bnorm)
        return act
```

Below is the code to train and save the model.

```
saver = tf.train.Saver()

    with tf.Session(config = config) as sess:
        sess.run(tf.global_variables_initializer())
        sess.run(tf.local_variables_initializer())
        sess.run(init_train_op)

        for epoch in range(MAX_EPOCHS):
            
            for step in range(10):
                
                l, _, se = sess.run(
                    [loss, train_op, mean_squared_error],
                     feed_dict = {training: True})

            print('\nRunning validation operation...')

            sess.run(init_val_op)
            for _ in range(10):
                val_out, val_l, val_se = sess.run(
                    [out, val_loss, val_mean_squared_error],
                    feed_dict = {training: False})
         
            sess.run(init_train_op) # switch back to training set

        #Save model
        print('Saving Model...\n')
        saver.save(sess, join(saveDir, './model_saver_validation'.format(modelIndex)), write_meta_graph = True)
```

Below is the code to load, update inputs, perform inference, and freeze the model.

```
# Dummy data for inference
b = np.zeros((1, 80, 160, 1), np.float32)
ill = np.ones((1,3), np.float32)
is_train = False

def freeze():
    with tf.Graph().as_default():
        with tf.Session() as sess:
            bIn = tf.placeholder(dtype=tf.float32, shape=[
                             1, 80, 160, 1], name='bIn')
            illumIn = tf.placeholder(dtype=tf.float32, shape=[
                                     1, 3], name='illumIn')
            training = tf.placeholder(tf.bool, shape=(), name = 'training')

            # Load the model metagraph and checkpoint
            meta_file = meta_graph #.meta file from saver.save()
            ckpt_file = checkpoint_file #checkpoint file

            # Load graph to redirect inputs from iterator to expected inputs
            saver = tf.train.import_meta_graph(meta_file, input_map={
                'IteratorGetNext:0': bIn,
                'IteratorGetNext:3': illumIn,
                'training:0': training},  clear_devices = True)
            
            tf.get_default_session().run(tf.global_variables_initializer())
            tf.get_default_session().run(tf.local_variables_initializer())
            saver.restore(tf.get_default_session(), ckpt_file)
            
            pred = tf.get_default_graph().get_tensor_by_name('Out:0')
            
            tf.get_default_session().run(pred, feed_dict={'bIn:0': b, 'poseIn:0': po, 'training:0': is_train})

            # Retrieve the protobuf graph definition and fix the batch norm nodes
            input_graph_def = sess.graph.as_graph_def()

            # Freeze the graph def
            output_graph_def = freeze_graph_def(
                sess, input_graph_def, output_node_names)

        # Serialize and dump the output graph to the filesystem
        with tf.gfile.GFile(frozen_graph, 'wb') as f:
            f.write(output_graph_def.SerializeToString())

freeze()
```

Below is the code to convert to CoreML.

```
tfcoreml.convert(
    tf_model_path=frozen_graph,
    mlmodel_path='./coreml_model.mlmodel',
    output_feature_names=['Out:0'],
    input_name_shape_dict={
        'bIn:0': [1, 80, 160, 1],
        'illumIn:0': [1, 3], 
        'training:0': []})
```

**Other info / logs**
Below is the error thrown by `tf-coreml`.
```
Loading the TF graph...
Graph Loaded.
Collecting all the 'Const' ops from the graph, by running it....

Traceback (most recent call last):
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1356, in _do_call
    return fn(*args)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1341, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1429, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[26] does not have value

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""tf2opencv.py"", line 392, in <module>
    'illumIn:0': [1, 3], 'poseIn:0': [1, 16], 'training:0': []})
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tfcoreml/_tf_coreml_converter.py"", line 586, in convert
    custom_conversion_functions=custom_conversion_functions)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tfcoreml/_tf_coreml_converter.py"", line 243, in _convert_pb_to_mlmodel
    tensors_evaluated = sess.run(tensors, feed_dict=input_feed_dict)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 950, in run
    run_metadata_ptr)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1173, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1350, in _do_run
    run_metadata)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1370, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[26] does not have value
```
"
32957,Unability to reach some modules (feature_column and data) with previous working code,"- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6
- TensorFlow installed from (source or binary): pip install tensorflow==2.0.0-beta1
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca 2.0.0
- Python version: v3.6.7:6ec5cf24b7, Oct 20 2018, 03:02:14

**Describe the current behavior**

`from tensorflow.feature_column import numeric_column`and `from tensorflow.data import Dataset` raise `ModuleNotFoundError: No module named 'tensorflow.feature_column'`and `ModuleNotFoundError: No module named 'tensorflow.data'`.

 The workaround `from tensorflow.python.data import Dataset` works but `from tensorflow.python.feature_column import numeric_column` fails and raises `ImportError: cannot import name 'numeric_column'`. To import `numeric_column` (or other functions of the module) I have found this workaround: `from tensorflow.compat.v2.feature_column import numeric_column`.
"
32956,Custom Loss in tensorflow 2.0,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Hi, I try to write a custom loss in tensorflow 2.0 RC. But the result of customLoss2 is not equal to sparse_categorical_crossentropy. How can I write it the right way? You can easily copy and run the code in Colab.

**Code to reproduce the issue**

```
from __future__ import absolute_import, division, print_function, unicode_literals
import os
import matplotlib.pyplot as plt
import tensorflow as tf
print(""TensorFlow version: {}"".format(tf.__version__))
print(""Eager execution: {}"".format(tf.executing_eagerly()))
from tensorflow.keras.layers import Dense, Flatten, Conv2D
from tensorflow.keras import Model

mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation='softmax')
])
def customLoss2(y_true, y_pred):
    return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True))
  
def customLoss3(y_true, y_pred):
    return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred))
  
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy', customLoss2])

model.fit(x_train, y_train, epochs=5)

model.evaluate(x_test, y_test)
```

**Other info / logs**
TensorFlow version: 2.0.0-rc2
Eager execution: True
Train on 60000 samples
Epoch 1/5
60000/60000 [==============================] - 5s 85us/sample - loss: 0.3025 - accuracy: 0.9114 - customLoss2: 1.6056
Epoch 2/5
60000/60000 [==============================] - 5s 84us/sample - loss: 0.1438 - accuracy: 0.9574 - customLoss2: 1.5303
Epoch 3/5
60000/60000 [==============================] - 5s 79us/sample - loss: 0.1063 - accuracy: 0.9677 - customLoss2: 1.5126
Epoch 4/5
60000/60000 [==============================] - 5s 78us/sample - loss: 0.0889 - accuracy: 0.9730 - customLoss2: 1.5038
Epoch 5/5
60000/60000 [==============================] - 5s 79us/sample - loss: 0.0738 - accuracy: 0.9769 - customLoss2: 1.4975
10000/1 [=====] - 1s 91us/sample - loss: 0.0398 - accuracy: 0.9763 - customLoss2: 1.4932
[0.07908454576081131, 0.9763, 1.4931514]
"
32954,Keras 2.2.4 Leaks Memory when using Tensorflow 2.0.0,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows and Ubuntu 19.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 7.6.0
- GPU model and memory: Quadro RTX 5000 16Gb

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
As I fit a custom model using the Keras API. I monitor the memory usage via Task Manager and I see that every .fit() call the memory increases until it eventually crashes the script with no warning whatsoever. It starts off with allocated 5gb of memory and by the time it crashes it as exceeded 16gb of memory.

**Describe the expected behavior**
I expect the memory to not continuously increase

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python

import numpy as np
import tensorflow as tf

class Generator:
    def __init__(self, latent_dim=5, seq_length=30, batch_size=28, hidden_size=100, num_generated_features=1):
        self.latent_dim = latent_dim
        self.seq_length = seq_length
        self.batch_size = batch_size
        self.hidden_size = hidden_size
        self.num_generated_features = num_generated_features

        # self.model = tf.keras.models.Sequential([
        #     LSTM(self.hidden_size, input_shape=(self.seq_length, self.latent_dim), return_sequences=True),
        #     tf.keras.layers.Dense(1, input_shape=[None, self.hidden_size]),
        #     tf.keras.layers.Activation('tanh'),
        #     Reshape(target_shape=(self.batch_size, self.seq_length, self.num_generated_features))
        # ])
        self.model = tf.keras.models.Sequential([
            tf.keras.layers.LSTM(self.hidden_size, input_shape=(
                self.seq_length, self.latent_dim), return_sequences=True, name='g_lstm1'),
            tf.keras.layers.LSTM(
                self.hidden_size, return_sequences=True, recurrent_dropout=0.4, name='g_lstm2'),
            tf.keras.layers.LSTM(1, return_sequences=True, name='g_lstm3')
        ], name='generator')


class Discriminator:
    def __init__(self, input_shape, hidden_size=100):
        self.model = tf.keras.models.Sequential([
            tf.keras.layers.LSTM(
                hidden_size, input_shape=input_shape, return_sequences=True, name='d_lstm'),
            tf.keras.layers.LSTM(
                hidden_size, return_sequences=True, name='d_lstm2', recurrent_dropout=0.4),
            tf.keras.layers.Dense(1, activation='linear', name='d_output')
        ], name='discriminator')

        self.model.compile(
            loss=self.d_loss, optimizer=tf.keras.optimizers.SGD(lr=0.1), metrics=['acc'])

    def d_loss(self, y_true, y_pred):
        loss = tf.keras.losses.binary_crossentropy(
            y_true, y_pred, from_logits=True)
        return loss


class GAN:
    real_loss = []
    fake_loss = []
    def __init__(self, *args, **kwargs):

        self.generator = Generator(*args, **kwargs)
        gen_output = (self.generator.seq_length,
                      self.generator.num_generated_features)
        self.discriminator = Discriminator(input_shape=gen_output)
        self.discriminator.model.trainable = False

        self.batch_size = self.generator.batch_size
        self.seq_length = self.generator.seq_length

        self.model = tf.keras.models.Sequential([
            self.generator.model,
            self.discriminator.model
        ], name='gan')

        self.model.compile(
            loss=self.gan_loss, optimizer=tf.keras.optimizers.SGD(lr=0.1), metrics=['acc'])

    def train(self, epochs, n_eval, d_train_steps=5, load_weights=False, metric='loss'):
        for epoch in range(epochs):
            start = time.time()

            for step in range(steps_over_data):
                tmp_r, tmp_f = [], []

                for _ in range(d_train_steps):

                    x_r, y_r = self.generator.real_samples()
                    x_f, y_f = self.generator.fake_samples()

                    real = self.discriminator.model.fit(
                        x_r, y_r, epochs=1, batch_size=self.batch_size, verbose=0, shuffle=True).history
                    fake = self.discriminator.model.fit(
                        x_f, y_f, epochs=1, batch_size=self.batch_size, verbose=0, shuffle=True).history

                    tmp_r.append(real[metric])
                    tmp_f.append(fake[metric])

            self.real_loss.append(np.mean(tmp_r))
            self.fake_loss.append(np.mean(tmp_f))

            x_gan = self.generator.sample_latent_space()
            y_gan = np.ones((self.batch_size, self.seq_length,
                             self.generator.num_generated_features)).astype(np.float32)

            self.model.fit(
                x_gan, y_gan, batch_size=self.batch_size, epochs=1, verbose=0)

if __name__ == '__main__':
    gan = GAN(latent_dim=5, seq_length=30, batch_size=128)
    gan.discriminator.model.summary()
    gan.load_weights()

    # crashes around epoch ~35
    gan.train(epochs=40, n_eval=1, d_train_steps=3,
              load_weights=True, metric='loss')

```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32952,ImportError: No module named 'tensorflow_core.estimator',"Hi, today I updated tensorflow. While trying to run tensorboard, I get the following error:

> ImportError: No module named 'tensorflow_core.estimator'

/home/niko/venv_tf2/bin/python /home/niko/workspace/pupil-detection-v2/tensorflow/diagnose_tensorboard.py
### Diagnostics

<details>
<summary>Diagnostics output</summary>

``````
--- check: autoidentify
INFO: diagnose_tensorboard.py version 4725c70c7ed724e2d1b9ba5618d7c30b957ee8a4

--- check: general
INFO: sys.version_info: sys.version_info(major=3, minor=5, micro=2, releaselevel='final', serial=0)
INFO: os.name: posix
INFO: os.uname(): posix.uname_result(sysname='Linux', nodename='helix', release='4.4.0-148-generic', version='#174-Ubuntu SMP Tue May 7 12:20:14 UTC 2019', machine='x86_64')
INFO: sys.getwindowsversion(): N/A

--- check: package_management
INFO: has conda-meta: False
INFO: $VIRTUAL_ENV: '/home/niko/venv_tf2'

--- check: installed_packages
INFO: installed: tensorboard==2.0.0
INFO: installed: tensorflow-gpu==2.0.0
INFO: installed: tensorflow-estimator==2.0.0

--- check: tensorboard_python_version
INFO: tensorboard.version.VERSION: '2.0.0'

--- check: tensorflow_python_version
INFO: tensorflow.__version__: '2.0.0'
INFO: tensorflow.__git_version__: 'v2.0.0-rc2-26-g64c3d38'

--- check: tensorboard_binary_path
INFO: which tensorboard: b'/home/niko/venv_tf2/bin/tensorboard\n'

--- check: readable_fqdn
INFO: socket.getfqdn(): 'helix'

--- check: stat_tensorboardinfo
INFO: directory: /tmp/.tensorboard-info
INFO: os.stat(...): os.stat_result(st_mode=16895, st_ino=2356525, st_dev=2066, st_nlink=2, st_uid=1000, st_gid=1000, st_size=4096, st_atime=1569914603, st_mtime=1569939157, st_ctime=1569939157)
INFO: mode: 0o40777

--- check: source_trees_without_genfiles
INFO: tensorboard_roots (1): ['/home/niko/venv_tf2/lib/python3.5/site-packages']; bad_roots (0): []

--- check: full_pip_freeze
INFO: pip freeze --all:
absl-py==0.8.0
astor==0.8.0
attrs==19.1.0
backcall==0.1.0
backports.weakref==1.0.post1
bleach==3.1.0
certifi==2019.6.16
chardet==3.0.4
colorama==0.4.1
cycler==0.10.0
decorator==4.4.0
defusedxml==0.6.0
entrypoints==0.3
enum34==1.1.6
gast==0.2.2
gitdb2==2.0.5
GitPython==3.0.2
google-pasta==0.1.7
grpcio==1.23.0
h5py==2.9.0
idna==2.8
ipykernel==5.1.2
ipython==7.7.0
ipython-genutils==0.2.0
jedi==0.15.1
Jinja2==2.10.1
json5==0.8.5
jsonschema==3.0.2
jupyter-client==5.3.1
jupyter-contrib-core==0.3.3
jupyter-core==4.5.0
jupyter-nbextensions-configurator==0.4.1
jupyterlab==1.0.9
jupyterlab-git==0.8.1
jupyterlab-server==1.0.6
Keras==2.2.5
Keras-Applications==1.0.8
Keras-Preprocessing==1.1.0
keras-rectified-adam==0.10.0
kiwisolver==1.1.0
Markdown==3.1.1
MarkupSafe==1.1.1
matplotlib==3.0.3
mistune==0.8.4
nbconvert==5.6.0
nbdime==1.1.0
nbformat==4.4.0
notebook==6.0.1
numpy==1.17.1
opencv-python==4.1.0.25
opt-einsum==3.0.1
pandas==0.24.2
pandocfilters==1.4.2
parso==0.5.1
pexpect==4.7.0
pickleshare==0.7.5
Pillow==6.1.0
pip==19.2.3
pkg-resources==0.0.0
prometheus-client==0.7.1
prompt-toolkit==2.0.9
protobuf==3.9.1
ptyprocess==0.6.0
Pygments==2.4.2
pyparsing==2.4.2
pyrsistent==0.15.4
python-dateutil==2.8.0
pytz==2019.2
PyYAML==5.1.2
pyzmq==18.1.0
requests==2.22.0
scipy==1.3.1
Send2Trash==1.5.0
setuptools==41.2.0
six==1.12.0
smmap2==2.0.5
tensorboard==2.0.0
tensorflow-estimator==2.0.0
tensorflow-gpu==2.0.0
termcolor==1.1.0
terminado==0.8.2
testpath==0.4.2
tornado==6.0.3
traitlets==4.3.2
urllib3==1.25.3
wcwidth==0.1.7
webencodings==0.5.1
Werkzeug==0.15.5
wheel==0.33.6
wrapt==1.11.2

``````

</details>

### Next steps

No action items identified. Please copy ALL of the above output,
including the lines containing only backticks, into your GitHub issue
or comment. Be sure to redact any sensitive information.

Process finished with exit code 0

"
32951,tf.keras.layers.StackedRNNCells() doesn't work with tf.contrib.rnn.ConvLSTMCell(),"Using tf.keras.layers.StackedRNNCells() with the ConvLSTM cell from tf.contrib.rnn.ConvLSTMCell() doesn't work.

```
enc_Conv2D_1 = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(16, 3, 2, 'same', activation=tf.nn.relu, 
                                                                                  kernel_initializer=tf.random_normal_initializer(stddev=math.sqrt(2.0/(kernel_size*kernel_size*16)))))
enc_Conv2D_2 = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(32, 3, 2, 'same', activation=tf.nn.relu, 
                                                                                  kernel_initializer=tf.random_normal_initializer(stddev=math.sqrt(2.0/(kernel_size*kernel_size*32)))))
enc_Conv2D_3 = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(64, 3, 2, 'same', activation=tf.nn.relu, 
                                                                                  kernel_initializer=tf.random_normal_initializer(stddev=math.sqrt(2.0/(kernel_size*kernel_size*64)))))
enc_Conv2D_4 = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(128, 3, 2, 'same', activation=tf.nn.relu, 
                                                                                  kernel_initializer=tf.random_normal_initializer(stddev=math.sqrt(2.0/(kernel_size*kernel_size*128)))))
   
enc_ConvLSTMCell = tf.contrib.rnn.ConvLSTMCell(2, [grid_size//16, grid_size//16, 128], 128, [5, 5])
enc_Stacked_ConvLSTMCells = tf.keras.layers.StackedRNNCells([enc_ConvLSTMCell]*2)
enc_ConvLSTMLayer = tf.keras.layers.RNN(enc_Stacked_ConvLSTMCells, return_state=True)

enc_conv1 = enc_Conv2D_1(enc_input_grids)
enc_conv2 = enc_Conv2D_2(enc_conv1)
enc_conv3 = enc_Conv2D_3(enc_conv2)
enc_conv4 = enc_Conv2D_4(enc_conv3)
                               
enc_output_and_state = enc_ConvLSTMLayer(enc_conv4)
```

When I run this code I get the following error:

```
Traceback (most recent call last):
  File ""E:\Code\LSTM_Prediction\Conv_LSTM_Encoder_Decoder\network.py"", line 349, in <module>
    net.build_network(map_grids, agent_enc_grids, other_vehs_grids, agent_dec_grids, grid_size, downsampling)   
  File ""E:\Code\LSTM_Prediction\Conv_LSTM_Encoder_Decoder\network.py"", line 217, in build_network
    enc_output_and_state = enc_ConvLSTMLayer(enc_conv4)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\keras\layers\recurrent.py"", line 701, in __call__
    return super(RNN, self).__call__(inputs, **kwargs)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 538, in __call__
    self._maybe_build(inputs)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 1603, in _maybe_build
    self.build(input_shapes)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\keras\layers\recurrent.py"", line 633, in build
    for dim in state_size
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\keras\layers\recurrent.py"", line 633, in <listcomp>
    for dim in state_size
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\tensor_shape.py"", line 1125, in as_shape
    return TensorShape(shape)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\tensor_shape.py"", line 690, in __init__
    self._dims = [as_dimension(d) for d in dims_iter]
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\tensor_shape.py"", line 690, in <listcomp>
    self._dims = [as_dimension(d) for d in dims_iter]
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\tensor_shape.py"", line 632, in as_dimension
    return Dimension(value)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\tensor_shape.py"", line 185, in __init__
    self._value = int(value)
TypeError: int() argument must be a string, a bytes-like object or a number, not 'TensorShapeV1'
```

This is clearly an issue with running StackedRNNCells() with the tf.contrib.rnn.ConvLSTMCell(), as if I modify the code to flatten the conv output and run with a standard LSTM cell, e.g. `tf.nn.rnn_cell.LSTMCell(state_size)`, it runs fine.
"
32950,Tensorflow 2.0 AdaDelta Optimizer,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
32949,WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000025328A10860>> could not be transformed and will be executed as-is,"verbosity=10

WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000025328A10860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000025328A10860>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000025328A10860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000025328A10860>>: AttributeError: module 'gast' has no attribute 'Num'
"
32948,tf2.0 model issue on conv3D with activation relu,"== tensorflow installed from info ==================
Name: tensorflow
Version: 2.0.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: /home/tbaggu/Desktop/pose-anomaly-detection/env/lib/python3.6/site-packages
Required-by:
`class C3Dmodel(Model):
  
    def __init__(self):
        super(C3Dmodel, self).__init__()
        self.conv1 = tf.keras.layers.Conv3D(filters=64,kernel_size=3,padding='SAME',activation=tf.keras.layers.ReLU)
        self.conv2 = tf.keras.layers.Conv3D(filters=128,kernel_size=3,padding='SAME',activation=tf.keras.layers.ReLU)
        self.conv3 = tf.keras.layers.Conv3D(filters=256,kernel_size=3,padding='SAME',activation=tf.keras.layers.ReLU)
        self.conv4 = tf.keras.layers.Conv3D(filters=512,kernel_size=3,padding='SAME',activation=tf.keras.layers.ReLU)
        self.pool1 = tf.keras.layers.MaxPool3D(pool_size=(1,2,2),strides=(1,2,2),padding='SAME')
        self.pool2 = tf.keras.layers.MaxPool3D(pool_size=2,strides=2,padding='SAME')
        self.fc1   = tf.keras.layers.Dense(4096,activation=tf.keras.layers.ReLU)
        self.fc2   = tf.keras.layers.Dense(512,activation=tf.keras.layers.ReLU)
        self.fc3   = tf.keras.layers.Dense(32,activation=tf.keras.layers.ReLU)
        self.fc4   = tf.keras.layers.Dense(1)
        self.dropout = tf.keras.layers.Dropout(0.5)
        self.flatten = tf.keras.layers.Flatten()

    def call(self,x):
        x = self.conv1(x)
        x = self.pool1(x)
        x = self.conv2(x)
        x = self.pool2(x)
        x = self.conv3(x)
        x = self.conv3(x)
        x = self.pool2(x)
        x = self.conv4(x)
        x = self.conv4(x)
        x = self.pool2(x)
        x = tf.pad(x, [[0, 0], [1, 1], [1, 1], [1, 1], [0, 0]])
        x = self.conv4(x)
        x = tf.pad(x, [[0, 0], [1, 1], [1, 1], [1, 1], [0, 0]])
        x = self.conv4(x)
        x = self.pool2(x)

        x = self.flatten(x)
        x = self.fc1(x)
        x = self.dropout(x)
        x = self.fc2(x)
        x = self.dropout(x)
        x = self.fc3(x)
        x = self.dropout(x)
        return self.fc4(x)
`

Input shape : (2, 16, 240, 320, 3)

Error:
`Traceback (most recent call last):
  File ""train.py"", line 45, in <module>
    main()
  File ""train.py"", line 43, in main
    tN.train()
  File ""train.py"", line 39, in train
    print(self.model(batch))
  File ""#HOME/env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 891, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""#HOME/src/model.py"", line 68, in call
    x = self.conv1(x)
  File ""#HOME/env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 891, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""#HOME/env/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/convolutional.py"", line 211, in call
    return self.activation(outputs)
  File ""#HOME/env/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/advanced_activations.py"", line 299, in __init__
    if max_value is not None and max_value < 0.:
  File ""#HOME/env/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 876, in __bool__
    return bool(self._numpy())
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()`

"
32947,TensorFlow 1.14 self-compiled build installs successfully but fails at runtime with 'ParallelInterleaveDataset' object has no attribute '_flat_structure',"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS 7.6
- TensorFlow installed from (source or binary): source
- TensorFlow version: ""git clone --branch r1.14 https://github.com/tensorflow/tensorflow.git""
- Python version: ""Python 3.6.8""
- Installed using virtualenv? pip? conda?: ""bazel built pip package""
- Bazel version (if compiling from source): ""https://github.com/bazelbuild/bazel/releases/download/0.24.1/bazel-0.24.1-installer-linux-x86_64.sh""
- GCC/Compiler version (if compiling from source): ""gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)""
- CUDA/cuDNN version: CUDA ""10.0"", cuDNN ""7-7.6.4.38-1.cuda10.0""
- GPU model and memory: ""Tesla K80, 11441MiB""


**Describe the problem**
The problem occurs after a error-free tensorflow source build. Trying to use this self-built package with the Object Detection API ""train.py"" legacy script results in the error shown below. The script runs without problems when using the official pip tensorflow-gpu package. 

The error:
```
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
Traceback (most recent call last):
  File ""/tensorflow/train.py"", line 184, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python3.6/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.6/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""/tensorflow/train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""/tensorflow/models/research/object_detection/legacy/trainer.py"", line 280, in train
    train_config.prefetch_queue_capacity, data_augmentation_options)
  File ""/tensorflow/models/research/object_detection/legacy/trainer.py"", line 59, in create_input_queue
    tensor_dict = create_tensor_dict_fn()
  File ""/tensorflow/train.py"", line 121, in get_next
    dataset_builder.build(config)).get_next()
  File ""/tensorflow/models/research/object_detection/builders/dataset_builder.py"", line 141, in build
    config.input_path[:], input_reader_config)
  File ""/tensorflow/models/research/object_detection/builders/dataset_builder.py"", line 86, in read_dataset
    sloppy=config.shuffle))
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1853, in apply
    return DatasetV1Adapter(super(DatasetV1, self).apply(transformation_func))
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1290, in apply
    dataset = transformation_func(self)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/data/experimental/ops/interleave_ops.py"", line 94, in _apply_fn
    buffer_output_elements, prefetch_input_elements)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/data/ops/readers.py"", line 253, in __init__
    **self._flat_structure)
AttributeError: 'ParallelInterleaveDataset' object has no attribute '_flat_structure'
```


Importing this self built package works without any apparent problems:
```
[root@b39e843663d1 tensorflow]# python3
Python 3.6.8 (default, Aug  7 2019, 17:28:10)
[GCC 4.8.5 20150623 (Red Hat 4.8.5-39)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
2019-10-01 09:25:39.270112: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
>>> 
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I've created a docker container to build the package with the following settings:

```
...
ENV PYTHON_BIN_PATH=""/usr/bin/python3""
ENV PYTHON_LIB_PATH=""/usr/local/lib64/python3.6/site-packages""
ENV TF_ENABLE_XLA=""Y""
ENV TF_NEED_OPENCL_SYCL=""N""
ENV TF_NEED_ROCM=""N""
ENV TF_NEED_CUDA=""Y""
ENV TF_NEED_TENSORRT=""Y""
ENV TF_CUDA_VERSION=""10""
ENV TF_CUDNN_VERSION=""7""
ENV TF_NCCL_VERSION=""2.4.8""
ENV TF_TENSORRT_VERSION=""5""
ENV TF_CUDA_PATHS=""/lib64,/usr/lib64,/usr/include,/usr/local/cuda-10.0""
ENV TF_CUDA_COMPUTE_CAPABILITIES=""7.5,3.7""
ENV TF_CUDA_CLANG=""N""
ENV GCC_HOST_COMPILER_PATH=""/usr/bin/gcc""
ENV TF_NEED_MPI=""N""
ENV CC_OPT_FLAGS=""-mavx -mavx2 -mfma -mfpmath=both -msse4.2 -march=native""
ENV TF_SET_ANDROID_WORKSPACE=""N""

RUN bash configure

RUN bazel build --config=opt --config=mkl --config=numa --config=noignite --config=nokafka //tensorflow/tools/pip_package:build_pip_package
RUN ./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
RUN pip3 install /tmp/tensorflow_pkg/tensorflow-*.whl
...
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
[root@b39e843663d1 tensorflow]# python3 /tensorflow/train.py --logtostderr --train_dir=/training_dataset/ --pipeline_config_path=/training_dataset/model.config
2019-10-01 09:23:10.704622: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /tensorflow/models/research/slim/nets/inception_resnet_v2.py:373: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.

WARNING:tensorflow:From /tensorflow/models/research/slim/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.

WARNING:tensorflow:From /tensorflow/train.py:55: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.

WARNING:tensorflow:From /tensorflow/train.py:184: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/absl/app.py:250: main (from __main__) is deprecated and will be removed in a future version.
Instructions for updating:
Use object_detection/model_main.py.
W1001 09:23:13.366838 140335641364288 deprecation.py:323] From /usr/local/lib/python3.6/site-packages/absl/app.py:250: main (from __main__) is deprecated and will be removed in a future version.
Instructions for updating:
Use object_detection/model_main.py.
WARNING:tensorflow:From /tensorflow/train.py:90: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.

W1001 09:23:13.367076 140335641364288 deprecation_wrapper.py:119] From /tensorflow/train.py:90: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.

WARNING:tensorflow:From /tensorflow/models/research/object_detection/legacy/trainer.py:266: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.create_global_step
W1001 09:23:13.375600 140335641364288 deprecation.py:323] From /tensorflow/models/research/object_detection/legacy/trainer.py:266: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.create_global_step
WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.
W1001 09:23:13.398977 140335641364288 dataset_builder.py:72] num_readers has been reduced to 1 to match input file shards.
WARNING:tensorflow:From /tensorflow/models/research/object_detection/builders/dataset_builder.py:86: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.experimental.parallel_interleave(...)`.
W1001 09:23:13.406527 140335641364288 deprecation.py:323] From /tensorflow/models/research/object_detection/builders/dataset_builder.py:86: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.experimental.parallel_interleave(...)`.
WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W1001 09:23:13.406693 140335641364288 deprecation.py:323] From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
Traceback (most recent call last):
  File ""/tensorflow/train.py"", line 184, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python3.6/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.6/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""/tensorflow/train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""/tensorflow/models/research/object_detection/legacy/trainer.py"", line 280, in train
    train_config.prefetch_queue_capacity, data_augmentation_options)
  File ""/tensorflow/models/research/object_detection/legacy/trainer.py"", line 59, in create_input_queue
    tensor_dict = create_tensor_dict_fn()
  File ""/tensorflow/train.py"", line 121, in get_next
    dataset_builder.build(config)).get_next()
  File ""/tensorflow/models/research/object_detection/builders/dataset_builder.py"", line 141, in build
    config.input_path[:], input_reader_config)
  File ""/tensorflow/models/research/object_detection/builders/dataset_builder.py"", line 86, in read_dataset
    sloppy=config.shuffle))
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1853, in apply
    return DatasetV1Adapter(super(DatasetV1, self).apply(transformation_func))
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1290, in apply
    dataset = transformation_func(self)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/data/experimental/ops/interleave_ops.py"", line 94, in _apply_fn
    buffer_output_elements, prefetch_input_elements)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/data/ops/readers.py"", line 253, in __init__
    **self._flat_structure)
AttributeError: 'ParallelInterleaveDataset' object has no attribute '_flat_structure'
```

Haven't seen this error anywhere yet with googling, so not sure whether this is a fault of my own build or a bug :/ 

Any help with this will be very appreciated!
"
32946,Error converting .pb model to .tflite,"System information
- OS Platform and Distribution : Windows 10 64-bit
- TensorFlow installed from (source or binary): Binary
-TensorFlow Version - 1.13.1
-Onnx Version - 1.6.0

I have pytorch model and I have to deploy it into android, so I saved the model in onnx and then converted the model in .pb
But not able to convert the model in tensorflowlite.


` import tensorflow as tf

graph_def_file = ""tensorf.pb""
input_arrays = ['Const']
output_arrays = ['concat_98']

converter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays)
tflite_model = converter.convert()
open(""tensorf_lite.tflite"", ""wb"").write(tflite_model) `

Error log -

`Traceback (most recent call last): File ""tfliteconvertor.py"", line 8, in <module> tflite_model = converter.convert() File ""C:\Program Files\Python37\lib\site-packages\tensorflow\lite\python\lite.py"", line 455, in convert **converter_kwargs) File ""C:\Program Files\Python37\lib\site-packages\tensorflow\lite\python\convert.py"", line 442, in toco_convert_impl input_data.SerializeToString()) File ""C:\Program Files\Python37\lib\site-packages\tensorflow\lite\python\convert.py"", line 205, in toco_convert_protos ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr)) tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info. 2019-10-01 12:18:57.243087: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: PyFunc 2019-10-01 12:18:57.243738: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: ""WrapDatasetVariant"" device_type: ""CPU""') for unknown op: WrapDatasetVariant 2019-10-01 12:18:57.244227: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: ""WrapDatasetVariant"" device_type: ""GPU"" host_memory_arg: ""input_handle"" host_memory_arg: ""output_handle""') for unknown op: WrapDatasetVariant 2019-10-01 12:18:57.244653: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: ""UnwrapDatasetVariant"" device_type: ""CPU""') for unknown op: UnwrapDatasetVariant 2019-10-01 12:18:57.245022: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: ""UnwrapDatasetVariant"" device_type: ""GPU"" host_memory_arg: ""input_handle"" host_memory_arg: ""output_handle""') for unknown op: UnwrapDatasetVariant 2019-10-01 12:18:57.245590: F tensorflow/lite/toco/import_tensorflow.cc:114] Check failed: attr.value_case() == AttrValue::kType (1 vs. 6)`

"
32945,Image load error,"I'm using tf: '2.0.0-rc1'

In the process to load image, you said

for image, label in labeled_ds.take(1):
  print(""Image shape: "", image.numpy().shape)
  print(""Label: "", label.numpy())

InvalidArgumentError: {{function_node __inference_Dataset_map_process_path_378}} slice index -1 of dimension 0 out of bounds.
	 [[{{node strided_slice}}]] [Op:IteratorGetNextSync]


and then:
train_ds = prepare_for_training(labeled_ds)

image_batch, label_batch = next(iter(train_ds))

InvalidArgumentError: {{function_node __inference_Dataset_map_process_path_378}} slice index -1 of dimension 0 out of bounds.
	 [[{{node strided_slice}}]] [Op:IteratorGetNextSync]

some kind of problem...
"
32944,Incorrect behaviour creating constants from complex arrays of length >=64,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0
- Python version: 3.7.3
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: Intel Iris Plus Graphics 640 1536 MB

**Describe the current behavior**
The following code produces an array of zeros after running several times (for me it seems to reliably be on the fourth run):
```tf.Session().run(tf.constant(1j*np.arange(64))```

**Describe the expected behavior**
That code should produce an array `[0j, 1j, 2j, ...]` no matter how many times it's run.

**Code to reproduce the issue**
```
import numpy as np
import tensorflow as tf
session = tf.Session()
for _ in range(4):
    print(session.run(tf.constant(1j*np.arange(64))))
```
Also see Colab notebook here: https://colab.research.google.com/drive/1Ci-bqoMpgK4JfCt0s2eUFjHR0vgOo_PF

**Other info / logs**
I have no idea what's happening, but some observations:
 - only occurs for arrays >=64 in length
 - also occurs if we construct the list manually instead of with numpy (e.g. `session.run(tf.constant([0] + [1j]*63))`)
 - doesn't occur if `1j` is replaced by `1+1j`
 - doesn't occur if the constant is created first, then multiplied by `1j` (e.g. `session.run(1j*tf.constant(np.arange(64).astype(np.complex128)))` is fine)
"
32943,TensorFlow 2.x Java/Maven release,"Hello,

Our library really depends on `libtensorflow` and `libtensorflow_jni_gpu` releases on Maven, is there any ETA for releasing TensorFlow 2.0 on Maven? The latest release is `1.14.0` now.

Many thanks"
32942,Tensorflow 2 with CUDA 9?,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 2.0
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: 1060 and 6Gb
- **Exact command to reproduce**:   import tensorflow as tf


### Describe the problem

I have CUDA 9 which works with older version of TF but now with TF 2.0 when running following command in Python I get the error  **Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found**

`import tensorflow as tf`

Does TF 2 support building from source on Windows 10? I dont see CUDA 9 support mentioned on this link https://www.tensorflow.org/install/gpu

"
32941,doesn't support control flow ops,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 18.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (or github SHA if from source): '1.15.0-dev20190821'


**Provide the text output from tflite_convert**
2019-10-01 11:52:29.446292: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-10-01 11:52:29.446369: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-10-01 11:52:29.446391: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-10-01 11:52:29.446410: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-10-01 11:52:29.446431: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2019-10-01 11:52:29.446452: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-10-01 11:52:29.446470: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2019-10-01 11:52:29.446486: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-10-01 11:52:29.446503: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2019-10-01 11:52:29.446519: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-10-01 11:52:29.446538: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-10-01 11:52:29.446556: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-10-01 11:52:29.446575: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-10-01 11:52:29.446593: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-10-01 11:52:29.446611: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-10-01 11:52:29.446630: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-10-01 11:52:29.446649: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-10-01 11:52:29.446668: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-10-01 11:52:29.446686: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-10-01 11:52:29.446705: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-10-01 11:52:29.446723: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-10-01 11:52:29.446742: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-10-01 11:52:29.446760: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-10-01 11:52:29.446778: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-10-01 11:52:29.446799: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-10-01 11:52:29.446830: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-10-01 11:52:29.446850: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-10-01 11:52:29.446865: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-10-01 11:52:29.446879: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-10-01 11:52:29.446897: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-10-01 11:52:29.446912: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-10-01 11:52:29.446926: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-10-01 11:52:29.446944: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-10-01 11:52:29.446958: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-10-01 11:52:29.446973: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-10-01 11:52:29.447055: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-10-01 11:52:29.447091: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: LoopCond
2019-10-01 11:52:29.447124: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit
2019-10-01 11:52:29.447144: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit
2019-10-01 11:52:29.447162: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit
2019-10-01 11:52:29.447348: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayConcatV3
2019-10-01 11:52:29.447371: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayConcatV3
2019-10-01 11:52:29.447391: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayConcatV3
2019-10-01 11:52:29.447420: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: NonMaxSuppressionV3
2019-10-01 11:52:29.447953: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: CropAndResize
2019-10-01 11:52:29.448061: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: NonMaxSuppressionV3
2019-10-01 11:52:29.448096: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3
2019-10-01 11:52:29.448115: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3
2019-10-01 11:52:29.448133: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3
2019-10-01 11:52:29.448321: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: NonMaxSuppressionV3
2019-10-01 11:52:29.448763: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: CropAndResize
2019-10-01 11:52:29.449086: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: NonMaxSuppressionV3
2019-10-01 11:52:29.456571: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 483 operators, 916 arrays (0 quantized)
2019-10-01 11:52:29.464789: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 461 operators, 876 arrays (0 quantized)
2019-10-01 11:52:29.474610: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 461 operators, 876 arrays (0 quantized)
2019-10-01 11:52:29.484660: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 435 operators, 814 arrays (0 quantized)
2019-10-01 11:52:29.495248: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 435 operators, 814 arrays (0 quantized)
2019-10-01 11:52:29.503154: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 435 operators, 814 arrays (0 quantized)
2019-10-01 11:52:29.511581: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 2160448 bytes, theoretical optimal value: 2160320 bytes.
2019-10-01 11:52:29.513604: I tensorflow/lite/toco/toco_tooling.cc:454] Number of parameters: 496191
2019-10-01 11:52:29.664171: E tensorflow/lite/toco/toco_tooling.cc:481] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

```
# Copy and paste here
```TensorFlow Lite currently doesn't support control flow ops: Enter, Exit, Merge, Switch. We are working on supporting control flow ops, please see github issue at https://github.com/tensorflow/tensorflow/issues/28485. Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DIV, EXPAND_DIMS, GATHER, GREATER, MAX_POOL_2D, MUL, PACK, REDUCE_MAX, REDUCE_PROD, RESHAPE, RESIZE_BILINEAR, SELECT, SHAPE, SOFTMAX, SQUEEZE, STRIDED_SLICE, SUB, TILE, WHERE. Here is a list of operators for which you will need custom implementations: CropAndResize, LoopCond, NonMaxSuppressionV3, TensorArrayConcatV3, TensorArrayV3, TensorArrayWriteV3.
Traceback (most recent call last):
  File ""/home/konsultera/.local/bin/toco_from_protos"", line 10, in <module>
    sys.exit(main())
  File ""/home/konsultera/.local/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 89, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/home/konsultera/.local/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/home/konsultera/.local/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 52, in execute
    enable_mlir_converter)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

TensorFlow Lite currently doesn't support control flow ops: Enter, Exit, Merge, Switch. We are working on supporting control flow ops, please see github issue at https://github.com/tensorflow/tensorflow/issues/28485. Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DIV, EXPAND_DIMS, GATHER, GREATER, MAX_POOL_2D, MUL, PACK, REDUCE_MAX, REDUCE_PROD, RESHAPE, RESIZE_BILINEAR, SELECT, SHAPE, SOFTMAX, SQUEEZE, STRIDED_SLICE, SUB, TILE, WHERE. Here is a list of operators for which you will need custom implementations: CropAndResize, LoopCond, NonMaxSuppressionV3, TensorArrayConcatV3, TensorArrayV3, TensorArrayWriteV3.

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32940,My ground_truth.pbtxt and images folder of Preprocessing the minival dataset code are empty!,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):Source
- TensorFlow version (use command below):1.15
- Python version:3.6
- Bazel version (if compiling from source):0.26.1
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:No
- GPU model and memory:



when I run this command I get images folder and ground_truth.pbtxt empty:
bazel run //tensorflow/lite/tools/evaluation/tasks/coco_object_detection:preprocess_coco_minival -- \
  --images_folder=/path/to/val2014 \
  --instances_file=/path/to/instances_val2014.json \
  --whitelist_file=/path/to/minival_whitelist.txt \
  --output_folder=/path/to/output/folder


"
32939,Quantize training flow in TF 2.0,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):2.0.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
In TF 1.x I use quantize training flow to create fixed point models that run faster on edge devices without GPU. I certainly want that be available in TF 2.0.

**Will this change the current api? How?**
It shouldn't.

**Who will benefit with this feature?**
IoT & mobile AI practitioners.

**Any Other info.**
"
32938,Documentation for streaming training data from disk.,"I think we need more detailed description and example for streaming training data from disk on
https://www.tensorflow.org/guide/data#basic_mechanics.

It is mentioned on other documents, but no how-tos. It would be helpful if we add how to implement streaming data from disk and improvements on TF2.0, if any.

> When iterating over training data that fits in memory, feel free to use regular Python iteration. Otherwise, tf.data.Dataset is the best way to stream training data from disk. https://www.tensorflow.org/guide/effective_tf2

> For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If tensors contains one or more large NumPy arrays, consider the alternative described in this guide.
https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices
** The link to ""this guide"" is broken.


> The tf.data API supports a variety of file formats so that you can process large datasets that do not fit in memory. For example, the TFRecord file format is a simple record-oriented binary format that many TensorFlow applications use for training data. The tf.data.TFRecordDataset class enables you to stream over the contents of one or more TFRecord files as part of an input pipeline.
https://www.tensorflow.org/guide/data#consuming_tfrecord_data

Also, it would be helpful if we make it clear...
- If the tf.data.TFRecordDataset is the only class that supports streaming.
- If the TFRecord is the only file format that supports streaming.
- If the user needs to convert their dataset to TFRecord format.
- If the trained model can be used with TFLite. 
- etc..."
32937,Is it possible to convert a FP16 checkpoints to be compatible with FP32 runs ?,"Hi, looks like AMP ( or FP16) trained  checkpoints are not compatible with non AMP  session runs in Tensorflow.   Is it possible to convert a check point to be compatible with both AMP and non-AMP runs ? 


"
32933,tensorflow_transform can't be used with TF2.0,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
tensorflow_transform throwing error when Tf 2.0.0 version is installed. 

     17 # pylint: disable=wildcard-import
     18 from tensorflow_transform import coders
---> 19 from tensorflow_transform.analyzers import *
     20 from tensorflow_transform.api import apply_function
     21 from tensorflow_transform.inspect_preprocessing_fn import *

/usr/local/lib/python3.7/site-packages/tensorflow_transform/analyzers.py in <module>
     42 
     43 
---> 44 from tensorflow.contrib.boosted_trees.python.ops import gen_quantile_ops
     45 from tensorflow.contrib.boosted_trees.python.ops import quantile_ops
     46 from tensorflow.python.ops import resources

ModuleNotFoundError: No module named 'tensorflow.contrib'

**Describe the expected behavior**
Should work with tfx pipeline such as tensorflow_transform and tensorflow_model_analysis.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Install tensorflow and tfx in python 3.7
import tensorflow as tf
import tensorflow_transform as tft
import tensorflow_model_analysis as tfma


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32932,BatchMatMul for TFLite,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):tf2.0.0-rc1
- Are you willing to contribute it (Yes/No):No

**Describe the feature and the current behavior/state.**
Currently tf.matmul hides batch matmuls in code and this means seemingly benign operations (tf.matmul is listed as supported) will cause a model to not be convertable to TFLite without using TF_SELECT_OPS and compiling a custom interpreter.
**Will this change the current api? How?** No

**Who will benefit with this feature?** All users who wish to convert models to TFLite models and use matmul for batch multiplication.

**Any Other info.**
"
32930,Example title,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version: python3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
No.
**Describe the expected behavior**
I do not know
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
`pip install ~~~ `

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
~~"
32928,morphological layer,"How can I implement efficiently a layer of morphological neurons (see the papers DOI: [10.1109/ICPR.1996.547657](https://doi.org/10.1109/ICPR.1996.547657) or DOI: [10.1109/TNN.2003.809427](https://doi.org/10.1109/TNN.2003.809427)) in tensorflow? Morphological neurons are obtained by replacing the product by the sum and the sum by either the maximum or the minimum operations. In its simplest form, the morphological neurons are given by

![image](https://user-images.githubusercontent.com/51181168/65959161-abe0ba80-e427-11e9-93f7-f7d72756a0bd.png)  or ![image](https://user-images.githubusercontent.com/51181168/65959172-b307c880-e427-11e9-9d8e-651cecbe87dc.png) , for all i=1,...,m,

where  is the synaptic weight, x =  is the input vector and  is the output. The mathematical background for morphological neurons is lattice theory, which also the mathematical background for fuzzy sets and mathematical morphology - a theory widely used for image processing and analysis.
"
32927,set_weights crashes in tf.function because it tries to convert tensors into numpy arrays,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2 rc2
- Python version: 3.6
- CUDA/cuDNN version: 10
- GPU model and memory: 1070, n/a

**Describe the current behavior**
if you try set_weights inside tf.function, your program crashes because keras attempts to convert tensors into numpy arrays

```
        self.layer.set_weights(weights)
CAUSES:
        x.assign(np.asarray(value, dtype=dtype(x)))
    anaconda3/envs/tfg/lib/python3.6/site-packages/numpy/core/_asarray.py:85 asarray
        return array(a, dtype, copy=False, order=order)

    TypeError: __array__() takes 1 positional argument but 2 were given
OR
       weights.append(w.numpy())
CAUSES:
    AttributeError: 'Tensor' object has no attribute 'numpy'
OR
        weights.append(w.eval())
CAUSES
    ValueError: Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`
```

**Describe the expected behavior**
keras should work as described in the API without calling numpy behind the scenes in graph mode and crashing

**Code to reproduce the issue**
```
from functools import reduce
from operator import mul
import tensorflow as tf
B = tf.keras.backend
L = tf.keras.layers


def get_size(shape):
    return shape[0] if len(shape) is 1 else reduce(mul, shape)


def pipe(*args, repeats=1):

    def call(x):
        for i in range(repeats):
            for arg in args:
                x = arg(x)
        return x
    return call


class Hyper(L.Wrapper):

    def __init__(self, layer):
        super().__init__(layer)

    def build(self, shape):
        super().build(shape)
        shapes = [w.shape for w in self.layer.get_weights()]
        self.hypers = []
        for k, shape in enumerate(shapes):
            print(""shape"", k, shape)
            hyper = L.Dense(get_size(shape))
            super().__setattr__(""hyper_{k}"", hyper)
            reshape = L.Reshape(shape)
            super().__setattr__(""reshape_{k}"", reshape)
            self.hypers.append(pipe(hyper, reshape))
        self.flatten = L.Flatten()

    @tf.function
    def call(self, inputs):
        batch = B.int_shape(inputs)[0]
        Y = []
        for x in tf.split(inputs, batch):
            x = self.flatten(x)
            weights = []
            for hyper in self.hypers:
                w = hyper(x)
                w = tf.squeeze(w, 0)
                weights.append(w)
            self.layer.set_weights(weights)
            y = self.layer(x)
            Y.append(y)
        return tf.concat(Y, 0)

**Other info / logs**
"
32926,"""Upgrading your code to TensorFlow 2.0"" is behind a Medium paywall","## URL(s) with the issue:

https://medium.com/tensorflow/upgrading-your-code-to-tensorflow-2-0-f72c3a4d83b5

## Description of issue (what needs changing):

This upgrade guide is linked from the TensorFlow 2.0 release notes. If the viewer has exceeded their quota on Medium, they are blocked by a paywall and cannot read the upgrade guide. Is this intentional?

![image](https://user-images.githubusercontent.com/475017/65907458-68883c80-e392-11e9-82d0-edf8ad7efbe0.png)
"
32925,tensorflow 2 rc2 + tensorrt6 compile error,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source V2.0.0-rc2
- TensorFlow version:
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 26.1
- GCC/Compiler version (if compiling from source):7.4..0
- CUDA/cuDNN version:10.1.243 / 7.6.4
- GPU model and memory: rtx 2060 6GB



**Describe the problem**
during compilation the file '/usr/include/x86_64-linux-gnu/NvInferRTSafe.h' can not be found, i suspect this is a path issue. I installed tensorRT using a debian local repo using the nvidia instructions. 

**seems to work without tensorrt in config [still compiling]**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

bazel build --verbose_failures --config=v2 //tensorflow/tools/pip_package:build_pip_package




**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

important line (i think): cp: cannot stat '/usr/include/x86_64-linux-gnu/NvInferRTSafe.h': No such file or directory

WARNING: The following configs were expanded more than once: [v2]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=204
INFO: Reading rc options for 'build' from /home/kevin/Documents/tensorflow/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --config=v2
INFO: Reading rc options for 'build' from /home/kevin/Documents/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/kevin/bin/python --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.6/dist-packages --python_path=/home/kevin/bin/python --config=xla --config=tensorrt --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda --action_env TF_CUDA_COMPUTE_CAPABILITIES=7.0,7.5 --action_env LD_LIBRARY_PATH=/usr/local/cuda-10.1/lib64 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/gcc --config=cuda --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:v2 in file /home/kevin/Documents/tensorflow/.bazelrc: --define=tf_api_version=2
INFO: Found applicable config definition build:xla in file /home/kevin/Documents/tensorflow/.tf_configure.bazelrc: --define with_xla_support=true
INFO: Found applicable config definition build:tensorrt in file /home/kevin/Documents/tensorflow/.bazelrc: --action_env TF_NEED_TENSORRT=1
INFO: Found applicable config definition build:cuda in file /home/kevin/Documents/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file /home/kevin/Documents/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain
INFO: Found applicable config definition build:v2 in file /home/kevin/Documents/tensorflow/.bazelrc: --define=tf_api_version=2
DEBUG: /home/kevin/.cache/bazel/_bazel_kevin/7795dc8f03c6c042206d41951ce00a84/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5: 
Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default
WARNING: /home/kevin/Documents/tensorflow/tensorflow/core/BUILD:2441:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:abi.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/kevin/Documents/tensorflow/tensorflow/core/BUILD:2441:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:annotation.h' directly. You should either move the file to this package or depend on an appropriate rule there
... this warning continues a lot of times

ERROR: /home/kevin/.cache/bazel/_bazel_kevin/7795dc8f03c6c042206d41951ce00a84/external/local_config_tensorrt/BUILD:43:1: Executing genrule @local_config_tensorrt//:tensorrt_include failed (Exit 1): bash failed: error executing command 
  (cd /home/kevin/.cache/bazel/_bazel_kevin/7795dc8f03c6c042206d41951ce00a84/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda-10.1/lib64 \
    PATH=/usr/local/cuda-10.1/bin:/usr/local/cuda-10.1/NsightCompute-2019.1:/home/kevin/bin:/home/kevin/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; cp -f ""/usr/include/x86_64-linux-gnu/NvInfer.h"" ""bazel-out/host/bin/external/local_config_tensorrt/tensorrt/include/NvInfer.h"" && cp -f ""/usr/include/x86_64-linux-gnu/NvUtils.h"" ""bazel-out/host/bin/external/local_config_tensorrt/tensorrt/include/NvUtils.h"" && cp -f ""/usr/include/x86_64-linux-gnu/NvInferPlugin.h"" ""bazel-out/host/bin/external/local_config_tensorrt/tensorrt/include/NvInferPlugin.h"" && cp -f ""/usr/include/x86_64-linux-gnu/NvInferVersion.h"" ""bazel-out/host/bin/external/local_config_tensorrt/tensorrt/include/NvInferVersion.h"" && cp -f ""/usr/include/x86_64-linux-gnu/NvInferRTSafe.h"" ""bazel-out/host/bin/external/local_config_tensorrt/tensorrt/include/NvInferRTSafe.h"" && cp -f ""/usr/include/x86_64-linux-gnu/NvInferRTExt.h"" ""bazel-out/host/bin/external/local_config_tensorrt/tensorrt/include/NvInferRTExt.h"" && cp -f ""/usr/include/x86_64-linux-gnu/NvInferPluginUtils.h"" ""bazel-out/host/bin/external/local_config_tensorrt/tensorrt/include/NvInferPluginUtils.h"" ')
Execution platform: @bazel_tools//platforms:host_platform
cp: cannot stat '/usr/include/x86_64-linux-gnu/NvInferRTSafe.h': No such file or directory
Target //tensorflow/tools/pip_package:build_pip_package failed to build


"
32924,Malformed documentation for tf.strings.split,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:
https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/strings/split

## Description of issue (what needs changing):
Last section is raw markdown instead of formatted HTML.

### Clear description

For example, why should someone use this method? How is it useful?

### Correct links

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined

Are return values defined?

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example?

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
32923,No documentation for tf.strings.reduce_join,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:
https://www.tensorflow.org/api_docs/python/tf/strings/reduce_join

## Description of issue (what needs changing):
Add documentation for this method.

### Clear description

### Correct links

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined

Are return values defined?

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example?

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
32921,Unable to input categorical columns in my preprocessing layer for keras,"Hi All,

I am trying to use python to run the [example](https://www.tensorflow.org/tutorials/load_data/csv) which is given on the tensorflow website on working with .CSV data. This example is meant to use some features to predict if a passenger is labeled dead or alive on the titanic. Everything goes well until the point where I attempt to run a fit on the training data data. The error below is generated:

FailedPreconditionError: Table already initialized.
	 [[{{node sequential_9/dense_features_17/embark_town_indicator/embark_town_lookup/hash_table/table_init/LookupTableImportV2}}]] [Op:__inference_keras_scratch_graph_11639]

If I attempt to use the numeric features alone the code runs to completion. I have output the layers from a batch (small size batch of 3 - 5) and it makes sense to the data which is input.   Has anyone had similar experience or would anyone kindly help with this? Thank you. I have added the link to the website so that it can be accessed by anyone checking but if you need the ipython file, I can provide that too. Thanks in advance."
32920,How to explitly save the values of every buffer in a tflite model?,"When using interpreter.get_tensor() to get the values in a tensor after calling interpreter.invoke(), some values seem to be overwritten. If I understand correctly, to explicitly prevent those values from being overwritten, we have to append the names of the tensors we care about in the ""output_arrays"" argument when calling tf.lite.TFLiteConverter.from_saved_model(). However, some tensors in a tflite model are called ""xx_int8"" and they don't exist in the original saved model. In this case, how do we include them in ""output_arrays""?"
32919,Multiple sessions with per_process_gpu_memory_fraction,"**System information**
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): 1.14.0
- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0
- Python version: 3.6.8
- CUDA/cuDNN version: 10.0 / 7.4.2.24-1
- GPU model and memory: GTX 1070ti, 8119Mb

**Describe the current behavior**
Start multiple sessions with gpu_options and different per_process_gpu_memory_fraction, but tensorflow outputs same amount of memory for both sessions:
```
...
tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5a67330 executing computations on platform CUDA. Devices:
tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1070 Ti, Compute Capability 6.1
tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3696000000 Hz
tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5a645c0 executing computations on platform Host. Devices:
tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce GTX 1070 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683
pciBusID: 0000:01:00.0
tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 500 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)

... init second ...

tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 500 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
```

**Describe the expected behavior**
I expect each session to allocate amount of memory specified in config.gpu_options

```
...
Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 500 MB memory)
...
Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 300 MB memory)
```

**Code to reproduce the issue**
    
    def init_first(self):
        graph_def = tf.compat.v1.GraphDef()
        with tf.io.gfile.GFile(self.MODEL_PATH[0], 'rb') as f:
            graph_def.ParseFromString(f.read())

        graph = tf.Graph()
        with graph.as_default():
            tf.import_graph_def(graph_def, name='import')

        input = graph.get_tensor_by_name('import/image_tensor:0')
        output = [
            graph.get_tensor_by_name('import/boxes:0'),
            graph.get_tensor_by_name('import/scores:0'),
        ]

        config = tf.compat.v1.ConfigProto()
        config.gpu_options.allow_growth = True
        config.gpu_options.per_process_gpu_memory_fraction = 0.061
        sess = tf.compat.v1.Session(graph=graph, config=config)
        return sess, input, output

    def init_second(self):
        graph_def = tf.compat.v1.GraphDef()
        with tf.io.gfile.GFile(self.MODEL_PATH[1], 'rb') as f:
            graph_def.ParseFromString(f.read())

        graph = tf.Graph()
        with graph.as_default():
            tf.import_graph_def(graph_def)

        config = tf.compat.v1.ConfigProto()
        config.gpu_options.allow_growth = True
        config.gpu_options.per_process_gpu_memory_fraction = 0.37
        sess = tf.compat.v1.Session(graph=graph, config=config)

        output = ('import/softmax_2/softmax:0',
                  'import/conv6-2/conv6-2:0',
                  'import/conv6-3/conv6-3:0')
        input = 'import/Placeholder_2:0'

        return sess, input, output

    firsrt = init_first()
    second = init_second()


"
32916,local variable 'nccl_lib_path' referenced before assignment,"When I compile the tensorflow-1.12.0 version , I process command :
./configure

and then I got this error:
  File ""./configure.py"", line 1693, in <module>
    main()
  File ""./configure.py"", line 1612, in main
    set_tf_nccl_install_path(environ_cp)
  File ""./configure.py"", line 1208, in set_tf_nccl_install_path
    nccl_lib_path = os.path.join(nccl_install_path, nccl_lib_path)
UnboundLocalError: local variable 'nccl_lib_path' referenced before assignment


my bazel version is 0.15.0,  gcc is 5.4.0"
32915,Wrong links in API document and search results,"Items in TensorFlow Core r1.14 is linked to r2.0.
In search results, all informations are correct except the link.
All links point to RC version, not r1.14.

I checked tf.nn.dynamic_rnn and tf.nn.fused_batch_norm.
dynamic_rnn is linked to [https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn) now.
But [https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/nn/dynamic_rnn](https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/nn/dynamic_rnn) is right."
32914,set tflite interpreter gpu delegate on android，it's output is different without gpu delegate,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):android
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: MI PAD 4 PLUS
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below):1.14
- Python version:3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
set tflite interpreter gpu delegate on android，it's output is different without gpu delegate, and it seems that whatever data i fed, the model output is the same, is it something wrong with my code ？and i have checked my code, it's output is correct without set the gpu delegate.

**Describe the expected behavior**
use gpu to accelerate inference
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
here is the code i set the tflite interpreter options:

```
tfliteModel = loadModelFile(activity);
    switch (device) {
        case NNAPI:
            tfliteOptions.setUseNNAPI(true);
            break;
        case GPU:
            gpuDelegate = new GpuDelegate();
            tfliteOptions.addDelegate(gpuDelegate);
            break;
        case CPU:
            break;
    }
    tfliteOptions.setNumThreads(numThreads);
    tflite = new Interpreter(tfliteModel, tfliteOptions);
```
and here is the code i used to inference(where the input is a four dim array, and output is a two dim array, corresponding to the model input and output dimentions):
`tflite.run(audioData, output);`
**Other info / logs**
14:11:28.433 30654-30654/? I/tflite: Created TensorFlow Lite delegate for GPU.
2019-09-30 14:11:28.435 30654-30654/? I/tflite: Initialized TensorFlow Lite runtime.
2019-09-30 14:11:28.464 30654-30654/? I/Adreno: QUALCOMM build                   : dcd4b96, I568c71768a
    Build Date                       : 04/30/18
    OpenGL ES Shader Compiler Version: EV031.22.00.01_06
    Local Branch                     : 
    Remote Branch                    : quic/gfx-adreno.lnx.1.0.r33-rel
    Remote Branch                    : NONE
    Reconstruct Branch               : NOTHING
2019-09-30 14:11:28.467 30654-30654/? I/Adreno: PFP: 0x005ff087, ME: 0x005ff063
2019-09-30 14:11:28.471 30654-30654/? I/zygote64: android::hardware::configstore::V1_0::ISurfaceFlingerConfigs::hasWideColorDisplay retrieved: 0
2019-09-30 14:11:28.472 30654-30654/? E/libEGL: call to OpenGL ES API with no current context (logged once per thread)
2019-09-30 14:11:28.659 3275-3275/? D/EventBus: [3275, u0] send(AppTransitionFinishedEvent)
2019-09-30 14:11:28.659 3275-3275/? D/EventBus: [3275, u0]  -> ForcedResizableInfoActivityController [0xe31f776, P1] onBusEvent(AppTransitionFinishedEvent)
2019-09-30 14:11:28.659 3275-3275/? D/EventBus: [3275, u0] onBusEvent(AppTransitionFinishedEvent) duration: 17 microseconds, avg: 159
2019-09-30 14:11:28.659 3275-3275/? D/EventBus: [3275, u0] send(AppTransitionFinishedEvent)
2019-09-30 14:11:28.659 3275-3275/? D/EventBus: [3275, u0]  -> ForcedResizableInfoActivityController [0xe31f776, P1] onBusEvent(AppTransitionFinishedEvent)
2019-09-30 14:11:28.659 3275-3275/? D/EventBus: [3275, u0] onBusEvent(AppTransitionFinishedEvent) duration: 6 microseconds, avg: 159
2019-09-30 14:11:29.152 30654-30674/? D/OpenGLRenderer: HWUI GL Pipeline
2019-09-30 14:11:29.161 30654-30654/? I/Toast: Show toast from OpPackageName:com.example.dm.testing, PackageName:com.example.dm.testing
2019-09-30 14:11:29.201 30654-30674/? I/OpenGLRenderer: Initialized EGL, version 1.4
2019-09-30 14:11:29.201 30654-30674/? D/OpenGLRenderer: Swap behavior 2
2019-09-30 14:11:29.321 2629-2679/? I/ActivityManager: Displayed com.example.dm.testing/.MainActivity: +1s153ms
2019-09-30 14:11:29.322 4049-4487/? D/PowerKeeper.Event: notifyActivityLaunchTime: com.example.dm.testing/.MainActivity totalTime: 1153
2019-09-30 14:11:29.322 598-598/? W//system/bin/hwservicemanager: getTransport: Cannot find entry vendor.qti.hardware.iop@1.0::IIop/default in either framework or device manifest.
2019-09-30 14:11:29.322 2629-2679/? E/ANDR-PERF-JNI: Iop tryGetService failed
2019-09-30 14:11:29.360 2629-16635/? D/ActivityTrigger: ActivityTrigger activityStopTrigger 
2019-09-30 14:11:29.363 6198-6198/? D/Launcher.AdPendantUtils: updateAdvertisementPendantVisibility, mIsAdPendantEnable=false, mIsPullActionEnable=false, mIsEditDisabled=true, mCurrentScreenId=2, mDefaultScreenId=2, mIsLauncherVisible=false, mIsMinusOneScreenShow=false
2019-09-30 14:11:29.452 3880-8501/? D/com.xiaomi.common.Network: Http POST Response Code: 200
2019-09-30 14:11:31.156 768-815/? E/ANDR-PERF-OPTSHANDLER: perf_lock_rel: updated /sys/class/mmc_host/mmc0/clk_scaling/enable with 1
     return value 2
"
32913,Attention :module 'tensorflow.python.keras.api._v2.keras.layers' has no attribute 'Attention',"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Win10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below):2.0.0a0
- Python version:3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`
model = keras.Sequential()
model.add(tf.keras.layers.Embedding(max_words, embed_size))
model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)))
model.add(tf.keras.layers.Attention())
**Describe the current behavior**
AttributeError: module 'tensorflow.python.keras.api._v2.keras.layers' has no attribute 'Attention'
**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32912,model.fit with tf.data.Dataset.from_generator can't infer shape,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOs 10.13.6
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.0.0-beta1-5101-gc75bb66a99 2.0.0-rc0
- Python version: 3.6.4 
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

`model.fit` when given a `tf.data` dataset (that uses a `from_generator`) throws `ValueError: as_list() is not defined on an unknown TensorShape.` 

see https://gist.github.com/matpalm/779c2c67d5ec195845ae2ab01570d883 for full traceback

**Describe the expected behavior**

model.fit should be able to infer the shape of the input.

**Code to reproduce the issue**

```
import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

inp = Input(shape=(3,))
output = Dense(1, activation='sigmoid')(inp)
model = Model(inp, output)
model.compile(optimizer=Adam(1e-2), loss='binary_crossentropy')

def simple_generator():
    while True:
        yield [0.5, 0.2, -0.3], 0.0
        yield [-0.5, 0.3, -0.1], 1.0
        
dataset = tf.data.Dataset.from_generator(simple_generator,
                                         output_types=(tf.float32,
                                                       tf.float32))
dataset = dataset.batch(4).prefetch(1)

model.fit(dataset)
```

throws `ValueError: as_list() is not defined on an unknown TensorShape.`

note: the following works...

```
for X, y in dataset:
  model.fit(X, y)
  break
```

**Other info / logs**

see https://gist.github.com/matpalm/779c2c67d5ec195845ae2ab01570d883 for full traceback"
32911,tf.losses.mean_squared_error returns a list in tensorflow '2.0.0-rc1',"Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
```uname -mrs
Linux 3.10.0-514.el7.x86_64 x86_64
```
TensorFlow installed from (source or binary): binary (pip)
TensorFlow version (use command below): 'v2.0.0-rc0-101-gd2d2566'
Python version: 3.6.6 :: Anaconda, Inc.
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A



I just updated my TensorFlow to version:
```
tf.__version__
'2.0.0-rc1'
```
using pip install.  I am using python version:

```
python -V
Python 3.6.6 :: Anaconda, Inc.
```

the bug is that `tf.losses.mean_squared_error`  returns a list rather than a scaler. A simple code to replicate this:

```
np.random.seed(seed=10)
a, b = np.random.rand(5,1), np.random.rand(5,1)
tf.losses.mean_squared_error(a,b).numpy()
```

returns:

`array([0.29868848, 0.03143916, 0.01609916, 0.33604403, 0.16823713])`


you can find a way around it by using 

`tf.losses.MeanSquaredError()(a,b).numpy()`

which returns:
`0.17010`"
32910,No `libtensorflow_framework.so`,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-55-generic x86_64)
- TensorFlow installed from (source or binary): `pip install tensorflow-gpu==2.0.0rc2`
- TensorFlow version (use command below): v2.0.0-rc1-51-g2646d23 2.0.0-rc2
- Python version: 3.5.2
- CUDA/cuDNN version: 10.0/7.6.4.38
- GPU model and memory: GTX 1080 Ti 11GB

**Describe the current behavior**
No `libtensorflow_framework.so` inside `/home/username/.env/python3/lib/python3.5/site-packages/tensorflow_core`. Only `libtensorflow_framework.so.2` is present.

**Describe the expected behavior**
Both `libtensorflow_framework.so.2` and `libtensorflow_framework.so` should be present in `/home/username/.env/python3/lib/python3.5/site-packages/tensorflow_core`.
"
32909,No libtensorflow_framework.so,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-55-generic x86_64)
- TensorFlow installed from (source or binary): `pip install tensorflow-gpu==2.0.0rc2`
- TensorFlow version (use command below): v2.0.0-rc1-51-g2646d23 2.0.0-rc2
- Python version: 3.5.2
- CUDA/cuDNN version: 10.0/7.6.4.38
- GPU model and memory: GTX 1080 Ti 11GB

**Describe the current behavior**
No `libtensorflow_framework.so` inside `/home/username/.env/python3/lib/python3.5/site-packages/tensorflow_core`. Only `libtensorflow_framework.so.2` is present.

**Describe the expected behavior**
Both `libtensorflow_framework.so.2` and `libtensorflow_framework.so` should be present in `/home/username/.env/python3/lib/python3.5/site-packages/tensorflow_core`.
"
32908,a bizarre mistake-----InvalidArgumentError: slice index -1 of dimension 0 out of bounds.（）,"My computer is installed with win10, tf-nightly-2.0-preview==2.0.0dev20190926，python3.7&python3.6.8

Tensorflow standard template: https://tensorflow.google.cn/tutorials/load_data/images#setup

When I run the tensorflow standard template, I get an error in

    for image, label in labeled_ds.take(1):
        print(""Image shape: "", image.numpy().shape)
        print(""Label: "", label.numpy())

 the error message :
InvalidArgumentError: slice index -1 of dimension 0 out of bounds.
[[{{node strided_slice}}]]
Encountered when executing an operation using EagerExecutor. 
This error cancels all future operations and poisons their output tensors.


However, in google colab I installed tf-nightly-2.0-preview==2.0.0dev20190926, python==3.6,
Running the code but it runs fine without errors

If I want to run this standard template code on my computer, how should I modify it?"
32907,tensorflow c sdk crash when called by a java program,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Red Hat 4.8.5-4**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **tensorflow-gpu-linux-x86_64-1.13.1**
- Python version: **No**
- CUDA/cuDNN version: **cuda 10.0**
- GPU model and memory:  **NVIDIA Tesla v100 GPU 16GB**
- JAVA version: 
`openjdk version ""1.8.0_65""
OpenJDK Runtime Environment (build 1.8.0_65-b17)
OpenJDK 64-Bit Server VM (build 25.65-b01, mixed mode)`

**Describe the current behavior**

I write a dynamic library with the tensorflow c sdk which can be called normally by a c++ program but when I wrap it as a jni and call it from a java program, it will crash:

`A fatal error has been detected by the Java Runtime Environment:
SIGSEGV (0xb) at pc=0x00007fab17a903dd, pid=26065, tid=140380063385344
JRE version: OpenJDK Runtime Environment (8.0_65-b17) (build 1.8.0_65-b17)
Java VM: OpenJDK 64-Bit Server VM (25.65-b01 mixed mode linux-amd64 compressed oops)
Problematic frame:
C  [libtensorflow.so+0x9b03dd]  tensorflow::TF_TensorToTensor(TF_Tensor const*, tensorflow::Tensor*)+0x1d
`
What's the reason for that? Thanks.

**Code to reproduce the issue**

A sample class like [here](https://gist.github.com/knsong/4f39a467cb43d55e6176ffefed27863c), which was instantiated globally.  When called by the java program, the code will crash at this [line](https://gist.github.com/knsong/4f39a467cb43d55e6176ffefed27863c#file-sample-code-L142)
"
32906,"An error occur in ""cwise_op_gpu"" and faile to complete when building the tf2.0_rc2","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10 1903
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0-rc2
- Python version: Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] :: Anaconda, Inc. on win32
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): ms vs2017 15.9.16
- CUDA/cuDNN version: cuda10.1 cudnn7.5
- GPU model and memory: one rtx2080ti with 11GB



**Describe the problem**

when I run ""bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package""
I got an error and failed to build


```
ERROR: C:/users/bill_/desktop/tensorflow-2.0.0-rc2/tensorflow/core/kernels/BUILD:3717:1: C++ compilation of rule '//tensorflow/core/kernels:cwise_op_gpu' failed (Exit 1): python.exe failed: error executing command
  cd C:/users/bill_/_bazel_bill_/7zm7bpa5/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt;
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\LIB\amd64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\WINDOWS\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\Tools;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Windows Kits\10\bin\x86;;C:\WINDOWS\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Anaconda3/envs/tf-2.0/python.exe
    SET PYTHON_LIB_PATH=C:/Anaconda3/envs/tf-2.0/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\bill_\AppData\Local\Temp
    SET TF_CONFIGURE_IOS=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=7.0,8.0
    SET TF_NEED_CUDA=1
    SET TMP=C:\Users\bill_\AppData\Local\Temp
  C:/Anaconda3/envs/tf-2.0/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-opt/bin/external/jpeg /Iexternal/com_google_protobuf /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/local_config_tensorrt /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cublas_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/bin/external/gif_archive /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-opt/bin/external/gif_archive/windows /Iexternal/com_google_protobuf/src /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_cuda/cuda/cublas/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cublas/include /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG -w -DWIN32_LEAN_AND_MEAN -DNOGDI /arch:AVX -DGOOGLE_CUDA=1 -DTENSORFLOW_MONOLITHIC_BUILD /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COMPILE_LIBRARY -x cuda -DGOOGLE_CUDA=1 -nvcc_options=relaxed-constexpr -nvcc_options=ftz=true /Fobazel-out/x64_windows-opt/bin/tensorflow/core/kernels/_objs/cwise_op_gpu/cwise_op_gpu_xlogy.cu.o /c tensorflow/core/kernels/cwise_op_gpu_xlogy.cu.cc
Execution platform: @bazel_tools//platforms:host_platform
nvcc fatal   : Unsupported gpu architecture 'compute_80'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 481.922s, Critical Path: 146.36s
INFO: 4933 processes: 4933 local.
FAILED: Build did NOT complete successfully
```




**Provide the exact sequence of commands / steps that you executed before running into the problem**

I have build the tensorflow2.0 with https://tensorflow.google.cn/install/source_windows

I run ""python ./configure.py"" with 

```
(tf-2.0) C:\Users\bill_\Desktop\tensorflow-2.0.0-rc2>python .\configure.py
WARNING: Running Bazel server needs to be killed, because the startup options are different.
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.26.1 installed.
Please specify the location of python. [Default is C:\Anaconda3\envs\tf-2.0\python.exe]:


Found possible Python library paths:
  C:\Anaconda3\envs\tf-2.0\lib\site-packages
Please input the desired Python library path to use.  Default is [C:\Anaconda3\envs\tf-2.0\lib\site-packages]

Do you wish to build TensorFlow with XLA JIT support? [y/N]:
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Found CUDA 10.1 in:
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include
Found cuDNN 7 in:
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 7.0,7.5


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:
Eigen strong inline overridden.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=gdr            # Build with GDR support.
        --config=verbs          # Build with libverbs support.
        --config=ngraph         # Build with Intel nGraph support.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v2             # Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=noignite       # Disable Apache Ignite support.
        --config=nokafka        # Disable Apache Kafka support.
        --config=nonccl         # Disable NVIDIA NCCL support.
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32905," ""tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)"" is not compatible with ""tf.keras.LearningRateScheduler"" and ""tf.keras.ReduceLROnPlateau""","<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**Ubuntu 18.04**
- TensorFlow version (you are using): 1.14.0
- Are you willing to contribute it (Yes/No): No

After calling ""opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)"", the returned opt lost its attribute ""lr"".
"
32904,  Inference time by quantized model is longer than that by non-quantized model in Tensorflow,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 1806
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  ARMv8 AARCH64
- TensorFlow installed from (source or binary):  source
- TensorFlow version (use command below):  1.12.3
- Python version:  2.7.15
- Bazel version (if compiling from source):  0.15.0-dist   
- GCC/Compiler version (if compiling from source):  7.3
- CUDA/cuDNN version:  NA
- GPU model and memory: NA

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I want to measure Tensorflow inference time with different models on my 4-core ARMv8 CPU.
I downloaded quantized and float point models from https://www.tensorflow.org/lite/guide/hosted_models. Then run benchmark_model to measure inference time. 
Please see attached picture for my benchmarking result.
From the result, you can see the inference time of the quantized models is always a little bit longer than the float point model. 
I also did the same benchmark with TFLite, the inference time of the quantized models is much lower as I expect.

**Describe the expected behavior**
 I think the inference time of quantized models should be much lower than that of float point models.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

The 
![tensorflow](https://user-images.githubusercontent.com/44886342/65829997-fc4c0100-e2dd-11e9-8bd5-c8a3d2b69e82.jpg)
commands as below:
For quantized models:
    echo ""inception_v4""
    bazel-bin/tensorflow/tools/benchmark/benchmark_model --graph=model/classfication_int8/inception_v4/inception_v4.pb  --input_layer=input --input_layer_type=float --input_layer_shape=1,299,299,3 --output_layer=InceptionV4/Logits/Predictions  --show_run_order=false --num_threads=1 --show_flops

    echo ""mobilenet_v1_1.0_224""
    bazel-bin/tensorflow/tools/benchmark/benchmark_model --graph=model/classfication_int8/mobilenet_v1_1.0_224/mobilenet_v1_1.0_224.pb --show_flops --input_layer=input --input_layer_type=float --input_layer_shape=1,224,224,3 --output_layer=MobilenetV1/Predictions/Reshape_1  --show_run_order=false --num_threads=1

    echo ""mobilenet_v2_1.0_224""
    bazel-bin/tensorflow/tools/benchmark/benchmark_model --graph=model/classfication_int8/mobilenet_v2_1.0_224/mobilenet_v2_1.0_224.pb  --output_layer=output  --show_run_order=false --num_threads=1 --show_flops

For float point models:
    echo ""inception_v4""
    bazel-bin/tensorflow/tools/benchmark/benchmark_model --graph=model/classfication-fp32/inception_v4/inception_v4.pb --show_flops --input_layer=input --input_layer_type=float --input_layer_shape=1,299,299,3 --output_layer=InceptionV4/Logits/Predictions  --show_run_order=false --num_threads=1

    echo ""mobilenet_v1_1.0_224""
    bazel-bin/tensorflow/tools/benchmark/benchmark_model --graph=model/classfication-fp32/mobilenet_v1_1.0_224/mobilenet_v1_1.0_224.pb --show_flops --input_layer=input --input_layer_type=float --input_layer_shape=1,224,224,3 --output_layer=MobilenetV1/Predictions/Reshape_1  --show_run_order=false --num_threads=1

    echo ""mobilenet_v2_1.0_224""
    bazel-bin/tensorflow/tools/benchmark/benchmark_model --graph=model/classfication-fp32/mobilenet_v2_1.0_224/mobilenet_v2_1.0_224.pb --show_flops --input_layer=input --input_layer_type=float  --output_layer=MobilenetV2/Predictions/Reshape_1  --show_run_order=false --num_threads=1

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32903,Let us decide the index of the blank in tf.nn.ctc_greedy_decoder ,"**System information**
- TensorFlow version (you are using): 2.0.0rc1 
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

I am using ctc_greedy_decoder function to decode my output from logits. In the ctc_loss function, we have the opportunity to choose blank_index equals what, but in the ctc_greedy_decoder function, it seems equals (num_classes-1) by default. Thus I have to write a map function by myself or change my lookup table

**Will this change the current api? How?**

Please add a parameter to this function in order to choose which index we use."
32902,bug when supplying metadata files for embeddings in tensorboard callback,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):MacOS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):2.0.0-dev20190927
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


**Describe the current behavior**
getting the error:
`ValueError: Unrecognized `Embedding` layer names passed to `keras.callbacks.TensorBoard` `embeddings_metadata` argument: dict_keys(['char_embeddings'])`
when passing embeddings_metadata to tensorflow.keras.callback.TesnorBoard
**Describe the expected behavior**
use the metadata in tensorboard
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/callbacks.py#L1547

```
            if layer.name in embedding.metadata_path:
              embedding.metadata_path = self.embeddings_metadata.pop(layer.name)

```
should be:
```
            if layer.name in self.embeddings_metadata:
              embedding.metadata_path = self.embeddings_metadata.pop(layer.name)

```
"
32901,whether cuda toolkit 10.1 version supports tensorflow 1.14Version?,trying to install tensorflow 2.0 in windows 10. Already installed CUDA toolkit 10.1 and cudNN
32900,cannot run the model from Saver,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04
- TensorFlow version:Tensorflow nightly
- Python version:3.6
- Installed using virtualenv? pip? conda?:pip
- CUDA/cuDNN version:7/10

**Describe the problem**
I use tf.train.Saver() to save the model, but when I train to restore it, I find the output didn't change even though the inputs are different. Here is my code to save the model:

```
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import gzip
import os
import sys
import time
import joblib
import math
import numpy
from six.moves import urllib
from six.moves import xrange  
from PIL import Image
from sklearn.metrics import confusion_matrix as sk_confusion_matrix
from sklearn.metrics import classification_report
import tensorflow as tf

os.environ[""CUDA_VISIBLE_DEVICES""] = ""1""
FLAGS = None
IMAGE_HEIGHT = 128
IMAGE_WEITH = 128
NUM_CHANNELS = 1
NUM_LABELS = 4
SEED = 66478  # Set to None for random seed.
BATCH_SIZE = 32
EVAL_BATCH_SIZE = 32
EVAL_FREQUENCY = 10  # Number of steps between evaluations.

def data_type():
  """"""Return the type of the activations, weights, and placeholder variables.""""""
  if FLAGS.use_fp16:
    return tf.float16
  else:
    return tf.float32


def fake_data(num_images):
  """"""Generate a fake dataset that matches the dimensions of MNIST.""""""
  data = numpy.ndarray(
      shape=(num_images, IMAGE_HEIGHT, IMAGE_WEITH, NUM_CHANNELS),
      dtype=numpy.float32)
  labels = numpy.zeros(shape=(num_images,), dtype=numpy.int64)
  for image in xrange(num_images):
    label = image % 2
    data[image, :, :, 0] = label - 0.5
    labels[image] = label
  return data, labels


def error_rate(predictions, labels):
  Confusion_matrix=sk_confusion_matrix(numpy.argmax(predictions, 1).tolist(), labels.tolist())
  print('Confusion_matrix:')
  print(Confusion_matrix)  

  Se1 = Confusion_matrix[1,1]+Confusion_matrix[2,2]+Confusion_matrix[3,3]
  Se2 = Confusion_matrix[1,1]+Confusion_matrix[1,0]+Confusion_matrix[1,2]+Confusion_matrix[1,3]+Confusion_matrix[2,2]+Confusion_matrix[2,0]+Confusion_matrix[2,1]+Confusion_matrix[2,3]+Confusion_matrix[3,3]+Confusion_matrix[3,0]+Confusion_matrix[3,1]+Confusion_matrix[3,2]
  Se = Se1/Se2
  Sp = Confusion_matrix[0,0]/(Confusion_matrix[0,0]+Confusion_matrix[0,1]+Confusion_matrix[0,2]+Confusion_matrix[0,3]) 
  Acc = (Se+Sp)*100/2

  target_names = ['class 0', 'class 1', 'class 2', 'class 3']

  print()
  accuracy = 100.0-(100.0 *numpy.sum(numpy.argmax(predictions, 1) == labels)/predictions.shape[0])
  
  """"""Return the error rate based on dense predictions and sparse labels.""""""
  return 100.0 - (
      100.0 *
      numpy.sum(numpy.argmax(predictions, 1) == labels) /
      predictions.shape[0]), Acc

def GroupNorm(x, G, eps=1e-05):
    # x: input features with shape [N,H,W,C]
    # gamma, beta: scale and offset, with shape [1,C,1,1]
    # G: number of groups for GN
  N, H, W, C = x.shape
 # N = BATCH_SIZE
  gamma = tf.ones([1, 1, 1, C])
  beta = tf.zeros([1, 1, 1, C])
#  x = tf.reshape(x, [-1, G, H, W, C // G])

  mean = tf.reduce_max(x, axis=[1,2,3], keep_dims=True)

  var = tf.subtract(x,mean)
  var = var*var
  var = tf.reduce_max(x, axis=[1,2,3], keep_dims=True)
 
  x1 = tf.subtract(x,mean) / tf.sqrt(var + eps)
#  x2 = tf.reshape(x1, [-1, H, W, C])
  return x1 * gamma + beta

class ResBlock(object):

  def __init__(self, stride_num=1, downsample=False):
    self.conv1_weights = tf.Variable(
      tf.truncated_normal([1, 1, 64, 64],  # 1x1 filter, depth 64.
                          stddev=0.1,
                          seed=SEED, dtype=data_type()))  

    self.conv2_weights = tf.Variable(
      tf.truncated_normal([3, 3, 64, 64],  # 3x3 filter, depth 64.
                          stddev=0.1,
                          seed=SEED, dtype=data_type()))
    self.conv3_weights = tf.Variable(
      tf.truncated_normal([3, 3, 64, 64],  # 3x3 filter, depth 64.
                          stddev=0.1,
                          seed=SEED, dtype=data_type()))  
    self.stride_num = stride_num
    self.downsample = downsample

  def forward(self, data):
    with tf.name_scope('ResNet'):
    # shortcut = x
      shortcut = data
      # out = self.relu(self.norm1(x))
#      axis = list(range(len(data.get_shape()) - 1))
      with tf.name_scope('BN1'):
        out = GroupNorm(x=data, G=32)
        #mean, variance = tf.nn.moments(data, axis)
        #out = tf.nn.batch_normalization(data, mean, variance, 0, 1, 0.001)
      with tf.name_scope('relu1'):
        out = tf.nn.relu(out)
      #  if self.downsample is not None:
        #   shortcut = self.downsample(out)
      with tf.name_scope('downsample'):
        if self.downsample is True:
          shortcut = tf.nn.conv2d(out,
                                  self.conv1_weights,
                                  strides=[1, self.stride_num, self.stride_num, 1],
                                  padding='SAME')
        #  out = self.conv1(out)
      with tf.name_scope('conv1'):
        out = tf.nn.conv2d(out,
                          self.conv2_weights,
                          strides=[1, self.stride_num, self.stride_num, 1],
                          padding='SAME')
      #  out = self.droupout(out)
      #  out = self.norm2(out)
      with tf.name_scope('BN2'):
        out = GroupNorm(x=out, G=32)

      #  out = self.relu(out) 
      with tf.name_scope('relu2'):
        out = tf.nn.relu(out)   
      #  out = self.conv2(out)
      with tf.name_scope('conv2'):
        out = tf.nn.conv2d(out,
                          self.conv3_weights,
                          strides=[1, 1, 1, 1],
                          padding='SAME')
    return shortcut+out

class BRN(object):

  def __init__(self):
    self.ResNet_0_0 = ResBlock(2, True)
    self.ResNet_0_1 = ResBlock(2, True)
    self.ResNet_1_0 = ResBlock(2, True)
    self.ResNet_1_1 = ResBlock(2, True)
    self.ResNet_0 = ResBlock(1, False)
    self.ResNet_1 = ResBlock(1, False)
    self.ResNet_2 = ResBlock(1, False)
    self.ResNet_3 = ResBlock(1, False)
    self.ResNet_4 = ResBlock(1, False)
    self.ResNet_5 = ResBlock(1, False)
    self.ResNet_6 = ResBlock(1, False)
    self.ResNet_7 = ResBlock(1, False)
    self.ResNet_8 = ResBlock(1, False)
    self.ResNet_9 = ResBlock(1, False)
    self.ResNet_10 = ResBlock(1, False)
    self.ResNet_11 = ResBlock(1, False)
    self.ResNet_12 = ResBlock(1, False)
    self.ResNet_13 = ResBlock(1, False)
    self.ResNet_14 = ResBlock(1, False)
    self.ResNet_15 = ResBlock(1, False)
    self.ResNet_16 = ResBlock(1, False)
    self.ResNet_17 = ResBlock(1, False)
    self.ResNet_18 = ResBlock(1, False)
    self.ResNet_19 = ResBlock(1, False)
    self.ResNet_20 = ResBlock(1, False)
    self.ResNet_21 = ResBlock(1, False)
    self.conv1_weights = tf.Variable(
      tf.truncated_normal([3, 3, NUM_CHANNELS, 64],  
                          stddev=0.1,
                          seed=SEED, dtype=data_type()))  
    self.conv2_weights = tf.Variable(
      tf.truncated_normal([3, 3, NUM_CHANNELS, 64],  
                          stddev=0.1,
                          seed=SEED, dtype=data_type()))  
    self.fc_weights = tf.Variable(tf.truncated_normal([64, NUM_LABELS],
                                                stddev=0.1,
                                                seed=SEED,
                                                dtype=data_type()))

    self.fc_biases = tf.Variable(tf.constant(0.1, shape=[NUM_LABELS], dtype=data_type()))

  def forward(self, stft, mfcc):

    with tf.name_scope('BRNcon1'):
      out_s = tf.nn.conv2d(stft,
                        self.conv1_weights,
                        strides= [1, 1, 1, 1],
                        padding='VALID')
    with tf.name_scope('resnetblocks_1'):
      out_s = self.ResNet_0_0.forward(out_s)
    with tf.name_scope('resnetblocks_2'):
      out_s = self.ResNet_0_1.forward(out_s)
    with tf.name_scope('resnetblocks_3'):
      out_s = self.ResNet_0.forward(out_s)
    with tf.name_scope('resnetblocks_4'):
      out_s = self.ResNet_2.forward(out_s)
    with tf.name_scope('resnetblocks_5'):
      out_s = self.ResNet_4.forward(out_s)
    with tf.name_scope('resnetblocks_6'):
      out_s = self.ResNet_6.forward(out_s)
    with tf.name_scope('resnetblocks_7'):
      out_s = self.ResNet_8.forward(out_s)
    with tf.name_scope('resnetblocks_8'):
      out_s = self.ResNet_10.forward(out_s)
    with tf.name_scope('resnetblocks_9'):
      out_s = self.ResNet_12.forward(out_s)
    with tf.name_scope('resnetblocks_10'):
      out_s = self.ResNet_14.forward(out_s)
    with tf.name_scope('resnetblocks_11'):
      out_s = self.ResNet_16.forward(out_s)
    with tf.name_scope('resnetblocks_12'):
      out_s = self.ResNet_18.forward(out_s)
    with tf.name_scope('resnetblocks_13'):
      out_s = self.ResNet_20.forward(out_s)
    with tf.name_scope('brns_bn1'):
      out_s = GroupNorm(x=out_s, G=32)

    with tf.name_scope('brn_relu_1'):
      out_s = tf.nn.relu(out_s)
    with tf.name_scope('brn_pool1'):
      out_s = tf.nn.avg_pool(out_s,
                            ksize=[1,out_s.shape[2],out_s.shape[2],1],
                            strides=[1, 1, 1, 1],
                            padding='VALID')

    with tf.name_scope('BRNcon2'):
      out_m = tf.nn.conv2d(mfcc,
                        self.conv2_weights,
                        strides= [1, 1, 1, 1],
                        padding='VALID')
    with tf.name_scope('resnetblockm_1'):
      out_m = self.ResNet_1_0.forward(out_m)
    with tf.name_scope('resnetblockm_2'):
      out_m = self.ResNet_1_1.forward(out_m)
    with tf.name_scope('resnetblockm_3'):
      out_m = self.ResNet_1.forward(out_m)
    with tf.name_scope('resnetblockm_4'):
      out_m = self.ResNet_3.forward(out_m)  
    with tf.name_scope('resnetblockm_5'):
      out_m = self.ResNet_5.forward(out_m)
    with tf.name_scope('resnetblockm_6'):
      out_m = self.ResNet_7.forward(out_m)
    with tf.name_scope('resnetblockm_7'):
      out_m = self.ResNet_9.forward(out_m)
    with tf.name_scope('resnetblockm_8'):
      out_m = self.ResNet_11.forward(out_m)
    with tf.name_scope('resnetblockm_9'):
      out_m = self.ResNet_13.forward(out_m)
    with tf.name_scope('resnetblockm_10'):
      out_m = self.ResNet_15.forward(out_m)
    with tf.name_scope('resnetblockm_11'):
      out_m = self.ResNet_17.forward(out_m)
    with tf.name_scope('resnetblockm_12'):
      out_m = self.ResNet_19.forward(out_m)
    with tf.name_scope('resnetblockm_13'):
      out_m = self.ResNet_21.forward(out_m)
    with tf.name_scope('brnm_bn1'):
      out_m = GroupNorm(x=out_m, G=32)

    with tf.name_scope('brn_relu_2'):
      out_m = tf.nn.relu(out_m)
    with tf.name_scope('brn_pool2'):
      out_m = tf.nn.avg_pool(out_m,
                            ksize=[1,out_m.shape[2],out_m.shape[2],1],
                            strides=[1, 1, 1, 1],
                            padding='VALID')
    with tf.name_scope('maumul'):
    
      out = tf.multiply(out_s,out_m)
    with tf.name_scope('fc'):

      out_shape = out.get_shape().as_list()
      reshape = tf.reshape(
          out,
          [-1, out_shape[1] * out_shape[2] * out_shape[3]])    
      out = tf.add(tf.matmul(reshape, self.fc_weights), self.fc_biases, name=""logits_"")

    return out

def main(_):

  def loss_function(weight, logits, labels):
    labels = tf.one_hot(labels,4)
    labels = tf.cast(labels, tf.float32)
    first = tf.reduce_sum(tf.multiply(-labels, logits),1)
    second_0 = tf.add(tf.exp(logits[:,0]),tf.exp(logits[:,1]))
    second_1 = tf.add(tf.exp(logits[:,2]),tf.exp(logits[:,3]))
    log = tf.log(tf.add(second_1,second_0))
    weight = tf.transpose(tf.reduce_sum(tf.multiply(labels, weight),1))
    output = tf.multiply(weight,tf.add(first,log))

    return output

  def normalize(stft):
    stft_1 = numpy.empty([stft.shape[0],128,128])
    stft_2 = numpy.empty([stft_1.shape[0],stft_1.shape[1],stft_1.shape[2],1])
    for i in range(stft_1.shape[0]):
      image = Image.fromarray(stft[i,:,:])
      image = image.resize([128,128])
      stft_1[i,:,:] = numpy.array(image)

      min = numpy.min(stft_1[i,:,:])
      max = numpy.max(stft_1[i,:,:])
      stft_1[i,:,:] = (stft_1[i,:,:]-min)/(max-min)
      stft_2[i,:,:,:] = stft_1[i,:,:].reshape((stft_1.shape[1],stft_1.shape[2],1))
    return stft_2  

  if FLAGS.self_test:
    
    train_data, train_labels = fake_data(256)
    validation_data, validation_labels = fake_data(EVAL_BATCH_SIZE)
    test_data, test_labels = fake_data(EVAL_BATCH_SIZE)
    num_epochs = 1
  else:
    # Get the data.
    
    stft_training, mfcc_training, labels_training = joblib.load(open(FLAGS.input, mode='rb'))
    stft_test, mfcc_test, labels_test = joblib.load(open(FLAGS.test, mode='rb'))

    stft_test = numpy.array(stft_test)
    mfcc_test = numpy.array(mfcc_test)
    labels_test = numpy.array(labels_test)
    stft_test = normalize(stft_test)
    mfcc_test = normalize(mfcc_test)

    stft_training = numpy.array(stft_training)
    mfcc_training = numpy.array(mfcc_training)
    labels_training = numpy.array(labels_training)
    stft_training = normalize(stft_training)
    mfcc_training = normalize(mfcc_training)

    stft_shape = stft_training.shape
    stft_shape = (None, stft_shape[1], stft_shape[2], 1)

    mfcc_shape = mfcc_training.shape
    mfcc_shape = (None, mfcc_shape[1], mfcc_shape[2], 1)

    labels_shape = labels_training.shape
    labels_shape = (None)

    stft_placeholder = tf.placeholder(stft_training.dtype, stft_shape)
    labels_placeholder = tf.placeholder(labels_training.dtype, labels_shape)
    mfcc_placeholder = tf.placeholder(mfcc_training.dtype, mfcc_shape)
    
    dataset_training = tf.data.Dataset.from_tensor_slices((stft_placeholder, mfcc_placeholder, labels_placeholder))
    dataset_training  = dataset_training.apply(
        tf.data.experimental.shuffle_and_repeat(len(stft_training), None))  
    dataset_training  = dataset_training.batch(BATCH_SIZE)
    dataset_training  = dataset_training.prefetch(1)
    iterator_training = dataset_training.make_initializable_iterator()
    next_element_training = iterator_training.get_next()
    num_epochs = FLAGS.epochs

  train_size = labels_training.shape[0]


  stft_holder = tf.placeholder(
        name=""stft_holder"",
        dtype=data_type(),
        shape=(None, IMAGE_HEIGHT, IMAGE_WEITH, NUM_CHANNELS))
  mfcc_holder = tf.placeholder(
        name=""mfcc_holder"",
        dtype=data_type(),
        shape=(None, IMAGE_HEIGHT, IMAGE_WEITH, NUM_CHANNELS))
  labels = tf.placeholder(tf.int64, shape=(None,))

  with tf.name_scope('test_input'):
    stft_t = tf.placeholder(
        data_type(),
        shape=(None, IMAGE_HEIGHT, IMAGE_WEITH, NUM_CHANNELS))
    mfcc_t = tf.placeholder(
        data_type(),
        shape=(None, IMAGE_HEIGHT, IMAGE_WEITH, NUM_CHANNELS))

  model = BRN()
  
  logits = model.forward(stft_holder, mfcc_holder)
  out1 = tf.identity(logits,name=""out1"")

  try:
    scalar_summary = tf.scalar_summary
    SummaryWrite = tf.train.SummaryWrite
    merge_summary = tf.merge_summary
  except:
    scalar_summary = tf.summary.scalar
    SummaryWrite = tf.summary.FileWriter
    merge_summary = tf.summary.merge
  with tf.name_scope('loss'):
    weights = [1.0, 1.7, 4.1, 5.7]
    mid = loss_function(weights, logits=logits, labels=labels)
#    mid = tf.nn.sparse_softmax_cross_entropy_with_logits(
#       labels=labels, logits=logits)

    loss = tf.reduce_sum(mid)
    
    loss_summary = scalar_summary('loss', loss)

    
    # L2 regularization for the fully connected parameters.
    regularizers = (tf.nn.l2_loss(model.conv1_weights) + tf.nn.l2_loss(model.conv2_weights) +
                    tf.nn.l2_loss(model.fc_weights) + tf.nn.l2_loss(model.fc_biases))
    # Add the regularization term to the loss.
    loss += 0.02 * regularizers

    batch = tf.Variable(0, dtype=data_type())
  # Use simple momentum for the optimization.
  with tf.name_scope('train'):

    optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)

  # Predictions for the current training minibatch.
  train_prediction = tf.nn.softmax(logits)
  eval_prediction = tf.nn.softmax(model.forward(stft_t, mfcc_t))

  # Create a local session to run the training.
  start_time = time.time()

  def eval_in_batches(stft_data, mfcc_data, sess, type):
    """"""Get all predictions for a dataset by running it in small batches.""""""
    size = stft_data.shape[0]
    if size < EVAL_BATCH_SIZE:
      raise ValueError(""batch size for evals larger than dataset: %d"" % size)
    predictions = numpy.ndarray(shape=(size, NUM_LABELS), dtype=numpy.float32)
    for begin in xrange(0, size, EVAL_BATCH_SIZE):
      end = begin + EVAL_BATCH_SIZE
      if end <= size:
        if type == 'train':
          predictions[begin:end, :] = sess.run(
              train_prediction,
              feed_dict={stft_holder: stft_data[begin:end, ...], mfcc_holder: mfcc_data[begin:end, ...]})
        else: 
          predictions[begin:end, :] = sess.run(
              eval_prediction,
              feed_dict={stft_t: stft_data[begin:end, ...], mfcc_t: mfcc_data[begin:end, ...]})
      else:
        if type == 'train':
          batch_predictions = sess.run(
              train_prediction,
              feed_dict={stft_holder: stft_data[-EVAL_BATCH_SIZE:, ...], mfcc_holder: mfcc_data[-EVAL_BATCH_SIZE:, ...]})
        else:
           batch_predictions = sess.run(
              eval_prediction,
              feed_dict={stft_t: stft_data[-EVAL_BATCH_SIZE:, ...], mfcc_t: mfcc_data[-EVAL_BATCH_SIZE:, ...]})
        predictions[begin:, :] = batch_predictions[begin - size:, :]
    return predictions

  saver = tf.train.Saver()
  config = tf.ConfigProto()
  config.gpu_options.allow_growth = True  

  with tf.Session(config=config) as sess:
    # Run all the initializers to prepare the trainable parameters.
    tf.global_variables_initializer().run()

    merged = tf.summary.merge_all()
    writer = SummaryWrite(FLAGS.logs + 'train', sess.graph)
    print('Initialized!')
    sess.run(iterator_training.initializer, feed_dict={stft_placeholder:stft_training,
                      mfcc_placeholder:mfcc_training,
                      labels_placeholder:labels_training})

    # Loop through training steps.
    for step in xrange(int(num_epochs * train_size) // BATCH_SIZE):

      batch_stft, batch_mfcc, batch_labels = sess.run(next_element_training)
  
      feed_dict = {stft_holder: batch_stft,
                   mfcc_holder: batch_mfcc,
                   labels: batch_labels}
      # Run the optimizer to update weights.

      sess.run(optimizer, feed_dict=feed_dict)
      # print some extra information once reach the evaluation frequency
      if step % EVAL_FREQUENCY == 0:
        # fetch some extra nodes' data
        summary, l = sess.run([merged, loss],
                                      feed_dict=feed_dict)
        writer.add_summary(summary, step)
        elapsed_time = time.time() - start_time
        start_time = time.time()
        rate, acc = error_rate(eval_in_batches(stft_training, mfcc_training, sess, 'train'), labels_training)
        acc_summary = scalar_summary('accuracy', acc)
        print('Step %d (epoch %.2f), Minibatch loss: %.3f, Minibatch error: %.1f%%, Accuracy:%.4f' %
              (step, float(step) * BATCH_SIZE / train_size,
              l,rate, acc))

        
    # Finally print the result!
        sys.stdout.flush()
        test_error, test_acc = error_rate(eval_in_batches(stft_test, mfcc_test, sess, 'test'), labels_test)
        print('Testset error: %.1f%%, Accuracy:%.4f' % (test_error, test_acc))

    
    saver.save(sess, './local_ckpt3')        
    writer.close()



if __name__ == '__main__':
#  dev = '/gpu:0'
  parser = argparse.ArgumentParser()
  parser.add_argument(
      '--use_fp16',
      default=False,
      help='Use half floats instead of full floats if True.',
      action='store_true')
  parser.add_argument(
      '--self_test',
      default=False,
      action='store_true',
      help='True if running a self test.')
  parser.add_argument(
      '--input',
      default='wavelet_stft.p')
  parser.add_argument(
      '--test',
      default='wavelet_stft_test.p')  
  parser.add_argument(
      '--epochs',
      type=float,
      default=0.2)  
  parser.add_argument(
      '--logs',
      default='')  
  FLAGS, unparsed = parser.parse_known_args()
  tf.enable_resource_variables()
  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)

```

and the code for restore model:
```
def main(_):

  config = tf.ConfigProto()
  config.gpu_options.allow_growth = True  
  with tf.Session(config=config) as sess:
    # Run all the initializers to prepare the trainable parameters.
    tf.global_variables_initializer().run()
    saver = tf.train.import_meta_graph('./local_ckpt3.meta', clear_devices=True)
    saver.restore(sess, './local_ckpt3')
    graph = tf.get_default_graph()
    w1 = graph.get_tensor_by_name(""stft_holder:0"")
    w2 = graph.get_tensor_by_name(""mfcc_holder:0"")
    input_shape = (2,128,128,1)
    input_data = numpy.array(numpy.random.random_sample(input_shape), dtype=numpy.float32)
    input_data2 = numpy.array(numpy.random.random_sample(input_shape), dtype=numpy.float32)
    # Loop through training steps.
  
    feed_dict = {w1: input_data,
                w2: input_data2}
      # Run the optimizer to update weights.
    op_to_restore = graph.get_tensor_by_name(""out1:0"")
    result = sess.run(op_to_restore, feed_dict=feed_dict)
    print(result)
```
I can't find the reason of this question, then I check the ckpt file, and I find it seems that the model get the output though return the tensor stored in the ckpt file instead of running again in second code. I cannot figure out how to correct it ,thanks for your help anyway


"
32899,Model training does not improve accuracy,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 19.04
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0.0-rc2
- Python version: 3.7.3
- CUDA/cuDNN version: release 10.0, V10.0.130
- GPU model and memory: nVidia GTX 1080 Ti

**Describe the current behavior**
When attempting to train a sequential model on the MNIST dataset, the model remains at 11% accuracy.  This is only resolved when the inputs are scaled by 255.

**Describe the expected behavior**
The model should improve in accuracy.

**Code to reproduce the issue**
https://github.com/PacktPublishing/What-s-New-in-TensorFlow-2.0/blob/master/Chapter02/end_to_end_sequential.py

**Other info / logs**
Please reference the issue at https://github.com/PacktPublishing/What-s-New-in-TensorFlow-2.0/issues/1 for more information.  This behavior was not observed in 2.0.0-beta0
"
32898,"tf.lite not support Merge, RandomUniform, Switch","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 1.14.0


**Provide the text output from tflite_convert**

```
2019-09-29 11:27:27.677102: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 4963 operators, 8473 arrays (0 quantized)
2019-09-29 11:27:28.107098: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 4959 operators, 8467 arrays (0 quantized)
2019-09-29 11:27:28.858936: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 4959 operators, 8467 arrays (0 quantized)
2019-09-29 11:27:29.762413: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 4823 operators, 8044 arrays (0 quantized)
2019-09-29 11:27:30.695850: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 4823 operators, 8044 arrays (0 quantized)
2019-09-29 11:27:31.376652: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 4823 operators, 8044 arrays (0 quantized)
2019-09-29 11:27:31.590536: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 19328 bytes, theoretical optimal value: 2048 bytes.
2019-09-29 11:27:31.918181: E tensorflow/lite/toco/toco_tooling.cc:456] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, FLOOR, FULLY_CONNECTED, GATHER, GREATER_EQUAL, LOGISTIC, MUL, PACK, REDUCE_PROD, RESHAPE, SELECT, SHAPE, SLICE, STRIDED_SLICE, SUM, TILE, UNIQUE, WHERE, ZEROS_LIKE. Here is a list of operators for which you will need custom implementations: Merge, RandomUniform, Switch.
Traceback (most recent call last):
  File ""/home/user/.local/bin/toco_from_protos"", line 11, in <module>
    sys.exit(main())
  File ""/home/user/.local/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 59, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/home/user/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python3.6/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.6/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/home/user/.local/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 33, in execute
    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, FLOOR, FULLY_CONNECTED, GATHER, GREATER_EQUAL, LOGISTIC, MUL, PACK, REDUCE_PROD, RESHAPE, SELECT, SHAPE, SLICE, STRIDED_SLICE, SUM, TILE, UNIQUE, WHERE, ZEROS_LIKE. Here is a list of operators for which you will need custom implementations: Merge, RandomUniform, Switch.
```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```python
converter = tf.lite.TFLiteConverter.from_saved_model(
    model_path, input_arrays=input_arrays, output_arrays=output_arrays)
converter.post_training_quantize=True
converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,
                                       tf.lite.OpsSet.SELECT_TF_OPS]
tflite_quantized_model = converter.convert()
open(""quantized_model.tflite"", ""wb"").write(tflite_quantized_model)
```
"
32897,TF2: tfp.distributions.Normal in tf.keras.Model issue,"I just found that if I returned a distribution in `call` not a tensor and called `self(tf.keras.Input())` in `__init__`, it would throw an exception `'Normal' object has no attribute 'shape'`
```python
class MyModel(tf.keras.Model):
    def __init__(self):
        super(MyModel, self).__init__()

        self.l = tf.keras.layers.Dense(2)
        self(tf.keras.Input(shape=(3,)))

    def call(self, inputs):
        x = self.l(inputs)

        policy = tfp.distributions.Normal(loc=x, scale=x)
        return policy


model = MyModel()
```"
32896,How to get cublas handle to run cublas function?,"I write a custom op using cublas function `cublasCgetrfBatched` and `cublasCgetriBatched`, the functions use cublas handle as a input param, however the `cublasCreate(&handle);` cost nearly 100ms. 
I think the TF has already integrate CUBLAS module, `cublasCreate(&handle)` muse have been invoked in the init process, then how to get the handle? An example may be the best!"
32895,"Decorated the call methods of tf.keras.Model subclass with @tf.function or not , result is very different","System information

OS Platform and Distribution
Mac os (10.14.6)
TensorFlow installed from (source or binary):
binary
TensorFlow version (use command below):
2.0.0rc1
Python version:
Python 3.6.4
I implement a  model with tensorflow keras just copy [official tutorials](https://www.tensorflow.org/tutorials/quickstart/advanced). The code is shown below

```
import numpy as np
from tensorflow.keras.layers import Dense, Flatten, Conv2D
from tensorflow.keras import Model
import tensorflow as tf
import time
mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train, x_test
y_train, y_test = y_train , y_test
x_train = x_train[..., tf.newaxis].astype(np.float64)
x_test = x_test[..., tf.newaxis].astype(np.float64)
train_ds = tf.data.Dataset.from_tensor_slices(
(x_train, y_train)).shuffle(1000).batch(256)
test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(256)

class MyModel(Model):
    def __init__(self, *args, **kwargs):
        super(MyModel, self).__init__(args, kwargs)
        self.conv1 = Conv2D(32, 3, activation='relu')
        self.flatten = Flatten()
        self.d1 = Dense(128, activation='relu')
        self.d2 = Dense(10, activation='softmax')
        
    def call(self, x):
        x = self.conv1(x)
        x = self.flatten(x)
        x = self.d1(x)
        return self.d2(x)
model = MyModel()
model.build((512, 28, 28, 1))
loss_object = tf.keras.losses.SparseCategoricalCrossentropy()
optimizer = tf.keras.optimizers.Adam()
train_loss = tf.keras.metrics.Mean(name='train_loss')
train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')

test_loss = tf.keras.metrics.Mean(name='test_loss')
test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')
@tf.function
def train_step(images, labels):
    with tf.GradientTape() as tape:
        predictions = model(images)
        loss = loss_object(labels, predictions)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))

    train_loss(loss)
    train_accuracy(labels, predictions)
@tf.function
def test_step(images, labels):
    predictions = model(images)
    t_loss = loss_object(labels, predictions)

    test_loss(t_loss)
    test_accuracy(labels, predictions)

EPOCHS = 100

for epoch in range(EPOCHS):
    start = time.time()
    for images, labels in train_ds:
        train_step(tf.cast(images, tf.float32), labels)

    model.reset_metrics()
    for test_images, test_labels in test_ds:
        test_step(tf.cast(test_images, tf.float32), test_labels)
    end = time.time()
    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}, cost:{}'
    print(
        template.format(epoch + 1, train_loss.result(),
                        train_accuracy.result() * 100, test_loss.result(),
                        test_accuracy.result() * 100, end -start))
```
The result show below
```
Epoch 1, Loss: 3.311713457107544, Accuracy: 89.99500274658203, Test Loss: 0.22570940852165222, Test Accuracy: 95.55000305175781, cost:14.249950170516968
Epoch 2, Loss: 1.7273597717285156, Accuracy: 93.3933334350586, Test Loss: 0.18705108761787415, Test Accuracy: 96.28499603271484, cost:13.976628065109253
Epoch 3, Loss: 1.1731806993484497, Accuracy: 95.00333404541016, Test Loss: 0.1706559658050537, Test Accuracy: 96.61333465576172, cost:14.125600099563599
Epoch 4, Loss: 0.8890712261199951, Accuracy: 95.9800033569336, Test Loss: 0.1615695059299469, Test Accuracy: 96.78250122070312, cost:14.998674154281616
Epoch 5, Loss: 0.7157263159751892, Accuracy: 96.64299774169922, Test Loss: 0.15943895280361176, Test Accuracy: 96.91000366210938, cost:14.496413230895996
Epoch 6, Loss: 0.5996174216270447, Accuracy: 97.10027313232422, Test Loss: 0.15636563301086426, Test Accuracy: 97.01667022705078, cost:12.977428197860718
Epoch 7, Loss: 0.5162842273712158, Accuracy: 97.44285583496094, Test Loss: 0.15558230876922607, Test Accuracy: 97.07571411132812, cost:13.014786005020142
Epoch 8, Loss: 0.4553721249103546, Accuracy: 97.66041564941406, Test Loss: 0.1616506278514862, Test Accuracy: 97.0574951171875, cost:14.170269966125488
Epoch 9, Loss: 0.4086601734161377, Accuracy: 97.81925964355469, Test Loss: 0.16302895545959473, Test Accuracy: 97.0955581665039, cost:13.114216804504395
Epoch 10, Loss: 0.3699580430984497, Accuracy: 97.98149871826172, Test Loss: 0.16444820165634155, Test Accuracy: 97.10900115966797, cost:12.783933162689209
Epoch 11, Loss: 0.3377893567085266, Accuracy: 98.12287902832031, Test Loss: 0.1649542897939682, Test Accuracy: 97.1427230834961, cost:12.907387256622314
Epoch 12, Loss: 0.31051450967788696, Accuracy: 98.2509765625, Test Loss: 0.16677971184253693, Test Accuracy: 97.17666625976562, cost:12.56903600692749
Epoch 13, Loss: 0.28760311007499695, Accuracy: 98.35563659667969, Test Loss: 0.168002188205719, Test Accuracy: 97.21846008300781, cost:13.040626049041748
Epoch 14, Loss: 0.26825159788131714, Accuracy: 98.44261932373047, Test Loss: 0.17205245792865753, Test Accuracy: 97.22142791748047, cost:13.130248785018921
Epoch 15, Loss: 0.2515304386615753, Accuracy: 98.51310729980469, Test Loss: 0.17627017199993134, Test Accuracy: 97.23332977294922, cost:12.66043996810913
Epoch 16, Loss: 0.2367033064365387, Accuracy: 98.58124542236328, Test Loss: 0.17778924107551575, Test Accuracy: 97.26062774658203, cost:12.836369037628174
Epoch 17, Loss: 0.2236185222864151, Accuracy: 98.6434326171875, Test Loss: 0.17880629003047943, Test Accuracy: 97.2752914428711, cost:12.845327854156494
Epoch 18, Loss: 0.21163013577461243, Accuracy: 98.706298828125, Test Loss: 0.18086740374565125, Test Accuracy: 97.29389190673828, cost:13.29944920539856
Epoch 19, Loss: 0.20079432427883148, Accuracy: 98.76526641845703, Test Loss: 0.18283464014530182, Test Accuracy: 97.31105041503906, cost:12.92448115348816
Epoch 20, Loss: 0.19131030142307281, Accuracy: 98.81291198730469, Test Loss: 0.18425647914409637, Test Accuracy: 97.32350158691406, cost:12.959301948547363
.....
```

But I find can  Decorated the call methods with call method from [this](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/function#used_in_the_tutorials).

so I add annotation @tf.function to call method with above code(The rest of the code remains unchanged), just like this
```
   @tf.function
    def call(self, x):
        x = self.conv1(x)
        x = self.flatten(x)
        x = self.d1(x)
        return self.d2(x)
```
The result show below
```
Epoch 1, Loss: 11.509495735168457, Accuracy: 28.50666618347168, Test Loss: 11.525819778442383, Test Accuracy: 28.400001525878906, cost:14.542086839675903
Epoch 2, Loss: 11.499279022216797, Accuracy: 28.60249900817871, Test Loss: 11.579718589782715, Test Accuracy: 28.064998626708984, cost:13.102718114852905
Epoch 3, Loss: 11.471456527709961, Accuracy: 28.78444480895996, Test Loss: 11.532645225524902, Test Accuracy: 28.369998931884766, cost:13.346691846847534
Epoch 4, Loss: 11.456918716430664, Accuracy: 28.886667251586914, Test Loss: 11.518753051757812, Test Accuracy: 28.497499465942383, cost:13.706462860107422
Epoch 5, Loss: 11.445549011230469, Accuracy: 28.961334228515625, Test Loss: 11.492920875549316, Test Accuracy: 28.6560001373291, cost:13.56296968460083
Epoch 6, Loss: 11.424226760864258, Accuracy: 29.0897216796875, Test Loss: 11.270418167114258, Test Accuracy: 30.030000686645508, cost:13.60814118385315
Epoch 7, Loss: 11.216507911682129, Accuracy: 30.37714385986328, Test Loss: 11.099614143371582, Test Accuracy: 31.09000015258789, cost:13.579961061477661
Epoch 8, Loss: 11.050827980041504, Accuracy: 31.407499313354492, Test Loss: 10.950920104980469, Test Accuracy: 32.01874923706055, cost:13.574744701385498
Epoch 9, Loss: 10.918838500976562, Accuracy: 32.227962493896484, Test Loss: 10.83558464050293, Test Accuracy: 32.7400016784668, cost:12.771528959274292
Epoch 10, Loss: 10.808745384216309, Accuracy: 32.91266632080078, Test Loss: 10.74200439453125, Test Accuracy: 33.32600021362305, cost:12.579218864440918
Epoch 11, Loss: 10.719698905944824, Accuracy: 33.46500015258789, Test Loss: 10.672236442565918, Test Accuracy: 33.759090423583984, cost:12.781628131866455
Epoch 12, Loss: 10.644211769104004, Accuracy: 33.93402862548828, Test Loss: 10.609025955200195, Test Accuracy: 34.154998779296875, cost:13.060226202011108
Epoch 13, Loss: 10.57997989654541, Accuracy: 34.33307647705078, Test Loss: 10.551304817199707, Test Accuracy: 34.51692199707031, cost:13.11635136604309
Epoch 14, Loss: 10.525317192077637, Accuracy: 34.671546936035156, Test Loss: 10.500534057617188, Test Accuracy: 34.834999084472656, cost:13.43892216682434
Epoch 15, Loss: 10.476855278015137, Accuracy: 34.972442626953125, Test Loss: 10.458395957946777, Test Accuracy: 35.099334716796875, cost:13.528913021087646
Epoch 16, Loss: 10.435961723327637, Accuracy: 35.22645950317383, Test Loss: 10.421695709228516, Test Accuracy: 35.329376220703125, cost:13.542529821395874
Epoch 17, Loss: 10.397627830505371, Accuracy: 35.46509552001953, Test Loss: 10.387943267822266, Test Accuracy: 35.541175842285156, cost:12.953328132629395
Epoch 18, Loss: 10.317543983459473, Accuracy: 35.95990753173828, Test Loss: 10.273320198059082, Test Accuracy: 36.2488899230957, cost:13.799224138259888
Epoch 19, Loss: 10.211501121520996, Accuracy: 36.61640167236328, Test Loss: 10.166360855102539, Test Accuracy: 36.910526275634766, cost:13.552597045898438
Epoch 20, Loss: 10.113394737243652, Accuracy: 37.22416687011719, Test Loss: 10.071305274963379, Test Accuracy: 37.49850082397461, cost:12.61729907989502
......
``` 
When you compare these two results, The first code is obviously better than the second. **I've done many experiments, and the results are the same**. The first get better acc and loss just after one epoch, but the second will iterated many time from a not so good result and finally can get a better result.

so my question are :
(1) What causes these two different results？ because I have added @tf.function annotation to the method train_step, so whether I add  @tf.function annotation to call method or not , the result should the same.
(2) tensorflow keras will run with graph model automatic，why add  @tf.function annotation to the call method in [this api doc](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/function#used_in_the_tutorials)"
32894,convolution2d_transpose() got an unexpected keyword argument 'kernel_constraint',"when i use tensorflow.contrib.layers.convolution2d_transpose()
using tensorflow1.13-gpu, ubuntu
i got this error:
convolution2d_transpose() got an unexpected keyword argument 'kernel_constraint'

but i check the document, there exits 'kernel_constraint' parameter in tensorflow.contrib.layers.convolution2d_transpose
how to fix it???
"
32892,Broken Link for API Development Recommendations,"[This page](https://www.tensorflow.org/community/contribute/docs) links to https://www.tensorflow.org/customize, which doesn't exist.

(""We encourage the community to develop and maintain support for other languages with the approach recommended by the TensorFlow maintainers."")
"
32891,ImportError: /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow.so: cannot open shared object file: No such file or directory,"This error is obtained while importing tensorflow in a program.

- OS Platform and Distribution : Linux raspberry 4.9.0-11-686 #1 SMP Debian 4.9.189-3+deb9u1 (2019-09-20) i686 GNU/Linux
- TensorFlow installed from (source or binary): using pip3 python package
- Python version: 3.5.3
- TensorFlow version = 0.11.0

""import tensorflow as tf""
The above command is throwing ImportError as follows:
>>> import tensorflow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py"", line 23, in <module>
    from tensorflow.python import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow.so: cannot open shared object file: No such file or directory
"
32890,[TF 2.0 API Docs] tf.nn.batch_normalization,"## URL(s) with the issue:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/nn/batch_normalization

## Description of issue (what needs changing):
Documentation

### Clear description
Yes
### Correct links
Yes
Is the link to the source code correct?
Yes
### Parameters defined
Yes
Are all parameters defined and formatted correctly?
No.

- tf.nn.moments(..., keep_dims=True) 

In TF2 version of tf.nn.moments, keep_dims keyword should be ""keepdims"" instead

- Does not implement the equation as given, but the equation 11 in Algorithm 2 of the paper.
![image](https://user-images.githubusercontent.com/1215029/65819369-429b5480-e239-11e9-98e7-07bbf34f18d9.png)


### Returns defined
Are return values defined?
Yes
### Raises listed and defined
No
### Usage example
Is there a usage example?
No
### Request visuals, if applicable
No
Are there currently visuals? If not, will it clarify the content?
No
### Submit a pull request?
Yes
Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
32889,TypeError: An op outside of the function building code is being passed,"I have realized a code to take previous lstm outputs as current inputs as a sequence generator. But when I want to pass states of lstm cells to next batch by take states as a member of the generator class, it rise an exception ""TypeError: An op outside of the function building code is being passed"". When add `@tf.function` to `main` function in my code, the exception became ""AttributeError: 'Tensor' object has no attribute '_numpy'"". I suspect this is about the compute graph, but I can't understand the reason. 

My tensorflow version is 2.0.0-rc1.
```
In [4]: tf.__version__                                                                                                                                
Out[4]: '2.0.0-rc1'
```

I don't know whether it is a bug or not. Thank you for your attention.

The code is:

```python
import tensorflow as tf
from tensorflow import keras
import numpy as np


class RNNGenerator(keras.layers.Layer):
    def __init__(self, rnn_dim, rnn_layer_num, seq_len, seq_width, batch_size, **kwargs):
        self.rnn_dim = rnn_dim              # lstm hidden dim
        self.rnn_layer_num = rnn_layer_num  # number of lstm cell layers
        self.seq_len = seq_len              # length of squence
        self.seq_width = seq_width          # width of squence
        self.batch_size = batch_size

        self._cells = {}

        super().__init__(**kwargs)

    def build(self, input_shape):
        # input dense
        self._dense_in = keras.layers.Dense(self.rnn_dim)

        # many lstm cell layers
        for i in range(self.rnn_layer_num):
            cell = keras.layers.LSTMCell(units=self.rnn_dim)
            self._cells[i] = cell

        # output dense
        self._dens_out = keras.layers.Dense(self.seq_width, activation=""sigmoid"")

        super().build(input_shape)

    def call(self, inputs):
        inputs = tf.squeeze(inputs, 1)

        # init cells' states
        states = getattr(self, 'states', None)
        if states is None:
            states = {}
            for i, cell in self._cells.items():
                init_cell_states = [tf.random.uniform([self.batch_size, self.rnn_dim]),
                                    tf.random.uniform([self.batch_size, self.rnn_dim])]
                states[i] = init_cell_states

        # --- prev outputs as current inputs
        rand_prev = tf.random.uniform([self.batch_size, self.seq_width], dtype=tf.float32)
        prev_inputs = tf.concat([rand_prev, inputs], -1)
        outputs = []
        for _ in range(self.seq_len):
            cell_inputs = self._dense_in(prev_inputs)
            for i, cell in self._cells.items():
                cell_outputs, states[i] = cell(cell_inputs, states[i])
                cell_inputs = cell_outputs
            step_outputs = self._dens_out(cell_outputs)
            prev_inputs = tf.concat([step_outputs, inputs], -1)

            outputs.append(tf.expand_dims(step_outputs, 1))

        # reserve cells' state in current batch
        self.states = states

        return tf.concat(outputs, 1)

# @tf.function
def main():
    batch_size = 17
    seq_width = 4
    rand_input_dim = 3
    inputs = keras.layers.Input(shape=[1, rand_input_dim])
    outputs = RNNGenerator(rnn_dim=64, rnn_layer_num=2, seq_len=10, seq_width=seq_width, batch_size=batch_size)(inputs)
    outputs = keras.layers.Flatten()(outputs)
    outputs = keras.layers.Dense(1)(outputs)
    outputs = tf.nn.sigmoid(outputs)
    model = keras.models.Model(inputs=inputs, outputs=outputs)

    # model.summary()

    X = np.random.rand(batch_size, 1, rand_input_dim).astype(np.float32)
    y = np.zeros([batch_size, 1])

    model.compile(loss=""sparse_categorical_crossentropy"", optimizer=""sgd"", metrics=[""accuracy""])
    model.fit(X, y, batch_size=32, epochs=10)


if __name__ == ""__main__"":
    main()
```

Running info without `@tf.function` before `main`:

```bash
2019-09-28 21:43:30.966344: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-09-28 21:43:30.980391: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f88282a2f60 executing computations on platform Host. Devices:
2019-09-28 21:43:30.980415: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
Train on 17 samples
Epoch 1/10
WARNING:tensorflow:From /Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
17/17 [==============================] - 2s 98ms/sample
Traceback (most recent call last):
  File ""/Users/xxxxxx/Desktop/GAN-seq/test.py"", line 85, in <module>
    main()
  File ""/Users/xxxxxx/Desktop/GAN-seq/test.py"", line 81, in main
    model.fit(X, y, batch_size=32, epochs=10)
  File ""/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py"", line 728, in fit
    use_multiprocessing=use_multiprocessing)
  File ""/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 324, in fit
    total_epochs=epochs)
  File ""/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 123, in run_one_epoch
    batch_outs = execution_function(iterator)
  File ""/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 86, in execution_function
    distributed_function(input_fn))
  File ""/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 457, in __call__
    result = self._call(*args, **kwds)
  File ""/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 520, in _call
    return self._stateless_fn(*args, **kwds)
  File ""/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1823, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File ""/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1141, in _filtered_call
    self.captured_inputs)
  File ""/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1224, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager)
  File ""/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 511, in call
    ctx=ctx)
  File ""/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py"", line 76, in quick_execute
    raise e
  File ""/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py"", line 61, in quick_execute
    num_outputs)
TypeError: An op outside of the function building code is being passed
a ""Graph"" tensor. It is possible to have Graph tensors
leak out of the function building context by including a
tf.init_scope in your function building code.
For example, the following function will fail:
  @tf.function
  def has_init_scope():
    my_constant = tf.constant(1.)
    with tf.init_scope():
      added = my_constant * 2
The graph tensor has name: model/rnn_generator/lstm_cell_10/mul_2:0
```
Running info with `@tf.function` before `main`:
```bash
2019-09-28 21:48:39.227708: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-09-28 21:48:39.242540: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fdda3d85620 executing computations on platform Host. Devices:
2019-09-28 21:48:39.242575: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
WARNING:tensorflow:From /Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Train on 17 samples
Epoch 1/10
Traceback (most recent call last):
  File ""/Users/xxxxxx/Desktop/GAN-seq/test.py"", line 85, in <module>
    main()
  File ""/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 457, in __call__
    result = self._call(*args, **kwds)
  File ""/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 503, in _call
    self._initialize(args, kwds, add_initializers_to=initializer_map)
  File ""/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 408, in _initialize
    *args, **kwds))
  File ""/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1848, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2150, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2041, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py"", line 915, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 358, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py"", line 905, in wrapper
    raise e.ag_error_metadata.to_exception(e)
AttributeError: in converted code:
    relative to /Users/xxxxxx:

    Desktop/GAN-seq/test.py:81 main  *
        model.fit(X, y, batch_size=32, epochs=10)
    anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py:728 fit
        use_multiprocessing=use_multiprocessing)
    anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py:674 fit
        steps_name='steps_per_epoch')
    anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py:393 model_iteration
        batch_outs = f(ins_batch)
    anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py:3635 __call__
        [x._numpy() for x in outputs],  # pylint: disable=protected-access
    anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py:3635 <listcomp>
        [x._numpy() for x in outputs],  # pylint: disable=protected-access

    AttributeError: 'Tensor' object has no attribute '_numpy'

```"
32888,RuntimeError: Variable *= value not supported. Use `var.assign(var * value)` to modify the variable or `var = var * value` to get a new Tensor object. When using tf2.0 rc2 and keras min_max_norm constraints for kernel and bias constraints in Google colab,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- TensorFlow installed from (source or binary): pip
- TensorFlow version GIT_VERSION: v2.0.0-rc1-51-g2646d23
- TensorFlow version : v2.0.0-rc2

```!python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""```
```v2.0.0-rc1-51-g2646d23 2.0.0-rc2```

**Code to reproduce the issue**
```
def create_model(drop, init, embedd_out, kernal_size, padding):
  DROP = drop
  INIT = init
  EMBEDD_OUT = embedd_out
  KERNEL = kernal_size
  PADDING = padding

  inputs = Input(shape=(MAX_SEQ_LENGTH,), name='feature')
  embedded = Embedding(input_dim=len(tokenizer.word_index) + 1, mask_zero=True, output_dim=EMBEDD_OUT,
                       input_length=MAX_SEQ_LENGTH, name='seq_embedd') (inputs)

  conv_filters = [64]
  i = 0  
  for u in conv_filters: 
    if i == 0:
      conv_1 = Conv1D(u, kernel_size=KERNEL, kernel_constraint=min_max_norm(3), bias_constraint=min_max_norm(3),
                      padding=PADDING, activation='relu', kernel_initializer=INIT, name='conv_{}_{}'.format(u, i)) (embedded)
    else:
      conv_1 = Conv1D(u, kernel_size=KERNEL, kernel_constraint=min_max_norm(3), bias_constraint=min_max_norm(3),
                      padding=PADDING, activation='relu', kernel_initializer=INIT, name='conv_{}_{}'.format(u, i)) (conv_1)
    conv_1 = MaxPool1D(KERNEL, padding=PADDING, name='max_{}_{}'.format(u, i)) (conv_1)
    conv_1 = Dropout(DROP) (conv_1)
    i += 1

  conv_filters = [128]
  i = 0  
  for u in conv_filters: 
    if i == 0:
      conv_2 = Conv1D(u, kernel_size=KERNEL, kernel_constraint=min_max_norm(3), bias_constraint=min_max_norm(3),
                      padding=PADDING, activation='relu', kernel_initializer=INIT, name='conv_{}_{}'.format(u, i)) (embedded)
    else:
      conv_2 = Conv1D(u, kernel_size=KERNEL, kernel_constraint=min_max_norm(3), bias_constraint=min_max_norm(3),
                      padding=PADDING, activation='relu', kernel_initializer=INIT, name='conv_{}_{}'.format(u, i)) (conv_2)
    conv_2 = MaxPool1D(KERNEL, padding=PADDING, name='max_{}_{}'.format(u, i)) (conv_2)
    conv_2 = Dropout(DROP) (conv_2)
    i += 1

  merge = keras.layers.concatenate([conv_1, conv_2])
  reduce_1 = AvgPool1D(KERNEL, padding=PADDING, name='avg_reduce_1') (merge)

  lstm_units = [64]
  i = 0  
  for u in lstm_units: 
    if i == 0:
      lstm_1 = Bidirectional(LSTM(u, return_sequences=True, kernel_constraint=min_max_norm(3), bias_constraint=min_max_norm(3),
                                  dropout=DROP, recurrent_dropout=DROP, name='lstm_{}_{}'.format(u, i))) (reduce_1)
    elif i == len(lstm_units):      
      lstm_1 = Bidirectional(LSTM(u, kernel_constraint=min_max_norm(3), bias_constraint=min_max_norm(3),
                                  dropout=DROP, recurrent_dropout=DROP, name='lstm_{}_{}'.format(u, i))) (lstm_1)
    else:
      lstm_1 = Bidirectional(LSTM(u, return_sequences=True, kernel_constraint=min_max_norm(3), bias_constraint=min_max_norm(3),
                                  dropout=DROP, recurrent_dropout=DROP, name='lstm_{}_{}'.format(u, i))) (lstm_1)
    i += 1

  lstm_units = [128]
  i = 0  
  for u in lstm_units: 
    if i == 0:
      lstm_2 = Bidirectional(LSTM(u, return_sequences=True, kernel_constraint=min_max_norm(3), bias_constraint=min_max_norm(3),
                                  dropout=DROP, recurrent_dropout=DROP, name='lstm_{}_{}'.format(u, i))) (reduce_1)
    elif i == len(lstm_units):      
      lstm_2 = Bidirectional(LSTM(u, kernel_constraint=min_max_norm(3), bias_constraint=min_max_norm(3),
                                  dropout=DROP, recurrent_dropout=DROP, name='lstm_{}_{}'.format(u, i))) (lstm_2)
    else:
      lstm_2 = Bidirectional(LSTM(u, return_sequences=True, kernel_constraint=min_max_norm(3), bias_constraint=min_max_norm(3),
                                  dropout=DROP, recurrent_dropout=DROP, name='lstm_{}_{}'.format(u, i))) (lstm_2)
    i += 1

  merge = keras.layers.concatenate([lstm_1, lstm_2])
  reduce_2 = GlobalAveragePooling1D(name='g_avg_reduce') (merge)

  flat = Flatten() (reduce_2)

  dense_units = [512, 512]
  i = 0  
  for u in dense_units: 
    if i == 0:
      dense = Dense(u, kernel_initializer=INIT, kernel_constraint=min_max_norm(3), bias_constraint=min_max_norm(3),
                    activation='relu', name='dense_{}_{}'.format(u, i)) (flat)  
    else:
      dense = Dense(u, kernel_initializer=INIT, kernel_constraint=min_max_norm(3), bias_constraint=min_max_norm(3),
                    activation='relu', name='dense_{}_{}'.format(u, i)) (dense)
    dense = Dropout(DROP) (dense)
    i += 1

  outputs = Dense(2, kernel_initializer=INIT, activation='softmax', name='label') (dense)
  model = Model(inputs=[inputs], outputs=[outputs])
  model.compile(optimizer=keras.optimizers.Adam(amsgrad=True), loss='sparse_categorical_crossentropy', metrics=['acc'])
  return model
```
**Other info / logs**
Train for 3 steps, validate for 1 steps
Epoch 1/500
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Can save best model only with val_loss available, skipping.
WARNING:tensorflow:Can save best model only with val_acc available, skipping.
WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: 

RuntimeError                              Traceback (most recent call last)
<ipython-input-7-65d8bf471350> in <module>()
     13 
     14 model.fit(training_set, epochs=500, validation_data=validation_set,
---> 15           verbose=2, steps_per_epoch=X.shape[0]//BS, class_weight=class_weights, callbacks=callbacks)

21 frames
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py in __imul__(self, unused_other)
   1227 
   1228   def __imul__(self, unused_other):
-> 1229     raise RuntimeError(""Variable *= value not supported. Use ""
   1230                        ""`var.assign(var * value)` to modify the variable or ""
   1231                        ""`var = var * value` to get a new Tensor object."")

RuntimeError: Variable *= value not supported. Use `var.assign(var * value)` to modify the variable or `var = var * value` to get a new Tensor object.
"
32886,image classification code sample error,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:
https://www.tensorflow.org/beta/tutorials/images/classification

## Description of issue (what needs changing):
```
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
```
should be
```
acc = history.history['acc']
val_acc = history.history['val_acc']
```
### Clear description
When I copy and paste the code and run it directly, the model is trained for a few minutes/a while but I get KeyError: 'accuracy' and then when I change that, get KeyError: 'val_accuracy'

Someone would use this to perform image recognition with code they take directly from the Tensorflow docs on this page.

### Correct links
It is not correct yet. I am thinking of making a PR to the docs repo

### Parameters defined
n/a

### Returns defined

n/a

### Raises listed and defined

n/a

### Usage example

to train images

### Request visuals, if applicable

There's a few.

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style

I am!
"
32885,"When importing TensorFlow, error loading Hadoop","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): gLinux (like Debian)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.1-12014-gab20de6 2.0.0-dev20190927
- Python version: 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

Importing TensorFlow prints an error:

> 2019-09-27 13:51:34.191065: E tensorflow/core/platform/hadoop/hadoop_file_system.cc:132] HadoopFileSystem load error: libhdfs.so: cannot open shared object file: No such file or directory

**Describe the expected behavior**

Importing TensorFlow should not print errors about Hadoop. I don’t have
Hadoop installed and am not interested in using it.

**Code to reproduce the issue**

```
$ cd ""$(mktemp -d)""
$ virtualenv -q -p python3.6 ./ve
$ . ./ve/bin/activate
(ve) $ pip install -q tf-nightly-2.0-preview==2.0.0.dev20190927
(ve) $ python -c 'import tensorflow as tf; print(tf.__version__)'
2019-09-27 13:51:21.263111: E tensorflow/core/platform/hadoop/hadoop_file_system.cc:132] HadoopFileSystem load error: libhdfs.so: cannot open shared object file: No such file or directory
2.0.0-dev20190927
```

**Other info / logs**

Probably relevant: <https://github.com/tensorflow/tensorflow/pull/32649>
"
32884,tf.image.extract_patches bug - incorrect values,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
docker built from tensorflow/tensorflow:2.0.0rc0-gpu-py3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
docker built from tensorflow/tensorflow:2.0.0rc0-gpu-py3
- TensorFlow version (use command below):
2.0.0rc0
- Python version:
3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
10
- GPU model and memory:

**Describe the current behavior**
I am using tf.image.extract_patches to get patches of an image read by cv2.
I pad the image with zeros and so I noticed, that the last extracted patch (after reshaping the output) has unexpected values at the very end (last pixels of the last row). They look like coming from another part of the original image. All the other patches look quite all right.
I also tried retyping the image to float32 before it goes to extract_patches - didn't help.

**Code to reproduce the issue**
```
def image_to_patches(image, patch_size, stride):                                                                                                                                                                         
    target_width = ((image.shape[1] - patch_size) // stride + 1) * stride + patch_size + 1                                                                                                                  
    target_height = ((image.shape[0] - patch_size) // stride + 1) * stride + patch_size + 1                                                                                                                 
                                                                                                                                                                                                            
    image = np.pad(image, ((0, target_height - image.shape[0]), (0, target_width - image.shape[1]), (0, 0)))                                                                                                
    # here, the last row of `image` is all zeros                                                                                                                                                                      
    batched_image = np.expand_dims(image, 0)                                                                                                                                                                
    patches = tf.image.extract_patches(                                                                                                                                                                     
        images=batched_image,                                                                                                                                                                               
        sizes=[1, patch_size, patch_size, 1],                                                                                                                                                               
        strides=[1, stride, stride, 1],                                                                                                                                                                     
        rates=[1, 1, 1, 1],                                                                                                                                                                                 
        padding='VALID',                                                                                                                                                                                    
        name=None                                                                                                                                                                                           
    )                                                                                                                                                                                                       
    patches = np.array(patches)                                                                                                                                                                             
    patches = np.resize(patches, (patches.shape[0] * patches.shape[1] * patches.shape[2], -1))                                                                                                              
    patches = np.resize(patches, (patches.shape[0], patch_size, patch_size, image.shape[-1]))                                                                                                               
    return patches                                  
```
"
32881,Tensorflow device indexes mismatch with nvidia-smi,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0rc1
- Python version: 3.7.4
- CUDA/cuDNN version: 10.0 / 7.6.0
- GPU model and memory: RTX 2080 TI  11GB

**Describe the current behavior**

I have two gpus.
My code is designed to determine GPUs info and their indexes before import tensorflow by using nvidia-smi
but tensorflow device indexes mismatch nvidia-smi indexes.

tensorflow log:
```
2019-09-27 21:10:02.247000: I tensorflow/stream_executor/platform/default/dso_lo
ader.cc:44] Successfully opened dynamic library cudart64_100.dll
2019-09-27 21:10:04.473000: I tensorflow/stream_executor/platform/default/dso_lo
ader.cc:44] Successfully opened dynamic library nvcuda.dll
2019-09-27 21:10:04.550000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1

618] Found device 0 with properties:
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:03:00.0
2019-09-27 21:10:04.555000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1

618] Found device 1 with properties:
name: GeForce GT 730 major: 3 minor: 5 memoryClockRate(GHz): 0.9015
pciBusID: 0000:01:00.0

2019-09-27 21:10:04.559000: I tensorflow/stream_executor/platform/default/dlopen
_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-09-27 21:10:04.566000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1
746] Adding visible gpu devices: 0, 1
```

nvidia-smi log:
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 436.30       Driver Version: 436.30       CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GT 730     WDDM  | 00000000:01:00.0 N/A |                  N/A |
| 61%   69C    P0    N/A /  N/A |    959MiB /  2048MiB |     N/A      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce RTX 208... WDDM  | 00000000:03:00.0 Off |                  N/A |
| 40%   78C    P2   143W / 250W |  10992MiB / 11264MiB |     89%      Default |
+-------------------------------+----------------------+----------------------+
```

**Describe the expected behavior**

expected tensorflow match nvidia-smi device indexes

**Code to reproduce the issue**

no code required. Just launch tensorflow initialization to show the info of the devices.
"
32880,deprecation warning when training a simple embedding with custom function,"- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6
- TensorFlow installed from (source or binary): pip install tensorflow==2.0.0-beta1
- TensorFlow version (use command below): v2.0.0-beta1-5101-gc75bb66a99 2.0.0-rc0
- Python version: v3.6.7:6ec5cf24b7, Oct 20 2018, 03:02:14

**Describe the current behavior**

The following code trains a simple embedding on temporal indices, Z, to match a given time series X:
```
import tensorflow as tf
from tensorflow.keras import Model, losses, optimizers
from tensorflow.keras.layers import Embedding
import numpy as np


def extract(indices, X): 
    # extract the lines of X given by the indices
    # if indices.shape = 4
    # and X.shape = (20, 5)
    # then the output has shape (4, 5)
    indices = tf.reshape(indices, [-1, 1])
    return tf.gather_nd(X, indices)


class Test(Model):

    def __init__(self, X, **kwargs):
        super(Test, self).__init__(**kwargs)
        self.X = X

        self.optimizer = optimizers.Adam(learning_rate=1e-3)
        self.loss = losses.CategoricalCrossentropy()

        self.Z = Embedding(X.shape[0], X.shape[1])

    def call(self, inputs, training=None):
        # inputs are temporal indices
        Z_t = self.Z(inputs)
        return Z_t

    def custom_train(self):

        @tf.function
        def _train_Step(index):
            with tf.GradientTape() as tape:
                Z_t = self(index, training=True)
                X_t = extract(index, self.X)
                loss = self.loss(X_t, Z_t)

            grads = tape.gradient(loss, self.trainable_variables)
            self.optimizer.apply_gradients(zip(grads, self.trainable_variables))
            return
        # this is a toy exemple: we simply make one training step on the index 0
        _train_Step(tf.constant([0]))
        return 
    
T = 20
n = 5
X = np.random.rand(T, n)
model = Test(X=X)
model.custom_train()
```
 and I get the following warning about the use of a deprecated method:
```
WARNING: Logging before flag parsing goes to stderr.
W0927 19:04:48.957805 140735485318016 deprecation.py:323] From /path/to/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where

```

This reminds me about [an other issue](https://github.com/tensorflow/tensorflow/issues/29881) I opened with a lot of deprecation warnings arising when I use the API. This issue is closed but the warnings are still here (and, in fact, the release of the rc0 has brought a new one, see my last comment there).

"
32879,I want a example code to use the embedding projector in tensorboard by tensorflow-2.0.,"I want a example code to use the embedding projector in tensorboard by tensorflow-2.0.
"
32878,Iembedding projector ,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
32877,matmul / matvec / einsum bug when dim exceeds 2**16,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 16.04**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): git: **v1.12.1-9365-gff401a6** tf: **1.15.0-dev20190821**
- Python version: 3.6.8
- CUDA/cuDNN version: 10.0 / 6.0.21  (also tested on 10.1 / 7.1.4 / GTX-10708GB
- GPU model and memory: Quadro  2GB

**Describe the current behavior**
GPU float32 implementations of `tf.matmul`, `tf.matvec` and `tf.einsum` all fail when applied to tensors with a `dimension >= 2**16 = 65536` (though not necessarily the summation dimension). Could be related to [this](https://github.com/tensorflow/tensorflow/issues/31022).

Of particular note:

* The example below shows the bug as a result of summing over a dimension of size `2` and inputs of typical size (even `ones`), so is not a case of overflow of the individual entries.
* The issue does not appear when using `tf.float64` or cpu implementation.
* Results are **not deterministic** - at least, not when the large dimension is `2 ** 16 + 1` or more and using `tf.linalg.matvec` implementation, thoguh they do appear stable on at `2 ** 16`. The `tf.linalg.matvec` implementation seems to 'converge' to the the hacky `tf.matmul` version, thoguh that converged value is different to the `einsum` value.
* The issue does not appear in this example when running in graph mode, though I believe it still does occur in other circumstances. The more complicated example I extracted this from is not resolved by running in graph mode, but I cannot produce a simpler example that exhibits failure in graph mode. This may be because of the non-determinism discussed above.

**Describe the expected behavior**
Consistency of implementation between CPU/GPU, correct values.

**Code to reproduce the issue**
```python
import numpy as np
import tensorflow as tf

np.random.seed(123)


def matvec_prim_np(a, b):
    """"""numpy implementation of tf.linalg.matvec(a, b, transpose_a=True).""""""
    return np.sum(a * np.expand_dims(b, axis=-1), axis=-2)


def matvec_einsum_np(a, b):
    return np.einsum('ijk,ij->ik', a, b)


def matvec_linalg(a, b):
    return tf.linalg.matvec(a, b, transpose_a=True)


def matvec_matmul(a, b):
    return tf.squeeze(tf.linalg.matmul(a,
                                       tf.expand_dims(b, axis=-1),
                                       transpose_a=True),
                      axis=-1)


def matvec_prim(a, b):
    return tf.reduce_sum(a * tf.expand_dims(b, axis=-1), axis=-2)


def matvec_einsum(a, b):
    a = tf.convert_to_tensor(a)
    b = tf.convert_to_tensor(b)
    return tf.einsum('ijk,ij->ik', a, b)


def max_err(x, y):
    if hasattr(x, 'numpy'):
        x = x.numpy()
    if hasattr(y, 'numpy'):
        y = y.numpy()
    return np.max(np.abs(x - y))


def report(N, values='ones', dtype=np.float64, device='/gpu:0'):
    shapes = (N, 2, 2), (N, 2)
    if values == 'ones':
        a, b = (np.ones(s, dtype=dtype) for s in shapes)
    elif values == 'uniform':
        a, b = (np.random.uniform(size=s).astype(dtype=dtype) for s in shapes)
    elif values == 'normal':
        a, b = (np.random.normal(size=s).astype(dtype=dtype) for s in shapes)
    else:
        raise ValueError('values must be ""one"", ""uniform"" or ""normal"", '
                         'got {}'.format(values))
    a_extents = np.min(a), np.max(a)
    b_extents = np.min(b), np.max(b)
    base = matvec_prim_np(a, b)
    names = ['matvec_einsum_np']
    vals = [matvec_einsum_np(a, b)]
    fns = (matvec_prim, matvec_linalg, matvec_matmul, matvec_einsum)
    a = tf.constant(a, dtype=dtype)
    b = tf.constant(b, dtype=dtype)
    # call each function twice to demonstrate non-determinism of linalg.matvec
    with tf.device(device):
        tf_vals = tuple(fn(a, b) for fn in fns) + tuple(fn(a, b) for fn in fns)
    names.extend(fn.__name__ for fn in fns)
    names.extend(fn.__name__ for fn in fns)
    if tf.executing_eagerly():
        tf_vals = [v.numpy() for v in tf_vals]
    else:
        with tf.Session() as sess:
            tf_vals = sess.run(tf_vals)
    vals.extend(tf_vals)

    print('##### {} - {} - {} - {} ####'.format(N, dtype.__name__, values,
                                                device))
    print('a extents: {}'.format(a_extents))
    print('b extents: {}'.format(b_extents))
    for name, value in zip(names, vals):
        print('{:20s}: {}'.format(name, max_err(base, value)))


works = 65535
just_too_big = 65536
even_bigger = 65537
dtype = np.float64

tf.compat.v1.enable_eager_execution()
for device in '/cpu:0', '/gpu:0':
    for dtype in (np.float64, np.float32):
        for N in works, just_too_big, even_bigger:
            for values in ('ones', 'uniform', 'normal'):
                report(N, dtype=dtype, values=values, device=device)
```

**Other info / logs**
All results in the above are good except for `N >= 65536`, `dtype=tf.float32` and `device='/gpu:0'`. Those logs are below.

```
##### 65535 - float32 - normal - /gpu:0 ####
a extents: (-4.471484, 4.361648)
b extents: (-5.123302, 4.7598076)
matvec_einsum_np    : 0.0
matvec_prim         : 0.0
matvec_linalg       : 0.0
matvec_matmul       : 0.0
matvec_einsum       : 0.0
matvec_prim         : 0.0
matvec_linalg       : 0.0
matvec_matmul       : 0.0
matvec_einsum       : 0.0
##### 65536 - float32 - ones - /gpu:0 ####
a extents: (1.0, 1.0)
b extents: (1.0, 1.0)
matvec_einsum_np    : 0.0
matvec_prim         : 0.0
matvec_linalg       : 1.0
matvec_matmul       : 2.0
matvec_einsum       : 1.0
matvec_prim         : 0.0
matvec_linalg       : 1.0
matvec_matmul       : 2.0
matvec_einsum       : 1.0
##### 65536 - float32 - uniform - /gpu:0 ####
a extents: (1.0896997e-06, 0.9999953)
b extents: (1.2711744e-06, 0.99999875)
matvec_einsum_np    : 0.0
matvec_prim         : 0.0
matvec_linalg       : 0.24248471856117249
matvec_matmul       : 0.3159205913543701
matvec_einsum       : 0.4631575047969818
matvec_prim         : 0.0
matvec_linalg       : 0.24248471856117249
matvec_matmul       : 0.3159205913543701
matvec_einsum       : 0.4631575047969818
##### 65536 - float32 - normal - /gpu:0 ####
a extents: (-4.715384, 4.746153)
b extents: (-4.523676, 4.59162)
matvec_einsum_np    : 0.0
matvec_prim         : 0.0
matvec_linalg       : 0.6585559248924255
matvec_matmul       : 0.709877073764801
matvec_einsum       : 0.4629881978034973
matvec_prim         : 0.0
matvec_linalg       : 0.6585559248924255
matvec_matmul       : 0.709877073764801
matvec_einsum       : 0.4629881978034973
##### 65537 - float32 - ones - /gpu:0 ####
a extents: (1.0, 1.0)
b extents: (1.0, 1.0)
matvec_einsum_np    : 0.0
matvec_prim         : 0.0
matvec_linalg       : 4.812917232513428
matvec_matmul       : 2.0
matvec_einsum       : 1.0
matvec_prim         : 0.0
matvec_linalg       : 2.0
matvec_matmul       : 2.0
matvec_einsum       : 1.0
##### 65537 - float32 - uniform - /gpu:0 ####
a extents: (6.188006e-07, 0.9999941)
b extents: (5.5208016e-06, 0.9999933)
matvec_einsum_np    : 0.0
matvec_prim         : 0.0
matvec_linalg       : 2.9982364177703857
matvec_matmul       : 0.4082830250263214
matvec_einsum       : 0.40019217133522034
matvec_prim         : 0.0
matvec_linalg       : 0.4082830250263214
matvec_matmul       : 0.4082830250263214
matvec_einsum       : 0.40019217133522034
##### 65537 - float32 - normal - /gpu:0 ####
a extents: (-5.1086445, 4.5197787)
b extents: (-4.8438015, 3.928705)
matvec_einsum_np    : 0.0
matvec_prim         : 0.0
matvec_linalg       : 4.819068908691406      <----------- same function, same inputs
matvec_matmul       : 2.1483089923858643
matvec_einsum       : 2.5970025062561035
matvec_prim         : 0.0
matvec_linalg       : 2.1483089923858643     <----------- same function, same inputs
matvec_matmul       : 2.1483089923858643
matvec_einsum       : 2.5970025062561035
```
"
32876,"How to use estimator predict on multi machine, predict run on every worker?",
32875,tf.keras.metrics.MeanIoU have some conflicts with sparse_categorical_crossentropy,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.0 RC
- Python version: 3.6
- CUDA/cuDNN version: 10.0
- GPU model and memory: 12Gb

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
```
mIOU = tf.keras.metrics.MeanIoU(num_classes=20)
model.compile(optimizer='Adam',
              loss='sparse_categorical_crossentropy',
              metrics=[""accuracy"", mIOU])
```

Hi, sparse_categorical_crossentropy will calculate the probabilities of 20 classes, which have the shape (None, 64, 1024, 20). However, the label has the shape (None, 64, 1024). Thus, the mIOU got unequal shape inputs. It got errors
"
32874,scientific research on the evaluation of Tensorflow,"Hello, everyone!
We are students from Linkoping University, Sweden. We are doing research on evaluation of Tensorflow, so we need help!
There is a questionnaire we need you to answer! Just 1-2 minutes!
Thank you so much for your patience and kindness!

https://docs.google.com/forms/d/e/1FAIpQLSfvqIUxxI5mm82dr_M8Ja7y_eGG0mqXzfCLQ0c6ehjqSjAQkA/viewform?usp=sf_link"
32873,"XLA bug w/ Keras: ""Node name contains invalid characters""","**System information**
- Have I written custom code: yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): tensorflow-gpu==1.14.0+nv
- Python version: 3.6.8
- CUDA/cuDNN version: 10.1
- GPU model and memory: RTX 2080 Ti, NVIDIA Driver 418.43

**Describe the current behavior**

Shows the warning at the predict time:
```2019-09-27 11:02:42.377419: W tensorflow/core/common_runtime/process_function_library_runtime.cc:667] Ignoring multi-device function optimization failure: Invalid argument: Node '_arg_segments_ids_input_0_1_0_arg': Node name contains invalid characters```

And doesn't enable XLA optimizations as I can see (performance doesn't improve)

**Describe the expected behavior**

There should be no problem at all.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

Occured in a real-case keras-bert based code, but reproduces in the following code:

```
import numpy as np
import tensorflow as tf
from tensorflow.keras.backend import set_session
from tensorflow.keras.layers import Input, Embedding, Add
from tensorflow.keras.models import Model

config = tf.ConfigProto()
config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1
set_session(tf.Session(config=config))

seq_len = 80
batch_size=5000

tokens_ids_input = Input(shape=(seq_len, ), dtype='int32', name='tokens_ids_input')
segments_ids_input = Input(shape=(seq_len, ), dtype='int32', name='segments_ids_input')

output = Add()([Embedding(input_dim=5000, output_dim=16)(tokens_ids_input), Embedding(input_dim=2, output_dim=16)(segments_ids_input)])

model = Model(inputs=[tokens_ids_input, segments_ids_input], outputs=[output])

model.predict([np.zeros((batch_size, seq_len)), np.zeros((batch_size, seq_len))])
```"
32872,"In batching.map_and_batch, is map_fn for each sample build by random order?","Hi, 
I meet the difficulty to reproduce the result with fixed random_seed.
tf.set_random_seed(0) is set before building a graph, for each run (rerun the script without modifying the codes), samples (after shuffle) are feed into network by the same order. But the output of `tf.image.random_flip_left_right` in `map_fn` in `batching.map_and_batch` still produce random flip image.  **In short, same input file, random flip result.**

If I set `seed=0` in `tf.image.random_flip_left_right`, the outputs are still random. So I think the order in which the graph is build is the key. The order may not be controlled by that global random seed. 
Any suggestion or reference for this problem, please? I use tf1.8 with horovod. 
Thank you.

"
32870,No gradient defined for operation 'ExtractVolumePatches',"I saw it already been solved for images but it still hasn't been solved for 3D volumes.

The ExtractImagePatches solution:
https://github.com/tensorflow/tensorflow/issues/2921

I'm using tf 1.12 and get the following message:
LookupError: No gradient defined for operation 'gpu_0/local_z_3d/EXTRACT_LOCAL_Z' (op type: ExtractVolumePatches)
"
32869,sparse_categorical_crossentropy does not use underlying logits of tf.keras.layers.Softmax,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0.0-rc0
- Python version: 3.6

**Describe the current behavior**
As per the comment in `sparse_categorical_crossentropy`, the function will use the underlying logits for better numerical stability if the last op is a softmax: https://github.com/tensorflow/tensorflow/blob/1ae01a07f5e4b2911218d5fc3419b3eaed48b75b/tensorflow/python/keras/backend.py#L4522-L4533

But since keras adds an identity op to the output of every layer, the last node of a layer will always be `Identity` so the condition in the above if statement will never fail for keras models and the underlying logits are never used: https://github.com/tensorflow/tensorflow/blob/b0aa37c3fdff00b5c69bce11c716c3e56f656dd4/tensorflow/python/keras/engine/base_layer.py#L851-L853

Using non-keras-layers like `tf.nn.softmax` will work, however.

**Describe the expected behavior**
Instead, `sparse_categorical_crossentropy` should check the first non-`Identity` node when determining whether the last node is a softmax node. I.e. it should first walk the chain of identity nodes.

**Code to reproduce the issue**
```
import tensorflow as tf

x = tf.keras.layers.Input(shape=[10])

softmax_keras = tf.keras.layers.Softmax()(x)
softmax_tf = tf.nn.softmax(x)

y_true = tf.keras.layers.Input(shape=[])
loss_keras = tf.keras.losses.sparse_categorical_crossentropy(y_true, softmax_keras)
loss_tf = tf.keras.losses.sparse_categorical_crossentropy(y_true, softmax_tf)

def transitive_inputs(tensor):
    return set([tensor.op.type] + [inp for tensor in tensor.op.inputs for inp in transitive_inputs(tensor)])

print('Softmax' in transitive_inputs(loss_keras)) # prints ""True""
print('Softmax' in transitive_inputs(loss_tf)) # prints ""False""
```"
32868,Combo TPU/TFRecords for model.evaluate is not working,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- TensorFlow installed from (source or binary): 
- TensorFlow version (use command below): 1.14
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: TPU Colab

**Describe the current behavior**
On a TPU (colab) running model.evaluate on a tf.data.Dataset build with TFRecords throw :
Compilation failure: Dynamic Spatial reduce window is not supported: %reduce-window.21 = f32[1,127,127,3]{3,2,1,0} reduce-window(f32[1,256,256,3]{3,2,1,0} %reshape.12, f32[] %constant.16), window={size=1x3x3x1 stride=1x2x2x1}, to_apply=%max_F32.17, metadata={op_type=""MaxPool"" op_name=""max_pooling2d_4/MaxPool""}
	TPU compilation failed

The fit method work perfectly with the same dataset.
Evaluation working perfectly if i rebuild the model and load the weights on a CPU/GPU instance.

I don't have this issue on TPU if the tf.data.Dataset is not built from TFRecords

**Describe the expected behavior**
model.evaluate should work and provide a result close from the last fit iteration

**Code to reproduce the issue**

Put it on Colab and replace GOOGLE_BUCKET_TO_DEFINE by a real bucket (2 occurrences)

```
import tensorflow as tf
import numpy as np

#get a image as input data for model : tensorflow logo
!curl https://avatars0.githubusercontent.com/u/15658638?s=256 --output tensor_logo.png

#build 8 tfrecords based on the logo downloaded
def build_tf_records():
  def serialize_example_pyfunction(image, label):
    feature = {
        'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image.numpy()])),
        'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label.numpy()]))
    }
    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))
    return example_proto.SerializeToString()

  def tf_serialize_example(image, label):
    tf_string = tf.py_function(
      serialize_example_pyfunction,
      (image, label),  # pass these args to the above function.
      tf.string)      # the return type is `tf.string`.
    return tf.reshape(tf_string, ())# The result is a scalar

  dataset = tf.data.Dataset.from_tensor_slices(['/content/tensor_logo.png' for x in range(8)])
  dataset = dataset.map(lambda x : (tf.read_file(x), 0))
  dataset = dataset.map(tf_serialize_example)
  return dataset

#write tf records to disc and move on a google bucket
with tf.Session() as sess:
  filename = 'tfrecord.test'
  writer = tf.data.experimental.TFRecordWriter(filename)
  writting = writer.write(build_tf_records())
  sess.run(writting)

!ls

from google.colab import auth
auth.authenticate_user()

!gsutil cp tfrecord.test gs://GOOGLE_BUCKET_TO_DEFINE/

# the aim is to quickly build 8 items (x, y=0) with x of shape (256, 256, 3)
def train_input_fn_dummy_data():
  dataset = tf.data.Dataset.from_tensor_slices(([0 for x in range(8)]))
  dataset = dataset.map(lambda x : (tf.random.normal((256, 256, 3)), [0]))
  dataset = dataset.batch(8)
  return dataset
  

def train_input_fn_from_tf_records():
  # Create a description of the features.
  feature_description = {
    'image': tf.FixedLenFeature([], tf.string),
    'label': tf.FixedLenFeature([], tf.int64, default_value=0)
  }

  def _parse_function(example_proto):
    # Parse the input tf.Example proto using the dictionary above.
    return tf.parse_single_example(example_proto, feature_description)

  def _process_string_image(dic):
    image_string = dic['image']
    image_decoded = tf.image.decode_png(image_string, channels=3)
    image_decoded = tf.cast(image_decoded, tf.float32)/255.
    return image_decoded, tf.cast([dic['label']], tf.int32)

  list_files = tf.data.Dataset.list_files('gs://GOOGLE_BUCKET_TO_DEFINE/tfrecord.test')
  raw_tfrecords = tf.data.TFRecordDataset(list_files)
  files_as_dict = raw_tfrecords.map(_parse_function)
  files = files_as_dict.map(_process_string_image)
  files = files.batch(8, drop_remainder=True)
  
  return files

#basic check to compare train_input_fn_dummy_data, train_input_fn_from_tf_records
#can't be run after TPU initialisation
with tf.Session() as sess:
  batch = train_input_fn_dummy_data().make_one_shot_iterator().get_next()
  while True:
      try:
          records = sess.run(batch)
          print('shape of dummy items :', records[0].shape, records[1].shape)
      except tf.errors.OutOfRangeError: break
  batch = train_input_fn_from_tf_records().make_one_shot_iterator().get_next()
  while True:
      try:
          records = sess.run(batch)
          print('shape of tfrecords items :', records[0].shape, records[1].shape)
      except tf.errors.OutOfRangeError: break

##shape of dummy items : (8, 256, 256, 3) (8, 1)
##shape of tfrecords items : (8, 256, 256, 3) (8, 1)


#initialize tpu only once
if not('strategy' in globals()):
  resolver = tf.contrib.cluster_resolver.TPUClusterResolver()
  tf.contrib.distribute.initialize_tpu_system(resolver)
  strategy = tf.contrib.distribute.TPUStrategy(resolver)

#build model and compile
with strategy.scope():
  inputs = tf.keras.layers.Input(shape=(256, 256, 3))
  x = tf.keras.layers.MaxPooling2D((3, 3), strides=(2, 2))(inputs)
  output = tf.keras.layers.GlobalAveragePooling2D()(x)
  output = tf.keras.layers.Dense(1, activation = 'sigmoid')(output)
  model = tf.keras.Model(inputs=inputs, outputs=output)
  model.compile('adam', loss='binary_crossentropy', metrics=['binary_accuracy'])

model.summary()

# fit the model, no issue
print('train dummy')
model.fit(train_input_fn_dummy_data(), epochs= 1, steps_per_epoch=1)
print('train tfrecords')
model.fit(train_input_fn_from_tf_records(), epochs= 1, steps_per_epoch=1)
print('evaluate dummy')
model.evaluate(train_input_fn_dummy_data(), steps=1)
print('evaluate tfrecords')
model.evaluate(train_input_fn_from_tf_records(), steps=1)

##train dummy
##WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. ##Please invoke `shuffle()` on input dataset.
##WARNING:tensorflow:From /usr/local/lib/python3.6/dist-##packages/tensorflow/python/keras/engine/training_distributed.py:411: Variable.load (from ##tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
##Instructions for updating:
##Prefer Variable.assign which has equivalent behavior in 2.X.
##1/1 [==============================] - 0s 397ms/step - loss: 0.3180 - ##binary_accuracy: 1.0000
##train tfrecords
##1/1 [==============================] - 1s 786ms/step - loss: 0.4993 - ##binary_accuracy: 1.0000
##evaluate dummy
##1/1 [==============================] - 1s 1s/step
##1/1 [==============================] - 1s 1s/step
##evaluate tfrecords
##---------------------------------------------------------------------------
##UnimplementedError                        Traceback (most recent call last)
##/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, ##fn, *args)
##   1355     try:
##-> 1356       return fn(*args)
##   1357     except errors.OpError as e:
##
##10 frames
##UnimplementedError: From /job:worker/replica:0/task:0:
##Compilation failure: Dynamic Spatial reduce window is not supported: %reduce-window.21 ##= f32[1,127,127,3]{3,2,1,0} reduce-window(f32[1,256,256,3]{3,2,1,0} %reshape.12, f32[] ##%constant.16), window={size=1x3x3x1 stride=1x2x2x1}, to_apply=%max_F32.17, ##metadata=##{op_type=""MaxPool"" op_name=""max_pooling2d_4/MaxPool""}
##	TPU compilation failed
##	 [[{{node TPUReplicateMetadata_3}}]]
##
##During handling of the above exception, another exception occurred:


```
**Other info / logs**

The fit method work perfectly with the same dataset.
Evaluation working perfectly if i rebuild the model and load the weights on a CPU/GPU instance.

I don't have this issue on TPU if the tf.data.Dataset is not built from TFRecords

Full StackTrace : 
UnimplementedError                        Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1355     try:
-> 1356       return fn(*args)
   1357     except errors.OpError as e:

10 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1340       return self._call_tf_sessionrun(
-> 1341           options, feed_dict, fetch_list, target_list, run_metadata)
   1342 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)
   1428         self._session, options, feed_dict, fetch_list, target_list,
-> 1429         run_metadata)
   1430 

UnimplementedError: From /job:worker/replica:0/task:0:
Compilation failure: Dynamic Spatial reduce window is not supported: %reduce-window.21 = f32[1,127,127,3]{3,2,1,0} reduce-window(f32[1,256,256,3]{3,2,1,0} %reshape.12, f32[] %constant.16), window={size=1x3x3x1 stride=1x2x2x1}, to_apply=%max_F32.17, metadata={op_type=""MaxPool"" op_name=""max_pooling2d_4/MaxPool""}
	TPU compilation failed
	 [[{{node TPUReplicateMetadata_3}}]]

During handling of the above exception, another exception occurred:

UnimplementedError                        Traceback (most recent call last)
<ipython-input-8-22e1b9ed9dfe> in <module>()
      6 model.evaluate(train_input_fn_dummy_data(), steps=1)
      7 print('evaluate tfrecords')
----> 8 model.evaluate(train_input_fn_from_tf_records(), steps=1)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in evaluate(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)
    902             sample_weight=sample_weight,
    903             steps=steps,
--> 904             callbacks=callbacks)
    905 
    906     batch_size = self._validate_or_infer_batch_size(batch_size, steps, x)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_distributed.py in evaluate_distributed(model, x, y, batch_size, verbose, sample_weight, steps, callbacks)
    168   if distributed_training_utils.is_tpu_strategy(model._distribution_strategy):
    169     return experimental_tpu_test_loop(
--> 170         model, dataset, verbose=verbose, steps=steps, callbacks=callbacks)
    171   else:
    172     return training_arrays.test_loop(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_distributed.py in experimental_tpu_test_loop(model, dataset, verbose, steps, callbacks)
    562     callbacks._call_batch_hook(mode, 'begin', current_step, batch_logs)
    563     try:
--> 564       _, batch_outs = K.batch_get_value([test_op, output_tensors])
    565     except errors.OutOfRangeError:
    566       warning_msg = 'Make sure that your dataset can generate at least '

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in batch_get_value(tensors)
   3008     raise RuntimeError('Cannot get value inside Tensorflow graph function.')
   3009   if tensors:
-> 3010     return get_session(tensors).run(tensors)
   3011   else:
   3012     return []

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    948     try:
    949       result = self._run(None, fetches, feed_dict, options_ptr,
--> 950                          run_metadata_ptr)
    951       if run_metadata:
    952         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1171     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1172       results = self._do_run(handle, final_targets, final_fetches,
-> 1173                              feed_dict_tensor, options, run_metadata)
   1174     else:
   1175       results = []

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1348     if handle is None:
   1349       return self._do_call(_run_fn, feeds, fetches, targets, options,
-> 1350                            run_metadata)
   1351     else:
   1352       return self._do_call(_prun_fn, handle, feeds, fetches)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1368           pass
   1369       message = error_interpolation.interpolate(message, self._graph)
-> 1370       raise type(e)(node_def, op, message)
   1371 
   1372   def _extend_graph(self):

UnimplementedError: From /job:worker/replica:0/task:0:
Compilation failure: Dynamic Spatial reduce window is not supported: %reduce-window.21 = f32[1,127,127,3]{3,2,1,0} reduce-window(f32[1,256,256,3]{3,2,1,0} %reshape.12, f32[] %constant.16), window={size=1x3x3x1 stride=1x2x2x1}, to_apply=%max_F32.17, metadata={op_type=""MaxPool"" op_name=""max_pooling2d_4/MaxPool""}
	TPU compilation failed
	 [[node TPUReplicateMetadata_3 (defined at <ipython-input-8-22e1b9ed9dfe>:8) ]]

Original stack trace for 'TPUReplicateMetadata_3':
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py"", line 477, in start
    ioloop.IOLoop.instance().start()
  File ""/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py"", line 832, in start
    self._run_callback(self._callbacks.popleft())
  File ""/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py"", line 605, in _run_callback
    ret = callback()
  File ""/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py"", line 536, in <lambda>
    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))
  File ""/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py"", line 450, in _handle_events
    self._handle_recv()
  File ""/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py"", line 480, in _handle_recv
    self._run_callback(callback, msg)
  File ""/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py"", line 432, in _run_callback
    callback(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py"", line 235, in dispatch_shell
    handler(stream, idents, msg)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py"", line 533, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py"", line 2718, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py"", line 2828, in run_ast_nodes
    if self.run_code(code, result):
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py"", line 2882, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-8-22e1b9ed9dfe>"", line 8, in <module>
    model.evaluate(train_input_fn_from_tf_records(), steps=1)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py"", line 904, in evaluate
    callbacks=callbacks)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_distributed.py"", line 170, in evaluate_distributed
    model, dataset, verbose=verbose, steps=steps, callbacks=callbacks)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_distributed.py"", line 520, in experimental_tpu_test_loop
    _test_step_fn, args=(test_input_data,))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py"", line 249, in experimental_run_v2
    return _tpu_run(self, fn, args, kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py"", line 196, in _tpu_run
    maximum_shapes=maximum_shapes)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/tpu/tpu.py"", line 592, in replicate
    maximum_shapes=maximum_shapes)[1]
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/tpu/tpu.py"", line 854, in split_compile_and_replicate
    num_replicas=num_replicas, use_tpu=use_tpu, **metadata_kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_tpu_ops.py"", line 6039, in tpu_replicate_metadata
    name=name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3616, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 2005, in __init__
    self._traceback = tf_stack.extract_stack()"
32866,The use of tflite Model of C3D Network in Android ,"I use a model of C3D network training to convert to a tflite format; 
use in the android project available at:
https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android
The following question arises: is android not currently supported Con3D? 5D data?

E/tensorflow: CameraActivity: Exception!
    java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/strided_slice.cc StridedSlice op only supports 1D-4D input arrays.
    Node number 0 (STRIDED_SLICE) failed to prepare.
    
        at org.tensorflow.lite.NativeInterpreterWrapper.allocateTensors(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:96)
        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:61)
        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:224)
        at org.tensorflow.lite.examples.classification.tflite.Classifier.<init>(Classifier.java:189)
        at org.tensorflow.lite.examples.classification.tflite.ClassifierFloatMobileNet.<init>(ClassifierFloatMobileNet.java:41)
        at org.tensorflow.lite.examples.classification.tflite.Classifier.create(Classifier.java:100)
        at org.tensorflow.lite.examples.classification.ClassifierActivity.recreateClassifier(ClassifierActivity.java:168)
        at org.tensorflow.lite.examples.classification.ClassifierActivity.onPreviewSizeChosen(ClassifierActivity.java:71)
        at org.tensorflow.lite.examples.classification.CameraActivity.onPreviewFrame(CameraActivity.java:232)
        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1261)
        at android.os.Handler.dispatchMessage(Handler.java:110)
        at android.os.Looper.loop(Looper.java:203)
        at android.app.ActivityThread.main(ActivityThread.java:6406)
        at java.lang.reflect.Method.invoke(Native Method)
        at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:1113)
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:974)

"
32865,fit_generator validation steps running on CPU?,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see below.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Nvidia L4T 32.2.1 (based on Ubuntu 18.04) running on Jetson Nano
- TensorFlow installed from (source or binary): binary from Nvidia Jetson repo.
- TensorFlow version (use command below): unknown 1.14.0 (Also seen on 1.13.0)
- Python version: 3.6.8
- CUDA/cuDNN version: CUDA 10.0.326, cuDNN 7.5.0.56-1+cuda10.0
- GPU model and memory: NVIDIA Jetson NANO/TX1, 4Gb (shared)

**Describe the current behavior**

Training steps use 100% of GPU as reported by tegrastats; validation steps use 0% of GPU and 350% CPU. 3.4Gb of memory is used and 338k is free. 

**Describe the expected behavior**

Validation steps should also be run on GPU.

**Code to reproduce the issue**

Full code at  https://github.com/simoncozens/atokern/blob/master/badkerndetector.py and https://github.com/simoncozens/atokern/blob/master/nntools.py . Relevant portion:

```
    checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath=output_dir+'/output/kernmodel-cp-val.hdf5', verbose=0, save_best_only=True, monitor=""val_loss"")
    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=lr_decay, patience=20, verbose=1, mode='auto', min_delta=1e-6, cooldown=100, min_lr=0)
    tensorboard = tf.keras.callbacks.TensorBoard(log_dir=output_dir+""/output"", histogram_freq=1,
    write_graph=False, write_grads=False,batch_size=self.batch_size,update_freq='batch',
    write_images=False,profile_batch=0 )

    callback_list = [
      checkpointer,
      reduce_lr,
      tensorboard,
    ]

    history = self.model.fit_generator(
      generator = self.generator,
      validation_data = self.validation_generator,
      steps_per_epoch = steps_per_epoch,
      validation_steps = validation_steps,
      epochs=epochs, verbose=1, callbacks=callback_list,
      max_queue_size=30
    )
```

**Other info / logs**

My suspicion is that this is memory allocation related, as I've had to reduce the batch size - with larger batch sizes, the training will run all training steps and then OOM when it enters the validation stage.

Is it possible that the validation step can't allocate something on the GPU and so uses CPU instead?"
32864,BatchNormalization update ops with gradient tape,how to update moving mean and moving var in each BatchNormalization layer in gradient tape mode?
32863,save and recover problem in TensorFlow,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04.2
- TensorFlow version:1.14.0
- Python version:3.6.9
- Installed using virtualenv? pip? conda?:conda
- CUDA/cuDNN version:7/10

**Describe the problem**
I try to save the model by Saver.save(), and then use restore to recover the model, but with I get the same results acculated from the save file even though their inputs are different. I don't understand why I get the wrong answer, maybe because the output is stored?

**Provide the exact sequence of commands / steps that you executed before running into the problem**

the save code:
```
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import gzip
import os
import sys
import time
import joblib
import math
import numpy
from six.moves import urllib
from six.moves import xrange  
from PIL import Image
from sklearn.metrics import confusion_matrix as sk_confusion_matrix
from sklearn.metrics import classification_report
import tensorflow as tf

os.environ[""CUDA_VISIBLE_DEVICES""] = ""1""
FLAGS = None
IMAGE_HEIGHT = 128
IMAGE_WEITH = 128
NUM_CHANNELS = 1
NUM_LABELS = 4
SEED = 66478  # Set to None for random seed.
BATCH_SIZE = 32
EVAL_BATCH_SIZE = 32
EVAL_FREQUENCY = 10  # Number of steps between evaluations.

def data_type():
  """"""Return the type of the activations, weights, and placeholder variables.""""""
  if FLAGS.use_fp16:
    return tf.float16
  else:
    return tf.float32


def fake_data(num_images):
  """"""Generate a fake dataset that matches the dimensions of MNIST.""""""
  data = numpy.ndarray(
      shape=(num_images, IMAGE_HEIGHT, IMAGE_WEITH, NUM_CHANNELS),
      dtype=numpy.float32)
  labels = numpy.zeros(shape=(num_images,), dtype=numpy.int64)
  for image in xrange(num_images):
    label = image % 2
    data[image, :, :, 0] = label - 0.5
    labels[image] = label
  return data, labels


def error_rate(predictions, labels):
  Confusion_matrix=sk_confusion_matrix(numpy.argmax(predictions, 1).tolist(), labels.tolist())
  print('Confusion_matrix:')
  print(Confusion_matrix)  

  Se1 = Confusion_matrix[1,1]+Confusion_matrix[2,2]+Confusion_matrix[3,3]
  Se2 = Confusion_matrix[1,1]+Confusion_matrix[1,0]+Confusion_matrix[1,2]+Confusion_matrix[1,3]+Confusion_matrix[2,2]+Confusion_matrix[2,0]+Confusion_matrix[2,1]+Confusion_matrix[2,3]+Confusion_matrix[3,3]+Confusion_matrix[3,0]+Confusion_matrix[3,1]+Confusion_matrix[3,2]
  Se = Se1/Se2
  Sp = Confusion_matrix[0,0]/(Confusion_matrix[0,0]+Confusion_matrix[0,1]+Confusion_matrix[0,2]+Confusion_matrix[0,3]) 
  Acc = (Se+Sp)*100/2

  target_names = ['class 0', 'class 1', 'class 2', 'class 3']

  print()
  accuracy = 100.0-(100.0 *numpy.sum(numpy.argmax(predictions, 1) == labels)/predictions.shape[0])
  
  """"""Return the error rate based on dense predictions and sparse labels.""""""
  return 100.0 - (
      100.0 *
      numpy.sum(numpy.argmax(predictions, 1) == labels) /
      predictions.shape[0]), Acc

def GroupNorm(x, G, eps=1e-05):
    # x: input features with shape [N,H,W,C]
    # gamma, beta: scale and offset, with shape [1,C,1,1]
    # G: number of groups for GN
  N, H, W, C = x.shape
  N = BATCH_SIZE
  gamma = tf.ones([1, 1, 1, C])
  beta = tf.zeros([1, 1, 1, C])
  x = tf.reshape(x, [N, G, H, W, C // G])
  mean, var = tf.nn.moments(x, [2, 3, 4], keep_dims=True)
  x = (x-mean) / tf.sqrt(var + eps)
  x = tf.reshape(x, [N, H, W, C])
  return x * gamma + beta

class ResBlock(object):

  def __init__(self, stride_num=1, downsample=False):
    self.conv1_weights = tf.Variable(
      tf.truncated_normal([1, 1, 64, 64],  # 1x1 filter, depth 64.
                          stddev=0.1,
                          seed=SEED, dtype=data_type()))  

    self.conv2_weights = tf.Variable(
      tf.truncated_normal([3, 3, 64, 64],  # 3x3 filter, depth 64.
                          stddev=0.1,
                          seed=SEED, dtype=data_type()))
    self.conv3_weights = tf.Variable(
      tf.truncated_normal([3, 3, 64, 64],  # 3x3 filter, depth 64.
                          stddev=0.1,
                          seed=SEED, dtype=data_type()))  
    self.stride_num = stride_num
    self.downsample = downsample

  def forward(self, data):
    with tf.name_scope('ResNet'):
    # shortcut = x
      shortcut = data
      # out = self.relu(self.norm1(x))
#      axis = list(range(len(data.get_shape()) - 1))
      with tf.name_scope('BN1'):
        out = GroupNorm(x=data, G=32)
        #mean, variance = tf.nn.moments(data, axis)
        #out = tf.nn.batch_normalization(data, mean, variance, 0, 1, 0.001)
      with tf.name_scope('relu1'):
        out = tf.nn.relu(out)
      #  if self.downsample is not None:
        #   shortcut = self.downsample(out)
      with tf.name_scope('downsample'):
        if self.downsample is True:
          shortcut = tf.nn.conv2d(out,
                                  self.conv1_weights,
                                  strides=[1, self.stride_num, self.stride_num, 1],
                                  padding='SAME')
        #  out = self.conv1(out)
      with tf.name_scope('conv1'):
        out = tf.nn.conv2d(out,
                          self.conv2_weights,
                          strides=[1, self.stride_num, self.stride_num, 1],
                          padding='SAME')
      #  out = self.droupout(out)
      #  out = self.norm2(out)
      with tf.name_scope('BN2'):
        out = GroupNorm(x=out, G=32)

      #  out = self.relu(out) 
      with tf.name_scope('relu2'):
        out = tf.nn.relu(out)   
      #  out = self.conv2(out)
      with tf.name_scope('conv2'):
        out = tf.nn.conv2d(out,
                          self.conv3_weights,
                          strides=[1, 1, 1, 1],
                          padding='SAME')
    return shortcut+out

class BRN(object):

  def __init__(self):
    self.ResNet_0_0 = ResBlock(2, True)
    self.ResNet_0_1 = ResBlock(2, True)
    self.ResNet_1_0 = ResBlock(2, True)
    self.ResNet_1_1 = ResBlock(2, True)
    self.ResNet_0 = ResBlock(1, False)
    self.ResNet_1 = ResBlock(1, False)
    self.ResNet_2 = ResBlock(1, False)
    self.ResNet_3 = ResBlock(1, False)
    self.ResNet_4 = ResBlock(1, False)
    self.ResNet_5 = ResBlock(1, False)
    self.ResNet_6 = ResBlock(1, False)
    self.ResNet_7 = ResBlock(1, False)
    self.ResNet_8 = ResBlock(1, False)
    self.ResNet_9 = ResBlock(1, False)
    self.ResNet_10 = ResBlock(1, False)
    self.ResNet_11 = ResBlock(1, False)
    self.ResNet_12 = ResBlock(1, False)
    self.ResNet_13 = ResBlock(1, False)
    self.ResNet_14 = ResBlock(1, False)
    self.ResNet_15 = ResBlock(1, False)
    self.ResNet_16 = ResBlock(1, False)
    self.ResNet_17 = ResBlock(1, False)
    self.ResNet_18 = ResBlock(1, False)
    self.ResNet_19 = ResBlock(1, False)
    self.ResNet_20 = ResBlock(1, False)
    self.ResNet_21 = ResBlock(1, False)
    self.conv1_weights = tf.Variable(
      tf.truncated_normal([3, 3, NUM_CHANNELS, 64],  
                          stddev=0.1,
                          seed=SEED, dtype=data_type()))  
    self.conv2_weights = tf.Variable(
      tf.truncated_normal([3, 3, NUM_CHANNELS, 64],  
                          stddev=0.1,
                          seed=SEED, dtype=data_type()))  
    self.fc_weights = tf.Variable(tf.truncated_normal([64, NUM_LABELS],
                                                stddev=0.1,
                                                seed=SEED,
                                                dtype=data_type()))

    self.fc_biases = tf.Variable(tf.constant(0.1, shape=[NUM_LABELS], dtype=data_type()))

  def forward(self, stft, mfcc):

    with tf.name_scope('BRNcon1'):
      out_s = tf.nn.conv2d(stft,
                        self.conv1_weights,
                        strides= [1, 1, 1, 1],
                        padding='VALID')
    with tf.name_scope('resnetblocks_1'):
      out_s = self.ResNet_0_0.forward(out_s)
    with tf.name_scope('resnetblocks_2'):
      out_s = self.ResNet_0_1.forward(out_s)
    with tf.name_scope('resnetblocks_3'):
      out_s = self.ResNet_0.forward(out_s)
    with tf.name_scope('resnetblocks_4'):
      out_s = self.ResNet_2.forward(out_s)
    with tf.name_scope('resnetblocks_5'):
      out_s = self.ResNet_4.forward(out_s)
    with tf.name_scope('resnetblocks_6'):
      out_s = self.ResNet_6.forward(out_s)
    with tf.name_scope('resnetblocks_7'):
      out_s = self.ResNet_8.forward(out_s)
    with tf.name_scope('resnetblocks_8'):
      out_s = self.ResNet_10.forward(out_s)
    with tf.name_scope('resnetblocks_9'):
      out_s = self.ResNet_12.forward(out_s)
    with tf.name_scope('resnetblocks_10'):
      out_s = self.ResNet_14.forward(out_s)
    with tf.name_scope('resnetblocks_11'):
      out_s = self.ResNet_16.forward(out_s)
    with tf.name_scope('resnetblocks_12'):
      out_s = self.ResNet_18.forward(out_s)
    with tf.name_scope('resnetblocks_13'):
      out_s = self.ResNet_20.forward(out_s)
    with tf.name_scope('brns_bn1'):
      out_s = GroupNorm(x=out_s, G=32)

    with tf.name_scope('brn_relu_1'):
      out_s = tf.nn.relu(out_s)
    with tf.name_scope('brn_pool1'):
      out_s = tf.nn.avg_pool(out_s,
                            ksize=[1,out_s.shape[2],out_s.shape[2],1],
                            strides=[1, 1, 1, 1],
                            padding='VALID')

    with tf.name_scope('BRNcon2'):
      out_m = tf.nn.conv2d(mfcc,
                        self.conv2_weights,
                        strides= [1, 1, 1, 1],
                        padding='VALID')
    with tf.name_scope('resnetblockm_1'):
      out_m = self.ResNet_1_0.forward(out_m)
    with tf.name_scope('resnetblockm_2'):
      out_m = self.ResNet_1_1.forward(out_m)
    with tf.name_scope('resnetblockm_3'):
      out_m = self.ResNet_1.forward(out_m)
    with tf.name_scope('resnetblockm_4'):
      out_m = self.ResNet_3.forward(out_m)  
    with tf.name_scope('resnetblockm_5'):
      out_m = self.ResNet_5.forward(out_m)
    with tf.name_scope('resnetblockm_6'):
      out_m = self.ResNet_7.forward(out_m)
    with tf.name_scope('resnetblockm_7'):
      out_m = self.ResNet_9.forward(out_m)
    with tf.name_scope('resnetblockm_8'):
      out_m = self.ResNet_11.forward(out_m)
    with tf.name_scope('resnetblockm_9'):
      out_m = self.ResNet_13.forward(out_m)
    with tf.name_scope('resnetblockm_10'):
      out_m = self.ResNet_15.forward(out_m)
    with tf.name_scope('resnetblockm_11'):
      out_m = self.ResNet_17.forward(out_m)
    with tf.name_scope('resnetblockm_12'):
      out_m = self.ResNet_19.forward(out_m)
    with tf.name_scope('resnetblockm_13'):
      out_m = self.ResNet_21.forward(out_m)
    with tf.name_scope('brnm_bn1'):
      out_m = GroupNorm(x=out_m, G=32)

    with tf.name_scope('brn_relu_2'):
      out_m = tf.nn.relu(out_m)
    with tf.name_scope('brn_pool2'):
      out_m = tf.nn.avg_pool(out_m,
                            ksize=[1,out_m.shape[2],out_m.shape[2],1],
                            strides=[1, 1, 1, 1],
                            padding='VALID')
    with tf.name_scope('maumul'):
    
      out = tf.multiply(out_s,out_m)
    with tf.name_scope('fc'):

      out_shape = out.get_shape().as_list()
      reshape = tf.reshape(
          out,
          [out_shape[0], out_shape[1] * out_shape[2] * out_shape[3]])    
      out = tf.add(tf.matmul(reshape, self.fc_weights), self.fc_biases, name=""logits_"")

    return out

def main(_):

  def loss_function(weight, logits, labels):
    labels = tf.one_hot(labels,4)
    labels = tf.cast(labels, tf.float32)
    first = tf.reduce_sum(tf.multiply(-labels, logits),1)
    second_0 = tf.add(tf.exp(logits[:,0]),tf.exp(logits[:,1]))
    second_1 = tf.add(tf.exp(logits[:,2]),tf.exp(logits[:,3]))
    log = tf.log(tf.add(second_1,second_0))
    weight = tf.transpose(tf.reduce_sum(tf.multiply(labels, weight),1))
    output = tf.multiply(weight,tf.add(first,log))

    return output

  def normalize(stft):
    stft_1 = numpy.empty([stft.shape[0],128,128])
    stft_2 = numpy.empty([stft_1.shape[0],stft_1.shape[1],stft_1.shape[2],1])
    for i in range(stft_1.shape[0]):
      image = Image.fromarray(stft[i,:,:])
      image = image.resize([128,128])
      stft_1[i,:,:] = numpy.array(image)

      min = numpy.min(stft_1[i,:,:])
      max = numpy.max(stft_1[i,:,:])
      stft_1[i,:,:] = (stft_1[i,:,:]-min)/(max-min)
      stft_2[i,:,:,:] = stft_1[i,:,:].reshape((stft_1.shape[1],stft_1.shape[2],1))
    return stft_2  

  if FLAGS.self_test:
    
    train_data, train_labels = fake_data(256)
    validation_data, validation_labels = fake_data(EVAL_BATCH_SIZE)
    test_data, test_labels = fake_data(EVAL_BATCH_SIZE)
    num_epochs = 1
  else:
    # Get the data.
    
    stft_training, mfcc_training, labels_training = joblib.load(open(FLAGS.input, mode='rb'))
    stft_test, mfcc_test, labels_test = joblib.load(open(FLAGS.test, mode='rb'))

    stft_test = numpy.array(stft_test)
    mfcc_test = numpy.array(mfcc_test)
    labels_test = numpy.array(labels_test)
    stft_test = normalize(stft_test)
    mfcc_test = normalize(mfcc_test)

    stft_training = numpy.array(stft_training)
    mfcc_training = numpy.array(mfcc_training)
    labels_training = numpy.array(labels_training)
    stft_training = normalize(stft_training)
    mfcc_training = normalize(mfcc_training)

    stft_shape = stft_training.shape
    stft_shape = (None, stft_shape[1], stft_shape[2], 1)

    mfcc_shape = mfcc_training.shape
    mfcc_shape = (None, mfcc_shape[1], mfcc_shape[2], 1)

    labels_shape = labels_training.shape
    labels_shape = (None)

    stft_placeholder = tf.placeholder(stft_training.dtype, stft_shape)
    labels_placeholder = tf.placeholder(labels_training.dtype, labels_shape)
    mfcc_placeholder = tf.placeholder(mfcc_training.dtype, mfcc_shape)
    
    dataset_training = tf.data.Dataset.from_tensor_slices((stft_placeholder, mfcc_placeholder, labels_placeholder))
    dataset_training  = dataset_training.apply(
        tf.data.experimental.shuffle_and_repeat(len(stft_training), None))  
    dataset_training  = dataset_training.batch(BATCH_SIZE)
    dataset_training  = dataset_training.prefetch(1)
    iterator_training = dataset_training.make_initializable_iterator()
    next_element_training = iterator_training.get_next()
    num_epochs = FLAGS.epochs

  train_size = labels_training.shape[0]


  stft_holder = tf.placeholder(
        name=""stft_holder"",
        dtype=data_type(),
        shape=(None, IMAGE_HEIGHT, IMAGE_WEITH, NUM_CHANNELS))
  mfcc_holder = tf.placeholder(
        name=""mfcc_holder"",
        dtype=data_type(),
        shape=(None, IMAGE_HEIGHT, IMAGE_WEITH, NUM_CHANNELS))
  labels = tf.placeholder(tf.int64, shape=(None,))

  with tf.name_scope('test_input'):
    stft_t = tf.placeholder(
        data_type(),
        shape=(EVAL_BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WEITH, NUM_CHANNELS))
    mfcc_t = tf.placeholder(
        data_type(),
        shape=(EVAL_BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WEITH, NUM_CHANNELS))

  model = BRN()
  logits = model.forward(stft_holder, mfcc_holder)
  out1 = tf.identity(logits, name=""out"")
  try:
    scalar_summary = tf.scalar_summary
    SummaryWrite = tf.train.SummaryWrite
    merge_summary = tf.merge_summary
  except:
    scalar_summary = tf.summary.scalar
    SummaryWrite = tf.summary.FileWriter
    merge_summary = tf.summary.merge
  with tf.name_scope('loss'):
    weights = [1.0, 1.7, 4.1, 5.7]
    mid = loss_function(weights, logits=logits, labels=labels)
#    mid = tf.nn.sparse_softmax_cross_entropy_with_logits(
#       labels=labels, logits=logits)

    loss = tf.reduce_sum(mid)
    
    loss_summary = scalar_summary('loss', loss)

    
    # L2 regularization for the fully connected parameters.
    regularizers = (tf.nn.l2_loss(model.conv1_weights) + tf.nn.l2_loss(model.conv2_weights) +
                    tf.nn.l2_loss(model.fc_weights) + tf.nn.l2_loss(model.fc_biases))
    # Add the regularization term to the loss.
    loss += 0.02 * regularizers

    batch = tf.Variable(0, dtype=data_type())
  # Use simple momentum for the optimization.
  with tf.name_scope('train'):

    optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)

  # Predictions for the current training minibatch.
  train_prediction = tf.nn.softmax(logits)
  eval_prediction = tf.nn.softmax(model.forward(stft_t, mfcc_t))

  # Create a local session to run the training.
  start_time = time.time()

  def eval_in_batches(stft_data, mfcc_data, sess, type):
    """"""Get all predictions for a dataset by running it in small batches.""""""
    size = stft_data.shape[0]
    if size < EVAL_BATCH_SIZE:
      raise ValueError(""batch size for evals larger than dataset: %d"" % size)
    predictions = numpy.ndarray(shape=(size, NUM_LABELS), dtype=numpy.float32)
    for begin in xrange(0, size, EVAL_BATCH_SIZE):
      end = begin + EVAL_BATCH_SIZE
      if end <= size:
        if type == 'train':
          predictions[begin:end, :] = sess.run(
              train_prediction,
              feed_dict={stft_holder: stft_data[begin:end, ...], mfcc_holder: mfcc_data[begin:end, ...]})
        else: 
          predictions[begin:end, :] = sess.run(
              eval_prediction,
              feed_dict={stft_t: stft_data[begin:end, ...], mfcc_t: mfcc_data[begin:end, ...]})
      else:
        if type == 'train':
          batch_predictions = sess.run(
              train_prediction,
              feed_dict={stft_holder: stft_data[-EVAL_BATCH_SIZE:, ...], mfcc_holder: mfcc_data[-EVAL_BATCH_SIZE:, ...]})
        else:
           batch_predictions = sess.run(
              eval_prediction,
              feed_dict={stft_t: stft_data[-EVAL_BATCH_SIZE:, ...], mfcc_t: mfcc_data[-EVAL_BATCH_SIZE:, ...]})
        predictions[begin:, :] = batch_predictions[begin - size:, :]
    return predictions

  saver = tf.train.Saver()
  config = tf.ConfigProto()
  config.gpu_options.allow_growth = True  

  with tf.Session(config=config) as sess:
    # Run all the initializers to prepare the trainable parameters.
    tf.global_variables_initializer().run()

    merged = tf.summary.merge_all()
    writer = SummaryWrite(FLAGS.logs + 'train', sess.graph)
    print('Initialized!')
    sess.run(iterator_training.initializer, feed_dict={stft_placeholder:stft_training,
                      mfcc_placeholder:mfcc_training,
                      labels_placeholder:labels_training})

    # Loop through training steps.
    for step in xrange(int(num_epochs * train_size) // BATCH_SIZE):

      batch_stft, batch_mfcc, batch_labels = sess.run(next_element_training)
  
      feed_dict = {stft_holder: batch_stft,
                   mfcc_holder: batch_mfcc,
                   labels: batch_labels}
      # Run the optimizer to update weights.

      sess.run(optimizer, feed_dict=feed_dict)
      # print some extra information once reach the evaluation frequency
      if step % EVAL_FREQUENCY == 0:
        # fetch some extra nodes' data
        summary, l = sess.run([merged, loss],
                                      feed_dict=feed_dict)
        writer.add_summary(summary, step)
        elapsed_time = time.time() - start_time
        start_time = time.time()
        rate, acc = error_rate(eval_in_batches(stft_training, mfcc_training, sess, 'train'), labels_training)
        acc_summary = scalar_summary('accuracy', acc)
        print('Step %d (epoch %.2f), Minibatch loss: %.3f, Minibatch error: %.1f%%, Accuracy:%.4f' %
              (step, float(step) * BATCH_SIZE / train_size,
              l,rate, acc))

        
    # Finally print the result!
        sys.stdout.flush()
        test_error, test_acc = error_rate(eval_in_batches(stft_test, mfcc_test, sess, 'test'), labels_test)
        print('Testset error: %.1f%%, Accuracy:%.4f' % (test_error, test_acc))

    saver.save(sess, './local_ckpt')        
    writer.close()



if __name__ == '__main__':
#  dev = '/gpu:0'
  parser = argparse.ArgumentParser()
  parser.add_argument(
      '--use_fp16',
      default=False,
      help='Use half floats instead of full floats if True.',
      action='store_true')
  parser.add_argument(
      '--self_test',
      default=False,
      action='store_true',
      help='True if running a self test.')
  parser.add_argument(
      '--input',
      default='wavelet_stft.p')
  parser.add_argument(
      '--test',
      default='wavelet_stft_test.p')  
  parser.add_argument(
      '--epochs',
      type=int,
      default=1)  
  parser.add_argument(
      '--logs',
      default='')  
  FLAGS, unparsed = parser.parse_known_args()
  tf.enable_resource_variables()
  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
```

and here is recover code:
```
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import gzip
import os
import sys
import time
import joblib
import math
import numpy
from six.moves import urllib
from six.moves import xrange  
from PIL import Image
from sklearn.metrics import confusion_matrix as sk_confusion_matrix
from sklearn.metrics import classification_report
import tensorflow as tf

os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""
FLAGS = None
IMAGE_HEIGHT = 128
IMAGE_WEITH = 128
NUM_CHANNELS = 1
NUM_LABELS = 4
SEED = 66478  # Set to None for random seed.
BATCH_SIZE = 2
EVAL_BATCH_SIZE = 32
EVAL_FREQUENCY = 10  # Number of steps between evaluations.

tf.reset_default_graph()

def data_type():
  """"""Return the type of the activations, weights, and placeholder variables.""""""
  if FLAGS.use_fp16:
    return tf.float16
  else:
    return tf.float32


def fake_data(num_images):
  """"""Generate a fake dataset that matches the dimensions of MNIST.""""""
  data = numpy.ndarray(
      shape=(num_images, IMAGE_HEIGHT, IMAGE_WEITH, NUM_CHANNELS),
      dtype=numpy.float32)
  labels = numpy.zeros(shape=(num_images,), dtype=numpy.int64)
  for image in xrange(num_images):
    label = image % 2
    data[image, :, :, 0] = label - 0.5
    labels[image] = label
  return data, labels


def error_rate(predictions, labels):
  Confusion_matrix=sk_confusion_matrix(numpy.argmax(predictions, 1).tolist(), labels.tolist())
  print('Confusion_matrix:')
  print(Confusion_matrix)  

  Se1 = Confusion_matrix[1,1]+Confusion_matrix[2,2]+Confusion_matrix[3,3]
  Se2 = Confusion_matrix[1,1]+Confusion_matrix[1,0]+Confusion_matrix[1,2]+Confusion_matrix[1,3]+Confusion_matrix[2,2]+Confusion_matrix[2,0]+Confusion_matrix[2,1]+Confusion_matrix[2,3]+Confusion_matrix[3,3]+Confusion_matrix[3,0]+Confusion_matrix[3,1]+Confusion_matrix[3,2]
  Se = Se1/Se2
  Sp = Confusion_matrix[0,0]/(Confusion_matrix[0,0]+Confusion_matrix[0,1]+Confusion_matrix[0,2]+Confusion_matrix[0,3]) 
  Acc = (Se+Sp)*100/2

  target_names = ['class 0', 'class 1', 'class 2', 'class 3']

  print()
  accuracy = 100.0-(100.0 *numpy.sum(numpy.argmax(predictions, 1) == labels)/predictions.shape[0])
  
  """"""Return the error rate based on dense predictions and sparse labels.""""""
  return 100.0 - (
      100.0 *
      numpy.sum(numpy.argmax(predictions, 1) == labels) /
      predictions.shape[0]), Acc

def GroupNorm(x, G, eps=1e-05):
    # x: input features with shape [N,H,W,C]
    # gamma, beta: scale and offset, with shape [1,C,1,1]
    # G: number of groups for GN
  N, H, W, C = x.shape
  gamma = tf.ones([1, 1, 1, C])
  beta = tf.zeros([1, 1, 1, C])
  x = tf.reshape(x, [N, G, H, W, C // G])
  mean, var = tf.nn.moments(x, [2, 3, 4], keep_dims=True)
  x = (x-1) / tf.sqrt(1 + eps)
  x = tf.reshape(x, [N, H, W, C])
  return x * gamma + beta

class ResBlock(object):

  def __init__(self, stride_num=1, downsample=False):
    self.conv1_weights = tf.Variable(
      tf.truncated_normal([1, 1, 64, 64],  # 1x1 filter, depth 64.
                          stddev=0.1,
                          seed=SEED, dtype=data_type()))  

    self.conv2_weights = tf.Variable(
      tf.truncated_normal([3, 3, 64, 64],  # 3x3 filter, depth 64.
                          stddev=0.1,
                          seed=SEED, dtype=data_type()))
    self.conv3_weights = tf.Variable(
      tf.truncated_normal([3, 3, 64, 64],  # 3x3 filter, depth 64.
                          stddev=0.1,
                          seed=SEED, dtype=data_type()))  
    self.stride_num = stride_num
    self.downsample = downsample

  def forward(self, data):
    with tf.name_scope('ResNet'):
    # shortcut = x
      shortcut = data
      # out = self.relu(self.norm1(x))
#      axis = list(range(len(data.get_shape()) - 1))
      with tf.name_scope('BN1'):
        out = GroupNorm(x=data, G=32)
        #mean, variance = tf.nn.moments(data, axis)
        #out = tf.nn.batch_normalization(data, mean, variance, 0, 1, 0.001)
      with tf.name_scope('relu1'):
        out = tf.nn.relu(out)
      #  if self.downsample is not None:
        #   shortcut = self.downsample(out)
      with tf.name_scope('downsample'):
        if self.downsample is True:
          shortcut = tf.nn.conv2d(out,
                                  self.conv1_weights,
                                  strides=[1, self.stride_num, self.stride_num, 1],
                                  padding='SAME')
        #  out = self.conv1(out)
      with tf.name_scope('conv1'):
        out = tf.nn.conv2d(out,
                          self.conv2_weights,
                          strides=[1, self.stride_num, self.stride_num, 1],
                          padding='SAME')
      #  out = self.droupout(out)
      #  out = self.norm2(out)
      with tf.name_scope('BN2'):
        out = GroupNorm(x=out, G=32)

      with tf.name_scope('relu2'):
        out = tf.nn.relu(out)   
      #  out = self.conv2(out)
      with tf.name_scope('conv2'):
        out = tf.nn.conv2d(out,
                          self.conv3_weights,
                          strides=[1, 1, 1, 1],
                          padding='SAME')
    return shortcut+out

class BRN(object):

  def __init__(self):
    self.ResNet_0_0 = ResBlock(2, True)
    self.ResNet_0_1 = ResBlock(2, True)
    self.ResNet_1_0 = ResBlock(2, True)
    self.ResNet_1_1 = ResBlock(2, True)
    self.ResNet_0 = ResBlock(1, False)
    self.ResNet_1 = ResBlock(1, False)
    self.ResNet_2 = ResBlock(1, False)
    self.ResNet_3 = ResBlock(1, False)
    self.ResNet_4 = ResBlock(1, False)
    self.ResNet_5 = ResBlock(1, False)
    self.ResNet_6 = ResBlock(1, False)
    self.ResNet_7 = ResBlock(1, False)
    self.ResNet_8 = ResBlock(1, False)
    self.ResNet_9 = ResBlock(1, False)
    self.ResNet_10 = ResBlock(1, False)
    self.ResNet_11 = ResBlock(1, False)
    self.ResNet_12 = ResBlock(1, False)
    self.ResNet_13 = ResBlock(1, False)
    self.ResNet_14 = ResBlock(1, False)
    self.ResNet_15 = ResBlock(1, False)
    self.ResNet_16 = ResBlock(1, False)
    self.ResNet_17 = ResBlock(1, False)
    self.ResNet_18 = ResBlock(1, False)
    self.ResNet_19 = ResBlock(1, False)
    self.ResNet_20 = ResBlock(1, False)
    self.ResNet_21 = ResBlock(1, False)
    self.conv1_weights = tf.Variable(
      tf.truncated_normal([3, 3, NUM_CHANNELS, 64],  
                          stddev=0.1,
                          seed=SEED, dtype=data_type()))  
    self.conv2_weights = tf.Variable(
      tf.truncated_normal([3, 3, NUM_CHANNELS, 64],  
                          stddev=0.1,
                          seed=SEED, dtype=data_type()))  
    self.fc_weights = tf.Variable(tf.truncated_normal([64, NUM_LABELS],
                                                stddev=0.1,
                                                seed=SEED,
                                                dtype=data_type()))

    self.fc_biases = tf.Variable(tf.constant(0.1, shape=[NUM_LABELS], dtype=data_type()))

  def forward(self, stft, mfcc):

    with tf.name_scope('BRNcon1'):
      out_s = tf.nn.conv2d(stft,
                        self.conv1_weights,
                        strides= [1, 1, 1, 1],
                        padding='VALID')
    with tf.name_scope('resnetblocks_1'):
      out_s = self.ResNet_0_0.forward(out_s)
    with tf.name_scope('resnetblocks_2'):
      out_s = self.ResNet_0_1.forward(out_s)
    with tf.name_scope('resnetblocks_3'):
      out_s = self.ResNet_0.forward(out_s)
    with tf.name_scope('resnetblocks_4'):
      out_s = self.ResNet_2.forward(out_s)
    with tf.name_scope('resnetblocks_5'):
      out_s = self.ResNet_4.forward(out_s)
    with tf.name_scope('resnetblocks_6'):
      out_s = self.ResNet_6.forward(out_s)
    with tf.name_scope('resnetblocks_7'):
      out_s = self.ResNet_8.forward(out_s)
    with tf.name_scope('resnetblocks_8'):
      out_s = self.ResNet_10.forward(out_s)
    with tf.name_scope('resnetblocks_9'):
      out_s = self.ResNet_12.forward(out_s)
    with tf.name_scope('resnetblocks_10'):
      out_s = self.ResNet_14.forward(out_s)
    with tf.name_scope('resnetblocks_11'):
      out_s = self.ResNet_16.forward(out_s)
    with tf.name_scope('resnetblocks_12'):
      out_s = self.ResNet_18.forward(out_s)
    with tf.name_scope('resnetblocks_13'):
      out_s = self.ResNet_20.forward(out_s)
    with tf.name_scope('brns_bn1'):
      out_s = GroupNorm(x=out_s, G=32)

    with tf.name_scope('brn_relu_1'):
      out_s = tf.nn.relu(out_s)
    with tf.name_scope('brn_pool1'):
      out_s = tf.nn.avg_pool(out_s,
                            ksize=[1,out_s.shape[2],out_s.shape[2],1],
                            strides=[1, 1, 1, 1],
                            padding='VALID')

    with tf.name_scope('BRNcon2'):
      out_m = tf.nn.conv2d(mfcc,
                        self.conv2_weights,
                        strides= [1, 1, 1, 1],
                        padding='VALID')
    with tf.name_scope('resnetblockm_1'):
      out_m = self.ResNet_1_0.forward(out_m)
    with tf.name_scope('resnetblockm_2'):
      out_m = self.ResNet_1_1.forward(out_m)
    with tf.name_scope('resnetblockm_3'):
      out_m = self.ResNet_1.forward(out_m)
    with tf.name_scope('resnetblockm_4'):
      out_m = self.ResNet_3.forward(out_m)  
    with tf.name_scope('resnetblockm_5'):
      out_m = self.ResNet_5.forward(out_m)
    with tf.name_scope('resnetblockm_6'):
      out_m = self.ResNet_7.forward(out_m)
    with tf.name_scope('resnetblockm_7'):
      out_m = self.ResNet_9.forward(out_m)
    with tf.name_scope('resnetblockm_8'):
      out_m = self.ResNet_11.forward(out_m)
    with tf.name_scope('resnetblockm_9'):
      out_m = self.ResNet_13.forward(out_m)
    with tf.name_scope('resnetblockm_10'):
      out_m = self.ResNet_15.forward(out_m)
    with tf.name_scope('resnetblockm_11'):
      out_m = self.ResNet_17.forward(out_m)
    with tf.name_scope('resnetblockm_12'):
      out_m = self.ResNet_19.forward(out_m)
    with tf.name_scope('resnetblockm_13'):
      out_m = self.ResNet_21.forward(out_m)
    with tf.name_scope('brnm_bn1'):
      out_m = GroupNorm(x=out_m, G=32)

    with tf.name_scope('brn_relu_2'):
      out_m = tf.nn.relu(out_m)
    with tf.name_scope('brn_pool2'):
      out_m = tf.nn.avg_pool(out_m,
                            ksize=[1,out_m.shape[2],out_m.shape[2],1],
                            strides=[1, 1, 1, 1],
                            padding='VALID')
    with tf.name_scope('maumul'):
    
      out = tf.multiply(out_s,out_m)
    with tf.name_scope('fc'):

      out_shape = out.get_shape().as_list()
      reshape = tf.reshape(
          out,
          [out_shape[0], out_shape[1] * out_shape[2] * out_shape[3]])    
      out = tf.add(tf.matmul(reshape, self.fc_weights), self.fc_biases)

    return out

def main(_):


  def normalize(stft):
    stft_1 = numpy.empty([stft.shape[0],128,128])
    stft_2 = numpy.empty([stft_1.shape[0],stft_1.shape[1],stft_1.shape[2],1])
    for i in range(stft_1.shape[0]):
      image = Image.fromarray(stft[i,:,:])
      image = image.resize([128,128])
      stft_1[i,:,:] = numpy.array(image)

      min = numpy.min(stft_1[i,:,:])
      max = numpy.max(stft_1[i,:,:])
      stft_1[i,:,:] = (stft_1[i,:,:]-min)/(max-min)
      stft_2[i,:,:,:] = stft_1[i,:,:].reshape((stft_1.shape[1],stft_1.shape[2],1))
    return stft_2  

  if FLAGS.self_test:
    
    train_data, train_labels = fake_data(256)
    validation_data, validation_labels = fake_data(EVAL_BATCH_SIZE)
    test_data, test_labels = fake_data(EVAL_BATCH_SIZE)
    num_epochs = 1
  else:
    # Get the data.
    
    stft_training, mfcc_training, labels_training = joblib.load(open(FLAGS.test, mode='rb'))
    stft_test, mfcc_test, labels_test = joblib.load(open(FLAGS.test, mode='rb'))

    stft_test = numpy.array(stft_test)
    mfcc_test = numpy.array(mfcc_test)
    labels_test = numpy.array(labels_test)
    stft_test = normalize(stft_test)
    mfcc_test = normalize(mfcc_test)

    stft_training = numpy.array(stft_training)
    mfcc_training = numpy.array(mfcc_training)
    labels_training = numpy.array(labels_training)
    stft_training = normalize(stft_training)
    mfcc_training = normalize(mfcc_training)

    stft_shape = stft_training.shape
    stft_shape = (None, stft_shape[1], stft_shape[2], 1)

    mfcc_shape = mfcc_training.shape
    mfcc_shape = (None, mfcc_shape[1], mfcc_shape[2], 1)

    labels_shape = labels_training.shape
    labels_shape = (None)

    stft_placeholder = tf.placeholder(stft_training.dtype, stft_shape)
    labels_placeholder = tf.placeholder(labels_training.dtype, labels_shape)
    mfcc_placeholder = tf.placeholder(mfcc_training.dtype, mfcc_shape)
    
    dataset_training = tf.data.Dataset.from_tensor_slices((stft_placeholder, mfcc_placeholder, labels_placeholder))
    dataset_training  = dataset_training.apply(
        tf.data.experimental.shuffle_and_repeat(len(stft_training), None))  
    dataset_training  = dataset_training.batch(BATCH_SIZE)
    dataset_training  = dataset_training.prefetch(1)
    iterator_training = dataset_training.make_initializable_iterator()
    next_element_training = iterator_training.get_next()
    num_epochs = FLAGS.epochs

  train_size = labels_training.shape[0]


  stft_holder = tf.placeholder(
          name=""stft_holder"",
          dtype=data_type(),
          shape=(BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WEITH, NUM_CHANNELS))
  mfcc_holder = tf.placeholder(
          name=""mfcc_holder"",
          dtype=data_type(),
          shape=(BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WEITH, NUM_CHANNELS))
  labels = tf.placeholder(tf.int64, shape=(BATCH_SIZE,))

  with tf.name_scope('test_input'):
    stft_t = tf.placeholder(
        data_type(),
        shape=(EVAL_BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WEITH, NUM_CHANNELS))
    mfcc_t = tf.placeholder(
        data_type(),
        shape=(EVAL_BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WEITH, NUM_CHANNELS))

  model = BRN()
  logits = model.forward(stft_holder, mfcc_holder)
  out1 = tf.identity(logits, name=""out"")

  saver = tf.train.Saver()


  config = tf.ConfigProto()
  config.gpu_options.allow_growth = True  

  with tf.Session(config=config) as sess:
    # Run all the initializers to prepare the trainable parameters.

    saver.restore(sess, ""./local_ckpt"")

    stft_shape = (2,128,128,1)
    batch_stft = numpy.array(numpy.random.random_sample(stft_shape), dtype=numpy.float32)
    batch_mfcc = numpy.array(numpy.random.random_sample(stft_shape), dtype=numpy.float32)

    feed_dict = {stft_holder: batch_stft,
                mfcc_holder: batch_mfcc}
      # Run the optimizer to update weights.

    results = sess.run(out1, feed_dict=feed_dict)
    print(results)



if __name__ == '__main__':
#  dev = '/gpu:0'
  parser = argparse.ArgumentParser()
  parser.add_argument(
      '--use_fp16',
      default=False,
      help='Use half floats instead of full floats if True.',
      action='store_true')
  parser.add_argument(
      '--self_test',
      default=False,
      action='store_true',
      help='True if running a self test.')
  parser.add_argument(
      '--input',
      default='wavelet_stft.p')
  parser.add_argument(
      '--test',
      default='wavelet_stft_test.p')  
  parser.add_argument(
      '--epochs',
      type = int,
      default=1)  
  parser.add_argument(
      '--logs',
      default='')  
  FLAGS, unparsed = parser.parse_known_args()
  tf.enable_resource_variables()
  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
```
and I get answer like this:
```
[[0.10859548 0.09478829 0.1094515  0.08498076]
 [0.10859548 0.09478829 0.1094515  0.08498076]]
```
but they should be different
Thanks for your reading anaway"
32859,Warming tensorflow: Entity could not be transformed and will be executed as-is.,"**System information**
- Custom layers of tf.layer.Layers 
- Linux Ubuntu 16.04)
- TensorFlow installed from: Conda
- TensorFlow version: 1.14
- Python version: 3.7
- CUDA/cuDNN version: 10.0.130/7.6.0
- GPU model and memory: Nvidia TitanX 1080Ti, 12GB

**a Snippet of Error message**

> ERROR:tensorflow:Error converting <bound method EmbeddingSharedWeights.call of <model.EmbeddingSharedWeights object at 0x7fede45b6828>>
> Traceback (most recent call last):
>   File ""/home/username/data3/conda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py"", line 524, in to_graph
>     return conversion.convert(entity, program_ctx)
>   File ""/home/username/data3/conda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 306, in convert
>     entity, program_ctx, free_nonglobal_var_names)
>   File ""/home/username/data3/conda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 229, in _convert_with_cache
>     entity, program_ctx)
>   File ""/home/username/data3/conda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 433, in convert_entity_to_ast
>     nodes, name, entity_info = convert_func_to_ast(o, program_ctx)
>   File ""/home/username/data3/conda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 624, in convert_func_to_ast
>     node = node_to_graph(node, context)
>   File ""/home/username/data3/conda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 667, in node_to_graph
>     node = converter.apply_(node, context, return_statements)
>   File ""/home/username/data3/conda3/lib/python3.7/site-packages/tensorflow/python/autograph/core/converter.py"", line 380, in apply_
>     node = converter_module.transform(node, context)
>   File ""/home/username/data3/conda3/lib/python3.7/site-packages/tensorflow/python/autograph/converters/return_statements.py"", line 412, in transform
>     node = transformer.visit(node)
>   File ""/home/username/data3/conda3/lib/python3.7/site-packages/tensorflow/python/autograph/core/converter.py"", line 317, in visit
>     return super(Base, self).visit(node)
>   File ""/home/username/data3/conda3/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/transformer.py"", line 480, in visit
>     result = super(Base, self).visit(node)
>   File ""/home/username/data3/conda3/lib/python3.7/ast.py"", line 262, in visit
>     return visitor(node)
>   File ""/home/username/data3/conda3/lib/python3.7/site-packages/tensorflow/python/autograph/converters/return_statements.py"", line 363, in visit_FunctionDef
>     converted_body = self._visit_statement_block(node, node.body)
>   File ""/home/username/data3/conda3/lib/python3.7/site-packages/tensorflow/python/autograph/converters/return_statements.py"", line 287, in _visit_statement_block
>     nodes = self.visit_block(nodes, after_visit=self._postprocess_statement)
>   File ""/home/username/data3/conda3/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/transformer.py"", line 371, in visit_block
>     replacement = self.visit(node)
>   File ""/home/username/data3/conda3/lib/python3.7/site-packages/tensorflow/python/autograph/core/converter.py"", line 317, in visit
>     return super(Base, self).visit(node)
>   File ""/home/username/data3/conda3/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/transformer.py"", line 480, in visit
>     result = super(Base, self).visit(node)
>   File ""/home/username/data3/conda3/lib/python3.7/ast.py"", line 262, in visit
>     return visitor(node)
>   File ""/home/username/data3/conda3/lib/python3.7/site-packages/tensorflow/python/autograph/converters/return_statements.py"", line 237, in visit_Return
>     retval=retval)
>   File ""/home/username/data3/conda3/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/templates.py"", line 260, in replace
>     replacements[k] = _convert_to_ast(replacements[k])
>   File ""/home/username/data3/conda3/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/templates.py"", line 222, in _convert_to_ast
>     return gast.Name(id=n, ctx=None, annotation=None)
>   File ""/home/username/data3/conda3/lib/python3.7/site-packages/gast/gast.py"", line 19, in create_node
>     format(Name, nbparam, len(Fields))
> AssertionError: Bad argument number for Name: 3, expecting 4
> WARNING:tensorflow:Entity <bound method EmbeddingSharedWeights.call of <model.EmbeddingSharedWeights object at 0x7fede45b6828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method EmbeddingSharedWeights.call of <model.EmbeddingSharedWeights object at 0x7fede45b6828>>: AssertionError: Bad argument number for Name: 3, expecting 4

[error.log](https://github.com/tensorflow/tensorflow/files/3660952/error.log)
**Describe the expected behavior**
There should be no error messages.
**a Snippet of Code to reproduce the issue**
```python

import tensorflow as tf


class LayerNormalization(tf.layers.Layer):
    """"""Applies layer normalization.""""""

    def __init__(self, hidden_size):
        super().__init__()
        self.hidden_size = hidden_size

    def build(self, input_shape):
        with tf.variable_scope(""layer_norm""):
            self.scale = tf.get_variable(""layer_norm_scale"", [self.hidden_size], 
              initializer=tf.ones_initializer())
            self.bias = tf.get_variable(""layer_norm_bias"", [self.hidden_size], 
              initializer=tf.zeros_initializer())
        self.built = True

    def call(self, x, epsilon=1e-6):
        mean = tf.reduce_mean(x, axis=[-1], keepdims=True)
        variance = tf.reduce_mean(tf.square(x - mean), axis=[-1], keepdims=True)
        norm_x = (x - mean) * tf.rsqrt(variance + epsilon)
        return norm_x * self.scale + self.bias


if __name__ == ""__main__"":
    x = tf.random_uniform((23, 29))
    ln = LayerNormalization(29)

    y = ln(x)

    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True

    sess = tf.Session(config=config)
    sess.run(tf.global_variables_initializer())
    sess.run(y)

```
"
32858,when will tensor flow support python3.7 ?,"when will tenor flow support python 3.7 ? tried the installation with 2.0 , which also does not support python 3.7."
32857,TFLite Reshape not supporting resizeInputTensor with -1,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pixel 3, Pixel 2
- TensorFlow installed from (source or binary): Anaconda
- TensorFlow version (use command below): 1.14
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I am trying to deploy Deepspeech 2 with `tf.lite.experimental.nn.bidirectional_dynamic_rnn` on android. Inside the inference since the input audio is of dynamic length, I try to call `resizeInputTensor` in the c++ api on Android to match the length of the audio, which throws 
`tensorflow/contrib/lite/kernels/reshape.cc:58 num_input_elements != num_output_elements`
similar to #23600. In the inference code there is a line of `reshape` which is from [-1, 1, 38, 25] to   [-1, 1, 38*25], (note that it collapses the fourth dimension but doesn't do anything with the batch dimension. After reading the source code for `reshape.cc` it seems like even if the batch size is indicated as -1, during conversion the batch size is explicitly computed and stored in the model, which is the main cause of this bug.

**Describe the expected behavior**
`reshape` conversion recomputes the reshape size whenever `resizeInputTensor` is called. 
 
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
std::vector<int> sizes = {1, 240, 240, 3};

interpreter_->ResizeInputTensor(interpreter_->inputs()[0], sizes);

if (interpreter_->AllocateTensors() != kTfLiteOk) {
    LOG(INFO) << ""Failed to allocate tensors!"" << ""\n"";
    return false;
```
Given that interpreter_ contains resizeops with -1 specified.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached."
32855,Hash mismatch,"<em>Cannot install tensorflow on Raspberry Pi 0W v1.3 owing to hash mismatch. tag:bug_template</em>

**System information**

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian Buster
- TensorFlow installed from (source or binary): using pip
- TensorFlow version (use command below): n/a; not installed yet
- Python version: 3.7

**Describe the current behavior**
Hash mismatch for 
https://www.piwheels.org/simple/tensorflow/tensorflow-1.13.1-cp37-none-linux_armv6l.whl#sha256=25f4ff027beec1e568baf8e90a07bad59d354560533d6b37318b9efeb70beeb1:
**Describe the expected behavior**
Expected sha256 25f4ff027beec1e568baf8e90a07bad59d354560533d6b37318b9efeb70beeb1

**Code to reproduce the issue**
Got        3aad2c162168a62adae30e226bcddfef74e5db1ca98bec1383958fb580e67123

```
$ python3 -m pip install tensorflow
Looking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple
Collecting tensorflow
  Downloading https://www.piwheels.org/simple/tensorflow/tensorflow-1.13.1-cp37-none-linux_armv6l.whl (87.1MB)
    100% |████████████████████████████████| 87.1MB 644bytes/s 
THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.
    tensorflow from https://www.piwheels.org/simple/tensorflow/tensorflow-1.13.1-cp37-none-linux_armv6l.whl#sha256=25f4ff027beec1e568baf8e90a07bad59d354560533d6b37318b9efeb70beeb1:
        Expected sha256 25f4ff027beec1e568baf8e90a07bad59d354560533d6b37318b9efeb70beeb1
             Got        3aad2c162168a62adae30e226bcddfef74e5db1ca98bec1383958fb580e67123

$ 
```
Thanks for any guidance on resolving the issue.

Kind regards.
"
32851,"""/device:GPU:0"" or ""/device:XLA_GPU:0""  ? ","When XLA is enabled with  TF_XLA_FLAGS=--tf_xla_auto_jit=2, should I see ""/device:GPU:0"" or ""/device:XLA_GPU:0"" in  Tensorflow output ?   "
32849,TFlite conversion of tf.keras model fails,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): pip install
- TensorFlow version (use command below):v2.0.0-beta1-0-g8e423e3d56 2.0.0-beta1
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: No
- GPU model and memory: Nvidia titan-Xp

**Describe the current behavior**
```
import tensorflow as tf
model = tf.keras.models.load_model('keras_model.h5')
model.summary()
Model: ""model""
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            [(None, 1048576)]    0
__________________________________________________________________________________________________
embedding (Embedding)           (None, 1048576, 8)   2056        input_1[0][0]
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 2097, 128)    512128      embedding[0][0]
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 2097, 128)    512128      embedding[0][0]
__________________________________________________________________________________________________
multiply (Multiply)             (None, 2097, 128)    0           conv1d[0][0]
                                                                 conv1d_1[0][0]
__________________________________________________________________________________________________
global_max_pooling1d (GlobalMax (None, 128)          0           multiply[0][0]
__________________________________________________________________________________________________
dense (Dense)                   (None, 128)          16512       global_max_pooling1d[0][0]
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 1)            129         dense[0][0]
==================================================================================================
Total params: 1,042,953
Trainable params: 1,042,953
Non-trainable params: 0
__________________________________________________________________________________________________
converter = tf.lite.TFLiteConverter.from_keras_model(model)
```
The converter fails to convert the model
```
>>> converter.convert()
2019-09-26 14:39:27.048354: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
2019-09-26 14:39:27.048553: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session
2019-09-26 14:39:27.065544: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize
2019-09-26 14:39:27.066324: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.002ms.
2019-09-26 14:39:27.066655: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0ms.
Traceback (most recent call last):
  File ""/home/sridhar/PE_CSV/malenv3/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 427, in import_graph_def
    graph._c_graph, serialized, options)  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: Input 0 of node model/embedding/embedding_lookup was passed float from model/embedding/embedding_lookup/Read/ReadVariableOp/resource:0 incompatible with expected resource.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/sridhar/PE_CSV/malenv3/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 348, in convert
    self._funcs[0])
  File ""/home/sridhar/PE_CSV/malenv3/lib/python3.6/site-packages/tensorflow/python/framework/convert_to_constants.py"", line 252, in convert_variables_to_constants_v2
    new_output_names)
  File ""/home/sridhar/PE_CSV/malenv3/lib/python3.6/site-packages/tensorflow/python/eager/wrap_function.py"", line 607, in function_from_graph_def
    wrapped_import = wrap_function(_imports_graph_def, [])
  File ""/home/sridhar/PE_CSV/malenv3/lib/python3.6/site-packages/tensorflow/python/eager/wrap_function.py"", line 585, in wrap_function
    collections={}),
  File ""/home/sridhar/PE_CSV/malenv3/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py"", line 716, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/sridhar/PE_CSV/malenv3/lib/python3.6/site-packages/tensorflow/python/eager/wrap_function.py"", line 80, in __call__
    return self.call_with_variable_creator_scope(self._fn)(*args, **kwargs)
  File ""/home/sridhar/PE_CSV/malenv3/lib/python3.6/site-packages/tensorflow/python/eager/wrap_function.py"", line 86, in wrapped
    return fn(*args, **kwargs)
  File ""/home/sridhar/PE_CSV/malenv3/lib/python3.6/site-packages/tensorflow/python/eager/wrap_function.py"", line 605, in _imports_graph_def
    importer.import_graph_def(graph_def, name="""")
  File ""/home/sridhar/PE_CSV/malenv3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/home/sridhar/PE_CSV/malenv3/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 431, in import_graph_def
    raise ValueError(str(e))
ValueError: Input 0 of node model/embedding/embedding_lookup was passed float from model/embedding/embedding_lookup/Read/ReadVariableOp/resource:0 incompatible with expected resource.
```
I'm able to use the model on a python based inference engine. I'm trying to just compress the model to deploy it on smaller setup and consume via a c/c++ wrapper."
32845,tensorboard connection problem,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04**
- TensorFlow installed from (source or binary): pip (docker)
- TensorFlow version: 1.13.2
- Python version: 3
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version:10.1
- GPU model and memory: Quadro P3000

Good afternoon!

I am using the tensorflow image from [docker](https://hub.docker.com/r/tensorflow/tensorflow/tags?page=1&name=1.13) and am having trouble getting tensorboard to run in my container.  I run the following command to start the container:

`
sudo docker container run -p 8888:8888 -p 6006:6006 -it --rm --runtime=nvidia -v $(pwd):/opt/ -v $(pwd)/:/tf/ '$image-name
`
When I run my trial notebook, I get the following error image (see attached file).  I'm not sure what I'm doing wrong.  If I need to provide more I will.  Thank you!


![issue](https://user-images.githubusercontent.com/40045042/65709154-cac10480-e055-11e9-88d1-f4d7dfe00351.png)
"
32844,RPi0W - no go,"<em>Using instructions at [https://www.tensorflow.org/lite/guide/build_rpi](https://www.tensorflow.org/lite/guide/build_rpi )for installation on Raspberry Pi Zero W v1.3. Perhaps impractical owing to ARMv6 processor rev 7 (v6l)</em>

**System information**
processor	: 0
model name	: ARMv6-compatible processor rev 7 (v6l)
BogoMIPS	: 697.95
Features	: half thumb fastmult vfp edsp java tls 
CPU implementer	: 0x41
CPU architecture: 7
CPU variant	: 0x0
CPU part	: 0xb76
CPU revision	: 7

Hardware	: BCM2835
Revision	: 9000c1

- Linux 4.19.66+ #1253 Thu Aug 15 11:37:30 BST 2019 armv6l GNU/Linux

- TensorFlow installed from (source or binary): [source](https://github.com/tensorflow/tensorflow))
- TensorFlow version: sorry, don't know :(
- Python version: 3.7
- Installed using virtualenv? pip? conda?: [script](./tensorflow/lite/tools/make/download_dependencies.sh)
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): gcc as in Raspbian Buster (no user mods)
- CUDA/cuDNN version: n/a
- GPU model and memory:

```
MemTotal:         443080 kB
MemFree:          160252 kB
MemAvailable:     327036 kB
```

**Describe the problem**

**sh
+ set -e
+++ dirname ./tensorflow/lite/tools/make/build_rpi_lib.sh
++ cd ./tensorflow/lite/tools/make
++ pwd
+ SCRIPT_DIR=/home/pi/projects/tensorflow/tensorflow/lite/tools/make
+ TENSORFLOW_DIR=/home/pi/projects/tensorflow/tensorflow/lite/tools/make/../../../..
+ make -j 4 TARGET=rpi -C /home/pi/projects/tensorflow/tensorflow/lite/tools/make/../../../.. -f tensorflow/lite/tools/make/Makefile
make: Entering directory '/home/pi/projects/tensorflow'
make: Nothing to be done for 'all'.
make: Leaving directory '/home/pi/projects/tensorflow'
**

**Any other info / logs**
Will be glad to provide any upon specific request because I really don't know what is happening beneath the covers of the installation script
"
32842,Using Dataset.map with python dicts as dataset elements passes Tensors without numpy values to the mapping function,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **YES**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): **Pycharm, pip-install**
- TensorFlow version (use command below): **v2.0.0-rc1-51-g2646d23074 2.0.0-rc2**
- Python version: **3.6**
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

I'm doing a simple pipeline where I load a dataset from a csv file, convert the dataset to a dict structure (where one element of the dataset is a dict with strings as keys and tf.Tensor as values) and then try to use this resulting dataset. The issue is that, when I create the dict-based dataset, I can see that the numpy values are there in the Tensors, BUT when I use Dataset.map to apply a function to the dict-based dataset, I can't access the numpy values inside this function. The numpy values """"disappear"""".

**Describe the expected behavior**

I should be able to see and use the numpy values.

**Code to reproduce the issue**

IMPORTANT: replace the TODO in the beginning of the code with the path that the csv I provided after the code, below, will have in your system.

```
import os
import tensorflow as tf

#TODO REPLACE THE LINE BELOW BY ONE THAT DESCRIBES THE PATH OF THE CSV IN YOUR SYSTEM
data_source = os.path.join(""."", ""Data"", ""mini.csv"")

#Help function to convert tuples of Tensors to dicts, so I have the column names from the csv as keys
def _convert_to_dict(*el):
    base_list = [""var1"", ""var2"", ""var3""] #as in the csv header
    dicto = dict()
    for i in range(len(base_list)):
        dicto[base_list[i]] = el[i]
    return dicto

#Simple function which will try to investigate and use the numpy values inside the Tensors in the datapoints
def _use_dict(dicto, key):
    """"""
    dicto: dict with string keys and Tensors as values
    :param key: a specific string key
    :return:
    """"""
    print(""Printing inspections from _use_dict function"")
    print(dicto) #Here we print the dataset element passed to the function, which is a dictionary, and see that no numpy value is associated with the Tensors
    print(dicto[key]) #We look closer, no numpy values
    print(dicto[key].numpy()) #No numpy value, so we get an exception here
    return None

def load_and_preprocess_dataset(): #Main function which will load the dataset from csv and transform to a dict-like structure
    #Load the dataset from CSV
    dataset = tf.data.experimental.CsvDataset(
        header=True,
        filenames=data_source,
        record_defaults=[
            tf.int64,
            tf.string,
            tf.float64,
        ]
    )
    #Convert each element from the dataset from a tuple of Tensors to a dict with keys being the csv header values, and values being the corresponding Tensors
    dataset = dataset.map(map_func=_convert_to_dict)
    #Here we inspect the values of the dataset, and observe that the Tensors contain numpy values
    print(""Printing dataset elements after transforming to dict-like structure"")
    for a in dataset:
        print(a)
    #Here we try to apply a function which accepts a dict-like structured dataset element as input
    dataset = dataset.map(map_func=lambda x: _use_dict(dicto=x, key=""var1""))
    return dataset

if __name__ == '__main__':
    dataset = load_and_preprocess_dataset()
```

Csv file content:

```
var1,var2,var3
2,""foo"",1.3
3,""bar"",1.5
4,""barfoo"",1.8
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
Stack trace:

2019-09-26 16:49:14.948332: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
Printing dataset elements after transforming to dict-like structure
{'var1': <tf.Tensor: id=27, shape=(), dtype=int64, numpy=2>, 'var2': <tf.Tensor: id=28, shape=(), dtype=string, numpy=b'foo'>, 'var3': <tf.Tensor: id=29, shape=(), dtype=float64, numpy=1.3>}
{'var1': <tf.Tensor: id=30, shape=(), dtype=int64, numpy=3>, 'var2': <tf.Tensor: id=31, shape=(), dtype=string, numpy=b'bar'>, 'var3': <tf.Tensor: id=32, shape=(), dtype=float64, numpy=1.5>}
{'var1': <tf.Tensor: id=33, shape=(), dtype=int64, numpy=4>, 'var2': <tf.Tensor: id=34, shape=(), dtype=string, numpy=b'barfoo'>, 'var3': <tf.Tensor: id=35, shape=(), dtype=float64, numpy=1.8>}
Printing inspections from _use_dict function
{'var1': <tf.Tensor 'args_0:0' shape=() dtype=int64>, 'var2': <tf.Tensor 'args_1:0' shape=() dtype=string>, 'var3': <tf.Tensor 'args_2:0' shape=() dtype=float64>}
Tensor(""args_0:0"", shape=(), dtype=int64)
Traceback (most recent call last):
  File ""...temp.py"", line 50, in <module>
    dataset = load_and_preprocess_dataset()
  File ""...temp.py"", line 46, in load_and_preprocess_dataset
    dataset = dataset.map(map_func=lambda x: _use_dict(dicto=x, key=""var1""))
  File ""...site-packages\tensorflow_core\python\data\ops\dataset_ops.py"", line 1211, in map
    return MapDataset(self, map_func, preserve_cardinality=True)
  File ""...site-packages\tensorflow_core\python\data\ops\dataset_ops.py"", line 3416, in __init__
    use_legacy_function=use_legacy_function)
  File ""...site-packages\tensorflow_core\python\data\ops\dataset_ops.py"", line 2695, in __init__
    self._function = wrapper_fn._get_concrete_function_internal()
  File ""...site-packages\tensorflow_core\python\eager\function.py"", line 1854, in _get_concrete_function_internal
    *args, **kwargs)
  File ""...site-packages\tensorflow_core\python\eager\function.py"", line 1848, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""...site-packages\tensorflow_core\python\eager\function.py"", line 2150, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""...site-packages\tensorflow_core\python\eager\function.py"", line 2041, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""...site-packages\tensorflow_core\python\framework\func_graph.py"", line 915, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""...site-packages\tensorflow_core\python\data\ops\dataset_ops.py"", line 2689, in wrapper_fn
    ret = _wrapper_helper(*args)
  File ""...site-packages\tensorflow_core\python\data\ops\dataset_ops.py"", line 2634, in _wrapper_helper
    ret = autograph.tf_convert(func, ag_ctx)(*nested_args)
  File ""...site-packages\tensorflow_core\python\autograph\impl\api.py"", line 237, in wrapper
    raise e.ag_error_metadata.to_exception(e)
AttributeError: in converted code:
    relative to ...:

    temp.py:46 None  *
        dataset = dataset.map(map_func=lambda x: _use_dict(dicto=x, key=""var1""))
    temp.py:25 _use_dict  *
        print(dicto[key].numpy()) #No numpy value, so we get an exception here

    AttributeError: 'Tensor' object has no attribute 'numpy'


Process finished with exit code 1

```
"
32841,Problems running mnist_estimator in distributed mode,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12
- Python version: 3.6.5
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
Running below code that I found in many pages on the net, I faced some problems:

```
import json
import os
import tensorflow as tf
from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets
data_dir = '.\\MNIST_data'
log_dir = '.\log_dist'
batch_size = 512
tf.logging.set_verbosity(tf.logging.INFO)

def keras_model(lr, decay):
    """"""Return a CNN Keras model""""""
    input_tensor = tf.keras.layers.Input(shape=(784,), name='input')

    temp = tf.keras.layers.Reshape([28, 28, 1], name='input_image')(input_tensor)
    for i, n_units in enumerate([32, 64]):
        temp = tf.keras.layers.Conv2D(n_units, kernel_size=3, strides=(2, 2),
                                      activation='relu', name='cnn'+str(i))(temp)
        temp = tf.keras.layers.Dropout(0.5, name='dropout'+str(i))(temp)
    temp = tf.keras.layers.GlobalAvgPool2D(name='average')(temp)
    output = tf.keras.layers.Dense(10, activation='softmax', name='output')(temp)

    model = tf.keras.models.Model(inputs=input_tensor, outputs=output)
    optimizer = tf.keras.optimizers.Adam(lr=lr, decay=decay)
    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    print(model.summary())
    return model


def main():
    """"""Main function""""""
    data = read_data_sets(data_dir,
                          one_hot=False,
                          fake_data=False)
    model = keras_model(lr=0.001, decay=0.001)
    config = tf.estimator.RunConfig(
                model_dir=log_dir,
                save_summary_steps=1,
                save_checkpoints_steps=100)
    estimator = tf.keras.estimator.model_to_estimator(model, model_dir=log_dir, config=config)

    train_input_fn = tf.estimator.inputs.numpy_input_fn(
                         x={'input': data.train.images},
                         y=data.train.labels,
                         num_epochs=None,   # run forever
                         batch_size=batch_size,
                         shuffle=True)
    eval_input_fn = tf.estimator.inputs.numpy_input_fn(
                         x={'input': data.test.images},
                         y=data.test.labels,
                         num_epochs=1,
                         shuffle=False)

    train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn,
                                        max_steps=2000)
    eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn,
                                      #throttle_secs=1,
                                      steps=None    # until the end of evaluation data
                                      )

    evaluate_result = tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
    print(""Evaluation results:"")
    for key in evaluate_result[0].keys():
        print(""   {}: {}"".format(key, evaluate_result[0][key]))
```

And then the rest of the code just included the TF_CONFIG definition for chief, worker and ps. I faced below issues:

- I was able to run this code on Tensorflow 1.12 but not on Tensorflow 1.13, where I got the error `ValueError: Can not squeeze dim[1], expected a dimension of 1, got 10 for 'metrics/acc/remove_squeezable_dimensions/Squeeze' (op: 'Squeeze') with input shapes: [512,10]`. What is the reason?
- I could get evaluation results printed at the end of training when I was running program in non-distributed mode, but I get below error when it tries to print the final evaluation results in distributed mode:
```
Traceback (most recent call last):
  File ""mnist_estimator.py"", line 81, in <module>
    main()
  File ""mnist_estimator.py"", line 62, in main
    for key in evaluate_result[0].keys():
TypeError: 'NoneType' object is not subscriptable
```
- The final loss for distributed learning was higher than non-distributed learning (for the same number of training steps). What can be the reason? Is it the nature of distribution?
- When running in distributed mode, the chief or worker are not waiting for the other party to start and immediately starts training (when the other party joins they do the task together, though). I thought they should wait for each other to be ready (as it was in my previous experiences with Tensorflow distributed training), isn't it?
- What I read in Tensorflow-related pages about data-parallelism is that there are the same copies of code for different servers except in assignment in TF_CONFIG. The chief synchronizes the parameters update and parameter servers keeps the parameters, but I don't clearly understand who split the data between different workers. Is there just one copy at the chief server and it will split the data and send batches to the workers, or the workers each have a local copy of data and do the splitting and skip some data themselves?"
32840,TFLite Different behaviour after upgrading gradle,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung S9
- TensorFlow version (use command below): tensorflow-lite and tensorflow-lite-gpu nightly

**Describe the current behavior**
I have a project that uses the android camera and every 500ms processes a frame in the background thread. I'm using a GPU delegate.  

I recently upgraded the gradle-wrapper from `5.1.1` to `5.6.2`. After this everything seemed to remain the same, except for:
1. model is inferring much faster (15%-30% faster)
2. model is taking much more time to initialize inferences (about 8 seconds, before it was around 1 second)

Even though I'm pleased with the apparent improved inference times, the 7 second waiting to start running inferences is a pain.. Is there any reason for that?

I've checked the logs (which I attach) and realized I have - at least - a new additional print in the 5.6.2 version:
`Initialized OpenCL-based API.`
The inferences only start after this print so I imagine it might have to due with the delay mentioned.

Thank you for your help!

PS: I've attached the logs but I don't know how much of an help they could be

[android-debug 5.1.1.log](https://github.com/tensorflow/tensorflow/files/3658022/android-debug.5.1.1.log)
[android-debug 5.6.2.log](https://github.com/tensorflow/tensorflow/files/3658023/android-debug.5.6.2.log)
"
32839,Inputs to eager execution function cannot be Keras symbolic tensors,"Version:
2.0.0-rc

Python Version:
python 3.7


**this is the shape check function!**
'''@tf.function
def shape_check(input_channels,filters,bottom ,second):
    shortcut = tf.cond(
                            tf.equal(input_channels, filters),
                            lambda :bottom,
                            lambda :second 
                        )
    return shortcut'''



**The Error is poping here**
this is a class function and i tried writing tf.cond() but it got different error!
'''def _basic_block(self, bottom, filters):
        input_channels = tf.shape(bottom)[-1]
        conv = self._conv_bn_activation(bottom, filters, 3, 1)
        conv = self._conv_bn_activation(conv, filters, 3, 1)
        input_channels = tf.shape(bottom)[-1]
        shortcut = shape_check(input_channels,filters,bottom,self._conv_bn_activation(bottom, filters, 1, 1))
        
        return conv + shortcut'''




"
32837,Missing GPU implementation,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Linux Kubuntu 18.04
- TensorFlow installed from: binary (with pip3)
- TensorFlow version: 1.14.0 (v1.14.0-rc1-22-gaf24dc91b5)
- Python version: 3.6.8
- CUDA/cuDNN version: 10.0.130
- GPU model and memory: NVidia Titan Xp

**Describe the current behavior**
The op tf.norm cannot be assigned to the GPU in all configurations (see minimal example below).

**Describe the expected behavior**
I see no theoretical reason for the tf.norm op not to be fully portable to the GPU. I suspect this is a bug, not an intended behavior.

**Code to reproduce the issue**
```
import tensorflow as tf

b = 10
width = 16
height = 16
channels = 3

def norm_simple(X):
  return tf.norm(X,axis=[1,2],ord=2)

def norm_custom(X):
  X_r = tf.reshape(X,shape=[b,width*height,channels])
  return tf.norm(X_r,axis=1,ord=2)

s = tf.Session()

A = tf.ones(shape=[b,width,height,channels])

with tf.device('/cpu:0'):
  n1_cpu = norm_simple(A)
s.run(n1_cpu)
print("" === norm_simple works on CPU"")

with tf.device('/cpu:0'):
  n2_cpu = norm_custom(A)
s.run(n2_cpu)
print("" === norm_custom works on CPU"")

with tf.device('/gpu:0'):
 n2_gpu = norm_custom(A)
s.run(n2_gpu)
print("" === norm_custom works on GPU"")

with tf.device('/gpu:0'):
 n1_gpu = norm_simple(A)
s.run(n1_gpu)
print("" === norm_simple works on GPU"")

```

**Other info / logs**
Execution of the above code on my machine:
```
 === norm_simple works on CPU
 === norm_custom works on CPU
 === norm_custom works on GPU
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1356, in _do_call
    return fn(*args)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1339, in _run_fn
    self._extend_graph()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1374, in _extend_graph
    tf_session.ExtendSession(self._session)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation norm_3/map/TensorArray: Could not satisfy explicit device specification '' because the node {{colocation_node norm_3/map/TensorArray}} was colocated with a group of nodes that required incompatible device '/device:GPU:0'. All available devices [/job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:1, /job:localhost/replica:0/task:0/device:XLA_CPU:0, /job:localhost/replica:0/task:0/device:GPU:0, /job:localhost/replica:0/task:0/device:GPU:1]. 
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='' supported_device_types_=[CPU, XLA_CPU, XLA_GPU] possible_devices_=[]
Switch: GPU CPU XLA_CPU XLA_GPU 
TensorArrayScatterV3: CPU XLA_CPU XLA_GPU 
TensorArrayReadV3: CPU XLA_CPU XLA_GPU 
Enter: GPU CPU XLA_CPU XLA_GPU 
TensorArrayV3: CPU XLA_CPU XLA_GPU 
Const: GPU CPU XLA_CPU XLA_GPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  norm_3/Const (Const) /device:GPU:0
  norm_3/map/TensorArray (TensorArrayV3) 
  norm_3/map/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3 (TensorArrayScatterV3) /device:GPU:0
  norm_3/map/while/TensorArrayReadV3/Enter (Enter) /device:GPU:0
  norm_3/map/while/TensorArrayReadV3 (TensorArrayReadV3) /device:GPU:0
  norm_3/map/while/cond/Switch_1 (Switch) /device:GPU:0
  norm_3/map/while/cond/add/Switch (Switch) /device:GPU:0

         [[{{node norm_3/map/TensorArray}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""bugreport.py"", line 36, in <module>
    s.run(n1_gpu)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 950, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1173, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _do_run
    run_metadata)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1370, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation norm_3/map/TensorArray: Could not satisfy explicit device specification '' because the node node norm_3/map/TensorArray (defined at bugreport.py:9) placed on device No device assignments were active during op 'norm_3/map/TensorArray' creation.  was colocated with a group of nodes that required incompatible device '/device:GPU:0'. All available devices [/job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:1, /job:localhost/replica:0/task:0/device:XLA_CPU:0, /job:localhost/replica:0/task:0/device:GPU:0, /job:localhost/replica:0/task:0/device:GPU:1]. 
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='' supported_device_types_=[CPU, XLA_CPU, XLA_GPU] possible_devices_=[]
Switch: GPU CPU XLA_CPU XLA_GPU 
TensorArrayScatterV3: CPU XLA_CPU XLA_GPU 
TensorArrayReadV3: CPU XLA_CPU XLA_GPU 
Enter: GPU CPU XLA_CPU XLA_GPU 
TensorArrayV3: CPU XLA_CPU XLA_GPU 
Const: GPU CPU XLA_CPU XLA_GPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  norm_3/Const (Const) /device:GPU:0
  norm_3/map/TensorArray (TensorArrayV3) 
  norm_3/map/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3 (TensorArrayScatterV3) /device:GPU:0
  norm_3/map/while/TensorArrayReadV3/Enter (Enter) /device:GPU:0
  norm_3/map/while/TensorArrayReadV3 (TensorArrayReadV3) /device:GPU:0
  norm_3/map/while/cond/Switch_1 (Switch) /device:GPU:0
  norm_3/map/while/cond/add/Switch (Switch) /device:GPU:0

         [[node norm_3/map/TensorArray (defined at bugreport.py:9) ]]Additional information about colocations:No node-device colocations were active during op 'norm_3/map/TensorArray' creation.
No device assignments were active during op 'norm_3/map/TensorArray' creation.

Original stack trace for 'norm_3/map/TensorArray':
  File ""bugreport.py"", line 35, in <module>
    n1_gpu = norm_simple(A)
  File ""bugreport.py"", line 9, in norm_simple
    return tf.norm(X,axis=[1,2],ord=2)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/linalg_ops.py"", line 600, in norm
    ops.convert_to_tensor(axis))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/map_fn.py"", line 228, in map_fn
    for elem in elems_flat]
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/map_fn.py"", line 228, in <listcomp>
    for elem in elems_flat]
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py"", line 1086, in __init__
    name=name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py"", line 164, in __init__
    self._handle, self._flow = create()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py"", line 161, in create
    name=scope)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 8085, in tensor_array_v3
    tensor_array_name=tensor_array_name, name=name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3616, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 2005, in __init__
    self._traceback = tf_stack.extract_stack()

```
"
32836,Detected cudnn out-of-bounds write in conv scratch buffer! This is likely a cudnn bug,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): both source and binary
- TensorFlow version (use command below): 1.14.0
- Python version:3.7
- Bazel version (if compiling from source): 0.25.2
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: cuda10, cuda10.1,  7.6.0, 7.6.2
- GPU model and memory: 2080Ti

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


2019-09-26 17:28:07.044771: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:157] Detected cudnn out-of-bounds write in conv input/output buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.
2019-09-26 17:28:07.044827: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:166] Redzone mismatch in LHS redzone of buffer 0x7fd3e5090200 at offset 0; expected ffffffffffffffff but was 3b0387303ca4a820.
2019-09-26 17:28:07.044863: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:167] HloInstruction %custom-call = (f32[5,5,2,64]{1,0,2,3}, u8[0]{0}) custom-call(f32[155,2,64,64]{3,2,1,0} %arg8.9, f32[155,64,60,60]{3,2,1,0} %select.42), window={size=5x5}, dim_labels=bf01_01io->bf01, custom_call_target=""__cudnn$convBackwardFilter"", metadata={op_type=""Conv2DBackpropFilter"" op_name=""HICO_1/gradients/HICO_0/resnet_v1_50_3/conv1_sp/Conv2D_grad/Conv2DBackpropFilter""}, backend_config=""{\""convResultScale\"":1}""
2019-09-26 17:28:07.044874: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:112] Device: GeForce RTX 2080 Ti
2019-09-26 17:28:07.044881: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:113] Platform: Compute Capability 7.5
2019-09-26 17:28:07.044889: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:114] Driver: 10010 (418.87.0)
2019-09-26 17:28:07.044896: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:115] Runtime: <undefined>
2019-09-26 17:28:07.044907: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:122] cudnn version: 7.6.2
2019-09-26 17:28:07.065798: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:157] Detected cudnn out-of-bounds write in conv input/output buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.
2019-09-26 17:28:07.065836: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:166] Redzone mismatch in LHS redzone of buffer 0x7fd3e5090200 at offset 0; expected ffffffffffffffff but was 3b0387303ca4a820.
2019-09-26 17:28:07.065866: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:167] HloInstruction %custom-call = (f32[5,5,2,64]{1,0,2,3}, u8[0]{0}) custom-call(f32[155,2,64,64]{3,2,1,0} %arg8.9, f32[155,64,60,60]{3,2,1,0} %select.42), window={size=5x5}, dim_labels=bf01_01io->bf01, custom_call_target=""__cudnn$convBackwardFilter"", metadata={op_type=""Conv2DBackpropFilter"" op_name=""HICO_1/gradients/HICO_0/resnet_v1_50_3/conv1_sp/Conv2D_grad/Conv2DBackpropFilter""}, backend_config=""{\""convResultScale\"":1}""
2019-09-26 17:28:07.065877: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:112] Device: GeForce RTX 2080 Ti
2019-09-26 17:28:07.065885: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:113] Platform: Compute Capability 7.5
2019-09-26 17:28:07.065892: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:114] Driver: 10010 (418.87.0)
2019-09-26 17:28:07.065899: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:115] Runtime: <undefined>
2019-09-26 17:28:07.065909: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:122] cudnn version: 7.6.2
2019-09-26 17:28:07.082404: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:157] Detected cudnn out-of-bounds write in conv input/output buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.
2019-09-26 17:28:07.082436: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:166] Redzone mismatch in LHS redzone of buffer 0x7fd3e5090200 at offset 0; expected ffffffffffffffff but was 3b0387303ca4a820.
2019-09-26 17:28:07.082464: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:167] HloInstruction %custom-call = (f32[5,5,2,64]{1,0,2,3}, u8[0]{0}) custom-call(f32[155,2,64,64]{3,2,1,0} %arg8.9, f32[155,64,60,60]{3,2,1,0} %select.42), window={size=5x5}, dim_labels=bf01_01io->bf01, custom_call_target=""__cudnn$convBackwardFilter"", metadata={op_type=""Conv2DBackpropFilter"" op_name=""HICO_1/gradients/HICO_0/resnet_v1_50_3/conv1_sp/Conv2D_grad/Conv2DBackpropFilter""}, backend_config=""{\""convResultScale\"":1}""
2019-09-26 17:28:07.082474: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:112] Device: GeForce RTX 2080 Ti
2019-09-26 17:28:07.082482: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:113] Platform: Compute Capability 7.5
2019-09-26 17:28:07.082490: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:114] Driver: 10010 (418.87.0)
2019-09-26 17:28:07.082497: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:115] Runtime: <undefined>
2019-09-26 17:28:07.082506: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:122] cudnn version: 7.6.2
2019-09-26 17:28:07.109990: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:157] Detected cudnn out-of-bounds write in conv input/output buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.
2019-09-26 17:28:07.110017: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:166] Redzone mismatch in LHS redzone of buffer 0x7fd3e5090200 at offset 0; expected ffffffffffffffff but was 3b0387303ca4a820.
2019-09-26 17:28:07.110045: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:167] HloInstruction %custom-call = (f32[5,5,2,64]{1,0,2,3}, u8[0]{0}) custom-call(f32[155,2,64,64]{3,2,1,0} %arg8.9, f32[155,64,60,60]{3,2,1,0} %select.42), window={size=5x5}, dim_labels=bf01_01io->bf01, custom_call_target=""__cudnn$convBackwardFilter"", metadata={op_type=""Conv2DBackpropFilter"" op_name=""HICO_1/gradients/HICO_0/resnet_v1_50_3/conv1_sp/Conv2D_grad/Conv2DBackpropFilter""}, backend_config=""{\""convResultScale\"":1}""
2019-09-26 17:28:07.110055: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:112] Device: GeForce RTX 2080 Ti
2019-09-26 17:28:07.110064: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:113] Platform: Compute Capability 7.5
2019-09-26 17:28:07.110071: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:114] Driver: 10010 (418.87.0)
2019-09-26 17:28:07.110078: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:115] Runtime: <undefined>
2019-09-26 17:28:07.110088: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:122] cudnn version: 7.6.2
2019-09-26 17:28:07.144289: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:157] Detected cudnn out-of-bounds write in conv input/output buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.
2019-09-26 17:28:07.144361: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:166] Redzone mismatch in LHS redzone of buffer 0x7fd3e5090200 at offset 0; expected ffffffffffffffff but was 3b0387303ca4a820.
2019-09-26 17:28:07.144396: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:167] HloInstruction %custom-call = (f32[5,5,2,64]{1,0,2,3}, u8[0]{0}) custom-call(f32[155,2,64,64]{3,2,1,0} %arg8.9, f32[155,64,60,60]{3,2,1,0} %select.42), window={size=5x5}, dim_labels=bf01_01io->bf01, custom_call_target=""__cudnn$convBackwardFilter"", metadata={op_type=""Conv2DBackpropFilter"" op_name=""HICO_1/gradients/HICO_0/resnet_v1_50_3/conv1_sp/Conv2D_grad/Conv2DBackpropFilter""}, backend_config=""{\""convResultScale\"":1}""
2019-09-26 17:28:07.144406: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:112] Device: GeForce RTX 2080 Ti
2019-09-26 17:28:07.144414: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:113] Platform: Compute Capability 7.5
2019-09-26 17:28:07.144422: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:114] Driver: 10010 (418.87.0)
2019-09-26 17:28:07.144429: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:115] Runtime: <undefined>
2019-09-26 17:28:07.144440: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:122] cudnn version: 7.6.2
2019-09-26 17:28:07.161422: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:157] Detected cudnn out-of-bounds write in conv input/output buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.
2019-09-26 17:28:07.161467: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:166] Redzone mismatch in LHS redzone of buffer 0x7fd3e5090200 at offset 0; expected ffffffffffffffff but was 3b0387303ca4a820.
2019-09-26 17:28:07.161501: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:167] HloInstruction %custom-call = (f32[5,5,2,64]{1,0,2,3}, u8[0]{0}) custom-call(f32[155,2,64,64]{3,2,1,0} %arg8.9, f32[155,64,60,60]{3,2,1,0} %select.42), window={size=5x5}, dim_labels=bf01_01io->bf01, custom_call_target=""__cudnn$convBackwardFilter"", metadata={op_type=""Conv2DBackpropFilter"" op_name=""HICO_1/gradients/HICO_0/resnet_v1_50_3/conv1_sp/Conv2D_grad/Conv2DBackpropFilter""}, backend_config=""{\""convResultScale\"":1}""
2019-09-26 17:28:07.161512: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:112] Device: GeForce RTX 2080 Ti
2019-09-26 17:28:07.161520: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:113] Platform: Compute Capability 7.5
2019-09-26 17:28:07.161529: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:114] Driver: 10010 (418.87.0)
2019-09-26 17:28:07.161536: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:115] Runtime: <undefined>
2019-09-26 17:28:07.161547: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:122] cudnn version: 7.6.2
2019-09-26 17:28:07.176122: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:157] Detected cudnn out-of-bounds write in conv input/output buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.
2019-09-26 17:28:07.176175: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:166] Redzone mismatch in LHS redzone of buffer 0x7fd3e5090200 at offset 0; expected ffffffffffffffff but was 3b0387303ca4a820.
2019-09-26 17:28:07.176206: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:167] HloInstruction %custom-call = (f32[5,5,2,64]{1,0,2,3}, u8[0]{0}) custom-call(f32[155,2,64,64]{3,2,1,0} %arg8.9, f32[155,64,60,60]{3,2,1,0} %select.42), window={size=5x5}, dim_labels=bf01_01io->bf01, custom_call_target=""__cudnn$convBackwardFilter"", metadata={op_type=""Conv2DBackpropFilter"" op_name=""HICO_1/gradients/HICO_0/resnet_v1_50_3/conv1_sp/Conv2D_grad/Conv2DBackpropFilter""}, backend_config=""{\""convResultScale\"":1}""
2019-09-26 17:28:07.176217: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:112] Device: GeForce RTX 2080 Ti
2019-09-26 17:28:07.176226: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:113] Platform: Compute Capability 7.5
2019-09-26 17:28:07.176234: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:114] Driver: 10010 (418.87.0)
2019-09-26 17:28:07.176241: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:115] Runtime: <undefined>
2019-09-26 17:28:07.176253: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:122] cudnn version: 7.6.2
2019-09-26 17:28:07.191145: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:157] Detected cudnn out-of-bounds write in conv input/output buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.
2019-09-26 17:28:07.191200: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:166] Redzone mismatch in LHS redzone of buffer 0x7fd3e5090200 at offset 0; expected ffffffffffffffff but was 3b0387303ca4a820.
2019-09-26 17:28:07.191232: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:167] HloInstruction %custom-call = (f32[5,5,2,64]{1,0,2,3}, u8[0]{0}) custom-call(f32[155,2,64,64]{3,2,1,0} %arg8.9, f32[155,64,60,60]{3,2,1,0} %select.42), window={size=5x5}, dim_labels=bf01_01io->bf01, custom_call_target=""__cudnn$convBackwardFilter"", metadata={op_type=""Conv2DBackpropFilter"" op_name=""HICO_1/gradients/HICO_0/resnet_v1_50_3/conv1_sp/Conv2D_grad/Conv2DBackpropFilter""}, backend_config=""{\""convResultScale\"":1}""
2019-09-26 17:28:07.191243: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:112] Device: GeForce RTX 2080 Ti
2019-09-26 17:28:07.191251: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:113] Platform: Compute Capability 7.5
2019-09-26 17:28:07.191259: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:114] Driver: 10010 (418.87.0)
2019-09-26 17:28:07.191266: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:115] Runtime: <undefined>
2019-09-26 17:28:07.191279: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:122] cudnn version: 7.6.2
2019-09-26 17:28:07.207762: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:157] Detected cudnn out-of-bounds write in conv input/output buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.
2019-09-26 17:28:07.207808: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:166] Redzone mismatch in LHS redzone of buffer 0x7fd3e5090200 at offset 0; expected ffffffffffffffff but was 3b0387303ca4a820.
2019-09-26 17:28:07.207856: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:167] HloInstruction %custom-call = (f32[5,5,2,64]{1,0,2,3}, u8[0]{0}) custom-call(f32[155,2,64,64]{3,2,1,0} %arg8.9, f32[155,64,60,60]{3,2,1,0} %select.42), window={size=5x5}, dim_labels=bf01_01io->bf01, custom_call_target=""__cudnn$convBackwardFilter"", metadata={op_type=""Conv2DBackpropFilter"" op_name=""HICO_1/gradients/HICO_0/resnet_v1_50_3/conv1_sp/Conv2D_grad/Conv2DBackpropFilter""}, backend_config=""{\""convResultScale\"":1}""
2019-09-26 17:28:07.207867: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:112] Device: GeForce RTX 2080 Ti
2019-09-26 17:28:07.207874: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:113] Platform: Compute Capability 7.5
2019-09-26 17:28:07.207881: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:114] Driver: 10010 (418.87.0)
2019-09-26 17:28:07.207889: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:115] Runtime: <undefined>
2019-09-26 17:28:07.207901: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:122] cudnn version: 7.6.2
2019-09-26 17:28:07.225787: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:157] Detected cudnn out-of-bounds write in conv input/output buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.
2019-09-26 17:28:07.225859: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:166] Redzone mismatch in LHS redzone of buffer 0x7fd3e5090200 at offset 0; expected ffffffffffffffff but was 3b0387303ca4a820.
2019-09-26 17:28:07.225900: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:167] HloInstruction %custom-call = (f32[5,5,2,64]{1,0,2,3}, u8[0]{0}) custom-call(f32[155,2,64,64]{3,2,1,0} %arg8.9, f32[155,64,60,60]{3,2,1,0} %select.42), window={size=5x5}, dim_labels=bf01_01io->bf01, custom_call_target=""__cudnn$convBackwardFilter"", metadata={op_type=""Conv2DBackpropFilter"" op_name=""HICO_1/gradients/HICO_0/resnet_v1_50_3/conv1_sp/Conv2D_grad/Conv2DBackpropFilter""}, backend_config=""{\""convResultScale\"":1}""
2019-09-26 17:28:07.225934: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:112] Device: GeForce RTX 2080 Ti
2019-09-26 17:28:07.225948: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:113] Platform: Compute Capability 7.5
2019-09-26 17:28:07.225966: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:114] Driver: 10010 (418.87.0)
2019-09-26 17:28:07.225981: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:115] Runtime: <undefined>
2019-09-26 17:28:07.226003: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:122] cudnn version: 7.6.2
2019-09-26 17:28:07.226796: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:525] Internal: All algorithms tried for convolution %custom-call = (f32[5,5,2,64]{1,0,2,3}, u8[0]{0}) custom-call(f32[155,2,64,64]{3,2,1,0} %arg8.9, f32[155,64,60,60]{3,2,1,0} %select.42), window={size=5x5}, dim_labels=bf01_01io->bf01, custom_call_target=""__cudnn$convBackwardFilter"", metadata={op_type=""Conv2DBackpropFilter"" op_name=""HICO_1/gradients/HICO_0/resnet_v1_50_3/conv1_sp/Conv2D_grad/Conv2DBackpropFilter""}, backend_config=""{\""convResultScale\"":1}"" failed.  Falling back to default algorithm.
"
32835,TF doesn't build anymore with Bazel's --incompatible_remove_legacy_whole_archive,"Some time ago we migrated Tensorflow to build with this flag flipped, but it regressed again (https://buildkite.com/bazel/bazel-at-head-plus-downstream/builds/1207#d1e5940d-bde3-46cd-8734-1b6f2fddabf5).

The incompatible flag: https://github.com/bazelbuild/bazel/issues/7362

I'm looking into it. This is a tracking issue for this effort :)"
32833,TF 2.0 RC Image Segmentation Tutorial,"The tutorial is found here: https://www.tensorflow.org/beta/tutorials/images/segmentation

In cell 10, the provided Google Colab Notebook has the following error:
OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.
"
32832,Support TRANSPOSE on GPU Delegate,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 
- TensorFlow installed from (source or binary): binary(tensorflow-lite-gpu:0.0.0-nightly)
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

```
# Copy and paste here
```

Also, please include a link to a GraphDef or the model if possible.

My model is quantilized to FP16.

**Any other info / logs**
05-19 15:05:29.819 +0000 21043 21043 W TFLiteInfer: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Next operations are not supported by GPU delegate:
05-19 15:05:29.819 +0000 21043 21043 W TFLiteInfer: TRANSPOSE: Operation is not supported.
05-19 15:05:29.819 +0000 21043 21043 W TFLiteInfer: First 129 operations will run on the GPU, and the remaining 175 on the CPU.tensorflow/lite/kernels/fully_connected.cc:110 is_optional_bias_float != true (0 != 1)Node number 35 (FULLY_CONNECTED) failed to prepare.
05-19 15:05:29.819 +0000 21043 21043 W TFLiteInfer: tensorflow/lite/kernels/conv.cc:259 bias->type != input_type (10 != 1)Node number 3 (CONV_2D) failed to prepare.
"
32831,Failed to compile with CUDA 10.1 on Windows,"Trying to build TF with CUDA 10.1 support with following env:

**Enviroment**
``` 
OS: Winwdows Pro 64 bits, 1903, 18362.356
bazel : 0.26.1
msys64 : MYSIS2 64 Bit, 20190524
Visual Studio: Community 2019,  Version 16.3.1
CUDA: 10.1, cuda_10.1.243_426.00_win10
  - cuDNN : cudnn-10.1-windows10-x64-v7.6.3.30
  - TensorRT : TensorRT-6.0.1.5.Windows10.x86_64.cuda-10.1.cudnn7.6
Conda : 4.7.10, 64 bits
  - Python : 3.6.9
CPU: E5-2678 v3
GPU: RTX 2080

**Target** TF version: v2.0.0-rc1
```

**Steps to reproduce the issue**
Basically, following https://www.tensorflow.org/install/source_windows on a fresh installed Windows OS.

A detailed note: 
https://github.com/LiyuCode/blue_notes/blob/master/Windows/Tensorflow/tensorflow_with_latest_cuda_on_windows.md .

**Problem**
Failed to compile when executing the following command: 

```
bazel build --config=opt --config=cuda --config=v2 --define=no_tensorflow_py_deps=true --copt=-nvcc_options=disable-warnings //tensorflow/tools/pip_package:build_pip_package
```

The error message:
```
C:\users\lcode\_bazel_lcode\hagsv6rp\execroot\org_tensorflow\external\eigen_archive\unsupported\Eigen\CXX11\src/Tensor/TensorExecutor.h(381): error: calling a __host__ function(""std::operator -<float> "") from a __device__ function(""Eigen::internal::EigenMetaKernelEval< ::Eigen::TensorEvaluator<const  ::Eigen::TensorAssignOp< ::Eigen::TensorMap< ::Eigen::Tensor<    ::std::complex<float> , (int)1, (int)1, int> , (int)16,  ::Eigen::MakePointer> , const  ::Eigen::TensorCwiseUnaryOp< ::Eigen::internal::scalar_opposite_op<    ::std::complex<float> > , const  ::Eigen::TensorMap< ::Eigen::Tensor<const     ::std::complex<float> , (int)1, (int)1, int> , (int)16,  ::Eigen::MakePointer> > > ,  ::Eigen::GpuDevice> , int, (bool)0> ::run"") is not allowed

C:\users\lcode\_bazel_lcode\hagsv6rp\execroot\org_tensorflow\external\eigen_archive\unsupported\Eigen\CXX11\src/Tensor/TensorExecutor.h(381): error: identifier ""std::operator -<float> "" is undefined in device code

C:\users\lcode\_bazel_lcode\hagsv6rp\execroot\org_tensorflow\external\eigen_archive\unsupported\Eigen\CXX11\src/Tensor/TensorExecutor.h(381): error: calling a __host__ function(""std::operator -<double> "") from a __device__ function(""Eigen::internal::EigenMetaKernelEval< ::Eigen::TensorEvaluator<const  ::Eigen::TensorAssignOp< ::Eigen::TensorMap< ::Eigen::Tensor<    ::std::complex<double> , (int)1, (int)1, int> , (int)16,  ::Eigen::MakePointer> , const  ::Eigen::TensorCwiseUnaryOp< ::Eigen::internal::scalar_opposite_op<    ::std::complex<double> > , const  ::Eigen::TensorMap< ::Eigen::Tensor<const     ::std::complex<double> , (int)1, (int)1, int> , (int)16,  ::Eigen::MakePointer> > > ,  ::Eigen::GpuDevice> , int, (bool)0> ::run"") is not allowed

C:\users\lcode\_bazel_lcode\hagsv6rp\execroot\org_tensorflow\external\eigen_archive\unsupported\Eigen\CXX11\src/Tensor/TensorExecutor.h(381): error: identifier ""std::operator -<double> "" is undefined in device code

4 errors detected in the compilation of ""C:/Users/lcode/AppData/Local/Temp/nvcc_inter_files_tmp_dir/tmp_5sdgd1b/cwise_op_gpu_neg.cu.compute_75.cpp1.ii"".
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 1023.611s, Critical Path: 353.85s
INFO: 3709 processes: 3709 local.
FAILED: Build did NOT complete successfully
```

**Other info**
1. Compillation was ok about two days ago with same steps on a different computer
2. if checkout `v2.0.0-rc2` instead of `v2.0.0-rc1`, same error will occure.
"
32830,TfLite ObjectDetection Demo crashed on Android with armeabi,"java.lang.UnsatisfiedLinkError: No implementation found for long org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter(int) (tried Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter and Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter__I)
        at org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:71)
        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:52)
        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:114)
        at com.tfdetection.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:123)"
32821,Optimizer Fails to Run in Compatibility Mode (tested for Adam and Gradient Descent),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac 10.12.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.0.0-rc1
- Python version:3.7.1
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A (CPU)
- GPU model and memory:N/A (CPU)

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Optimizer increments the global step by 1 and stops without changing the network.
**Describe the expected behavior**
Optimizer runs metric to convergence
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```python
import tensorflow.compat.v1 as tf
import numpy as np
tf.disable_v2_behavior()
tf.disable_eager_execution()
input = tf.placeholder(dtype=tf.float32, shape=(None,28,28), name=""input"")
reshape = tf.reshape(input, [tf.shape(input)[0],784])
w1 = tf.Variable(tf.random_normal([128,784], dtype=tf.float32), name=""w1"")
b1 = tf.Variable(tf.random_normal([128], dtype=tf.float32), name=""b1"")
layer_one_unbiased = tf.matmul(w1,tf.transpose(reshape))
layer_one_biased = tf.add(tf.transpose(layer_one_unbiased),b1)
activated_layer_one = tf.nn.relu(layer_one_biased)
w2 = tf.Variable(tf.random_normal([10,128]), dtype=tf.float32, name=""w2"")
b2 = tf.Variable(tf.random_normal([10], dtype=tf.float32, name=""b2""))
layer_two_unbiased = tf.matmul(w2,tf.transpose(activated_layer_one))
layer_two_biased = tf.add(tf.transpose(layer_two_unbiased), b2)
labels= tf.placeholder(dtype=tf.int32, shape=(None))
loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=layer_two_biased)
global_step = tf.Variable(0, name='global_step', trainable=False)
#optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)
optimizer = tf.train.GradientDescentOptimizer(0.1)
train = optimizer.minimize(loss, global_step=global_step, var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))

predictions = tf.nn.softmax(layer_two_biased, name=""final"")
#label_acc = tf.one_hot(labels,10)
#accuracy = 1 - tf.norm(tf.subtract(predictions,label_acc), axis=1)/2
acc, acc_op = tf.metrics.accuracy(tf.argmax(predictions,1), labels)
#accuracy_averaged = tf.math.reduce_mean(accuracy)

fashion_mnist = tf.keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
train_images = train_images/255.0
test_images = test_images/255.0
print(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))

sess = tf.Session()
sess.run(tf.global_variables_initializer())
sess.run(tf.local_variables_initializer())
feed_dict = {input: train_images, labels: train_labels}
print(sess.run([loss,train,acc_op, global_step],feed_dict=feed_dict))

feed_dict2 = {input: test_images, labels: test_labels}
print(""Test accuracy"", sess.run(acc_op, feed_dict = feed_dict2))
print(""Training accuracy"",sess.run(acc_op, feed_dict = feed_dict))
saver =tf.train.Saver()
save_path = saver.save(sess, ""model.ckpt"")
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Code successfully runs so no error occurs but output of global step value is 1. Note that this network matches the fashion mnist network but is just using tf instead of tf.keras.layers to encode the layers."
32819,Shared and mutated state between training and validation callbacks,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Datalab
- Mobile device: NA
- TensorFlow installed from: Binary
- TensorFlow version: 1.15.0rc1
- Python version: 3.5.6
- Bazel version: NA
- GCC/Compiler version: NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**

When invoking `keras.Model.fit` with two `data.Dataset`s, one is for training, and one is validation, so that the training one is infinite with `epochs` and `steps_per_epoch` provided to `fit`, and the validation one is finite without any additional parameters to `fit`, the progress bar shows incorrect number of steps after the first validation. In particular, the number of steps per epoch gets set to the number of steps in the validation set.

**Describe the expected behavior**

The prescribed `steps_per_epoch` remains the same in the progress bar for all epochs.

**Code to reproduce the issue**

See below.

**Other info / logs**

I believe the problem is due to this modification when the input is exhausted:

https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/python/keras/engine/training_v2.py#L136

The number of steps gets overwritten. However, `self.params` is shared across both the training and validation callbacks, which essentially misleads the progress bar for training. The state share is potentially due to the following line:

https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/python/keras/engine/training_v2.py#L347

In other words, `validation_callbacks` is based on `training_callbacks`. `configure_callbacks` should probably take `callbacks` instead."
32818,Add function to gather custom objects - easy pickling for models,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.14.0
- Are you willing to contribute it (Yes/No): idk

**Describe the feature and the current behavior/state.**
Keras models should be able to iterate through their config and find any layers/functions that would be considered `custom_objects`. Currently, you need to handle them manually.

My goal is to be able to do:
```python
with open('custom_objects.pkl', 'wb') as f:
    pickle.dump(gather_custom_objects(model), f)

with open('model_spec.pkl', 'wb') as f:
    pickle.dump(model.get_config(), f)

...

with open('custom_objects.pkl', 'rb') as f:
    custom_objects = pickle.load(f)

with open('model_spec.pkl', 'rb') as f:
    model = model_from_config(pickle.load(f), custom_objects=custom_objects)
```

**Will this change the current api? How?**
It would add a utility function `gather_custom_objects(model)`

**Who will benefit with this feature?**
People who want to not have to think about custom objects in their models.

**Any Other info.**
I realize it might not be trivial to do in all cases (non-standard layer configs, for example) which is probably why it's not implemented yet."
32817,Re-emerged Issue #31509 - BaseCollectiveExecutor::StartAbort Out of range:,"The previous issue described in #31509 was fixed, but I am now experiencing exactly the same issue  with all the same setup using the latest nightly build of TF2.0 when using tf.keras.optimizers.Adam"
32816,huge runtime increase for keras converted tflite on some android devices,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Honor Play (COR-AL00) and Poco F1
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): Tested on various versions(1.14, 1.15rc1, 2.0rc1)
- Python version: 3.6
- Bazel version (if compiling from source): NIL
- GCC/Compiler version (if compiling from source): NIL
- CUDA/cuDNN version: NIL
- GPU model and memory: NIL

**Describe the current behavior**
Honor Play and POCO F1 phones give nearly same performance for TFLite models converted using TensorFlow (.pb) to TFLite whereas TFLite model converted using Keras (h5) to TFLite is behaving strange i.e. runtime on HonorPlay(COR-AL00) is 3 times higher than POCOF1.
 
Example:  
Mobilenet-Unet (converted from keras(h5) to tflite)
Average model runtime of mobilenet-unet(Poco): 60ms
Average model runtime of mobilenet-unet(Honor): 180ms

Simple - Unet (converted from tensorflow(pb) to tflite)
Average model runtime of unet(Poco): 50ms
Average model runtime of unet(Honor): 60ms

**Describe the expected behavior**
Same runtime for models with same architecture, no matter whether it is keras(h5) converted to tflite, or pb converted to tflite.

**Code to reproduce the issue**
Official Benchmark tool test and android integration test (Benchmark version- 1.14, and Android(tflite gpu delegate-nightly)

**Other info / logs**
The base keras(h5) model has been created using pure keras, not tf-keras.

**### Benchmark tool results for sample model (google drive link attached):**

**### HONOR PLAY RESULTS**
STARTING!
Min num runs: [50]
Min runs duration (seconds): [1]
Inter-run delay (seconds): [-1]
Num threads: [1]
Benchmark name: []
Output prefix: []
Min warmup runs: [1]
Min warmup runs duration (seconds): [0.5]
Graph: [/data/local/tmp/testing_mobilenet_sigmoid_divtry_tf15_1.tflite]
Input layers: []
Input shapes: []
Use nnapi : [0]
Use legacy nnapi : [0]
Use gpu : [1]
Allow fp16 : [0]
Enable op profiling: [0]
Loaded model /data/local/tmp/testing_mobilenet_sigmoid_divtry_tf15_1.tflite
resolved reporter
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for GPU.
ERROR: Next operations are not supported by GPU delegate:
CAST: Operation is not supported.
DIV: Expected 2 input tensor(s), but node has 1 runtime input(s).
RESHAPE: 
First 93 operations will run on the GPU, and the remaining 3 on the CPU.
Applied GPU delegate.
Initialized session in 894.605ms
Running benchmark for at least 1 iterations and at least 0.5 seconds
count=3 first=227955 curr=174845 min=174845 max=227955 avg=204373 std=22086

Running benchmark for at least 50 iterations and at least 1 seconds
count=50 first=148625 curr=101260 min=97334 max=148625 avg=103508 std=7440

Average inference timings in us: Warmup: 204373, Init: 894605, no stats: 103508

**### POCO F1 RESULTS** 
STARTING!
Min num runs: [50]
Min runs duration (seconds): [1]
Inter-run delay (seconds): [-1]
Num threads: [1]
Benchmark name: []
Output prefix: []
Min warmup runs: [1]
Min warmup runs duration (seconds): [0.5]
Graph: [/data/local/tmp/testing_mobilenet_sigmoid_divtry_tf15_1.tflite]
Input layers: []
Input shapes: []
Use nnapi : [0]
Use legacy nnapi : [0]
Use gpu : [1]
Allow fp16 : [0]
Enable op profiling: [0]
Loaded model /data/local/tmp/testing_mobilenet_sigmoid_divtry_tf15_1.tflite
resolved reporter
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for GPU.
ERROR: Next operations are not supported by GPU delegate:
CAST: Operation is not supported.
DIV: Expected 2 input tensor(s), but node has 1 runtime input(s).
RESHAPE: 
First 93 operations will run on the GPU, and the remaining 3 on the CPU.
Applied GPU delegate.
Initialized session in 612.655ms
Running benchmark for at least 1 iterations and at least 0.5 seconds
count=15 first=75811 curr=30928 min=30909 max=75811 avg=34068.6 std=11156

Running benchmark for at least 50 iterations and at least 1 seconds
count=50 first=31179 curr=31509 min=30836 max=32212 avg=31148.3 std=261

Average inference timings in us: Warmup: 34068.6, Init: 612655, no stats: 31148.3

**### SAMPLE MODEL**
[https://drive.google.com/file/d/1PvvoIzfrzLY8VNw5wTdpA6hxOFw_Ri9B/view?usp=sharing](https://drive.google.com/file/d/1PvvoIzfrzLY8VNw5wTdpA6hxOFw_Ri9B/view?usp=sharing)"
32815,tf.io.gfile.copy and tf.gfile.Copy input and output same path with overwrite removes all contents,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.14.0
- Python version: 2.7.15
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
`cat label.pbtxt`
> Some txt in the file

`tf.io.gfile.Copy('label.pbtxt', 'label.pbtxt', overwrite=True)`

`cat label.pbtxt`
> <Empty text file>

**Describe the expected behavior**
`cat label.pbtxt`
> Some txt in the file

`tf.io.gfile.Copy('label.pbtxt', 'label.pbtxt', overwrite=True)`

`cat label.pbtxt`
> Some txt in the file

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
`import tensorflow as tf `
`tf.io.gfile.Copy('label.pbtxt', 'label.pbtxt', overwrite=True)`

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32814,gpu_options doesn't work,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 16.04, Ubuntu 18.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): **pip install way**
- TensorFlow version (use command below): 
        **pip install tensorflow-gpu==1.12.\* and 
        pip install tensorflow-gpu==1.14.\***
- Python version: **python 3.6**
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: **tf1.12 with cuda9, tf1.14 with cuda10**
- GPU model and memory: **ubuntu 16.04 with 12G, ubuntu 18.04 with 24G**

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I used the codes in AdaNet like
```python
GPU_OPTIONS = tf.GPUOptions(allow_growth=True)
CONFIG = tf.ConfigProto(gpu_options=GPU_OPTIONS)
sess = tf.Session(config = CONFIG)
```
But it still occupied the whole gpu memory when I run the AdaNet. For example: 
(1) If there is only 4G memory left, it will occupied the remaining 6G memory
(2) If there is no other process occupying the GPU memory (i.e., there is 10G memory left), it will occupied the whole 10G memory when I run the AdaNet. 
So I think it doesn't need 10G to run but it takes that much any way.

**Describe the expected behavior**
I expect it only takes the memory it needs, instead of taking all of them. Therefore, I could make use of the 10G GPU memory better. 
But now, I have no idea how to fix this issue. Could anybody please give me some suggestions? Thanks a lot.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
I used the codes there [AdaNet](https://github.com/tensorflow/adanet) and change all ""tf.Session()"" to ""tf.Session(config=CONFIG)"" with codes as I mentioned before.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32813,Windows chief can not establish session with unix worker,"**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, CentOS 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12.2
- Python version: 3.6.5 (on Windows), 3.6.3 (on CentOS)
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 9.0
- GPU model and memory: GTX 1080 Ti, RTX 2080 Ti

**Describe the current behavior**
I run a simple code to report on the system devices in a 2-worker cluster. When Unix system is the chief (task_id=0), it can communicate and establish session with Windows worker and display cluster devices. However, when the Windows system becomes chief it can not establish session with Unix worker (hangs on displaying ' CreateSession still waiting for response from worker: /job:worker/replica:0/task:1'). Both system can reach each other in both cases via ping.

**Code to reproduce the issue**

Code for chief:
```
import tensorflow as tf
init = tf.global_variables_initializer()
cluster_spec = tf.train.ClusterSpec({'worker' : [(IP_ADDRESS1:PORT1), (IP_ADDRESS2:PORT2)]})
task_idx=0
server = tf.train.Server(cluster_spec, job_name='worker', task_index=task_idx)
with tf.Session(server.target) as sess:
    sess.run(init)
    print(sess.list_devices())
```

Code for Worker:
```
import tensorflow as tf
init = tf.global_variables_initializer()
cluster_spec = tf.train.ClusterSpec({'worker' : [(IP_ADDRESS1:PORT1), (IP_ADDRESS2:PORT2)]})
task_idx=1
server = tf.train.Server(cluster_spec, job_name='worker', task_index=task_idx)
server.join()
```

IP_ADDRESS1 is always the address of the chief system and IP_ADDRESS2 is the adddress of worker, and it is swapped when swapping Windows and Unix systems rules.

"
32812,deleted,
32810,Send/Recv of collective_ops hangs in a distributed environment ,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Linux CentOS 7.6.1810
- Mobile device if the issue happens on mobile device: No
- TensorFlow installed from: Binary
- TensorFlow version: v1.13.1-0-g6612da8951
- Python version: 3.6.8
- Bazel version: None
- GCC/Compiler version: None
- CUDA/cuDNN version: None
- GPU model and memory: None

**Describe the current behavior**

The monitored session hangs in there fetching the send/recv tensors of `collective_ops`. The same code works well for fetching `all_reduce` tensors.

**Describe the expected behavior**

The send/recv tensors `tensors` gives proper answer on all workers.

**Code to reproduce the issue**
```python
""""""Illustrate send/recv in collective_ops""""""

MP_METHOD = 'fork'  # 'fork' (UNIX), 'spawn' (WINDOWS);
NUM_PROCESSES = 2
JOB = 'worker'

def process_fn(hosts, task_index):
    """"""process fn""""""
    import time
    import tensorflow as tf
    from tensorflow.python.ops import collective_ops

    cluster_spec = tf.train.ClusterSpec({JOB: hosts})
    host_devices = list()
    for task, _ in enumerate(hosts):
        host_devices.append(tf.DeviceSpec(
            job=JOB, replica=0, task=task, device_type='cpu', device_index=0))
    chief_host_device = host_devices[0]

    # unconfigured collective_group_leader make each worker the leader
    # '/replica:0' is necessary in the configuration.
    collective_group_leader, _, _ = \
        chief_host_device.to_string().partition('/device')
    config = tf.ConfigProto()
    config.experimental.collective_group_leader = collective_group_leader
    server = tf.train.Server(cluster_spec, config=config,
                             job_name=JOB, task_index=task_index)
    run_options = tf.RunOptions()
    run_options.experimental.collective_graph_key = 1
    with tf.Graph().as_default():
        weights = list()
        tensors = list()
        instance_key = 1
        for task, device in enumerate(host_devices):
            with tf.device(device), tf.variable_scope('{}{}'.format(JOB, task)):
                weight = tf.get_variable('weight', shape=[])
                weights.append(weight)

                # send/recv
                if task == task_index:
                    tensor = collective_ops.broadcast_send(
                        weight, weight.shape, weight.dtype,
                        len(hosts), 1, instance_key)
                else:
                    tensor = collective_ops.broadcast_recv(
                        weight.shape, weight.dtype,
                        len(hosts), 1, instance_key)
                tensors.append(tensor)
                instance_key += 1

#                # allreduce
#                if task == task_index:
#                    tensor = collective_ops.all_reduce(
#                        weight, len(hosts), 0, instance_key, 'Add', 'Div')
#                    tensors.append(tensor)
#                    instance_key += 1

        if task_index == 0:
            session_creator = tf.train.ChiefSessionCreator(
                master=server.target)
        else:
            session_creator = tf.train.WorkerSessionCreator(
                master=server.target)
        with tf.train.MonitoredSession(session_creator=session_creator) \
                as mon_sess:
            print('task {} running.'.format(task_index))
            result_weights = mon_sess.run(weights, options=run_options)
            print('task {} sense {}'.format(task_index, result_weights))
            result_tensors = mon_sess.run(tensors, options=run_options)
            print('task {} broadcast {}'.format(task_index, result_tensors))
            time.sleep(10)

def start_process():
    """"""start process""""""
    import time
    import multiprocessing as mp

    port = 60000
    host_fmt = 'localhost:{}'
    hosts = list()
    for process_index in range(NUM_PROCESSES):
        hosts.append(host_fmt.format(port + process_index))
    mp_ctx = mp.get_context(MP_METHOD)
    processes = list()
    for process_index in range(NUM_PROCESSES):
        process = mp_ctx.Process(target=process_fn,
                                 args=(hosts, process_index,))
        processes.append(process)
        process.start()
        time.sleep(0.1)
    for process in processes:
        process.join()

if __name__ == '__main__':
    start_process()
```

**Other info / logs**
```console
(tf-1.13-py3) [huwh1@huwh1-centos worksync]$ python tf_distribute_collective_ops_sendrecv.py 
2019-09-25 17:46:29.612863: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-09-25 17:46:29.625630: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3408000000 Hz
2019-09-25 17:46:29.625962: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2c2e220 executing computations on platform Host. Devices:
2019-09-25 17:46:29.625984: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-09-25 17:46:29.627731: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:252] Initialize GrpcChannelCache for job worker -> {0 -> localhost:60000, 1 -> localhost:60001}
2019-09-25 17:46:29.628640: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:391] Started server with target: grpc://localhost:60000
WARNING:tensorflow:From /home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2019-09-25 17:46:29.696555: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-09-25 17:46:29.709593: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3408000000 Hz
2019-09-25 17:46:29.709906: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2c2e410 executing computations on platform Host. Devices:
2019-09-25 17:46:29.709932: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-09-25 17:46:29.711328: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:252] Initialize GrpcChannelCache for job worker -> {0 -> localhost:60000, 1 -> localhost:60001}
2019-09-25 17:46:29.712186: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:391] Started server with target: grpc://localhost:60001
WARNING:tensorflow:From /home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2019-09-25 17:46:29.741447: I tensorflow/core/distributed_runtime/master_session.cc:1192] Start master session 22dc988b8d402f7f with config: experimental { collective_group_leader: ""/job:worker/replica:0/task:0"" }
task 0 running.
task 0 sense [-0.30021727, -0.797495]
2019-09-25 17:46:29.798887: I tensorflow/core/distributed_runtime/master_session.cc:1192] Start master session bedba16e0ccc4c27 with config: experimental { collective_group_leader: ""/job:worker/replica:0/task:0"" }
task 1 running.
2019-09-25 17:46:29.820772: I tensorflow/core/distributed_runtime/base_rendezvous_mgr.cc:159] Skipping rendezvous re-initialization.
2019-09-25 17:46:29.820874: I tensorflow/core/distributed_runtime/base_rendezvous_mgr.cc:159] Skipping rendezvous re-initialization.
2019-09-25 17:46:29.821341: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Aborted: Cleanup 70896605979375878
	 [[{{node worker1/CollectiveBcastRecv}}]]
task 1 sense [-0.30021727, -0.797495]
```
There is a related issue #31913, which can be solved by specifying the `experimental.collective_group_leader` in [tf.ConfigProto](https://www.tensorflow.org/api_docs/python/tf/ConfigProto/Experimental#collective_group_leader).
An info about `rendezvous re-initialization` and a warning about`BaseCollectiveExecutor` are raised from the `send/recv`. Nevertheless, the script works fine for the commented `allreduce`. "
32809,TF 2.0 Feature: Flops calculation,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): TF 2.0 RC2
- Are you willing to contribute it (Yes/No):



**Describe the feature and the current behavior/state.**

I am missing the opportunity to compute the number of floating point operations of a tf.keras Model in TF 2.0. 
In TF 1.x tf.profiler was available [see here](https://stackoverflow.com/questions/45085938) but I can find anything equivalent for TF 2.0 yet.

**Will this change the current api? How?**

**Who will benefit with this feature?**

Everbody interested in the computational complexity of a TensorFlow model.

**Any Other info.**
"
32808,Why the communication of distributed training not hidden when using XLA?,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): None
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): nvcr.io/nvidia/tensorflow:19.03-py3 ; 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): 


- TensorFlow version (use command below):  tf 1.13.1 ; 
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

 Recently, we have designed a tool to monitor the communication of distributed training process in microsecond. As we all know that the commnuication is hidden in the back-propagation progress; and that's true in my monitoring, like the yellow line in the figure below, that is with XLA disabled. But when we enable the XLA, the traffic between GPU is suspend until the finish of the back-propagation in my figure (blue line). So, is there anyone can help me to figure out the principle behind this phenomenon. Since the traffic can not be hidden in the back-propagation progress, the requirement of the bandwidth for the hardware, like nv-link in the server or infiniband network between servers goes up a lot. 

![下载 (3)](https://user-images.githubusercontent.com/5318606/65588391-0ad2a980-dfba-11e9-8c70-1815edc17bce.png)


**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32807,AttributeError: 'Tensor' object has no attribute '_lazy_read',"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6.5
- Bazel version (if compiling from source): no
- GCC/Compiler version (if compiling from source): no
- CUDA/cuDNN version: no
- GPU model and memory: no

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I run this following code
```
import tensorflow as tf
import sys
#tf.enable_eager_execution()
a = tf.constant([[[i] * 2 for i in range(7)]]*3, dtype=tf.float32)
indice = tf.constant([[1,3,6,0],[1,4,6,0],[2,4,6,0]])
def fn(args):
    n = args[3]
    temp = args[0]
    index_first = args[1]
    index_second = args[2]
    time = tf.constant(0)
    #out_temp = args[4]
    #out_temp = tf.TensorArray(tf.float32, n,
    #                           tensor_array_name=""output_array"")
    with tf.variable_scope(""foo"", reuse=tf.AUTO_REUSE):
        out_temp = tf.get_variable(name='temp',shape=[3,2],dtype=tf.float32, initializer=tf.constant_initializer(0))
        #zero = lambda: tf.zeros([3,2], dtype=tf.int32)
        #out_temp = tf.Variable(zero)
    out_list = []
    def loop_fn(t, x, ind_1, ind_2, n, out_temp):
        #print(ind_1[t], ind_2[t])
        #print(x[ind_1[t]:ind_2[t]])
        vec = tf.reduce_mean(tf.cast(x[ind_1[t]:ind_2[t]], tf.float32), axis=0)
        #print(vec)
        #vec = tf.Print(vec, [vec], message='vec: ')
        #print_op = tf.print('vec:', vec, output_stream=sys.stdout)
        #out_temp = tf.assign(out_temp[t], vec)
        #out_temp = out_temp.write(t, vec)
        #op = tf.assign(out_temp[t], vec)
        with tf.variable_scope(""foo"", reuse=tf.AUTO_REUSE):
            #out_temp = tf.get_variable(name='temp',shape=[3,2],dtype=tf.float32)
            out_temp = tf.scatter_update(out_temp, t, vec)
        #print(out_temp)
        with tf.control_dependencies([out_temp]):
            return t+1, x, ind_1, ind_2, n, out_temp
    out = tf.while_loop(lambda t, *_: t < n, loop_fn, (time, temp, index_first, index_second, n, out_temp))
    #out_temp = tf.get_variable(name='temp',shape=[3,2],dtype=tf.float32)
    out_temp = out[-1]
    print(out_temp)
    print_op = tf.print('temp:', out_temp, output_stream=sys.stdout)
    #out_temp = tf.Print(out_temp, [out_temp], message='temp')
    with tf.control_dependencies([out_temp]):
        return [out_temp, args[1], args[2], args[3]]

n = [tf.shape(indice)[1] - 1]
n = tf.tile(n,[tf.shape(indice)[0]])
#out_tas = tf.tile(out_ta, [tf.shape(indice)[0]])
#a_temp = tf.zeros([tf.shape(a)[0], tf.shape(indice)[1]-1, 5], dtype=tf.float32)
out = tf.map_fn(fn, [a, indice[:,:-1], indice[:,1:], n])
print(out[0])

with tf.Session() as sess:
    tf.global_variables_initializer().run()
    print(sess.run(out))
```
It print the good result in eager mode, but it return AttributeError: 'Tensor' object has no attribute '_lazy_read' when I close the eager mode. I think I do feed a tf.Variable to scatter_update, but it still return the error message.
**Describe the expected behavior**
Return the value like eager mode.
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32806,keras.Model.fit does not work with custom layers in Tensorflow 2.0,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- TensorFlow installed from (source or binary): Colab
- TensorFlow version (use command below): 2.0.0-rc1
- Python version: 3.6.8 (default, Jan 14 2019, 11:02:34) 
[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]]

**Describe the current behavior**
https://colab.research.google.com/drive/1dUGcOfRIXkP6ZaI-9xKRXAntf8ZzS6bp

```python
class DualLayer(keras.layers.Layer):
  def __init__(self, units):
    super().__init__(self)
    self.first_layer = keras.layers.Dense(units, 'relu')
    self.second_layer = keras.layers.Dense(units, 'relu')
  
  def call(self, input):
    return self.second_layer(self.first_layer(input))

model = keras.Sequential([keras.layers.Flatten(), DualLayer(128), keras.layers.Dense(10, activation='softmax')])
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
history = model.fit(train_images, train_labels, batch_size=32, epochs=5)
```

This code dies at model.compile with `RecursionError: maximum recursion depth exceeded while calling a Python object`.According to the stack trace, it seems like it falls into an infinite loop with `hasattr` and `layer.trainable = value`.
``` hasattr(self.__class__, name)):
   2255       try:
-> 2256         super(tracking.AutoTrackable, self).__setattr__(name, value)
   2257       except AttributeError:
   2258         raise AttributeError(

/tensorflow-2.0.0-rc1/python3.6/tensorflow_core/python/keras/engine/base_layer.py in trainable(self, value)
    915     self._trainable = value
    916     for layer in getattr(self, '_layers', []):
--> 917       layer.trainable = value
    918 
    919   @property

/tensorflow-2.0.0-rc1/python3.6/tensorflow_core/python/keras/engine/base_layer.py in __setattr__(self, name, value)
   2254         hasattr(self.__class__, name)):
   2255       try:
-> 2256         super(tracking.AutoTrackable, self).__setattr__(name, value)
   2257       except AttributeError:
   2258         raise AttributeError(
```
**Describe the expected behavior**
Tensorflow 2.x should work with a custom nested layer as Tensorflow 1.x can.

**Code to reproduce the issue**

2.0.0-rc1:https://colab.research.google.com/drive/1dUGcOfRIXkP6ZaI-9xKRXAntf8ZzS6bp
1.14.0:
https://colab.research.google.com/drive/1DeOxKVDM8xEJmU_8GZo-h0iJkWGA4De8
"
32805,model_pruning: Why 50%  and 90% zeros of the stripped models are the same size?,"1. I'm trying tensorflow/contrib/model_pruning/examples/cifar10.  I use strip_pruning_vars to  remove pruning ops from the trained graph, strip_pruning_vars  shows 50% and 90% zeros respectively, but their final model are the same size.

2.  As described in the doc，“For now, it is assumed that the underlying hardware platform will provide mechanisms for compressing the sparse tensors and/or accelerating the sparse tensor computations“
   Does it mean that model pruning requires hardware or inference engine support in sparse matrices? 
   In the current version, can I implement inference acceleration on x86 or GPU with the model pruning?

"
32802,Problem to transform an custom efficient-net with an unofficial API to tf-lite version,"**System information**
- OS Platform and Distribution (Linux Ubuntu 18.04):
- TensorFlow installed from source:
- TensorFlow version (1.12):
- Keras version (2.2.4)


Hello, I use this Keras-repository to deploy my efficient-net-model for specific application.
https://github.com/titu1994/keras-efficientnets

I want to transform the h5-file to tflite and met some problems.

My code is:
```
import tensorflow as tf
import keras
import h5py
import keras_efficientnets
from custom_objects import EfficientNetConvInitializer
from custom_objects import EfficientNetDenseInitializer
from custom_objects import Swish, DropConnect
if __name__ == ""__main__"":
    debugger = EfficientNetConvInitializer()
    converter = tf.contrib.lite.TFLiteConverter.from_keras_model_file(""efficient_net_v1_190925.h5"")
    converter.optimizations = [tf.contrib.lite.Optimize.OPTIMIZE_FOR_SIZE]
    tflite_model = converter.convert()
    open(""efficient_net_wrap_finger.tflite"", ""wb"").write(tflite_model)
```
My problem is:

> Using TensorFlow backend.
> Traceback (most recent call last):
>   File ""model_convert.py"", line 10, in <module>
>     converter = tf.contrib.lite.TFLiteConverter.from_keras_model_file(""efficient_net_v1_190925.h5"")
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/lite/python/lite.py"", line 368, in from_keras_model_file
>     keras_model = _keras.models.load_model(model_file)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/saving.py"", line 230, in load_model
>     model = model_from_config(model_config, custom_objects=custom_objects)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/saving.py"", line 310, in model_from_config
>     return deserialize(config, custom_objects=custom_objects)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/layers/serialization.py"", line 64, in deserialize
>     printable_module_name='layer')
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 173, in deserialize_keras_object
>     list(custom_objects.items())))
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 1292, in from_config
>     process_layer(layer_data)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 1278, in process_layer
>     layer = deserialize_layer(layer_data, custom_objects=custom_objects)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/layers/serialization.py"", line 64, in deserialize
>     printable_module_name='layer')
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 175, in deserialize_keras_object
>     return cls.from_config(config['config'])
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1606, in from_config
>     return cls(**config)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/layers/convolutional.py"", line 488, in __init__
>     kernel_initializer=initializers.get(kernel_initializer),
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py"", line 155, in get
>     return deserialize(identifier)
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py"", line 147, in deserialize
>     printable_module_name='initializer')
>   File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 163, in deserialize_keras_object
>     raise ValueError('Unknown ' + printable_module_name + ': ' + class_name)
> ValueError: Unknown initializer: EfficientNetConvInitializer

What is the root-cause for this problem?

Can I clarify that keras-efficient-net API is not compatible with official tensorflow lite support?
Is there any work-around that I can build an efficent-net and depoly on my mobile device?

Thanks & Regards!"
32801,UpSampling2D doesn't support bfloat16,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): nightly
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
TypeError: Value passed to parameter 'images' has DataType bfloat16 not in list of allowed values: int8, uint8, int16, uint16, int32, int64, float16, float32, float64
**Describe the expected behavior**
support bfloat16
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
``` python
import tensorflow as tf
input = tf.keras.Input(shape=(28, 28, 1), name='img',dtype=tf.bfloat16)
x = tf.keras.layers.UpSampling2D(3)(input)
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32800,"""Autograph capabilities and limitations"" link in AutoGraph doc is outdated","## URL(s) with the issue:
https://www.tensorflow.org/guide/autograph

## Description of issue (what needs changing):
""Autograph capabilities and limitations."" link points an out dated doc at:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/LIMITATIONS.md

### Clear description

- There was a similar bug in https://github.com/tensorflow/tensorflow/issues/22280 in 2018.
- This issue was reported [in the closed bug](https://github.com/tensorflow/tensorflow/issues/22280#issuecomment-527699503)) but this comment did not get a reply. Thus, I'm filing a new bug.

### Submit a pull request?

Actually, this issue was fixed on Aug 6th, 1.5+ month ago.
https://github.com/tensorflow/docs/commit/c29c6fa8202fcd8da4ab8d8072c5f7dacf7c160a#diff-039832f2dbb662a37df6e0fa64ebe35e"
32799,Inconsistency between keras model.predict() and model.call(),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.0.0-rc0-101-gd2d2566 2.0.0-rc1
- Python version: Python 3.7.4
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

I am trying to extract the features generated by the model's DenseFeatures layer by creating a new model based on the inputs of the original model and the outputs of the DenseFeature layers. When using this submodel I notice an inconsistency between model.call() and model.predict(). If we provide extra columns to the model in model.predict() the model behaves reliably by processing the inputs using keras.engine.training_utils.standardize_input_data(). However if model.call() is used this doesn't happen and the model orders the columns based on alphabetical order as per nest.flatten(), thus the model crashes with the type cast exception if the extra columns are of a different format. Please see the code provided to reproduce this issue. I can work around this by using the predict() method. 

**Describe the expected behavior**

Expected behaviour is that the two methods to use a model are consistent. The model.call() processes the inputs using the standardize_input_data() method to ensure the input data is as expected.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
import urllib.request as request
import tensorflow as tf
import pandas as pd


def download_data(download_path: str):
    url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data'
    header_line = 'age,sex,cp,trestbps,chol,fbs,restecg,thalach,exang,oldpeak,slope,ca,thal,target\n'

    # Download the data and add the column headers
    request.urlretrieve(url, 'heart.data')
    with open(download_path, 'w') as output:
        output.write(header_line)
        with open('heart.data', 'r') as input_data:
            output.writelines(input_data.readlines())


def preprocess_df(df, categorical_columns):
    """"""Ensure categorical columns are treated as string inputs""""""
    col_types = {key : str for key in categorical_columns.keys()}
    df = df.astype(col_types)
    return df


def df_to_dataset(dataframe, target_column='target', shuffle=True, batch_size=5):
    """"""Dataset preparation code from the tensorflow tutorial""""""
    dataframe = dataframe.copy()
    labels = dataframe.pop(target_column)
    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), tf.one_hot(labels, depth=2)))
    if shuffle:
        ds = ds.shuffle(buffer_size=len(dataframe))
    ds = ds.batch(batch_size)
    return ds


if __name__ == '__main__':

    # Download dataset
    data_path = 'heart.csv'
    download_data(data_path)
    df = pd.read_csv(data_path)

    # Setup feature columns
    numeric_columns = [""age"", ""chol""]
    categorical_columns = {""thal"": df['thal'].unique()}
    feature_columns = {}
    inputs = {}
    for feature_name in numeric_columns:
        feature_columns[feature_name] = tf.feature_column.numeric_column(feature_name)
        inputs[feature_name] = tf.keras.Input(name=feature_name, shape=(), dtype=tf.float32)

    for feature_name, vocab in categorical_columns.items():
        vocab.sort()
        cat_col = tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocab)
        feature_columns[feature_name] = tf.feature_column.indicator_column(cat_col)
        inputs[feature_name] = tf.keras.Input(name=feature_name, shape=(), dtype=tf.string)

    # Prepare input data
    df = preprocess_df(df, categorical_columns)
    batch_size = 5  # A small batch sized is used for demonstration purposes
    train_ds = df_to_dataset(df, target_column='target', batch_size=batch_size)

    # Create Model
    input_tensors = []
    feature_names = list(feature_columns.keys())
    feature_names.sort()
    for column_name in feature_names:
        features = feature_columns[column_name]
        x = tf.keras.layers.DenseFeatures(features, name=f'{column_name}_feature')(inputs)
        input_tensors.append(x)

    x = tf.keras.layers.Concatenate()(input_tensors)
    x = tf.keras.layers.Dense(units=24, activation='relu', name='dense_0')(x)
    x = tf.keras.layers.Dense(units=24, activation='relu', name='dense_1')(x)
    y_pred = tf.keras.layers.Dense(units=2, activation='softmax', name='output_layer')(x)
    model = tf.keras.Model(inputs=inputs, outputs=y_pred)

    model.compile(optimizer=tf.keras.optimizers.Adam(),
                  loss=tf.keras.losses.CategoricalCrossentropy(),
                  metrics=['accuracy'],
                  run_eagerly=True)

    model.summary()
    model.fit(train_ds, epochs=1)

    # Create a new keras model to extract the features
    # the actual model is using.
    outputs = []
    for column_name in feature_names:
        outputs.append(model.get_layer(f'{column_name}_feature').output)

    feature_extractor = tf.keras.Model(model.input, outputs)

    for i, (X, _) in enumerate(train_ds):

        # Predict works as it calls keras.engine.training_utils.standardize_input_data() internally
        # this modifies the input so that if extra columns are passed they are removed and column
        # order is changed as per the model inputs specified.
        # out = feature_extractor.predict(X)

        # Model call() doesn't use the above util method and thus fails
        # as the ordering of the input columns doesn't match the input
        out = feature_extractor(X)
        print(out)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
Traceback (most recent call last):
  File ""check_model_clone_4.py"", line 103, in <module>
    out = feature_extractor(X)
  File ""/home/user/anaconda3/envs/rc1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 891, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""/home/user/anaconda3/envs/rc1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py"", line 707, in call
    convert_kwargs_to_constants=base_layer_utils.call_context().saving)
  File ""/home/user/anaconda3/envs/rc1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py"", line 859, in _run_internal_graph
    output_tensors = layer(computed_tensors, **kwargs)
  File ""/home/user/anaconda3/envs/rc1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 891, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""/home/user/anaconda3/envs/rc1/lib/python3.7/site-packages/tensorflow_core/python/feature_column/dense_features.py"", line 133, in call
    self._state_manager)
  File ""/home/user/anaconda3/envs/rc1/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py"", line 4357, in get_dense_tensor
    return transformation_cache.get(self, state_manager)
  File ""/home/user/anaconda3/envs/rc1/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py"", line 2608, in get
    transformed = column.transform_feature(self, state_manager)
  File ""/home/user/anaconda3/envs/rc1/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py"", line 4296, in transform_feature
    transformation_cache, state_manager)
  File ""/home/user/anaconda3/envs/rc1/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py"", line 3771, in get_sparse_tensors
    transformation_cache.get(self, state_manager), None)
  File ""/home/user/anaconda3/envs/rc1/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py"", line 2608, in get
    transformed = column.transform_feature(self, state_manager)
  File ""/home/user/anaconda3/envs/rc1/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py"", line 3749, in transform_feature
    return self._transform_input_tensor(input_tensor, state_manager)
  File ""/home/user/anaconda3/envs/rc1/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py"", line 3726, in _transform_input_tensor
    prefix='column_name: {} input_tensor'.format(self.key))
  File ""/home/user/anaconda3/envs/rc1/lib/python3.7/site-packages/tensorflow_core/python/feature_column/utils.py"", line 58, in assert_string_or_int
    '{} dtype must be string or integer. dtype: {}.'.format(prefix, dtype))
ValueError: column_name: thal input_tensor dtype must be string or integer. dtype: <dtype: 'float32'>.
```
"
32798,can not watch the version of tensorflow 1.14.0 by tensorflow.__version__,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
here are some information about installing some packages like python and tensorflow[CPU] 
```
Python 3.7.4 [Clang 10.0.0 (clang-1000.11.45.5)] 
tensorflow                       1.14.0
tensorflow-estimator             1.14.0
tensorflow-federated             0.7.0
tensorflow-model-optimization    0.1.2
```
`import tensorflow as tf` is ok, however, i want to check tensorflow version by `tf.__version__` , errors happened:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/util/module_wrapper.py"", line 171, in __getattr__
    raise e
  File ""/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/util/module_wrapper.py"", line 168, in __getattr__
    attr = getattr(self._tfmw_wrapped_module, name)
AttributeError: module 'tensorflow' has no attribute '__version__'
```"
32796,When does tensorflow lite support 3d cnn?,"
I trained 3d cnn network on tensorflow, now I want to port to Android phone, but browsed the document and found that tensorflow-lite does not support 3d cnn operation, when will it support it?"
32795,tf.config namespace not available in rc1/2 (was available in beta0),"**System information**
tf_env.txt attached

**The Issue**
tf.config namespace is not available in TF 2.0 rc1 and rc2 while it was available in beta0 releases and before. I see no indication that this namespace is being deprecated or changed in the documentation.

```
AttributeError Traceback (most recent call last)
<ipython-input-1-a65a6ef9c8ca> in <module>
      1 import tensorflow as tf
----> 2 print(tf.config.experimental.list_physical_devices('CPU'))
AttributeError: module 'tensorflow' has no attribute 'config'
```

**Expected behavior**
API functionality under tf.config namespace should be available unless otherwise indicated in the documentation. The latest API info says it should be there:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/config/experimental/list_physical_devices

**Code to reproduce the issue**
```python
import tensorflow as tf
print(tf.config.experimental.list_physical_devices('CPU'))
```
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/3650032/tf_env.txt)
"
32793,"device_lib.list_local_devices() InvalidArgumentError: Invalid device ordinal value (1). Valid range is [0, 0].","
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): gLinux
- TensorFlow installed from (source or binary): pip3 installed
- TensorFlow version (use command below): 2.0.0-rc1
- Python version: 3.6.2
- CUDA/cuDNN version: 10.0, 7.6.3
- GPU model and memory:

output of ```nvidia-smi``` from the terminal:
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 430.34       Driver Version: 430.34       CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Quadro P1000        Off  | 00000000:65:00.0  On |                  N/A |
| 37%   52C    P0    N/A /  N/A |   1289MiB /  4037MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  TITAN RTX           Off  | 00000000:B3:00.0 Off |                  N/A |
| 41%   29C    P8    14W / 280W |   1155MiB / 24220MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
```

NOTE: in the above output it shows that it is using ```CUDA Version: 10.``` but my ```LD_LIBRARY_PATH``` environment variable is pointing to CUDA 10.0.

Snippet of code that cause the problem:
```python
import tensorflow as tf
from tensorflow.python.client import device_lib

device_lib.list_local_devices()
```

error message:
```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-2-b6f1169dc7e5> in <module>
     2 from tensorflow.python.client import device_lib
     3 
---> 4 device_lib.list_local_devices()

~/.local/lib/python3.6/site-packages/tensorflow_core/python/client/device_lib.py in list_local_devices(session_config)
     39   return [
     40       _convert(s)
---> 41       for s in pywrap_tensorflow.list_devices(session_config=session_config)
     42   ]

~/.local/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py in list_devices(session_config)
   2247     return ListDevicesWithSessionConfig(session_config.SerializeToString())
   2248   else:
-> 2249     return ListDevices()
   2250 
   2251 

InvalidArgumentError: Invalid device ordinal value (1). Valid range is [0, 0].
	while setting up XLA_GPU_JIT device number 1
```

Potential cause and current workaround:
In the terminal output I notice that because the Quadro P1000  in my workstation only has 5 multiprocessor and so by default tf will not use it (minimum 8), so I added the following line to my ```.bashrc```

```
export TF_MIN_GPU_MULTIPROCESSOR_COUNT=5
```

and run ```source .bashrc``` and it works. Another potential solution if I don't want to set the min GPU Multiprocessor count I can remove the Quadro P1000 from my workstation. I suspect that there is an inconsistency within list_local_devices() that fetch all GPUs in the workstation but didn't update base on min gpu multiprocessor count rule. So I run another experiment to see if I can reproduce the error after setting ```TF_MIN_GPU_MULTIPROCESSOR_COUNT``` to 5

The below code will reproduce the same error:

```python
import tensorflow as tf
from tensorflow.python.client import device_lib

gpus = tf.config.experimental.list_physical_devices('GPU')
tf.config.experimental.set_visible_devices(gpus[0], 'GPU')
device_lib.list_local_devices()
```
This will produce the same error but if we call ```device_lib.list_local_devices()``` before calling ```tf.config.experimental.set_visible_devices(gpus[0], 'GPU')``` and then we call ```device_lib.list_local_devices()``` again, there is no error. I suspect that maybe setting device to visible may interact weirdly with list_local_devices().


"
32791,Mixnet graph freezing issue,"**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.14.0
- Python version: 3.7.4
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
I have been trying to covert the [Mixnet](https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet/mixnet) model to Core ML using the [tfcoreml](https://github.com/tf-coreml/tf-coreml) converter. I froze the graph, then tried converting it, and that's when I get thrown with the following error:
```
1199 ops in the final graph.

Loading the TF graph...
Graph Loaded.
Now finding ops in the TF graph that can be dropped for inference
Collecting all the 'Const' ops from the graph, by running it....
---------------------------------------------------------------------------
OutOfRangeError                           Traceback (most recent call last)
~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1355     try:
-> 1356       return fn(*args)
   1357     except errors.OpError as e:

~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1340       return self._call_tf_sessionrun(
-> 1341           options, feed_dict, fetch_list, target_list, run_metadata)
   1342 

~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)
   1428         self._session, options, feed_dict, fetch_list, target_list,
-> 1429         run_metadata)
   1430 

OutOfRangeError: Node 'mixnet-s/mixnet_model/stem/batch_normalization/FusedBatchNormV3' (type: 'Add', num of outputs: 1) does not have output 5

During handling of the above exception, another exception occurred:

OutOfRangeError                           Traceback (most recent call last)
<ipython-input-12-6b7b8d71599d> in <module>
     48                      mlmodel_path=ml_model,
     49                      output_feature_names=['logits'],
---> 50                      input_name_shape_dict={})

~/anaconda3/envs/tf/lib/python3.7/site-packages/tfcoreml/_tf_coreml_converter.py in convert(tf_model_path, mlmodel_path, output_feature_names, input_name_shape_dict, image_input_names, is_bgr, red_bias, green_bias, blue_bias, gray_bias, image_scale, class_labels, predicted_feature_name, predicted_probabilities_output, add_custom_layers, custom_conversion_functions, use_coreml_3)
    619       predicted_probabilities_output=predicted_probabilities_output,
    620       add_custom_layers=add_custom_layers,
--> 621       custom_conversion_functions=custom_conversion_functions)

~/anaconda3/envs/tf/lib/python3.7/site-packages/tfcoreml/_tf_coreml_converter.py in _convert_pb_to_mlmodel(tf_model_path, mlmodel_path, output_feature_names, input_name_shape_dict, image_input_names, is_bgr, red_bias, green_bias, blue_bias, gray_bias, image_scale, class_labels, predicted_feature_name, predicted_probabilities_output, add_custom_layers, custom_conversion_functions)
    260     else:
    261       const_tensor_names = []
--> 262     tensors_evaluated = sess.run(tensors, feed_dict=input_feed_dict)
    263     for i in range(len(tensor_names)):
    264       if tensor_names[i] not in SHAPE_DICT:

~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    948     try:
    949       result = self._run(None, fetches, feed_dict, options_ptr,
--> 950                          run_metadata_ptr)
    951       if run_metadata:
    952         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1171     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1172       results = self._do_run(handle, final_targets, final_fetches,
-> 1173                              feed_dict_tensor, options, run_metadata)
   1174     else:
   1175       results = []

~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1348     if handle is None:
   1349       return self._do_call(_run_fn, feeds, fetches, targets, options,
-> 1350                            run_metadata)
   1351     else:
   1352       return self._do_call(_prun_fn, handle, feeds, fetches)

~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1368           pass
   1369       message = error_interpolation.interpolate(message, self._graph)
-> 1370       raise type(e)(node_def, op, message)
   1371 
   1372   def _extend_graph(self):

OutOfRangeError: Node 'mixnet-s/mixnet_model/stem/batch_normalization/FusedBatchNormV3' (type: 'Add', num of outputs: 1) does not have output 5

```
I presume this is an issue with the way I am freezing the graph. The checkpoint was taken from the official [Mixnet](https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet/mixnet) implementation. I am relatively new to TF and am still learning. Please help me resolve this issue and let me know in case further information is needed. Suggestions for converting to Core ML are also welcome. Thanks in advance!

**Code to reproduce the issue**
You might have to install TF 1.14.0
```python
import tensorflow as tf
import tfcoreml as tf_converter
from tensorflow.compat.v1 import graph_util

tf.logging.set_verbosity(tf.logging.ERROR)

# Download the checkpoint
model_name = 'mixnet-s'
!wget https://storage.googleapis.com/cloud-tpu-checkpoints/mixnet/{model_name}.tar.gz -O {model_name}.tar.gz
!tar -zxvf {model_name}.tar.gz
ckpt_dir = model_name

def _freeze_graph(model_folder):
    # Retrieve the checkpoint fullpath
    checkpoint = tf.train.get_checkpoint_state(model_folder)
    input_checkpoint = checkpoint.model_checkpoint_path
    
    # File fullname of the freezed graph
    absolute_model_folder = ""/"".join(input_checkpoint.split('/')[:-1])
    output_graph = absolute_model_folder + ""/model.pb""

    # Before exporting the graph, get the output nodes
    output_node_names = ""logits""

    # Clear devices to allow TF to control on which device it will load operations
    clear_devices = True
    
    # Import the meta graph and retrieve a Saver
    saver = tf.train.import_meta_graph(input_checkpoint + '.meta',
                                       clear_devices=clear_devices)

    # Retrieve the protobuf graph definition
    graph = tf.get_default_graph()
    input_graph_def = graph.as_graph_def()
    
    # Start a session and restore the graph weights
    with tf.Session() as sess:
        saver.restore(sess, input_checkpoint)

        # Use a built-in TF helper to export variables to constants
        output_graph_def = graph_util.convert_variables_to_constants(
            sess,
            input_graph_def, 
            output_node_names.split("","")
        ) 

        # Serialize and dump the output graph to the filesystem
        with tf.gfile.GFile(output_graph, ""wb"") as f:
            f.write(output_graph_def.SerializeToString())
        print(""%d ops in the final graph."" % len(output_graph_def.node))

_freeze_graph(ckpt_dir)

ml_model = 'mixnet-s/mixnet_core.mlmodel'
frozen_model = 'mixnet-s/model.pb'

tf_converter.convert(tf_model_path=frozen_model,
                     mlmodel_path=ml_model,
                     output_feature_names=['logits'],
                     input_name_shape_dict={})
```"
32790,ModuleNotFoundError: No module named 'tensorflow.examples.tutorials',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (macOS Mojave 10.14.6):
- TensorFlow installed from (source):
- TensorFlow version: 2.0.0-rc1
- Python version: Python 3.7.4
- Installed using virtualenv? pip? conda?: pip

**Describe the problem**
I am trying to run a basic mnist classifier tutorial using tensorflow (without keras) and I am getting the following error:
Upon running:
`from tensorflow.examples.tutorials.mnist import input_data`
I get the following error:
ModuleNotFoundError: No module named 'tensorflow.examples.tutorials'

Any help of how to fix this issue is highly appreciated.

"
32787,Noisy loss in distributed training,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12 & 1.13
- Python version: 3.6.5
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
I am working on distributed learning in tensorflow through estimators API using below simple code template:

```
runConfig = tf.estimator.RunConfig(session_config=config, 
                                model_dir=log_dir,
                                save_summary_steps=1,
                                save_checkpoints_steps=train_steps)
estimator = tf.keras.estimator.model_to_estimator(model, model_dir=log_dir, config=runConfig) 
train_spec = tf.estimator.TrainSpec(input_fn=lambda: read_dataset(...), max_steps=...) 
eval_spec = tf.estimator.EvalSpec(input_fn=lambda: read_dataset(...), start_delay_secs=1,
                                throttle_secs=1, steps=None) 
tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
```

My model is defined as a simple 2-layer LSTM model in keras, and read_dataset() functions return datasets that I use for training and validation purposes.  The training/validation data files  and model directory are set in a shared place available to all workers. The whole code is exactly the same for servers (ps, chief, and worker) except the task setting in TF_CONFIG.
When I train model in single-worker configuration, the loss graph I see in tensorboard is gradually downward and reasonable.

![single](https://user-images.githubusercontent.com/17579773/65541564-975a6a80-dedb-11e9-9a56-f77d2b5906a8.jpg)

When using two machines of one chief and one worker, the total run time is less (as expected) but the loss graph is very noisy and higher than single-server case.

![double](https://user-images.githubusercontent.com/17579773/65541692-d092da80-dedb-11e9-96bc-a3a3b0f0d92e.jpg)

I expected to see the same performance in both cases, but it seems that training in the second server ruins the situation. Is there any special provision/setting/additional code that I should include in my work?"
32786,Keras Layer.compute_output_shape calls `build` with wrong input shape,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0rc2
- Python version: 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

When using `Layer.compute_output_shape` or `Layer.compute_output_signature` on a Layer with a `build` function, the `build(input_shape=...)` argument is always set to None.

**Describe the expected behavior**

The `input_shape` should be set to the shape passed to `compute_output_shape`.

**Code to reproduce the issue**
``` python
import tensorflow as tf

shape = (1, 2)


class MyLayer(tf.keras.layers.Layer):
    def build(self, input_shape):
        print(input_shape)
        assert input_shape == shape

    def call(self, inputs):
        return inputs


layer = MyLayer()
layer.compute_output_shape(shape)
```
"
32785,Converting tf fashion mnist model with Supported Operations to TFLite breaks due to Operand Shape Mismatch,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac 10.12.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A
- TensorFlow installed from (source or binary): binary (pip)
- TensorFlow version (use command below):2.0.0-rc1
- Python version: 3.7.1
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
When converting model to tflite a supported operation is attempted to be fused and a dimension error occurs.
**Describe the expected behavior**
When converting model to tflite the supported operation fuses correctly.
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
import tensorflow.compat.v1 as tf
import numpy as np
tf.disable_v2_behavior()

fashion_mnist = tf.keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
train_images = train_images/255.0
test_images = test_images/255.0

input = tf.placeholder(dtype=tf.float32, shape=(None,28,28), name=""input"")
reshape = tf.reshape(input, [tf.shape(input)[0],784])
w1 = tf.Variable(tf.random_normal([128,784], dtype=tf.float32), name=""w1"")
b1 = tf.Variable(tf.random_normal([128], dtype=tf.float32), name=""b1"")
layer_one_unbiased = tf.matmul(w1,tf.transpose(reshape))
print(layer_one_unbiased, b1)
layer_one_biased = tf.add(tf.transpose(layer_one_unbiased),b1)
print(layer_one_biased)
activated_layer_one = tf.nn.relu(layer_one_biased)
w2 = tf.Variable(tf.random_normal([10,128]), name=""w2"")
b2 = tf.Variable(tf.random_normal([10], name=""b2""))
print(activated_layer_one)
layer_two_unbiased = tf.matmul(w2,tf.transpose(activated_layer_one))
print(tf.transpose(layer_two_unbiased), b2)
print(layer_two_unbiased, b2)
layer_two_biased = tf.add(tf.transpose(layer_two_unbiased), b2)
print(layer_two_biased)
predictions = tf.nn.softmax(layer_two_biased, name=""final"")
labels= tf.placeholder(dtype=tf.int32, shape=(None))
loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=layer_two_biased)
optimizer = tf.train.AdamOptimizer()
train = optimizer.minimize(loss)
label_acc = tf.one_hot(labels,10)
accuracy = 1 - tf.norm(tf.subtract(predictions,label_acc), axis=1)/2
accuracy_averaged = tf.math.reduce_mean(accuracy)
sess = tf.Session()
sess.run(tf.global_variables_initializer())
feed_dict = {input: train_images, labels: train_labels}
sess.run(train,feed_dict=feed_dict)
feed_dict2 = {input: test_images, labels: test_labels}
print(""Test accuracy"", sess.run(accuracy_averaged, feed_dict = feed_dict2))
print(""Training accuracy"",sess.run(accuracy_averaged, feed_dict = feed_dict))
saver =tf.train.Saver()
save_path = saver.save(sess, ""model.ckpt"")
tf.io.write_graph(sess.graph, """", 'train.pbtxt')
#converter = tf.lite.TFLiteConverter.from_session(sess, [input], [predictions])
#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
#tflite_model = converter.convert()
#open('converted_model.tflite', ""wb"").write(tflite_model)
sess.close()

# Above code is the code used to generate the model
freeze_graph.py is then run on the output of the above code.
# The below code is used to convert
import tensorflow.compat.v1 as tf
import numpy as np
tf.disable_v2_behavior()
graph_def_file = ""freeze_graph.pbtxt""
input_arrays = [""input""]
output_arrays = [""final""]
converter = tf.lite.TFLiteConverter.from_frozen_graph(
  graph_def_file, input_arrays, output_arrays)
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)	
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
WARNING: Logging before flag parsing goes to stderr.
W0924 12:49:42.307250 140736348050368 deprecation.py:323] From /Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2019-09-24 12:49:42.308630: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-09-24 12:49:42.324658: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fb5c10b8740 executing computations on platform Host. Devices:
2019-09-24 12:49:42.324684: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2019-09-24 12:49:42.336278: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
2019-09-24 12:49:42.336378: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2019-09-24 12:49:42.344845: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize
2019-09-24 12:49:42.344864: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 27 nodes (-4), 27 edges (-4), time = 3.984ms.
2019-09-24 12:49:42.344871: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 27 nodes (0), 27 edges (0), time = 0.999ms.
Traceback (most recent call last):
  File ""conversion.py"", line 11, in <module>
    tflite_model = converter.convert()
  File ""/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py"", line 983, in convert
    **converter_kwargs)
  File ""/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py"", line 449, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File ""/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py"", line 200, in toco_convert_protos
    raise ConverterError(""See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: See console for info.
2019-09-24 12:49:44.232035: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 14 operators, 27 arrays (0 quantized)
2019-09-24 12:49:44.232297: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 14 operators, 27 arrays (0 quantized)
2019-09-24 12:49:44.232628: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 13 operators, 26 arrays (0 quantized)
2019-09-24 12:49:44.232720: F tensorflow/lite/toco/graph_transformations/fuse_binary_into_preceding_affine.cc:62] Operand shape mismatch.
Fatal Python error: Aborted

Current thread 0x00007fffbc0853c0 (most recent call first):
  File ""/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 52 in execute
  File ""/Users/t.capes/miniconda3/lib/python3.7/site-packages/absl/app.py"", line 251 in _run_main
  File ""/Users/t.capes/miniconda3/lib/python3.7/site-packages/absl/app.py"", line 300 in run
  File ""/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py"", line 40 in run
  File ""/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 89 in main
  File ""/Users/t.capes/miniconda3/bin/toco_from_protos"", line 10 in <module>"
32784,module 'tensorflow_core._api.v2.nn' has no attribute 'rnn_cell',"File ""F:\AI\RedditChatBot\nmt-chatbot-master\nmt\gnmt_model.py"", line 262, in <module>
    class GNMTAttentionMultiCell(tf.nn.rnn_cell.MultiRNNCell):
AttributeError: module 'tensorflow_core._api.v2.nn' has no attribute 'rnn_cell'"
32783,GELU activation Functions?,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): TF 2.0.

- Are you willing to contribute it (Yes/No):
Yes, pending review, I would be happy to write this in.

**Describe the feature and the current behavior/state.**
Create a high-level API implementing the GELU activation function.  This was the activation function used in BERT[0], and the GELU authors prove its success on  a number of benchmarks like CIFAR-10[1].

Gelu is not mentioned in a search on tensorflow.org[2]. 

[0] ""We use a gelu activation (Hendrycks and Gimpel, 2016) rather than the standard relu, following OpenAI GPT."" https://arxiv.org/pdf/1810.04805.pdf
[1]""Ultimately, the GELU obtains a median error rate of 7.89%, the ReLU obtains 8.16%, and the ELU obtains 8.41%.""  https://arxiv.org/pdf/1606.08415.pdf
[2] https://www.tensorflow.org/s/results?q=gelu

**Will this change the current api? How?**
Yes, this would expand the API. A good way to implement it would be adding to activations.py [3] The implementation used in BERT, which may need some modification, is here [4].

[3] https://github.com/tensorflow/tensorflow/blob/00fad90125b18b80fe054de1055770cfb8fe4ba3/tensorflow/python/keras/activations.py
[4] https://github.com/google-research/bert/blob/master/modeling.py#L264

**Who will benefit with this feature?**
All developers writing custom layers, particularly those using keras.layers.Dense. 

After implementing, we will not have to worry about any optimization problems which may come from mixing activation functions training on top of BERT models.

**Any Other info.**
"
32782,Building TF 2.0 from source returns 1.14 wheel,"- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0 (master branch)
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: 10.1/7.6.3
- GPU model and memory: RTX2080TI

I am trying to build tensorflow 2.0 with GPU support. During the ./configure I enable CUDA (the locations are properly found). Afterwards I start the build with:

bazel build --config=v2 //tensorflow/tools/pip_package:build_pip_package

I had to link python to python3, since Ubuntu 18 has no python 2 and bazel has problems, when no python link is available. The build finishes without any fails.

./bazel-bin/tensorflow/tools/pip_package/build_pip_package --nightly_flag /tmp/tensorflow_pkg

After building the wheel with the above line I can only build tensorflow 1.14

$ ls /tmp/tensorflow_pkg/
tf_nightly-1.14.0-cp36-cp36m-linux_x86_64.whl

Have been trying it multiple times now, even on a clean and fresh Ubuntu 18 setup.

Cheers
"
32781,GPU Conv Fusion Code is disabled in grappler remapper.cc,"The grappler remapper optimizer disabled the ability to fuse conv related op in GPU.

Since related code is inside 
#ifndef INTEL_MKL
#endif 

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/remapper.cc"
32780,How to use Tensorflowlite SDK with iOS 8,"Hello @petewarden ,

I want to support tensorlowlite to my existing project which is having iOS 8 min target. But I am getting error while including it using pod targeted OS version does not support use of thread local variables in __ZNK6tflite13eigen_support12_GLOBAL__N_122EigenThreadPoolWrapper15CurrentThreadIdEv for architecture

Can anyone help me how to fix this issue?
Any help is appreciable.

Thanks
"
32779,Parallel Mapping of Dataset Does Not Ensure Same Random Numbers,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
- OS Platform and Distribution: Windows10
- TensorFlow installed from: binary
- TensorFlow version: 1.15.0-dev20190812 
- Python version: 3.7 (64-bit)

**Describe the current behavior**
I am doing data augmentations using Dataset map method. To ensure that the images and the labels (labels are images, too) are transformed in the same way (e.g. the same rotation, zooming, etc.), I set the random seeds at the same value.

In each of the following test, I run the test program for an adequately long time and use `tf.assert_equal` to capture the bug. If I do not set parallel in the map method, it goes well and augmentation yields the same random numbers. But if I set `num_parallel_calls=4` or `num_parallel_calls=tf.data.experimental.AUTOTUNE`, the assertion failure is soon triggered. This inconsistency does not happen for all samples but happens brokenly.

**Describe the expected behavior**
Fix the RNG bug in parallel mode maybe?
"
32778,Incorrect number of weights in custom layer containing keras layer,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.0, 7.1
- GPU model and memory: Quadro

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
```
class CustomLayer(tf.keras.layers.Layer):
    def __init__(self, dims, **kwargs):
        super(CustomLayer, self).__init__(**kwargs)
        self.dims = dims
        self.dense = tf.keras.layers.Dense(units=self.dims)
        
    def call(self, x, mask=None):
        return self.dense(x)
    
    def get_config(self):
        return {""dims"": self.dims}
    
inp = tf.keras.layers.Input(shape=(10,))
x = CustomLayer(32)(inp)
model = tf.keras.models.Model(inp, x)
print(model.summary())
```

The summary contains no weights:
```
Layer (type)                 Output Shape              Param #   
=================================================================
input_5 (InputLayer)         (None, 10)                0         
_________________________________________________________________
custom_layer_2 (CustomLayer) (None, 32)                0         
=================================================================
Total params: 0
Trainable params: 0
Non-trainable params: 0
```

If I try to train, it doesn't seem to do anything
```
model.compile(""sgd"",""mse"")
X,Y = np.random.rand(1000,10), np.zeros((1000, 32))
model.fit(X,Y, epochs=20)
```
```
Epoch 1/20
1000/1000 [==============================] - 0s 28us/step - loss: 0.1625
Epoch 2/20
1000/1000 [==============================] - 0s 25us/step - loss: 0.1625
Epoch 3/20
1000/1000 [==============================] - 0s 29us/step - loss: 0.1625
Epoch 4/20
1000/1000 [==============================] - 0s 28us/step - loss: 0.1625
Epoch 5/20
1000/1000 [==============================] - 0s 24us/step - loss: 0.1625
Epoch 6/20
1000/1000 [==============================] - 0s 24us/step - loss: 0.1625
Epoch 7/20
1000/1000 [==============================] - 0s 26us/step - loss: 0.1625
Epoch 8/20
1000/1000 [==============================] - 0s 22us/step - loss: 0.1625
Epoch 9/20
1000/1000 [==============================] - 0s 22us/step - loss: 0.1625
Epoch 10/20
1000/1000 [==============================] - 0s 25us/step - loss: 0.1625
Epoch 11/20
1000/1000 [==============================] - 0s 22us/step - loss: 0.1625
Epoch 12/20
1000/1000 [==============================] - 0s 25us/step - loss: 0.1625
Epoch 13/20
1000/1000 [==============================] - 0s 23us/step - loss: 0.1625
Epoch 14/20
1000/1000 [==============================] - 0s 26us/step - loss: 0.1625
Epoch 15/20
1000/1000 [==============================] - 0s 23us/step - loss: 0.1625
Epoch 16/20
1000/1000 [==============================] - 0s 26us/step - loss: 0.1625
Epoch 17/20
1000/1000 [==============================] - 0s 23us/step - loss: 0.1625
Epoch 18/20
1000/1000 [==============================] - 0s 23us/step - loss: 0.1625
Epoch 19/20
1000/1000 [==============================] - 0s 23us/step - loss: 0.1625
Epoch 20/20
1000/1000 [==============================] - 0s 23us/step - loss: 0.1625
```

**Describe the expected behavior**

I would like to use Keras layers within the custom layer, and have them trainable.
"
32776,"Sym-links in nvcc C compiler path causes ""undeclared inclusion(s) in rule '//tensorflow/core:lib_hash_crc32c_accelerate_internal'""","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Redhat Enterprise Linux 7.x
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.14.0
- Python version: 3.7.4
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): 4.9.2
- CUDA/cuDNN version: 10.0.130/7.4.2.24
- GPU model and memory: Tesla P100

**Describe the problem**

We have compilers installed in `/shared/ucl/apps/gcc/4.9.2`.  Unfortunately `/shared` is a sym-link to `/lustre/shared`.  Attempting to build Tensorflow with Cuda support results in this sym-link being inconsistently de-referenced meaning that some rules refer to the `/shared` location while the system include paths refer to `/lustre/shared`.  This seems to be an issue only with the C compiler used by nvcc, not with the one used to build the rest of the code.

Telling the configure script to use `/lustre/shared/ucl/apps/gcc/4.9.2/bin/gcc` as the nvcc c compiler *works around the problem* but is less than ideal as this path is different on different clusters. It culd also be an issue if a user on a mult-user system uses their own install of GCC in `/home`where `/home` is a sym-link. This appears to be a bug in the way the configure script and/or bazel deals with sym-links.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

The script at https://github.com/owainkenwayucl/install-scripts/blob/master/scripts/tensorflow/tensorflow-1.14.0-py37-gpu_install

But effectively:

```
CONFIG_OPTS=""--config=cuda --copt=-march=broadwell --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --copt=-O3""
export TF_CUDA_PATHS=/shared/ucl/apps/cuda/10.0.130/gnu-4.9.2,/shared/ucl/apps/cudnn/7.4.2.24/10.0/cuda
./configure

bazel build --verbose_failures $CONFIG_OPTS //tensorflow/tools/pip_package:build_pip_package 
```

Answering ""yes"" to build with cuda and selecting defaults for compute capability, nccl etc.

"
32775,Undefined symbol tensorflow::functor::CSRSparseMatrixTranspose,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 1.14 (github master aebcf430467918e646c2fb65372bdd9eeb320745)
- Python version: 3.7.4
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.29
- GCC/Compiler version (if compiling from source): 8.3.0
- CUDA/cuDNN version: 10.1/7.6
- GPU model and memory: GTX1070 8GB



**Describe the problem**
Build process completes without issue, installs without issue. However, after issuing `python -c ""import tensorflow""` the following output is observed

```
Traceback (most recent call last):
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.7/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.7/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /usr/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow7functor24CSRSparseMatrixTransposeIN5Eigen9GpuDeviceESt7complexIfEEclEPNS_15OpKernelContextEbRKNS_15CSRSparseMatrixEPS9_

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/usr/lib/python3.7/site-packages/tensorflow/__init__.py"", line 98, in <module>
    from tensorflow_core import *
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/usr/lib/python3.7/site-packages/tensorflow/__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""/usr/lib/python3.7/site-packages/tensorflow/__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""/usr/lib/python3.7/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.7/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.7/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /usr/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow7functor24CSRSparseMatrixTransposeIN5Eigen9GpuDeviceESt7complexIfEEclEPNS_15OpKernelContextEbRKNS_15CSRSparseMatrixEPS9_


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```sh
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
./bazel-bin/tensorflow/tools/pip_package/build_pip_package --nightly_flag /tmp/tensorflow_pkg
sudo -H pip install tensorflow_pkg/tf_nightly-1.14.0-cp37-cp37m-linux_x86_64.whl
python -c ""import tensorflow""
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```sh
$ cat .tf_configure.bazelrc
build --action_env PYTHON_BIN_PATH=""/usr/bin/python""
build --action_env PYTHON_LIB_PATH=""/usr/lib/python3.7/site-packages""
build --python_path=""/usr/bin/python""
build:xla --define with_xla_support=true
build --config=xla
build --config=tensorrt
build --action_env TF_CUDA_VERSION=""10.1""
build --action_env TF_CUDNN_VERSION=""7.6""
build --action_env TF_TENSORRT_VERSION=""6.0.1""
build --action_env TF_NCCL_VERSION=""2.4.8""
build --action_env TF_CUDA_PATHS=""/opt/cuda,/usr""
build --action_env CUDA_TOOLKIT_PATH=""/opt/cuda""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""6.1""
build --action_env GCC_HOST_COMPILER_PATH=""/usr/bin/gcc-8""
build --config=cuda
build:opt --copt=-march=native
build:opt --copt=-Wno-sign-compare
build:opt --copt=-mtune=native
build:opt --copt=-O3
build:opt --copt=-fPIC
build:opt --copt=-DNDEBUG
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
build:v2 --define=tf_api_version=2
test --flaky_test_attempts=3
test --test_size_filters=small,medium
test --test_tag_filters=-benchmark-test,-no_oss,-oss_serial
test --build_tag_filters=-benchmark-test,-no_oss
test --test_tag_filters=-gpu
test --build_tag_filters=-gpu
build --action_env TF_CONFIGURE_IOS=""0""
```

Missing symbol is
`tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, std::complex<float> >::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)`

```sh
$ cat /proc/cpuinfo | head -n5
processor	: 0
vendor_id	: AuthenticAMD
cpu family	: 23
model		: 8
model name	: AMD Ryzen 7 2700X Eight-Core Processor
```
"
32774,Are complex variables supported in eager mode?,"**System information**
- TensorFlow version (you are using): 2.0.0-rc1
- Are you willing to contribute it (Yes/No): No


**Describe the feature and the current behavior/state.**

Currently (unless I'm not doing things correctly) it seems that there is no support for complex variables. This fails:
```python
    a = tf.Variable(4.0+1.0j) # a = tf.Variable(4.0) works

    def f(x):
      return x*a

    loss = lambda: tf.abs(f(5)-5)

    optimizer = tf.optimizers.Adam(learning_rate=0.1)
    optimizer.minimize(loss, [a])
```

> WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.complex128

and 
> ValueError: Invalid type tf.complex128 for Variable:0, expected: [tf.float32, tf.float64, tf.float16, tf.bfloat16].


**Will this change the current api? How?**
Potentially yes

**Who will benefit with this feature?**
All of the users who used to rely on complex numbers!
"
32772,Training parameter to model passed as None in 1.14.0,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary (pip)
- TensorFlow version (use command below): v1.14.0-0-g87989f6959 1.14.0
- Python version: 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.1/7.5
- GPU model and memory: GTX 1080Ti

**Describe the current behavior**
Training parameter to model passed as None

**Describe the expected behavior**
Training parameter should be True in training and False in inference

**Code to reproduce the issue**
```
import numpy as np
import tensorflow as tf


class MyModel(tf.keras.Model):
  def __init__(self):
    super(MyModel, self).__init__()
    self.dense = tf.keras.layers.Dense(4)

  def call(self, inputs, training=False):
    print('Training', training)
    return self.dense(inputs)
    
    
class Gen(tf.keras.utils.Sequence):
    def __len__(self):
        return 10
    
    def __getitem__(self, i):
        return np.ones((32, 100)), np.ones((32, 4))
    
model = MyModel()
model.compile(optimizer=tf.train.AdagradOptimizer(0.001), loss='categorical_crossentropy', metrics=['accuracy'])


# training should be False
model.build(input_shape=(32, 100))

# training should be True
model.fit_generator(generator=Gen(), epochs=1, validation_data=Gen())
```

**Other info / logs**
Outputs:
```
Training False
Training None
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
10/10 [==============================] - 1s 127ms/step - loss: 17.8394 - acc: 1.0000 - val_loss: 16.1181 - val_acc: 1.0000
<tensorflow.python.keras.callbacks.History at 0x7f4754342e80>
```
"
32771,"TensorFlow is broken, unusable on Raspberry Pi","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, code is pasted below
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian Buster
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Raspberry Pi 4 with 4 GB RAM
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.7.3
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

On Windows I use python-3.7.4 with TensorFlow 1.13.1, 1.14.0 and 2.0.0rc1

**Describe the current behavior**
I train a model on Windows (gen_test_train_data.py, then train_model.py), using the same TF version that's available on RPi. I can test the model on Windows just fine using model_visual_test.py.
If I transfer the model to the RPi, I cannot load it. I get mysterious errors when running model_visual_test.py:

```
Traceback (most recent call last):
  File ""model_visual_test.py"", line 16, in <module>
    model = keras.models.load_model('saved_model.h5')
  File ""/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py"", line 140, in load_model
    loader_impl.parse_saved_model(filepath)
  File ""/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/python/saved_model/loader_impl.py"", line 83, in parse_saved_model
    constants.SAVED_MODEL_FILENAME_PB))
OSError: SavedModel file does not exist at: saved_model.h5/{saved_model.pbtxt|saved_model.pb}
```

It doesn't work even if I train the model on the RPi, by running gen_test_train_data.py, then run train_model.py, which takes forever. It completes training, but then it fails to load with

```
Train on 8000 samples
8000/8000 [==============================] - 1305s 163ms/sample - loss: 0.5675 - acc: 0.8609
Traceback (most recent call last):
  File ""train_model.py"", line 122, in <module>
    keras.models.save_model(model, 'saved_model.h5')
  File ""/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py"", line 104, in save_model
    model, filepath, overwrite, include_optimizer)
  File ""/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py"", line 73, in save_model_to_hdf5
    raise ImportError('`save_model` requires h5py.')
ImportError: `save_model` requires h5py.
```

No, that's wrong. I do have the h5 module installed on the Raspberry Pi:

```
$ pip3 list --user | grep h5
h5py                 2.10.0
```

In other words, a model trained on the RPi fails to load on the RPi. A model trained on Win10 runs fine on Win10, but fails to load on the RPi. This is with a code that works perfect on Windows 10 and macOS with all kinds of TF versions between 1.13.1 and 2.0.0rc1, GPU or CPU versions.

**Describe the expected behavior**
I mean, it should just work, shouldn't it? Why is this so hard?

**Code to reproduce the issue**
https://github.com/FlorinAndrei/TensorAim/tree/tf-bug-report
Run gen_test_train_data.py. Then on that run train_model.py. It fails to run its own model, generated on RPi.
Or run gen_test_train_data.py on Windows, run train_model.py on Windows, transfer the model on the RPi, then run model_visual_test.py on it. It will fail to load.
I will leave this branch untouched so you can test the bug report.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32770,Asset file not exported in the SavedModel when using tf.lookup.StaticVocabularyTable,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Ubuntu 16.04
- TensorFlow installed from: binary
- TensorFlow version: 2.0.0rc2
- Python version: 3.6.6

**Describe the current behavior**

When using `tf.lookup.StaticVocabularyTable`, the asset file is not exported in the SavedModel assets directory. However, it is correctly saved when using `tf.lookup.StaticHashTable`.

**Describe the expected behavior**

The vocabulary file should be saved in the assets directory of the SavedModel.

**Code to reproduce the issue**

```python
import os
import shutil
import tensorflow as tf

class Model(tf.keras.layers.Layer):

    def __init__(self, vocabulary_path):
        super(Model, self).__init__()
        initializer = tf.lookup.TextFileInitializer(
            vocabulary_path,
            tf.string,
            tf.lookup.TextFileIndex.WHOLE_LINE,
            tf.int64,
            tf.lookup.TextFileIndex.LINE_NUMBER)
        self.table = tf.lookup.StaticVocabularyTable(initializer, num_oov_buckets=1)
        #self.table = tf.lookup.StaticHashTable(initializer, 0)

    def call(self, tokens):
        return self.table.lookup(tokens)

    @tf.function(input_signature=(tf.TensorSpec([None], dtype=tf.string),))
    def serve(self, tokens):
        return self(tokens)


vocabulary_path = ""/tmp/vocab.txt""
with open(vocabulary_path, ""w"") as vocabulary_file:
    vocabulary_file.write(""a\nb\nc\n"")

model = Model(vocabulary_path)

export_dir = ""/tmp/model""
if os.path.exists(export_dir):
    shutil.rmtree(export_dir)
tf.saved_model.save(model, export_dir, signatures=model.serve)
assets = os.listdir(os.path.join(export_dir, ""assets""))
assert len(assets) == 1
```

**Other info / logs**

The code above raises an `AssertionError` as the assets directory is empty."
32769,"model.fit() raise FailedPreconditionError after applying ""tf.distribute.MirroredStrategy()"" ","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):tf1.14/tf2.0.0rc1
- Python version:3.7.4
- Bazel version (if compiling from source):N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: 9.0/7.6.0(tf1.14.0) , 10.0.130/7.6.0(tf2.0.0rc1) 
- GPU model and memory: 8G*2

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I create and complie the keras model under `with mirrored_strategy.scope():` ，then i got ""FailedPreconditionError "" when i excute model.fit();
if i move the code out from `with mirrored_strategy.scope():`, everything is ok.

**Describe the expected behavior**
when I create and complie the keras model under `with mirrored_strategy.scope():` , do not raise the exception and  multi GPU  can come into use.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
mirrored_strategy = tf.distribute.MirroredStrategy()
with mirrored_strategy.scope():
    # Create the base model from the pre-trained model MobileNet V2
    base_model = tf.keras.applications.InceptionV3(input_shape=IMG_SHAPE,
                                                   include_top=False,
                                                   weights='imagenet')
    base_model.trainable = False
    model = tf.keras.Sequential([
      base_model,
      keras.layers.GlobalAveragePooling2D(),
      keras.layers.Dense(len(label_names), activation='sigmoid')
    ])

    model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),
                  loss='binary_crossentropy',
                  metrics=['categorical_accuracy'])

callbacks = [
    tf.keras.callbacks.TensorBoard(log_dir=log_dir),
    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,
                                       save_weight_only=True)
]

epochs = 10
history = model.fit(train_dataset, 
                    epochs=epochs,
                    callbacks=callbacks)
```


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

--------------------------------------------------------------------
FailedPreconditionError                   Traceback (most recent call last)
<ipython-input-28-35cd9d3d6f8f> in <module>
      9                               validation_data=validation_ds,
     10                               validation_steps=validation_steps,
---> 11                               callbacks=callbacks)

/usr/local/miniconda3/envs/tf_2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    726         max_queue_size=max_queue_size,
    727         workers=workers,
--> 728         use_multiprocessing=use_multiprocessing)
    729 
    730   def evaluate(self,

/usr/local/miniconda3/envs/tf_2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
    683         validation_steps=validation_steps,
    684         validation_freq=validation_freq,
--> 685         steps_name='steps_per_epoch')
    686 
    687   def evaluate(self,

/usr/local/miniconda3/envs/tf_2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)
    297           else:
    298             actual_inputs = ins()
--> 299           batch_outs = f(actual_inputs)
    300         except errors.OutOfRangeError:
    301           if is_dataset:

/usr/local/miniconda3/envs/tf_2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py in __call__(self, inputs)
   3465 
   3466     fetched = self._callable_fn(*array_vals,
-> 3467                                 run_metadata=self.run_metadata)
   3468     self._call_fetch_callbacks(fetched[-len(self._fetches):])
   3469     output_structure = nest.pack_sequence_as(

/usr/local/miniconda3/envs/tf_2.0/lib/python3.7/site-packages/tensorflow_core/python/client/session.py in __call__(self, *args, **kwargs)
   1470         ret = tf_session.TF_SessionRunCallable(self._session._session,
   1471                                                self._handle, args,
-> 1472                                                run_metadata_ptr)
   1473         if run_metadata:
   1474           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

FailedPreconditionError: 2 root error(s) found.
  (0) Failed precondition: Error while reading resource variable conv2d_64/kernel from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/conv2d_64/kernel/N10tensorflow3VarE does not exist.
	 [[{{node inception_v3_1/conv2d_64/Conv2D/ReadVariableOp}}]]
	 [[batch_normalization_42/Const/_189]]
  (1) Failed precondition: Error while reading resource variable conv2d_64/kernel from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/conv2d_64/kernel/N10tensorflow3VarE does not exist.
	 [[{{node inception_v3_1/conv2d_64/Conv2D/ReadVariableOp}}]]
0 successful operations.
1 derived errors ignored.
"
32768,"RHEL pip install 2.0-rc issues, No module named tensorflow_core.keras","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos 7.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.0-rc2, 2.0-rc1
- Python version: 3.6.5
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
Running pip installs the package without issues but fails to import keras when using.
I have tried both gpu and cpu packages with same result.


**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
python3 -m pip install tensorflow-gpu==2.0.0-rc2
from tensorflow import keras
inputs = keras.Input(shape=(784,))

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/pyinstalls/python3.6/tensorflow/__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""/pyinstalls/python3.6/tensorflow/__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""/pyinstalls/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked
ModuleNotFoundError: No module named tensorflow_core.keras

import tensorflow.python.keras
ModuleNotFoundError: No module named tensorflow_core.python
```


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32767,Build Error: failed (Exit 1): crosstool_wrapper_driver_is_not_gcc,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): https://github.com/tensorflow/tensorflow
- TensorFlow version: Both 1.14 & 1.13
- Python version: 3.6
- Installed using virtualenv? pip? conda?: No
- Bazel version (if compiling from source): 0.24.1 for 1.14 & 0.19.2 for 1.13
- GCC/Compiler version (if compiling from source): Output of gcc --version is 7.4.0
- CUDA/cuDNN version: CUDA 10.0 / cuDNN 7.5
- GPU model and memory: 16 GB Volta Arch GPU


When I try to build Tensorflow from source, I get annoying errors with both versions of 1.13 and 1.14. When I try to build 1.13 with the below command;

`bazel build --config=opt --config=nonccl --verbose_failures     //tensorflow/tools/pip_package:build_pip_package     //tensorflow:libtensorflow_cc.so     //tensorflow:libtensorflow_framework.so     --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0`

I get the following error after maybe 3-4 hours:

```
ERROR: /root/tensorflow/tensorflow/contrib/tensorrt/BUILD:268:1: C++ compilation of rule '//tensorflow/contrib/tensorrt:trt_conversion' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda-10.0 \
    CUDNN_INSTALL_PATH=/usr/lib/aarch64-linux-gnu \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc-5 \
    LD_LIBRARY_PATH=/usr/local/cuda-10.0/targets/aarch64-linux/lib: \
    PATH=/root/bazel/output:/usr/local/cuda-10.0/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3.6/dist-packages \
    TENSORRT_INSTALL_PATH=/usr/lib/aarch64-linux-gnu \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=7.2 \
    TF_CUDA_VERSION=10.0 \
    TF_CUDNN_VERSION=7 \
    TF_NCCL_VERSION='' \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL_SYCL=0 \
    TF_NEED_ROCM=0 \
    TF_TENSORRT_VERSION=5.1.6 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/aarch64-opt/bin/tensorflow/contrib/tensorrt/_objs/trt_conversion/convert_nodes.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/tensorflow/contrib/tensorrt/_objs/trt_conversion/convert_nodes.pic.o' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -DTF_USE_SNAPPY -iquote . -iquote bazel-out/aarch64-opt/genfiles -iquote bazel-out/aarch64-opt/bin -iquote external/com_google_absl -iquote bazel-out/aarch64-opt/genfiles/external/com_google_absl -iquote bazel-out/aarch64-opt/bin/external/com_google_absl -iquote external/bazel_tools -iquote bazel-out/aarch64-opt/genfiles/external/bazel_tools -iquote bazel-out/aarch64-opt/bin/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/aarch64-opt/genfiles/external/eigen_archive -iquote bazel-out/aarch64-opt/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/aarch64-opt/genfiles/external/local_config_sycl -iquote bazel-out/aarch64-opt/bin/external/local_config_sycl -iquote external/nsync -iquote bazel-out/aarch64-opt/genfiles/external/nsync -iquote bazel-out/aarch64-opt/bin/external/nsync -iquote external/gif_archive -iquote bazel-out/aarch64-opt/genfiles/external/gif_archive -iquote bazel-out/aarch64-opt/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/aarch64-opt/genfiles/external/jpeg -iquote bazel-out/aarch64-opt/bin/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/aarch64-opt/genfiles/external/protobuf_archive -iquote bazel-out/aarch64-opt/bin/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/aarch64-opt/genfiles/external/com_googlesource_code_re2 -iquote bazel-out/aarch64-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/aarch64-opt/genfiles/external/farmhash_archive -iquote bazel-out/aarch64-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/aarch64-opt/genfiles/external/fft2d -iquote bazel-out/aarch64-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/aarch64-opt/genfiles/external/highwayhash -iquote bazel-out/aarch64-opt/bin/external/highwayhash -iquote external/zlib_archive -iquote bazel-out/aarch64-opt/genfiles/external/zlib_archive -iquote bazel-out/aarch64-opt/bin/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/aarch64-opt/genfiles/external/local_config_cuda -iquote bazel-out/aarch64-opt/bin/external/local_config_cuda -iquote external/double_conversion -iquote bazel-out/aarch64-opt/genfiles/external/double_conversion -iquote bazel-out/aarch64-opt/bin/external/double_conversion -iquote external/local_config_tensorrt -iquote bazel-out/aarch64-opt/genfiles/external/local_config_tensorrt -iquote bazel-out/aarch64-opt/bin/external/local_config_tensorrt -isystem external/eigen_archive -isystem bazel-out/aarch64-opt/genfiles/external/eigen_archive -isystem bazel-out/aarch64-opt/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/aarch64-opt/genfiles/external/nsync/public -isystem bazel-out/aarch64-opt/bin/external/nsync/public -isystem external/gif_archive/lib -isystem bazel-out/aarch64-opt/genfiles/external/gif_archive/lib -isystem bazel-out/aarch64-opt/bin/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/aarch64-opt/genfiles/external/protobuf_archive/src -isystem bazel-out/aarch64-opt/bin/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/aarch64-opt/genfiles/external/farmhash_archive/src -isystem bazel-out/aarch64-opt/bin/external/farmhash_archive/src -isystem external/zlib_archive -isystem bazel-out/aarch64-opt/genfiles/external/zlib_archive -isystem bazel-out/aarch64-opt/bin/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/aarch64-opt/genfiles/external/local_config_cuda/cuda -isystem bazel-out/aarch64-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/aarch64-opt/genfiles/external/local_config_cuda/cuda/cuda/include -isystem bazel-out/aarch64-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/aarch64-opt/genfiles/external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/aarch64-opt/bin/external/local_config_cuda/cuda/cuda/include/crt -isystem external/double_conversion -isystem bazel-out/aarch64-opt/genfiles/external/double_conversion -isystem bazel-out/aarch64-opt/bin/external/double_conversion -isystem external/local_config_tensorrt/include -isystem bazel-out/aarch64-opt/genfiles/external/local_config_tensorrt/include -isystem bazel-out/aarch64-opt/bin/external/local_config_tensorrt/include '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections '-march=native' -Wno-sign-compare '-D_GLIBCXX_USE_CXX11_ABI=0' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-DGOOGLE_CUDA=1' '-DGOOGLE_TENSORRT=1' -pthread '-DGOOGLE_CUDA=1' '-DGOOGLE_TENSORRT=1' -c tensorflow/contrib/tensorrt/convert/convert_nodes.cc -o bazel-out/aarch64-opt/bin/tensorflow/contrib/tensorrt/_objs/trt_conversion/convert_nodes.pic.o)
tensorflow/contrib/tensorrt/convert/convert_nodes.cc: In constructor 'tensorflow::tensorrt::convert::TRT_TensorOrWeights::TRT_TensorOrWeights(nvinfer1::DataType, const nvinfer1::Dims&, int)':
tensorflow/contrib/tensorrt/convert/convert_nodes.cc:516:60: error: invalid new-expression of abstract class type 'tensorflow::tensorrt::convert::TRT_TensorOrWeights::SimpleITensor'
     : simple_itensor_(new SimpleITensor(trt_dtype, trt_dims)),
                                                            ^
tensorflow/contrib/tensorrt/convert/convert_nodes.cc:459:28: note:   because the following virtual functions are pure within 'tensorflow::tensorrt::convert::TRT_TensorOrWeights::SimpleITensor':
 class TRT_TensorOrWeights::SimpleITensor : public nvinfer1::ITensor {
                            ^
In file included from ./tensorflow/contrib/tensorrt/log/trt_logger.h:23:0,
                 from ./tensorflow/contrib/tensorrt/convert/convert_nodes.h:26,
                 from tensorflow/contrib/tensorrt/convert/convert_nodes.cc:16:
bazel-out/aarch64-opt/genfiles/external/local_config_tensorrt/tensorrt/include/NvInfer.h:774:18: note: 	virtual bool nvinfer1::ITensor::dynamicRangeIsSet() const
     virtual bool dynamicRangeIsSet() const = 0;
                  ^
bazel-out/aarch64-opt/genfiles/external/local_config_tensorrt/tensorrt/include/NvInfer.h:779:18: note: 	virtual void nvinfer1::ITensor::resetDynamicRange()
     virtual void resetDynamicRange() = 0;
                  ^
bazel-out/aarch64-opt/genfiles/external/local_config_tensorrt/tensorrt/include/NvInfer.h:786:19: note: 	virtual float nvinfer1::ITensor::getDynamicRangeMin() const
     virtual float getDynamicRangeMin() const = 0;
                   ^
bazel-out/aarch64-opt/genfiles/external/local_config_tensorrt/tensorrt/include/NvInfer.h:793:19: note: 	virtual float nvinfer1::ITensor::getDynamicRangeMax() const
     virtual float getDynamicRangeMax() const = 0;

```

And if I try to build 1.14 with the command below;

`bazel build --config=opt --config=nonccl //tensorflow/tools/pip_package:build_pip_package --verbose_failures --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0""`

I get the following error:

```
ERROR: /opt/tf13/tensorflow/tensorflow/lite/kernels/BUILD:286:1: C++ compilation of rule '//tensorflow/lite/kernels:builtin_op_kernels' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /root/.cache/bazel/_bazel_root/8c266c5a221eef177229796f3ca6ace6/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda-10.0/targets/aarch64-linux/lib: \
    PATH=/root/bazel/output:/usr/local/cuda-10.0/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/lite/kernels/_objs/builtin_op_kernels/depthwise_conv.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/lite/kernels/_objs/builtin_op_kernels/depthwise_conv.pic.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -iquote . -iquote bazel-out/host/genfiles -iquote bazel-out/host/bin -iquote external/gemmlowp -iquote bazel-out/host/genfiles/external/gemmlowp -iquote bazel-out/host/bin/external/gemmlowp -iquote external/com_google_absl -iquote bazel-out/host/genfiles/external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/arm_neon_2_x86_sse -iquote bazel-out/host/genfiles/external/arm_neon_2_x86_sse -iquote bazel-out/host/bin/external/arm_neon_2_x86_sse -iquote external/eigen_archive -iquote bazel-out/host/genfiles/external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/genfiles/external/local_config_sycl -iquote bazel-out/host/bin/external/local_config_sycl -iquote external/flatbuffers -iquote bazel-out/host/genfiles/external/flatbuffers -iquote bazel-out/host/bin/external/flatbuffers -iquote external/fft2d -iquote bazel-out/host/genfiles/external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/farmhash_archive -iquote bazel-out/host/genfiles/external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -isystem external/eigen_archive -isystem bazel-out/host/genfiles/external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem tensorflow/lite/schema -isystem bazel-out/host/genfiles/tensorflow/lite/schema -isystem bazel-out/host/bin/tensorflow/lite/schema -isystem external/flatbuffers/include -isystem bazel-out/host/genfiles/external/flatbuffers/include -isystem bazel-out/host/bin/external/flatbuffers/include -isystem external/farmhash_archive/src -isystem bazel-out/host/genfiles/external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 '-march=native' -g0 -DFARMHASH_NO_CXX_STRING -Wno-sign-compare '-Wno-error=reorder' -c tensorflow/lite/kernels/depthwise_conv.cc -o bazel-out/host/bin/tensorflow/lite/kernels/_objs/builtin_op_kernels/depthwise_conv.pic.o)
Execution platform: @bazel_tools//platforms:host_platform
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h: In static member function 'static void tflite::optimized_ops::depthwise_conv::PackMacroBlock<(tflite::DepthwiseConvImplementation)3, (tflite::DepthwiseConvDepthMultiplication)0, 0>::PackMacroBlockNeon(const uint8*, int8*, const tflite::optimized_ops::depthwise_conv::DepthwiseConvDotProdParams*)':
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:5981:24: note: use -flax-vector-conversions to permit conversions between vectors with differing element types or numbers of subparts
           input_data_a = vld1q_u8(input_data_0);
                        ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:5981:24: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:5982:24: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
           input_data_b = vld1q_u8(input_data_0 + 1 * input_depth);
                        ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:5983:24: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
           input_data_c = vld1q_u8(input_data_0 + 2 * input_depth);
                        ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:5984:24: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
           input_data_d = vld1q_u8(input_data_0 + 3 * input_depth);
                        ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:5993:55: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
             work_reg_a = veorq_s8(work_reg_a, sign_bit);
                                                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:5994:55: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
             work_reg_b = veorq_s8(work_reg_b, sign_bit);
                                                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6000:26: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
             input_data_a = vld1q_u8(input_data_0);
                          ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6001:26: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
             input_data_b = vld1q_u8(input_data_0 + 1 * input_depth);
                          ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6009:61: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
             work_reg_a_sp = veorq_s8(work_reg_a_sp, sign_bit);
                                                             ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6010:61: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
             work_reg_b_sp = veorq_s8(work_reg_b_sp, sign_bit);
                                                             ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6012:26: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
             input_data_c = vld1q_u8(input_data_0 + 2 * input_depth);
                          ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6013:26: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
             input_data_d = vld1q_u8(input_data_0 + 3 * input_depth);
                          ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6029:53: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
           work_reg_a = veorq_s8(work_reg_a, sign_bit);
                                                     ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6030:53: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
           work_reg_b = veorq_s8(work_reg_b, sign_bit);
                                                     ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6042:59: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
           work_reg_a_sp = veorq_s8(work_reg_a_sp, sign_bit);
                                                           ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6043:59: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
           work_reg_b_sp = veorq_s8(work_reg_b_sp, sign_bit);
                                                           ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'
   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)
                                                                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6053:26: note: in expansion of macro 'vld1q_lane_s8x8'
           input_data_a = vld1q_lane_s8x8(input_data_0, input_data_a, 0);
                          ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'
   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)
                                                                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6055:15: note: in expansion of macro 'vld1q_lane_s8x8'
               vld1q_lane_s8x8(input_data_0 + 1 * input_depth, input_data_b, 0);
               ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'
   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)
                                                                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6057:15: note: in expansion of macro 'vld1q_lane_s8x8'
               vld1q_lane_s8x8(input_data_0 + 2 * input_depth, input_data_c, 0);
               ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'
   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)
                                                                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6059:15: note: in expansion of macro 'vld1q_lane_s8x8'
               vld1q_lane_s8x8(input_data_0 + 3 * input_depth, input_data_d, 0);
               ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6066:53: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
           work_reg_a = veorq_s8(work_reg_a, sign_bit);
                                                     ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6067:53: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
           work_reg_b = veorq_s8(work_reg_b, sign_bit);
                                                     ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6085:24: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
           input_data_c = vdupq_n_u8(kSignBit);
                        ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'
   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)
                                                                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6086:26: note: in expansion of macro 'vld1q_lane_s8x8'
           input_data_a = vld1q_lane_s8x8(input_data_0, input_data_a, 0);
                          ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6087:24: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
           input_data_d = vdupq_n_u8(kSignBit);
                        ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'
   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)
                                                                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6090:17: note: in expansion of macro 'vld1q_lane_s8x8'
                 vld1q_lane_s8x8(input_data_0 + input_depth, input_data_b, 0);
                 ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'
   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)
                                                                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6092:30: note: in expansion of macro 'vld1q_lane_s8x8'
               input_data_c = vld1q_lane_s8x8(input_data_0 + 2 * input_depth,
                              ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6099:53: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
           work_reg_a = veorq_s8(work_reg_a, sign_bit);
                                                     ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6100:53: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
           work_reg_b = veorq_s8(work_reg_b, sign_bit);
                                                     ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h: In static member function 'static void tflite::optimized_ops::depthwise_conv::PackMacroBlock<(tflite::DepthwiseConvImplementation)3, (tflite::DepthwiseConvDepthMultiplication)0, 1>::PackMacroBlockNeon(int32, int32, const uint8*, int8*, const tflite::optimized_ops::depthwise_conv::DepthwiseConvDotProdParams*)':
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6244:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
               input_data_a = vld1q_u8(input_data_0);
                            ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6245:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
               input_data_b = vld1q_u8(input_data_0 + 1 * input_depth);
                            ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6246:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
               input_data_c = vld1q_u8(input_data_0 + 2 * input_depth);
                            ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6247:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
               input_data_d = vld1q_u8(input_data_0 + 3 * input_depth);
                            ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6256:59: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
                 work_reg_a = veorq_s8(work_reg_a, sign_bit);
                                                           ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6257:59: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
                 work_reg_b = veorq_s8(work_reg_b, sign_bit);
                                                           ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6263:30: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
                 input_data_a = vld1q_u8(input_data_0);
                              ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6264:30: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
                 input_data_b = vld1q_u8(input_data_0 + 1 * input_depth);
                              ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6272:65: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
                 work_reg_a_sp = veorq_s8(work_reg_a_sp, sign_bit);
                                                                 ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6273:65: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
                 work_reg_b_sp = veorq_s8(work_reg_b_sp, sign_bit);
                                                                 ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6275:30: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
                 input_data_c = vld1q_u8(input_data_0 + 2 * input_depth);
                              ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6276:30: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
                 input_data_d = vld1q_u8(input_data_0 + 3 * input_depth);
                              ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6292:57: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
               work_reg_a = veorq_s8(work_reg_a, sign_bit);
                                                         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6293:57: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
               work_reg_b = veorq_s8(work_reg_b, sign_bit);
                                                         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6305:63: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
               work_reg_a_sp = veorq_s8(work_reg_a_sp, sign_bit);
                                                               ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6306:63: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
               work_reg_b_sp = veorq_s8(work_reg_b_sp, sign_bit);
                                                               ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'
   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)
                                                                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6316:30: note: in expansion of macro 'vld1q_lane_s8x8'
               input_data_a = vld1q_lane_s8x8(input_data_0, input_data_a, 0);
                              ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'
   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)
                                                                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6317:30: note: in expansion of macro 'vld1q_lane_s8x8'
               input_data_b = vld1q_lane_s8x8(input_data_0 + 1 * input_depth,
                              ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'
   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)
                                                                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6319:30: note: in expansion of macro 'vld1q_lane_s8x8'
               input_data_c = vld1q_lane_s8x8(input_data_0 + 2 * input_depth,
                              ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'
   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)
                                                                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6321:30: note: in expansion of macro 'vld1q_lane_s8x8'
               input_data_d = vld1q_lane_s8x8(input_data_0 + 3 * input_depth,
                              ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6329:57: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
               work_reg_a = veorq_s8(work_reg_a, sign_bit);
                                                         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6330:57: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
               work_reg_b = veorq_s8(work_reg_b, sign_bit);
                                                         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6344:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
               input_data_a = vdupq_n_u8(-input_offset);
                            ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6345:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
               input_data_b = vdupq_n_u8(-input_offset);
                            ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6346:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
               input_data_c = vdupq_n_u8(-input_offset);
                            ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6347:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
               input_data_d = vdupq_n_u8(-input_offset);
                            ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'
   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)
                                                                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6349:32: note: in expansion of macro 'vld1q_lane_s8x8'
                 input_data_a = vld1q_lane_s8x8(input_data_0, input_data_a, 0);
                                ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'
   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)
                                                                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6351:34: note: in expansion of macro 'vld1q_lane_s8x8'
                   input_data_b = vld1q_lane_s8x8(input_data_0 + input_depth,
                                  ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'
   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)
                                                                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6354:36: note: in expansion of macro 'vld1q_lane_s8x8'
                     input_data_c = vld1q_lane_s8x8(
                                    ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6362:57: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
               work_reg_a = veorq_s8(work_reg_a, sign_bit);
                                                         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6363:57: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
               work_reg_b = veorq_s8(work_reg_b, sign_bit);
                                                         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6389:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
               input_data_a = vdupq_n_u8(-input_offset);
                            ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6390:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
               input_data_b = vld1q_u8(input_data_0 + 1 * input_depth);
                            ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6391:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
               input_data_c = vld1q_u8(input_data_0 + 2 * input_depth);
                            ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6392:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
               input_data_d = vld1q_u8(input_data_0 + 3 * input_depth);
                            ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6401:59: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
                 work_reg_a = veorq_s8(work_reg_a, sign_bit);
                                                           ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6402:59: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
                 work_reg_b = veorq_s8(work_reg_b, sign_bit);
                                                           ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6408:30: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
                 input_data_a = vdupq_n_u8(-input_offset);
                              ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6409:30: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
                 input_data_b = vld1q_u8(input_data_0 + 1 * input_depth);
                              ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6417:65: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
                 work_reg_a_sp = veorq_s8(work_reg_a_sp, sign_bit);
                                                                 ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6418:65: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
                 work_reg_b_sp = veorq_s8(work_reg_b_sp, sign_bit);
                                                                 ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6420:30: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
                 input_data_c = vld1q_u8(input_data_0 + 2 * input_depth);
                              ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6421:30: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
                 input_data_d = vld1q_u8(input_data_0 + 3 * input_depth);
                              ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6437:57: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
               work_reg_a = veorq_s8(work_reg_a, sign_bit);
                                                         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6438:57: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
               work_reg_b = veorq_s8(work_reg_b, sign_bit);
                                                         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6450:63: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
               work_reg_a_sp = veorq_s8(work_reg_a_sp, sign_bit);
                                                               ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6451:63: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
               work_reg_b_sp = veorq_s8(work_reg_b_sp, sign_bit);
                                                               ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6461:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
               input_data_a = vdupq_n_u8(-input_offset);
                            ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'
   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)
                                                                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6462:30: note: in expansion of macro 'vld1q_lane_s8x8'
               input_data_b = vld1q_lane_s8x8(input_data_0 + 1 * input_depth,
                              ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'
   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)
                                                                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6464:30: note: in expansion of macro 'vld1q_lane_s8x8'
               input_data_c = vld1q_lane_s8x8(input_data_0 + 2 * input_depth,
                              ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'
   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)
                                                                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6466:30: note: in expansion of macro 'vld1q_lane_s8x8'
               input_data_d = vld1q_lane_s8x8(input_data_0 + 3 * input_depth,
                              ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6474:57: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
               work_reg_a = veorq_s8(work_reg_a, sign_bit);
                                                         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6475:57: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
               work_reg_b = veorq_s8(work_reg_b, sign_bit);
                                                         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6490:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
               input_data_a = vdupq_n_u8(-input_offset);
                            ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6491:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
               input_data_b = vdupq_n_u8(-input_offset);
                            ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6492:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
               input_data_c = vdupq_n_u8(-input_offset);
                            ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6493:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
               input_data_d = vdupq_n_u8(-input_offset);
                            ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'
   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)
                                                                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6496:32: note: in expansion of macro 'vld1q_lane_s8x8'
                 input_data_b = vld1q_lane_s8x8(input_data_0 + input_depth,
                                ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'
   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)
                                                                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6499:34: note: in expansion of macro 'vld1q_lane_s8x8'
                   input_data_c = vld1q_lane_s8x8(input_data_0 + 2 * input_depth,
                                  ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6506:57: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
               work_reg_a = veorq_s8(work_reg_a, sign_bit);
                                                         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6507:57: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
               work_reg_b = veorq_s8(work_reg_b, sign_bit);
                                                         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h: In static member function 'static void tflite::optimized_ops::depthwise_conv::PackMacroBlock<(tflite::DepthwiseConvImplementation)3, (tflite::DepthwiseConvDepthMultiplication)1, 1>::PackMacroBlockNeon(int32, int32, const uint8*, int8*, const tflite::optimized_ops::depthwise_conv::DepthwiseConvDotProdParams*)':
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6647:75: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint64x1_t {aka __vector(1) long unsigned int}' for argument '1' to 'uint64x1_t vshl_u64(uint64x1_t, int64x1_t)'
       padding_mask = vshl_u64(padding_mask, vdup_n_s64(8 * copy_remaining));
                                                                           ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6659:20: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
           work_reg = vld1q_u8(input_block_data + input_block_offset);
                    ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6660:56: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x16_t vextq_s8(int8x16_t, int8x16_t, int)'
           work_reg = vextq_s8(padding_reg, work_reg, 15);
                                                        ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6661:49: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
           work_reg = veorq_s8(work_reg, sign_bit);
                                                 ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6669:20: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
           work_reg =
                    ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6671:49: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
           work_reg = veorq_s8(work_reg, sign_bit);
                                                 ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6679:25: error: cannot convert 'uint8x8_t {aka __vector(8) unsigned char}' to 'int8x8_t {aka __vector(8) signed char}' in assignment
           half_work_reg =
                         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6681:70: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x8_t vget_low_s8(int8x16_t)'
           half_work_reg = veor_s8(half_work_reg, vget_low_s8(sign_bit));
                                                                      ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6701:25: error: cannot convert 'uint8x8_t {aka __vector(8) unsigned char}' to 'int8x8_t {aka __vector(8) signed char}' in assignment
           half_work_reg =
                         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6705:76: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint64x1_t {aka __vector(1) long unsigned int}' for argument '1' to 'uint64x1_t vshl_u64(uint64x1_t, int64x1_t)'
               vshl_u64(half_work_reg, vdup_n_s64(-8 * (8 - copy_remaining)));
                                                                            ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6707:60: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x8_t vget_low_s8(int8x16_t)'
               vbsl_s8(padding_mask, vget_low_s8(padding_reg), half_work_reg);
                                                            ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6709:70: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x8_t vget_low_s8(int8x16_t)'
           half_work_reg = veor_s8(half_work_reg, vget_low_s8(sign_bit));
                                                                      ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6729:75: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint64x1_t {aka __vector(1) long unsigned int}' for argument '1' to 'uint64x1_t vshl_u64(uint64x1_t, int64x1_t)'
       padding_mask = vshl_u64(padding_mask, vdup_n_s64(8 * copy_remaining));
                                                                           ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:48:67: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'int32x2_t {aka __vector(2) int}' for argument '2' to 'int32x2_t vld1_lane_s32(const int32_t*, int32x2_t, int)'
   vld1_lane_s32(reinterpret_cast<const int32*>(src), reg, lane_num)
                                                                   ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6741:27: note: in expansion of macro 'vld1_lane_8x4'
           half_work_reg = vld1_lane_8x4(input_block_data + input_block_offset,
                           ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6743:58: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x8_t vget_low_s8(int8x16_t)'
           half_work_reg = vext_s8(vget_low_s8(padding_reg), half_work_reg, 7);
                                                          ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6744:70: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x8_t vget_low_s8(int8x16_t)'
           half_work_reg = veor_s8(half_work_reg, vget_low_s8(sign_bit));
                                                                      ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'
   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)
                                                                ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6746:11: note: in expansion of macro 'vst1_lane_8x4'
           vst1_lane_8x4(scratch_data, half_work_reg, 0);
           ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:48:67: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'int32x2_t {aka __vector(2) int}' for argument '2' to 'int32x2_t vld1_lane_s32(const int32_t*, int32x2_t, int)'
   vld1_lane_s32(reinterpret_cast<const int32*>(src), reg, lane_num)
                                                                   ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6753:15: note: in expansion of macro 'vld1_lane_8x4'
               vld1_lane_8x4(input_block_data + input_block_offset + copy_done,
               ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6755:70: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x8_t vget_low_s8(int8x16_t)'
           half_work_reg = veor_s8(half_work_reg, vget_low_s8(sign_bit));
                                                                      ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'
   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)
                                                                ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6759:11: note: in expansion of macro 'vst1_lane_8x4'
           vst1_lane_8x4(scratch_data + start_width + copy_done, half_work_reg,
           ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:48:67: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'int32x2_t {aka __vector(2) int}' for argument '2' to 'int32x2_t vld1_lane_s32(const int32_t*, int32x2_t, int)'
   vld1_lane_s32(reinterpret_cast<const int32*>(src), reg, lane_num)
                                                                   ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6776:27: note: in expansion of macro 'vld1_lane_8x4'
           half_work_reg = vld1_lane_8x4(
                           ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6781:76: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint64x1_t {aka __vector(1) long unsigned int}' for argument '1' to 'uint64x1_t vshl_u64(uint64x1_t, int64x1_t)'
               vshl_u64(half_work_reg, vdup_n_s64(-8 * (4 - copy_remaining)));
                                                                            ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6783:60: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x8_t vget_low_s8(int8x16_t)'
               vbsl_s8(padding_mask, vget_low_s8(padding_reg), half_work_reg);
                                                            ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6785:70: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x8_t vget_low_s8(int8x16_t)'
           half_work_reg = veor_s8(half_work_reg, vget_low_s8(sign_bit));
                                                                      ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'
   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)
                                                                ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6789:11: note: in expansion of macro 'vst1_lane_8x4'
           vst1_lane_8x4(scratch_data + start_width + copy_done, half_work_reg,
           ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'
   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)
                                                                ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6798:9: note: in expansion of macro 'vst1_lane_8x4'
         vst1_lane_8x4(scratch_data + start_width + copy_done, half_work_reg, 0);
         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'
   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)
                                                                ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6799:9: note: in expansion of macro 'vst1_lane_8x4'
         vst1_lane_8x4(scratch_data + start_width + copy_done + 4, half_work_reg,
         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'
   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)
                                                                ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6801:9: note: in expansion of macro 'vst1_lane_8x4'
         vst1_lane_8x4(scratch_data + start_width + copy_done + 8, half_work_reg,
         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'
   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)
                                                                ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6803:9: note: in expansion of macro 'vst1_lane_8x4'
         vst1_lane_8x4(scratch_data + start_width + copy_done + 12,
         ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6818:23: error: cannot convert 'uint8x8_t {aka __vector(8) unsigned char}' to 'int8x8_t {aka __vector(8) signed char}' in assignment
         half_work_reg = vdup_n_u8(-input_offset);
                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6831:68: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x8_t vget_low_s8(int8x16_t)'
         half_work_reg = veor_s8(half_work_reg, vget_low_s8(sign_bit));
                                                                    ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'
   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)
                                                                ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6842:9: note: in expansion of macro 'vst1_lane_8x4'
         vst1_lane_8x4(scratch_data_base + scratch_data_offset + 4,
         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'
   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)
                                                                ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6844:9: note: in expansion of macro 'vst1_lane_8x4'
         vst1_lane_8x4(scratch_data_base + scratch_data_offset + 8,
         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'
   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)
                                                                ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6846:9: note: in expansion of macro 'vst1_lane_8x4'
         vst1_lane_8x4(scratch_data_base + scratch_data_offset + 12,
         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'
   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)
                                                                ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6848:9: note: in expansion of macro 'vst1_lane_8x4'
         vst1_lane_8x4(scratch_data_base + scratch_data_offset + 16,
         ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6857:75: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint64x1_t {aka __vector(1) long unsigned int}' for argument '1' to 'uint64x1_t vshl_u64(uint64x1_t, int64x1_t)'
       padding_mask = vshl_u64(padding_mask, vdup_n_s64(8 * copy_remaining));
                                                                           ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6859:57: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint8x8_t {aka __vector(8) unsigned char}' for argument '2' to 'uint8x8_t vset_lane_u8(uint8_t, uint8x8_t, int)'
         padding_mask = vset_lane_u8(255, padding_mask, 0);
                                                         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6864:54: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint64x1_t {aka __vector(1) long unsigned int}' for argument '1' to 'uint64x1_t vshl_n_u64(uint64x1_t, int)'
           half_work_reg = vshl_n_u64(half_work_reg, 8);
                                                      ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6871:54: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'int64x1_t {aka __vector(1) long int}' for argument '1' to 'int64x1_t vshl_n_s64(int64x1_t, int)'
           half_work_reg = vshl_n_s64(half_work_reg, 8);
                                                      ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6874:58: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x8_t vget_low_s8(int8x16_t)'
             vbsl_s8(padding_mask, vget_low_s8(padding_reg), half_work_reg);
                                                          ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6876:68: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x8_t vget_low_s8(int8x16_t)'
         half_work_reg = veor_s8(half_work_reg, vget_low_s8(sign_bit));
                                                                    ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'
   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)
                                                                ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6880:9: note: in expansion of macro 'vst1_lane_8x4'
         vst1_lane_8x4(scratch_data_base + scratch_data_offset, half_work_reg,
         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'
   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)
                                                                ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6888:9: note: in expansion of macro 'vst1_lane_8x4'
         vst1_lane_8x4(scratch_data_base + scratch_data_offset + 4,
         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'
   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)
                                                                ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6890:9: note: in expansion of macro 'vst1_lane_8x4'
         vst1_lane_8x4(scratch_data_base + scratch_data_offset + 8,
         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'
   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)
                                                                ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6892:9: note: in expansion of macro 'vst1_lane_8x4'
         vst1_lane_8x4(scratch_data_base + scratch_data_offset + 12,
         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'
   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)
                                                                ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6894:9: note: in expansion of macro 'vst1_lane_8x4'
         vst1_lane_8x4(scratch_data_base + scratch_data_offset + 16,
         ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h: In static member function 'static void tflite::optimized_ops::depthwise_conv::PackMacroBlock<(tflite::DepthwiseConvImplementation)3, (tflite::DepthwiseConvDepthMultiplication)1, 0>::PackMacroBlockNeon(int32, int32, const uint8*, int8*, const tflite::optimized_ops::depthwise_conv::DepthwiseConvDotProdParams*)':
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6993:20: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment
           work_reg =
                    ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6995:49: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'
           work_reg = veorq_s8(work_reg, sign_bit);
                                                 ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7002:25: error: cannot convert 'uint8x8_t {aka __vector(8) unsigned char}' to 'int8x8_t {aka __vector(8) signed char}' in assignment
           half_work_reg =
                         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7004:70: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x8_t vget_low_s8(int8x16_t)'
           half_work_reg = veor_s8(half_work_reg, vget_low_s8(sign_bit));
                                                                      ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7023:25: error: cannot convert 'uint8x8_t {aka __vector(8) unsigned char}' to 'int8x8_t {aka __vector(8) signed char}' in assignment
           half_work_reg =
                         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7027:76: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint64x1_t {aka __vector(1) long unsigned int}' for argument '1' to 'uint64x1_t vshl_u64(uint64x1_t, int64x1_t)'
               vshl_u64(half_work_reg, vdup_n_s64(-8 * (8 - copy_remaining)));
                                                                            ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7029:70: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x8_t vget_low_s8(int8x16_t)'
           half_work_reg = veor_s8(half_work_reg, vget_low_s8(sign_bit));
                                                                      ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:48:67: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'int32x2_t {aka __vector(2) int}' for argument '2' to 'int32x2_t vld1_lane_s32(const int32_t*, int32x2_t, int)'
   vld1_lane_s32(reinterpret_cast<const int32*>(src), reg, lane_num)
                                                                   ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7057:15: note: in expansion of macro 'vld1_lane_8x4'
               vld1_lane_8x4(input_block_data + input_block_offset + copy_done,
               ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7059:70: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x8_t vget_low_s8(int8x16_t)'
           half_work_reg = veor_s8(half_work_reg, vget_low_s8(sign_bit));
                                                                      ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'
   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)
                                                                ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7062:11: note: in expansion of macro 'vst1_lane_8x4'
           vst1_lane_8x4(scratch_data + copy_done, half_work_reg, 0);
           ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:48:67: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'int32x2_t {aka __vector(2) int}' for argument '2' to 'int32x2_t vld1_lane_s32(const int32_t*, int32x2_t, int)'
   vld1_lane_s32(reinterpret_cast<const int32*>(src), reg, lane_num)
                                                                   ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7078:27: note: in expansion of macro 'vld1_lane_8x4'
           half_work_reg = vld1_lane_8x4(
                           ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7083:76: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint64x1_t {aka __vector(1) long unsigned int}' for argument '1' to 'uint64x1_t vshl_u64(uint64x1_t, int64x1_t)'
               vshl_u64(half_work_reg, vdup_n_s64(-8 * (4 - copy_remaining)));
                                                                            ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7085:70: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x8_t vget_low_s8(int8x16_t)'
           half_work_reg = veor_s8(half_work_reg, vget_low_s8(sign_bit));
                                                                      ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'
   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)
                                                                ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7088:11: note: in expansion of macro 'vst1_lane_8x4'
           vst1_lane_8x4(scratch_data + copy_done, half_work_reg, 0);
           ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'
   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)
                                                                ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7094:9: note: in expansion of macro 'vst1_lane_8x4'
         vst1_lane_8x4(scratch_data + copy_done, half_work_reg, 0);
         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'
   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)
                                                                ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7095:9: note: in expansion of macro 'vst1_lane_8x4'
         vst1_lane_8x4(scratch_data + copy_done + 4, half_work_reg, 0);
         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'
   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)
                                                                ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7096:9: note: in expansion of macro 'vst1_lane_8x4'
         vst1_lane_8x4(scratch_data + copy_done + 8, half_work_reg, 0);
         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'
   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)
                                                                ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7097:9: note: in expansion of macro 'vst1_lane_8x4'
         vst1_lane_8x4(scratch_data + copy_done + 12, half_work_reg, 0);
         ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7107:54: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint64x1_t {aka __vector(1) long unsigned int}' for argument '1' to 'uint64x1_t vshl_n_u64(uint64x1_t, int)'
           half_work_reg = vshl_n_u64(half_work_reg, 8);
                                                      ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7114:68: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x8_t vget_low_s8(int8x16_t)'
         half_work_reg = veor_s8(half_work_reg, vget_low_s8(sign_bit));
                                                                    ^
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,
                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,
                 from tensorflow/lite/kernels/depthwise_conv.cc:28:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'
   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)
                                                                ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7118:9: note: in expansion of macro 'vst1_lane_8x4'
         vst1_lane_8x4(scratch_data_base + scratch_data_offset, half_work_reg,
         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'
   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)
                                                                ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7124:9: note: in expansion of macro 'vst1_lane_8x4'
         vst1_lane_8x4(scratch_data_base + scratch_data_offset + 4,
         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'
   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)
                                                                ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7126:9: note: in expansion of macro 'vst1_lane_8x4'
         vst1_lane_8x4(scratch_data_base + scratch_data_offset + 8,
         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'
   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)
                                                                ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7128:9: note: in expansion of macro 'vst1_lane_8x4'
         vst1_lane_8x4(scratch_data_base + scratch_data_offset + 12,
         ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'
   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)
                                                                ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7130:9: note: in expansion of macro 'vst1_lane_8x4'
         vst1_lane_8x4(scratch_data_base + scratch_data_offset + 16,
         ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```

What is the problem here? It is so frustrating to wait 4 hours and see it explodes at the exact same point. I am also using `-flax-vector-conversions` `-fomit-frame-pointer` flags within the tensorflow/lite/tools/make/targets/aarch64_makefile.inc file but this doesn't resolve the issue. What am I missing?
"
32766,model_pruning: why does strip_pruning_vars always show 50% zeros?," I'm trying tensorflow/contrib/model_pruning/examples/cifar10/cifar10_train. Tensorboard shows conv1 sparsity increased to 70%，conv2 sparsity increased to 90%，but  strip_pruning_vars alway shows 50% zeros。
   
![image](https://user-images.githubusercontent.com/11002654/65488056-cd4b1f00-deda-11e9-92fa-9d77d009103c.png)

Use `tf.compat.v1.graph_util.extract_sub_graph`
I0924 14:51:55.225000 140698707568448 graph_util_impl.py:311] Froze 15 variables.
I0924 14:51:55.267543 140698707568448 graph_util_impl.py:364] Converted 15 variables to const ops.
I0924 14:51:55.336094 140698707568448 strip_pruning_vars_lib.py:69] conv1/weights/masked_weight has 4800 values, **50.00% zeros** 
I0924 14:51:55.345871 140698707568448 strip_pruning_vars_lib.py:69] conv2/weights/masked_weight has 102400 values, **50.00% zeros** 
I0924 14:51:55.447343 140698707568448 strip_pruning_vars_lib.py:69] local3/weights/masked_weight has 884736 values, **50.00% zeros** 
I0924 14:51:55.470997 140698707568448 strip_pruning_vars_lib.py:69] local4/weights/masked_weight has 73728 values, **50.00% zeros** 
I0924 14:51:55.495334 140698707568448 strip_pruning_vars_lib.py:69] softmax_linear/weights/masked_weight has 1920 values, **50.00% zeros** 
I0924 14:51:55.537974 140698707568448 strip_pruning_vars.py:73] 
Final graph written to /home/terse/code/programming/tensorflow/model_pruning/cifar_pruning_stripped.pb"
32765,A question about tensorflow lite," I am a machine learning beginner. And I want to know the scale of neural networks and and the number of parameters that tensorflow lite can support  so far, because I have not found a similar answer elsewhere. I look forward to your answer."
32764,ValueError when passing tensors to keras subclass model calls,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 / Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not tested
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.0.0-rc0 / rc1 / rc2
- Python version: 3.7.4
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 10.0 / 7.6.3
- GPU model and memory: Quadro K620

**Describe the current behavior**
Using conditional statements on tensors passed to keras subclassed models without `@tf.function` decorator leads to ValueError Exception
` ValueError: Incompatible shapes between op input and calculated input gradient.  Forward operation: Identity.  Input index: 0. Original input shape: (64, 1024).  Calculated input gradient shape: (32, 1024)`
Neither `tf.cond` or python's if statement work without decorator.

The Model works fine if the call function is decorated with `@tf.function` decorator.
Passing Python Boolean values instead of `tf.constant(True)` works fine, with and without `@tf.function` decorator.
  
**EDIT**: Eager mode works fine. (without any `@tf.function`)

**Describe the expected behavior**
The model should work without mandating `@tf.function` decorator. See the code for details.

**Code to reproduce the issue**
Code that reproduces the problem. [Colab Link](https://colab.research.google.com/drive/1zNGK5sc3JVVveg-5Fe3JE0MVqxgd6h6d)

**Other info / logs**

    ValueError: in converted code:

    <ipython-input-26-76bba83ea439>:12 train_step  *
        gradients_mdan = tape.gradient(model_loss, model.trainable_variables)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/backprop.py:1014 gradient
        unconnected_gradients=unconnected_gradients)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/imperative_grad.py:76 imperative_grad
        compat.as_str(unconnected_gradients.value))
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/backprop.py:138 _gradient_function
        return grad_fn(mock_op, *out_grads)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/cond_v2.py:120 _IfGrad
        true_graph, grads, util.unique_grad_fn_name(true_graph.name))
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/cond_v2.py:395 _create_grad_func
        func_graph=_CondGradFuncGraph(name, func_graph))
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py:915 func_graph_from_py_func
        func_outputs = python_func(*func_args, **func_kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/cond_v2.py:394 <lambda>
        lambda: _grad_fn(func_graph, grads), [], {},
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/cond_v2.py:373 _grad_fn
        src_graph=func_graph)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gradients_util.py:714 _GradientsHelper
        (op.name, i, t_in.shape, in_grad.shape))

    ValueError: Incompatible shapes between op input and calculated input gradient.  Forward operation: Identity.  Input index: 0. Original input shape: (64, 1024).  Calculated input gradient shape: (32, 1024)"
32763,tf.sparse.reduce_sum slower/less memory efficient than (unsorted_)segment_sum,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes (below)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **ubuntu 16.04**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **v1.12.1-9365-gff401a6 1.15.0-dev20190821**
- Python version: **3.6.9**
- CUDA/cuDNN version: **10.0** / ??
- GPU model and memory: **GTX-1070**

**Current behavior**
`tf.sparse.reduce_sum` (result of a custom kernel) **50-200x slower** and 2-5x less memory efficient compared to `tf.math.unsorted_segment_sum` implementation.

**Describe the expected behavior**
Custom kernel performance should be no worse than implementations in terms of other operations.

**Code to reproduce the issue**
A very basic implementation (without support for rank > 2 or multiple summation axes) is as follows.
```python
def seg_sum(sp, axis=1, ordered=False):
    """"""
    math.(unsorted_)segment_sum.

    Args:
        sp: rank 2 sparse tensor
        axis: int, axis along which to sum
        ordered: if True, other axis indices are assumed to be ascending.

    Returns:
        rank 1 dense tensor equivalent to tf.sparse.reduce_sum(sp, axis=axis)
    """"""
    if sp.shape.ndims != 2:
        raise NotImplementedError
    other_axis = 0 if axis in (1, -1) else 1
    if ordered:
        return tf.math.segment_sum(sp.values, sp.indices[:, other_axis])
    else:
        return tf.math.unsorted_segment_sum(sp.values,
                                            sp.indices[:, other_axis],
                                            sp.dense_shape[other_axis])
```
Extending this to support full functionality should not be difficult.

Basic testing/benchmarking script:
```python
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import numpy as np
import tensorflow as tf


def get_data(dense_shape, mean_edges=100, seed=123):
    """"""Generate random sparse matrix data.""""""
    N, M = dense_shape
    r = np.random.RandomState(seed)

    num_edges = int(mean_edges * N)

    flat_index = (r.uniform(size=num_edges) * N * M).astype(np.int64)
    # flat_index = np.concatenate([flat_index, np.arange(0, N * M, M)], axis=0)
    flat_index = np.array(sorted(set(flat_index)), dtype=np.int64)
    i, j = np.unravel_index(flat_index, (N, M))  # pylint: disable=unbalanced-tuple-unpacking
    sparse_indices = np.stack((i, j), axis=-1)
    weights = np.random.uniform(size=(i.shape[0],)).astype(np.float32)
    return sparse_indices, weights


def sparse_sum(sp, axis=1):
    """"""sparse.reduce_sum.""""""
    return tf.sparse.reduce_sum(sp, axis=axis)


def sparse_sum2(sp, axis=1):
    """"""sparse.reduce_sum_sparse -> to_dense.""""""
    return tf.sparse.to_dense(tf.sparse.reduce_sum_sparse(sp, axis=axis))


def seg_sum(sp, axis=1, ordered=False):
    """"""
    math.(unsorted_)segment_sum.

    Args:
        sp: rank 2 sparse tensor
        axis: int, axis along which to sum
        ordered: if True, other axis indices are assumed to be ascending.

    Returns:
        rank 1 dense tensor equivalent to tf.sparse.reduce_sum(sp, axis=axis)
    """"""
    if sp.shape.ndims != 2:
        raise NotImplementedError
    other_axis = 0 if axis in (1, -1) else 1
    if ordered:
        return tf.math.segment_sum(sp.values, sp.indices[:, other_axis])
    else:
        return tf.math.unsorted_segment_sum(sp.values,
                                            sp.indices[:, other_axis],
                                            sp.dense_shape[other_axis])


def compare(dense_shape, axis=1, ordered=False, **kwargs):
    sparse_indices, weights = get_data(dense_shape, **kwargs)
    sparse_indices = tf.constant(sparse_indices, dtype=tf.int64)
    weights = tf.constant(weights, dtype=tf.float32)
    sp = tf.SparseTensor(sparse_indices, weights, dense_shape)
    sparse = sparse_sum(sp, axis=axis)
    sparse2 = sparse_sum2(sp, axis=axis)
    seg = seg_sum(sp, axis=axis, ordered=ordered)
    sparse_grad, = tf.gradients(sparse, weights)
    # sparse2_grad, = tf.gradients(sparse2, weights)
    seg_grad, = tf.gradients(seg, weights)
    err = tf.reduce_max(tf.abs(seg - sparse))
    shape_err = tf.reduce_max(tf.abs(tf.shape(sparse) - tf.shape(seg)))
    err2 = tf.reduce_max(tf.abs(seg - sparse2))
    shape_err2 = tf.reduce_max(tf.abs(tf.shape(sparse2) - tf.shape(seg)))
    grad_err = tf.reduce_max(tf.abs(sparse_grad - seg_grad))
    # grad_err2 = tf.reduce_max(tf.abs(sparse2_grad - seg_grad))

    with tf.Session() as sess:
        err, shape_err, err2, shape_err2, grad_err = sess.run(
            (err, shape_err, err2, shape_err2, grad_err))
    assert (err < 1e-4)
    assert (shape_err == 0)
    assert (shape_err2 == 0)
    assert (grad_err < 1e-4)
    return err


def run_benchmarks(dense_shape, axis=1, ordered=False, **kwargs):
    sparse_indices, weights = get_data(dense_shape, **kwargs)
    sparse_indices = tf.constant(sparse_indices, dtype=tf.int64)
    weights = tf.constant(weights, dtype=tf.float32)
    sp = tf.SparseTensor(sparse_indices, weights, dense_shape)
    sparse = sparse_sum(sp, axis=axis)
    sparse_grad, = tf.gradients(sparse, weights)

    seg = seg_sum(sp, axis=axis, ordered=ordered)
    seg_grad = tf.gradients(seg, weights)

    sparse2 = sparse_sum2(sp, axis=axis)
    # sparse2_grad, = tf.gradients(sparse2, weights)
    # print(sparse2_grad)
    names = []
    time = []
    mem = []

    def update(name, result):
        time.append(result['wall_time'])
        mem.append(result['extras']['allocator_maximum_num_bytes_GPU_0_bfc'])
        names.append(name)

    with tf.Session() as sess:
        print('------------------')
        print('----- SPARSE -----')
        bm = tf.test.Benchmark()
        result = bm.run_op_benchmark(sess, (sparse, sparse_grad))
        update('sparse', result)
        # # no gradients to sparse2 - unfair comparison
        print('------------------')
        print('----- SPARSE2 -----')
        bm = tf.test.Benchmark()
        result = bm.run_op_benchmark(sess, (sparse2,))
        update('sparse2', result)
        print('------------------')
        print('----- SEG ----')
        bm = tf.test.Benchmark()
        result = bm.run_op_benchmark(sess, (seg, seg_grad))
        update('seg', result)

    time = np.array(time)
    i = np.argmin(time)
    best_time = time[i]
    print('Fastest:  {}, {}'.format(names[i], best_time))

    mem = np.array(mem)
    j = np.argmin(mem)
    best_mem = mem[j]
    print('Smallest: {}, {:.2f}mb'.format(names[j], best_mem / (1024**2)))

    print('rel time, rel mem, name')
    for name, t, m in zip(names, time, mem):
        print('{:.3f}, {:.3f}, {}'.format(t / best_time, m / best_mem, name))
    return time, mem, names


axis = 0
ordered = False
dense_shape = (int(1e4), int(1e6))
compare(axis=axis, dense_shape=dense_shape, ordered=ordered)
run_benchmarks(axis=axis, dense_shape=dense_shape, ordered=ordered)

```

**Other info / logs**
```
------------------
----- SPARSE -----
2019-09-24 16:03:05.305709: I tensorflow/core/profiler/lib/profiler_session.cc:205] Profiler session started.
entry {
  name: ""TensorFlowBenchmark.run_op_benchmark""
  iters: 10
  wall_time: 0.2983849048614502
  extras {
    key: ""allocator_maximum_num_bytes_GPU_0_bfc""
    value {
      double_value: 23998744.0
    }
  }
  extras {
    key: ""allocator_maximum_num_bytes_cpu""
    value {
      double_value: 4000000.0
    }
  }
  extras {
    key: ""allocator_maximum_num_bytes_gpu_host_bfc""
    value {
      double_value: 4.0
    }
  }
}

------------------
----- SPARSE2 -----
entry {
  name: ""TensorFlowBenchmark.run_op_benchmark""
  iters: 10
  wall_time: 0.32063305377960205
  extras {
    key: ""allocator_maximum_num_bytes_GPU_0_bfc""
    value {
      double_value: 19998956.0
    }
  }
  extras {
    key: ""allocator_maximum_num_bytes_cpu""
    value {
      double_value: 11582884.0
    }
  }
}

------------------
----- SEG ----
entry {
  name: ""TensorFlowBenchmark.run_op_benchmark""
  iters: 10
  wall_time: 0.0016645193099975586
  extras {
    key: ""allocator_maximum_num_bytes_GPU_0_bfc""
    value {
      double_value: 7999788.0
    }
  }
}

Fastest:  seg, 0.0016645193099975586
Smallest: seg, 7.63mb
rel time, rel mem, name
179.262, 3.000, sparse
192.628, 2.500, sparse2
1.000, 1.000, seg
```
"
32762,ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none) ERROR: No matching distribution found for tensorflow,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
32761, keras.metrics.Accuracy != keras.metrics.accuracy,"The following test 
```
@test_util.run_all_in_graph_and_eager_modes
class KerasAccuracyTest(test.TestCase):

 def test_accuracy_vs_accuracy(self):
    y_true = constant_op.constant([1, 0, 1])
    y_pred = constant_op.constant([0.8, 0.1, 0.9])
    ret_a_tensor = metrics.accuracy(y_true, y_pred)
    ret_a = np.mean(self.evaluate(ret_a_tensor))
    
    acc_obj = metrics.Accuracy(name='my_acc')
    self.evaluate(variables.variables_initializer(acc_obj.variables))
    update_op = acc_obj.update_state(y_true, y_pred)
    self.evaluate(update_op)
    ret_b = self.evaluate(acc_obj.result())
    self.assertEqual(ret_a, ret_b)
```

does not pass because 
https://github.com/tensorflow/tensorflow/blob/3d5e79e08ae299812e0eaf6183f4886591e932bd/tensorflow/python/keras/metrics.py#L576-L577

seems to be doing two casts they should not.

Also it is unclear to me what is the expected value for `ret_a` (which I would have understood to be 1, but currently returns 1/3).




"
32760,TensorFlow Lite save  from_session error,"xs = tf.placeholder(tf.float32, [10, datalen], name='input')
predic_val = tf.argmax(graph, 1, name='output')
converter = tf.lite.TFLiteConverter.from_session(sess, [xs], [predic_val])
tflite_model = converter.convert()
open('%s/twave_%d.lite' % (path,i), ""wb"").write(tflite_model)



2019-09-24 11:30:42.542541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-24 11:30:42.542549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2019-09-24 11:30:42.542557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2019-09-24 11:30:42.546249: E tensorflow/core/grappler/grappler_item_builder.cc:656] Init node is_training/Assign doesn't exist in graph
See console for info.
2019-09-24 11:30:46.310416: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 153 operators, 209 arrays (0 quantized)
2019-09-24 11:30:46.311793: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 153 operators, 209 arrays (0 quantized)
2019-09-24 11:30:46.311861: F ./tensorflow/lite/toco/model.h:352] Check failed: dims_.size() > i (0 vs. 0)
Fatal Python error: Aborted

Current thread 0x00007fb218c41740 (most recent call first):
  File ""/opt/python366/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 52 in execute
  File ""/opt/python366/lib/python3.6/site-packages/absl/app.py"", line 250 in _run_main
  File ""/opt/python366/lib/python3.6/site-packages/absl/app.py"", line 299 in run
  File ""/opt/python366/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py"", line 40 in run
  File ""/opt/python366/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 89 in main
  File ""/opt/python366/bin/toco_from_protos"", line 10 in <module>



2019-09-24 11:31:03.164355: W tensorflow/core/kernels/queue_base.cc:277] _4_batch/fifo_queue: Skipping cancelled enqueue attempt with queue not closed
2019-09-24 11:31:03.164499: W tensorflow/core/kernels/queue_base.cc:277] _0_input_producer: Skipping cancelled enqueue attempt with queue not closed
2019-09-24 11:31:03.164595: W tensorflow/core/kernels/queue_base.cc:277] _3_input_producer_1: Skipping cancelled enqueue attempt with queue not closed
2019-09-24 11:31:03.164863: W tensorflow/core/kernels/queue_base.cc:277] _1_shuffle_batch/random_shuffle_queue: Skipping cancelled enqueue attempt with queue not closed
2019-09-24 11:31:03.164932: W tensorflow/core/kernels/queue_base.cc:277] _1_shuffle_batch/random_shuffle_queue: Skipping cancelled enqueue attempt with queue not closed
2019-09-24 11:31:03.164955: W tensorflow/core/kernels/queue_base.cc:277] _1_shuffle_batch/random_shuffle_queue: Skipping cancelled enqueue attempt with queue not closed
2019-09-24 11:31:03.164971: W tensorflow/core/kernels/queue_base.cc:277] _1_shuffle_batch/random_shuffle_queue: Skipping cancelled enqueue attempt with queue not closed
2019-09-24 11:31:03.164986: W tensorflow/core/kernels/queue_base.cc:277] _1_shuffle_batch/random_shuffle_queue: Skipping cancelled enqueue attempt with queue not closed
INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Enqueue operation was cancelled"
32759,call custom op using c++,"I built a very simple custom op `zero_out` and try to run it using c++, the question is how to call this op, there is no doc about it.

code of custom op `zero_out`
```
#include ""tensorflow/core/framework/op.h""
#include ""tensorflow/core/framework/shape_inference.h""
#include ""tensorflow/core/framework/op_kernel.h""
using namespace tensorflow;

REGISTER_OP(""ZeroOut"")
.Input(""to_zero: float"")
.Output(""zeroed: float"")
.SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
c->set_output(0, c->input(0));
return Status::OK();
});

class ZeroOutOp : public OpKernel {
public:
    explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}

    void Compute(OpKernelContext* context) override {
        const Tensor& input_tensor = context->input(0);
        auto input = input_tensor.flat<float>();
        Tensor* output_tensor = NULL;
        OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),                                                     &output_tensor));
        auto output_flat = output_tensor->flat<float>();
        const int N = input.size();
        for (int i = 1; i < N; i++)
            output_flat(i) = 0;
        if (N > 0) output_flat(0) = input(0);
    }
};

REGISTER_KERNEL_BUILDER(Name(""ZeroOut"").Device(DEVICE_CPU), ZeroOutOp);
```
After building, I got `zero_out.so`, then I load the lib using c++
```
    TF_Status* status_load = TF_NewStatus();
    TF_Library* lib_handle = TF_LoadLibrary(""libzero_out.so"", status_load);
    TF_Code code = TF_GetCode(status_load);
    cout << ""code: "" << code << endl;  // output: 0

    TF_Buffer op_list_buf = TF_GetOpList(lib_handle);
    tensorflow::OpList op_list;
    op_list.ParseFromArray(op_list_buf.data, op_list_buf.length);
    cout << ""oplist size = "" << op_list.op_size() << endl;    // output: 1
    cout << ""oplist name = "" << op_list.op(0).name() << endl;    // output: ZeroOut
```
I think the op should have been registered, then how to invoke the op function, Could you give some examples or code snippet?
"
32755,"Bincount Op test ""test_negative"" fails with TF 2.0","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 16.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**
- TensorFlow installed from (source or binary): **Binary**
- TensorFlow version (use command below): **2.0.0-rc1**
- Python version: **3.6**
- Bazel version (if compiling from source): **N/A**
- GCC/Compiler version (if compiling from source): **N/A**
- CUDA/cuDNN version: **10.0**
- GPU model and memory: **RTX 2080 Ti**, **11G**

**Describe the current behavior**
The test_negative test in `tensorflow/python/kernel_tests/bincount_op_test.py` fails, as the bincount call with negative values does not throw an InvalidArgumentError. 

This behavior might be the result of the op being called on the GPU, as only the CPU call is expected to throw the error, as per the comment here: https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/kernel_tests/bincount_op_test.py#L107.

Setting CUDA_VISIBLE_DEVICES to be empty forces the op to run on CPU and the test passes (the invalid argument error is successfully thrown), but passing `use_gpu=False` as an option to the session wrapper does not have this effect.

**Describe the expected behavior**
The test_negative test should pass, as the call to bincount with a negative input value is expected to throw an InvalidArgumentError. 

**Code to reproduce the issue**
Run the python test `tensorflow/python/kernel_tests/bincount_op_test.py`.
"
32752,enter,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

```
# Copy and paste here
```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32751,tensorflow-gpu 2.0 pip package outdated/missing/name wrong,"## URL(s) with the issue:

https://www.tensorflow.org/install/pip

## Description of issue (what needs changing):

The current pip package that exists on PyPi is `tensorflow-gpu==2.0.0rc2`. The documentation says `tensorflow-gpu==2.0.0-rc1`, which is 2 typos.

The command to install it is `pip install tensorflow-gpu==2.0.0rc2`.

The documents say `pip install tensorflow-gpu==2.0.0-rc1`, which is two typos.

This bug has existed for all release candidates.

```python
ERROR: Could not find a version that satisfies the requirement tensorflow-gpu==dont_exist (from versions: 0.12.1, 1.0.0, 1.0.1, 1.1.0rc1, 1.1.0rc2, 1.1.0, 1.2.0rc0, 1.2.0rc1, 1.2.0rc2, 1.2.0, 1.2.1, 1.3.0rc0, 1.3.0rc1, 1.3.0rc2, 1.3.0, 1.4.0rc0, 1.4.0rc1, 1.4.0, 1.4.1, 1.5.0rc0, 1.5.0rc1, 1.5.0, 1.5.1, 1.6.0rc0, 1.6.0rc1, 1.6.0, 1.7.0rc0, 1.7.0rc1, 1.7.0, 1.7.1, 1.8.0rc0, 1.8.0rc1, 1.8.0, 1.9.0rc0, 1.9.0rc1, 1.9.0rc2, 1.9.0, 1.10.0rc0, 1.10.0rc1, 1.10.0, 1.10.1, 1.11.0rc0, 1.11.0rc1, 1.11.0rc2, 1.11.0, 1.12.0rc0, 1.12.0rc1, 1.12.0rc2, 1.12.0, 1.12.2, 1.12.3, 1.13.0rc0, 1.13.0rc1, 1.13.0rc2, 1.13.1, 1.13.2, 1.14.0rc0, 1.14.0rc1, 1.14.0, 1.15.0rc0, 1.15.0rc1, 2.0.0a0, 2.0.0b0, 2.0.0b1, 2.0.0rc0, 2.0.0rc1, 2.0.0rc2)
```
"
32748,AttributeError when passing tf.Variable into tf.function ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v2.0.0-rc2
- Python version: 3.7.2
- Bazel version (if compiling from source): 0.25.2
- GCC/Compiler version (if compiling from source): 5.4.0 20160609
- CUDA/cuDNN version: None
- GPU model and memory: None

**Describe the current behavior**
```python
import tensorflow as tf
@tf.function
def foo(v):
    pass

foo(tf.Variable(1))
foo(tf.Variable(1))
```

Using this code piece, tensorflow throws an error: `AttributeError: 'NoneType' object has no attribute 'shape'`

**Describe the expected behavior**
No error occured.

**Code to reproduce the issue**
Provided above

**Other info / logs**
None"
32743,Failed to load delegate from libedgetpu.so.1.0 with tflite_runtime 1.14,"**System information**
- Have I written code (based on the docs):
```
from tflite_runtime.interpreter import Interpreter
from tflite_runtime.interpreter import load_delegate
model_path='my_compiled_model.tflite'
interpreter = Interpreter(model_path,
  experimental_delegates=[load_delegate('libedgetpu.so.1.0')])
```
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
```
  Operating System: Ubuntu 18.04.3 LTS
            Kernel: Linux 4.15.0-60-generic
      Architecture: x86-64
```
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: laptop
- TensorFlow installed from (source or binary):
`pip3 install tflite_runtime-1.14.0-cp36-cp36m-linux_x86_64.whl`
- TensorFlow version (use command below): tflite_runtime 1.14
- Python version: `Python 3.6.5 :: Anaconda, Inc.`
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

This is the code that I ran:
```
from tflite_runtime.interpreter import Interpreter
from tflite_runtime.interpreter import load_delegate
model_path='my_compiled_model.tflite'
interpreter = Interpreter(model_path,
  experimental_delegates=[load_delegate('libedgetpu.so.1.0')])
```
following this tutorial:
https://www.tensorflow.org/lite/guide/python

This was working before, but somehow broken with this error:
```
Traceback (most recent call last):
  File ""/home/nam/anaconda3/lib/python3.6/site-packages/tflite_runtime/interpreter.py"", line 165, in load_delegate
    delegate = Delegate(library, options)
  File ""/home/nam/anaconda3/lib/python3.6/site-packages/tflite_runtime/interpreter.py"", line 119, in __init__
    raise ValueError(capture.message)
ValueError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""evaluate_edgetpu_cifar10.py"", line 51, in <module>
    interpreter = Interpreter(file_name,experimental_delegates=[load_delegate('libedgetpu.so.1.0')])
  File ""/home/nam/anaconda3/lib/python3.6/site-packages/tflite_runtime/interpreter.py"", line 168, in load_delegate
    library, str(e)))
ValueError: Failed to load delegate from libedgetpu.so.1.0
```
I have been messing around a lot with my machine since by installing different versions of tf. But for the purpose of using the tflite_runtime.interpreter's load_delegate function, shouldn't just the pip install works? 
Very weird behavior :/
also I do have `libedgetpu.so.1.0` installed here:
```
% ls /usr/lib/x86_64-linux-gnu/libedgetpu.so.1.0
/usr/lib/x86_64-linux-gnu/libedgetpu.so.1.0
```

Thanks in advance for the help!

[EDIT]
I guess I'll update the issue here with a solution so that any body else can reference:
`ValueError: Failed to load delegate from libedgetpu.so.1.0` really is just due to the delegate library not being able to communicate with the edgetpu. This is a very standard linux problem and has nothing to do with the tensorflow library or libedgetpu. The failures most likely stems from some type of errno from the kernel which returns as failure to the user side.

So the easiest fix is to run with sudo:
```
$ sudo python your_script.py
```

But the most permanent fix is to add your linux user to the `plugdev` group which will allows you to access devices without sudo (this will requires a reboot after):
```
$ sudo usermod -aG plugdev $USER
```"
32740,"TFLite: Converting 2 Operation Network[Slice, Transpose] results in converter error despite both ops being supported","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Sierra 10.12.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-rc1
- Python version: 3.7.1
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: CPU

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Model conversion from script errors unexpectedly with:
F tensorflow/lite/toco/tooling_util.cc:661] Check failed: dim >= 1 (0 vs. 1)
Fatal Python error: Aborted

Current thread 0x00007fffbc0853c0 (most recent call first):
  File ""/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 52 in execute
  File ""/Users/t.capes/miniconda3/lib/python3.7/site-packages/absl/app.py"", line 251 in _run_main
  File ""/Users/t.capes/miniconda3/lib/python3.7/site-packages/absl/app.py"", line 300 in run
  File ""/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py"", line 40 in run
  File ""/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 89 in main
  File ""/Users/t.capes/miniconda3/bin/toco_from_protos"", line 10 in <module>
**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
import tensorflow.compat.v1 as tf
import numpy as np

tf.disable_v2_behavior()
initial_input = tf.placeholder(dtype=tf.float32, shape=(None,5,1024))
cap_i = tf.slice(initial_input, [0,0,0], [0,5,1023])
cap_iT = tf.transpose(cap_i, perm=[0,2,1])

sess = tf.Session()
sess.run(tf.global_variables_initializer())
tf.io.write_graph(sess.graph_def, '', 'train.pbtxt')
converter = tf.lite.TFLiteConverter.from_session(sess, [initial_input], [cap_iT])
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model = converter.convert()
open('converted_model.tflite', ""wb"").write(tflite_model)
sess.close()
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
WARNING: Logging before flag parsing goes to stderr.
W0923 16:48:55.162434 140736348050368 deprecation.py:323] From /Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2019-09-23 16:48:55.168686: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-09-23 16:48:55.186235: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f8f68dba420 executing computations on platform Host. Devices:
2019-09-23 16:48:55.186260: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2019-09-23 16:48:55.192597: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
2019-09-23 16:48:55.192676: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2019-09-23 16:48:55.194251: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize
2019-09-23 16:48:55.194270: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2019-09-23 16:48:55.194277: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0ms.
2019-09-23 16:48:55.196041: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
2019-09-23 16:48:55.196100: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2019-09-23 16:48:55.198204: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize
2019-09-23 16:48:55.198218: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 6 nodes (-1), 5 edges (0), time = 0.619ms.
2019-09-23 16:48:55.198224: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 0.175ms.
Traceback (most recent call last):
  File ""tf_test_1.py"", line 16, in <module>
    tflite_model = converter.convert()
  File ""/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py"", line 983, in convert
    **converter_kwargs)
  File ""/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py"", line 449, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File ""/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py"", line 200, in toco_convert_protos
    raise ConverterError(""See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: See console for info.
2019-09-23 16:48:57.055966: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 2 operators, 6 arrays (0 quantized)
2019-09-23 16:48:57.056188: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 2 operators, 6 arrays (0 quantized)
2019-09-23 16:48:57.056303: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 2 operators, 6 arrays (0 quantized)
2019-09-23 16:48:57.056347: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 2 operators, 6 arrays (0 quantized)
2019-09-23 16:48:57.056388: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 2 operators, 6 arrays (0 quantized)
2019-09-23 16:48:57.056432: F tensorflow/lite/toco/tooling_util.cc:661] Check failed: dim >= 1 (0 vs. 1)
Fatal Python error: Aborted

Current thread 0x00007fffbc0853c0 (most recent call first):
  File ""/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 52 in execute
  File ""/Users/t.capes/miniconda3/lib/python3.7/site-packages/absl/app.py"", line 251 in _run_main
  File ""/Users/t.capes/miniconda3/lib/python3.7/site-packages/absl/app.py"", line 300 in run
  File ""/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py"", line 40 in run
  File ""/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 89 in main
  File ""/Users/t.capes/miniconda3/bin/toco_from_protos"", line 10 in <module>"
32737,"tf.keras.layers.Input has undefined shape when setting sparse=True, making it impossible to use in a Model","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.15.0rc1
- Python version:3.5.2
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1
- GPU model and memory: Nvidia GeForce GTX 1050, 4GB

**Describe the current behavior**
The tensor created by `tensorflow.keras.layers.Input(specified_shape, sparse=True)` has shape `(None,) + (None, ) * len(specified_shape)` and cannot be used as input to e.g., `Dense` layers. 

**Describe the expected behavior**
The tensor created by `tensorflow.keras.layers.Input(specified_shape, sparse=True)` has shape `(None,) + specified_shape` and can be used as input to e.g., `Dense` layers. This is the default behaviour in Keras. 

**Code to reproduce the issue**
```python
from tensorflow.keras.layers import Input, Dense

i = Input(shape=(1, ), sparse=True)
o = Dense(1)(i)
```

**Other info / logs**
Stack trace: 

```
Traceback (most recent call last):
  File ""test.py"", line 7, in <module>
    o = Dense(1)(i)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 824, in __call__
    self._maybe_build(inputs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 2146, in _maybe_build
    self.build(input_shapes)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/layers/core.py"", line 1009, in build
    raise ValueError('The last dimension of the inputs to `Dense` '
ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.

```
"
32736,1.15.0 release candidates missing from Docker Hub,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): N/A
- TensorFlow version: 1.15.0-rc1
- Python version: Python 2 or 3
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**

There does not appear to be a tagged container which matches the `1.15.0-rc1` (or _any_ `1.15`) release at https://hub.docker.com/r/tensorflow/tensorflow.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

I navigated to https://hub.docker.com/r/tensorflow/tensorflow/tags and entered 1.15 into the ""Filter Tags"" search box as illustrated [here](https://hub.docker.com/r/tensorflow/tensorflow/tags?page=1&name=1.15). I understand that _currently_ the `nightly-` builds are based on `1.15.0-devX`, but I would have expected there to be a specific tag for the release candidate (like there is for 2.0.0).

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32732,Finetune with additional layers,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu16.04
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.10
- Python version: 3.6
- CUDA/cuDNN version: 9.0
- GPU model and memory: 1070TI

I wonder how to finetune a model with several additional layers
I have a set of checkpoint, including .ckpt and .meta

I add two deconvolution layers to a previous model
```
flow_pred_x = tf.layers.conv2d_transpose(flow_x, filters=1, kernel_size=8, strides=4, padding='same', use_bias=False, name='slice_deconv_x')
flow_pred_y = tf.layers.conv2d_transpose(flow_y, filters=1, kernel_size=8, strides=4, padding='same', use_bias=False, name='slice_deconv_y')
```
<br>
I tried to use this at first<br>

'''
saver = tf.train.Saver()
saver.restore(session, './model.ckpt')
```
However, the error is unmatch<br>
Then I tried to save as .pb
```
constant_graph = graph_util.convert_variables_to_constants(self.sess, input_graph_def=self.sess.graph_def, output_node_names=['pwcnet/ctxt/refined_flow2', 'pwcnet/flow_x', 'pwcnet/flow_y'])
with tf.gfile.FastGFile('./modelflowxy.pb', mode='wb') as f:
       f.write(constant_graph.SerializeToString())
```
And load the .pb and add layers<br>
```
import tensorflow as tf
import numpy as np
from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file
with tf.gfile.FastGFile(r'./modelflowxy.pb', 'rb') as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())
    _ = tf.import_graph_def(graph_def, name='')
with tf.Session() as session:
   flow_x = tf.get_default_graph().get_tensor_by_name('pwcnet/flow_x:0')
    flow_y = tf.get_default_graph().get_tensor_by_name('pwcnet/flow_y:0')
    init = np.load('./slice_conv_x_0_w.npy')
    init = tf.constant_initializer(init)
    flow_pred_x = tf.layers.conv2d_transpose(flow_x, filters=1, kernel_size=8, strides=4, padding='same', use_bias=False, name='slice_deconv_x')
    flow_pred_y = tf.layers.conv2d_transpose(flow_y, filters=1, kernel_size=8, strides=4, padding='same', use_bias=False, name='slice_deconv_y')
    flow_pred = tf.concat([flow_pred_x, flow_pred_y], axis=3, name='flow_pred')
    init = tf.global_variables_initializer()
    session.run(init)
    saver = tf.train.Saver(var_list=tf.trainable_variables())
    saver.save(session, './ckpt_test2/all.ckpt')
```

It doesn't work too, and the layers in the previous model unmatch
And I check the new checkpoint, and find that only two newly added layers are recorded.<br>
So, I sincerely hope if anyone could tell me how to finetune model with additional layers "
32731,Tensorflow-gpu 2.0.0.rc1 or rc2 on Ubuntu 19.04 does not find the cuda libraries.,"I am trying to run Tensorflow-gpu 2.0.0.rc1 on ubunto 19.04. I have already installed the stable version (1.14.0) via conda and I have been using it without problem. Meaning all the libraries and drivers are fine however for new tensorflow I get the following errors which is odd because all the libraries are installed in the ""/usr/lib/x86_64-linux-gnu/"".

I have used 
`source ./myenv/bin/activate`
`pip install tensorflow=2.0.0.rc1`
`pip install tensorflow-gpu=2.0.0.rc1`

```
python -c ""import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))""
2019-09-23 16:20:27.250506: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2019-09-23 16:20:27.267348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.8095
pciBusID: 0000:05:00.0
2019-09-23 16:20:27.267424: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu/
2019-09-23 16:20:27.267468: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu/
2019-09-23 16:20:27.267507: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu/
2019-09-23 16:20:27.267545: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu/
2019-09-23 16:20:27.267583: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu/
2019-09-23 16:20:27.267620: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu/
2019-09-23 16:20:27.270227: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-09-23 16:20:27.270242: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2019-09-23 16:20:27.270441: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-09-23 16:20:27.294668: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3600075000 Hz
2019-09-23 16:20:27.295454: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4683530 executing computations on platform Host. Devices:
2019-09-23 16:20:27.295488: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2019-09-23 16:20:27.401522: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x46b6430 executing computations on platform CUDA. Devices:
2019-09-23 16:20:27.401568: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1080, Compute Capability 6.1
2019-09-23 16:20:27.401692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-23 16:20:27.401713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]
```"
32730,tf.nn.relu on nan inputs returns zeros on GPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see minimal example code section below.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1 and 1.14.0
- Python version: 3.6.5
- CUDA/cuDNN version: 10.0, 7.6.3
- GPU model and memory: GeForce GTX 1080 (8GB)

**Describe the current behavior**
The behavior of `tf.nn.relu` when fed with `nan`-valued inputs is inconsistent:
- if the input is a constant tensor, `relu` returns `nan`.
- if the input is a variable tensor (like `nan` wrapped into `tf.Variable` or multiplied by a random tensor), it returns zeros.

This behavior can only be observed on the GPU. On the CPU, `relu` consistently returns `nan`. The behavior on the CPU is also consistent with other activation functions.


**Describe the expected behavior**
`tf.nn.relu` should handle `nan` inputs from all sources consistently. To keep consistency with other activation functions, it should return `nan` in all cases.


**Code to reproduce the issue**
The assertions below all pass. The second and third assertion (the `not` assertions) would be expected to fail.
```python
x1 = tf.nn.relu(np.nan)
x2 = tf.nn.relu(np.nan * tf.random_normal(shape=[]))
x3 = tf.nn.relu(tf.Variable(np.nan))

with tf.device(""/cpu:0""):
    x1_cpu = tf.nn.relu(np.nan)
    x2_cpu = tf.nn.relu(np.nan * tf.random_normal(shape=[]))
    x3_cpu = tf.nn.relu(tf.Variable(np.nan))

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    assert np.all(np.isnan(sess.run(x1)))
    assert not np.any(np.isnan(sess.run(x2)))  # should fail but does not
    assert not np.any(np.isnan(sess.run(x3)))  # should fail but does not

    assert np.all(np.allclose(sess.run(x2), 0.0))
    assert np.all(np.allclose(sess.run(x3), 0.0))

    assert np.all(np.isnan(sess.run(x1_cpu)))
    assert np.all(np.isnan(sess.run(x2_cpu)))
    assert np.all(np.isnan(sess.run(x3_cpu)))
```

"
32729,"ValueError: Variable <tf.Variable 'basic_nstepTD_1/noisy_dense/bias:0' shape=(3,) dtype=float32> has `None` for gradient.","I am trying to implement a self-defined noisy-layer, when I put the `add` operator in the `build` function
```python 
self.kernel = self.weight_mu + self.weight_sigma * self.weights_eps
self.bias = self.bias_mu + self.bias_sigma * self.bias_eps
```
, it will raise the error that  ` ValueError: Variable <tf.Variable 'basic_nstepTD_1/noisy_dense/bias:0' shape=(3,) dtype=float32> has 'None' for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval.`  But when I put that two-line code into the `call` function, the error is disappeared.  
My code:
```python
class NoisyDense(kl.Layer):
    def __init__(self, units, std_init=0.5):
        super().__init__()
        self.units = units
        self.std_init = std_init

    def build(self, input_shape):
        self.reset_noise(input_shape[-1])
        mu_range = 1 / np.sqrt(input_shape[-1])
        mu_initializer = tf.random_uniform_initializer(-mu_range, mu_range)
        sigma_initializer = tf.constant_initializer(self.std_init / np.sqrt(self.units))

        self.weight_mu = tf.Variable(initial_value=mu_initializer(shape=(input_shape[-1], self.units), dtype='float32'),
                                     trainable=True)

        self.weight_sigma = tf.Variable(initial_value=sigma_initializer(shape=(input_shape[-1], self.units), dtype='float32'),
                                        trainable=True)

        self.bias_mu = tf.Variable(initial_value=mu_initializer(shape=(self.units,), dtype='float32'),
                                     trainable=True)

        self.bias_sigma = tf.Variable(initial_value=sigma_initializer(shape=(self.units,), dtype='float32'),
                                        trainable=True)

        self.kernel = self.weight_mu + self.weight_sigma * self.weights_eps
        self.bias = self.bias_mu + self.bias_sigma * self.bias_eps

    def call(self, inputs):
        # output = tf.tensordot(inputs, self.kernel, 1)
        # tf.nn.bias_add(output, self.bias)
        # return output
        # self.kernel = self.weight_mu + self.weight_sigma * self.weights_eps
        # self.bias = self.bias_mu + self.bias_sigma * self.bias_eps
        return tf.matmul(inputs, self.kernel) + self.bias

    def _scale_noise(self, dim):
        noise = tf.random.normal([dim])
        return tf.sign(noise) * tf.sqrt(tf.abs(noise))

    def reset_noise(self, input_shape):
        eps_in = self._scale_noise(input_shape)
        eps_out = self._scale_noise(self.units)
        self.weights_eps = tf.multiply(tf.expand_dims(eps_in, 1), eps_out)
        self.bias_eps = eps_out
```
"
32728,Memory leak when loading and unloading multiple graphs,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.2 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not applicable
- TensorFlow installed from (source or binary): I used tensorflow/tensorflow:1.14.0-gpu-py3 docker image (tested also on different version both with and without docker)
- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0
- Python version: Python 3.6.8
- Bazel version (if compiling from source): Not applicable
- GCC/Compiler version (if compiling from source): Not applicable
- CUDA/cuDNN version: Cuda 10.0, cuDNN 7
- GPU model and memory: GeForce GTX TITAN X (tested also on GeForce GTX 1080)

**Describe the current behavior**

When I load multiple Inception V3 graphs into memory and afterwards unload all of them I get a memory leak. With 20 Inception graphs loaded and unloaded, the RAM usage goes up to around 2GB. With 100 Inception graphs, the RAM usage goes up to around 10GB. If I load and unload Inception V3 graphs one by one, there is no memory leak, RAM usage stays below 1GB, does not matter how many Inception V3 graphs I load.

I tried to load this Inception V3 graph multiple times for purposes of testing: http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz . In reality, I need to load different Inception V3 graphs into memory.

**Describe the expected behavior**

The expected behaviour is that I am able to load multiple Inception V3 graphs and afterwards unload all of them without any memory leak.

**Code to reproduce the issue**
```
import sys 
import os
import tensorflow as tf
import gc
import time

class InceptionV3Graph:
    def __init__(self, graph_path):
        with tf.gfile.FastGFile(graph_path, 'rb') as f:
            graph_def = tf.GraphDef()
            graph_def.ParseFromString(f.read())
            _ = tf.import_graph_def(graph_def, name='')
        self.sess = tf.Session(graph=tf.get_default_graph())

    def close(self):
        tf.reset_default_graph()
        gc.collect()
        self.sess.close()

if __name__ == ""__main__"":
    graph_path = ""/path/to/classify_image_graph_def.pb"" # you can get classify_image_graph_def.pb from http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz
    N_GRAPHS = 100 
    graphs = dict()
    for i in range(N_GRAPHS):
        print(""Loading graph {}"".format(i+1))
        graphs[i] = InceptionV3Graph(graph_path)
        # If you uncomment these two lines below there won't be any the memory leak
        #graphs[i].close()
        #del graphs[i]
    for i in range(N_GRAPHS):
        print(""Unloading graph {}"".format(i+1))
        if i in graphs:
            graphs[i].close()
            del graphs[i]
    print(graphs)
    gc.collect()
    print(""All graphs unloaded"")
    time.sleep(120)
    print(""Quitting..."")
```


**Other info / logs**
I successfully reproduced the problem on two different Linux machines (Ubuntu 16.04 and 18.04) with and without docker. However, the memory leak does not seem to reproduce on Windows 10.
"
32726,How to avoid tensorflow@gpu copy outputs to cpu?,"I use tensorflow 1.7.0 at gpu. I call Session::Run function, the inputs Tensor is already gpu Tensor, so input and forward calculate is well. But before output, I see that every outputs has been copied from gpu to cpu, and this is not what I hope. I just want the outputs are all gpu Tensor. How can I do?

```
// tensorflow/core/public/session.h
class Session {
...
  virtual Status Run(const std::vector<std::pair<string, Tensor> >& inputs,
                     const std::vector<string>& output_tensor_names,
                     const std::vector<string>& target_node_names,
                     std::vector<Tensor>* outputs) = 0;
...
};
```"
32725,run interpreter.invoke() just show Aborted (core dumped),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
-(yes) Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
-(Linux Ubuntu 16.04) OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- (conda) TensorFlow installed from (source or binary):
- (1.13.1 and 1.14) TensorFlow version (use command below):
- (3.6.8) Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
When I use tf version 1.13.1 to convert pb to tflite, it shows dim not matched error, but when I use 1.14 to convert, it succeeds to save the tflite file. But when I use the test code in the tf doc, it just shows core dumped.
**Describe the expected behavior**
i want to use the tflite file in android, however, i even can't use it in python
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
code generated the tflite file
```
import tensorflow as tf

graph_def_file = ""../model/resnet18/weights-89-0.952.pb""
input_arrays = [""input""]
output_arrays = [""lambda_1/l2_normalize""]

converter = tf.lite.TFLiteConverter.from_frozen_graph(
  graph_def_file, input_arrays, output_arrays, input_shapes={""input"" : [1, 257, 400,1]})
tflite_model = converter.convert()
open(""../model/resnet18/converted_model.tflite"", ""wb"").write(tflite_model)
```

code to test in python(from doc)
```
import tensorflow as tf
import numpy as np

# Load TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path=""/home/dm/Desktop/VGG-Speaker-Recognition/model/resnet18/converted_model.tflite"")
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
print(input_details)
print(output_details)
# Test model on random input data.
input_shape = input_details[0]['shape']
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
print(input_data.shape)
interpreter.set_tensor(input_details[0]['index'], input_data)

interpreter.invoke()
# The function `get_tensor()` returns a copy of the tensor data.
# Use `tensor()` in order to get a pointer to the tensor.
output_data = interpreter.get_tensor(output_details[0]['index'])
print(output_data)
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
converted with 1.13.1

     ture_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA  
    2019-09-23 14:33:07.069746: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3696000000 Hz  
    2019-09-23 14:33:07.072588: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x564ecd24a790 executing computations on platform Host. Devices:  
    2019-09-23 14:33:07.072658: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>  
    Ignore 'tcmalloc: large alloc' warnings.  
    Traceback (most recent call last):  
      File ""pb-to-tflite.py"", line 9, in <module>  
        tflite_model = converter.convert()  
      File ""/home/dm/anaconda3/envs/s-t3/lib/python3.6/site- 
     packages/tensorflow/lite/python/lite.py"", line 455, in convert  
        **converter_kwargs)  
      File ""/home/dm/anaconda3/envs/s-t3/lib/python3.6/site- 
     packages/tensorflow/lite/python/convert.py"", line 442, in toco_convert_impl  
        input_data.SerializeToString())  
      File ""/home/dm/anaconda3/envs/s-t3/lib/python3.6/site-packages/tensorflow/lite/python/convert.py"", line 205, in toco_convert_protos  
        ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
    tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.  
    2019-09-23 14:33:21.269591: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 529 operators, 854 arrays (0 quantized)  
    2019-09-23 14:33:21.275653: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 527 operators, 849 arrays (0 quantized)  
    2019-09-23 14:33:21.283483: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 527 operators, 849 arrays (0 quantized)  
    2019-09-23 14:33:21.330461: F tensorflow/lite/toco/graph_transformations/propagate_fixed_sizes.cc:117] Check failed: dim_x == dim_y (512 vs. 10)Dimensions must match  
    Aborted (core dumped) 

 


converted by 1.14
 

    2019-09-23 14:38:45.285133: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize  
    2019-09-23 14:38:45.285519: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 440 nodes (-112), 631 edges (-112), time = 213.324ms.  
    2019-09-23 14:38:45.285580: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 440 nodes (0), 631 edges (0), time = 54.184ms.  


and when I use the code above to test ,it just shows

     [{'name': 'input', 'index': 95, 'shape': array([  1, 257, 400,   1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]  
    [{'name': 'lambda_1/l2_normalize', 'index': 96, 'shape': array([  1, 512], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]  
    (1, 257, 400, 1)  
    Aborted (core dumped)  

and I could use netron to open the generated tflite file
[the network structure it shows][1]

here is the code of this model
https://github.com/WeidiXie/VGG-Speaker-Recognition

could somebody give me some help?

  [1]: https://i.stack.imgur.com/zlSAn.png
"
32724,Tensorflow Lite for Android on x86_64,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
ubuntu 16.0
- TensorFlow installed from (source or binary):
Yes
- TensorFlow version (or github SHA if from source):
master


Hi  , I like to build Tensorflow Lite to run on x86_64 with Android.
I see there is package downloaded  NEON_2_SSE and there is a scripts in tools/make build_XXX for Android/Ios/RPI
Can anyone suggest how to combine both SSE and Android .
Thanks."
32723,model.ckpt-180000.data-00000-of-00001.tempstate5599057054002993368 appeared,"I am running a image resnet152 model in imagenet dataset, However, when 18W step, a strange file named model.ckpt-180000.data-00000-of-00001.tempstate5599057054002993368 appeared, then my validation is wrong like  

OP_REQUIRES failed at save_restore_v2_ops.cc:134 : Unknown: output/checkpoint/model.ckpt-180000.data-00000-of-00001.tempstate5599057054002993368; Input/output error .

It troubled me many days , is there anyone meet the same problem? hope someone can help me. thank you!!


"
32722,AttributeError: module 'tensorflow.python.ops.gen_logging_ops' has no attribute '_scalar_summary'v,"Hello 
I getting this error while train my data i  already install latest version of tensor flow  
errors

AttributeError: module 'tensorflow.python.ops.gen_logging_ops' has no attribute '_scalar_summary'"
32721,how to feed mere than one enter in tensorlow c++,"hi I just new some help with this graph 
![image](https://user-images.githubusercontent.com/30030792/65396297-9eae3680-dd6a-11e9-9c23-62a9c25ceac8.png)
 as you can see in the graph, I have more than one enter, are 3 images, I made in the past some feeding in c++ tensorfow but only with one enter I mean one std::string and one tensor, how can I feed the 3 images at the same time 3 std::strings and the 3 tensors calling just one session thanks 
"
32720,tensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value with BatchNorm and Map_fn,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

Any help would be very greatly appreciated! Thanks!

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.06 & OSX 10.14.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14 on both os's, tf-gpu on ubuntu
- Python version: 3.7
- CUDA/cuDNN version: (ubuntu only) 10.1
- GPU model and memory: Titan X Pascal

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Applying batch normalization on top of batchwise tf.map_fn results in retval[0] error.  Note that this fails even when tf.map_fn operates over convolution filters. In this latter case, we have a different filter for each batch element, and so cannot apply the normal tf.conv functions without a map.

**Describe the expected behavior**
Applying batch normalization on top of batchwise tf.map_fn should pass without issue, as in this case it should emulate batchwise matmul.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

Below is the code. Alternatively, it can be found in this txt file:
[run_dummy.txt](https://github.com/tensorflow/tensorflow/files/3643105/run_dummy.txt)



```python3
import tensorflow as tf


class DummyModelRunner(object):
    def __init__(self):
        self.optimizer = tf.train.AdamOptimizer()

        with tf.variable_scope('Dummy_Vars', use_resource=True):
            self.variables = self.create_variables()

        self.is_train = tf.placeholder_with_default(False, shape=[], name='is_train')

        self.batch_inputs = tf.ones(shape=[256, 10], dtype=tf.float32, name='e1_embs')
        actual_answers = tf.ones(shape=[256, 10, 5], dtype=tf.float32, name='actual_answers')
        targets = tf.ones(shape=[256, 5], dtype=tf.float32, name='targets')

        self.batch_outputs = self.create_predictions(input_embs=self.batch_inputs)

        self.predictions = self.compute_likelihoods(self.batch_outputs, actual_answers)

        self.loss = self.create_loss(self.predictions, targets)

        # The following control dependency is needed in order for batch
        # normalization to work correctly.
        self.update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
        with tf.control_dependencies(self.update_ops):
            self.train_op = self.optimizer.minimize(self.loss)
            gradients, variables = zip(*self.optimizer.compute_gradients(self.loss))
            gradients, _ = tf.clip_by_global_norm(gradients, 5.0)
            self.train_op = self.optimizer.apply_gradients(zip(gradients, variables))

    def create_variables(self):
        """"""
        create all network variables
        :return: all variable dictionary
        """"""
        weight = tf.get_variable(name='DummyWeight',
                                 shape=[10, 10],
                                 dtype=tf.float32,
                                 initializer=tf.contrib.layers.xavier_initializer())
        bias = tf.get_variable(name='DummyBiasRel',
                               shape=[1, 10],
                               dtype=tf.float32,
                               initializer=tf.zeros_initializer())

        variables = {'weights': weight,
                     'biases': bias}

        return variables


    def create_predictions(self, input_embs):
        weight, bias = self.variables['weights'], self.variables['biases']
        is_train_batch_norm = self.is_train

        def matmul(pair):
            input_tensor = pair[0][None]
            projection_tensor = pair[1]
            projection = tf.matmul(input_tensor, projection_tensor)[0]
            return (projection, tf.zeros([]))

        output = tf.reshape(input_embs, [tf.shape(input_embs)[0], -1])
        # Next two lines cause problem
        weight = tf.broadcast_to(weight, [256, 10, 10])
        output = tf.map_fn(fn=matmul, elems=(output, weight))[0] + bias

        output = tf.layers.batch_normalization(output, momentum=.1, reuse=tf.AUTO_REUSE,
                                               training=is_train_batch_norm, fused=True,
                                               name='DummyBatchNorm')

        return output

    def compute_likelihoods(self, input_vectors, actual_answers):
        with tf.name_scope('output_layer'):

            predictions = tf.matmul(input_vectors[:, None, :], actual_answers)[:, 0, :]

        return predictions

    def create_loss(self, predictions, targets):
        with tf.name_scope('loss'):
            loss = tf.reduce_sum(
                tf.compat.v1.losses.sigmoid_cross_entropy(targets, predictions),
                name='loss')
        return loss


if __name__ == '__main__':
    # Create the model.
    # Can be /GPU:0 for ubuntu
    with tf.device('/CPU:0'):
        # We are using resource variables because due to some implementation details, this allows us to
        # better utilize GPUs while training.
        with tf.variable_scope('variables', use_resource=True):

            model = DummyModelRunner()

    # Create a TensorFlow session and start training.
    config = tf.ConfigProto(allow_soft_placement=True)
    config.gpu_options.allow_growth = True
    session = tf.Session(config=config)
    saver = tf.train.Saver()

    # Initialize the values of all variables and the train dataset iterator.
    session.run(tf.global_variables_initializer())

    for step in range(100):
        # print('Hi!')
        feed_dict = {model.is_train: True}

        loss, _ = session.run((model.loss, model.train_op), feed_dict)

        print(loss)
        exit()
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Traceback below (deprecation warning's excluded. Please let me know if I should include those as well):
Traceback (most recent call last):
  File ""/Users/georgestoica/Desktop/Research/venvs/rl_3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1356, in _do_call
    return fn(*args)
  File ""/Users/georgestoica/Desktop/Research/venvs/rl_3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1341, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/Users/georgestoica/Desktop/Research/venvs/rl_3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1429, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/georgestoica/Desktop/Research/new_exploration/src/qa_cpg/run_cpg_minimal.py"", line 111, in <module>
    loss, _ = session.run((model.loss, model.train_op), feed_dict)
  File ""/Users/georgestoica/Desktop/Research/venvs/rl_3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 950, in run
    run_metadata_ptr)
  File ""/Users/georgestoica/Desktop/Research/venvs/rl_3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1173, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/Users/georgestoica/Desktop/Research/venvs/rl_3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1350, in _do_run
    run_metadata)
  File ""/Users/georgestoica/Desktop/Research/venvs/rl_3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1370, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value

"
32719,bundle_reader read tensor larger than 2GB will lead index overflow,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

in tensor_bundle.cc method GetValue 
if the tensor has entry.size() larger than 2GB the underlying hdfspread method accept a int32 type of size but entry.size() has 64 bit 
the static_cast will trigger a error of invalid param since it will cast the 64bit to a negative integer

**Describe the expected behavior**

if the entry size larger than 2GB, we should read it from parts not in a whole

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32718,[TF2.0] TensorFlow nightly build for Python 3.7,"**System information**
- OS Platform and Distribution: **macOS**
- TensorFlow version: **tf-nightly-2.0-preview**
- Python version: **3.7**
- Installed using virtualenv? pip? conda?: **conda**

Hello everyone,

I can't install tensorflow 2.0 nighly build for `py3.7`:

```bash
(py3.7) artemav:~/code/GPflow (awav/gpflow-2.0)
→ pip install tf-nightly-2.0-preview
Collecting tf-nightly-2.0-preview
  ERROR: Could not find a version that satisfies the requirement tf-nightly-2.0-preview (from versions: none)
ERROR: No matching distribution found for tf-nightly-2.0-preview
```

"
32717, Did you open MPI when compiling the WHL package for this URL?https://pypi.org/project/tensorflow/1.14.0/#files,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Did you open MPI when compiling the WHL package for this URL?https://pypi.org/project/tensorflow/1.14.0/#files
![image](https://user-images.githubusercontent.com/25795827/65386223-27c56d80-dd6b-11e9-9d2b-949b5b71688d.png)

thanks！"
32716,Tensorflow Serving Bad Docker Example,"Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/tfx/serving/docker

## Description of issue (what needs changing):
The first example does not work when you get to this command:

docker run -t --rm -p 8501:8501 \
    -v ""$TESTDATA/saved_model_half_plus_two_cpu:/models/half_plus_two"" \
    -e MODEL_NAME=half_plus_two \
    tensorflow/serving &

The command will repeatedly return the error:
""No versions of servable half_plus_two found under base path /models/half_plus_two""

This happens when you go through all of the steps sequentially, as they are listed in that example. 

Additionally, the example has the headline ""One of the easiest ways to get started using TensorFlow Serving is with Docker.""

This is contradicted by the difficulties experienced with that example. 

### Submit a pull request?
I'm afraid I don't know what the correct command is, so I currently will not be submitting a pull request. "
32715,About FGSM implementation in the tutorial,"## URL(s) with the issue:

https://www.tensorflow.org/beta/tutorials/generative/adversarial_fgsm
## Description of issue (what needs changing):

The FGSM  implementaiton in the documentation seems to be incorrect.

### Clear description

In the doc, `image_probs` which is equal to the value `model.predict(image)`,  is used to calculate the perturbation.
```python
perturbations = create_adversarial_pattern(image, image_probs)
```

The `create_adversarial_pattern` function takes `input_image` and `input_label`. So the above code is the same as the blow code.
```python
perturbations = create_adversarial_pattern(input_image=image, input_label=model.predict(image))
```

However, `input_label` must be not the predicted _probability_ of the model, but the (one hot encoded) correct _label_ of input_image, I think. In fact, it is calculated that
```python
prediction = pretrained_model(input_image)
loss = loss_object(input_label, prediction)
```
in the `create_adversarial_pattern` function.

Sorry if I have misunderstood.

ref. ""EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES,"" https://arxiv.org/pdf/1412.6572.pdf, p3.

### ### Submit a pull request?
If my understanding is correct, I will submit a PR. But I do not have confidence that my understanding is correct yet.


"
32713,Ussage via JNI (libtensorflow-1.14.0.jar+libtensorflow-src.jar+tensorflow_jni.dll)  JDK=1.8,"Hello! Append lib (libtensorflow-1.14.0.jar) as User library on Eclipse, include libtensorflow-src.jar as source for this user lib, try to build examle and java take me:
`Exception in thread ""main"" java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library for OS: windows, architecture: x86_64.`

why resoure name is `org/tensorflow/native/windows-x86_64/tensorflow_jni.dll`?

Note: 
libtensorflow-1.14.0.jar+libtensorflow-src.jar+tensorflow_jni.dll  in project folder.

WTD?
"
32712,TPUStrategy.make_dataset_iterator does not work for a Dataset created by tf.data.Dataset.from_generator,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab default environment
- TensorFlow installed from (source or binary): Colab  default
- TensorFlow version (use command below): 1.14
- Python version: Python 3

**Describe the current behavior**
I first create a Dataset with an iterator, then create a TPU iterator with the Dataset. After that, the iterator is used as the second input of `TPUStrategy.experimental_run`. As I run the output of `experimental_run`,  an error occurs:
```
/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1340       return self._call_tf_sessionrun(
-> 1341           options, feed_dict, fetch_list, target_list, run_metadata)
   1342 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)
   1428         self._session, options, feed_dict, fetch_list, target_list,
-> 1429         run_metadata)
   1430 

AbortedError: Session 5c40387f44056164 is not found.

During handling of the above exception, another exception occurred:

AbortedError                              Traceback (most recent call last)
<ipython-input-7-b0bded0c272b> in <module>()
     18     # Custom training loop
     19     session.run(train_iterator_init)
---> 20     session.run(dist_train)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    948     try:
    949       result = self._run(None, fetches, feed_dict, options_ptr,
--> 950                          run_metadata_ptr)
    951       if run_metadata:
    952         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1171     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1172       results = self._do_run(handle, final_targets, final_fetches,
-> 1173                              feed_dict_tensor, options, run_metadata)
   1174     else:
   1175       results = []

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1348     if handle is None:
   1349       return self._do_call(_run_fn, feeds, fetches, targets, options,
-> 1350                            run_metadata)
   1351     else:
   1352       return self._do_call(_prun_fn, handle, feeds, fetches)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1368           pass
   1369       message = error_interpolation.interpolate(message, self._graph)
-> 1370       raise type(e)(node_def, op, message)
   1371 
   1372   def _extend_graph(self):

AbortedError: Session 5c40387f44056164 is not found.
```
**Describe the expected behavior**
Run without error.
**Code to reproduce the issue**
```
import os
import pprint
import tensorflow as tf
import numpy as np
resolver = tf.contrib.cluster_resolver.TPUClusterResolver('grpc://' + os.environ['COLAB_TPU_ADDR'])
tf.contrib.distribute.initialize_tpu_system(resolver)
strategy = tf.contrib.distribute.TPUStrategy(resolver)

def my_generator():
    for i in range(100):
      x = np.random.rand(28,28).astype('float32')
      y = np.zeros([], dtype='int32')
      yield x,y

def train_fn(inputs):
    return inputs[0]

with strategy.scope():
  config = tf.ConfigProto()
  config.allow_soft_placement = True
  cluster_spec = resolver.cluster_spec()
  if cluster_spec:
    config.cluster_def.CopyFrom(cluster_spec.as_cluster_def())
  print('Starting training...')
  # Do all the computations inside a Session (as opposed to doing eager mode)
  with tf.Session(target=resolver.master(), config=config) as session:  
    #train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(8, drop_remainder=True)
    train_dataset = tf.data.Dataset.from_generator(my_generator, ('float32', 'int32'), ([28,28], [])).batch(8, drop_remainder=True)
    train_iterator = strategy.make_dataset_iterator(train_dataset)
    train_iterator_init = train_iterator.initialize()
    dist_train = strategy.experimental_run(train_fn, train_iterator).values

    # Custom training loop
    session.run(train_iterator_init)
    session.run(dist_train)
```
This snippet of code is a simplified version of  [this Colab notebook](https://colab.research.google.com/github/tensorflow/docs/blob/r2.0rc/site/en/tutorials/distribute/tpu_custom_training.ipynb).  The original `tf.data.Dataset.from_tensor_slices` works fine but  `tf.data.Dataset.from_generator` fails.

At the same time, I provide a [Colab notebook](https://colab.research.google.com/drive/1SNCAPKRcF_9XCSunRYvb6rGWkOcrweuN) for reproducing this issue.

"
32711,tf.pow throwing negative or zero value for large value of exponent.,"I am using tensorflow 1.14.0 version with Python 3 Google compute engine backend in Google Colab having a RAM 12.72 GB, Disk 48.97 GB and tf.GIT_VERSION v1.14.0-0-g87989f6959

While performing tf.pow operation I have observed that tf.pow returns negative or zero value for large value of exponent.

```
a = tf.constant(50)
b = tf.constant(9)
c = tf.constant(10)
d = tf.constant(15)
e = tf.constant(100)

with tf.Session() as sess:
  a_b, a_c, a_d, a_e = sess.run([tf.pow(a,b),
                                 tf.pow(a, c),
                                 tf.pow(a, d),
                                 tf.pow(a, e)])
  print('a_b is: ', a_b)
  print('a_c is: ', a_c)
  print('a_d is: ',a_d)
  print('a_e is:', a_e)

```
Printed values are:
```
 a_b is:  1507045888
 a_c is:  -1957116928
 a_d is:  -606830592
 a_e is: 0

```
**I also tried this with tensorflow 2.0.0-alpha0 version and getting the same error.**
```
import tensorflow as tf
a = tf.constant(50)
b = tf.constant(9)
c = tf.constant(10)
d = tf.constant(15)
e = tf.constant(100)

print('a_b is: ', tf.pow(a, b))
print('a_c is: ', tf.pow(a, c))
print('a_d is: ', tf.pow(a, d))
print('a_e is:',  tf.pow(a, e))

```
printed values are 
```
a_b is:  tf.Tensor(1507045888, shape=(), dtype=int32)
a_c is:  tf.Tensor(-1957116928, shape=(), dtype=int32)
a_d is:  tf.Tensor(-606830592, shape=(), dtype=int32)
a_e is: tf.Tensor(0, shape=(), dtype=int32)
```"
32710,Android Examples,"i already install and run tensorflow examples but it consist of tf_classify,tf_detect,tf_speech and tf_style. so can anyone tell me how to install only tf_detect ?"
32709,out of memory flood on the simplest op,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win7 x64
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0.0rc1
- Python version: 3.7.4
- CUDA/cuDNN version: 10.0, 3.7.5
- GPU model and memory: GTX 1060, 6GB

**Describe the current behavior**

```
import tensorflow as tf
x = [[2.]]
m = tf.matmul(x, x)
```

results infinite flood:
```
2019-09-21 14:55:58.537800: I tensorflow/stream_executor/cuda/cuda_driver.cc:830
] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: ou
t of memory
2019-09-21 14:55:58.540800: I tensorflow/stream_executor/cuda/cuda_driver.cc:830
] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: ou
t of memory
2019-09-21 14:55:58.543800: I tensorflow/stream_executor/cuda/cuda_driver.cc:830
] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: ou
t of memory
2019-09-21 14:55:58.546800: I tensorflow/stream_executor/cuda/cuda_driver.cc:830
] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: ou
t of memory
2019-09-21 14:55:58.549800: I tensorflow/stream_executor/cuda/cuda_driver.cc:830
] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: ou
t of memory
2019-09-21 14:55:58.552800: I tensorflow/stream_executor/cuda/cuda_driver.cc:830
] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: ou
t of memory
2019-09-21 14:55:58.555800: I tensorflow/stream_executor/cuda/cuda_driver.cc:830
] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: ou
t of memory
2019-09-21 14:55:58.566800: I tensorflow/stream_executor/cuda/cuda_driver.cc:830
] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: ou
t of memory
2019-09-21 14:55:58.569800: I tensorflow/stream_executor/cuda/cuda_driver.cc:830
] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: ou
t of memory
2019-09-21 14:55:58.572800: I tensorflow/stream_executor/cuda/cuda_driver.cc:830
] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: ou
t of memory
```

**Describe the expected behavior**

expected to work properly

**Code to reproduce the issue**

```
import tensorflow as tf
x = [[2.]]
m = tf.matmul(x, x)
```

**Other logs**

```
2019-09-21 14:58:51.205800: I tensorflow/stream_executor/platform/default/dso_lo
ader.cc:44] Successfully opened dynamic library cudart64_100.dll
>>> tf.constant(1)
2019-09-21 14:58:54.148800: I tensorflow/stream_executor/platform/default/dso_lo
ader.cc:44] Successfully opened dynamic library nvcuda.dll
2019-09-21 14:58:54.220800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1
618] Found device 0 with properties:
name: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:01:00.0
2019-09-21 14:58:54.226800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1
618] Found device 1 with properties:
name: GeForce GT 730 major: 3 minor: 5 memoryClockRate(GHz): 0.9015
pciBusID: 0000:03:00.0
2019-09-21 14:58:54.229800: I tensorflow/stream_executor/platform/default/dlopen
_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-09-21 14:58:54.239800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1
731] Ignoring visible gpu device (device: 1, name: GeForce GT 730, pci bus id: 0
000:03:00.0, compute capability: 3.5) with core count: 2. The minimum required c
ount is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROC
ESSOR_COUNT.
2019-09-21 14:58:54.245800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1
746] Adding visible gpu devices: 0
2019-09-21 14:58:54.248800: I tensorflow/core/platform/cpu_feature_guard.cc:142]
 Your CPU supports instructions that this TensorFlow binary was not compiled to
use: AVX2
2019-09-21 14:58:54.308800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1
618] Found device 0 with properties:
name: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:01:00.0
2019-09-21 14:58:54.312800: I tensorflow/stream_executor/platform/default/dlopen
_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-09-21 14:58:54.319800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1
746] Adding visible gpu devices: 0
2019-09-21 14:58:55.057800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1
159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-21 14:58:55.061800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1
165]      0
2019-09-21 14:58:55.064800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1
178] 0:   N
2019-09-21 14:58:55.071800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1
304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 wit
h 4675 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bu
s id: 0000:01:00.0, compute capability: 6.1)
```
"
32708,module 'tensorflow' has no attribute 'enable_eager_execution',"I installed *tensorflow-gpu==2.0.0rc1* from pip

```
>>> tf.enable_eager_execution()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: module 'tensorflow' has no attribute 'enable_eager_execution'
```"
32707,Out of memory error during training,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 / Google Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary (Win10)
- TensorFlow version (use command below): 2.0.0rc1
- Python version: 3.6.8 (Colab) / 3.6.9 (Win10)
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: Cuda 10 CuDNN 7.6.3.30 (Win10)
- GPU model and memory: Geforce 1080 Ti, 12 GB (Win10)

**Describe the current behavior**
I am getting an out of memory error during the training of a encoder-decoder model using the subclassing api of tensorflow 2.

**Describe the expected behavior**
Once the graph is built and the batch size is constant, the memory usage of the model should stay the same so that there is no out of memory error upcoming during the training process.

**Code to reproduce the issue**
I have created a minimum example out of my model in google colab:
https://colab.research.google.com/drive/1Xbzcj0ZlALhLhJhv-JaKSVUu3rfB3Urh

**Other info / logs**
The error occurs at about example index (current_index) 25000 of 80000, so there must be an issue that something is written on the gpu memory which leads to the following oom error:

```
<ipython-input-2-fe0e8431350e> in train_model(self, data, batch_size)
     27 
     28             with tf.GradientTape() as tape:
---> 29                 batch_output = self.model([input_sequences_batch, output_sequences_batch])
     30                 loss = self.loss_fn(data[1, current_index:current_index + batch_size, :], batch_output)
     31 

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    889           with base_layer_utils.autocast_context_manager(
    890               self._compute_dtype):
--> 891             outputs = self.call(cast_inputs, *args, **kwargs)
    892           self._handle_activity_regularization(inputs, outputs)
    893           self._set_mask_metadata(inputs, outputs, input_masks)

<ipython-input-2-fe0e8431350e> in call(self, inputs)
     72 
     73         decoder_output = self.sequence_decoder(self.embeddings(output_sequence[:, :-1]), initial_state=[state_h, state_c])
---> 74         output = self.time_distributed_dense(decoder_output)
     75         return output

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    889           with base_layer_utils.autocast_context_manager(
    890               self._compute_dtype):
--> 891             outputs = self.call(cast_inputs, *args, **kwargs)
    892           self._handle_activity_regularization(inputs, outputs)
    893           self._set_mask_metadata(inputs, outputs, input_masks)

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/wrappers.py in call(self, inputs, training, mask)
    252         inner_mask_shape = self._get_shape_tuple((-1,), mask, 2)
    253         kwargs['mask'] = K.reshape(mask, inner_mask_shape)
--> 254       y = self.layer(inputs, **kwargs)
    255       # Shape: (num_samples, timesteps, ...)
    256       output_shape = self.compute_output_shape(input_shape).as_list()

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    889           with base_layer_utils.autocast_context_manager(
    890               self._compute_dtype):
--> 891             outputs = self.call(cast_inputs, *args, **kwargs)
    892           self._handle_activity_regularization(inputs, outputs)
    893           self._set_mask_metadata(inputs, outputs, input_masks)

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/core.py in call(self, inputs)
   1056         outputs = gen_math_ops.mat_mul(inputs, self.kernel)
   1057     if self.use_bias:
-> 1058       outputs = nn.bias_add(outputs, self.bias)
   1059     if self.activation is not None:
   1060       return self.activation(outputs)  # pylint: disable=not-callable

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py in bias_add(value, bias, data_format, name)
   2716       value = ops.convert_to_tensor(value, name=""input"")
   2717       bias = ops.convert_to_tensor(bias, dtype=value.dtype, name=""bias"")
-> 2718     return gen_nn_ops.bias_add(value, bias, data_format=data_format, name=name)
   2719 
   2720 

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_nn_ops.py in bias_add(value, bias, data_format, name)
    753       else:
    754         message = e.message
--> 755       _six.raise_from(_core._status_to_exception(e.code, message), None)
    756   # Add nodes to the TensorFlow graph.
    757   if data_format is None:

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

ResourceExhaustedError: OOM when allocating tensor with shape[3136,50000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:BiasAdd] name: model/time_distributed/dense/BiasAdd/
```"
32706,GPU never in use with Java API,"Hi, I followed the instructions with (this)(https://www.tensorflow.org/install/lang_java?hl=zh-cn).
And I add my implementations with bert pb model, but the GPUs are never used
![image](https://user-images.githubusercontent.com/7105813/65370990-769ed480-dc91-11e9-80db-244ed652ff9d.png).
I got Tesla K80, and cuda 9.0 
![image](https://user-images.githubusercontent.com/7105813/65371023-bd8cca00-dc91-11e9-98e2-74f53302ed3e.png)
![image](https://user-images.githubusercontent.com/7105813/65371031-e0b77980-dc91-11e9-8b45-3a2a8746b0bf.png)

Any suggestions?


"
32705,Reset tf.metrics.mean,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):1.8
- Are you willing to contribute it (Yes/No):yes



**Describe the feature and the current behavior/state.**
Current usage:
m, op_update = tf.metrics.mean(x)
m is the mean of the stream of x. op_update is the update op of local variable ""count"" and ""total"".

However, it's natural to reset all history of the stream, that is, to reset ""count"" and ""total"" to 0. It could look like:
m, op_update, op_reset = tf.metrics.mean(x).


**Will this change the current api? How?**
Yes, tf.metrics.mean will have one more output variable.
Maybe other tf.metrics functions should be alike.

**Who will benefit with this feature?**
I think this feature is natural.
For example, those who need to measure the mean accuracy in every 20 iterations, all accuracy 
data before these 20 iterations should be discarded.


**Any Other info.**
"
32704,nn,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod

## Description of issue (what needs changing):

### Clear description

For example, why should someone use this method? How is it useful?

### Correct links

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined

Are return values defined?

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example?

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
32703,Keras/tensorflow not working in Command Line ,"Hi Team, 

I am trying to execute my pyhton code using Keras and Numpy Library on command Prompt.
It is working fine with Jupyter Notebook.
[keras_prediction - Copy.txt](https://github.com/tensorflow/tensorflow/files/3638219/keras_prediction.-.Copy.txt)


I am facing the following error.


_D:\Vivek\Python\Sigmoid_basic>python keras_prediction.py
Using TensorFlow backend.
Traceback (most recent call last):
  File ""D:\Vivek\Python\Python_64\lib\site-packages\tensorflow\python\platform\s
elf_check.py"", line 47, in preload_check
    ctypes.WinDLL(build_info.msvcp_dll_name)
  File ""D:\Vivek\Python\Python_64\lib\ctypes\__init__.py"", line 364, in __init__

    self._handle = _dlopen(self._name, mode)
OSError: [WinError 126] The specified module could not be found

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""keras_prediction.py"", line 2, in <module>
    import keras as ks
  File ""D:\Vivek\Python\Python_64\lib\site-packages\keras\__init__.py"", line 3,
in <module>
    from . import utils
  File ""D:\Vivek\Python\Python_64\lib\site-packages\keras\utils\__init__.py"", li
ne 6, in <module>
    from . import conv_utils
  File ""D:\Vivek\Python\Python_64\lib\site-packages\keras\utils\conv_utils.py"",
line 9, in <module>
    from .. import backend as K
  File ""D:\Vivek\Python\Python_64\lib\site-packages\keras\backend\__init__.py"",
line 1, in <module>
    from .load_backend import epsilon
  File ""D:\Vivek\Python\Python_64\lib\site-packages\keras\backend\load_backend.p
y"", line 89, in <module>
    from .tensorflow_backend import *
  File ""D:\Vivek\Python\Python_64\lib\site-packages\keras\backend\tensorflow_bac
kend.py"", line 5, in <module>
    import tensorflow as tf
  File ""D:\Vivek\Python\Python_64\lib\site-packages\tensorflow\__init__.py"", lin
e 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-im
port
  File ""D:\Vivek\Python\Python_64\lib\site-packages\tensorflow\python\__init__.p
y"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""D:\Vivek\Python\Python_64\lib\site-packages\tensorflow\python\pywrap_ten
sorflow.py"", line 30, in <module>
    self_check.preload_check()
  File ""D:\Vivek\Python\Python_64\lib\site-packages\tensorflow\python\platform\s
elf_check.py"", line 55, in preload_check
    % build_info.msvcp_dll_name)
ImportError: Could not find 'msvcp140.dll'. TensorFlow requires that this DLL be
 installed in a directory that is named in your %PATH% environment variable. You
 may install this DLL by downloading Visual C++ 2015 Redistributable Update 3 fr
om this URL: https://www.microsoft.com/en-us/download/details.aspx?id=53587_


Please find my code attached.

Please advise

Thanks and Regards
Vivek Srivastava"
32702,Keras casts targets to incorrect dtype,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0rc1
- Python version: 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

Keras tries to cast targets to the dtypes of the model outputs. Currently it assumes that every model output has a corresponding target, so when doing this casting it just matches outputs and targets up one-to-one. But in the case where some outputs are not part of the loss function (i.e. they were missing from the loss dictionary passed to `compile`), this may match outputs to the wrong targets. Then it casts targets to the wrong dtype, causing errors.

**Describe the expected behavior**

Keras should match up targets with the correct output when casting, according to the loss dictionary defined in `compile`.  If a model output is not part of the loss function, then it should be ignored when casting targets.

**Code to reproduce the issue**

``` python
import tensorflow as tf
import numpy as np

inp = tf.keras.layers.Input(shape=(1,))
out0 = tf.cast(inp, tf.int32)
out1 = tf.cast(inp, tf.float64)

model = tf.keras.Model(inputs=inp, outputs=[out0, out1])

model.compile(loss={model.output_names[1]: tf.losses.mse})

model.evaluate(
    np.ones((1, 1), dtype=np.float32),
    {model.output_names[1]: np.ones((1, 1), dtype=np.float32)},
)
```

This results in the error
```
TypeError: Value passed to parameter 'x' has DataType int32 not in list of allowed values: float16, float32, float64, complex64, complex128
```
"
32701,Segmentation Fault (core dumped) while training on 1.14,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Running a public github repo
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.2 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): 1.14.0
- TensorFlow version (use command below): 1.14.0
- Python version: 2.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: NVIDIA TITAN

**Describe the current behavior**

I am attempting to train the neural network in this repo, [sketch2normal](https://github.com/Ansire/sketch2normal)

Every time I attempt to train the model, it always gives me an error message `Segmentation fault (core dumped)` at Epoch 0 sometime between 10/449 -  90/449. I'm also getting many warning messages saying that many of the methods being called, like `tf.summary.FileWriter`, `tf.summary.merge`, and more are deprecated.  

After attempting to train with multiple print statements, I found that the place where the code seg faults is in the train function:
```
      def train(self, args):

        self.d_optim = tf.train.RMSPropOptimizer(learning_rate=args.lr).minimize(self.d_loss, var_list=self.d_vars)
        self.g_optim = tf.train.RMSPropOptimizer(learning_rate=args.lr).minimize(self.g_loss, var_list=self.g_vars)
        self.clip_d_vars_ops = [val.assign(tf.clip_by_value(val, -self.clamp, self.clamp)) for val in self.d_vars]
        tf.global_variables_initializer().run()

        init_op = tf.global_variables_initializer()
        self.sess.run(init_op)

        self.g_summary = tf.summary.merge([self.fake_B_sum, self.real_B_sum,self.d_loss_fake_sum, self.g_loss_sum])
        self.d_summary = tf.summary.merge([self.d_loss_real_sum, self.d_loss_sum])
        self.visual_loss_summary = tf.summary.merge([self.pixel_wised_loss_sum, self.masked_loss_sum])
        self.writer = tf.summary.FileWriter(""./logs"", self.sess.graph)

        counter = 1
        start_time = time.time()

        if self.load(args.checkpoint_dir):
            print("" [*] Load SUCCESS"")
        else:
            print("" [!] Load failed..."")

        for epoch in xrange(args.epoch):
            data = glob('./datasets/{}/train/*.png'.format(self.dataset_name))
            np.random.shuffle(data)
            batch_idxs = min(len(data), 1e8) // (self.batch_size*self.n_critic)
            print('[*] run optimizor...')

            for idx in xrange(0, batch_idxs):
                errD=.0
                batch_list = [self.load_training_imgs(data, idx+i) for i in xrange(self.n_critic)]
                for j in range(self.n_critic):
                    batch_images = batch_list[j]
                    _, errD, errd_real, errd_fake, errVis_sum, summary_str = self.sess.run([self.d_optim, self.d_loss,
                                                                                self.d_loss_real, self.d_loss_fake,
                                                                                            self.visual_loss_summary,
                                                                                            self.d_summary],
                                                                                           feed_dict={self.real_data: batch_images})
                    self.sess.run(self.clip_d_vars_ops)
                    self.writer.add_summary(summary_str, counter)
                    self.writer.add_summary(errVis_sum, counter)

                # Update G network
                _, errG, summary_str = self.sess.run([self.g_optim, self.g_loss, self.g_summary],
                                               feed_dict={self.real_data: batch_list[np.random.randint(0, self.n_critic, size=1)[0]]})
                self.writer.add_summary(summary_str, counter)

                current = time.time()
                print(""Epoch: [%2d] [%4d/%4d] time: %4.4f, d_loss: %.8f, g_loss: %.8f"" \
                      % (epoch, idx, batch_idxs, current - start_time, errD, errG))
                start_time = current

                if np.mod(counter, 100) == 1:
                    self.sample_model(args.sample_dir, epoch, idx)

                if np.mod(counter, 1000) == 2:
                    self.save(args.checkpoint_dir, counter)
                counter += 1
```

Specifically on this line, where self.sess.run is called:
```
                    _, errD, errd_real, errd_fake, errVis_sum, summary_str = self.sess.run([self.d_optim, self.d_loss,
                                                                                self.d_loss_real, self.d_loss_fake,
                                                                                            self.visual_loss_summary,
                                                                                            self.d_summary],
                                                                                           feed_dict={self.real_data: batch_images})
```
**Describe the expected behavior**
The expected behavior is that it would finish training. Can you provide any recommendations as to how to stop it from seg faulting during training (and why it stops at a different point each time)? I'm wondering if the warnings could have to do with it?  I also have seen other people online having issues with tf.Session, so I'm wondering if that could be it?
"
32698,Tensorflow v2.0rc* is impossible to install with pip 18.1,"I'm installing TF on Debian 10 with python 3.6: `Debian 4.19.37-5+deb10u1rodete2 (2019-08-06 > 2018) x86_64 GNU/Linux`

TF 2.0beta1 had support for the manylinux1 tag which allowed me to successfully install it.
TF 2.0rc1 switched to the `manylinux2010` tag which is not supported on my Debian installation with pip 18.1 and rc1 cannot be installed.

Can TF not require upgrading pip to install?

```
https://pypi.org/simple/tensorflow/ :
tensorflow-2.0.0b1-cp36-cp36m-manylinux1_x86_64.whl
tensorflow-2.0.0rc0-cp36-cp36m-manylinux2010_x86_64.whl

```

```
python3 -c ""import wheel.pep425tags as w; print(w.get_supported())"" | tr '\n' '\0' | sed -E 's/\),/),\n/g'
[('cp36', 'cp36m', 'linux_x86_64'),
 ('cp36', 'abi3', 'linux_x86_64'),
 ('cp36', 'none', 'linux_x86_64'),
 ('cp35', 'abi3', 'linux_x86_64'),
 ('cp34', 'abi3', 'linux_x86_64'),
 ('cp33', 'abi3', 'linux_x86_64'),
 ('cp32', 'abi3', 'linux_x86_64'),
 ('cp36', 'none', 'any'),
 ('cp3', 'none', 'any'),
 ('cp35', 'none', 'any'),
 ('cp34', 'none', 'any'),
 ('cp33', 'none', 'any'),
 ('cp32', 'none', 'any'),
 ('cp31', 'none', 'any'),
 ('cp30', 'none', 'any'),
 ('py3', 'none', 'linux_x86_64'),
 ('py36', 'none', 'any'),
 ('py3', 'none', 'any'),
 ('py35', 'none', 'any'),
 ('py34', 'none', 'any'),
 ('py33', 'none', 'any'),
 ('py32', 'none', 'any'),
 ('py31', 'none', 'any'),
 ('py30', 'none', 'any')]
```"
32697,"In Windows, use of fit_generator and evaluate_generator result in ""builtins.TypeError: 'NoneType' object is not subscriptable""","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes, provided lower in the post.

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 7, Python 3.7, tensorflow 1.13.1

- TensorFlow installed from (source or binary):
via pip

- TensorFlow version (use command below): 1.13,1

- Python version: 3.7
- CUDA/cuDNN version: 10
- GPU model and memory: RTX

**Describe the current behavior**
In Windows, when run on a GPU, the provided code results in th following error:
```
File ""c:\python37\Lib\threading.py"", line 885, in _bootstrap
  self._bootstrap_inner()
File ""c:\python37\Lib\threading.py"", line 917, in _bootstrap_inner
  self.run()
File ""c:\python37\Lib\threading.py"", line 865, in run
  self._target(*self._args, **self._kwargs)
File ""c:\python37\Lib\multiprocessing\pool.py"", line 121, in worker
  result = (True, func(*args, **kwds))
File ""c:\python37\Lib\site-packages\tensorflow\python\keras\utils\data_utils.py"", line 445, in get_index
  return _SHARED_SEQUENCES[uid][i]

builtins.TypeError: 'NoneType' object is not subscriptable
```

**Describe the expected behavior**
No error

**Code to reproduce the issue**
```
# system imports
import os
import random

# lib imports
import matplotlib.pyplot as plt
import numpy as np
import scipy as sp
import cv2
import tensorflow as tf
import sklearn
import sklearn.metrics
import tqdm
import tensorflow as tf
import os

import matplotlib.pyplot as plt

EPS = np.finfo(float).eps

#=====================================================================================================================================================
# Input parameters
#-----------------------------------------------------------------------------------------------------------------------------------------------------
batchSize = 16
imageSize = 335
tileSize = 256

#-----------------------------------------------------------------------------------------------------------------------------------------------------
def unet(numCoefs, input_size = (192,192,1), shrinkFactor = 1, name = ''):
    inputs = tf.keras.layers.Input(input_size)
    conv1 = tf.keras.layers.Conv2D(64//shrinkFactor, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)
    conv1 = tf.keras.layers.Conv2D(64//shrinkFactor, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)
    pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2))(conv1)
    conv2 = tf.keras.layers.Conv2D(128//shrinkFactor, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)
    conv2 = tf.keras.layers.Conv2D(128//shrinkFactor, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)
    pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2))(conv2)    
    conv3 = tf.keras.layers.Conv2D(256//shrinkFactor, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)
    conv3 = tf.keras.layers.Conv2D(256//shrinkFactor, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)
    pool3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2))(conv3)    
    conv4 = tf.keras.layers.Conv2D(512//shrinkFactor, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)
    conv4 = tf.keras.layers.Conv2D(512//shrinkFactor, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)
    drop4 = tf.keras.layers.Dropout(0.0)(conv4)
    pool4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2))(drop4)    

    conv5 = tf.keras.layers.Conv2D(1024//shrinkFactor, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)
    conv5 = tf.keras.layers.Conv2D(1024//shrinkFactor, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', name='code_'+name)(conv5)
    drop5 = tf.keras.layers.Dropout(0.0)(conv5)
    
    up6 = tf.keras.layers.Conv2D(512//shrinkFactor, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(tf.keras.layers.UpSampling2D(size = (2,2), interpolation='bilinear')(drop5))
    merge6 = tf.keras.layers.concatenate([drop4,up6], axis = 3)
    conv6 = tf.keras.layers.Conv2D(512//shrinkFactor, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)
    conv6 = tf.keras.layers.Conv2D(512//shrinkFactor, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)

    up7 = tf.keras.layers.Conv2D(192//shrinkFactor, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(tf.keras.layers.UpSampling2D(size = (2,2), interpolation='bilinear')(conv6))
    merge7 = tf.keras.layers.concatenate([conv3,up7], axis = 3)
    conv7 = tf.keras.layers.Conv2D(192//shrinkFactor, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)
    conv7 = tf.keras.layers.Conv2D(192//shrinkFactor, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)

    up8 = tf.keras.layers.Conv2D(128//shrinkFactor, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(tf.keras.layers.UpSampling2D(size = (2,2), interpolation='bilinear')(conv7))
    merge8 = tf.keras.layers.concatenate([conv2,up8], axis = 3)
    conv8 = tf.keras.layers.Conv2D(128//shrinkFactor, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)
    conv8 = tf.keras.layers.Conv2D(128//shrinkFactor, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)

    up9 = tf.keras.layers.Conv2D(64//shrinkFactor, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(tf.keras.layers.UpSampling2D(size = (2,2), interpolation='bilinear')(conv8))
    merge9 = tf.keras.layers.concatenate([conv1,up9], axis = 3)
    conv9 = tf.keras.layers.Conv2D(64//shrinkFactor, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)
    conv9 = tf.keras.layers.Conv2D(64//shrinkFactor, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)
    conv9 = tf.keras.layers.Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)
    conv10 = tf.keras.layers.Conv2D(1, 1, activation = 'relu')(conv9)
    
    r = tf.keras.layers.Lambda(lambda x: (x - tf.keras.backend.min(x)) / (tf.keras.backend.max(x) - tf.keras.backend.min(x)), name = 'reconstruction_'+name)(conv10)

    model = tf.keras.models.Model(inputs, r)
    
    return model
#-----------------------------------------------------------------------------------------------------------------------------------------------------
def buildModel():           
    numCoefs = 3
    reconstruction = unet(numCoefs, input_size=(256,256,1), shrinkFactor=4)
    model = tf.keras.models.Model(reconstruction.inputs[0], reconstruction.outputs[0])
        
    return model
#-----------------------------------------------------------------------------------------------------------------------------------------------------

#=====================================================================================================================================================
# Define and create generators
#-----------------------------------------------------------------------------------------------------------------------------------------------------
class TrainGenerator(tf.keras.utils.Sequence):
    #-------------------------------------------------------------------------------------------------------------------------------------------------
    def __init__(self, batchSize, tileSize, imageSize):        
        self._batchSize = batchSize
        self._imageSize = imageSize
        self._tileSize = tileSize
        self._numFiles = 1000
        return
    #
    #-------------------------------------------------------------------------------------------------------------------------------------------------
    def next(self):
        return self.__getitem__(0)
    #-------------------------------------------------------------------------------------------------------------------------------------------------
    def __getitem__(self, idx):
        # idx intentionally not used
        return self._next()
    #
    #-------------------------------------------------------------------------------------------------------------------------------------------------
    # Returns number of steps/iterations to perform to go through all the data once
    def getStepsPerEpoch(self):
        return self._numFiles // self._batchSize 
    #-------------------------------------------------------------------------------------------------------------------------------------------------
    def __len__(self):
        return self.getStepsPerEpoch()
    #
    #-------------------------------------------------------------------------------------------------------------------------------------------------
    # Returns a batch of data.    
    def _next(self):         
        xRaw = np.zeros((self._batchSize, self._imageSize, self._imageSize, 1), dtype='complex64')
        # Prep for net input
        x = np.zeros((self._batchSize, self._tileSize, self._tileSize, 1), dtype='float32')
        xSlc = np.zeros((self._batchSize, self._tileSize, self._tileSize, 1), dtype='complex64')
        xDefocus = np.zeros((self._batchSize, self._tileSize, self._tileSize, 1), dtype='float32')
        y = np.zeros(self._batchSize)        
                
        for k in range(self._batchSize):
            y[k] = 1
        #              
        
        # More augmentation
        for k in range(self._batchSize):                   
            # Random Crops
            crop = self._imageSize - self._tileSize            
            startX = np.random.randint(crop)
            startY = np.random.randint(crop)
            slc = xRaw[k, startX:startX + self._tileSize, startY:startY + self._tileSize, 0]     
            x[k, :,:, 0] = 0                
        #

        return xDefocus, x
    #-------------------------------------------------------------------------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------------------------------------------------------------------------
class TestGenerator(tf.keras.utils.Sequence):
    #-------------------------------------------------------------------------------------------------------------------------------------------------
    def __init__(self, batchSize, tileSize = 256, imageSize = 335):        
        self._batchSize = batchSize
        self._tileSize = tileSize
        self._imageSize = imageSize       
        self._numFiles = 5000
        return
    #-------------------------------------------------------------------------------------------------------------------------------------------------    
    def __len__(self):
        return self.getStepsPerEpoch()    
    #-------------------------------------------------------------------------------------------------------------------------------------------------
    # Returns number of steps/iterations to perform to go through all the data once
    def getStepsPerEpoch(self):
        return self._numFiles // self._batchSize
    #
    #-------------------------------------------------------------------------------------------------------------------------------------------------
    def __getitem__(self, idx):        
        return self.next(idx)
    #    
    #-------------------------------------------------------------------------------------------------------------------------------------------------
    # Returns batch sized used to iniitalize
    def getBatchSize(self):
        return self._batchSize    
    #-------------------------------------------------------------------------------------------------------------------------------------------------
    # Returns a batch of data.
    def next(self, batch_id):
        xs = np.zeros((self._batchSize, self._tileSize , self._tileSize , 1), dtype='float32')
        xSlc = np.zeros((self._batchSize, self._tileSize , self._tileSize , 1), dtype='complex64')
        ys = np.zeros(self._batchSize)
        xsOrig = np.zeros((self._batchSize, self._tileSize , self._tileSize , 1), dtype='float32')                   

        masterOffset = (self._imageSize -self._tileSize )//2
        center = [self._imageSize //2, self._imageSize //2]
        for k in range(self._batchSize):            
            tile = np.zeros((335,335))
            
            # Center  Crops
            crop = self._imageSize  - self._tileSize             
            startX = crop//2
            startY = crop//2
            slc = tile[startX:startX + self._tileSize , startY:startY + self._tileSize]                                     

            # Label
            ys[k] = 1
            
        #
        
        return xs, xsOrig
    #-------------------------------------------------------------------------------------------------------------------------------------------------

#-----------------------------------------------------------------------------------------------------------------------------------------------------
def main():
   
    #-----------------------------------------------------------------------------------------------------------------------------------------------------
    # So many random seeds
    random.seed(1)
    np.random.seed(1)
    tf.random.set_random_seed(1)
    
    #=====================================================================================================================================================
    # Create output folder of model
    #-----------------------------------------------------------------------------------------------------------------------------------------------------          
    # Create our two generators
    trainGenerator = TrainGenerator(batchSize, 256, 335)    
    testGenerator = TestGenerator(batchSize, 256, 335)
        
    #-----------------------------------------------------------------------------------------------------------------------------------------------------
    # Build model
    model = buildModel()
    model.summary()
    
    #-----------------------------------------------------------------------------------------------------------------------------------------------------
    # Log the model traincompileing parameters
    stepsPerEpochTrain = int(trainGenerator.getStepsPerEpoch())
    stepsPerEpochTest = testGenerator.getStepsPerEpoch()
    print('=== Fitting Model ===')
    print(' Batch size:                  {0}'.format(batchSize))
    print(' Steps per training epoch:    {0}'.format(stepsPerEpochTrain))    
    print(' Steps per testing epoch:     {0}'.format(stepsPerEpochTest))
    #-----------------------------------------------------------------------------------------------------------------------------------------------------
    
    #=====================================================================================================================================================
    # Begin training            
    model.compile(optimizer = tf.keras.optimizers.RMSprop(lr=1e-3), loss = 'mse')    
    
    for epoch in range(0,9999):
        print('========== Epoch {0} =========='.format(epoch))
        
        #-------------------------------------------------------------------------------------------------------------------------------------------------
        # Train the model
        history = model.fit_generator(trainGenerator, workers=5)         
        trainError = history.history['loss'][0]
    
        #-------------------------------------------------------------------------------------------------------------------------------------------------  
        # Dump out mosaic from a batch
        x,y = testGenerator.next(1)
               
        #-------------------------------------------------------------------------------------------------------------------------------------------------
        # Test the model    
        r = model.evaluate_generator(trainGenerator, workers=5, verbose=1)  # idg Leaving this here for postarity. maybe keras will work someday.

        # Compute test loss
        test_err = np.mean(r)    
    # end training epoch
    return
#-----------------------------------------------------------------------------------------------------------------------------------------------------
  
if __name__== ""__main__"":
    main()

    print('done')
```

This code will spit out Nan's but that is okay.  Its just to address the point.  In windows, the error you will get after the first epoch is:
```
File ""c:\python37\Lib\threading.py"", line 885, in _bootstrap
  self._bootstrap_inner()
File ""c:\python37\Lib\threading.py"", line 917, in _bootstrap_inner
  self.run()
File ""c:\python37\Lib\threading.py"", line 865, in run
  self._target(*self._args, **self._kwargs)
File ""c:\python37\Lib\multiprocessing\pool.py"", line 121, in worker
  result = (True, func(*args, **kwds))
File ""c:\python37\Lib\site-packages\tensorflow\python\keras\utils\data_utils.py"", line 445, in get_index
  return _SHARED_SEQUENCES[uid][i]

builtins.TypeError: 'NoneType' object is not subscriptable
```"
32696,[TF1.14] Network compiles localy but not on TPU with error 'InvalidArgumentError: Undeclared output of TPU computation.',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14
- Python version: 3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: TPU 

I created a DenseNet, which I succesfully compile it locally with GPU, but when I try to compile on  a TPU device, the following error appears
```
Traceback (most recent call last):
  File ""attention_dense.py"", line 258, in <module>
    model.fit(get_training_dataset(), validation_data=get_validation_dataset(),  initial_epoch=0, steps_per_epoch=steps_per_epoch ,validation_steps=val_steps, epochs=EPOCHS, verbose=1, callbacks=clbk)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py"", line 649, in fit
    validation_freq=validation_freq)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_distributed.py"", line 128, in fit_distributed
    validation_freq=validation_freq)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_distributed.py"", line 395, in experimental_tpu_fit_loop
    callbacks._call_begin_hook(mode)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/callbacks.py"", line 262, in _call_begin_hook
    self.on_train_begin()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/callbacks.py"", line 378, in on_train_begin
    callback.on_train_begin(logs)
  File ""/home/frank_lab/clr.py"", line 122, in on_train_begin
    K.set_value(self.model.optimizer.lr, self.base_lr)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py"", line 3038, in set_value
    get_session().run(assign_op, feed_dict={assign_placeholder: value})
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py"", line 462, in get_session
    _initialize_variables(session)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py"", line 879, in _initialize_variables
    [variables_module.is_variable_initialized(v) for v in candidate_vars])
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 950, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1173, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1350, in _do_run
    run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1370, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Undeclared output of TPU computation. A common cause of this error is variable initializers that depend on the TPU computation. Edge: node dense_1_2/re_lu_1/Relu (defined at attention_dense.py:258) :0 -> node tf_op_layer_add/add (defined at attention_dense.py:258) :0
```
The code is the following

```
def conv(input, kernel, filt, stride, dilation, pad='same'):
    x = layers.Conv2D(filters=filt, kernel_size=kernel, strides=stride, dilation_rate=dilation, padding=pad, kernel_regularizer=tf.keras.regularizers.l2(l=0.01))(input)
    return x

def conv_down(input, filters):
    x = conv(input, 3, filters, 2 ,1)
    x = layers.BatchNormalization(axis=-1, fused=True)(x)
    x = layers.LeakyReLU(alpha=0.1)(x) 
    return x

def conv_block(input, filters, stride=1, dilation=2, pad='same', bottleneck=True):
    x = layers.BatchNormalization(axis=-1, fused=True)(input)
    x = layers.LeakyReLU(alpha=0.1)(x) 
    if bottleneck:
        x = conv(x, kernel=1, filt=(filters*4), dilation=dilation, stride=1, pad=pad)
        x = layers.BatchNormalization(axis=-1, fused=True)(x)
        x = layers.LeakyReLU(alpha=0.1)(x) 
    x = conv(x, kernel=3, filt=filters, stride=stride, dilation=dilation, pad=pad)
    return x

def dense_block(x, filters, layers, bottleneck=True):
    x_list = [x]
    for i in range(layers):
        cb = conv_block(x, filters, dilation=2, bottleneck=True)
        x_list.append(cb)
        x = tf.keras.layers.concatenate([x, cb], axis=-1)
        x = attention(x)
    return x

def transition_block(input, filters, att=True):
    x = layers.BatchNormalization(axis=-1, fused=True)(input)
    x = layers.LeakyReLU(alpha=0.1)(x) 
    x = conv(x, kernel=1, filt=filters, stride=1, dilation=2, pad='same')
    x = layers.AveragePooling2D((2,2), strides=(2,2))(x)
    if att:
        x = attention(x)
    return x
    
def attention(input):
    x = channel_att(input)
    x = spatial_att(x)
    return x
    
def channel_att(input, ratio=8):
    channel = input.get_shape()[-1]
    ####
    avg_pool = tf.keras.layers.GlobalAveragePooling2D()(input)
    avg_pool = tf.keras.layers.Reshape((1,1,channel))(avg_pool)
    max_pool = tf.keras.layers.GlobalMaxPooling2D()(input)
    max_pool = tf.keras.layers.Reshape((1,1,channel))(max_pool)
    ####
    mlp_0 = layers.Dense(units=channel//ratio, activation=layers.ReLU())
    mlp_1 = layers.Dense(units=channel, activation=layers.ReLU())
    avg_ = mlp_1(mlp_0(avg_pool))
    max_ = mlp_1(mlp_0(max_pool))
    scale = keras.activations.sigmoid(avg_+max_)
    return input*scale

def spatial_att(input, kernel=7):
    avg_pool = tf.math.reduce_mean(input, axis=[3], keepdims=True)
    max_pool = tf.math.reduce_max(input, axis=[3], keepdims=True)
    concat = tf.concat([avg_pool, max_pool], axis=3)
    concat = layers.Conv2D(filters=1, kernel_size=kernel, padding='same',use_bias=False)(concat)
    concat = keras.activations.sigmoid(concat)
    return input*concat


def create_model():
    Input = layers.Input(shape=(540, 540, 3))
    x = conv(Input, kernel=3, filt=64, stride=1, dilation=2)
    for i in range(8):
        x = dense_block(x, filters=128, layers=3, bottleneck=True)
        x = transition_block(x, filters=128, att=True)
    x = dense_block(x, filters=128, layers=4, bottleneck=True)
    x = layers.BatchNormalization(axis=-1, fused=True)(x)
    x = conv(x, kernel=1, filt=45, stride=1, dilation=1)
    model = tf.keras.Model(inputs=Input, outputs=x)
    model.compile(optimizer=keras.optimizers.SGD(), loss=custom_loss)
    return model

with tpu_strategy.scope():
    model=create_model()

model.fit(get_training_dataset(), validation_data=get_validation_dataset(),  initial_epoch=0, steps_per_epoch=steps_per_epoch ,validation_steps=val_steps, epochs=EPOCHS, verbose=1, callbacks=clbk)
```"
32694,"""TypeError: list indices must be integers or slices, not str"" in preempted_hook.py ","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below):1.14
- Python version:3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.0
- GPU model and memory: Cloud TPU v2

**Describe the current behavior**
```
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/tpu/preempted_hook.py"", line 89, in run
    self._cluster._tpu, response['state'], response['health'])  # pylint: disable=protected-access
TypeError: list indices must be integers or slices, not str
```
Looks like response is a list and not a dictionary.

**Describe the expected behavior**
No exception thrown

**Other info / logs**
N/A"
32693,Tensorflow 2.0 tf.lite.TFLiteConverter.from_keras_model giving 'str' object has no attribute 'call',"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 on Amazon EC2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): pip 
- TensorFlow version (use command below): tensorflow 2.0 rc1
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: On CPU and on google colab also
- GPU model and memory: On google colab, 

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

It's not able to convert the LSTM model to tflite format

**Describe the expected behavior**
it should be able to convert the model to tflite format.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_keras_model(""language_small_adam-01.hdf5"")
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-58-c5d53462d122> in <module>
      1 import tensorflow as tf
----> 2 converter = tf.lite.TFLiteConverter.from_keras_model(""language_small_adam-01.hdf5"")
      3 tflite_model = converter.convert()
      4 open(""converted_model.tflite"", ""wb"").write(tflite_model)

~/anaconda3/envs/test/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py in from_keras_model(cls, model)
    380       TFLiteConverter object.
    381     """"""
--> 382     func = _saving_utils.trace_model_call(model)
    383     concrete_func = func.get_concrete_function()
    384     return cls([concrete_func])

~/anaconda3/envs/test/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saving_utils.py in trace_model_call(model, input_signature)
    122   """"""
    123   if input_signature is None:
--> 124     if isinstance(model.call, def_function.Function):
    125       input_signature = model.call.input_signature
    126 

AttributeError: 'str' object has no attribute 'call'"
32692,Hi ,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

```
# Copy and paste here
```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32691,TF 1.14.0 training crashes with unimplemented Conv2D errors (works fine in TF 1.13.2),"**Environment**
- Ubuntu 16.04:
- Docker based tensorflow/tensorflow:1.14.0-gpu
- tensor2tensor==1.14.0 (pip installed in container)
- Python 2.7
- CUDA/cuDNN version: 10/7 (defaults from docker image)
- GPUs (tested on many from 1080 to RTX Titan)

**Issue**
Change in Tensorflow has broken tensor2tensor librispeech training.

Running librispeech training crashes with Unimplemented Conv2D errors.
```
  (0) Unimplemented:  The Conv2D op currently only supports the NHWC tensor format on the CPU. The op was given the format: NCHW
         [[{{node Conv2D}}]]
  (1) Unimplemented:  The Conv2D op currently only supports the NHWC tensor format on the CPU. The op was given the format: NCHW
         [[{{node Conv2D}}]]
         [[Shape_3/_8]]
```

**Expected behavior**
This works fine in earlier versions of Tensorflow (e.g. 1.13.2).

**Code to reproduce the issue**
Via Nvidia Docker Hub
run tensorflow/tensorflow:1.14.0-gpu
pip install tensorflow-hub && pip install tensor2tensor
apt-get update && apt-get install sox
`t2t-trainer --problem=librispeech_clean_small --model=transformer --output_dir=/models/JUNK --data_dir=/data/ --save_checkpoints_secs=1800 --schedule=train --hparams_set=transformer_librispeech`
(note: sox and --generate are only needed once, to prep the dataset)

**Other info / logs**
Related to closed issue #32017."
32690,How to restore weights of model from zoo as trainable or how to train custom model with object detection script,"Hello
I'm trying to add some new outs to ssd mobilenet v2 taken from model zoo. How to restore graph and variables data to train my additional layers and fine tune mobilenet for better future extracting.</br>
In my tries i used tf.Saver and simple import_meta_graph and restore.</br>
Or push me to right way with using TFObjectDetection API for this task"
32689,tf.train.import_meta_graph raise ValueError(str(e)) ValueError: Cannot add function '__inference_Dataset_flat_map_read_one_file_11' because a different function with the same name already exists.,"Traceback (most recent call last):
  File ""test.py"", line 121, in <module>
    test()
  File ""test.py"", line 67, in test
    saver = tf.train.import_meta_graph('checkpoint/-301.meta')
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1449, in import_meta_graph
    **kwargs)[0]
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1473, in _import_meta_graph_with_return_elements
    **kwargs))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/meta_graph.py"", line 857, in import_scoped_meta_graph_with_return_elements
    return_elements=return_elements)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/importer.py"", line 431, in import_graph_def
    raise ValueError(str(e))
ValueError: Cannot add function '__inference_Dataset_flat_map_read_one_file_11' because a different function with the same name already exists.

using tensorflow version 1.14

？？？ why? how?"
32687,CocoaPods could not find compatible version for pod 'TensorFlowLiteSwift'- with iOS 8 deployment target,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32686,"Hi,","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
