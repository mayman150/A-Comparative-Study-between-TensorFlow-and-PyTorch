Issue Number,Issue Title,Issue Body
56858,How to freeze past of FakeQuantWithMinMaxVars when i fakequatize training?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

binary

### Tensorflow Version

TF 1.15

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The model backbone FakeQuantWithMinMaxVars will be freezed When i fintinue a fakequatize model`s head.
```


### Standalone code to reproduce the issue

```shell
When I finetune the trained fake-quantization model, the FakeQuantWithMinMaxVars in the backbone network will change, relative to my pre-trained model, but I don't want this value to change, how can I modify code to ensure this value will not be change during training.
```


### Relevant log output

_No response_</details>"
56857,next  PR,"next  PR

_Originally posted by @DharmendraGITHB in https://github.com/tensorflow/tensorflow/issues/38668#issuecomment-1192153328_"
56855,tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 0 [Op:IteratorGetNext] when reading from S3 storage,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.8.1-10-g2ea19cbb575 2.8.2

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hello,

We see the error in ""Relevant log output"" when training using tensorflow-cpu. We were able to create a small stress test that exhibits the same failure. Both the training and stress test are essentially loading tf-records from S3 and we have seen the failure occur anywhere from ~30min to 20+ hours.

This happens when we run with single process as well as when we run with 8 processes. When running with 8 processes, one process fails and the rest keep running, but other processes may proceed to fail in the same manner later on if we let it keep running.

We are running on AWS with various instance types. The dataset used is imagenet tf-records stored on an S3 bucket.

To reproduce we do the following: 
0. (Place the imagenet tf-records onto S3) and set TRAIN_TFRECORDS_PATH in data.py
1. Launch an EC2 instance with a Deep Learning AMI (Ubuntu 18.04) Version 50.0 AMI (for example ami-0050625d58fa27b6d on us-west-2)
2. Run tensorflow docker -- tensorflow/tensorflow:2.8.2
3. pip install psutil -y
4. Copy two files below (data.py and run.sh)
5. chmod u+x run.sh
6. run.sh
```


### Standalone code to reproduce the issue

```shell
run.sh:
for i in {0..7}
do
   python data.py -p $i 2>&1 | tee ""error${i}.log"" &
done

data.py:
import tensorflow as tf
import tensorflow_io as tfio
import copy
from pathlib import Path
import os
import argparse
import psutil
from datetime import datetime

TRAIN_TFRECORDS_PATH = ""s3://test/tf_records/""
IMAGE_SIZE = (192, 192)


def _image_shape(image):
    shape = tf.shape(image)
    return shape[0], shape[1]

def read_tfrecord(example):
    features = {
        ""image"": tf.io.FixedLenFeature([], tf.string),
        ""label"": tf.io.FixedLenFeature([], tf.int64)
    }
    example = tf.io.parse_single_example(example, features)
    image = tf.image.decode_jpeg(example[""image""])
    class_num = example[""label""]
    return image, class_num


def resize_and_crop_image(image):
    image = tf.image.crop_to_bounding_box(image, 0, 0, 16, 16)
    return image


def normalize(image, dtype):
    imagenet_mean = tf.constant([0.485, 0.456, 0.406], dtype=dtype) * 255
    imagenet_std = tf.constant([0.229, 0.224, 0.225], dtype=dtype) * 255
    image = tf.cast(image, dtype=dtype)
    image = (image - imagenet_mean) / imagenet_std
    return image

s3_filenames = [
    f'{TRAIN_TFRECORDS_PATH}5.99.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.0.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.1.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.10.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.100.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.101.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.102.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.103.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.104.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.105.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.106.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.107.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.108.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.109.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.11.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.110.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.111.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.112.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.113.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.114.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.115.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.116.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.117.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.118.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.119.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.12.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.120.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.121.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.122.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.123.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.124.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.125.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.126.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.127.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.128.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.129.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.13.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.130.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.131.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.132.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.133.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.134.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.135.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.136.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.137.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.138.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.139.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.14.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.140.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.141.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.142.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.143.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.144.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.145.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.146.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.147.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.148.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.149.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.15.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.150.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.151.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.152.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.153.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.154.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.155.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.156.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.157.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.158.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.159.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.16.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.160.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.161.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.162.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.163.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.164.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.165.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.166.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.167.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.168.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.169.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.17.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.170.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.171.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.172.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.173.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.174.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.175.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.176.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.177.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.178.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.179.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.18.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.180.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.181.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.182.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.183.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.184.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.185.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.186.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.187.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.188.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.189.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.19.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.190.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.191.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.192.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.193.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.194.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.195.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.196.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.197.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.198.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.199.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.2.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.20.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.200.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.201.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.202.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.203.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.204.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.205.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.206.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.207.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.208.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.209.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.21.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.210.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.211.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.212.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.213.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.214.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.215.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.216.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.217.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.218.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.22.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.220.tfrecord',
    f'{TRAIN_TFRECORDS_PATH}6.221.tfrecord'
]


def init_train_dataset(s3_filenames_list):

    def tfrecord_parser_fn(raw_record):
        image, label = read_tfrecord(raw_record)
        dataset = {""image"": image, ""label"": label}
        return dataset, label

    def preprocess_sample(features, label):
        features[""image""] = resize_and_crop_image(features[""image""])
        return features, label

    print(""Loading data from S3"")
    filenames = s3_filenames_list

    print(f""using the following file {filenames}"")

    train_dataset = tf.data.TFRecordDataset(filenames=filenames)
    train_dataset = train_dataset.repeat().map(tfrecord_parser_fn).map(
        preprocess_sample).batch(200)
    return train_dataset


def remove_missing_rec(listin):
    tmp_list = copy.deepcopy(listin)
    for tfrec in tmp_list:
        if not tf.io.gfile.exists(tfrec):
            print('record is missing ',tfrec);
            listin.remove(tfrec)
    print(""list size: "",len(listin))
    return listin



def main(p,s3_filenames):
    pid = os.getpid()
    print('LIST LENGTH BEFORE REMOVE: ', len(s3_filenames))
    s3_filenames = remove_missing_rec(s3_filenames);
    print('LIST LENGTH AFTER REMOVE: ', len(s3_filenames))
    dataset = init_train_dataset(s3_filenames)
    dataset_iter = iter(dataset)
    print(""****************************  PROCESS ID: "",pid,""  ******************************"")
    process = psutil.Process(pid)
    for i,data in enumerate(dataset_iter):
        print(""Dataset loaded p: "",p,"" iter: "",i)
        if i%30==0:
            print(""time: "",datetime.now().time())
            used_mem = psutil.virtual_memory().used
            print(""used Virtual memory: {} Mb"".format(used_mem / 1024 / 1024))
            mem_info = process.memory_info()
            print(""Process memory: {} Mb"".format(mem_info.rss / 1024 / 1024))

if __name__ == ""__main__"":
    parser = argparse.ArgumentParser()
    parser.add_argument('-p', type=int, required=True,help=""proc number"")
    args = parser.parse_args()
    p = args.p
    main(p,s3_filenames)
```


### Relevant log output

```shell
...
Dataset loaded p:  0  iter:  13291
Dataset loaded p:  0  iter:  13292
Dataset loaded p:  0  iter:  13293
Dataset loaded p:  0  iter:  13294
Dataset loaded p:  0  iter:  13295
Dataset loaded p:  0  iter:  13296
Dataset loaded p:  0  iter:  13297
Dataset loaded p:  0  iter:  13298
Dataset loaded p:  0  iter:  13299
Dataset loaded p:  0  iter:  13300
Dataset loaded p:  0  iter:  13301
Dataset loaded p:  0  iter:  13302
Dataset loaded p:  0  iter:  13303
Dataset loaded p:  0  iter:  13304
Dataset loaded p:  0  iter:  13305
Dataset loaded p:  0  iter:  13306
Dataset loaded p:  0  iter:  13307
Dataset loaded p:  0  iter:  13308
Traceback (most recent call last):
  File ""data.py"", line 263, in <module>
    main(p,s3_filenames)
  File ""data.py"", line 248, in main
    for i,data in enumerate(dataset_iter):
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 836, in __next__
    return self._next_internal()
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 819, in _next_internal
    ret = gen_dataset_ops.iterator_get_next(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 2923, in iterator_get_next
    _ops.raise_from_not_ok_status(e, name)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 7186, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 0 [Op:IteratorGetNext]
```
</details>"
56852,assertion failed: [Trying to access a placeholder that is not supposed to be executed. This means you are executing a graph generated from the cross-replica context in an in-replica context.],"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: 
Yes.
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 
Linux Ubuntu 18.04
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
No.
-   **TensorFlow installed from (source or binary)**:
-No.
-   **TensorFlow version (use command below)**:
2.8.0
-   **Python version**:
3.9
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
Running on CPU
-   **GPU model and memory**:
Running on CPU
-   **Exact command to reproduce**:
```python
import logging
import tensorflow as tf

tf.get_logger().setLevel(""ERROR"")
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

def get_decoder():
    """"""Build the decoder.""""""
    decoder = tf.keras.Sequential()
    decoder.add(tf.keras.layers.Dense(2))
    return decoder

def get_model(encoder):
    encoder.trainable = False
    decoder = get_decoder()
    inputs = encoder.input
    emb = encoder(inputs, training=False)
    outputs = decoder(emb)
    return tf.keras.Model(inputs=inputs, outputs=outputs, name=""model"")

# local path to SavedModel
model_dir = ""toy_model""
feats_dim = 64
distribute_strategy = tf.distribute.MirroredStrategy()
# build variables inside scope so they are mirrored across gpu replicas
with distribute_strategy.scope():
    # load pretrained encoder from tf SavedModel
    encoder = tf.keras.models.load_model(
        model_dir,
    ).get_layer('model')
    encoder.build(tf.TensorShape([None, None, feats_dim]))
    # Build decoder on top of frozen encoder
    model = get_model(encoder=encoder)
    model(tf.random.normal([1, 100, 64]))
    model.summary()
    opt = tf.keras.optimizers.Adam(learning_rate=0.001)
    model.compile(
            loss=tf.keras.losses.SparseCategoricalCrossentropy(),
            optimizer=opt
    )

```

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
I would like to load a pretrained `SavedModel` and use it as a (frozen) encoder for a downstream task. I am trying to load the pretrained model and extract the base within the `tf.distribute.MirroredStrategy`, the use this base model to build a new model with a simple decoder. However, when calling `.compile`, an assertion error is raised:
```bash
Node: 'Assert/Assert'
assertion failed: [Trying to access a placeholder that is not supposed to be executed. This means you are executing a graph generated from the cross-replica context in an in-replica context.]
	 [[{{node Assert/Assert}}]] [Op:__inference_restored_function_body_9043]
```
This works fine if I use an `.h5` format instead of the (preferred) `SavedModel` format. 

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

[toy_model.tar.gz](https://github.com/tensorflow/tensorflow/files/9185167/toy_model.tar.gz)

"
56851,Load tf lite model to XLA ,"Hi,
I would like to know if it possible to take a TF lite model (quarantined) and compile into executable code using XLA/JIT. In this lick you have an example about how to create an executable given an tf model : https://www.tensorflow.org/xla/tfcompile. In other words can I use ""tfcompile"" with tf lite quarantined models?
"
56849,Inconsistent results from `tf.raw_ops.LRNGrad` between CPU and GPU,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04.4 LTS

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

5.1.1

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

11.2

### GPU model and memory

RTX 3090 2*24G

### Current Behaviour?

```shell
The results of `LRNGrad` operators are inconsistent between CPU and GPU.

I've used two calls `tf.raw_ops.LRNGrad` and `nn.lrn_grad`,
and also changed the order of calls for different devices, 
still inconsistent results.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.python.ops import nn
from tensorflow.python.ops import random_ops

# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/lrn_op.cc

input_grads = random_ops.random_uniform(
        shape=[1, 1, 1, 3],
        minval=-10000,
        maxval=10000,
        dtype=tf.float32,
        seed=2022)
input_img   = random_ops.random_uniform(
        shape=[1, 1, 1, 3],
        minval=-10000,
        maxval=10000,
        dtype=tf.float32,
        seed=2022)
output_img  = random_ops.random_uniform(
        shape=[1, 1, 1, 3],
        minval=-10000,
        maxval=10000,
        dtype=tf.float32,
        seed=2022)


with tf.device('/GPU:0'):
    out = tf.raw_ops.LRNGrad(input_grads=input_grads, input_image=input_img, output_image=output_img)
    #out = nn.lrn_grad(input_grads=input_grads, input_image=input_img, output_image=output_img)
    print(out)

with tf.device('/CPU:0'):
    out = tf.raw_ops.LRNGrad(input_grads=input_grads, input_image=input_img, output_image=output_img)
    #out = nn.lrn_grad(input_grads=input_grads, input_image=input_img, output_image=output_img)
    print(out)
```


### Relevant log output

```shell
# python LRNgrad-test.py
2022-07-21 12:54:57.023583: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-07-21 12:54:57.132843: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-07-21 12:54:57.161994: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-07-21 12:55:00.101305: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-07-21 12:55:01.526010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22298 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:9b:00.0, compute capability: 8.6
2022-07-21 12:55:01.527383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22298 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:c8:00.0, compute capability: 8.6
2022-07-21 12:55:02.763201: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100

tf.Tensor([[[[-0.29212222  0.97755533 -0.28474247]]]], shape=(1, 1, 1, 3), dtype=float32)

tf.Tensor([[[[2362.0498 1360.1172 2242.2402]]]], shape=(1, 1, 1, 3), dtype=float32)
```
</details>"
56848,TensorflowLite model run on Hexagon DSP different from CPU,"Hi, I convert a ""resnet-like"" .tflite model(quantized by int8) and find it has a certain precision loss on hexagon compared to cpu,  then i print the output probability,  they are really different.
I used to discover the ""inference_diff"" tool (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks/inference_diff) that you guys provided and now test with it, the result is as follows:
---------------------------------------------------------------------------------------------------------------------------
```
./run_eval --model_file=resnet_quantized.tflite --delegate=hexagon                                                                                             
INFO: Initialized TensorFlow Lite runtime.
loaded libcdsprpc.so
Hexagon delegate created.
INFO: TfLiteHexagonDelegate delegate: 34 nodes delegated out of 36 nodes with 1 partitions.
VERBOSE: Replacing 34 node(s) with delegate (TfLiteHexagonDelegate) node, yielding 3 partitions.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
native : lite/tools/evaluation/stages/inference_profiler_stage.cc:78 Test interpreter has been initialized.
native : lite/tools/evaluation/stages/tflite_inference_stage.cc:146 
native : lite/tools/evaluation/stages/inference_profiler_stage.cc:92 Reference interpreter (1 thread on CPU) has been initialized.
Num evaluation runs: 50
Reference run latency: avg=240374(us), std_dev=142248(us)
Test run latency: avg=19779(us), std_dev=5008(us)
OutputDiff[0]: avg_error=0.000274915, std_dev=4.89316e-05
```
----------------------------------------------------------------------------------------------------------------------------
It looks like little difference, but in general, the output probability of the quantized model are mostly 0, so this probably doesn't mean much. Then I remove the softmax operation at the end of the model, and try again:
----------------------------------------------------------------------------------------------------------------------------
```
./run_eval --model_file=resnet_quantized_without_softmax.tflite --delegate=hexagon                                                                             
INFO: Initialized TensorFlow Lite runtime.
loaded libcdsprpc.so
Hexagon delegate created.
INFO: TfLiteHexagonDelegate delegate: 33 nodes delegated out of 35 nodes with 1 partitions.
VERBOSE: Replacing 33 node(s) with delegate (TfLiteHexagonDelegate) node, yielding 3 partitions.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
native : lite/tools/evaluation/stages/inference_profiler_stage.cc:78 Test interpreter has been initialized.
native : lite/tools/evaluation/stages/tflite_inference_stage.cc:146 
native : lite/tools/evaluation/stages/inference_profiler_stage.cc:92 Reference interpreter (1 thread on CPU) has been initialized.
Num evaluation runs: 50
Reference run latency: avg=234793(us), std_dev=146766(us)
Test run latency: avg=19949.9(us), std_dev=5333(us)
OutputDiff[0]: avg_error=2.66955, std_dev=0.73623
```
----------------------------------------------------------------------------------------------------------------------------
Now the difference is much bigger. I wonder whether these are normal, and what is the approximate level of accuracy loss of the model generally. I upload these two models which named resnet_quantized.tflite.tar.gz and resnet_quantized_without_softmax.tflite.
Thanks [!](url)
[resnet_quantized.tflite.tar.gz](https://github.com/tensorflow/tensorflow/files/9157839/resnet_quantized.tflite.tar.gz)
[resnet_quantized_without_softmax.tflite.tar.gz](https://github.com/tensorflow/tensorflow/files/9157842/resnet_quantized_without_softmax.tflite.tar.gz)

"
56847,Nondeterministic result on TPU,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

TPU v3-8

### Current Behaviour?

```shell
Use ""TF_DETERMINISTIC_OPS = 1"" or ""tf.config.experimental.enable_op_determinism()"" can get determistic result on GPU.

But the results are nondetermistic on cloud TPU.
```


### Standalone code to reproduce the issue

```shell
https://github.com/edwardyehuang/CAR

The code on repo above can get determistic result on GPU, but the result is nondetermistic on TPU.
```


### Relevant log output

```shell
TPU 1st RUN (1000 steps):

1000/1000 [==============================] - 506s 395ms/step - loss: 1.6268 - IOU: 0.3178 - g_1_orl: 0.6300 - g_1_sal: 0.0168 - val_loss: 1.3395 - val_IOU: 0.2370

TPU 2nd RUN (1000 steps):

1000/1000 [==============================] - 645s 448ms/step - loss: 1.6488 - IOU: 0.3095 - g_1_orl: 0.6314 - g_1_sal: 0.0162 - val_loss: 1.7416 - val_IOU: 0.1793
```
</details>"
56845,[Feature Request] GELU activation with the Hexagon delegate,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 2.9.1

I think I'd be able to implement this myself, but wanted to see if there was any interest in including this upstream.  Most of this I'm writing out to make sure my own understanding is correct.

### The problem

I'd like to add support for the GELU op to the Hexagon Delegate.  The motivation for this is mostly for use with [DistilBERT](https://huggingface.co/distilbert-base-multilingual-cased), which uses this activation function in its feedforward network layers.  (Also used by BERT, GPT-3, RoBERTa, etc.)

Adding this as a supported op for the Hexagon delegate would avoid creating a graph partition/transferring between DSP<-->CPU each time the GELU activation function is used.

### How I'd implement this

GELU in TF Lite is implemented as a lookup table when there are integer inputs ([here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/activations.cc#L120-L140) and [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/reference/gelu.h#L37-L53)).

This same approach could be used for the Hexagon delegate, as it has int8/uint8 data types and also supports lookup tables.

I'd plan to do this by adding a new op builder in the delegate, populating a lookup table for each node as is currently done for the CPU version of the op, and then using the [Gather_8](https://source.codeaurora.org/quic/hexagon_nn/nnlib/tree/hexagon/ops/src/op_gather.c)  nnlib library function to do the lookup.

### Possible workaround

A workaround I thought of:

I'm going to try removing the [pattern matching](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/transforms/optimize_patterns.td#L1034-L1095) for approximate GELU in MLIR, and then using the approximate version of GELU (so that using tanh and not Erf).  This will probably be slower, but should let me keep execution on the DSP.

Since this will then be tanh, addition, multiplication ops instead of GELU they should all be runnable by the DSP."
56842,Wrong gradient in forward mode auto differentiation for `tf.random.stateless_parameterized_truncated_normal`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Jacobian matrix elements are not equal in forward mode and backward mode with the same input.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

shape = [2, 3]
seed = [7, 17]
means = 13.0
stddevs = tf.constant([[0.8059583, 0.09676647, 0.08382106],
                       [0.8149866, 0.44204712, 0.5636599]], dtype=tf.float32)
minvals = [-1.0, -2.0, -1000.0]
maxvals = [[10000.0], [1.0]]
name = None
with tf.GradientTape(persistent=True, ) as g:
  g.watch(stddevs)
  tf.random.set_seed(42)
  res_backward = tf.random.stateless_parameterized_truncated_normal(shape, seed, means=means, stddevs=stddevs,
                                                                    minvals=minvals, maxvals=maxvals, )
# shape=(2,3,2,3)
jacobian = g.jacobian(res_backward,stddevs)
print(jacobian[0][1])

tangents = tf.constant([[0.,1.,0.],
 [0.,0.,0.]],shape=(2,3),dtype=tf.float32)
with tf.autodiff.ForwardAccumulator(stddevs,tangents) as acc:
  res_forward = tf.random.stateless_parameterized_truncated_normal(shape, seed, means=means, stddevs=stddevs,
                                                                   minvals=minvals, maxvals=maxvals, )
print(acc.jvp(res_forward))
```


### Relevant log output

```shell
tf.Tensor(
[[ 0.         0.8470439  0.       ]
 [-0.        -0.        -0.       ]], shape=(2, 3), dtype=float32)
tf.Tensor(
[[0.        0.8470439 0.       ]
 [      nan       nan       nan]], shape=(2, 3), dtype=float32)
```
</details>"
56840,Unit test failure on high CPU core count machines,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

CentOS 7

### Mobile device

n/a

### Python version

3.8.10

### Bazel version

5.1.1

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

```shell
//tensorflow/python/data/experimental/kernel_tests/service:cross_trainer_cache_test fails if there are more than 48 CPU cores in the machine being used to test.
```


### Standalone code to reproduce the issue

```shell
bazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=2 --test_output=all --cache_test_results=no --config=nonccl --copt=-mtune=generic --copt=-march=armv8-a --copt=-O3 --verbose_failures --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --build_tests_only -- //tensorflow/python/data/experimental/kernel_tests/service:cross_trainer_cache_test
```


### Relevant log output

```shell
FAIL: testConcurrentReaders_test_mode_graph_tfapiversion_2 (__main__.CrossTrainerCacheTest)
CrossTrainerCacheTest.testConcurrentReaders_test_mode_graph_tfapiversion_2
testConcurrentReaders_test_mode_graph_tfapiversion_2(mode='graph', tf_api_version=2)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/cross_trainer_cache_test.runfiles/absl_py/absl/testing/parameterized.py"", line 314, in bound_param_test
    return test_method(self, **testcase_params)
  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/cross_trainer_cache_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 362, in decorated
    execute_test_method()
  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/cross_trainer_cache_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 345, in execute_test_method
    test_method(**kwargs_to_pass)
  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/cross_trainer_cache_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/cross_trainer_cache_test.py"", line 97, in testConcurrentReaders
    self.assertEqual(self.evaluate(iterators[j]()), i)
AssertionError: 9 != 0

----------------------------------------------------------------------
```
</details>"
56839,Tensorflow add new device runtime like cuda?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf 2.6.0

### Custom Code

Yes

### OS Platform and Distribution

ubuntu 18.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

3.7.2

### GCC/Compiler version

7.5

### CUDA/cuDNN version

11.2

### GPU model and memory

_No response_

### Current Behaviour?

```shell
how to develop custom tensorflow code in new device runtime?
where are some resources about secondary development documentation?
```


### Standalone code to reproduce the issue

```shell
no
```


### Relevant log output

_No response_</details>"
56838,Tensorflow lite examples Java ,"Where is tensorflow object detection app java project, only kotlin is present "
56837,`tf.config.set_logical_device_configuration` not working when attempting to create multiple virtual GPUs,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tensorflow-macos v2.8.0 tensorflow-metal v0.5.0 

### Custom Code

No

### OS Platform and Distribution

macOS-12.2.1

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I'm developing on a system with a single GPU (Apple M1 Pro) and trying to simulate multiple GPUs with virtual devices.

Using the examples found here: https://www.tensorflow.org/guide/gpu#using_multiple_gpus I get the following output:

```
systemMemory: 32.00 GB
maxCacheSize: 10.67 GB

1 Physical GPU, 1 Logical GPUs
```

where I was expecting the Logical GPUs to be greater than the Physical GPU count.

I have tested `tf.config.set_logical_device_configuration` for increasing Logical device count for CPUs, and this indeed _does_ work for CPU virtualisation.

### Standalone code to reproduce the issue

Taken from the code snippet found at: https://www.tensorflow.org/guide/gpu#using_multiple_gpus

```
gpus = tf.config.list_physical_devices(""GPU"")
if gpus:
    # Create 2 virtual GPUs with 1GB memory each
    try:
        tf.config.set_logical_device_configuration(
            gpus[0],
            [
                tf.config.LogicalDeviceConfiguration(memory_limit=1024),
                tf.config.LogicalDeviceConfiguration(memory_limit=1024),
            ],
        )
        logical_gpus = tf.config.list_logical_devices(""GPU"")
        print(len(gpus), ""Physical GPU,"", len(logical_gpus), ""Logical GPUs"")
    except RuntimeError as e:
        # Virtual devices must be set before GPUs have been initialized
        print(e)
```


### Relevant log output

```shell
1 Physical GPU, 1 Logical GPUs
```
</details>"
56836,Wrong gradient calculated by `tf.autodiff.ForwardAccumulator` for API `tf.optimizers.schedules.ExponentialDecay`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Forward mode differentiation for the case below should be `0.0` but got `nan`, inconsistent with the gradient calculated in reverse mode.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
initial_learning_rate = 14.01
decay_steps = 100
decay_rate = 0.0
staircase = False
name = None

input = tf.constant(0.9369918869034664, dtype=tf.float64)

exp_decay = tf.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps, decay_rate,
                                                                  staircase=staircase, )

with tf.GradientTape(persistent=True) as g:
  g.watch(input)
  res_backward = exp_decay(input)
grad_backward = g.gradient(res_backward,input)
print(grad_backward)

with tf.autodiff.ForwardAccumulator(input, tf.constant(1,dtype=tf.float64)) as acc_0:
  res_forward = exp_decay(input)
grad_forward = acc_0.jvp(res_forward)
print(grad_forward)
```


### Relevant log output

```shell
tf.Tensor(0.0, shape=(), dtype=float64)
tf.Tensor(nan, shape=(), dtype=float32)
```
</details>"
56835,Tflite ios batch_size greater than 1 not working,"My model was trained with Pytorch. Convert this model to tflite model with input shape {1, 3, 256, 256} and is working as expected. But change this model to input shape {4, 3, 256, 256} the inference result is not correct.
"
56834,fail to import tensorflow_recommenders in google colab,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf 2.8.2

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Import failed with AttributeError
```


### Standalone code to reproduce the issue

```shell
import tensorflow_recommenders as tfrs
```


### Relevant log output

```shell
AttributeError                            Traceback (most recent call last)
<ipython-input-47-fc3b737dea98> in <module>()
----> 1 import tensorflow_recommenders as tfrs

7 frames
/usr/local/lib/python3.7/dist-packages/tensorflow_recommenders/layers/embedding/tpu_embedding_layer.py in <module>()
     54 _DUMMY_NAME = ""tpu_embedding_helper_dummy""
     55 
---> 56 _EMBEDDING_V2 = tf.tpu.experimental.HardwareFeature.EmbeddingFeature.V2
     57 _EMBEDDING_V1 = tf.tpu.experimental.HardwareFeature.EmbeddingFeature.V1
     58 _EMBEDDING_UNSUPPORTED = tf.tpu.experimental.HardwareFeature.EmbeddingFeature.UNSUPPORTED

AttributeError: module 'tensorflow.compat.v2.tpu.experimental' has no attribute 'HardwareFeature'
```
</details>"
56833,"Forward AD threw error in `tf.autodiff.ForwardAccumulator` for `tf.keras.layers.MaxPooling3D`, but backward AD succeeded with same input","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Forward AD threw error, but backward AD succeeded with same input. They should throw error both as expected. This behaviour can be reproduced in some similar APIs like `tf.keras.layers.AveragePooling2D` and `tf.keras.layers.AveragePooling3D`.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

pool_size = [2,1,1]
strides = [2,2,2]
padding = ""valid""
data_format = ""channels_last""
input = tf.constant(0.895205,shape=[1,1,1,1,1], dtype=tf.float32)
layer = tf.keras.layers.MaxPooling3D(pool_size=pool_size, strides=strides, padding=padding, data_format=data_format, )

with tf.GradientTape(persistent=True, ) as g:
    g.watch(input)
    res_backward = layer(input)
grad_backward = g.jacobian(res_backward,res_backward)
print(""res_backward:"",res_backward)
print(""grad_backward:"",grad_backward)

tangents = tf.constant(1.,dtype=tf.float32,shape=[1,1,1,1,1])
with tf.autodiff.ForwardAccumulator(input, tangents) as acc:
    res_forward = layer(input)
    grad_jvp = acc.jvp(res_forward)
    print(""res_forward:"", res_forward)
    print(""grad_forward"", grad_jvp)
```


### Relevant log output

```shell
res_backward: tf.Tensor([], shape=(1, 0, 1, 1, 1), dtype=float32)
grad_backward: tf.Tensor([], shape=(1, 0, 1, 1, 1, 1, 0, 1, 1, 1), dtype=float32)
ValueError: Negative dimension size caused by subtracting 2 from 1 for '{{node gradient_tape/gradient_tape/MaxPool3DGradGrad}} = MaxPool3DGradGrad[T=DT_FLOAT, data_format=""NDHWC"", ksize=[1, 2, 1, 1, 1], padding=""VALID"", strides=[1, 2, 2, 2, 1]](inputs, outputs, tangents)' with input shapes: [1,1,1,1,1], [1,0,1,1,1], [1,1,1,1,1].
```
</details>"
56832,Different outputs at inference when different versions of TensorFlow are used to perform quantization during conversion to .tflite,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colaboratory
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): Issue with various versions of TenrorFlow

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

Perform conversion and Dynamic range quantization:
```
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_quant_model = converter.convert()
```
Perform inference with the generated model
```
import numpy as np
import cv2
from matplotlib import pyplot as plt


# load in image, resize and normalize
fn = '/content/114.jpeg'
img = cv2.imread(fn)[..., ::-1]
img = cv2.resize(img, (256, 128), interpolation=cv2.INTER_AREA)
inputs = (img.astype(np.float32) / 127.5) - 1.
inputs = np.expand_dims(inputs, 0)


# load TFLite model and set params
mod_path = '/content/model_converted.tflite'
interpreter = tf.lite.Interpreter(model_path=mod_path)
interpreter.allocate_tensors()
input_index = interpreter.get_input_details()[0][""index""]
output_index = interpreter.get_output_details()[0][""index""]

# run inference
interpreter.set_tensor(input_index, inputs)
interpreter.invoke()
borderi = interpreter.get_tensor(output_index)
print(type(borderi))

border = list(borderi[0])
print(border)
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- The issue here can be reproduced by using different versions of TensorFlow to convert and quantize the model. Check below for the versions and the results:

Version 2.3 result:
[[0.3330792  0.27843374]
 [0.40366906 0.250709  ]
 [0.48135388 0.24179175]
 [0.5636318  0.24119636]
 [0.6400584  0.2480475 ]
 [0.6005912  0.3724205 ]
 [0.5628794  0.3754966 ]
 [0.48778155 0.40332332]
 [0.3985254  0.3810564 ]
 [0.34678322 0.3575413 ]
 [0.52223235 0.3023828 ]
 [0.5825569  0.30930337]]

Version 2.4 result:
[[0.3341408  0.28175732]
 [0.40583873 0.25392115]
 [0.48184934 0.24735713]
 [0.55686283 0.2499246 ]
 [0.63230157 0.26033315]
 [0.6035842  0.37444225]
 [0.5606153  0.37672445]
 [0.4867457  0.39785832]
 [0.4086929  0.38542932]
 [0.35176694 0.3601483 ]
 [0.51643425 0.31068143]
 [0.5710402  0.31282204]]

Version 2.5 result:
[[0.3330438  0.28234786]
 [0.40442967 0.25366008]
 [0.48140293 0.24701637]
 [0.55590045 0.24874169]
 [0.63329554 0.2600382 ]
 [0.60310024 0.373787  ]
 [0.56032354 0.37653822]
 [0.48664606 0.39639705]
 [0.40806895 0.38597408]
 [0.3511423  0.35862684]
 [0.5150639  0.30999073]
 [0.57102746 0.31190303]]

Version 2.7 result:
[[0.3341408  0.28175732]
 [0.40583873 0.25392115]
 [0.48184934 0.24735713]
 [0.55686283 0.2499246 ]
 [0.63230157 0.26033315]
 [0.6035842  0.37444225]
 [0.5606153  0.37672445]
 [0.4867457  0.39785832]
 [0.4086929  0.38542932]
 [0.35176694 0.3601483 ]
 [0.51643425 0.31068143]
 [0.5710402  0.31282204]]

Version 2.8.2 result:
[[0.33199123 0.28214237]
 [0.40430748 0.2546782 ]
 [0.48055232 0.24780622]
 [0.555871   0.2505081 ]
 [0.63259435 0.2612171 ]
 [0.6028995  0.3754816 ]
 [0.5595857  0.37812153]
 [0.48534507 0.397737  ]
 [0.40594587 0.38545215]
 [0.3501406  0.35963872]
 [0.5159489  0.31203791]
 [0.57132494 0.31412107]]

IMPORTANT= This happens only when we apply Dynamic range quantization. Without quantization the outputs are the same for the different versions of TensorFlow and they are:
[[0.33383682 0.28292328]
 [0.40593028 0.2521686 ]
 [0.4803361  0.24693146]
 [0.5551163  0.24844697]
 [0.63013196 0.26205748]
 [0.60453784 0.37450537]
 [0.55849314 0.37527987]
 [0.4858575  0.3947457 ]
 [0.4057536  0.38485324]
 [0.3498182  0.35843956]
 [0.51736057 0.30842167]
 [0.5722974  0.31274813]]

So the question here is why the quantization algorithm is different at the different versions of TensorFlow and in the end which one is correct?

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
56830,`tf.math.reduce_std` have wrong gradient when input has only one element,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened when compute the gradient of `tf.math.reduce_std`. When the input `x` is a shape `[1]` tensor, the reduced standard deviation is always `0`, so the gradient should be `0` (the numerical gradient computed by `tf.test.compute_gradient` is also `0`). However, the current gradient wrongly gives `nan`.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

x = tf.random.uniform([1], minval=-1, maxval=1, dtype=tf.float64)
print(x)

with tf.GradientTape(persistent=False,) as g:
  g.watch(x)
  y = tf.math.reduce_std(x, axis=0, keepdims=True)
  print(y)
print(""gradient"", g.gradient(y, x))

@tf.function
def test_func(x):
  return tf.math.reduce_std(x, axis=0, keepdims=True)
theoretical, numerical = tf.test.compute_gradient(test_func, [x])

print(""theoretical gradient"", theoretical)
print(""numerical gradient"", numerical)
```


### Relevant log output

```shell
tf.Tensor([-0.83151082], shape=(1,), dtype=float64)
tf.Tensor([0.], shape=(1,), dtype=float64)
gradient tf.Tensor([nan], shape=(1,), dtype=float64)
theoretical gradient (array([[nan]]),)
numerical gradient (array([[0.]]),)
```
</details>"
56822,"tf.linalg.eigh provides inaccurate results, conflicting with tf.linalg.eig","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.8.2

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I am working on a project in which I need to compute the eigenvalues of each matrix element in a batch. While implementing, I discovered that `tf.linalg.eigh` was returning different results from `tf.linalg.eig` to statistical significance. Weirdly, both `tf.linalg.eig` and `tf.linalg.eigvals` return the same values as do `tf.linalg.eigh` and `tf.linalg.eigvals`.

Matrix `[[1, 0], [1, 2]]` has eigenvalues = 2, 1 with eigenvectors = (0, 1) and (-1, 1) respectively. Both `eig` and `eigvals` get this correct, but both `eigh` and `eigvalsh` incorrectly return the values `0.38196601, 2.61803399`.


### Standalone code to reproduce the issue

See the following for the above buggy example.
https://colab.research.google.com/drive/1iJ14rLtDJLcM_ZvshCTkv0Y4vnJVN8vQ?usp=sharing


### Relevant log output

_No response_</details>"
56820,Different result in tflite conversion,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.5.0

### 2. Code


![image](https://user-images.githubusercontent.com/28351316/179805179-ed562a87-0bbd-4ca7-9e35-74583cc0b2f8.png)








I use the function above to convert TF fake quantize model to 8 bit tflite model. When I set time=True, which means input shape equals [1, 360, 640, 3], the tflite model in netron is below. It's the right model I want.

![right](https://user-images.githubusercontent.com/28351316/179800350-1673c16d-9491-4d84-ae72-fb09dc626876.png)


But when I set input shape=[1,None,None,3], the converted tflite shows below. It's strange that there are some unexpected op above bilinear upsample op. I found a model performance degradation when I convert the tflite model below, So I want to know how the difference comes. Thanks!

![wrong](https://user-images.githubusercontent.com/28351316/179800787-8103799c-1be3-4378-8b0c-915dfdb4688c.png)


"
56819,RaggedTensors should have a 'name' attribute,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

2.9.x

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Currently, `tf.RaggedTensor` objects have no `name` attribute like other tensor objects. Further, `__slots__` for `tf.RaggedTensor` is declared, so adding attributes is not allowed either. This is an issue particularly when saving a model to be promoted into a tf serving environment. When the `SavedModelBuilder` encounters `tf.RaggedTensor` in an input signature it decomposes that ragged tensor into two dense tensors with names like `args_0` and `args_0_1`. If `tf.RaggedTensor` had a name attribute and this was respected by `SavedModelBuilder` serving models with multiple ragged inputs would be easier/possible.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

class TestModel(tf.keras.Model):
    @tf.function(input_signature=[{""ragged_tensor_input"": tf.RaggedTensorSpec(shape=(None,None,None), dtype=tf.string), ""nested_tensor_input"": {""nested_tensor1"": tf.TensorSpec(shape=(1), dtype=tf.int32, name=""nested_tensor1"")}}])
    def call(self, input_record):
        return input_record

model = TestModel()

model({""ragged_tensor_input"": tf.ragged.constant([[[""hey""]]]), ""nested_tensor_input"": {""nested_tensor1"": [1]}})
model.save('./')

loaded = tf.saved_model.load('./')

loaded.signatures

!saved_model_cli show --dir ./ --all
```


### Relevant log output

```shell
_SignatureMap({'serving_default': <ConcreteFunction signature_wrapper(*, args_0_1, args_0_2, args_0, nested_tensor1) at 0x7FD180203C90>})


MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:

signature_def['__saved_model_init_op']:
  The given SavedModel SignatureDef contains the following input(s):
  The given SavedModel SignatureDef contains the following output(s):
    outputs['__saved_model_init_op'] tensor_info:
        dtype: DT_INVALID
        shape: unknown_rank
        name: NoOp
  Method name is: 

signature_def['serving_default']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['args_0'] tensor_info:
        dtype: DT_STRING
        shape: (-1)
        name: serving_default_args_0:0
    inputs['args_0_1'] tensor_info:
        dtype: DT_INT64
        shape: (-1)
        name: serving_default_args_0_1:0
    inputs['args_0_2'] tensor_info:
        dtype: DT_INT64
        shape: (-1)
        name: serving_default_args_0_2:0
    inputs['nested_tensor1'] tensor_info:
        dtype: DT_INT32
        shape: (1)
        name: serving_default_nested_tensor1:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['nested_tensor_input_nested_tensor1'] tensor_info:
        dtype: DT_INT32
        shape: (1)
        name: PartitionedCall:0
    outputs['ragged_tensor_input'] tensor_info:
        dtype: DT_INVALID
        shape: ()
        name: 
  Method name is: tensorflow/serving/predict

Defined Functions:
  Function Name: '__call__'
    Option #1
      Callable with:
        Argument #1
          DType: dict
          Value: {'nested_tensor_input': {'nested_tensor1': TensorSpec(shape=(1,), dtype=tf.int32, name='input_record/nested_tensor_input/nested_tensor1')}, 'ragged_tensor_input': RaggedTensorSpec(TensorShape([None, None, None]), tf.string, 2, tf.int64)}
    Option #2
      Callable with:
        Argument #1
          DType: dict
          Value: {'ragged_tensor_input': RaggedTensorSpec(TensorShape([None, None, None]), tf.string, 2, tf.int64), 'nested_tensor_input': {'nested_tensor1': TensorSpec(shape=(1,), dtype=tf.int32, name='nested_tensor1')}}

  Function Name: '_default_save_signature'
    Option #1
      Callable with:
        Argument #1
          DType: dict
          Value: {'nested_tensor_input': {'nested_tensor1': TensorSpec(shape=(1,), dtype=tf.int32, name='nested_tensor1')}, 'ragged_tensor_input': RaggedTensorSpec(TensorShape([None, None, None]), tf.string, 2, tf.int64)}

  Function Name: 'call'
    Option #1
      Callable with:
        Argument #1
          DType: dict
          Value: {'ragged_tensor_input': RaggedTensorSpec(TensorShape([None, None, None]), tf.string, 2, tf.int64), 'nested_tensor_input': {'nested_tensor1': TensorSpec(shape=(1,), dtype=tf.int32, name='nested_tensor1')}}

  Function Name: 'call_and_return_all_conditional_losses'
    Option #1
      Callable with:
        Argument #1
          DType: dict
          Value: {'nested_tensor_input': {'nested_tensor1': TensorSpec(shape=(1,), dtype=tf.int32, name='input_record/nested_tensor_input/nested_tensor1')}, 'ragged_tensor_input': RaggedTensorSpec(TensorShape([None, None, None]), tf.string, 2, tf.int64)}
    Option #2
      Callable with:
        Argument #1
          DType: dict
          Value: {'nested_tensor_input': {'nested_tensor1': TensorSpec(shape=(1,), dtype=tf.int32, name='nested_tensor1')}, 'ragged_tensor_input': RaggedTensorSpec(TensorShape([None, None, None]), tf.string, 2, tf.int64)}
```
</details>"
56818,"TFlite android inference produces slightly different results for the same input, is it normal?","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.3.0

### Custom Code

Yes

### OS Platform and Distribution

windows10

### Mobile device

android

### Python version

3.6

### Bazel version

4.2.1

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I use tflite android api to get sentence embeddings from my tflite model.
`Map<Integer, Object> outputs = new HashMap<>();
outputs.put(0, embeddings);
tflite.runForMultipleInputsOutputs(new Object[]{inputIds, attentionMask}, outputs);`
TFlite android inference produces slightly different results for the same input, is it normal? What causes the small differences
query: -0.03889307#-0.20992874#-0.012840451...
query: -0.038892966#-0.20992874#-0.012840495...
query: -0.03889298#-0.20992877#-0.012840421...
```


### Standalone code to reproduce the issue

```shell
The code is inconvenient to share, I just want to know if this phenomenon is normal? If it is not normal, is there a way to avoid it?
```


### Relevant log output

_No response_</details>"
56817,Bulding Tensorflow Lite on ubuntu 16.04 (GCC 5.4.0),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf 2.8.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu 16.04

### Mobile device

Ubuntu 16.04

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

5.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I follow the [tutorial](https://www.tensorflow.org/lite/guide/build_cmake) to build tensorflow lite on Ubuntu 16.04. The error shows 

```shell
xnnpack/src/f16-dwconv/gen/up8x25-minmax-neonfp16arith-acc2.c:1:0: error: unknown value 'armv8.2-a+fp16' for -march
```

I know I have a old gcc environment. But I have no choice. Is there any way to build on Ubuntu 16.04 using gcc 5.4.0?


### Standalone code to reproduce the issue

Use Ubuntu 16.04 `apt update` and `apt upgrade -y` then follow [tutorial](https://www.tensorflow.org/lite/guide/build_cmake) to build tensorflow lite


### Relevant log output

```shell
xnnpack/src/f16-dwconv/gen/up8x25-minmax-neonfp16arith-acc2.c:1:0: error: unknown value 'armv8.2-a+fp16' for -march
```
</details>"
56815,TF should use protobuf 4,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

python 3.9.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tensorflow 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.2 which is incompatible.

Tensorflow should update this requirement!
```


### Standalone code to reproduce the issue

```shell
pip install <packages>
```


### Relevant log output

```shell
tensorflow 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.2 which is incompatible.
```
</details>"
56814,[TF.io] Incompatible with AWS S3 filepaths,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

RHEL

### Mobile device

_No response_

### Python version

3.7.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Apparently, from the AWS logs it seems the Headers are obtained but the response body is empty.

I'm using `TFDS`, However its accessing (via `etils`) the TF backend to create a stream for the S3 object. Hence why its reproducible from a `TF` method. 

This was the full error from TFDS, for those interested, to verify that it does indeed call `tf.io` as a backend to execute the request. 

```
Traceback (most recent call last):
  File ""scripts/kecam_tester.py"", line 47, in <module>
    description='Preprocessed TFRecords of BDD100K dataset, long only, delayed by 60 frames for each tuple.',
  File ""/home/awesome/.local/lib/python3.7/site-packages/tensorflow_datasets/core/folder_dataset/write_metadata_utils.py"", line 80, in write_metadata
    f for f in data_dir.iterdir() if naming.FilenameInfo.is_valid(f.name)
  File ""/home/awesome/.local/lib/python3.7/site-packages/tensorflow_datasets/core/folder_dataset/write_metadata_utils.py"", line 80, in <listcomp>
    f for f in data_dir.iterdir() if naming.FilenameInfo.is_valid(f.name)
  File ""/home/awesome/.local/lib/python3.7/site-packages/etils/epath/gpath.py"", line 126, in iterdir
    for f in self._backend.listdir(self._path_str):
  File ""/home/awesome/.local/lib/python3.7/site-packages/etils/epath/backend.py"", line 191, in listdir
    return self.gfile.listdir(path)
  File ""/home/awesome/.local/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 769, in list_directory_v2
    message=""Could not find directory {}"".format(path))
tensorflow.python.framework.errors_impl.NotFoundError: Could not find directory s3://s-laion/ssd-videos/new_tfrecs
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

# Insert any S3 filepath
tf.io.gfile.listdir('s3://...')
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/awesome/.local/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 769, in list_directory_v2
    message=""Could not find directory {}"".format(path))
tensorflow.python.framework.errors_impl.NotFoundError: Could not find directory s3://...
```
</details>"
56812,Segmentation Fault when using NCCL with MultiWorkerMirroredStrategy,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu
-   **TensorFlow installed from (source or binary)**: 
-   **TensorFlow version (use command below)**: 2.6.5
-   **Python version**: 3.8.10
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**: 
-   **CUDA/cuDNN version**:  11.6
-   **GPU model and memory**: V100
-   **Exact command to reproduce**: 


### Describe the problem
I am using 2 VMs each running a docker container. Each container uses 1 GPU.

I am trying to use TF's MutliWorkerMirroredStrategy. Currently if I choose the RING, the training successfully runs on 2 VMs (1 GPU each):
com_options=tf.distribute.experimental.CommunicationOptions(implementation=tf.distribute.experimental.CommunicationImplementation.RING)
strategy = tf.distribute.MultiWorkerMirroredStrategy(communication_options=com_options)

BUT 

When I switch to NCCL:
com_options=tf.distribute.experimental.CommunicationOptions(implementation=tf.distribute.experimental.CommunicationImplementation.NCCL)
strategy = tf.distribute.MultiWorkerMirroredStrategy(communication_options=com_options)

I get a Segmentation fault on my second VM, and then my first VM's training also stops.


### Source code / logs
#### When I run the training script on VM1:
2022-07-18 18:00:15.159072: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> VM_1_IP:6006, 1 -> VM_2_IP3:6006}
2022-07-18 18:00:15.159629: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:427] Started server with target: grpc://VM_1_IP:6006
80d296ca77be:32:176 [0] NCCL INFO Bootstrap : Using eth0:172.17.0.2<0>
80d296ca77be:32:176 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
80d296ca77be:32:176 [0] NCCL INFO NET/IB : No device found.
80d296ca77be:32:176 [0] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.2<0>
80d296ca77be:32:176 [0] NCCL INFO Using network Socket

#### Then I run modify 'TF_CONFIG' environment variable, and run the the same training script on VM2, when I get this output:
a64a33f123c8:508:652 [0] NCCL INFO Bootstrap : Using eth0:172.17.0.2<0>
a64a33f123c8:508:652 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
a64a33f123c8:508:652 [0] NCCL INFO NET/IB : No device found.
a64a33f123c8:508:652 [0] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.2<0>
a64a33f123c8:508:652 [0] NCCL INFO Using network Socket
a64a33f123c8:508:659 [0] NCCL INFO Call to connect returned Connection refused, retrying
a64a33f123c8:508:659 [0] NCCL INFO Call to connect returned Connection refused, retrying
a64a33f123c8:508:659 [0] NCCL INFO Call to connect returned Connection refused, retrying
a64a33f123c8:508:659 [0] NCCL INFO Call to connect returned Connection refused, retrying

and eventually...

a64a33f123c8:508:659 [0] bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/socket.h:406 NCCL WARN Connect to 172.17.0.2<49089> failed : Connection refused
a64a33f123c8:508:659 [0] NCCL INFO external/nccl_archive/src/bootstrap.cc:354 -> 2
a64a33f123c8:508:659 [0] NCCL INFO external/nccl_archive/src/init.cc:564 -> 2
a64a33f123c8:508:659 [0] NCCL INFO external/nccl_archive/src/init.cc:878 -> 2

Segmentation fault (core dumped)

#### Back on VM1, I see:
NCCL version 2.8.3+cudaCUDA_MAJOR.CUDA_MINOR
WARNING:tensorflow:/job:worker/replica:0/task:1 seems down, retrying 1/3
WARNING:tensorflow:/job:worker/replica:0/task:1 seems down, retrying 1/3
WARNING:tensorflow:/job:worker/replica:0/task:1 seems down, retrying 2/3
WARNING:tensorflow:/job:worker/replica:0/task:1 seems down, retrying 2/3
ERROR:tensorflow:Cluster check alive failed, /job:worker/replica:0/task:1 is down, aborting collectives: failed to connect to all addresses
Additional GRPC error information from remote target /job:worker/replica:0/task:1:
:{""created"":""@1658167387.074927655"",""description"":""Failed to pick subchannel"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3941,""referenced_errors"":[{""created"":""@1658167387.073967999"",""description"":""failed to connect to all addresses"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":393,""grpc_status"":14}]}
ERROR:tensorflow:Cluster check alive failed, /job:worker/replica:0/task:1 is down, aborting collectives: failed to connect to all addresses
Additional GRPC error information from remote target /job:worker/replica:0/task:1:
:{""created"":""@1658167387.074927655"",""description"":""Failed to pick subchannel"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3941,""referenced_errors"":[{""created"":""@1658167387.073967999"",""description"":""failed to connect to all addresses"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":393,""grpc_status"":14}]}
2022-07-18 18:03:07.075266: E tensorflow/core/common_runtime/base_collective_executor.cc:247] BaseCollectiveExecutor::StartAbort Unavailable: cluster check alive failed, /job:worker/replica:0/task:1 is down


Why does switching to NCCL cause this segmentation fault error?
Any help is appreciated!









"
56810,Error in loading  TF model ,"<details><summary>Click to expand!</summary> 
 
### Issue Type

Bug

### Source

binary

### Tensorflow Version

tensorflow-macos 2.9.2, colab 2.8.2, linux 2.8.2

### Custom Code

No

### OS Platform and Distribution

mac m1, linux, colab 

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Error when loading model
1
```shell
model of tf.keras.layers.Add with const in the second arg 
save_format tf - ok
save_format h5 - error
```

2
```shell
model of tf.keras.layers.Add with const in the first arg
save_format tf - error
save_format h5 - error
```
3 
```shell
model of x1 == x2 
save_format tf - error
save_format h5 - error
```


### Standalone code to reproduce the issue

## [Colab Code Link]( https://colab.research.google.com/drive/1ijPINk11gHQoO_bHIJLTYDKKz0HlB2d5?usp=sharing)
1
```shell
import numpy as np
import tensorflow as tf
1 x1 = tf.keras.layers.Input(shape=(1, 2, 3))
x2 = tf.constant(np.ones([1,1,1,3]))
x = tf.keras.layers.Add()([x1, x2])
model = tf.keras.Model(inputs=[x1], outputs=[x])
model.compile()
print(model.predict(np.random.rand(1, 1, 2, 3)))
model.save(""Mymodel"", save_format='tf')
loaded_model = tf.keras.models.load_model(""Mymodel"") #ok
model.save(""model.h5"")
model = tf.keras.models.load_model(""model.h5"") #error
```

2
```shell
import numpy as np
import tensorflow as tf
x1 = tf.keras.layers.Input(shape=(1, 2, 3))
x2 = tf.constant(np.ones([1,1,3]))
x = tf.keras.layers.Add()([x2, x1])
model = tf.keras.Model(inputs=[x1], outputs=[x])
model.compile()
print(model.predict(np.random.rand(1, 1, 2, 3)))
model.save(""Mymodel"",save_format='tf')
loaded_model = tf.keras.models.load_model(""Mymodel"") #error
model.save(""model.h5"")
model = tf.keras.models.load_model(""model.h5"") #error
```

3
```shell
import numpy as np
import tensorflow as tf
x1 = tf.keras.layers.Input(shape=(1, 2, 3))
x2 = tf.keras.layers.Input(shape=(1, 2, 3))
x = x1 == x2
model = tf.keras.Model(inputs=[x1, x2], outputs=[x])
model.compile()
data= np.random.rand(1, 1, 2, 3)
print(model.predict([data, data]))
model.save(""Mymodel"",save_format='tf')
loaded_model = tf.keras.models.load_model(""Mymodel"") #error
model.save(""model.h5"")
model = tf.keras.models.load_model(""model.h5"") #error
```


### Relevant log output
1
```shell
[[[[1.4474285 1.5162606 1.9524314]
   [1.6375002 1.3645701 1.248564 ]]]]
INFO:tensorflow:Assets written to: Mymodel/assets
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-4-df88e1175bcb> in <module>()
      8 loaded_model = tf.keras.models.load_model(""Mymodel"") #ok
      9 model.save(""model.h5"")
---> 10 model = tf.keras.models.load_model(""model.h5"") #error

1 frames
/usr/local/lib/python3.7/dist-packages/keras/layers/merge.py in <setcomp>(.0)
     94                        f'Got {len(input_shape)} inputs. '
     95                        f'Full input_shape received: {input_shape}')
---> 96     batch_sizes = {s[0] for s in input_shape if s} - {None}
     97     if len(batch_sizes) > 1:
     98       raise ValueError(

TypeError: unhashable type: 'list'
```
2
```shell
WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fc230dc1c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
[[[[1.2783022 1.9072516 1.3729118]
   [1.1954207 1.6057456 1.5275352]]]]
INFO:tensorflow:Assets written to: Mymodel/assets
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
[<ipython-input-8-9aaa3c26f4cc>](https://localhost:8080/#) in <module>()
      6 print(model.predict(np.random.rand(1, 1, 2, 3)))
      7 model.save(""Mymodel"",save_format='tf')
----> 8 loaded_model = tf.keras.models.load_model(""Mymodel"") #error
      9 model.save(""model.h5"")
     10 model = tf.keras.models.load_model(""model.h5"") #error

1 frames
[/usr/local/lib/python3.7/dist-packages/keras/backend.py](https://localhost:8080/#) in ndim(x)
   1499 
   1500   """"""
-> 1501   return x.shape.rank
   1502 
   1503 

AttributeError: Exception encountered when calling layer ""add_2"" (type Add).

'list' object has no attribute 'shape'

Call arguments received:
   inputs=[[[['tf.Tensor(shape=(), dtype=float32)', 'tf.Tensor(shape=(), dtype=float32)', 'tf.Tensor(shape=(), dtype=float32)']]], 'tf.Tensor(shape=(None, 1, 2, 3), dtype=float32)']
```

3
```shell
[[[[ True  True  True]
   [ True  True  True]]]]
INFO:tensorflow:Assets written to: Mymodel/assets
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-7-6ed972a3dad2> in <module>()
      7 print(model.predict([data, data]))
      8 model.save(""Mymodel"",save_format='tf')
----> 9 loaded_model = tf.keras.models.load_model(""Mymodel"") #error
     10 model.save(""model.h5"")
     11 model = tf.keras.models.load_model(""model.h5"") #error

1 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py in op_dispatch_handler(*args, **kwargs)
   1074         if iterable_params is not None:
   1075           args, kwargs = replace_iterable_params(args, kwargs, iterable_params)
-> 1076         result = api_dispatcher.Dispatch(args, kwargs)
   1077         if result is not NotImplemented:
   1078           return result

TypeError: Missing required positional argument
```
</details>"
56808,openCL delegate generates '0' and 'random' values with 'tf.stack',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9.1, nightly version

### Custom Code

No

### OS Platform and Distribution

Android

### Mobile device

tested on Snapdragon 888, 865

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Our models with 'tf.stack' ('Pack' in the tflite version) nodes generate wrong results with the openCL delegate. Our experiments show that the output of a 'Pack' node in a tflite model contains lots of 'zeros' and random values when we use openCL delegate. This issue does not happen with other delegates like XNNPACK.
This issue is very similar to the issue that we reported before here:
https://github.com/tensorflow/tensorflow/issues/56732
```


### Standalone code to reproduce the issue

```shell
We have implemented a small tool to reproduce the mentioned issue with the 'tf.stack' node. Here is the link to the repository:
https://github.com/Bahar-BM/tflite-test
```


### Relevant log output

_No response_</details>"
56805,Test failure: //tensorflow/compiler/mlir/lite/tests:ops.mlir.test,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Source

source

### Tensorflow Version

latest master branch

### Custom Code

No
</details>

### Current Behaviour?

```shell
//tensorflow/compiler/mlir/lite/tests:ops.mlir.test failing with err:

Command Output (stderr):
--
within split at /.cache/bazel/3bd8c23d51d3748f12b7f754fd39a32b/execroot/org_tensorflow/bazel-out/k8-
opt/bin/tensorflow/compiler/mlir/lite/tests/ops.mlir.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/lite/tests/ops.mlir:273
8 offset :10:6: error: expected error ""failed to verify that bias and output must have same element type"" was not produced
  // expected-error @+1 {{failed to verify that bias and output must have same element type}}
     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

--

********************
********************
Failed Tests (1):
  MLIR tests :: ops.mlir


Testing Time: 0.31s
  Failed: 1
================================================================================
Target //tensorflow/compiler/mlir/lite/tests:ops.mlir.test up-to-date:
  bazel-bin/tensorflow/compiler/mlir/lite/tests/ops.mlir.test
INFO: Elapsed time: 5.309s, Critical Path: 0.71s
INFO: 2 processes: 2 local.
INFO: Build completed, 1 test FAILED, 2 total actions
//tensorflow/compiler/mlir/lite/tests:ops.mlir.test                      FAILED in 0.5s
  /.cache/bazel/3bd8c23d51d3748f12b7f754fd39a32b/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/t
ensorflow/compiler/mlir/lite/tests/ops.mlir.test/test.log

INFO: Build completed, 1 test FAILED, 2 total actions
```


### Standalone code to reproduce the issue

```shell
bazel test //tensorflow/compiler/mlir/lite/tests:ops.mlir.test --test_output=all
```"
56804,(CONCATENATION) failed to prepare,"### 1. System information

- OS Platform and Distribution: MacOS Monterey 12.4
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.9.1
- Python: 3.7.10

### 2. Code

Provide code to help us reproduce your issues using one of the following options:
converted checkpoint: https://drive.google.com/file/d/1iWPcJ3wC2xV-xz3lIiGQrgteGXZOLt5P/view?usp=sharing

```
interpreter = tf.lite.Interpreter(model_path=tflite_path)
interpreter.allocate_tensors()
input_data = np.ones([1, 3, 416, 416], dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)
interpreter.invoke()
output_data = interpreter.get_tensor(output_details[0]['index'])
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs

When model is converted:

> 2022-07-18 10:49:42.054397: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1901] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):
> Flex ops: FlexConv2D, FlexRange
> Details:
> 	tf.Conv2D(tensor<?x?x?x?xf32>, tensor<3x3x12x24xf32>) -> (tensor<?x?x?x24xf32>) : {data_format = ""NHWC"", device = """", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = ""VALID"", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}
> 	tf.Range(tensor<i64>, tensor<i64>, tensor<i64>) -> (tensor<?xi64>) : {device = """"}
> See instructions: https://www.tensorflow.org/lite/guide/ops_select

Model fails on `invoke` with:

> INFO: Created TensorFlow Lite delegate for select TF ops.
> 2022-07-18 10:28:53.074011: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
> INFO: TfLiteFlexDelegate delegate: 13 nodes delegated out of 797 nodes with 3 partitions.
> RuntimeError: tensorflow/lite/kernels/concatenation.cc:158 t->dims->data[d] != t0->dims->data[d] (9 != 13)Node number 250 (CONCATENATION) failed to prepare.

"
56803,"Error fetching projector config Cannot fetch projector config, possibly a Cross-Origin request error.","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
56802,Saving the output of detected objects in file,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf 2.8

### Custom Code

No

### OS Platform and Distribution

Windows 10

### Mobile device

Android

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I would like to save the output of detected objects in a .txt file in my android phone.
It should be like that whenever I detect some objects, they are saved in a file in android, what would be the best aproach to resolving this issue ?
```


### Standalone code to reproduce the issue

```shell
How to save the output of the detected objects in android ?
```


### Relevant log output

_No response_</details>"
56801,Build failed in Android Studio,"Tensorflow is a great work. I want to study arm64 computation from it. What is a good IDE to read TensorFlow Source Code. I try to build it with cmake in Android Studio. How to fix the error?
`Execution failed for task ':lite:configureCMakeDebug'.
> [CXX1402] /Users/yoline/AndroidStudioProjects/TFLite_Src/lite/src/main/cpp/CMakeLists.txt debug|arm64-v8a : Target all_microkernels::@80588a722c29e322520c produces multiple outputs /Users/yoline/AndroidStudioProjects/TFLite_Src/lite/.cxx/Debug/482v1h6t/arm64-v8a/_deps/xnnpack-build/CMakeFiles/all_microkernels.dir/src/f16-f32-vcvt/gen/vcvt-scalar-x1.c.o, /Users/yoline/AndroidStudioProjects/TFLite_Src/lite/.cxx/Debug/482v1h6t/arm64-v8a/_deps/xnnpack-build/CMakeFiles/all_microkernels.dir/src/f16-f32-vcvt/gen/vcvt-scalar-x2.c.o,`"
56799,Build issue on Apple M1/M2,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.9.1, 2.8.2, 2.7.3, 2.7.0

### Custom Code

No

### OS Platform and Distribution

macOS 12.4 (Apple M1 Pro)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

5.2.0, 4.2.2

### GCC/Compiler version

Apple Clang 13.1.6

### CUDA/cuDNN version

N/A

### GPU model and memory

N/A

### Current Behaviour?

I'm trying to update our [Spack](https://spack.io) build recipe with the latest versions of TensorFlow but I'm unable to get anything to build. I've tried multiple versions of both TF and Bazel.

With TensorFlow 2.9.1 and Bazel 5.2.0 I get:
```
ERROR: /private/var/folders/j1/68dlgpr91vlgs26vty2c8xk80000gn/T/ajstewart/spack-stage/spack-stage-py-tensorflow-2.9.1-oljb6pzisjhfdevgnogv3hwizwhgj2rs/spack-src/tensorflow/tools/pip_package/BUILD:72:10: While resolving toolchains for target //tensorflow/tools/pip_package:simple_console: No matching toolchains found for types @bazel_tools//tools/cpp:toolchain_type. Maybe --incompatible_use_cc_configure_from_rules_cc has been flipped and there is no default C++ toolchain added in the WORKSPACE file? See https://github.com/bazelbuild/bazel/issues/10134 for details and migration instructions.
```
With TensorFlow 2.9.1 and Bazel 4.2.2 I get:
```
ERROR: /private/var/folders/j1/68dlgpr91vlgs26vty2c8xk80000gn/T/spackqkjcmrsf/08f21de77d854b22a26da21255c81581/external/build_bazel_rules_swift/swift/internal/swift_protoc_gen_aspect.bzl:457:13: 'lambda' not supported, declare a function instead
```
With TensorFlow 2.8.2/2.7.3/2.7.0 and Bazel 4.2.2 I get:
```
bazel-out/darwin_arm64-opt/bin/tensorflow/core/protobuf/error_codes.pb.h:10:10: fatal error: 'google/protobuf/port_def.inc' file not found
#include <google/protobuf/port_def.inc>
         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
1 error generated.
```
Multiple users have reported this last build issue: https://github.com/spack/spack/issues/31229.

### Standalone code to reproduce the issue

https://github.com/spack/spack/pull/31615 is the PR where I'm working on our build recipe. This issue can be reproduced by first cloning my PR branch:
```console
$ git clone https://github.com/adamjstewart/spack.git
$ cd spack
$ git switch packages/py-tensorflow
$ . share/spack/setup-env.sh
```
Then, try to install TF using one of the following commands:
```console
$ spack install py-tensorflow
$ spack install py-tensorflow@2.9.1 ^bazel@5.2.0
$ spack install py-tensorflow@2.9.1 ^bazel@4.2.2
$ spack install py-tensorflow@2.8.2 ^bazel@4.2.2
...
```
Unfortunately, all of these fail at the moment. You don't have to try my PR branch, the develop branch also fails. But the develop branch hasn't been updated since 2.7.0 since none of our developers have managed to get newer versions of TF to build on any system, let alone M1. Would any TF developers be interested in helping maintain this build recipe?

You can see the full build recipe and patches we currently require by running `spack edit py-tensorflow` or by looking at the above PR.

### Relevant log output
 
TensorFlow 2.9.1 and Bazel 5.2.0:

* [build log](https://github.com/tensorflow/tensorflow/files/9129388/spack-build-out.txt)
* [build env](https://github.com/tensorflow/tensorflow/files/9129389/spack-build-env-mods.txt)

TensorFlow 2.9.1 and Bazel 4.2.2:

* [build log](https://github.com/tensorflow/tensorflow/files/9129393/spack-build-out.txt)
* [build env](https://github.com/tensorflow/tensorflow/files/9129394/spack-build-env-mods.txt)

TensorFlow 2.8.2 and Bazel 4.2.2:

* [build log](https://github.com/tensorflow/tensorflow/files/9129401/spack-build-out.txt)
* [build env](https://github.com/tensorflow/tensorflow/files/9129402/spack-build-env-mods.txt)
"
56798,Android use a view as input ,How to use a view as input for tensorflow detection instead of camera ? Please help as soon as possible 
56797,Android OTG camera access,"How to use external OTG camera for object detection in Android ,please help me to do as soon as possible "
56796,`l2_normalize` has wrong gradient when input is zero,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When the input of `tf.nn.l2_normalize` is a zero tensor, `tf.GradientTape` outputs a very large gradient `1000000`.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
x = tf.random.uniform([1,1], maxval=0,dtype=tf.float64)
with tf.GradientTape(persistent=True,) as g:
  g.watch(x)
  y = tf.nn.l2_normalize(x)
print(""gradient"", g.gradient(y, x))
```


### Relevant log output

```shell
gradient tf.Tensor([[1000000.]], shape=(1, 1), dtype=float64)
```
</details>"
56795,TFlite GPU delegate crash on Movenet,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.0

### Custom Code

No

### OS Platform and Distribution

Android 12

### Mobile device

Galaxy Fold

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Creating a TFLite interpreter with the movenet model here https://tfhub.dev/google/movenet/singlepose/lightning/4 with the TFLite GPU delegate crashes immediately.

Using the NNAPI delegate does not crash.
```


### Standalone code to reproduce the issue

```shell
Construct an interpreter with the movenet model and specify GPU Delegate V2 in the interpreter options.
```


### Relevant log output

_No response_</details>"
56794,tensorflow bazel build fatal error C1060: compiler is out of heap space,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

windows 11

### Mobile device

_No response_

### Python version

3.10

### Bazel version

Build label: 5.2.0

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
ERROR: C:/tensorflow/tensorflow/core/kernels/BUILD:3662:18: Compiling tensorflow/core/kernels/cwise_op_floor_mod.cc failed: (Exit 2): cl.exe failed: error executing command
  cd /d C:/bazeloutput/xv6zejqw/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.32.31326\ATLMFC\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.32.31326\include;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\shared;C:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\um;C:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\winrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\cppwinrt
    SET PATH=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.32.31326\bin\HostX64\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\VCPackages;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files\Microsoft Visual Studio\2022\Community\MSBuild\Current\bin\Roslyn;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\FSharp\Tools;C:\Program Files (x86)\Windows Kits\10\bin\10.0.19041.0\\x64;C:\Program Files (x86)\Windows Kits\10\bin\\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\\MSBuild\Current\Bin\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\Tools\;;C:\WINDOWS\system32;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\Linux\bin\ConnectionManagerExe
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Program Files/Python310/python.exe
    SET PYTHON_LIB_PATH=C:/Program Files/Python310/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\VAHEKH~1\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TMP=C:\Users\VAHEKH~1\AppData\Local\Temp
  C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.32.31326\bin\HostX64\x64\cl.exe @bazel-out/x64_windows-opt/bin/tensorflow/core/kernels/_objs/cwise_op/cwise_op_floor_mod.obj.params
# Configuration: d1a3427bdf04210e38874c42eb3259e634068d80c8bb59ed645a8b277091eab7
# Execution platform: @local_execution_config_platform//:platform
cl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release
cl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'
external/com_google_protobuf/src\google/protobuf/repeated_field.h(1769): fatal error C1060: compiler is out of heap space
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```


### Standalone code to reproduce the issue

```shell
bazel --output_user_root='C:\bazelOutput' build //tensorflow/tools/pip_package:build_pip_package 
tensorflow config cpu only
```


### Relevant log output

```shell
ERROR: C:/tensorflow/tensorflow/core/kernels/BUILD:3662:18: Compiling tensorflow/core/kernels/cwise_op_floor_mod.cc failed: (Exit 2): cl.exe failed: error executing command
  cd /d C:/bazeloutput/xv6zejqw/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.32.31326\ATLMFC\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.32.31326\include;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\shared;C:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\um;C:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\winrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\\cppwinrt
    SET PATH=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.32.31326\bin\HostX64\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\VCPackages;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files\Microsoft Visual Studio\2022\Community\MSBuild\Current\bin\Roslyn;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\FSharp\Tools;C:\Program Files (x86)\Windows Kits\10\bin\10.0.19041.0\\x64;C:\Program Files (x86)\Windows Kits\10\bin\\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\\MSBuild\Current\Bin\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\Tools\;;C:\WINDOWS\system32;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\Linux\bin\ConnectionManagerExe
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Program Files/Python310/python.exe
    SET PYTHON_LIB_PATH=C:/Program Files/Python310/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\VAHEKH~1\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TMP=C:\Users\VAHEKH~1\AppData\Local\Temp
  C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.32.31326\bin\HostX64\x64\cl.exe @bazel-out/x64_windows-opt/bin/tensorflow/core/kernels/_objs/cwise_op/cwise_op_floor_mod.obj.params
# Configuration: d1a3427bdf04210e38874c42eb3259e634068d80c8bb59ed645a8b277091eab7
# Execution platform: @local_execution_config_platform//:platform
cl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release
cl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'
external/com_google_protobuf/src\google/protobuf/repeated_field.h(1769): fatal error C1060: compiler is out of heap space
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 15002.224s, Critical Path: 939.48s
INFO: 12276 processes: 3415 internal, 8861 local.
FAILED: Build did NOT complete successfully
```
</details>"
56786,Using static_rnn error,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 1.14

### Custom Code

Yes

### OS Platform and Distribution

Linux

### Mobile device

_No response_

### Python version

python 3X

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hi,
When i use the static_rnn have some issus,my code like this:
main_input = Input(shape=(1, 42),batch_size=1, name='main_input')

tmp = Dense(24)(main_input)

tmp = tf.unstack(tmp,axis=1)
vad_gru_1 = tf.nn.rnn_cell.GRUCell(48)
init_state = vad_gru_1.zero_state(1, dtype=tf.float32)
vad_gru_2,_ = tf.nn.static_rnn(cell=vad_gru_1,inputs=tmp,initial_state=init_state)
```


### Standalone code to reproduce the issue

```shell
The issus is :

TypeError: This layer (""gru_cell_1"") takes an `inputs` argument in `call()`, and only the `inputs` argument may be specified as a positional argument. Pass everything else as a keyword argument (those arguments will not be tracked as inputs to the layer).
```


### Relevant log output

_No response_</details>"
56785,Support INT8 for tf.split and tf.one_hot,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Windows 10 Enterprise 20H2

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Tensorflow will not work with int8 tensors when using one_hot or splitv.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

int8_tensor = tf.constant([[1, 2],
                           [3, 4],
                           [5, 6]], dtype=tf.int8)
try:
    tf.one_hot(int8_tensor, 3)
except Exception as e:
    print(e)
```


### Relevant log output

```shell
tensorflow.python.framework.errors_impl.InvalidArgumentError: Value for attr 'Tlen' of int8 is not in the list of allowed values: int32, int64
	; NodeDef: {{node SplitV}}; Op<name=SplitV; signature=value:T, size_splits:Tlen, split_dim:int32 -> output:num_split*T; attr=num_split:int,min=1; attr=T:type; attr=Tlen:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]> [Op:SplitV] name: split

tensorflow.python.framework.errors_impl.InvalidArgumentError: Value for attr 'TI' of int8 is not in the list of allowed values: uint8, int32, int64
	; NodeDef: {{node OneHot}}; Op<name=OneHot; signature=indices:TI, depth:int32, on_value:T, off_value:T -> output:T; attr=axis:int,default=-1; attr=T:type; attr=TI:type,default=DT_INT64,allowed=[DT_UINT8, DT_INT32, DT_INT64]> [Op:OneHot]
```
</details>"
56784,[tflite/hexagon] The test output shape is not as expected,"https://github.com/tensorflow/tensorflow/blob/6a2e3dcd5f215a641050877d7c302107adafe217/tensorflow/lite/delegates/hexagon/builders/tests/concat_test.cc#L177

As 4 inputs and shape is 1x2x1x2(nhwc) and axis=2, so the output shape should be 1x2x4x2(nhwc), but the test out is 1x2x1x8(nhwc)

debug log as following:
```bash
hexagon_nn_execute_new(gid=6 ic=4 oc=1)
input: 
	nhwc: 1x2x1x2 [139 91 80 44]
	nhwc: 1x2x1x2 [22 62 82 141]
	nhwc: 1x2x1x2 [136 87 76 204]
	nhwc: 1x2x1x2 [45 114 148 252]
	
output: 
	nhwc: 1x2x1x8 [255 0 255 255 255 0 255 255 0 0 255 255 0 255 255 255]
	
[       OK ] QuantizedConcatenationOpModel.FourInputsAxis2_UInt8 (10 ms)
```
BTW: 
the others tests of concat output as expected."
56782,GPU much slower than CPU for text processing using tensorflow-macos,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

TF 2.8

### Custom Code

Yes

### OS Platform and Distribution

MacOS 12.4

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am trying to run the notebook Text classification with an RNN https://www.tensorflow.org/text/tutorials/text_classification_rnn from the TensorFlow website.
The code has LSTM and Bidirectional layers

Although I get the message ""Plugin optimizer for device_type GPU is enabled"", the GPU is not used.

When the GPU is enabled, although not used,  the time is 56 minutes/epoch.
When I am only using the CPU is 264 seconds/epoch.

I face the same issue when trying to run Transformer model for language understanding https://www.tensorflow.org/text/tutorials/transformer

I am using MacBook Pro 14 (10 CPU cores, 16 GPU cores) and TensorFlow-macos 2.8 with TensorFlow-metal 0.5.0. I face the same problem for TensorFlow-macos 2.9.2 too.

My environment has:
tensorflow-macos     2.8.0 
tensorflow-metal     0.5.0
tensorflow-text      2.8.1 
tensorflow-datasets    4.6.0         
tensorflow-deps      2.8.0            
tensorflow-hub      0.12.0           
tensorflow-metadata    1.8.0          
         
When I am using CNNs the GPU is fully enabled and 3-4 times faster than when only using the CPU.
```


### Standalone code to reproduce the issue

```shell
The codes that I have observed the issue are from the TensorFlow website:
https://www.tensorflow.org/text/tutorials/text_classification_rnn
and
https://www.tensorflow.org/text/tutorials/transformer
```


### Relevant log output

_No response_</details>"
56777,coreml output,
56775,Model save repetition causes threading synchronization issue,"Hi everyone,

## Short description
I have encountered a problem with saving a model, or rather overriding a saved model. It seems that the problem is somehow connected with threading. Below a paste a source code to reproduce, and logs:

## Source code
```
import tensorflow as tf

path = './checkpoint/model'

model = tf.keras.Sequential([
    tf.keras.layers.Dense(100),
    tf.keras.layers.Dense(100)
])

model(tf.zeros([1, 100]))

checkpoint = tf.train.Checkpoint(model=model)
manager = tf.train.CheckpointManager(checkpoint, directory=path, max_to_keep=5)

manager.save()
print('saved1')
manager.save()  # here an error occurs
print('saved2')
```

## Errors:
The errors I get are (every script execution causes randomly a different error, thus I expect that it's synch issue):
```
terminate called after throwing an instance of 'std::system_error'
  what():  Invalid argument
```
or
```
terminate called after throwing an instance of 'std::system_error'
  what():  Owner **died**
```
or simply without error but with exit code:
```
Process finished with exit code 136 (interrupted by signal 8: SIGFPE)
```
or:
```
python: ../nptl/pthread_mutex_lock.c:428: __pthread_mutex_lock_full: Assertion `e != ESRCH || !robust' failed.
```
or
```
Process finished with exit code 139 (interrupted by signal 11: SIGSEGV)
```
or:
```
The futex facility returned an unexpected error code.
```

## System and libraries

The TensorFlow package that I use is compiled by myself in r2.9 version for GPU (RTX 3060 12GB, 8.6 CC, cudnn 8.4, cuda 11.6, GCC 9.3). The CPU is: AMD Ryzen 9 5950x 16-core processor  32

TensorFlow compile command:
```
bazel build --jobs=16 --config=nogcp --config=numa --config=monolithic --config=cuda //tensorflow/tools/pip_package:build_pip_package
```

## Help
Does anyone have any idea what the problem may be? When I compiled tensorflow in a similar way on another computer (laptop) everything works fine.
"
56774,python implementation of tf.io.gfile.isdir exhibits different behavior than c++ implementation,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9.1 (master branch)

### Custom Code

No

### OS Platform and Distribution

RHEL 8.4

### Mobile device

None

### Python version

3.9

### Bazel version

4.2.1

### GCC/Compiler version

8.4.1

### CUDA/cuDNN version

None

### GPU model and memory

None

### Current Behaviour?

```shell
io.gfile.isdir() is exported from function is_directory_v2() in tensorflow/python/lib/io/file_io.py.  It currently traps all filesystem errors and converts them to 'false'.  

However in C++, function IsDirectory() in tensorflow/core/platform/file_system.h documents that only the FAILED_PRECONDITION error should be trapped and converted to false (object exists and is a file), all other filesystem errors like NOT_FOUND and PERMISSION_DENIED should throw an exception.  This makes sense, if an object does not exist or a directory that can't be accessed, it should not be reported to be a file (return of false).  

m.def ""IsDirectory"" in tensorflow/python/lib/io/file_io_wrapper.c implements the logic documented in IsDirectory(), mapping only FAILED_PRECONDITION to false and throwing all other errors.

The result is that the 'isdir' python API behaves differently than the C++ API.  I think the C++ implementation is the correct one.  

Function list_directory_v2() in the same file is written assuming the current behavior of is_directory_v2(), it assumes all errors will be mapped to false, it will throw NOT_FOUND if the object is actually a file, or if there is a permissions failure on the directory.

Function walk_v2() also assumes the current behavior of all errors being mapped to false, and so a NOT_FOUND or permissions failure will cause the directory item to be reported as a file.

The fix is to remove the exception clause in is_directory_v2() and fix the documentation.  I believe that list_directory_v2() and walk_v2() in the same file also need tweaks to adapt to the isdir behavior change.
```


### Standalone code to reproduce the issue

```shell
From a python test program in tensorflow/io:

# broken, returns false, expected NOT_FOUND
no_file = tf.io.gfile.join(test_root, ""nonexistent"")
with self.assertRaises(tf.errors.NotFoundError):
    tf.io.gfile.isdir(no_file)

# broken, returns false, expected NOT_FOUND
file_no_parent = tf.io.gfile.join(test_root, ""no_parent"", ""nonexistent"")
with self.assertRaises(tf.errors.NotFoundError):
    tf.io.gfile.isdir(file_no_parent)
```


### Relevant log output

_No response_</details>"
56773,"Error during fit on distributed dataset with multiple GPUs. ""ValueError: When providing a distributed dataset, you must specify the number of steps to run.""","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9

### Custom Code

No

### OS Platform and Distribution

Linux Debian 4.19.194-3

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.7 / 8.3

### GPU model and memory

Tesla V100-PCIE-32GB x2

### Current Behaviour?

```shell
When trying to train a model on multiple GPUs, we get a ValueError at the start of fit method.
The error is caused by the dataset distribution, done through MirroredStrategy and strategy.experimental_distribute_dataset(). If the distribution is removed from the code, no error occurs, but the dataset is not distributed.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

def _parse_function(example):
    _float_feature = tf.io.FixedLenFeature([], tf.float32, default_value=0.0)
    feature_description = {
        'f1': _float_feature,
        'f2': _float_feature,
        'f3': _float_feature,
        'f4': _float_feature,
        'f5': _float_feature,
        'label': tf.io.FixedLenFeature([], tf.int64, default_value=0),
    }
    samples = tf.io.parse_example(example, feature_description)
    label = samples['label']
    features = tf.stack([
            samples['f1'],
            samples['f2'],
            samples['f3'],
            samples['f4'],
            samples['f5']],
            axis=1)
    return (features, label)

gpus = tf.config.list_logical_devices('GPU')
strategy = tf.distribute.MirroredStrategy(gpus)

batch_size_per_replica = 256
batch_size = batch_size_per_replica * strategy.num_replicas_in_sync

train_filename = './training_data.tfrec'
train_dataset = tf.data.TFRecordDataset([train_filename]
        ).batch(batch_size
        ).map(_parse_function)
val_filename = './val_data.tfrec'
val_dataset = tf.data.TFRecordDataset([val_filename]
        ).batch(batch_size
        ).map(_parse_function)

train_dataset = strategy.experimental_distribute_dataset(train_dataset)
val_dataset = strategy.experimental_distribute_dataset(val_dataset)

with strategy.scope():
    mdl = tf.keras.Sequential([
        tf.keras.layers.InputLayer(input_shape=(5,)),
        tf.keras.layers.Dense(5),
        tf.keras.layers.Dense(1, activation='sigmoid')
        ])
    mdl.compile(tf.keras.optimizers.Adam(),
        loss=tf.keras.losses.BinaryCrossentropy())

h = mdl.fit(
        train_dataset, 
        validation_data=val_dataset,
        verbose=0,
        epochs=50,
        batch_size=batch_size,
        )
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""main.py"", line 76, in <module>
    h = mdl.fit(
  File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py"", line 755, in _validate_args
    raise ValueError(""When providing a distributed dataset, you must ""
ValueError: When providing a distributed dataset, you must specify the number of steps to run.
```
</details>"
56772,Test failed from //tensorflow/cc:gradients_linalg_grad_test on GPU,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

master branch

### Custom Code

No

### OS Platform and Distribution

Linux 20.04 from docker tensorflow/tensorflow:devel-gpu

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

5.1.1

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

cuda 11.2 cuDNN 8

### GPU model and memory

RTX 3090 2*24G

### Current Behaviour?

```shell
When I test tensorflow under cuda, the GPU and CPU computation precision does not match and the precision error of GPU exceeds the threshold, causing the test to fail. 

The same test code can pass in CPU environment.

I'm not sure if this is an operator implementation issue in TF or a CUDA issue, would appreciate any help :)
```


### Standalone code to reproduce the issue

```shell
#run_cc_core.sh from official

set -e
set -x

N_JOBS=$(grep -c ^processor /proc/cpuinfo)

echo """"
echo ""Bazel will use ${N_JOBS} concurrent job(s).""
echo """"

# Run configure.
export PYTHON_BIN_PATH=`which python3`
export CC_OPT_FLAGS='-mavx'

export TF_NEED_ROCM=0
export TF_NEED_CUDA=1
export TF_CUDA_COMPUTE_CAPABILITIES=8.6

yes """" | $PYTHON_BIN_PATH configure.py




bazel test --config=cuda //tensorflow/cc:gradients_linalg_grad_test 

(`--config=opt` also failed)
```


### Relevant log output

```shell
Here is the error log:



exec ${PAGER:-/usr/bin/less} ""$0"" || exit 1
Executing tests from //tensorflow/cc:gradients_linalg_grad_test
-----------------------------------------------------------------------------
2022-07-12 14:35:32.516505: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-07-12 14:35:32.517745: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
[==========] Running 10 tests from 1 test suite.
[----------] Global test environment set-up.
[----------] 10 tests from LinalgGradTest
[ RUN      ] LinalgGradTest.Einsum_Transpose
2022-07-12 14:35:32.563180: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-07-12 14:35:36.818279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16045 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:25:00.0, compute capability: 8.6
2022-07-12 14:35:36.819707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22298 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:5b:00.0, compute capability: 8.6
2022-07-12 14:35:36.822428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22298 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:9b:00.0, compute capability: 8.6
2022-07-12 14:35:36.825100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22298 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:c8:00.0, compute capability: 8.6
2022-07-12 14:35:37.183394: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16045 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:25:00.0, compute capability: 8.6
2022-07-12 14:35:37.184359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22298 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:5b:00.0, compute capability: 8.6
2022-07-12 14:35:37.185297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22298 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:9b:00.0, compute capability: 8.6
2022-07-12 14:35:37.186235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22298 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:c8:00.0, compute capability: 8.6
[       OK ] LinalgGradTest.Einsum_Transpose (4672 ms)
[ RUN      ] LinalgGradTest.Einsum_TransposeBroadcast
2022-07-12 14:35:37.245616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16045 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:25:00.0, compute capability: 8.6
2022-07-12 14:35:37.246561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22298 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:5b:00.0, compute capability: 8.6
2022-07-12 14:35:37.247486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22298 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:9b:00.0, compute capability: 8.6
2022-07-12 14:35:37.248417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22298 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:c8:00.0, compute capability: 8.6
2022-07-12 14:35:37.309328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16045 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:25:00.0, compute capability: 8.6
2022-07-12 14:35:37.310255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22298 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:5b:00.0, compute capability: 8.6
2022-07-12 14:35:37.311183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22298 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:9b:00.0, compute capability: 8.6
2022-07-12 14:35:37.312104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22298 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:c8:00.0, compute capability: 8.6
[       OK ] LinalgGradTest.Einsum_TransposeBroadcast (108 ms)
[ RUN      ] LinalgGradTest.Einsum_MatMul
2022-07-12 14:35:37.350102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16045 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:25:00.0, compute capability: 8.6
2022-07-12 14:35:37.351035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22298 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:5b:00.0, compute capability: 8.6
2022-07-12 14:35:37.351955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22298 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:9b:00.0, compute capability: 8.6
2022-07-12 14:35:37.352873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22298 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:c8:00.0, compute capability: 8.6
2022-07-12 14:35:38.658637: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
2022-07-12 14:35:38.683804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16045 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:25:00.0, compute capability: 8.6
2022-07-12 14:35:38.684741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22298 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:5b:00.0, compute capability: 8.6
2022-07-12 14:35:38.685664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22298 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:9b:00.0, compute capability: 8.6
2022-07-12 14:35:38.686599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22298 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:c8:00.0, compute capability: 8.6
tensorflow/cc/gradients/linalg_grad_test.cc:50: Failure
Expected: (max_error) < (1e-3), actual: 0.0205651 vs 0.001
[  FAILED  ] LinalgGradTest.Einsum_MatMul (1434 ms)
[ RUN      ] LinalgGradTest.Einsum_MatMulComplex
2022-07-12 14:35:38.786432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16045 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:25:00.0, compute capability: 8.6
2022-07-12 14:35:38.787367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22298 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:5b:00.0, compute capability: 8.6
2022-07-12 14:35:38.788284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22298 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:9b:00.0, compute capability: 8.6
2022-07-12 14:35:38.789199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22298 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:c8:00.0, compute capability: 8.6
2022-07-12 14:35:38.883969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16045 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:25:00.0, compute capability: 8.6
2022-07-12 14:35:38.884917: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22298 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:5b:00.0, compute capability: 8.6
2022-07-12 14:35:38.885853: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22298 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:9b:00.0, compute capability: 8.6
2022-07-12 14:35:38.886794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22298 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:c8:00.0, compute capability: 8.6
[       OK ] LinalgGradTest.Einsum_MatMulComplex (224 ms)
[ RUN      ] LinalgGradTest.Einsum_MatMulBroadcast
2022-07-12 14:35:39.010302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16045 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:25:00.0, compute capability: 8.6
2022-07-12 14:35:39.011264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22298 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:5b:00.0, compute capability: 8.6
2022-07-12 14:35:39.012205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22298 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:9b:00.0, compute capability: 8.6
2022-07-12 14:35:39.013283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22298 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:c8:00.0, compute capability: 8.6
2022-07-12 14:35:39.094568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16045 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:25:00.0, compute capability: 8.6
2022-07-12 14:35:39.095516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22298 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:5b:00.0, compute capability: 8.6
2022-07-12 14:35:39.096454: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22298 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:9b:00.0, compute capability: 8.6
2022-07-12 14:35:39.097393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22298 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:c8:00.0, compute capability: 8.6
[       OK ] LinalgGradTest.Einsum_MatMulBroadcast (209 ms)
[ RUN      ] LinalgGradTest.Einsum_Trace
2022-07-12 14:35:39.217763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16045 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:25:00.0, compute capability: 8.6
2022-07-12 14:35:39.218724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22298 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:5b:00.0, compute capability: 8.6
2022-07-12 14:35:39.219663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22298 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:9b:00.0, compute capability: 8.6
2022-07-12 14:35:39.220599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22298 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:c8:00.0, compute capability: 8.6
2022-07-12 14:35:39.248566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16045 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:25:00.0, compute capability: 8.6
2022-07-12 14:35:39.249511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22298 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:5b:00.0, compute capability: 8.6
2022-07-12 14:35:39.250464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22298 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:9b:00.0, compute capability: 8.6
2022-07-12 14:35:39.251406: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22298 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:c8:00.0, compute capability: 8.6
[       OK ] LinalgGradTest.Einsum_Trace (84 ms)
[ RUN      ] LinalgGradTest.Einsum_TraceBroadcast
2022-07-12 14:35:39.300791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16045 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:25:00.0, compute capability: 8.6
2022-07-12 14:35:39.301769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22298 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:5b:00.0, compute capability: 8.6
2022-07-12 14:35:39.302759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22298 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:9b:00.0, compute capability: 8.6
2022-07-12 14:35:39.303709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22298 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:c8:00.0, compute capability: 8.6
2022-07-12 14:35:39.348560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16045 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:25:00.0, compute capability: 8.6
2022-07-12 14:35:39.349539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22298 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:5b:00.0, compute capability: 8.6
2022-07-12 14:35:39.350526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22298 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:9b:00.0, compute capability: 8.6
2022-07-12 14:35:39.351483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22298 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:c8:00.0, compute capability: 8.6
[       OK ] LinalgGradTest.Einsum_TraceBroadcast (124 ms)
[ RUN      ] LinalgGradTest.Einsum_DotProduct
2022-07-12 14:35:39.423936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16045 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:25:00.0, compute capability: 8.6
2022-07-12 14:35:39.424873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22298 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:5b:00.0, compute capability: 8.6
2022-07-12 14:35:39.425802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22298 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:9b:00.0, compute capability: 8.6
2022-07-12 14:35:39.426739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22298 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:c8:00.0, compute capability: 8.6
2022-07-12 14:35:39.444904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16045 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:25:00.0, compute capability: 8.6
2022-07-12 14:35:39.445840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22298 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:5b:00.0, compute capability: 8.6
2022-07-12 14:35:39.446776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22298 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:9b:00.0, compute capability: 8.6
2022-07-12 14:35:39.447707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22298 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:c8:00.0, compute capability: 8.6
[       OK ] LinalgGradTest.Einsum_DotProduct (74 ms)
[ RUN      ] LinalgGradTest.Einsum_OuterProduct
2022-07-12 14:35:39.498038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16045 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:25:00.0, compute capability: 8.6
2022-07-12 14:35:39.499049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22298 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:5b:00.0, compute capability: 8.6
2022-07-12 14:35:39.499978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22298 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:9b:00.0, compute capability: 8.6
2022-07-12 14:35:39.500903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22298 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:c8:00.0, compute capability: 8.6
2022-07-12 14:35:39.551515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16045 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:25:00.0, compute capability: 8.6
2022-07-12 14:35:39.552462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22298 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:5b:00.0, compute capability: 8.6
2022-07-12 14:35:39.553387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22298 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:9b:00.0, compute capability: 8.6
2022-07-12 14:35:39.554318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22298 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:c8:00.0, compute capability: 8.6
tensorflow/cc/gradients/linalg_grad_test.cc:50: Failure
Expected: (max_error) < (1e-3), actual: 0.0340652 vs 0.001
[  FAILED  ] LinalgGradTest.Einsum_OuterProduct (104 ms)
[ RUN      ] LinalgGradTest.Einsum_TwoInputReduction
2022-07-12 14:35:39.602237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16045 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:25:00.0, compute capability: 8.6
2022-07-12 14:35:39.603175: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22298 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:5b:00.0, compute capability: 8.6
2022-07-12 14:35:39.604102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22298 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:9b:00.0, compute capability: 8.6
2022-07-12 14:35:39.605027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22298 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:c8:00.0, compute capability: 8.6
2022-07-12 14:35:39.703831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16045 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:25:00.0, compute capability: 8.6
2022-07-12 14:35:39.704745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22298 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:5b:00.0, compute capability: 8.6
2022-07-12 14:35:39.705654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22298 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:9b:00.0, compute capability: 8.6
2022-07-12 14:35:39.706568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22298 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:c8:00.0, compute capability: 8.6
tensorflow/cc/gradients/linalg_grad_test.cc:50: Failure
Expected: (max_error) < (1e-3), actual: 0.353254 vs 0.001
[  FAILED  ] LinalgGradTest.Einsum_TwoInputReduction (238 ms)
[----------] 10 tests from LinalgGradTest (7272 ms total)

[----------] Global test environment tear-down
[==========] 10 tests from 1 test suite ran. (7273 ms total)
[  PASSED  ] 7 tests.
[  FAILED  ] 3 tests, listed below:
[  FAILED  ] LinalgGradTest.Einsum_MatMul
[  FAILED  ] LinalgGradTest.Einsum_OuterProduct
[  FAILED  ] LinalgGradTest.Einsum_TwoInputReduction

 3 FAILED TESTS
--
Coverage runner: Not collecting coverage for failed test.
The following commands failed with status 1
/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/cc/gradients_linalg_grad_test.runfiles/org_tensorflow/tensorflow/cc/gradients_linalg_grad_test

```


```
INFO: Build completed, 1 test FAILED, 8831 total actions
//tensorflow/cc:gradients_linalg_grad_test                               FAILED in 3 out of 3 in 17.1s
  Stats over 3 runs: max = 17.1s, min = 15.1s, avg = 15.8s, dev = 0.9s
  /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/cc/gradients_linalg_grad_test/test.log
  /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/cc/gradients_linalg_grad_test/test_attempts/attempt_1.log
  /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/cc/gradients_linalg_grad_test/test_attempts/attempt_2.log
```
```
</details>"
56770,Error Tensorflow lite model -android studio,"I'm trying to import a tensorflow lite model ( inception V3 model for detecting a lesions in a retinal images) into my android studio. I've followed a tutorial . i got this erreur , i don't know how to solve it .
my input image :
![0f96c358a250](https://user-images.githubusercontent.com/83914766/178984031-ab655b60-d1e8-40bf-b156-74860052fbd7.png)


i got this error 
java.lang.ArrayIndexOutOfBoundsException
            at android.graphics.Bitmap.checkPixelsAccess(Bitmap.java:1706)
            at android.graphics.Bitmap.getPixels(Bitmap.java:1647)
            at com.example.micro_model.MainActivity.classifyImage(MainActivity.java:133)
            at com.example.micro_model.MainActivity$2.onManagerConnected(MainActivity.java:225)
            at com.example.micro_model.MainActivity$1.onClick(MainActivity.java:192)
            at android.view.View.performClick(View.java:6256)
            at com.google.android.material.button.MaterialButton.performClick(MaterialButton.java:1119)

here is my code : 


**public class MainActivity extends AppCompatActivity {


    ImageView imageView;
    Button button;

    TextView result;
    int imageSize=299;



    private static final String TAG = ""MainActivity"";

    static {
        if (OpenCVLoader.initDebug()) {
            Log.d(TAG, ""opencv not load"");
        } else {
            Log.d(TAG, ""opencv loaded: "");
        }
    }




    public void classifyImage(Bitmap image)
    {
        try {
        ConvertedModel model = ConvertedModel.newInstance(getApplicationContext());
        TensorBuffer inputFeature0 = TensorBuffer.createFixedSize(new int[]{1, 299, 299, 3}, DataType.FLOAT32);
        int width =image.getWidth();
        int height =image.getHeight();
        ByteBuffer byteBuffer= ByteBuffer.allocateDirect(4 * imageSize * imageSize * 3 );
        byteBuffer.order(ByteOrder.nativeOrder());
        int [] intValues= new int [imageSize * imageSize];
        image.getPixels(intValues,0,width, 0, 0 , width,height);
        int pixel=0;
            for(int i = 0; i < imageSize; i++){
                for(int j = 0; j < imageSize; j++){
                    int val = intValues[pixel++];
                    byteBuffer.putFloat(((val >> 16) & 0xFF) * (1.f / 1));
                    byteBuffer.putFloat(((val >> 8) & 0xFF) * (1.f / 1));
                    byteBuffer.putFloat((val  & 0xFF) * (1.f / 1));

                }
            }



        inputFeature0.loadBuffer(byteBuffer);

        ConvertedModel.Outputs outputs = model.process(inputFeature0);
        TensorBuffer outputFeature0 = outputs.getOutputFeature0AsTensorBuffer();
            float[] confidence = outputFeature0.getFloatArray();
            int maxPos = 0;
            float maxConfidence = 0;
            for(int i = 0; i < confidence.length; i++){
                if(confidence[i] > maxConfidence){
                    maxConfidence = confidence[i];
                    maxPos = i;
                }
            }
            String[] classes = {""micro"",""no-micro""};
            result.setText(classes[maxPos]);




        model.close();

    } catch (IOException e) {
        // TODO Handle the exception
    }
    }

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main);
        imageView = findViewById(R.id.imageView);
        Button btn = findViewById(R.id.button);
        Button slc = findViewById(R.id.button2);
        result =findViewById(R.id.result);



        slc.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {

                if (!OpenCVLoader.initDebug()) {

                    OpenCVLoader.initAsync(OpenCVLoader.OPENCV_VERSION, getApplicationContext(), baseLoaderCallback1);
                } else
                    baseLoaderCallback1.onManagerConnected(LoaderCallbackInterface.SUCCESS);

            }
        });


        Log.e("""", ""onCreate: image id"" + R.id.image);

    }

    /**
     *
     */


    BaseLoaderCallback baseLoaderCallback1;

    {
        baseLoaderCallback1 = new BaseLoaderCallback(this) {
            @Override
            public void onManagerConnected(int status) {
                super.onManagerConnected(status);
                if (status == LoaderCallbackInterface.SUCCESS) {
                   Mat newImage= new Mat();
                    Bitmap img = BitmapFactory.decodeResource(getResources(),R.drawable.prepm);
                    Utils.bitmapToMat(img,newImage);
                    Imgproc.cvtColor(newImage, newImage, Imgproc.COLOR_RGB2BGR);
                    Bitmap bm = Bitmap.createBitmap(newImage.width(), newImage.height(), Bitmap.Config.ARGB_8888);
                    Utils.matToBitmap(newImage, bm);
                    imageView.setImageBitmap(bm);

                    //result.setText(img.channel);
                    //img=Bitmap.createScaledBitmap(img,imageSize,imageSize, false);
                   classifyImage(bm);

            

}


            }
        };
    }
}


"
56769,[XLA] Bincount dynamic output size in a loop,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

master

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
TF XLA fail with bincount with dynamic output shape in a loop
```


### Standalone code to reproduce the issue

```shell
!pip install tf-nightly-cpu
```

```python

import tensorflow as tf
@tf.function(jit_compile=True)
def bincount_test():
  for length in tf.range(2):
    values = tf.constant([1,1,2,3,2,4,4,5])
    output = tf.math.bincount(values, minlength=length) #[0 2 2 1 2 1]
  return output

 
print(bincount_test())
```


### Relevant log output

```shell
InvalidArgumentError: Input 1 to node `while/bincount/Bincount` with op Bincount must be a compile-time constant.

XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.
```
</details>"
56768,Unpredictable behaviour of tracked variables in subclassing layers and models,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.8.2

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I have shared a google colab notebook which I think is self explanatory. 

When subclassing a Keras Layer: 
* You can define variables in a list using list comprehension. 
* You can define other keras Layers in a list starting with an empty list and using append.
* You cannot define varaibles in a list starting with an empty list and using append.

When subclassing a Keras Model: 
* You can define other keras Layers in a list starting with an empty list and using append.
* You can define varaibles in a list starting with an empty list and using append.

Personally, I find the situation very unpredictable and strange.
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/108tvoGVHDTHuoMP39ZsPKUbSsnO_jRYr#scrollTo=4ikYkJzxWuHy
```


### Relevant log output

_No response_</details>"
56767,Cannot convert to unidirectional_sequence_rnn op in tflite,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RedHat7
- TensorFlow installation (pip package or built from source): pip 
- TensorFlow library (version, if pip package or github SHA, if built from source):2.6.3 

### 2. Code

I tried to gen a Keras Model via below api:

rnn_layer = tf.keras.layers.RNN(tf.keras.layers.SimpleRNNCell(n_hidden),)

and use below to convert to tflite:

converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)
tflite_model = converter.convert()
open(model_path, ""wb"").write(tflite_model)

But checked in Netron the graph of generated tflite liked below:
![image](https://user-images.githubusercontent.com/30307463/178934550-d3c5f86a-057b-4fe8-a36c-4129099be2a7.png)

You can see, while OP in graph but not unidirectional_sequence_rnn 

"
56766,Confusion about TFLite suitable models in combination with TF OD API,"Hello guys,

I would like to know which model exactly from `Model Zoo 1 & 2 ` (https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md  //// https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md) are suitable for TensorFlow Lite, or can be converted. 

My findings so far are:
1. TFLite only supports `uint8/int8`, so quantized models.
2. it is possible to convert` float32/float16` models to` uint8/int8` models using the converters of TF1&2
3. the Object Detection API supports for TF1&2 **only SSD models**

Questions:
1. when converting models other than SSD model to TFLite model, e.g. faster_rcnn_inception_v2_coco (float) this can only be run on a CPU or GPU, not with TensorFlow Lite (matching point 2 of findings).

I would be really interested in clearing up this mystery to make reliable statements.
"
56765,Segfault with `tf-to-hlo-pipeline` pass,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


The `tf-to-hlo-pipeline` pass will cause a segfault with certain IR.  In particular when there is an external function call, e.g.

```shell
call @foo() : () -> ()
```

I now realize that such an IR is not supposed to be used for this pass, but the segfault itself indicates a bug.  I've narrowed it down to this line here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tensorflow/transforms/guarantee_all_funcs_one_use.cc#L44.  Ironically, the segfault happens while attempting to produce a proper error message.


### Standalone code to reproduce the issue

```
tf-opt --tf-to-hlo-pipeline foo.mlir
```

where the contents of foo.mlir is

```
func.func @main() {
    call @foo() : () -> ()
    return
}
func.func private @foo() -> ()
```
```


### Relevant log output

_No response_</details>"
56763,"label_image:undefined reference to `tensorflow::str_util::EndsWith(absl::lts_20211102::string_view, absl::lts_20211102::string_view)'","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9

### Custom Code

No

### OS Platform and Distribution

linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

5.1.1

### GCC/Compiler version

9.4

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When I built the label_image example, it just didn't work like:
[1/1] Linking CXX executable test_tensorflow
FAILED: test_tensorflow 
: && /usr/bin/c++ -O3 -DNDEBUG  CMakeFiles/test_tensorflow.dir/main.cpp.o -o test_tensorflow -L/home/nkk/nkk/nkk/work/zht/src/../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow   -L/usr/local/lib -Wl,-rpath,/home/nkk/nkk/nkk/work/zht/src/../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow:/usr/local/lib:/home/nkk/nkk/nkk/work/zht/src/../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow/lite  /usr/local/lib/libabsl_bad_any_cast_impl.a  /usr/local/lib/libabsl_bad_optional_access.a  /usr/local/lib/libabsl_bad_variant_access.a  /usr/local/lib/libabsl_base.a  /usr/local/lib/libabsl_city.a  /usr/local/lib/libabsl_civil_time.a  /usr/local/lib/libabsl_cord.a  /usr/local/lib/libabsl_cord_internal.a  /usr/local/lib/libabsl_cordz_functions.a  /usr/local/lib/libabsl_cordz_handle.a  /usr/local/lib/libabsl_cordz_info.a  /usr/local/lib/libabsl_cordz_sample_token.a  /usr/local/lib/libabsl_debugging_internal.a  /usr/local/lib/libabsl_demangle_internal.a  /usr/local/lib/libabsl_examine_stack.a  /usr/local/lib/libabsl_exponential_biased.a  /usr/local/lib/libabsl_failure_signal_handler.a  /usr/local/lib/libabsl_flags.a  /usr/local/lib/libabsl_flags_commandlineflag.a  /usr/local/lib/libabsl_flags_commandlineflag_internal.a  /usr/local/lib/libabsl_flags_config.a  /usr/local/lib/libabsl_flags_internal.a  /usr/local/lib/libabsl_flags_marshalling.a  /usr/local/lib/libabsl_flags_parse.a  /usr/local/lib/libabsl_flags_private_handle_accessor.a  /usr/local/lib/libabsl_flags_program_name.a  /usr/local/lib/libabsl_flags_reflection.a  /usr/local/lib/libabsl_flags_usage.a  /usr/local/lib/libabsl_flags_usage_internal.a  /usr/local/lib/libabsl_graphcycles_internal.a  /usr/local/lib/libabsl_hash.a  /usr/local/lib/libabsl_hashtablez_sampler.a  /usr/local/lib/libabsl_int128.a  /usr/local/lib/libabsl_leak_check.a  /usr/local/lib/libabsl_leak_check_disable.a  /usr/local/lib/libabsl_log_severity.a  /usr/local/lib/libabsl_low_level_hash.a  /usr/local/lib/libabsl_malloc_internal.a  /usr/local/lib/libabsl_periodic_sampler.a  /usr/local/lib/libabsl_random_distributions.a  /usr/local/lib/libabsl_random_internal_distribution_test_util.a  /usr/local/lib/libabsl_random_internal_platform.a  /usr/local/lib/libabsl_random_internal_pool_urbg.a  /usr/local/lib/libabsl_random_internal_randen.a  /usr/local/lib/libabsl_random_internal_randen_hwaes.a  /usr/local/lib/libabsl_random_internal_randen_hwaes_impl.a  /usr/local/lib/libabsl_random_internal_randen_slow.a  /usr/local/lib/libabsl_random_internal_seed_material.a  /usr/local/lib/libabsl_random_seed_gen_exception.a  /usr/local/lib/libabsl_random_seed_sequences.a  /usr/local/lib/libabsl_raw_hash_set.a  /usr/local/lib/libabsl_raw_logging_internal.a  /usr/local/lib/libabsl_scoped_set_env.a  /usr/local/lib/libabsl_spinlock_wait.a  /usr/local/lib/libabsl_stacktrace.a  /usr/local/lib/libabsl_status.a  /usr/local/lib/libabsl_statusor.a  /usr/local/lib/libabsl_str_format_internal.a  /usr/local/lib/libabsl_strerror.a  /usr/local/lib/libabsl_strings.a  /usr/local/lib/libabsl_strings_internal.a  /usr/local/lib/libabsl_symbolize.a  /usr/local/lib/libabsl_synchronization.a  /usr/local/lib/libabsl_throw_delegate.a  /usr/local/lib/libabsl_time.a  /usr/local/lib/libabsl_time_zone.a  ../../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow//libtensorflow_cc.so  ../../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow//libtensorflow_framework.so.2  ../../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow//lite/libtensorflowlite.so && :
/usr/bin/ld: CMakeFiles/test_tensorflow.dir/main.cpp.o: in function `ReadTensorFromImageFile(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, int, float, float, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*)':
main.cpp:(.text+0x2b7b): undefined reference to `tensorflow::str_util::EndsWith(absl::lts_20211102::string_view, absl::lts_20211102::string_view)'
/usr/bin/ld: main.cpp:(.text+0x2bc1): undefined reference to `tensorflow::str_util::EndsWith(absl::lts_20211102::string_view, absl::lts_20211102::string_view)'
/usr/bin/ld: main.cpp:(.text+0x2bf3): undefined reference to `tensorflow::str_util::EndsWith(absl::lts_20211102::string_view, absl::lts_20211102::string_view)'
/usr/bin/ld: CMakeFiles/test_tensorflow.dir/main.cpp.o: in function `tensorflow::Status tensorflow::errors::NotFound<char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*>(char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*)':
main.cpp:(.text._ZN10tensorflow6errors8NotFoundIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_EEENS_6StatusEDpT_[_ZN10tensorflow6errors8NotFoundIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_EEENS_6StatusEDpT_]+0x3b4): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_20211102::string_view)'
/usr/bin/ld: CMakeFiles/test_tensorflow.dir/main.cpp.o: in function `tensorflow::Status tensorflow::errors::DataLoss<char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*, unsigned long, char const*, unsigned long>(char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*, unsigned long, char const*, unsigned long)':
main.cpp:(.text._ZN10tensorflow6errors8DataLossIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_mS3_mEEENS_6StatusEDpT_[_ZN10tensorflow6errors8DataLossIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_mS3_mEEENS_6StatusEDpT_]+0x96b): undefined reference to `tensorflow::strings::internal::CatPieces[abi:cxx11](std::initializer_list<absl::lts_20211102::string_view>)'
/usr/bin/ld: main.cpp:(.text._ZN10tensorflow6errors8DataLossIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_mS3_mEEENS_6StatusEDpT_[_ZN10tensorflow6errors8DataLossIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_mS3_mEEENS_6StatusEDpT_]+0x98a): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_20211102::string_view)'
/usr/bin/ld: CMakeFiles/test_tensorflow.dir/main.cpp.o: in function `main':
main.cpp:(.text.startup+0x6ab): undefined reference to `tensorflow::io::internal::JoinPathImpl[abi:cxx11](std::initializer_list<absl::lts_20211102::string_view>)'
/usr/bin/ld: main.cpp:(.text.startup+0x87f): undefined reference to `tensorflow::io::internal::JoinPathImpl[abi:cxx11](std::initializer_list<absl::lts_20211102::string_view>)'
collect2: error: ld returned 1 exit status
ninja: build stopped: subcommand failed.
```


### Standalone code to reproduce the issue

```shell
if (tensorflow::str_util::EndsWith(tensorflow::StringPiece(file_name.c_str(), file_name.size()),
                                       tensorflow::StringPiece("".png"", std::string("".png"").size()))) {
        image_reader = DecodePng(root.WithOpName(""png_reader""), file_reader,
                                 DecodePng::Channels(wanted_channels));
    } else if (tensorflow::str_util::EndsWith(tensorflow::StringPiece(file_name.c_str()),
                                              tensorflow::StringPiece("".gif""))) {
        // gif decoder returns 4-D tensor, remove the first dim
        image_reader =
                Squeeze(root.WithOpName(""squeeze_first_dim""),
                        DecodeGif(root.WithOpName(""gif_reader""), file_reader));
    } else if (tensorflow::str_util::EndsWith(tensorflow::StringPiece(file_name.c_str()),
                                              tensorflow::StringPiece("".bmp""))) {
        image_reader = DecodeBmp(root.WithOpName(""bmp_reader""), file_reader);
    } else {
        // Assume if it's neither a PNG nor a GIF then it must be a JPEG.
        image_reader = DecodeJpeg(root.WithOpName(""jpeg_reader""), file_reader,
                                  DecodeJpeg::Channels(wanted_channels));
    }
```


### Relevant log output

```shell
[1/1] Linking CXX executable test_tensorflow
FAILED: test_tensorflow 
: && /usr/bin/c++ -O3 -DNDEBUG  CMakeFiles/test_tensorflow.dir/main.cpp.o -o test_tensorflow -L/home/nkk/nkk/nkk/work/zht/src/../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow   -L/usr/local/lib -Wl,-rpath,/home/nkk/nkk/nkk/work/zht/src/../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow:/usr/local/lib:/home/nkk/nkk/nkk/work/zht/src/../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow/lite  /usr/local/lib/libabsl_bad_any_cast_impl.a  /usr/local/lib/libabsl_bad_optional_access.a  /usr/local/lib/libabsl_bad_variant_access.a  /usr/local/lib/libabsl_base.a  /usr/local/lib/libabsl_city.a  /usr/local/lib/libabsl_civil_time.a  /usr/local/lib/libabsl_cord.a  /usr/local/lib/libabsl_cord_internal.a  /usr/local/lib/libabsl_cordz_functions.a  /usr/local/lib/libabsl_cordz_handle.a  /usr/local/lib/libabsl_cordz_info.a  /usr/local/lib/libabsl_cordz_sample_token.a  /usr/local/lib/libabsl_debugging_internal.a  /usr/local/lib/libabsl_demangle_internal.a  /usr/local/lib/libabsl_examine_stack.a  /usr/local/lib/libabsl_exponential_biased.a  /usr/local/lib/libabsl_failure_signal_handler.a  /usr/local/lib/libabsl_flags.a  /usr/local/lib/libabsl_flags_commandlineflag.a  /usr/local/lib/libabsl_flags_commandlineflag_internal.a  /usr/local/lib/libabsl_flags_config.a  /usr/local/lib/libabsl_flags_internal.a  /usr/local/lib/libabsl_flags_marshalling.a  /usr/local/lib/libabsl_flags_parse.a  /usr/local/lib/libabsl_flags_private_handle_accessor.a  /usr/local/lib/libabsl_flags_program_name.a  /usr/local/lib/libabsl_flags_reflection.a  /usr/local/lib/libabsl_flags_usage.a  /usr/local/lib/libabsl_flags_usage_internal.a  /usr/local/lib/libabsl_graphcycles_internal.a  /usr/local/lib/libabsl_hash.a  /usr/local/lib/libabsl_hashtablez_sampler.a  /usr/local/lib/libabsl_int128.a  /usr/local/lib/libabsl_leak_check.a  /usr/local/lib/libabsl_leak_check_disable.a  /usr/local/lib/libabsl_log_severity.a  /usr/local/lib/libabsl_low_level_hash.a  /usr/local/lib/libabsl_malloc_internal.a  /usr/local/lib/libabsl_periodic_sampler.a  /usr/local/lib/libabsl_random_distributions.a  /usr/local/lib/libabsl_random_internal_distribution_test_util.a  /usr/local/lib/libabsl_random_internal_platform.a  /usr/local/lib/libabsl_random_internal_pool_urbg.a  /usr/local/lib/libabsl_random_internal_randen.a  /usr/local/lib/libabsl_random_internal_randen_hwaes.a  /usr/local/lib/libabsl_random_internal_randen_hwaes_impl.a  /usr/local/lib/libabsl_random_internal_randen_slow.a  /usr/local/lib/libabsl_random_internal_seed_material.a  /usr/local/lib/libabsl_random_seed_gen_exception.a  /usr/local/lib/libabsl_random_seed_sequences.a  /usr/local/lib/libabsl_raw_hash_set.a  /usr/local/lib/libabsl_raw_logging_internal.a  /usr/local/lib/libabsl_scoped_set_env.a  /usr/local/lib/libabsl_spinlock_wait.a  /usr/local/lib/libabsl_stacktrace.a  /usr/local/lib/libabsl_status.a  /usr/local/lib/libabsl_statusor.a  /usr/local/lib/libabsl_str_format_internal.a  /usr/local/lib/libabsl_strerror.a  /usr/local/lib/libabsl_strings.a  /usr/local/lib/libabsl_strings_internal.a  /usr/local/lib/libabsl_symbolize.a  /usr/local/lib/libabsl_synchronization.a  /usr/local/lib/libabsl_throw_delegate.a  /usr/local/lib/libabsl_time.a  /usr/local/lib/libabsl_time_zone.a  ../../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow//libtensorflow_cc.so  ../../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow//libtensorflow_framework.so.2  ../../3rdpartylib/tensorflow-2.9.0/bazel-bin/tensorflow//lite/libtensorflowlite.so && :
/usr/bin/ld: CMakeFiles/test_tensorflow.dir/main.cpp.o: in function `ReadTensorFromImageFile(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, int, float, float, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*)':
main.cpp:(.text+0x2b7b): undefined reference to `tensorflow::str_util::EndsWith(absl::lts_20211102::string_view, absl::lts_20211102::string_view)'
/usr/bin/ld: main.cpp:(.text+0x2bc1): undefined reference to `tensorflow::str_util::EndsWith(absl::lts_20211102::string_view, absl::lts_20211102::string_view)'
/usr/bin/ld: main.cpp:(.text+0x2bf3): undefined reference to `tensorflow::str_util::EndsWith(absl::lts_20211102::string_view, absl::lts_20211102::string_view)'
/usr/bin/ld: CMakeFiles/test_tensorflow.dir/main.cpp.o: in function `tensorflow::Status tensorflow::errors::NotFound<char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*>(char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*)':
main.cpp:(.text._ZN10tensorflow6errors8NotFoundIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_EEENS_6StatusEDpT_[_ZN10tensorflow6errors8NotFoundIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_EEENS_6StatusEDpT_]+0x3b4): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_20211102::string_view)'
/usr/bin/ld: CMakeFiles/test_tensorflow.dir/main.cpp.o: in function `tensorflow::Status tensorflow::errors::DataLoss<char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*, unsigned long, char const*, unsigned long>(char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*, unsigned long, char const*, unsigned long)':
main.cpp:(.text._ZN10tensorflow6errors8DataLossIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_mS3_mEEENS_6StatusEDpT_[_ZN10tensorflow6errors8DataLossIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_mS3_mEEENS_6StatusEDpT_]+0x96b): undefined reference to `tensorflow::strings::internal::CatPieces[abi:cxx11](std::initializer_list<absl::lts_20211102::string_view>)'
/usr/bin/ld: main.cpp:(.text._ZN10tensorflow6errors8DataLossIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_mS3_mEEENS_6StatusEDpT_[_ZN10tensorflow6errors8DataLossIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_mS3_mEEENS_6StatusEDpT_]+0x98a): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_20211102::string_view)'
/usr/bin/ld: CMakeFiles/test_tensorflow.dir/main.cpp.o: in function `main':
main.cpp:(.text.startup+0x6ab): undefined reference to `tensorflow::io::internal::JoinPathImpl[abi:cxx11](std::initializer_list<absl::lts_20211102::string_view>)'
/usr/bin/ld: main.cpp:(.text.startup+0x87f): undefined reference to `tensorflow::io::internal::JoinPathImpl[abi:cxx11](std::initializer_list<absl::lts_20211102::string_view>)'
collect2: error: ld returned 1 exit status
ninja: build stopped: subcommand failed.
```
</details>"
56760,"TPU pod  v4 ""RuntimeError: TPU cores on each host is not same."" ","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

TF 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

Python 3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am trying to run a model on cloud TPU V4 POD. The V4 TPU is created with version tpu-vm-tf-2.9.1-pod-v4 and environment variables/package installations set according to the guide here: https://cloud.google.com/tpu/docs/v4-users-guide

When initializing the TPU strategy, which works perfectly fine on a V4-8 running version v2-alpha-tpuv4, I get a RuntimeError stating ""TPU cores on each host is not same. This should not happen!""  Any help would be appreciated.
```


### Standalone code to reproduce the issue

```shell
import time
import os
import subprocess
import sys
import re
import argparse
import collections
import gzip
import math
import shutil
import matplotlib.pyplot as plt
import wandb
import numpy as np
import time
from datetime import datetime
import random

import seaborn as sns

import tensorflow as tf
import tensorflow.experimental.numpy as tnp
import tensorflow_addons as tfa
from tensorflow import strings as tfs
from tensorflow.keras import mixed_precision
from scipy import stats

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpupod)
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.experimental.TPUStrategy(resolver)
```


### Relevant log output

```shell
2022-07-13 19:28:05.566390: I tensorflow/core/tpu/tpu_api_dlsym_initializer.cc:116] Libtpu path is: libtpu.so
Missing key: 'TWIST' in 'tpu-env' (yaml) instance metadata.
test.py:25: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.
  from scipy.stats.stats import pearsonr
2022-07-13 19:28:20.657469: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-07-13 19:28:20.835472: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x2727500 initialized for platform TPU (this does not guarantee that XLA will be used). Devices:
2022-07-13 19:28:20.835513: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): TPU, 2a886c8
2022-07-13 19:28:20.835519: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (1): TPU, 2a886c8
2022-07-13 19:28:20.835523: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (2): TPU, 2a886c8
2022-07-13 19:28:20.835527: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (3): TPU, 2a886c8
2022-07-13 19:28:21.026508: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> 10.28.0.20:8470, 1 -> 10.28.0.19:8470}
2022-07-13 19:28:21.026560: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:44426}
2022-07-13 19:28:21.043816: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> 10.28.0.20:8470, 1 -> 10.28.0.19:8470}
2022-07-13 19:28:21.043844: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:44426}
2022-07-13 19:28:21.045847: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:44426
WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.
Traceback (most recent call last):
  File ""test.py"", line 43, in <module>
    strategy = tf.distribute.experimental.TPUStrategy(resolver)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/tpu_strategy.py"", line 696, in __init__
    TPUExtended(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/tpu_strategy.py"", line 864, in __init__
    self._tpu_metadata = self._tpu_cluster_resolver.get_tpu_system_metadata()
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py"", line 297, in get_tpu_system_metadata
    tpu_system_metadata_lib._query_tpu_system_metadata(  # pylint: disable=protected-access
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_system_metadata.py"", line 123, in _query_tpu_system_metadata
    raise RuntimeError(
RuntimeError: TPU cores on each host is not same. This should not happen!. devices: [_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0), _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0), _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0), _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0), _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0), _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0), _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0), _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0), _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0), _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0), _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0), _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0), _DeviceAttributes(/job:worker/replica:0/task:1/device:CPU:0, CPU, 0, 0), _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0), _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:0, TPU, 0, 0), _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:1, TPU, 0, 0), _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:2, TPU, 0, 0), _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:3, TPU, 0, 0)]
```
</details>"
56757,MultiHeadAttention instances within custom layers do not receive weights when loaded with SavedModel format,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.8.1

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04.4 LTS

### Mobile device

N/A

### Python version

3.10.4

### Bazel version

N/A

### GCC/Compiler version

N/A

### CUDA/cuDNN version

11.4

### GPU model and memory

N/A

### Current Behaviour?

```shell
When loading a Keras model saved with the SavedModel format, MultiHeadAttention instances contained within custom layers do not properly receive their loaded weights. Since the MultiHeadAttention layers' weights are different each time loading, it does seem more likely to be a loading issue rather than a saving issue. From the notebook provided, this issue seems to be exclusive to MultiHeadAttention as other nested layer instances (that I've worked with) properly receive their weights.
```


### Standalone code to reproduce the issue

```shell
https://gist.github.com/SirDavidLudwig/9c7fe36e4513ef040959ffd3178c1111
```


### Relevant log output

```shell
N/A
```
</details>"
56756,cannot import name 'anchor_generator_pb2' from 'object_detection.protos',"When running a verification script this error outputs: ImportError: cannot import name 'anchor_generator_pb2' from 'object_detection.protos'
I have been stuck at this error for many days and all the solutions haven't worked.
Any help is greatly appreciated."
56755,TF Windows build failing on Bazel CI with Bazel HEAD,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

last_green

### Custom Code

No

### OS Platform and Distribution

Windows

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

HEAD

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Build breaks (see error log at https://buildkite.com/bazel/bazel-at-head-plus-downstream/builds/2547#0181f78a-6047-4ece-9101-20a934b42e61)


### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

```shell
tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(434): error C2672: 'llvm::formatv': no matching overloaded function found
tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(435): error C2893: Failed to specialize function template 'llvm::formatv_object<unknown-type> llvm::formatv(const char *,Ts &&...)'
external/llvm-project/llvm/include\llvm/Support/FormatVariadic.h(251): note: see declaration of 'llvm::formatv'
tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(435): note: With the following template arguments:
tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(435): note: 'Ts={const absl::lts_20211102::string_view &, size_t}'
tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(433): error C2672: 'tensorflow::errors::InvalidArgument': no matching overloaded function found
tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(486): error C2672: 'mlir::Operation::getAttrOfType': no matching overloaded function found
tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(487): error C2440: 'initializing': cannot convert from 'const absl::lts_20211102::string_view' to 'mlir::StringAttr'
tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(487): note: No user-defined-conversion operator available that can perform this conversion, or the operator cannot be called
tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(502): error C2672: 'mlir::Operation::getAttrOfType': no matching overloaded function found
tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(502): error C2440: 'initializing': cannot convert from 'const absl::lts_20211102::string_view' to 'mlir::StringAttr'
tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(502): note: No user-defined-conversion operator available that can perform this conversion, or the operator cannot be called
tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(503): error C3536: 'topology_attr': cannot be used before it is initialized
tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(506): error C2672: 'mlir::Operation::getAttrOfType': no matching overloaded function found
tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(507): error C2440: 'initializing': cannot convert from 'const absl::lts_20211102::string_view' to 'mlir::StringAttr'
tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(507): note: No user-defined-conversion operator available that can perform this conversion, or the operator cannot be called
tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(508): error C3536: 'device_assignment_attr': cannot be used before it is initialized
tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(509): error C2672: 'llvm::formatv': no matching overloaded function found
tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(510): error C2893: Failed to specialize function template 'llvm::formatv_object<unknown-type> llvm::formatv(const char *,Ts &&...)'
external/llvm-project/llvm/include\llvm/Support/FormatVariadic.h(251): note: see declaration of 'llvm::formatv'
tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(510): note: With the following template arguments:
tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(510): note: 'Ts={const absl::lts_20211102::string_view &}'
tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(514): error C2664: 'tensorflow::StatusOr<llvm::SmallVector<int64_t,8>> tensorflow::GetDeviceCoordinates(mlir::ArrayAttr)': cannot convert argument 1 from 'int' to 'mlir::ArrayAttr'
tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(514): note: No constructor could take the source type, or constructor overload resolution was ambiguous
tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(423): note: see declaration of 'tensorflow::GetDeviceCoordinates'
tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(516): error C3536: 'status_or_device_coodinates': cannot be used before it is initialized
tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(523): error C2660: 'tensorflow::GetTPUCompilationAndExecutionDevices': function does not take 3 arguments
tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(444): note: see declaration of 'tensorflow::GetTPUCompilationAndExecutionDevices'
tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(527): error C3536: 'status_or_tpu_device_assignment': cannot be used before it is initialized
tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(531): error C2530: 'tpu_device_assignment': references must be initialized
tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc(533): error C3536: 'tpu_device_assignment': cannot be used before it is initialized
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```
</details>"
56754,Wrong forward gradient in `tf.autodiff.ForwardAccumulator` for `tf.sign`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Forward-mode auto grad result for `tf.sign` should be `0.0` with input `0.7887528`, but got `None`.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

x = tf.constant([0.7887528],tf.float32)
with tf.GradientTape(persistent=True,) as g:
  g.watch(x)
  res_backward = tf.sign(x)
print(g.jacobian(res_backward,x))

with tf.autodiff.ForwardAccumulator(x,tf.constant([1.],tf.float32)) as acc:
  res_forward = tf.sign(x)
print(acc.jvp(res_forward))
```


### Relevant log output

```shell
tf.Tensor([[0.]], shape=(1, 1), dtype=float32)
None
```
</details>"
56753,Cant use GPU when using Tensorflow Lite Model on Custom Object Detection,"Hi, im trying to use transferlearning on MobileNetV2 SSD for facial detection. When i run the model on the example provided for Object Detection using CPU it works well but when i use GPU, Android shows me this error:

org.tensorflow.lite.examples.objectdetection E/tflite: Following operations are not supported by GPU delegate:
    CUSTOM TFLite_Detection_PostProcess: TFLite_Detection_PostProcess
    PACK: OP is supported, but tensor type/shape isn't compatible.
    RESHAPE: OP is supported, but tensor type/shape isn't compatible.
    111 operations will run on the GPU, and the remaining 46 operations will run on the CPU. 

And the App stops working.

The code use for converting the saved model to tflite its the following:

import tensorflow as tf

saved_model_dir = 'url'

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
tflite_model = converter.convert()
open(""url"", ""wb"").write(tflite_model)


I use libraries and functions:
tf-nightly
export_tflite_graph_tf2.py
tflite_support_nightly


Thanks in advance.
"
56752,tensorflow jit_compile=True threads leak,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04.4 LTS

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
threads leak
```


### Standalone code to reproduce the issue

```shell
def makeModel()
    #Create a normal model...

    #important
    model.compile(optimizer=optimizers.SGD(learning_rate=0.01), loss=losses.MeanAbsoluteError(), jit_compile=True)

    return model


strategy = tf.distribute.MirroredStrategy(
    cross_device_ops=tf.distribute.NcclAllReduce())

with strategy.scope():
    model = makeModel()

model.fit(....
```


### Relevant log output

```shell
cat /proc/{pid of the process}/status


Name:   python3
Umask:  0022
State:  S (sleeping)
Tgid:   134252
Ngid:   0
Pid:    134252
PPid:   69587
TracerPid:      0
Uid:    0       0       0       0
Gid:    0       0       0       0
FDSize: 512
Groups: 0
NStgid: 134252
NSpid:  134252
NSpgid: 134252
NSsid:  69587
VmPeak: 249801300 kB
VmSize: 249801300 kB
VmLck:         0 kB
VmPin:         0 kB
VmHWM:  37140600 kB
VmRSS:  37140600 kB
RssAnon:        33552768 kB
RssFile:         1614936 kB
RssShmem:        1972896 kB
VmData: 95053716 kB
VmStk:       160 kB
VmExe:      2784 kB
VmLib:   2600056 kB
VmPTE:    194828 kB
VmSwap:        0 kB
HugetlbPages:          0 kB
CoreDumping:    0
THP_enabled:    1
Threads:        29850
SigQ:   0/256322
SigPnd: 0000000000000000
ShdPnd: 0000000000000000
SigBlk: 0000000000000000
SigIgn: 0000000001001000
SigCgt: 0000000180000002
CapInh: 0000000000000000
CapPrm: 0000003fffffffff
CapEff: 0000003fffffffff
CapBnd: 0000003fffffffff
CapAmb: 0000000000000000
NoNewPrivs:     0
Seccomp:        0
Speculation_Store_Bypass:       thread vulnerable
Cpus_allowed:   ffff
Cpus_allowed_list:      0-15
Mems_allowed:   00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000001
Mems_allowed_list:      0
voluntary_ctxt_switches:        19571
nonvoluntary_ctxt_switches:     1302
```
</details>"
56751,tf.image.decode_image() fails for BMP with additional header-information,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9.1

### Custom Code

No

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I have a Dataset of bitmap-images with an additional header containing several metadata about the image. The file structure, though, complies to the standard and is readabla by any program, that can read BMP. (Unfortunately, I'm not allowed to share details on that file.)

Using tf 2.3.1 I could read those images with tf.io.read_file() and then tf.image.decode_image() without a problem. Another project using tf 2.9.1 fails with ""InvalidArgumentError: Input size should match (header_size + ..."" (https://github.com/tensorflow/tensorflow/blob/eb8425f115e5a93274f709cdfaf254798f9aa4c7/tensorflow/core/kernels/image/decode_image_op.cc#L607)

I get the same differing size (293056) for channels=0 and channels=3, while the bitmap is grayscale and has only one channel (setting it to 1 raises an immetiate error https://github.com/tensorflow/tensorflow/blob/eb8425f115e5a93274f709cdfaf254798f9aa4c7/tensorflow/core/kernels/image/decode_image_op.cc#L533). 

Inside the image file, the biBitCount is set to 8, which should be evaluated to 1 img_channels internally (see https://github.com/tensorflow/tensorflow/blob/eb8425f115e5a93274f709cdfaf254798f9aa4c7/tensorflow/core/kernels/image/decode_image_op.cc#L572 ). 

As far as I could dig into ist, this commit https://github.com/tensorflow/tensorflow/commit/0859ec0386ffa55739cbe831f38942c53027c12f added the check for differing sizes, which got into my way.

As I've understood, this check was introduced to fail for corrupted (too small) files - so I'd suggest, to change it to require size_diff >= 0 which would allow additional information inside the image, but prevent corrupted files.
```


### Standalone code to reproduce the issue

```shell
Unfortunately, I can't share those modified BMPs - so I can't share anything reproducible. Sorry for that.
```


### Relevant log output

_No response_</details>"
56750,docs: Google Colab button at the top of this page,"> docs: To follow this tutorial, run the notebook in Google Colab by clicking the button at the top of this page.
https://github.com/tensorflow/docs/blob/master/site/en/tutorials/quickstart/beginner.ipynb

Feature request: please add a screenshot or describe in more detail for first time users where amongst many the Google Colab button is located and how to find it. Apparently none of the buttons is named ""Google Colab"". Thank you!"
56749,tensorflow_ranking.keras.losses.ListMLELoss() gives -inf ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Fedpea 36

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
In 'tensorflow_ranking' (i.e., in short tfr) package, 
tfr.keras.losses.ListMLELoss() produces 'inf' result for float numbers with large decimals since TF converts them to 0. 
I spent 2 days to find the reason; in the ListMLELoss() source code, line 1304 
{sums = tf.math.log(sums) - sorted_logits}
when sums is a floating number with large decimals, it is counted as zero and the result would be inf. 
We can change the aforementioned line as follows to have numerical stability:
{sums = tf.math.log(sums + tf.keras.backend.epsilon()) - sorted_logits}
Also, tfr.keras.metrics.NDCGMetric() suffers from similar problem as well but for large numbers (implementation of Normalized Discounted Cumulative Gain is so complicated in TF.tfr); sklearn gives the correct answer and has clear implementation, while TF gives Nan.!!
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf 
import tensorflow_ranking as tfr
from sklearn.metrics import ndcg_score

y_true = [np.arange(0,1000, dtype='float32')]
y_pred = [np.arange(0,1000, dtype='float32')]

loss = tfr.keras.losses.ListMLELoss()
print('loss: ', loss(y_true, y_pred).numpy() )

ndcg = tfr.keras.metrics.NDCGMetric()
print('ndcg by tensorflow: ',ndcg(y_true, y_pred))

print('ndcg by skilear:', ndcg_score(y_true, y_pred))
```


### Relevant log output

```shell
2022-07-13 16:00:41.562605: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2022-07-13 16:00:41.562634: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: XX
2022-07-13 16:00:41.562640: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: XX
2022-07-13 16:00:41.562688: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 510.68.2
2022-07-13 16:00:41.562705: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 510.68.2
2022-07-13 16:00:41.562710: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 510.68.2
2022-07-13 16:00:41.562899: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
loss:  -inf
ndcg by tensorflow:  tf.Tensor(nan, shape=(), dtype=float32)
ndcg by skilear: 0.9999999999999998
```
</details>"
56748,"some numpy operation(FFT,FFT2d,RFFT,RFFT2d) failed when building a model ","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.2

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.0/8.0

### GPU model and memory

NVIDIA A6000 /48G

### Current Behaviour?

```shell
I want to use API ""tf.signal.rfft2d"" to create a model, and when I set the shape of the input tensor as (None,None,3), i.e., dynamic shape, tf.signal.rfft2d produce something abnormal. The shape of the output of tf.signal.rfft2d will be [None,None,None,None]. This won't happen when the shape of the input tensor is a constant, e.g.,(1024,1024,3), but this constrains the flexibility of the model. I've tried to use tf.keras.layers.Lambda and tf.raw_ops.RFFT2d but all failed. I also spotted that even if the model can successfully be complied with the input shape as [None,None,3], it failed to produce real value when accepting real data. I'm wondering whether it's because tf.signal.rfft2d only accept tensor or numpy object whose shape has a real value instead of None. Who has met this problem before?
```


### Standalone code to reproduce the issue

```shell
this is my code:
def ResBLock_do_fft_bench(inputs,filters):
    axis = 1
    x = K.permute_dimensions(inputs,(0,3,1,2))
    x = tf.keras.layers.Lambda(lambda v:tf.raw_ops.RFFT2D(input=v,fft_length=(tf.shape(v)[2],tf.shape(v)[3]//2+1)))(x)
    y_imag = tf.compat.v1.imag(x)
    y_real = tf.compat.v1.real(x)
    y_f = tf.concat([y_real, y_imag], axis=axis)
    y_f = tf.keras.layers.Permute((2,3,1))(y_f)
    y = BasicConv_do(y_f,filters*2,kernel_size=1,stride=1,relu=True)
    y = BasicConv_do(y,filters*2,kernel_size=1,stride=1,relu=False)
    y = tf.keras.layers.Permute((3,1,2))(y)
    y_real, y_imag = tf.split(y, 2, axis=axis)
    y = tf.complex(y_real, y_imag)
    y = tf.keras.layers.Lambda(lambda v:tf.raw_ops.IRFFT2D(input=v,fft_length=(-1,-1)))(y)
    y = tf.keras.layers.Permute((2,3,1))(y)
    res = BasicConv_do(inputs,filters,kernel_size=3,stride=1,relu=True)
    res = BasicConv_do(res,filters,kernel_size=3,stride=1,relu=False)
    return res + inputs + y
def FFT_Block():
    input = tf.keras.layers.Input(shape = [None,None,32],name='blur_image')
    out = ResBLock_do_fft_bench(input,32)
    model = tf.keras.models.Model(inputs=input,outputs=out,name='test')
    return model
```


### Relevant log output

```shell
the shape of the x before fft is:
(None, 32, None, None)
after fft:
(None, 32, None, None).

When the model takes a real value,this happens:
Traceback (most recent call last):
  File ""D:\Anaconda\envs\AIM\lib\site-packages\tensorflow\python\ops\gen_spectral_ops.py"", line 906, in irfft2d
    tld.op_callbacks, input, fft_length, ""Treal"", Treal)
tensorflow.python.eager.core._FallbackException: This function does not handle the case of the path where all inputs are not already EagerTensors.


if I change tf.raw_ops.RFFT2D to tf.signal.rfft2d, there will be another bug:
the shape of the x before fft:
(None, 32, None, None)
after fft:
(None, None, None, None)
```
</details>"
56747,TensorflowLite model can't run on Hexagon DSP device. Failed to apply delegate: Failed: Failed to execute graph.. ERROR: Node number 557 (TfLiteHexagonDelegate) failed to invoke.,"I convert the tfmodel to tflite, and when inference with python, it can run success. 
But in DSP, using the Hexagon C API (https://www.tensorflow.org/lite/performance/hexagon_delegate), in cpu run normal, but in DSP i get the below error. 
The hexagon_delegate so I used is https://storage.cloud.google.com/download.tensorflow.org/tflite/hexagon_nn_skel_v1.20.0.1.run
model only  include 2 conv and 1 conformer layers
Log
hexagon/ops/src/op_split.c:87:uneven split: 1 / 2
hexagon/src/execute.c:167:execute() failed on node id=181 err=-1
hexagon/src/interface.c:1297:fail in execute_inner()

----------------
ERROR: Failed: Failed to execute graph..
ERROR: Node number 139 (TfLiteHexagonDelegate) failed to invoke.

model can find here.
[encoder_6.zip](https://github.com/tensorflow/tensorflow/files/9098521/encoder_6.zip)

python code as this, and is running in tf2.4.0, the dsp's tf is 2.4.0 too.
import tensorflow as tf
import sys
import os
import numpy as np
tfmodel=sys.argv[1]#'temp.tflite'
wav_path='../../asr/BAC009S0764W0121.wav'
interpreter = tf.lite.Interpreter(model_path=tfmodel)
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
input_wav = np.random.rand(8000).astype(np.float32)
interpreter.set_tensor(input_details[0]['index'], input_wav)
interpreter.invoke()
output_data = interpreter.get_tensor(output_details[0]['index']).astype(np.float32)





"
56746,[`tf.data` - `from_generator()` and `.interleave()`] - Funky Behavior,"We are trying to combine `N_DATASETS_TO_INTERLEAVE` `from_generator()` Datasets using dataset Interleaving. And it seems that it's absolutely not working.

For some reason, it looks like the print is happening only once. Which could be due to some form of autograph running, but we tried everything we could to stop that, but nothing I could think of works.

You will say, it's just a `print()` what's the problem here. The problem is that if you execute any python code in this `from_generator()` it seems to be executed only once.

Google Colab: https://colab.research.google.com/drive/1WFvSvU8CCc1A1bfVbUb6ck_EzvE9o3sS?usp=sharing

```python
import tensorflow as tf

N_DATASETS_TO_INTERLEAVE = 10

@tf.autograph.experimental.do_not_convert
def hello(idx):
  for j in range(idx):
    yield f""IDX: {idx}""
    

@tf.autograph.experimental.do_not_convert
def interleave_fn(_):
    print(""[INFO] Calling Interleave Fn"")         # THIS LINE SHOULD BE PRINTED `N_DATASETS_TO_INTERLEAVE` times. Only appears once.
    return tf.data.Dataset.from_generator(
        hello, args=(_,), output_types=tf.string
    )


ds = tf.data.Dataset.range(N_DATASETS_TO_INTERLEAVE).interleave(
    interleave_fn
)

options = tf.data.Options()
options.experimental_optimization.apply_default_optimizations = False
ds = ds.with_options(options)


@tf.autograph.experimental.do_not_convert
def get_dataset(_ds):
    for x in iter(_ds):
        yield x


for x in get_dataset(ds):
  print(x)
```


Output:
```bash
[INFO] Calling Interleave Fn
WARNING:tensorflow:From <ipython-input-1-af809f9fe8c1>:15: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_types is deprecated and will be removed in a future version.
Instructions for updating:
Use output_signature instead
tf.Tensor(b'IDX: 1', shape=(), dtype=string)
tf.Tensor(b'IDX: 2', shape=(), dtype=string)
tf.Tensor(b'IDX: 2', shape=(), dtype=string)
tf.Tensor(b'IDX: 3', shape=(), dtype=string)
tf.Tensor(b'IDX: 3', shape=(), dtype=string)
tf.Tensor(b'IDX: 4', shape=(), dtype=string)
tf.Tensor(b'IDX: 3', shape=(), dtype=string)
tf.Tensor(b'IDX: 4', shape=(), dtype=string)
tf.Tensor(b'IDX: 4', shape=(), dtype=string)
tf.Tensor(b'IDX: 5', shape=(), dtype=string)
tf.Tensor(b'IDX: 4', shape=(), dtype=string)
tf.Tensor(b'IDX: 5', shape=(), dtype=string)
tf.Tensor(b'IDX: 5', shape=(), dtype=string)
tf.Tensor(b'IDX: 6', shape=(), dtype=string)
tf.Tensor(b'IDX: 5', shape=(), dtype=string)
tf.Tensor(b'IDX: 6', shape=(), dtype=string)
tf.Tensor(b'IDX: 5', shape=(), dtype=string)
tf.Tensor(b'IDX: 6', shape=(), dtype=string)
tf.Tensor(b'IDX: 6', shape=(), dtype=string)
tf.Tensor(b'IDX: 7', shape=(), dtype=string)
tf.Tensor(b'IDX: 6', shape=(), dtype=string)
tf.Tensor(b'IDX: 7', shape=(), dtype=string)
tf.Tensor(b'IDX: 6', shape=(), dtype=string)
tf.Tensor(b'IDX: 7', shape=(), dtype=string)
tf.Tensor(b'IDX: 7', shape=(), dtype=string)
tf.Tensor(b'IDX: 8', shape=(), dtype=string)
tf.Tensor(b'IDX: 7', shape=(), dtype=string)
tf.Tensor(b'IDX: 8', shape=(), dtype=string)
tf.Tensor(b'IDX: 7', shape=(), dtype=string)
tf.Tensor(b'IDX: 8', shape=(), dtype=string)
tf.Tensor(b'IDX: 7', shape=(), dtype=string)
tf.Tensor(b'IDX: 8', shape=(), dtype=string)
tf.Tensor(b'IDX: 8', shape=(), dtype=string)
tf.Tensor(b'IDX: 9', shape=(), dtype=string)
tf.Tensor(b'IDX: 8', shape=(), dtype=string)
tf.Tensor(b'IDX: 9', shape=(), dtype=string)
tf.Tensor(b'IDX: 8', shape=(), dtype=string)
tf.Tensor(b'IDX: 9', shape=(), dtype=string)
tf.Tensor(b'IDX: 8', shape=(), dtype=string)
tf.Tensor(b'IDX: 9', shape=(), dtype=string)
tf.Tensor(b'IDX: 9', shape=(), dtype=string)
tf.Tensor(b'IDX: 9', shape=(), dtype=string)
tf.Tensor(b'IDX: 9', shape=(), dtype=string)
tf.Tensor(b'IDX: 9', shape=(), dtype=string)
tf.Tensor(b'IDX: 9', shape=(), dtype=string)
```

Why this matter ?

We are trying to read and interleave multiple files that have a funky format. No native TF OPs can help to do that.
We need to use a generator because the files are massive (dataset 100GB+), we can't load the whole file at once.
We need to use interleaving because we have multiple files to interleave together.

Annnnd ... As you can see ... The python part of this code is only executed once.

We would expect to see (`[INFO] Calling Interleave Fn` printed `N_DATASETS_TO_INTERLEAVE` times).
```python
[INFO] Calling Interleave Fn
[INFO] Calling Interleave Fn
[INFO] Calling Interleave Fn
...
[INFO] Calling Interleave Fn
[INFO] Calling Interleave Fn
[INFO] Calling Interleave Fn
```

My hunch is that `tf.data.from_generator()` does not allow python generator. Only pure tensorflow generators. Any idea how to maybe combine `tf.py_function` in a generator fashion ?

@reedwm in case you have any idea ;) "
56743,Building `tflite` for `manylinux`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf 2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

NA

### Python version

3.8, 3.9, 3.10

### Bazel version

5.0.0

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am trying to build a `tflite` wheel on GitHub Actions (`ubuntu-latest`) with `bash tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh`. How does one request a `manylinux` build? Currently the build completes but produces a `linux_x86_64` suffixed wheel.
```


### Standalone code to reproduce the issue

```shell
Run `bash tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh` on Ubuntu 20.04 on GitHub Actions.
```


### Relevant log output

_No response_</details>"
56742,"Different TensorFlow Keras Code with same logic, tend to show very different behaviour.","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

binary

### Tensorflow Version

v2.9.0-18-gd8ce9f9c301 2.9.1

### Custom Code

No

### OS Platform and Distribution

Windows 11 64 Bit

### Mobile device

_No response_

### Python version

3.9.5

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11

### GPU model and memory

_No response_

### Current Behaviour?


I firstly apologize because despite days of pain, I'm still not very clear if its a bug or something which is keeping me from it. I'm not much experienced with me and really need someone experienced to guide me. Any clarification about status of bug or problem from my side is very much appreciated. 

So, I've two different codes for a DC GAN, which is basically a Generative Adverbial Neural Network using Cov Nets, despite similar model architecture and similar training steps, my code fails to converge in the way and speed of the second code. 

My Notebook (First version): https://colab.research.google.com/drive/12bMfjZgld6ETzPhPsz7HzoWHdL8056yG?usp=sharing
GFG Version (Second Version):  https://www.geeksforgeeks.org/deep-convolutional-gan-with-keras/

1. Results with my code vs the GFG Version of the Code.

GFG Version of Code:
  [1]: https://i.stack.imgur.com/k2UVN.png
*(GFG Version of the Code, after 1st Epoch and 128 batch size)*

My version of Code:
  [2]: https://i.stack.imgur.com/ciaAS.png

*(My Version of the Code, after training 1000 batch size and 60 epochs in total)*

As you can see, my model fails severely to converge to the desired result and for some reason, I notice mode collapse or something which doesn't lets my model converge to the desired results.

I'm posting the code of my training sequence as well as a bit of explaination of what it does in the following section:

```shell
def training_sequence(batch_size=1000, epochs=60):
    for epoch in range(epochs):
        print(""Epoch: "" + str(epoch + 1) + "" of "" + str(epochs))
        #Training Discriminator:

        GAN.layers[0].trainable = False
        GAN.layers[1].trainable = True

        #1 for real samples and 0 for fake samples
        fake_generated_examples = generator(generate_random_sequence(size = int(batch_size / 2)))
        zeros = tf.zeros((int(batch_size / 2), 1, 1, 1))

        #train discriminator
        discriminator.fit(fake_generated_examples, zeros)
        ones = tf.ones((int(batch_size / 2), 1, 1, 1))
        real_examples = x_train[int(batch_size / 2)*(epoch) : int(batch_size / 2)*(epoch + 1)].reshape(int(int(batch_size / 2)*(epoch + 1) - int(batch_size / 2)*(epoch)), 28, 28, 1)

        #train discriminator
        discriminator.fit(real_examples, ones)

        #Training Generator
        GAN.layers[0].trainable = True
        GAN.layers[1].trainable = False

        #train generator
        GAN.fit(generate_random_sequence(size = int(batch_size / 2)), ones)
```

There are two models (I'll post the summary just a bit after their introduction) - 
 1. Discriminator: Takes in the input of `(28, 28, 1)` and outputs: `[[[[1]]]]` for real samples (the ones from the dataset) and `[[[[0]]]]` for fake ones (which are generated from Generators.
 2. GAN: It is a Sequence of Generator and Discriminator models. It takes in a random value of `(1, 100)` and returns the output from the discriminator. 

`generate_random_sequences(size)` is just a function to generate a number of random 100 numbers of `size` rows and `(1,100)` columns which are fed into the generator.

`int(batch_size / 2)*(epoch)` used in the code is just used to select the first `batch_size` instances from the dataset in each epoch loop and keep selecting the next batch size and so on, so there is no repetition of selection of a data from dataset during each epoch and they remain unique.

Here's my model architecture:
  [3]: https://i.stack.imgur.com/2XeyJ.png

As you can see it's very similar to GFG Version of Models.

The training logic is similar in both cases, we train discriminator and then, GAN by locking discriminator by discriminator.trainable = False and training Generator this way. Despite training steps being similar for both models and same architecture, my model fails severely to converge as like the first one. I suppose its a problem with me, but even if its the case of bug, any guide or help would be very appreciated.

I'd refrain from using Gradient Tape from Tensorflow and stick to Keras for a while for simplicity.
```


### Standalone code to reproduce the issue

```shell
My Notebook: https://colab.research.google.com/drive/12bMfjZgld6ETzPhPsz7HzoWHdL8056yG?usp=sharing
GFG Version:  https://www.geeksforgeeks.org/deep-convolutional-gan-with-keras/
```


### Relevant log output

```shell
[1]: GFG Version
[2]: My version
  [1]: https://i.stack.imgur.com/k2UVN.png
  [2]: https://i.stack.imgur.com/ciaAS.png
```
</details>"
56740,_ragged_tensor_to_tensor_grad() fails in eager mode under some circumstances,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.6

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Training some models that use `RaggedTensor` with Keras works fine if the model is compiled with `run_eagerly=False`, but when `run_eagerly=True`, training fails with the following traceback:

```
...
/usr/lib/python3/dist-packages/keras/engine/training.py:1184: in fit
    tmp_logs = self.train_function(iterator)
/usr/lib/python3/dist-packages/keras/engine/training.py:853: in train_function
    return step_function(self, iterator)
/usr/lib/python3/dist-packages/keras/engine/training.py:842: in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))
/usr/lib/python3/dist-packages/tensorflow/python/distribute/distribute_lib.py:1286: in run
    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
/usr/lib/python3/dist-packages/tensorflow/python/distribute/distribute_lib.py:2849: in call_for_each_replica
    return self._call_for_each_replica(fn, args, kwargs)
/usr/lib/python3/dist-packages/tensorflow/python/distribute/distribute_lib.py:3632: in _call_for_each_replica
    return fn(*args, **kwargs)
/usr/lib/python3/dist-packages/tensorflow/python/autograph/impl/api.py:597: in wrapper
    return func(*args, **kwargs)
/usr/lib/python3/dist-packages/keras/engine/training.py:835: in run_step
    outputs = model.train_step(data)
/usr/lib/python3/dist-packages/keras/engine/training.py:791: in train_step
    self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
/usr/lib/python3/dist-packages/keras/optimizer_v2/optimizer_v2.py:521: in minimize
    loss, var_list=var_list, grad_loss=grad_loss, tape=tape)
/usr/lib/python3/dist-packages/keras/optimizer_v2/optimizer_v2.py:572: in _compute_gradients
    grads_and_vars = self._get_gradients(tape, loss, var_list, grad_loss)
/usr/lib/python3/dist-packages/keras/optimizer_v2/optimizer_v2.py:454: in _get_gradients
    grads = tape.gradient(loss, var_list, grad_loss)
/usr/lib/python3/dist-packages/tensorflow/python/eager/backprop.py:1090: in gradient
    unconnected_gradients=unconnected_gradients)
/usr/lib/python3/dist-packages/tensorflow/python/eager/imperative_grad.py:77: in imperative_grad
    compat.as_str(unconnected_gradients.value))
/usr/lib/python3/dist-packages/tensorflow/python/eager/backprop.py:159: in _gradient_function
    return grad_fn(mock_op, *out_grads)
/usr/lib/python3/dist-packages/tensorflow/python/ops/ragged/ragged_conversion_ops.py:83: in _ragged_tensor_to_tensor_grad
    row_partition_tensors=row_partition_tensors)
/usr/lib/python3/dist-packages/tensorflow/python/ops/gen_ragged_conversion_ops.py:289: in ragged_tensor_to_tensor
    _ops.raise_from_not_ok_status(e, name)
/usr/lib/python3/dist-packages/tensorflow/python/framework/ops.py:6941: in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
```

I have found that this is most likely due to an error at https://github.com/tensorflow/tensorflow/blob/7730bb302615aee24bbad653dcf7f7698d2dae5d/tensorflow/python/ops/ragged/ragged_conversion_ops.py#L66: in that line, there is a comparison with a bytestring `b""FIRST_DIM_SIZE""`, but in eager mode `row_partition_types` contain `str`s.

I have also found that the following snippet prevents the bug from happening:

```python
def monkeypatch_tensorflow_to_work_around_eager_mode_bug():
    """"""Monkey-patch Tensorflow to work around an eager mode bug. The bug is, apparently, that in
    https://github.com/tensorflow/tensorflow/blob/7730bb302615aee24bbad653dcf7f7698d2dae5d/tensorflow/python/ops/ragged/ragged_conversion_ops.py#L66
    there is a comparison with a bytestring b""FIRST_DIM_SIZE"", while in eager mode it's a plain str.
    """"""
    from tensorflow.python.eager import backprop
    original_MockOp = backprop._MockOp

    class PatchedMockOp(original_MockOp):
        def get_attr(self, attr):
            result = super().get_attr(attr)
            if attr == 'row_partition_types':
                assert isinstance(result, list)
                assert all(isinstance(item, str) for item in result)
                return [item.encode('ascii') for item in result]
            else:
                return result

    backprop._MockOp = PatchedMockOp
```


### Standalone code to reproduce the issue

```
(I have observed this bug with a large model, and stripping it down to a minimal reproducing example is painstakingly hard.)
```


### Relevant log output

_No response_</details>"
56739,Int-8 converter scales static model parameters incorrectly,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.9.1 (tensorflow-estimator 2.9.0)

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

https://colab.research.google.com/drive/1HN3tZCEeK79YPFT85udXf7FybBo6-PYf

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

After INT-8 conversion, the models parameters are scaled incorrectly (and unnecessarily) and model performance drops significantly.
The colab notebook compares a default conversion and a 8-bit quantized conversion and shows that the output changes and difference between classes can no longer be made. The same code produces the same issues with the upper mentioned TF installation and in the default colab notebook configuration.

The model consists of a simple custom layer that compares the input bounding box with an internal hardcoded bounding box. Only minimum, maximum and subtract operations are used. 
When converting the Keras sequential model to a default .tflite model (not quantized), the output is exactly as expected. Converting it with 8-bit quantization the model parameters are scaled in a way that clear, non-borderline cases are predicted incorrectly. 

I understand that scaling might be necessary for large models with diverse inputs, but in this case I would like have full control over what changes are made to the model during conversion, since it is a very simple model and the possible input is very predictable and narrowly defined.
Inputs and model parameters both range from 0-100, for which no scaling is needed and no overflow could possibly happen. This is (I hope) reflected in my representative dataset, which seems to be a necessary input for 8-bit integer quantization.

I do not mind the exact numerical output of the model, but the output of the model should be different for overlapping and non-overlapping bounding boxes, which is only given for the default converted tflite model.

To sum up, how can I convert or write a model for 8bit integers, in a way that I have direct control over the model parameters.
"
56737,Bug in `tf.autodiff.ForwardAccumulator` for `tf.signal.mdct`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Gradient in forward mode is inconsistent with that in backward mode for API `tf.signal.mdct`
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
signals = tf.constant([3.3425903], dtype=tf.float32)
frame_length = 49152
with tf.GradientTape(persistent=True,) as g:
      g.watch(signals)
      res = tf.reduce_sum(tf.signal.mdct(signals, frame_length, window_fn=None, pad_end=True, norm=None))
      print(res)
print(""gradient in reverse mode: "", g.gradient(res, signals))

with tf.autodiff.ForwardAccumulator(signals,tf.constant([1.],shape=[1],dtype=tf.float32)) as acc:
        res = tf.reduce_sum(tf.signal.mdct(signals, frame_length, window_fn=None, pad_end=True, norm=None, ))
        print(res)
print(""gradient in forward mode: "", acc.jvp(res))
```


### Relevant log output

```shell
tf.Tensor(11.523682, shape=(), dtype=float32)
gradient in reverse mode:  tf.Tensor([1.0006392], shape=(1,), dtype=float32)
tf.Tensor(11.523682, shape=(), dtype=float32)
gradient in forward mode:  tf.Tensor(3.4477844, shape=(), dtype=float32)
```
</details>"
56736,DepthwiseConv2D does not support bfloat16,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

cuda 11.7, cudnn 8.4

### GPU model and memory

NVIDIA A100

### Current Behaviour?

```shell
Depthwise convolution does not support bfloat16 data type, throws exception:


tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'DepthwiseConv2dNativeBackpropInput' used by {{node gradient_tape/deeplab/wall_bottom_horizontal_conv1/depthwise/DepthwiseConv2dNativeBackpropInput
}} with these attrs: [padding=""VALID"", dilations=[1, 1, 1, 1], T=DT_BFLOAT16, data_format=""NCHW"", strides=[1, 1, 1, 1], explicit_paddings=[]]

Registered devices: [CPU, GPU]
Registered kernels:
  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_BFLOAT16, DT_HALF]
  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_BFLOAT16, DT_HALF]
  device='GPU'; label='cudnn_grouped_convolution'; T in [DT_DOUBLE]
  device='GPU'; label='cudnn_grouped_convolution'; T in [DT_FLOAT]
  device='GPU'; label='cudnn_grouped_convolution'; T in [DT_HALF]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_HALF]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_HALF]
```
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.keras import mixed_precision

#this makes tf.keras.layers to use bfloat16 as default data format
mixed_precision.set_global_policy(""mixed_bfloat16"")

#simple model definition
input_tensor = tf.keras.Input((32,32,32))

#this fails with bfloat
output_tensor = tf.keras.layers.DepthwiseConv2D(kernel_size=(1,1))(input_tensor)

#this works with bfloat
#output_tensor = tf.keras.layers.Dense(32)(input_tensor)

#this fails with different error message
#output_tensor = tf.keras.layers.Conv2D(filters=32, kernel_size=(1,1))(input_tensor)

model = tf.keras.Model([input_tensor], [output_tensor])
model.compile(loss=""mse"")

#pass stupid data through model
random_input = tf.random.uniform((1,32,32,32), dtype=tf.bfloat16)
random_output = tf.random.uniform((1,32,32,32), dtype=tf.bfloat16)
model.fit(random_input, random_output)
```


### Relevant log output

_No response_</details>"
56735,CMAKE has no key to disable the build of python wrapper when building TF lite from source using Xcode or Android Studio,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

TF2.9

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18 or MacOS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
CMAKE has no key to disable the build of python wrapper when building TF lite from source using Xcode or Android Studio. 

current cmake in tensorflow/tensorflow/lite with the a non optional python wrapper, which triggers errors when using tensorflow as a submodule in an Xcode or Android studio project 

# Python interpreter wrapper.
add_library(_pywrap_tensorflow_interpreter_wrapper SHARED EXCLUDE_FROM_ALL
  ${TFLITE_SOURCE_DIR}/python/interpreter_wrapper/interpreter_wrapper.cc
  ${TFLITE_SOURCE_DIR}/python/interpreter_wrapper/interpreter_wrapper_pybind11.cc
  ${TFLITE_SOURCE_DIR}/python/interpreter_wrapper/numpy.cc
  ${TFLITE_SOURCE_DIR}/python/interpreter_wrapper/python_error_reporter.cc
  ${TFLITE_SOURCE_DIR}/python/interpreter_wrapper/python_utils.cc
)

# To remove ""lib"" prefix.
set_target_properties(_pywrap_tensorflow_interpreter_wrapper PROPERTIES PREFIX """")

target_include_directories(_pywrap_tensorflow_interpreter_wrapper
  PUBLIC
    ${TFLITE_INCLUDE_DIRS}
)

target_link_libraries(_pywrap_tensorflow_interpreter_wrapper
  tensorflow-lite
  ${CMAKE_DL_LIBS}
)
target_compile_options(_pywrap_tensorflow_interpreter_wrapper
  PUBLIC ${TFLITE_TARGET_PUBLIC_OPTIONS}
  PRIVATE ${TFLITE_TARGET_PRIVATE_OPTIONS}
)

I would expect a cmake key to enable or disable the python wrapper 
e.g.

if(TFLITE_ENABLE_pywrap)
...
endif()
```


### Standalone code to reproduce the issue

```shell
CMAKE has no key to disable the build of python wrapper when building TF lite from source using Xcode or Android Studio.
```


### Relevant log output

```shell
cannot find Python.h
```
</details>"
56734,"How to flat the model about TFLite and remove the ""while"" loop?","### 1. System information

- Linux:
- TensorFlow 2.9:

### 2. Code

The code of converter:

modelparh = r""model.h5""
model = tf.keras.models.load_model(modelparh, custom_objects = {'...'})

def representative_dataset_gen():
    for audio in x_train[0:500,:]:
        yield [audio.reshape(-1,x_train.shape[1],x_train.shape[2]).astype(""float32"")]

converter = TFLiteConverter.from_keras_model(model)

converter.optimizations = [tf.contrib.lite.Optimize.DEFAULT]

converter.representative_dataset = representative_dataset_gen

converter.target_spec.supported_ops=[tf.contrib.lite.OpsSet.TFLITE_BUILTINS,tf.contrib.lite.OpsSet.SELECT_TF_OPS,tf.contrib.lite.OpsSet.TFLITE_BUILTINS_INT8]

converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8
tflite_model = converter.convert()

savepath = r""model.tflite""
open(savepath, ""wb"").write(tflite_model)

-my issus is:
i can converter and quantification the model.but when i check the file of tflite by netron,find the model can't flat and have some ""while"" loop in gru graph. the graph like this:

while
1=0
2=0"
56733,flatc error when building from TF2.9.1 from source using cmake + ninja Android studio native library,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

CMAKE > 3.18  and  ninja 1.8.2

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Flatc/flatbuffer related Errors while building TF2.9.1 as a sub-module (native .so or .a library) in an Android Studio project using CMAKE.
The issue is reported in google's flatbuffer repo as well here, https://github.com/google/flatbuffers/issues/7297
I double checked and fund that flatbuffer is V2.0.6 but the issue still exists although the flatbuffer ticket is closed
```


### Standalone code to reproduce the issue

```shell
any simple Android studio project where TF2.9.1 is built using CMAKE.
a sample of a cmakelist.txt in the cpp folder is below


cmake_minimum_required(VERSION 3.18)
project(tfl_cpp_native)
# I have tried both of the below
set(CMAKE_CXX_STANDARD 14)
#set(CMAKE_CXX_STANDARD 17)

# add your source files to the projects e.g. detector.cc
# include_directories(...)


# add the tensorflow as submdoule, this will use the cmakelist.txt in tensorflow/tensorflow/lite
# to build the TF library and will automatically pull/download all dependencies including
# flattbuffer. from git 

add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/tensorflow/tensorflow/lite)

# Find OpenCV (optional but just in case)
set( OpenCV_DIR ""${CMAKE_CURRENT_SOURCE_DIR}/OpenCV-android-sdk/sdk/native/jni"" )
find_package(OpenCV REQUIRED)


# some log and ndk etc. lib 
find_library(z log  jnigraphics android GLESv3 EGL vulkan thread)

target_link_libraries(tfl_cpp_native
        tensorflow-lite #=> expected TF lite lib to be built
        ${OpenCV_LIBS}
        z
        log
        jnigraphics
        android
        GLESv3
        EGL
        vulkan
        )

add_library(tfl_cpp_native
        SHARED
        native-lib.cpp
        detector.cc
        ${SOURCE_FILES}
        )
```


### Relevant log output

```shell
File ""scripts/generate_code.py"", line 148, in <module>
    flatc(
  File ""scripts/generate_code.py"", line 82, in flatc
    result = subprocess.run(cmd, cwd=str(cwd), check=True)
  File ""/usr/lib/python3.8/subprocess.py"", line 493, in run
    with Popen(*popenargs, **kwargs) as process:
  File ""/usr/lib/python3.8/subprocess.py"", line 858, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File ""/usr/lib/python3.8/subprocess.py"", line 1704, in _execute_child
    raise child_exception_type(errno_num, err_msg, err_filename)
OSError: [Errno 8] Exec format error: '/path/to/build/flatbuffers-build/flatc'
flatbuffers-build/CMakeFiles/flatc.dir/build.make:560: recipe for target 'flatbuffers-build/flatc' failed
```
</details>"
56732,openCL delegate generates '0' and 'inf' values with reduce_sum,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.8, tf 2.9

### Custom Code

No

### OS Platform and Distribution

Android

### Mobile device

tested on Snapdragon 888, 865

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
My models with 'tf.math.reduce_sum' ('Sum' in the tflite version) nodes generate wrong results with the openCL delegate. My experiments show that the output of a 'Sum' node in a tflite model contains lots of 'zeros' and 'inf' values when we use openCL delegate. This issue does not happen with other delegates like XNNPACK.
```


### Standalone code to reproduce the issue

```shell
Here is a very simple model with a reduce_sum node in the structure:

x0 =  Input(shape=(23, 512))
x1 = tf.math.reduce_sum(x0, axis=-1)

model = Model([x0], [x1], name='test')
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.save('reduce_sum.h5')

The tflite version of this model generates zeros and inf values with the openCL delegate.
```


### Relevant log output

_No response_</details>"
56731,"TF fails to build on CPU when compiled with ""-march=skylake-avx512"" flag","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9.12

### Bazel version

5.1.1

### GCC/Compiler version

9.3.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Starting from 74059203de9c21ca26b27ea7609d49b3c87f2c19 commit, TF fails to build when compiled with `--copt=-march=skylake=avx512` flag on Cascade Lake (CLX) CPU systems or higher. The compilation succeeds when this flag is not passed.
```


### Standalone code to reproduce the issue

```shell
Command-line to trigger the build failure (on 74059203de9c21ca26b27ea7609d49b3c87f2c19 or newer commit):
bazel build --copt=""-O3"" --copt=-march=skylake-avx512 -c opt //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:250:5: error: ambiguous template instantiation for 'struct EigenForTFLite::internal::gemm_pack_rhs<float, long int, EigenForTFLite::internal::TensorContractionSubMapper<float, long int, 0, EigenForTFLite::TensorEvaluator<const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorImagePatchOp<-1, -1, const EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, long int>, 16> > >, EigenForTFLite::ThreadPoolDevice>, std::array<long int, 1>, std::array<long int, 1>, 16, true, false, 0, EigenForTFLite::MakePointer>, 8, 0, false, false>' 
  250 |     RhsPacker()(*rhsBlock, data_mapper, depth, cols); 
      |     ^~~~~~~~~~~
```
</details>"
56729,Difference in accuracy between EDGE TFLite Model and Quant TFLite Model.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

2.4.1

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

3.7.2

### GCC/Compiler version

8.3.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
There is difference in accuracy between edge tpu model and quant model.
Is that expected ?
```


### Standalone code to reproduce the issue

```shell
pi@gpucoder-tpu-1:~/Tensorflow/tensorflow/lite/examples/label_image $ ./a.out --tflite_model mobilenet_v2_1.0_224_quant.tflite --labels mobilenet_v1_1.0_224/labels.txt --image testdata/grace_hopper.bmp 

INFO: Loaded model mobilenet_v2_1.0_224_quant.tflite
INFO: resolved reporter
This Model is not Edge Tpu compiled 
INFO: invoked
INFO: average time: 43.3273 ms 
INFO: 0.984314: 4 4:tiger shark, Galeocerdo cuvieri
INFO: 0.0117647: 3 3:great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias
INFO: 0.00392157: 2 2:goldfish, Carassius auratus
INFO: 0.00392157: 1 1:tench, Tinca tinca


pi@gpucoder-tpu-1:~/Tensorflow/tensorflow/lite/examples/label_image $ ./a.out --tflite_model mobilenet_v2_1.0_224_quant_edgetpu.tflite --labels mobilenet_v1_1.0_224/labels.txt --image testdata/grace_hopper.bmp 
INFO: Loaded model mobilenet_v2_1.0_224_quant_edgetpu.tflite
INFO: resolved reporter
This Model is Edge Tpu compiled
INFO: invoked
INFO: average time: 4.4319 ms 
INFO: 0.937255: 4 4:tiger shark, Galeocerdo cuvieri
INFO: 0.054902: 2 2:goldfish, Carassius auratus
INFO: 0.00784314: 3 3:great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias
```


### Relevant log output

```shell
Accuracy is not same
```
</details>"
56727,RuntimeError: tensorflow/lite/kernels/read_variable.cc:74 variable_tensor->type != output->type (INT8 != FLOAT32)Node number 10 (READ_VARIABLE) failed to invoke.,"Hello TF Lite Cummunity! 
I'm doing tests with TF Lite converter on a machine, that, as far as I know, is equipped as follow:
### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic
- TensorFlow installation (pip package or built from source): installed via pip 
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.8.2

### 2. Code

Starting from a TensorFlow model, I perform the following steps:

1. I perform a pruning operation on the model (everything works fine);
2. I convert the pruned model into a TensorFlow Lite model using tf.lite.TFLiteConverter.from_saved_model (everything works fine);
3. I evaluate such a pruned lite model using tf.lite.Interpreter(model_content=tflite_model) and subsequently interpreter.invoke() (everything works fine);
4. I then try to perform post-training full integer quantization and evaluate the model again; post-training full integer quantization seems to work fine because the model is properly converted. However, when I try to perform model evaluation, I got the following error ""RuntimeError: tensorflow/lite/kernels/read_variable.cc:74 variable_tensor->type != output->type (INT8 != FLOAT32)Node number 10 (READ_VARIABLE) failed to invoke."", but I do not understand what could be the reason, and where I go wrong.

Here is the code:

```
interpreter.resize_tensor_input(input_details[""index""], (2947, 128,9))
  interpreter.resize_tensor_input(output_details[""index""], (2947, 6))
  interpreter.allocate_tensors()
  input_details = interpreter.get_input_details()[0]
  print(""INPUT DETAILS POST RESIZE"")
  print(input_details)
  output_details = interpreter.get_output_details()[0]
  print(""OUTPUT DETAILS POST RISIZE"")
  print(output_details)

  testX.dtype
  testX_numpy = np.array(testX, dtype=np.int8)
  testX_numpy.dtype

  # If required, quantize the input layer (from float to integer)
  input_scale, input_zero_point = input_details[""quantization""]
  print(""quantize the input layer (from float to integer)\n"")
  x_test_ = testX_numpy / input_scale + input_zero_point
  x_test_ = x_test_.astype(input_details[""dtype""])

  interpreter.set_tensor(input_details['index'], x_test_)
  interpreter.invoke()
  tflite_model_predictions = interpreter.get_tensor(output_details['index'])
```

### 3. Failure after conversion
As I've said, the model is properly converted, but during inference time interpreter.invoke() generates the following error:

RuntimeError: tensorflow/lite/kernels/read_variable.cc:74 variable_tensor->type != output->type (INT8 != FLOAT32)Node number 10 (READ_VARIABLE) failed to invoke.


### 5. (optional) Any other info / logs
Here is the result of prints from the above code:

INPUT DETAILS POST RESIZE
{'name': 'serving_default_lstm_input:0', 'index': 0, 'shape': array([2947,  128,    9], dtype=int32), 'shape_signature': array([ -1, 128,   9], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.027058085426688194, -1), 'quantization_parameters': {'scales': array([0.02705809], dtype=float32), 'zero_points': array([-1], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}
OUTPUT DETAILS POST RISIZE
{'name': 'StatefulPartitionedCall:0', 'index': 138, 'shape': array([2947,    6], dtype=int32), 'shape_signature': array([-1,  6], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.00390625, -128), 'quantization_parameters': {'scales': array([0.00390625], dtype=float32), 'zero_points': array([-128], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}

Here is the traceback:
RuntimeError                              Traceback (most recent call last)
in <module>()
--> 275   y_pred_qptfLite = predict_tflite_fullyint_quant(quantized_tflite_model)


in predict_tflite_fullyint_quant(tflite_model)
     59 
     60   interpreter.set_tensor(input_details['index'], x_test_)
---> 61   interpreter.invoke()
     62   tflite_model_predictions = interpreter.get_tensor(output_details['index'])
     63 


[/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/interpreter.py](https://localhost:8080/#) in invoke(self)
    914     """"""
    915     self._ensure_safe()
--> 916     self._interpreter.Invoke()
    917 
    918   def reset_all_variables(self):

RuntimeError: tensorflow/lite/kernels/read_variable.cc:74 variable_tensor->type != output->type (INT8 != FLOAT32)Node number 10 (READ_VARIABLE) failed to invoke.

I've Googled around to see if anyone had exactly the same kind of issue but did not find anything."
56725,'No toolchains found' when building libtensorflow on FreeBSD 13,"Trying to build libtensorflow on FreeBSD 13 I was getting this toolchain error:  
```
bazel build --config opt --toolchain_resolution_debug=@bazel_tools//tools/cpp:toolchain_type //tensorflow/tools/lib_package:libtensorflow
INFO: ToolchainResolution: Type @bazel_tools//tools/cpp:toolchain_type: target platform @local_execution_config_platform//:platform: Rejected toolchain @local_config_cc//:cc-compiler-freebsd; mismatching values: freebsd
INFO: ToolchainResolution: Type @bazel_tools//tools/cpp:toolchain_type: target platform @local_execution_config_platform//:platform: No toolchains found.
```
Looks like something happens to the execution platform here:
https://github.com/tensorflow/tensorflow/blob/9584c52ecfbbc33b3be48903cdadd41b4a2bb083/third_party/remote_config/remote_platform_configure.bzl#L3-L12
I'm not sure exactly how this all works, but adding a branch for freebsd seems to have fixed the issue:
```
         elif os.startswith(""freebsd""):
             platform = ""freebsd""
```

After that the toolchain is selected properly but the library still won't build.  I had to add the following lines to .bazelrc:
```
build:freebsd --cxxopt=-std=c++14
build:freebsd --host_cxxopt=-std=c++14
```
Now the code will compile, but it won't link. The linker will fail with `ld: error: undefined symbol: environ`.
After googling a bit, this seems to be a known issue when building shared objects referencing `environ` ([link](https://bugs.freebsd.org/bugzilla/show_bug.cgi?id=265008))
In the end I just removed `-z defs` in tensorflow/BUILD and after that libtensorflow built successfully. The mini hello_tf.c example from the official site worked fine too.

---
FreeBSD 13.1-RELEASE
clang 13.0.0
bazel 5.2.0
python 3.9.13
tensorflow-2.9.1
"
56724,raw_rnn equivalent in tensorflow 2.0,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I notice that TF2.0 does not include a raw_rnn api (https://www.tensorflow.org/api_docs/python/tf/compat/v1/nn/raw_rnn) that TF1.0 has. May I know how to achieve the behaviour of being able to control when to start and finish reading the sequence, in TF2.0, while generating variable-length sequence? 

In the text generation example (https://www.tensorflow.org/text/tutorials/text_generation) provided in the docs, to do a one-step prediction, the whole lstm time-steps has to be predicted and we take just the last prediction.

However, in raw_rnn, it is possible to sample one time-step at a time, and ending when a finished condition is true. 

Is there a difference between these two approach to generate variable-length sequence and train the lstm? How do I get the equivalent of raw_rnn functionality in TF2.0?

Thank you.
```


### Standalone code to reproduce the issue

```shell

https://www.tensorflow.org/text/tutorials/text_generation
```


### Relevant log output

_No response_</details>"
56723,__array_interface__ support?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

2.6.0

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Hi. Over at https://github.com/python-pillow/Pillow, our Image class has the `__array_interface__` attribute, to support converting Pillow images to NumPy. See https://numpy.org/doc/stable/reference/arrays.interface.html#object.__array_interface__

A recent discussion has revealed that TensorFlow's `reshape` method (and I have to imagine other methods as well) accepts an object that provides an `__array__` method, but not an object with an `__array_interface__` attribute.

My question - is there any interest from TensorFlow in supporting objects with `__array_interface__`?

### Standalone code to reproduce the issue

Here is code that fails with the latest version of Pillow.

```python
from PIL import Image
import tensorflow as tf

im = Image.new(""L"", (1, 1))
tf.reshape(im, (1, 1))
```

However, if my assertion that TensorFlow doesn't accept `__array_interface__` is at all in doubt, let me know and I'll put together a better example.
```


### Relevant log output

_No response_</details>"
56722,Has a conda-forge pull request been submitted for TF 2.9?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

binary

### Tensorflow Version

2.9

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
In another discussion on May 17 (https://github.com/keras-team/keras/issues/15964), I was informed that a bug fix that I need is available in Tensorflow 2.9.  I have been waiting for TF 2.9 to become available in the conda-forge repository.

The process for adding packages to Anaconda repositories is here: 

https://conda-forge.org/docs/maintainer/adding_pkgs.html

""To submit a package to the conda-forge channel, add its recipe and license to the staged-recipes repository and create a pull request. Once the pull request is merged, the package becomes available on the conda-forge channel.""

So it looks like someone on the Tensorflow team is responsible for getting the process started.  Has anyone done so?  Anaconda's package conflict resolution has eliminated the many dreaded compiler and CUDA incompatibility issues with Tensorflow that I experienced in previous years.

Thanks!
```


### Standalone code to reproduce the issue

```shell
No code.  

Just visit https://anaconda.org/search?q=tensorflow, and notice that the default Anaconda channel offers TF 2.6, while the cutting-edge conda-forge channel offers TF 2.8.  I can understand if the default repository continues to offer TF 2.6 if it is more stable, but I hope that 2.9 will be added to conda-forge.
```


### Relevant log output

_No response_</details>"
56720,RuntimeError: tensorflow/lite/kernels/concatenation.cc:158 t->dims->data[d] != t0->dims->data[d] (0 != 1)Node number 60 (CONCATENATION) failed to prepare.Failed to apply the default TensorFlow Lite delegate indexed at 0.,"### 1. System information

- OS Platform and Distribution: Ubuntu 18.04.5 
- TensorFlow installation: 2.8.0 (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:
Model can be accessed through link here: https://drive.google.com/file/d/1ucgrKsFkoWiEaZCtk_DmF_fIMqumhH4Z/view?usp=sharing
``` Python
interpreter = tf.lite.Interpreter(model_path=""./temp_model.tflite"")
interpreter.allocate_tensors()
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:
Conversion is successful. But when doing the inference, something wrong. The model can't be loaded.

RuntimeError: tensorflow/lite/kernels/concatenation.cc:158 t->dims->data[d] != t0->dims->data[d] (0 != 1)Node number 60 (CONCATENATION) failed to prepare.Failed to apply the default TensorFlow Lite delegate indexed at 0.


### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
56719,Issues when updating the code from tf1 to tf2,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Source

source

### Tensorflow Version

tf2

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.7.13

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
'keras.api._v1.keras.layers' has no attribute 'conv2d'. I'm trying to change my code from tf1 to tf2, I changed all tf.layers to tf.keras.layers since in tf2 keras is the High level API and I had this error message
```


### Standalone code to reproduce the issue

```shell
Traceback (most recent call last):
  File ""baseCNN_dbl_norm.py"", line 103, in <module>
    model = Model()
  File ""baseCNN_dbl_norm.py"", line 46, in __init__
    self.logits = self.build(self.images, self.n_classes)
  File ""baseCNN_dbl_norm.py"", line 57, in build
    conv1 = tf.keras.layers.conv2d(inputs=features,
  File ""C:\Users\LENOVO\.conda\envs\Dchecker1.0\lib\site-packages\tensorflow\python\util\module_wrapper.py"", line 232, in _getattr
    attr = getattr(self._tfmw_wrapped_module, name)
AttributeError: module 'keras.api._v1.keras.layers' has no attribute 'conv2d'
```


### Relevant log output

_No response_</details>"
56718,"'keras.api._v1.keras.layers' has no attribute 'conv2d' how can I fix this issue please? (I'm trying to change my code from tf1 to tf2, I changed all tf.layers to tf.keras.layers since in tf2 keras is the High level API and I had this error message, how can I fix it? ","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
56717,fault_tolerance_test fails on some systems,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.7.1

### Custom Code

No

### OS Platform and Distribution

Linux RHEL 7

### Mobile device

_No response_

### Python version

3.9.6

### Bazel version

3.7.2

### GCC/Compiler version

11.2

### CUDA/cuDNN version

11.4.1 / cuDNN 8.2.2.26

### GPU model and memory

8 * Tesla A100

### Current Behaviour?

```shell
When running the test suite during/after the build the test `//tensorflow/python/data/experimental/kernel_tests/service:fault_tolerance_test` fails on this system, while it seemingly passes on another system.
It seems to be very flaky or dependent on the number of CPUs/GPUs. This system has 96 cores and 8 GPUs.
Output is something like
> //tensorflow/python/data/experimental/kernel_tests/service:fault_tolerance_test (2/20 cached) FAILED in 18 out of 20 in 13.5s
```


### Standalone code to reproduce the issue

```shell
Run the following build command: `bazel test --config=noaws --config=nogcp --config=nohdfs --compilation_mode=opt --config=opt --subcommands --verbose_failures --jobs=1 --distinct_host_configuration=false --test_output=errors --build_tests_only --local_test_jobs=1 -- //tensorflow/python/data/experimental/kernel_tests/service:fault_tolerance_test`
```


### Relevant log output

```shell
======================================================================
FAIL: testAddWorkerMidJob_test_mode_eager_tfapiversion_2 (__main__.FaultToleranceTest)
FaultToleranceTest.testAddWorkerMidJob_test_mode_eager_tfapiversion_2
testAddWorkerMidJob_test_mode_eager_tfapiversion_2(mode='eager', tf_api_version=2)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/tmp/software/TensorFlow/2.7.1-foss-2021b-CUDA-11.4.1/lib/python3.9/site-packages/absl/testing/parameterized.py"", line 314, in bound_param_test
    return test_method(self, **testcase_params)
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.7.1/foss-2021b-CUDA-11.4.1/tmpBD23m_-bazel-tf/69a307561ec5a7cdace7b5c5a8971ea2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/fault_tolerance_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 366, in decorated
    execute_test_method()
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.7.1/foss-2021b-CUDA-11.4.1/tmpBD23m_-bazel-tf/69a307561ec5a7cdace7b5c5a8971ea2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/fault_tolerance_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 349, in execute_test_method
    test_method(**kwargs_to_pass)
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.7.1/foss-2021b-CUDA-11.4.1/tmpBD23m_-bazel-tf/69a307561ec5a7cdace7b5c5a8971ea2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/fault_tolerance_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/fault_tolerance_test.py"", line 220, in testAddWorkerMidJob
    self.assertCountEqual(2 * list(range(num_elements)), results)
AssertionError: Element counts were not equal:
```
</details>"
56713,Numpy 1.23.0 causes unit test failures,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

CentOS 7

### Mobile device

n/a

### Python version

3.8.10

### Bazel version

5.1.1

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

```shell
The latest release of numpy ie 1.23.0 is now causing unit test failures. See https://github.com/tensorflow/tensorflow/actions/runs/2547650056
```


### Standalone code to reproduce the issue

```shell
python -m pip install numpy==1.23.0
bazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=1 --test_output=all --cache_test_results=no --noremote_accept_cached --config=nonccl --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-requires-gpu --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-requires-gpu --verbose_failures --build_tests_only -- //tensorflow/python/kernel_tests/control_flow:scan_ops_test_cpu //tensorflow/python/kernel_tests/array_ops:pad_op_test_cpu //tensorflow/python/kernel_tests/array_ops:concat_op_test_cpu //tensorflow/python/kernel_tests/array_ops:array_ops_test_cpu //tensorflow/python/kernel_tests/array_ops:pad_op_test_gpu //tensorflow/python/kernel_tests/array_ops:concat_op_test_gpu //tensorflow/python/kernel_tests/array_ops:split_op_test_cpu //tensorflow/python/kernel_tests/array_ops:array_ops_test_gpu //tensorflow/python/kernel_tests/array_ops:split_op_test_gpu //tensorflow/python/kernel_tests/array_ops:slice_op_test_gpu //tensorflow/python/kernel_tests/array_ops:slice_op_test_cpu //tensorflow/python/kernel_tests/control_flow:scan_ops_test_gpu
```


### Relevant log output

```shell
Sample faliure
======================================================================
ERROR: test1D (__main__.CumprodTest)
CumprodTest.test1D
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1625, in decorated
    return f(self, *args, **kwargs)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 251, in test1D
    self._compareAll(x, axis)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 219, in _compareAll
    self._compare(x, axis, exclusive, reverse)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 210, in _compare
    np_out = handle_options(np.cumprod, x, axis, exclusive, reverse)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 47, in handle_options
    x = numpy_reverse(x, axis)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 37, in numpy_reverse
    return x[ix]
IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices

======================================================================
ERROR: test2D (__main__.CumprodTest)
CumprodTest.test2D
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1625, in decorated
    return f(self, *args, **kwargs)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 258, in test2D
    self._compareAll(x, axis)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 219, in _compareAll
    self._compare(x, axis, exclusive, reverse)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 210, in _compare
    np_out = handle_options(np.cumprod, x, axis, exclusive, reverse)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 47, in handle_options
    x = numpy_reverse(x, axis)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 37, in numpy_reverse
    return x[ix]
IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices

======================================================================
ERROR: test3D (__main__.CumprodTest)
CumprodTest.test3D
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1625, in decorated
    return f(self, *args, **kwargs)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 265, in test3D
    self._compareAll(x, axis)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 219, in _compareAll
    self._compare(x, axis, exclusive, reverse)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 210, in _compare
    np_out = handle_options(np.cumprod, x, axis, exclusive, reverse)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 47, in handle_options
    x = numpy_reverse(x, axis)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 37, in numpy_reverse
    return x[ix]
IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices

======================================================================
ERROR: test6D (__main__.CumprodTest)
CumprodTest.test6D
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1625, in decorated
    return f(self, *args, **kwargs)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 272, in test6D
    self._compareAll(x, axis)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 219, in _compareAll
    self._compare(x, axis, exclusive, reverse)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 210, in _compare
    np_out = handle_options(np.cumprod, x, axis, exclusive, reverse)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 47, in handle_options
    x = numpy_reverse(x, axis)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 37, in numpy_reverse
    return x[ix]
IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices

======================================================================
ERROR: testEmpty (__main__.CumprodTest)
CumprodTest.testEmpty
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1625, in decorated
    return f(self, *args, **kwargs)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 226, in testEmpty
    self._compareAll(x, axis)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 219, in _compareAll
    self._compare(x, axis, exclusive, reverse)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 210, in _compare
    np_out = handle_options(np.cumprod, x, axis, exclusive, reverse)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 47, in handle_options
    x = numpy_reverse(x, axis)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 37, in numpy_reverse
    return x[ix]
IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices

======================================================================
ERROR: testNaN (__main__.CumprodTest)
CumprodTest.testNaN
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1625, in decorated
    return f(self, *args, **kwargs)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 244, in testNaN
    self._compareAll(x, axis)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 219, in _compareAll
    self._compare(x, axis, exclusive, reverse)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 210, in _compare
    np_out = handle_options(np.cumprod, x, axis, exclusive, reverse)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 47, in handle_options
    x = numpy_reverse(x, axis)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 37, in numpy_reverse
    return x[ix]
IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices

======================================================================
ERROR: test1D (__main__.CumsumTest)
CumsumTest.test1D
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1625, in decorated
    return f(self, *args, **kwargs)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 118, in test1D
    self._compareAll(x, axis)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 86, in _compareAll
    self._compare(x, axis, exclusive, reverse)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 77, in _compare
    np_out = handle_options(np.cumsum, x, axis, exclusive, reverse)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 47, in handle_options
    x = numpy_reverse(x, axis)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 37, in numpy_reverse
    return x[ix]
IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices

======================================================================
ERROR: test2D (__main__.CumsumTest)
CumsumTest.test2D
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1625, in decorated
    return f(self, *args, **kwargs)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 125, in test2D
    self._compareAll(x, axis)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 86, in _compareAll
    self._compare(x, axis, exclusive, reverse)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 77, in _compare
    np_out = handle_options(np.cumsum, x, axis, exclusive, reverse)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 47, in handle_options
    x = numpy_reverse(x, axis)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 37, in numpy_reverse
    return x[ix]
IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices

======================================================================
ERROR: test3D (__main__.CumsumTest)
CumsumTest.test3D
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1625, in decorated
    return f(self, *args, **kwargs)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 132, in test3D
    self._compareAll(x, axis)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 86, in _compareAll
    self._compare(x, axis, exclusive, reverse)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 77, in _compare
    np_out = handle_options(np.cumsum, x, axis, exclusive, reverse)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 47, in handle_options
    x = numpy_reverse(x, axis)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 37, in numpy_reverse
    return x[ix]
IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices

======================================================================
ERROR: test6D (__main__.CumsumTest)
CumsumTest.test6D
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1625, in decorated
    return f(self, *args, **kwargs)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 139, in test6D
    self._compareAll(x, axis)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 86, in _compareAll
    self._compare(x, axis, exclusive, reverse)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 77, in _compare
    np_out = handle_options(np.cumsum, x, axis, exclusive, reverse)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 47, in handle_options
    x = numpy_reverse(x, axis)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 37, in numpy_reverse
    return x[ix]
IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices

======================================================================
ERROR: testEmpty (__main__.CumsumTest)
CumsumTest.testEmpty
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1625, in decorated
    return f(self, *args, **kwargs)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 93, in testEmpty
    self._compareAll(x, axis)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 86, in _compareAll
    self._compare(x, axis, exclusive, reverse)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 77, in _compare
    np_out = handle_options(np.cumsum, x, axis, exclusive, reverse)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 47, in handle_options
    x = numpy_reverse(x, axis)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 37, in numpy_reverse
    return x[ix]
IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices

======================================================================
ERROR: testLarge (__main__.CumsumTest)
CumsumTest.testLarge
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1625, in decorated
    return f(self, *args, **kwargs)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 2157, in decorated
    return func(self, *args, **kwargs)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 146, in testLarge
    self._compareAll(x, 0)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 86, in _compareAll
    self._compare(x, axis, exclusive, reverse)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 77, in _compare
    np_out = handle_options(np.cumsum, x, axis, exclusive, reverse)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 47, in handle_options
    x = numpy_reverse(x, axis)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 37, in numpy_reverse
    return x[ix]
IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices

======================================================================
ERROR: testNaN (__main__.CumsumTest)
CumsumTest.testNaN
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1625, in decorated
    return f(self, *args, **kwargs)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 111, in testNaN
    self._compareAll(x, axis)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 86, in _compareAll
    self._compare(x, axis, exclusive, reverse)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 77, in _compare
    np_out = handle_options(np.cumsum, x, axis, exclusive, reverse)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 47, in handle_options
    x = numpy_reverse(x, axis)
  File ""/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/control_flow/scan_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/control_flow/scan_ops_test.py"", line 37, in numpy_reverse
    return x[ix]
IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices

----------------------------------------------------------------------
Ran 29 tests in 4.799s

FAILED (errors=13, skipped=2)
================================================================================
```
</details>"
56712,TensorFlowLiteTaskVision and TensorFlowLiteSwift duplicate symbols for architecture arm64,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

MacOS 12.4

### Mobile device

iPhone 7

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am trying to install pod TensorFlowLiteVisionSwift and TensorFlowLiteTaskVision, It shows duplication of symbols issue arm64,
```


### Standalone code to reproduce the issue

```shell
Pod file:
https://colab.research.google.com/drive/1SpSz3CNIg8Bld_OxLpTShsZKkWUvWGVF#scrollTo=vrV9Zd6Wz0Jw&line=12&uniqifier=1

I am installing pods on below project:
https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/ios
```


### Relevant log output

```shell
duplicate symbol '_TfLiteDelegateCreate' in:
    /Users/mac-obs-46/Library/Developer/Xcode/DerivedData/ObjectDetection-bdfuzkzbfmaultaotsgbguetimeo/Build/Products/Debug-iphoneos/XCFrameworkIntermediates/TensorFlowLiteC/Core/TensorFlowLiteC.framework/TensorFlowLiteC
    /Users/mac-obs-46/Desktop/frameworkEfficiendted/examples-master-2/lite/examples/object_detection/ios/Pods/TensorFlowLiteTaskVision/Frameworks/TensorFlowLiteTaskVisionC.framework/TensorFlowLiteTaskVisionC
duplicate symbol '_TfLiteFloatArrayCreate' in:
    /Users/mac-obs-46/Library/Developer/Xcode/DerivedData/ObjectDetection-bdfuzkzbfmaultaotsgbguetimeo/Build/Products/Debug-iphoneos/XCFrameworkIntermediates/TensorFlowLiteC/Core/TensorFlowLiteC.framework/TensorFlowLiteC
    /Users/mac-obs-46/Desktop/frameworkEfficiendted/examples-master-2/lite/examples/object_detection/ios/Pods/TensorFlowLiteTaskVision/Frameworks/TensorFlowLiteTaskVisionC.framework/TensorFlowLiteTaskVisionC
duplicate symbol '_TfLiteFloatArrayFree' in:
    /Users/mac-obs-46/Library/Developer/Xcode/DerivedData/ObjectDetection-bdfuzkzbfmaultaotsgbguetimeo/Build/Products/Debug-iphoneos/XCFrameworkIntermediates/TensorFlowLiteC/Core/TensorFlowLiteC.framework/TensorFlowLiteC
    /Users/mac-obs-46/Desktop/frameworkEfficiendted/examples-master-2/lite/examples/object_detection/ios/Pods/TensorFlowLiteTaskVision/Frameworks/TensorFlowLiteTaskVisionC.framework/TensorFlowLiteTaskVisionC
duplicate symbol '_TfLiteFloatArrayGetSizeInBytes' in:
    /Users/mac-obs-46/Library/Developer/Xcode/DerivedData/ObjectDetection-bdfuzkzbfmaultaotsgbguetimeo/Build/Products/Debug-iphoneos/XCFrameworkIntermediates/TensorFlowLiteC/Core/TensorFlowLiteC.framework/TensorFlowLiteC
    /Users/mac-obs-46/Desktop/frameworkEfficiendted/examples-master-2/lite/examples/object_detection/ios/Pods/TensorFlowLiteTaskVision/Frameworks/TensorFlowLiteTaskVisionC.framework/TensorFlowLiteTaskVisionC
duplicate symbol '_TfLiteIntArrayCopy' in:
    /Users/mac-obs-46/Library/Developer/Xcode/DerivedData/ObjectDetection-bdfuzkzbfmaultaotsgbguetimeo/Build/Products/Debug-iphoneos/XCFrameworkIntermediates/TensorFlowLiteC/Core/TensorFlowLiteC.framework/TensorFlowLiteC
    /Users/mac-obs-46/Desktop/frameworkEfficiendted/examples-master-2/lite/examples/object_detection/ios/Pods/TensorFlowLiteTaskVision/Frameworks/TensorFlowLiteTaskVisionC.framework/TensorFlowLiteTaskVisionC
duplicate symbol '_TfLiteIntArrayCreate' in:
    /Users/mac-obs-46/Library/Developer/Xcode/DerivedData/ObjectDetection-bdfuzkzbfmaultaotsgbguetimeo/Build/Products/Debug-iphoneos/XCFrameworkIntermediates/TensorFlowLiteC/Core/TensorFlowLiteC.framework/TensorFlowLiteC
    /Users/mac-obs-46/Desktop/frameworkEfficiendted/examples-master-2/lite/examples/object_detection/ios/Pods/TensorFlowLiteTaskVision/Frameworks/TensorFlowLiteTaskVisionC.framework/TensorFlowLiteTaskVisionC
duplicate symbol '_TfLiteIntArrayEqual' in:
    /Users/mac-obs-46/Library/Developer/Xcode/DerivedData/ObjectDetection-bdfuzkzbfmaultaotsgbguetimeo/Build/Products/Debug-iphoneos/XCFrameworkIntermediates/TensorFlowLiteC/Core/TensorFlowLiteC.framework/TensorFlowLiteC
    /Users/mac-obs-46/Desktop/frameworkEfficiendted/examples-master-2/lite/examples/object_detection/ios/Pods/TensorFlowLiteTaskVision/Frameworks/TensorFlowLiteTaskVisionC.framework/TensorFlowLiteTaskVisionC
duplicate symbol '_TfLiteIntArrayEqualsArray' in:
    /Users/mac-obs-46/Library/Developer/Xcode/DerivedData/ObjectDetection-bdfuzkzbfmaultaotsgbguetimeo/Build/Products/Debug-iphoneos/XCFrameworkIntermediates/TensorFlowLiteC/Core/TensorFlowLiteC.framework/TensorFlowLiteC
    /Users/mac-obs-46/Desktop/frameworkEfficiendted/examples-master-2/lite/examples/object_detection/ios/Pods/TensorFlowLiteTaskVision/Frameworks/TensorFlowLiteTaskVisionC.framework/TensorFlowLiteTaskVisionC
duplicate symbol '_TfLiteIntArrayFree' in:
    /Users/mac-obs-46/Library/Developer/Xcode/DerivedData/ObjectDetection-bdfuzkzbfmaultaotsgbguetimeo/Build/Products/Debug-iphoneos/XCFrameworkIntermediates/TensorFlowLiteC/Core/TensorFlowLiteC.framework/TensorFlowLiteC
    /Users/mac-obs-46/Desktop/frameworkEfficiendted/examples-master-2/lite/examples/object_detection/ios/Pods/TensorFlowLiteTaskVision/Frameworks/TensorFlowLiteTaskVisionC.framework/TensorFlowLiteTaskVisionC
duplicate symbol '_TfLiteIntArrayGetSizeInBytes' in:
    /Users/mac-obs-46/Library/Developer/Xcode/DerivedData/ObjectDetection-bdfuzkzbfmaultaotsgbguetimeo/Build/Products/Debug-iphoneos/XCFrameworkIntermediates/TensorFlowLiteC/Core/TensorFlowLiteC.framework/TensorFlowLiteC
    /Users/mac-obs-46/Desktop/frameworkEfficiendted/examples-master-2/lite/examples/object_detection/ios/Pods/TensorFlowLiteTaskVision/Frameworks/TensorFlowLiteTaskVisionC.framework/TensorFlowLiteTaskVisionC
duplicate symbol '_TfLiteQuantizationFree' in:
    /Users/mac-obs-46/Library/Developer/Xcode/DerivedData/ObjectDetection-bdfuzkzbfmaultaotsgbguetimeo/Build/Products/Debug-iphoneos/XCFrameworkIntermediates/TensorFlowLiteC/Core/TensorFlowLiteC.framework/TensorFlowLiteC
    /Users/mac-obs-46/Desktop/frameworkEfficiendted/examples-master-2/lite/examples/object_detection/ios/Pods/TensorFlowLiteTaskVision/Frameworks/TensorFlowLiteTaskVisionC.framework/TensorFlowLiteTaskVisionC
duplicate symbol '_TfLiteSparsityFree' in:
    /Users/mac-obs-46/Library/Developer/Xcode/DerivedData/ObjectDetection-bdfuzkzbfmaultaotsgbguetimeo/Build/Products/Debug-iphoneos/XCFrameworkIntermediates/TensorFlowLiteC/Core/TensorFlowLiteC.framework/TensorFlowLiteC
    /Users/mac-obs-46/Desktop/frameworkEfficiendted/examples-master-2/lite/examples/object_detection/ios/Pods/TensorFlowLiteTaskVision/Frameworks/TensorFlowLiteTaskVisionC.framework/TensorFlowLiteTaskVisionC
duplicate symbol '_TfLiteTensorCopy' in:
    /Users/mac-obs-46/Library/Developer/Xcode/DerivedData/ObjectDetection-bdfuzkzbfmaultaotsgbguetimeo/Build/Products/Debug-iphoneos/XCFrameworkIntermediates/TensorFlowLiteC/Core/TensorFlowLiteC.framework/TensorFlowLiteC
    /Users/mac-obs-46/Desktop/frameworkEfficiendted/examples-master-2/lite/examples/object_detection/ios/Pods/TensorFlowLiteTaskVision/Frameworks/TensorFlowLiteTaskVisionC.framework/TensorFlowLiteTaskVisionC
duplicate symbol '_TfLiteTensorDataFree' in:
    /Users/mac-obs-46/Library/Developer/Xcode/DerivedData/ObjectDetection-bdfuzkzbfmaultaotsgbguetimeo/Build/Products/Debug-iphoneos/XCFrameworkIntermediates/TensorFlowLiteC/Core/TensorFlowLiteC.framework/TensorFlowLiteC
    /Users/mac-obs-46/Desktop/frameworkEfficiendted/examples-master-2/lite/examples/object_detection/ios/Pods/TensorFlowLiteTaskVision/Frameworks/TensorFlowLiteTaskVisionC.framework/TensorFlowLiteTaskVisionC
duplicate symbol '_TfLiteTensorFree' in:
    /Users/mac-obs-46/Library/Developer/Xcode/DerivedData/ObjectDetection-bdfuzkzbfmaultaotsgbguetimeo/Build/Products/Debug-iphoneos/XCFrameworkIntermediates/TensorFlowLiteC/Core/TensorFlowLiteC.framework/TensorFlowLiteC
    /Users/mac-obs-46/Desktop/frameworkEfficiendted/examples-master-2/lite/examples/object_detection/ios/Pods/TensorFlowLiteTaskVision/Frameworks/TensorFlowLiteTaskVisionC.framework/TensorFlowLiteTaskVisionC
duplicate symbol '_TfLiteTensorRealloc' in:
    /Users/mac-obs-46/Library/Developer/Xcode/DerivedData/ObjectDetection-bdfuzkzbfmaultaotsgbguetimeo/Build/Products/Debug-iphoneos/XCFrameworkIntermediates/TensorFlowLiteC/Core/TensorFlowLiteC.framework/TensorFlowLiteC
    /Users/mac-obs-46/Desktop/frameworkEfficiendted/examples-master-2/lite/examples/object_detection/ios/Pods/TensorFlowLiteTaskVision/Frameworks/TensorFlowLiteTaskVisionC.framework/TensorFlowLiteTaskVisionC
duplicate symbol '_TfLiteTensorReset' in:
    /Users/mac-obs-46/Library/Developer/Xcode/DerivedData/ObjectDetection-bdfuzkzbfmaultaotsgbguetimeo/Build/Products/Debug-iphoneos/XCFrameworkIntermediates/TensorFlowLiteC/Core/TensorFlowLiteC.framework/TensorFlowLiteC
    /Users/mac-obs-46/Desktop/frameworkEfficiendted/examples-master-2/lite/examples/object_detection/ios/Pods/TensorFlowLiteTaskVision/Frameworks/TensorFlowLiteTaskVisionC.framework/TensorFlowLiteTaskVisionC
duplicate symbol '_TfLiteTypeGetName' in:
    /Users/mac-obs-46/Library/Developer/Xcode/DerivedData/ObjectDetection-bdfuzkzbfmaultaotsgbguetimeo/Build/Products/Debug-iphoneos/XCFrameworkIntermediates/TensorFlowLiteC/Core/TensorFlowLiteC.framework/TensorFlowLiteC
    /Users/mac-obs-46/Desktop/frameworkEfficiendted/examples-master-2/lite/examples/object_detection/ios/Pods/TensorFlowLiteTaskVision/Frameworks/TensorFlowLiteTaskVisionC.framework/TensorFlowLiteTaskVisionC
duplicate symbol '_TfLiteXNNPackDelegateCreate' in:
    /Users/mac-obs-46/Library/Developer/Xcode/DerivedData/ObjectDetection-bdfuzkzbfmaultaotsgbguetimeo/Build/Products/Debug-iphoneos/XCFrameworkIntermediates/TensorFlowLiteC/Core/TensorFlowLiteC.framework/TensorFlowLiteC
    /Users/mac-obs-46/Desktop/frameworkEfficiendted/examples-master-2/lite/examples/object_detection/ios/Pods/TensorFlowLiteTaskVision/Frameworks/TensorFlowLiteTaskVisionC.framework/TensorFlowLiteTaskVisionC
duplicate symbol '_TfLiteXNNPackDelegateDelete' in:
    /Users/mac-obs-46/Library/Developer/Xcode/DerivedData/ObjectDetection-bdfuzkzbfmaultaotsgbguetimeo/Build/Products/Debug-iphoneos/XCFrameworkIntermediates/TensorFlowLiteC/Core/TensorFlowLiteC.framework/TensorFlowLiteC
    /Users/mac-obs-46/Desktop/frameworkEfficiendted/examples-master-2/lite/examples/object_detection/ios/Pods/TensorFlowLiteTaskVision/Frameworks/TensorFlowLiteTaskVisionC.framework/TensorFlowLiteTaskVisionC
duplicate symbol '_TfLiteXNNPackDelegateGetThreadPool' in:
    /Users/mac-obs-46/Library/Developer/Xcode/DerivedData/ObjectDetection-bdfuzkzbfmaultaotsgbguetimeo/Build/Products/Debug-iphoneos/XCFrameworkIntermediates/TensorFlowLiteC/Core/TensorFlowLiteC.framework/TensorFlowLiteC
    /Users/mac-obs-46/Desktop/frameworkEfficiendted/examples-master-2/lite/examples/object_detection/ios/Pods/TensorFlowLiteTaskVision/Frameworks/TensorFlowLiteTaskVisionC.framework/TensorFlowLiteTaskVisionC
duplicate symbol '_TfLiteXNNPackDelegateOptionsDefault' in:
    /Users/mac-obs-46/Library/Developer/Xcode/DerivedData/ObjectDetection-bdfuzkzbfmaultaotsgbguetimeo/Build/Products/Debug-iphoneos/XCFrameworkIntermediates/TensorFlowLiteC/Core/TensorFlowLiteC.framework/TensorFlowLiteC
    /Users/mac-obs-46/Desktop/frameworkEfficiendted/examples-master-2/lite/examples/object_detection/ios/Pods/TensorFlowLiteTaskVision/Frameworks/TensorFlowLiteTaskVisionC.framework/TensorFlowLiteTaskVisionC
duplicate symbol '_TfLiteXNNPackDelegateWeightsCacheCreate' in:
    /Users/mac-obs-46/Library/Developer/Xcode/DerivedData/ObjectDetection-bdfuzkzbfmaultaotsgbguetimeo/Build/Products/Debug-iphoneos/XCFrameworkIntermediates/TensorFlowLiteC/Core/TensorFlowLiteC.framework/TensorFlowLiteC
    /Users/mac-obs-46/Desktop/frameworkEfficiendted/examples-master-2/lite/examples/object_detection/ios/Pods/TensorFlowLiteTaskVision/Frameworks/TensorFlowLiteTaskVisionC.framework/TensorFlowLiteTaskVisionC
duplicate symbol '_TfLiteXNNPackWeightsCacheDelete' in:
    /Users/mac-obs-46/Library/Developer/Xcode/DerivedData/ObjectDetection-bdfuzkzbfmaultaotsgbguetimeo/Build/Products/Debug-iphoneos/XCFrameworkIntermediates/TensorFlowLiteC/Core/TensorFlowLiteC.framework/TensorFlowLiteC
    /Users/mac-obs-46/Desktop/frameworkEfficiendted/examples-master-2/lite/examples/object_detection/ios/Pods/TensorFlowLiteTaskVision/Frameworks/TensorFlowLiteTaskVisionC.framework/TensorFlowLiteTaskVisionC
ld: 24 duplicate symbols for architecture arm64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
```
</details>"
56711,"A error occured when using dense_to_ragged_batch in TFRecordDataset, is't a bug?","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

binary

### Tensorflow Version

tf 2.9.0

### Custom Code

Yes

### OS Platform and Distribution

Windows 10 21H1

### Mobile device

_No response_

### Python version

3.9.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hi there!
  I was trying to concat var with diffient shapes into batches by using raggedTensor.
  I had preprocessed wav into mfcc and stored it in tfrecords.
  When I loaded them with TFRecordDataset, and tried using dense_to_ragged_batch to stack var into a raggedTensor.
  A error occured and It said ""Cannot batch tensors with different shapes in component 0.""
  I had tried using tf.data.Dataset to achieve the same thing without preprocessing,and also using dense_to_ragged_batch to stack batches.But it works fun.
Yours,
Soya.
I'm sorry for my poor English.And a mini code shows below.
```


### Standalone code to reproduce the issue

```shell
def parse_tfrecord(proto):
    features = {
        'label': tf.io.FixedLenFeature([], tf.string),
        'mfcc': tf.io.FixedLenFeature([], tf.string)
    }
    out = tf.io.parse_single_example(proto, features)
    mfcc = tf.io.parse_tensor(out['mfcc'], out_type='float32')
    label = out['label']
    return label, mfcc
def parse_tfrecord_data(data_raw):
    line, mfcc = parse_tfrecord(data_raw)
    return mfcc
def get_datasets():
    record_files = [os.path.join(train_record_dir, filename) for filename in os.listdir(train_record_dir)]
    dataset = tf.data.TFRecordDataset(record_files, num_parallel_reads=4).map(parse_tfrecord_data,
                                                                              num_parallel_calls=16)
    dataset = dataset.apply(tf.data.experimental.dense_to_ragged_batch(batch_size=global_batch_size))
    return dataset

if __name__ == '__main__':
    tf.data.experimental.enable_debug_mode()
    dataset = get_datasets()
    batch = next(iter(dataset))
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""C:\Users\a1313\PycharmProjects\ConformerRefactor\dataloader.py"", line 66, in <module>
    batch = next(iter(dataset))
  File ""C:\Users\a1313\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\data\ops\iterator_ops.py"", line 766, in __next__
    return self._next_internal()
  File ""C:\Users\a1313\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\data\ops\iterator_ops.py"", line 749, in _next_internal
    ret = gen_dataset_ops.iterator_get_next(
  File ""C:\Users\a1313\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\ops\gen_dataset_ops.py"", line 3043, in iterator_get_next
    _ops.raise_from_not_ok_status(e, name)
  File ""C:\Users\a1313\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\framework\ops.py"", line 7164, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot batch tensors with different shapes in component 0. First element had shape [429,128] and element 6 had shape [420,128]. [Op:IteratorGetNext]
```
</details>"
56710,"Greater(nextafter(x), x) returns False","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Greater(nextafter(x), x) returns False
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

input = tf.constant([0.0, 0.25, np.finfo(tf.float32.as_numpy_dtype).tiny], dtype=tf.float32)
nextafter = tf.math.nextafter(input, tf.dtypes.as_dtype(tf.float32).max)
greater = tf.math.greater(nextafter, input)

print(""input"", input)
print(""nextafter"", nextafter)
print(""greater"", greater)

assert all(greater)
```


### Relevant log output

```shell
input tf.Tensor([0.0000000e+00 2.5000000e-01 1.1754944e-38], shape=(3,), dtype=float32)
nextafter tf.Tensor([1.4012985e-45 2.5000003e-01 1.1754945e-38], shape=(3,), dtype=float32)
greater tf.Tensor([False  True  True], shape=(3,), dtype=bool)
collected 0 items / 1 errors                                                                                                                                                                                                 

=========================================================================================================== ERRORS ===========================================================================================================
_______________________________________________________________________________________________________ ERROR repro.py _______________________________________________________________________________________________________
repro.py:12: in <module>
    assert all(greater)
E   assert False
E    +  where False = all(<tf.Tensor: shape=(3,), dtype=bool, numpy=array([False,  True,  True])>)
================================================================================================== short test summary info ===================================================================================================
ERROR repro.py - assert False
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Interrupted: 1 errors during collection !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
================================================================================================== 1 error in 5.84 seconds ===================================================================================================
```
</details>"
56709,Problem using Metal delegate on iPhone 13 mini,"### System information

Code written:
```
TFLGpuDelegateOptions options;
options.wait_type = TFLGpuDelegateWaitType::TFLGpuDelegateWaitTypePassive;
options.enable_quantization = true;
TfLiteDelegate * gpu_delegate = TFLGpuDelegateCreate(&options);
interpreter->ModifyDelegateWithGraph(gpu_delegate);
```
- Using an iPhone 13 mini, on iOS 15.5
- Tensorflow lite is compiled from source, using 2.9.1
- Cross compiled from m1 mac using cmake, and bazel 5.0.0 for the required frameworks
- Compiled using apple-clang 13.1
- XCode version 13.4.1

### Describe the problem
There is a problem trying to use Metal delegate on this specific phone (or perhaps the whole iPhone 13 family, confirmed to be happening on mini and pro). Have tested on iPhone 12 and below, and there isn't a problem with that.

### Logs
Error printed by tflite. Also, returned with TfLiteStatus of 2
```
ERROR: TfLiteMetalDelegate Prepare: newLibraryWithSource: program_source:35:3: error: use of undeclared identifier 'simdgroup_matrix'; did you mean 'simdgroup_barrier'?
  simdgroup_matrix<FLT, 8, 8> dst_sp0_8_ch0_8(0.0f);
  ^~~~~~~~~~~~~~~~
  simdgroup_barrier
/System/Library/PrivateFrameworks/GPUCompiler.framework/Libraries/lib/clang/31001.518/include/metal/metal_compute:67:17: note: 'simdgroup_barrier' declared here
METAL_FUNC void simdgroup_barrier(mem_flags flags)
                ^
program_source:35:3: error: 'simdgroup_barrier' does not name a template but is followed by template arguments
  simdgroup_matrix<FLT, 8, 8> dst_sp0_8_ch0_8(0.0f);
  ^               ~~~~~~~~~~~
/System/Library/PrivateFrameworks/GPUCompiler.framework/Libraries/lib/clang/31001.518/include/metal/metal_compute:67:17: note: non-template declaration found by name lookup
METAL_FUNC void simdgroup_barrier(mem_flags flags)
                ^
program_source:36:3: error: use of undeclared identifier 'simdgroup_matrix'; did you mean 'simdgroup_barrier'?
  simdgroup_matrix<FLT, 8, 8> dst_sp0_8_ch8_16(0.0f);
  ^~~~~~~~~~~~~~~~
  simdgroup_barrier
/System/Library/PrivateFrameworks/GPUCompiler.framework/Libraries/lib/clang/31001.518/include/metal/metal_compute:67:17: note: 'simdgroup_barrier' declared here
METAL_FUNC void simdgroup_barrier(mem_flags flags)
                ^
program_source:36:3: error: 'simdgroup_barrier' does not name a template but is followed by template arguments
  simdgroup_matrix<FLT, 8, 8> dst_sp0_8_ch8_16(0.0f);
  ^               ~~~~~~~~~~~
/System/Library/PrivateFrameworks/GPUCompiler.framework/Libraries/lib/clang/31001.518/include/metal/metal_compute:67:17: note: non-template declaration found by name lookup
METAL_FUNC void simdgroup_barrier(mem_flags flags)
                ^
program_source:37:3: error: use of undeclared identifier 'simdgroup_matrix'; did you mean 'simdgroup_barrier'?
  simdgroup_matrix<FLT, 8, 8> dst_sp8_16_ch0_8(0.0f);
  ^~~~~~~~~~~~~~~~
  simdgroup_barrier
/System/Library/PrivateFrameworks/GPUCompiler.framework/Libraries/lib/clang/31001.518/include/metal/metal_compute:67:17: note: 'simdgroup_barrier' declared here
METAL_FUNC void simdgroup_barrier(mem_flags flags)
                ^
program_source:37:3: error: 'simdgroup_barrier' does not name a template but is followed by template arguments
  simdgroup_matrix<FLT, 8, 8> dst_sp8_16_ch0_8(0.0f);
  ^               ~~~~~~~~~~~
/System/Library/PrivateFrameworks/GPUCompiler.framework/Libraries/lib/clang/31001.518/include/metal/metal_compute:67:17: note: non-template declaration found by name lookup
METAL_FUNC void simdgroup_barrier(mem_flags flags)
                ^
program_source:38:3: error: use of undeclared identifier 'simdgroup_matrix'; did you mean 'simdgroup_barrier'?
  simdgroup_matrix<FLT, 8, 8> dst_sp8_16_ch8_16(0.0f);
  ^~~~~~~~~~~~~~~~
  simdgroup_barrier
/System/Library/PrivateFrameworks/GPUCompiler.framework/Libraries/lib/clang/31001.518/include/metal/metal_compute:67:17: note: 'simdgroup_barrier' declared here
METAL_FUNC void simdgroup_barrier(mem_flags flags)
                ^
program_source:38:3: error: 'simdgroup_barrier' does not name a template but is followed by template arguments
  simdgroup_matrix<FLT, 8, 8> dst_sp8_16_ch8_16(0.0f);
  ^               ~~~~~~~~~~~
/System/Library/PrivateFrameworks/GPUCompiler.framework/Libraries/lib/clang/31001.518/include/metal/metal_compute:67:17: note: non-template declaration found by name lookup
METAL_FUNC void simdgroup_barrier(mem_flags flags)
                ^
program_source:39:3: error: use of undeclared identifier 'simdgroup_matrix'; did you mean 'simdgroup_barrier'?
  simdgroup_matrix<FLT, 8, 8> dst_sp16_24_ch0_8(0.0f);
  ^~~~~~~~~~~~~~~~
  simdgroup_barrier
/System/Library/PrivateFrameworks/GPUCompiler.framework/Libraries/lib/clang/31001.518/include/metal/metal_compute:67:17: note: 'simdgroup_barrier' declared here
METAL_FUNC void simdgroup_barrier(mem_flags flags)
                ^
program_source:39:3: error: 'simdgroup_barrier' does not name a template but is followed by template arguments
  simdgroup_matrix<FLT, 8, 8> dst_sp16_24_ch0_8(0.0f);
  ^               ~~~~~~~~~~~
/System/Library/PrivateFrameworks/GPUCompiler.framework/Libraries/lib/clang/31001.518/include/metal/metal_compute:67:17: note: non-template declaration found by name lookup
METAL_FUNC void simdgroup_barrier(mem_flags flags)
                ^
program_source:40:3: error: use of undeclared identifier 'simdgroup_matrix'; did you mean 'simdgroup_barrier'?
  simdgroup_matrix<FLT, 8, 8> dst_sp16_24_ch8_16(0.0f);
  ^~~~~~~~~~~~~~~~
  simdgroup_barrier
/System/Library/PrivateFrameworks/GPUCompiler.framework/Libraries/lib/clang/31001.518/include/metal/metal_compute:67:17: note: 'simdgroup_barrier' declared here
METAL_FUNC void simdgroup_barrier(mem_flags flags)
                ^
program_source:40:3: error: 'simdgroup_barrier' does not name a template but is followed by template arguments
  simdgroup_matrix<FLT, 8, 8> dst_sp16_24_ch8_16(0.0f);
  ^               ~~~~~~~~~~~
/System/Library/PrivateFrameworks/GPUCompiler.framework/Libraries/lib/clang/31001.518/include/metal/metal_compute:67:17: note: non-template declaration found by name lookup
METAL_FUNC void simdgroup_barrier(mem_flags flags)
                ^
program_source:41:3: error: use of undeclared identifier 'simdgroup_matrix'; did you mean 'simdgroup_barrier'?
  simdgroup_matrix<FLT, 8, 8> dst_sp24_32_ch0_8(0.0f);
  ^~~~~~~~~~~~~~~~
  simdgroup_barrier
/System/Library/PrivateFrameworks/GPUCompiler.framework/Libraries/lib/clang/31001.518/include/metal/metal_compute:67:17: note: 'simdgroup_barrier' declared here
METAL_FUNC void simdgroup_barrier(mem_flags flags)
                ^
program_source:41:3: error: 'simdgroup_barrier' does not name a template but is followed by template arguments
  simdgroup_matrix<FLT, 8, 8> dst_sp24_32_ch0_8(0.0f);
  ^               ~~~~~~~~~~~
/System/Library/PrivateFrameworks/GPUCompiler.framework/Libraries/lib/clang/31001.518/include/metal/metal_compute:67:17: note: non-template declaration found by name lookup
METAL_FUNC void simdgroup_barrier(mem_flags flags)
                ^
program_source:42:3: error: use of undeclared identifier 'simdgroup_matrix'; did you mean 'simdgroup_barrier'?
  simdgroup_matrix<FLT, 8, 8> dst_sp24_32_ch8_16(0.0f);
  ^~~~~~~~~~~~~~~~
  simdgroup_barrier
/System/Library/PrivateFrameworks/GPUCompiler.framework/Libraries/lib/clang/31001.518/include/metal/metal_compute:67:17: note: 'simdgroup_barrier' declared here
METAL_FUNC void simdgroup_barrier(mem_flags flags)
                ^
program_source:42:3: error: 'simdgroup_barrier' does not name a template but is followed by template arguments
  simdgroup_matrix<FLT, 8, 8> dst_sp24_32_ch8_16(0.0f);
  ^               ~~~~~~~~~~~
/System/Library/PrivateFrameworks/GPUCompiler.framework/Libraries/lib/clang/31001.518/include/metal/metal_compute:67:17: note: non-template declaration found by name lookup
METAL_FUNC void simdgroup_barrier(mem_flags flags)
                ^
program_source:60:5: error: use of undeclared identifier 'simdgroup_matrix'; did you mean 'simdgroup_barrier'?
    simdgroup_matrix<FLT, 8, 8> w_o0_8_i0_8;
    ^~~~~~~~~~~~~~~~
    simdgroup_barrier
/System/Library/PrivateFrameworks/GPUCompiler.framework/Libraries/lib/clang/31001.518/include/metal/metal_compute:67:17: note: 'simdgroup_barrier' declared here
METAL_FUNC void simdgroup_barrier(mem_flags flags)
                ^
program_source:60:5: error: 'simdgroup_barrier' does not name a template but is followed by template arguments
    simdgroup_matrix<FLT, 8, 8> w_o0_8_i0_8;
    ^               ~~~~~~~~~~~
/System/Library/PrivateFrameworks/GPUCompiler.framework/Libraries/lib/clang/31001.518/include/metal/metal_compute:67:17: note: non-template declaration found by name lookup
METAL_FUNC void simdgroup_barrier(mem_flags flags)
                ^
program_source:61:5: error: use of undeclared identifier 'simdgroup_matrix'; did you mean 'simdgroup_barrier'?
    simdgroup_matrix<FLT, 8, 8> w_o8_16_i0_8;
    ^~~~~~~~~~~~~~~~
    simdgroup_barrier
/System/Library/PrivateFrameworks/GPUCompiler.framework/Libraries/lib/clang/31001.518/include/metal/metal_compute:67:17: note: 'simdgroup_barrier' declared here
METAL_FUNC void simdgroup_barrier(mem_flags flags)
                ^
program_source:61:5: error: 'simdgroup_barrier' does not name a template but is followed by template arguments
    simdgroup_matrix<FLT, 8, 8> w_o8_16_i0_8;
    ^               ~~~~~~~~~~~
/System/Library/PrivateFrameworks/GPUCompiler.framework/Libraries/lib/clang/31001.518/include/metal/metal_compute:67:17: note: non-template declaration found by name lookup
METAL_FUNC void simdgroup_barrier(mem_flags flags)
                ^
program_source:62:5: error: use of undeclared identifier 'simdgroup_matrix'; did you mean 'simdgroup_barrier'?
    simdgroup_matrix<FLT, 8, 8> w_o0_8_i8_16;
    ^~~~~~~~~~~~~~~~
    simdgroup_barrier
/System/Library/PrivateFrameworks/GPUCompiler.framework/Libraries/lib/clang/31001.518/include/metal/metal_compute:67:17: note: 'simdgroup_barrier' declared here
METAL_FUNC void simdgroup_barrier(mem_flags flags)
                ^
program_source:62:5: error: 'simdgroup_barrier' does not name a template but is followed by template arguments
    simdgroup_matrix<FLT, 8, 8> w_o0_8_i8_16;
    ^               ~~~~~~~~~~~
/System/Library/PrivateFrameworks/GPUCompiler.framework/Libraries/lib/clang/31001.518/include/metal/metal_compute:67:17: note: non-template declaration found by name lookup
METAL_FUNC void simdgroup_barrier(mem_flags flags)
                ^
program_source:63:5: error: use of undeclared identifier 'simdgroup_matrix'; did you mean 'simdgroup_barrier'?
    simdgroup_matrix<FLT, 8, 8> w_o8_16_i8_16;
    ^~~~~~~~~~~~~~~~
    simdgroup_barrier
/System/Library/PrivateFrameworks/GPUCompiler.framework/Libraries/lib/clang/31001.518/include/metal/metal_compute:67:17: note: 'simdgroup_barrier' declared here
METAL_FUNC void simdgroup_barrier(mem_flags flags)
                ^
program_source:63:5: error: 'simdgroup_barrier' does not name a template but is followed by template arguments
    simdgroup_matrix<FLT, 8, 8> w_o8_16_i8_16;
    ^               ~~~~~~~~~~~
/System/Library/PrivateFrameworks/GPUCompiler.framework/Libraries/lib/clang/31001.518/include/metal/metal_compute:67:17: note: non-template declaration found by name lookup
METAL_FUNC void simdgroup_barrier(mem_flags flags)
                ^
program_source:65:20: error: use of undeclared identifier 'w_o0_8_i0_8'
    simdgroup_load(w_o0_8_i0_8, f_offseted_x1 + 0, 8);
                   ^
program_source:66:20: error: use of undeclared identifier 'w_o8_16_i0_8'
    simdgroup_load(w_o8_16_i0_8, f_offseted_x1 + 64, 8);
                   ^
program_source:67:20: error: use of undeclared identifier 'w_o0_8_i8_16'
    simdgroup_load(w_o0_8_i8_16, f_offseted_x1 + 128, 8);
                   ^
program_source:68:20: error: use of undeclared identifier 'w_o8_16_i8_16'
    simdgroup_load(w_o8_16_i8_16, f_offseted_x1 + 192, 8);
                   ^
program_source:73:5: error: use of undeclared identifier 'simdgroup_matrix'; did you mean 'simdgroup_barrier'?
    simdgroup_matrix<FLT, 8, 8> mat_src;
    ^~~~~~~~~~~~~~~~
    simdgroup_barrier
/System/Library/PrivateFrameworks/GPUCompiler.framework/Libraries/lib/clang/31001.518/include/metal/metal_compute:67:17: note: 'simdgroup_barrier' declared here
METAL_FUNC void simdgroup_barrier(mem_flags flags)
                ^
program_source:73:5: error: 'simdgroup_barrier' does not name a template but is followed by template arguments
    simdgroup_matrix<FLT, 8, 8> mat_src;
    ^               ~~~~~~~~~~~
/System/Library/PrivateFrameworks/GPUCompiler.framework/Libraries/lib/clang/31001.518/include/metal/metal_compute:67:17: note: non-template declaration found by name lookup
METAL_FUNC void simdgroup_barrier(mem_flags flags)
                ^
program_source:74:20: error: use of undeclared identifier 'mat_src'
    simdgroup_load(mat_src, tmp_src_x1 + 0, 8);  // loading sp[0_8] src_ch[0_8]
                   ^
program_source:75:9: error: use of undeclared identifier 'dst_sp0_8_ch0_8'
    MMA(dst_sp0_8_ch0_8, mat_src, w_o0_8_i0_8, dst_sp0_8_ch0_8);
        ^
program_source:75:26: error: use of undeclared identifier 'mat_src'
    MMA(dst_sp0_8_ch0_8, mat_src, w_o0_8_i0_8, dst_sp0_8_ch0_8);
                         ^
program_source:75:35: error: use of undeclared identifier 'w_o0_8_i0_8'
    MMA(dst_sp0_8_ch0_8, mat_src, w_o0_8_i0_8, dst_sp0_8_ch0_8);
                                  ^
program_source:75:48: error: use of undeclared identifier 'dst_sp0_8_ch0_8'
    MMA(dst_sp0_8_ch0_8, mat_src, w_o0_8_i0_8, dst_sp0_8_ch0_8);
                                               ^
program_source:76:36: error: use of undeclared identifier 'w_o8_16_i0_8'
    MMA(dst_sp0_8_ch8_16, mat_src, w_o8_16_i0_8, dst_sp0_8_ch8_16);
                                   ^
program_source:76:50: error: use of undeclared identifier 'dst_sp0_8_ch8_16'
    MMA(dst_sp0_8_ch8_16, mat_src, w_o8_16_i0_8, dst_sp0_8_ch8_16);
                                                 ^
program_source:76:9: error: use of undeclared identifier 'dst_sp0_8_ch8_16'
    MMA(dst_sp0_8_ch8_16, mat_src, w_o8_16_i0_8, dst_sp0_8_ch8_16);
        ^
program_source:76:27: error: use of undeclared identifier 'mat_src'; did you mean 'tmp_src'?
    MMA(dst_sp0_8_ch8_16, mat_src, w_o8_16_i0_8, dst_sp0_8_ch8_16);
                          ^~~~~~~
                          tmp_src
program_source:30:20: note: 'tmp_src' declared here
  threadgroup FLT4 tmp_src[SPATIAL_THREADS * 4];
                   ^
program_source:77:20: error: use of undeclared identifier 'mat_src'
    simdgroup_load(mat_src, tmp_src_x1 + 64, 8);  // loading sp[8_16] src_ch[0_8]
                   ^
program_source:78:9: error: use of undeclared identifier 'dst_sp8_16_ch0_8'
    MMA(dst_sp8_16_ch0_8, mat_src, w_o0_8_i0_8, dst_sp8_16_ch0_8);
        ^
program_source:78:27: error: use of undeclared identifier 'mat_src'
    MMA(dst_sp8_16_ch0_8, mat_src, w_o0_8_i0_8, dst_sp8_16_ch0_8);
                          ^
program_source:78:36: error: use of undeclared identifier 'w_o0_8_i0_8'
    MMA(dst_sp8_16_ch0_8, mat_src, w_o0_8_i0_8, dst_sp8_16_ch0_8);
                                   ^
program_source:78:49: error: use of undeclared identifier 'dst_sp8_16_ch0_8'
    MMA(dst_sp8_16_ch0_8, mat_src, w_o0_8_i0_8, dst_sp8_16_ch0_8);
                                                ^
program_source:79:9: error: use of undeclared identifier 'dst_sp8_16_ch8_16'
    MMA(dst_sp8_16_ch8_16, mat_src, w_o8_16_i0_8, dst_sp8_16_ch8_16);
        ^
program_source:79:28: error: use of undeclared identifier 'mat_src'
    MMA(dst_sp8_16_ch8_16, mat_src, w_o8_16_i0_8, dst_sp8_16_ch8_16);
                           ^
program_source:79:37: error: use of undeclared identifier 'w_o8_16_i0_8'
    MMA(dst_sp8_16_ch8_16, mat_src, w_o8_16_i0_8, dst_sp8_16_ch8_16);
                                    ^
program_source:79:51: error: use of undeclared identifier 'dst_sp8_16_ch8_16'
    MMA(dst_sp8_16_ch8_16, mat_src, w_o8_16_i0_8, dst_sp8_16_ch8_16);
                                                  ^
program_source:80:20: error: use of undeclared identifier 'mat_src'
    simdgroup_load(mat_src, tmp_src_x1 + 128, 8);  // loading sp[16_24] src_ch[0_8]
                   ^
program_source:81:9: error: use of undeclared identifier 'dst_sp16_24_ch0_8'
    MMA(dst_sp16_24_ch0_8, mat_src, w_o0_8_i0_8, dst_sp16_24_ch0_8);
        ^
program_source:81:28: error: use of undeclared identifier 'mat_src'
    MMA(dst_sp16_24_ch0_8, mat_src, w_o0_8_i0_8, dst_sp16_24_ch0_8);
                           ^
program_source:81:37: error: use of undeclared identifier 'w_o0_8_i0_8'
    MMA(dst_sp16_24_ch0_8, mat_src, w_o0_8_i0_8, dst_sp16_24_ch0_8);
                                    ^
program_source:81:50: error: use of undeclared identifier 'dst_sp16_24_ch0_8'
    MMA(dst_sp16_24_ch0_8, mat_src, w_o0_8_i0_8, dst_sp16_24_ch0_8);
                                                 ^
program_source:82:9: error: use of undeclared identifier 'dst_sp16_24_ch8_16'
    MMA(dst_sp16_24_ch8_16, mat_src, w_o8_16_i0_8, dst_sp16_24_ch8_16);
        ^
program_source:82:29: error: use of undeclared identifier 'mat_src'
    MMA(dst_sp16_24_ch8_16, mat_src, w_o8_16_i0_8, dst_sp16_24_ch8_16);
                            ^
program_source:82:38: error: use of undeclared identifier 'w_o8_16_i0_8'
    MMA(dst_sp16_24_ch8_16, mat_src, w_o8_16_i0_8, dst_sp16_24_ch8_16);
                                     ^
program_source:82:52: error: use of undeclared identifier 'dst_sp16_24_ch8_16'
    MMA(dst_sp16_24_ch8_16, mat_src, w_o8_16_i0_8, dst_sp16_24_ch8_16);
                                                   ^
program_source:83:20: error: use of undeclared identifier 'mat_src'
    simdgroup_load(mat_src, tmp_src_x1 + 192, 8);  // loading sp[24_32] src_ch[0_8]
                   ^
program_source:84:9: error: use of undeclared identifier 'dst_sp24_32_ch0_8'
    MMA(dst_sp24_32_ch0_8, mat_src, w_o0_8_i0_8, dst_sp24_32_ch0_8);
        ^
program_source:84:28: error: use of undeclared identifier 'mat_src'
    MMA(dst_sp24_32_ch0_8, mat_src, w_o0_8_i0_8, dst_sp24_32_ch0_8);
                           ^
program_source:84:37: error: use of undeclared identifier 'w_o0_8_i0_8'
    MMA(dst_sp24_32_ch0_8, mat_src, w_o0_8_i0_8, dst_sp24_32_ch0_8);
                                    ^
program_source:84:50: error: use of undeclared identifier 'dst_sp24_32_ch0_8'
    MMA(dst_sp24_32_ch0_8, mat_src, w_o0_8_i0_8, dst_sp24_32_ch0_8);
                                                 ^
program_source:85:9: error: use of undeclared identifier 'dst_sp24_32_ch8_16'
    MMA(dst_sp24_32_ch8_16, mat_src, w_o8_16_i0_8, dst_sp24_32_ch8_16);
        ^
program_source:85:29: error: use of undeclared identifier 'mat_src'
    MMA(dst_sp24_32_ch8_16, mat_src, w_o8_16_i0_8, dst_sp24_32_ch8_16);
                            ^
program_source:85:38: error: use of undeclared identifier 'w_o8_16_i0_8'
    MMA(dst_sp24_32_ch8_16, mat_src, w_o8_16_i0_8, dst_sp24_32_ch8_16);
                                     ^
program_source:85:52: error: use of undeclared identifier 'dst_sp24_32_ch8_16'
    MMA(dst_sp24_32_ch8_16, mat_src, w_o8_16_i0_8, dst_sp24_32_ch8_16);
                                                   ^
program_source:86:20: error: use of undeclared identifier 'mat_src'
    simdgroup_load(mat_src, tmp_src_x1 + 256, 8);  // loading sp[0_8] src_ch[8_16]
                   ^
program_source:87:9: error: use of undeclared identifier 'dst_sp0_8_ch0_8'
    MMA(dst_sp0_8_ch0_8, mat_src, w_o0_8_i8_16, dst_sp0_8_ch0_8);
        ^
program_source:87:26: error: use of undeclared identifier 'mat_src'
    MMA(dst_sp0_8_ch0_8, mat_src, w_o0_8_i8_16, dst_sp0_8_ch0_8);
                         ^
program_source:87:35: error: use of undeclared identifier 'w_o0_8_i8_16'
    MMA(dst_sp0_8_ch0_8, mat_src, w_o0_8_i8_16, dst_sp0_8_ch0_8);
                                  ^
program_source:87:49: error: use of undeclared identifier 'dst_sp0_8_ch0_8'
    MMA(dst_sp0_8_ch0_8, mat_src, w_o0_8_i8_16, dst_sp0_8_ch0_8);
                                                ^
program_source:88:9: error: use of undeclared identifier 'dst_sp0_8_ch8_16'
    MMA(dst_sp0_8_ch8_16, mat_src, w_o8_16_i8_16, dst_sp0_8_ch8_16);
        ^
program_source:88:27: error: use of undeclared identifier 'mat_src'
    MMA(dst_sp0_8_ch8_16, mat_src, w_o8_16_i8_16, dst_sp0_8_ch8_16);
                          ^
program_source:88:36: error: use of undeclared identifier 'w_o8_16_i8_16'
    MMA(dst_sp0_8_ch8_16, mat_src, w_o8_16_i8_16, dst_sp0_8_ch8_16);
                                   ^
program_source:88:51: error: use of undeclared identifier 'dst_sp0_8_ch8_16'
    MMA(dst_sp0_8_ch8_16, mat_src, w_o8_16_i8_16, dst_sp0_8_ch8_16);
                                                  ^
program_source:89:20: error: use of undeclared identifier 'mat_src'
    simdgroup_load(mat_src, tmp_src_x1 + 320, 8);  // loading sp[8_16] src_ch[8_16]
                   ^
program_source:90:9: error: use of undeclared identifier 'dst_sp8_16_ch0_8'
    MMA(dst_sp8_16_ch0_8, mat_src, w_o0_8_i8_16, dst_sp8_16_ch0_8);
        ^
program_source:90:27: error: use of undeclared identifier 'mat_src'
    MMA(dst_sp8_16_ch0_8, mat_src, w_o0_8_i8_16, dst_sp8_16_ch0_8);
                          ^
program_source:90:36: error: use of undeclared identifier 'w_o0_8_i8_16'
    MMA(dst_sp8_16_ch0_8, mat_src, w_o0_8_i8_16, dst_sp8_16_ch0_8);
                                   ^
program_source:90:50: error: use of undeclared identifier 'dst_sp8_16_ch0_8'
    MMA(dst_sp8_16_ch0_8, mat_src, w_o0_8_i8_16, dst_sp8_16_ch0_8);
                                                 ^
program_source:91:9: error: use of undeclared identifier 'dst_sp8_16_ch8_16'
    MMA(dst_sp8_16_ch8_16, mat_src, w_o8_16_i8_16, dst_sp8_16_ch8_16);
        ^
program_source:91:28: error: use of undeclared identifier 'mat_src'
    MMA(dst_sp8_16_ch8_16, mat_src, w_o8_16_i8_16, dst_sp8_16_ch8_16);
                           ^
program_source:91:37: error: use of undeclared identifier 'w_o8_16_i8_16'
    MMA(dst_sp8_16_ch8_16, mat_src, w_o8_16_i8_16, dst_sp8_16_ch8_16);
                                    ^
program_source:91:52: error: use of undeclared identifier 'dst_sp8_16_ch8_16'
    MMA(dst_sp8_16_ch8_16, mat_src, w_o8_16_i8_16, dst_sp8_16_ch8_16);
                                                   ^
program_source:92:20: error: use of undeclared identifier 'mat_src'
    simdgroup_load(mat_src, tmp_src_x1 + 384, 8);  // loading sp[16_24] src_ch[8_16]
                   ^
program_source:93:9: error: use of undeclared identifier 'dst_sp16_24_ch0_8'
    MMA(dst_sp16_24_ch0_8, mat_src, w_o0_8_i8_16, dst_sp16_24_ch0_8);
        ^
program_source:93:28: error: use of undeclared identifier 'mat_src'
    MMA(dst_sp16_24_ch0_8, mat_src, w_o0_8_i8_16, dst_sp16_24_ch0_8);
                           ^
program_source:93:37: error: use of undeclared identifier 'w_o0_8_i8_16'
    MMA(dst_sp16_24_ch0_8, mat_src, w_o0_8_i8_16, dst_sp16_24_ch0_8);
                                    ^
program_source:93:51: error: use of undeclared identifier 'dst_sp16_24_ch0_8'
    MMA(dst_sp16_24_ch0_8, mat_src, w_o0_8_i8_16, dst_sp16_24_ch0_8);
                                                  ^
program_source:94:9: error: use of undeclared identifier 'dst_sp16_24_ch8_16'
    MMA(dst_sp16_24_ch8_16, mat_src, w_o8_16_i8_16, dst_sp16_24_ch8_16);
        ^
program_source:94:29: error: use of undeclared identifier 'mat_src'
    MMA(dst_sp16_24_ch8_16, mat_src, w_o8_16_i8_16, dst_sp16_24_ch8_16);
                            ^
program_source:94:38: error: use of undeclared identifier 'w_o8_16_i8_16'
    MMA(dst_sp16_24_ch8_16, mat_src, w_o8_16_i8_16, dst_sp16_24_ch8_16);
                                     ^
program_source:94:53: error: use of undeclared identifier 'dst_sp16_24_ch8_16'
    MMA(dst_sp16_24_ch8_16, mat_src, w_o8_16_i8_16, dst_sp16_24_ch8_16);
                                                    ^
program_source:95:20: error: use of undeclared identifier 'mat_src'
    simdgroup_load(mat_src, tmp_src_x1 + 448, 8);  // loading sp[24_32] src_ch[8_16]
                   ^
program_source:96:9: error: use of undeclared identifier 'dst_sp24_32_ch0_8'
    MMA(dst_sp24_32_ch0_8, mat_src, w_o0_8_i8_16, dst_sp24_32_ch0_8);
        ^
program_source:96:28: error: use of undeclared identifier 'mat_src'
    MMA(dst_sp24_32_ch0_8, mat_src, w_o0_8_i8_16, dst_sp24_32_ch0_8);
                           ^
program_source:96:37: error: use of undeclared identifier 'w_o0_8_i8_16'
    MMA(dst_sp24_32_ch0_8, mat_src, w_o0_8_i8_16, dst_sp24_32_ch0_8);
                                    ^
program_source:96:51: error: use of undeclared identifier 'dst_sp24_32_ch0_8'
    MMA(dst_sp24_32_ch0_8, mat_src, w_o0_8_i8_16, dst_sp24_32_ch0_8);
                                                  ^
program_source:97:9: error: use of undeclared identifier 'dst_sp24_32_ch8_16'
    MMA(dst_sp24_32_ch8_16, mat_src, w_o8_16_i8_16, dst_sp24_32_ch8_16);
        ^
program_source:97:29: error: use of undeclared identifier 'mat_src'
    MMA(dst_sp24_32_ch8_16, mat_src, w_o8_16_i8_16, dst_sp24_32_ch8_16);
                            ^
program_source:97:38: error: use of undeclared identifier 'w_o8_16_i8_16'
    MMA(dst_sp24_32_ch8_16, mat_src, w_o8_16_i8_16, dst_sp24_32_ch8_16);
                                     ^
program_source:97:53: error: use of undeclared identifier 'dst_sp24_32_ch8_16'
    MMA(dst_sp24_32_ch8_16, mat_src, w_o8_16_i8_16, dst_sp24_32_ch8_16);
                                                    ^
program_source:110:23: error: use of undeclared identifier 'dst_sp0_8_ch0_8'
      simdgroup_store(dst_sp0_8_ch0_8, tmp_src_x1, 8);
                      ^
program_source:111:23: error: use of undeclared identifier 'dst_sp8_16_ch0_8'
      simdgroup_store(dst_sp8_16_ch0_8, tmp_src_x1 + 64, 8);
                      ^
program_source:112:23: error: use of undeclared identifier 'dst_sp16_24_ch0_8'
      simdgroup_store(dst_sp16_24_ch0_8, tmp_src_x1 + 64 * 2, 8);
                      ^
program_source:113:23: error: use of undeclared identifier 'dst_sp24_32_ch0_8'
      simdgroup_store(dst_sp24_32_ch0_8, tmp_src_x1 + 64 * 3, 8);
                      ^
program_source:122:23: error: use of undeclared identifier 'dst_sp0_8_ch8_16'
      simdgroup_store(dst_sp0_8_ch8_16, tmp_src_x1, 8);
                      ^
program_source:123:23: error: use of undeclared identifier 'dst_sp8_16_ch8_16'
      simdgroup_store(dst_sp8_16_ch8_16, tmp_src_x1 + 64, 8);
                      ^
program_source:124:23: error: use of undeclared identifier 'dst_sp16_24_ch8_16'
      simdgroup_store(dst_sp16_24_ch8_16, tmp_src_x1 + 64 * 2, 8);
                      ^
program_source:125:23: error: use of undeclared identifier 'dst_sp24_32_ch8_16'
      simdgroup_store(dst_sp24_32_ch8_16, tmp_src_x1 + 64 * 3, 8);
                      ^

ERROR: Node number 273 (TfLiteMetalDelegate) failed to prepare.
ERROR: Restored original execution plan after delegate application failure.
```"
56708,tensorflow.python.compiler.tensorrt converter build Bert model error,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9.1 (git version: v2.9.0-18-gd8ce9f9c301)

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.4

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened when convert a pre-trained bert model to tensorRT using converter.build() inside the tensorflow.python.compiler.tensorrt package. When I remove the line converter.build(), the converted model is successfully saved, but same error happens when I tried to inference the converted model with:

iterator = iter(test_dataset.take(1))
x = iterator.next()[0]['input_ids']
converted_model = tf.saved_model.load('models/bert-fp32')
infer = converted_model.signatures['serving_default']
output_layer = list(infer.structured_outputs.keys())[0]
infer(input_ids=x)[output_layer]
```


### Standalone code to reproduce the issue

```shell
import numpy as np
import os

import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow.python.compiler.tensorrt import trt_convert as trt
from transformers import (BertTokenizer, TFBertForSequenceClassification,
                          glue_convert_examples_to_features)

tokenizer = BertTokenizer.from_pretrained(""bert-base-uncased"")

data = tfds.load('glue/mrpc')
train_dataset = glue_convert_examples_to_features(data['train'], tokenizer, max_length=128, task='mrpc').batch(1)
test_dataset = glue_convert_examples_to_features(data['test'], tokenizer, max_length=128, task='mrpc').batch(1)

model_raw = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model_raw.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])
model_raw.fit(train_dataset, epochs=10, steps_per_epoch=115, shuffle=True)
model_raw.save('models/bert-raw')

def input_fn():
    input_shapes = [[(1, 128)],[(1, 128)],[(4, 128)]]
    for shapes in input_shapes:
        yield [np.zeros(x, dtype=np.int32) for x in shapes]

conversion_params = trt.TrtConversionParams(
    precision_mode=trt.TrtPrecisionMode.FP32
)
converter = trt.TrtGraphConverterV2(
    input_saved_model_dir='models/bert-raw',
    conversion_params=conversion_params
)
converter.convert()
converter.build(input_fn)
```


### Relevant log output

```shell
2022-07-08 03:24:55.251359: W tensorflow/compiler/tf2tensorrt/segment/segment.cc:1229] A total of 39 segments with at least minimum_segment_size=3 nodes have been found. TF-TRT will only convert the 20 largest segments. You can change this behavior by modifying the environment variable TF_TRT_MAX_ALLOWED_ENGINES=20
2022-07-08 03:24:55.252060: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:795] Number of TensorRT candidate segments: 20
2022-07-08 03:24:55.268302: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:909] Replaced segment 0 consisting of 13 nodes by TRTEngineOp_000_000.
2022-07-08 03:24:55.268416: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:909] Replaced segment 1 consisting of 13 nodes by TRTEngineOp_000_001.
2022-07-08 03:24:55.268518: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:909] Replaced segment 2 consisting of 13 nodes by TRTEngineOp_000_002.
2022-07-08 03:24:55.268599: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:909] Replaced segment 3 consisting of 13 nodes by TRTEngineOp_000_003.
2022-07-08 03:24:55.268676: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:909] Replaced segment 4 consisting of 13 nodes by TRTEngineOp_000_004.
2022-07-08 03:24:55.268752: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:909] Replaced segment 5 consisting of 13 nodes by TRTEngineOp_000_005.
2022-07-08 03:24:55.268856: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:909] Replaced segment 6 consisting of 13 nodes by TRTEngineOp_000_006.
2022-07-08 03:24:55.268933: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:909] Replaced segment 7 consisting of 13 nodes by TRTEngineOp_000_007.
2022-07-08 03:24:55.269024: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:909] Replaced segment 8 consisting of 13 nodes by TRTEngineOp_000_008.
2022-07-08 03:24:55.269103: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:909] Replaced segment 9 consisting of 13 nodes by TRTEngineOp_000_009.
2022-07-08 03:24:55.269176: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:909] Replaced segment 10 consisting of 13 nodes by TRTEngineOp_000_010.
2022-07-08 03:24:55.269270: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:909] Replaced segment 11 consisting of 13 nodes by TRTEngineOp_000_011.
2022-07-08 03:24:55.269358: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:909] Replaced segment 12 consisting of 13 nodes by TRTEngineOp_000_012.
2022-07-08 03:24:55.269433: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:909] Replaced segment 13 consisting of 13 nodes by TRTEngineOp_000_013.
2022-07-08 03:24:55.269522: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:909] Replaced segment 14 consisting of 13 nodes by TRTEngineOp_000_014.
2022-07-08 03:24:55.269595: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:909] Replaced segment 15 consisting of 13 nodes by TRTEngineOp_000_015.
2022-07-08 03:24:55.269664: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:909] Replaced segment 16 consisting of 12 nodes by TRTEngineOp_000_016.
2022-07-08 03:24:55.269735: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:909] Replaced segment 17 consisting of 13 nodes by TRTEngineOp_000_017.
2022-07-08 03:24:55.269821: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:909] Replaced segment 18 consisting of 13 nodes by TRTEngineOp_000_018.
2022-07-08 03:24:55.269905: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:909] Replaced segment 19 consisting of 13 nodes by TRTEngineOp_000_019.
2022-07-08 03:25:15.215526: W tensorflow/core/framework/op_kernel.cc:1733] INVALID_ARGUMENT: required broadcastable shapes
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
Input In [5], in <cell line: 15>()
     23 converter.convert()
     24 ###
---> 25 converter.build(input_fn)
     26 ###
     27 converter.save('models/bert-' + name)

File /anaconda/envs/azureml_py38/lib/python3.8/site-packages/tensorflow/python/compiler/tensorrt/trt_convert.py:1316, in TrtGraphConverterV2.build(self, input_fn)
   1314     func(**{k: ops.convert_to_tensor(v) for k, v in inp.items()})
   1315   else:
-> 1316     func(*map(ops.convert_to_tensor, inp))
   1318 if self._need_trt_profiles():
   1319   # Disable profile generation.
   1320   self._for_each_trt_node(self._converted_graph_def,
   1321                           partial(_set_profile_generation_mode, False))

File /anaconda/envs/azureml_py38/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1602, in ConcreteFunction.__call__(self, *args, **kwargs)
   1552 def __call__(self, *args, **kwargs):
   1553   """"""Executes the wrapped function.
   1554 
   1555   ConcreteFunctions have two signatures:
   (...)
   1600     TypeError: If the arguments do not match the function's signature.
   1601   """"""
-> 1602   return self._call_impl(args, kwargs)

File /anaconda/envs/azureml_py38/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py:243, in WrappedFunction._call_impl(self, args, kwargs, cancellation_manager)
    241   return self._call_flat(args, self.captured_inputs)
    242 else:
--> 243   return super(WrappedFunction, self)._call_impl(
    244       args, kwargs, cancellation_manager)

File /anaconda/envs/azureml_py38/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1620, in ConcreteFunction._call_impl(self, args, kwargs, cancellation_manager)
   1617     except TypeError:
   1618       raise structured_err
-> 1620 return self._call_with_flat_signature(args, kwargs, cancellation_manager)

File /anaconda/envs/azureml_py38/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1669, in ConcreteFunction._call_with_flat_signature(self, args, kwargs, cancellation_manager)
   1664   if not isinstance(
   1665       arg, (ops.Tensor, resource_variable_ops.BaseResourceVariable)):
   1666     raise TypeError(f""{self._flat_signature_summary()}: expected argument ""
   1667                     f""#{i}(zero-based) to be a Tensor; ""
   1668                     f""got {type(arg).__name__} ({arg})."")
-> 1669 return self._call_flat(args, self.captured_inputs, cancellation_manager)

File /anaconda/envs/azureml_py38/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1860, in ConcreteFunction._call_flat(self, args, captured_inputs, cancellation_manager)
   1856 possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)
   1857 if (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE
   1858     and executing_eagerly):
   1859   # No tape is watching; skip to running the function.
-> 1860   return self._build_call_outputs(self._inference_function.call(
   1861       ctx, args, cancellation_manager=cancellation_manager))
   1862 forward_backward = self._select_forward_and_backward_functions(
   1863     args,
   1864     possible_gradient_type,
   1865     executing_eagerly)
   1866 forward_function, args_with_tangents = forward_backward.forward()

File /anaconda/envs/azureml_py38/lib/python3.8/site-packages/tensorflow/python/eager/function.py:497, in _EagerDefinedFunction.call(self, ctx, args, cancellation_manager)
    495 with _InterpolateFunctionError(self):
    496   if cancellation_manager is None:
--> 497     outputs = execute.execute(
    498         str(self.signature.name),
    499         num_outputs=self._num_outputs,
    500         inputs=args,
    501         attrs=attrs,
    502         ctx=ctx)
    503   else:
    504     outputs = execute.execute_with_cancellation(
    505         str(self.signature.name),
    506         num_outputs=self._num_outputs,
   (...)
    509         ctx=ctx,
    510         cancellation_manager=cancellation_manager)

File /anaconda/envs/azureml_py38/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     52 try:
     53   ctx.ensure_initialized()
---> 54   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     55                                       inputs, attrs, num_outputs)
     56 except core._NotOkStatusException as e:
     57   if name is not None:

InvalidArgumentError: Graph execution error:

Detected at node 'StatefulPartitionedCall/tf_bert_for_sequence_classification/bert/embeddings/add/add' defined at (most recent call last):
    File ""/anaconda/envs/azureml_py38/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File ""/anaconda/envs/azureml_py38/lib/python3.8/runpy.py"", line 87, in _run_code
      exec(code, run_globals)
    File ""/anaconda/envs/azureml_py38/lib/python3.8/site-packages/ipykernel_launcher.py"", line 16, in <module>
      app.launch_new_instance()
    File ""/anaconda/envs/azureml_py38/lib/python3.8/site-packages/traitlets/config/application.py"", line 972, in launch_instance
      app.start()
    File ""/anaconda/envs/azureml_py38/lib/python3.8/site-packages/ipykernel/kernelapp.py"", line 677, in start
      self.io_loop.start()
    File ""/anaconda/envs/azureml_py38/lib/python3.8/site-packages/tornado/platform/asyncio.py"", line 199, in start
      self.asyncio_loop.run_forever()
    File ""/anaconda/envs/azureml_py38/lib/python3.8/asyncio/base_events.py"", line 570, in run_forever
      self._run_once()
    File ""/anaconda/envs/azureml_py38/lib/python3.8/asyncio/base_events.py"", line 1859, in _run_once
      handle._run()
    File ""/anaconda/envs/azureml_py38/lib/python3.8/asyncio/events.py"", line 81, in _run
      self._context.run(self._callback, *self._args)
    File ""/anaconda/envs/azureml_py38/lib/python3.8/site-packages/ipykernel/kernelbase.py"", line 457, in dispatch_queue
      await self.process_one()
    File ""/anaconda/envs/azureml_py38/lib/python3.8/site-packages/ipykernel/kernelbase.py"", line 446, in process_one
      await dispatch(*args)
    File ""/anaconda/envs/azureml_py38/lib/python3.8/site-packages/ipykernel/kernelbase.py"", line 353, in dispatch_shell
      await result
    File ""/anaconda/envs/azureml_py38/lib/python3.8/site-packages/ipykernel/kernelbase.py"", line 648, in execute_request
      reply_content = await reply_content
    File ""/anaconda/envs/azureml_py38/lib/python3.8/site-packages/ipykernel/ipkernel.py"", line 353, in do_execute
      res = shell.run_cell(code, store_history=store_history, silent=silent)
    File ""/anaconda/envs/azureml_py38/lib/python3.8/site-packages/ipykernel/zmqshell.py"", line 533, in run_cell
      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
    File ""/anaconda/envs/azureml_py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py"", line 2880, in run_cell
      result = self._run_cell(
    File ""/anaconda/envs/azureml_py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py"", line 2935, in _run_cell
      return runner(coro)
    File ""/anaconda/envs/azureml_py38/lib/python3.8/site-packages/IPython/core/async_helpers.py"", line 129, in _pseudo_sync_runner
      coro.send(None)
    File ""/anaconda/envs/azureml_py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py"", line 3134, in run_cell_async
      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
    File ""/anaconda/envs/azureml_py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py"", line 3337, in run_ast_nodes
      if await self.run_code(code, result, async_=asy):
    File ""/anaconda/envs/azureml_py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py"", line 3397, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File ""/tmp/ipykernel_89514/223432839.py"", line 23, in <cell line: 15>
      converter.convert()
Node: 'StatefulPartitionedCall/tf_bert_for_sequence_classification/bert/embeddings/add/add'
required broadcastable shapes
	 [[{{node StatefulPartitionedCall/tf_bert_for_sequence_classification/bert/embeddings/add/add}}]] [Op:__inference_pruned_43829]
```
</details>"
56706,AttributeError: module 'tensorflow.compat.v2.__internal__.distribute' has no attribute 'strategy_supports_no_merge_call' #53510,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
56703,Custom Op written in C API errors when requesting GPU_Device stream,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Feature Request

### Source

binary

### Tensorflow Version

2.8

### Custom Code

Yes

### OS Platform and Distribution

Linux 

### Mobile device

07010841551

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hi there!

I was trying to write a custom op using the tensorflow C API and in the cuda version I was facing errors while requesting for the GPU_Device stream.
My questions are:
1.Is there any way to convert SP_Stream(returned by TF_GetStream(context,status)) to cudastream?
2.What is the equivalent of eigen_device<GPUDevice>().stream() (c++) for the C API?
3.Where is the implementation of TF_OPKernelContext.

Thanks,
Yoga
```


### Standalone code to reproduce the issue

```shell
// C++ code
cudaStream_t theStream=context->eigen_device<GPUDevice>().stream();

// Equivalent C code
cudaStream_t theStream = TF_GetStream(context,status);
```


### Relevant log output

_No response_</details>"
56702,model gets 100% validation accuracy in the first epoch after keyboard interrupting previous model training.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

AMD RADEON 

### Current Behaviour?

```shell
I train a model for 20 epochs, after that, I interrupt training with the stop button in jupyter notebook. After rerunning the model fitting, the model gets validation 100% accuracy in the first epoch.

I expected that the model would train from scratch after keyboard interruption during the previous training.
```


### Standalone code to reproduce the issue

```shell
# nothing special
# please, the code bellow is not important...

inputs = tf.keras.layers.Input(shape = intput_shape)
x = tf.keras.layers.Dense(128)(inputs)
outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)

model = tf.keras.Model(inputs=inputs,outputs=outputs)
model.compile(loss=tf.keras.losses.BinaryCrossEntropy(), optimizer='adam',metrics='acc')
tb_callbacks = tf.keras.callbacks.TensorBoard('./logs/')
model.fit(train_data,validation_set=val_data,epochs=100)
# fitting...
# here, after 30 epochs, val_accuracy is around 95%, I interrupt a training process with stop button in the jupyter notebook
# run again fitting
model.fit(train_data,validation_set=val_data,epochs=100)
# 100% val_accuracy after first epoch
```


### Relevant log output

_No response_</details>"
56700,Saving and Loading a Dataset returns a different elements,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

MacOS Monterey

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


I am trying to save and subsequently load a dataset. I am observing that after loading the dataset the elements of it are many times different from the saved one.

In order to run the code I have two text files in the following structure with contents:
```shell
test
 |-neg->file.txt: Once again Mr. Costner has dragged out a movie
 |-pos->file.txt: I went and saw this movie last 
```

I am expecting that the two loops should print an identical Tensor.



### Standalone code to reproduce the issue

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

import pickle


def create_int_ds(test_ds):
    max_sequence_length = 10
    max_tokens = 20000
    text_vectorization = layers.TextVectorization(
        max_tokens=max_tokens,
        output_mode=""int"",
        output_sequence_length=max_sequence_length,
    )
    
    text_only_train_ds = test_ds.map(lambda x, y: x)
    text_vectorization.adapt(text_only_train_ds)
    int_test_ds = test_ds.map(
        lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)
    return int_test_ds


batch_size = 1
test_ds = keras.utils.text_dataset_from_directory(""test"", batch_size=batch_size)
int_test_ds = create_int_ds(test_ds)

take_test_ds = int_test_ds.take(1)
print(f""type of dataset: {type(take_test_ds)}"")
print(f""spec: {take_test_ds.element_spec}"")

for e in take_test_ds:
    print(e)

# Save
tf.data.experimental.save(take_test_ds, ""int_test_ds"")
with open('element_spec', 'wb') as out_:  # also save the element_spec to disk for future loading
    pickle.dump(take_test_ds.element_spec, out_)

# Load
with open('element_spec', 'rb') as in_:
    es = pickle.load(in_)
new_take_test_ds = tf.data.experimental.load(""int_test_ds"", element_spec=es)

print(type(new_take_test_ds)) 
for e in new_take_test_ds:
    print(e) # Shoud print the same as the previous for loop
```


### Relevant log output

```
type of dataset: <class 'tensorflow.python.data.ops.dataset_ops.TakeDataset'>
spec: (TensorSpec(shape=(None, 10), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))
(<tf.Tensor: shape=(1, 10), dtype=int64, numpy=array([[105, 183, 111,  16, 138, 159,  41,  10,   8, 147]])>, <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>)
<class 'tensorflow.python.data.experimental.ops.io._LoadDataset'>
(<tf.Tensor: shape=(1, 10), dtype=int64, numpy=array([[  4,  64,   7,  39,  11,   8, 121, 108, 184, 175]])>, <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>)
```
"
56699,JAX lib dependency for Tensorflow builds on M1 (ERROR: No matching distribution found for jaxlib~=0.1.75),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

TF 2.10

### Custom Code

No

### OS Platform and Distribution

MacOS M1

### Mobile device

_No response_

### Python version

Python 3.8

### Bazel version

5.0.0+

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Currently the Tensorflow builds with M1 are failing with:

 Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
ERROR: Could not find a version that satisfies the requirement jaxlib~=0.1.75 (from versions: 0.3.0, 0.3.2, 0.3.5, 0.3.7, 0.3.10)
ERROR: No matching distribution found for jaxlib~=0.1.75
```
```


### Standalone code to reproduce the issue

```shell
pip install -r ./tensorflow/tools/ci_build/release/requirements_mac.txt
```


### Relevant log output

```shell
+ pip install --upgrade pip
Requirement already satisfied: pip in /Users/admin/.pyenv/versions/3.8.13/lib/python3.8/site-packages (22.0.4)
Collecting pip
  Downloading pip-22.1.2-py3-none-any.whl (2.1 MB)
      2.1/2.1 MB 39.5 MB/s eta 0:00:00
Installing collected packages: pip
  Attempting uninstall: pip
    Found existing installation: pip 22.0.4
    Uninstalling pip-22.0.4:
      Successfully uninstalled pip-22.0.4
Successfully installed pip-22.1.2
+ pip install -r ./tensorflow/tools/ci_build/release/requirements_mac.txt
Collecting absl-py~=0.13.0
  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)
      132.1/132.1 kB 3.9 MB/s eta 0:00:00
Collecting astunparse~=1.6.3
  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)
Collecting flatbuffers~=2.0
  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)
Collecting google_pasta~=0.2
  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)
      57.5/57.5 kB 1.7 MB/s eta 0:00:00
Collecting h5py~=3.6.0
  Downloading h5py-3.6.0.tar.gz (384 kB)
      384.2/384.2 kB 11.8 MB/s eta 0:00:00
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
Collecting keras_preprocessing~=1.1.2
  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)
      42.6/42.6 kB 1.2 MB/s eta 0:00:00
Collecting numpy~=1.21.4
  Downloading numpy-1.21.6-cp38-cp38-macosx_11_0_arm64.whl (12.3 MB)
      12.3/12.3 MB 102.9 MB/s eta 0:00:00
Collecting opt_einsum~=3.3.0
  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)
      65.5/65.5 kB 1.9 MB/s eta 0:00:00
Collecting protobuf~=3.19.3
  Downloading protobuf-3.19.4-py2.py3-none-any.whl (162 kB)
      162.7/162.7 kB 4.9 MB/s eta 0:00:00
Collecting six~=1.16.0
  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)
Collecting termcolor~=1.1.0
  Downloading termcolor-1.1.0.tar.gz (3.9 kB)
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Collecting typing_extensions~=3.10.0.0
  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)
Collecting wheel~=0.36.2
  Downloading wheel-0.36.2-py2.py3-none-any.whl (35 kB)
Collecting wrapt~=1.12.1
  Downloading wrapt-1.12.1.tar.gz (27 kB)
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Collecting gast==0.4.0
  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)
Collecting keras-nightly~=2.10.0.dev
  Downloading keras_nightly-2.10.0.dev2022062107-py2.py3-none-any.whl (1.7 MB)
      1.7/1.7 MB 42.1 MB/s eta 0:00:00
Collecting tb-nightly~=2.9.0.a
  Downloading tb_nightly-2.9.0a20220502-py3-none-any.whl (5.8 MB)
      5.8/5.8 MB 92.0 MB/s eta 0:00:00
Collecting tf-estimator-nightly~=2.10.0.dev
  Downloading tf_estimator_nightly-2.10.0.dev2022062108-py2.py3-none-any.whl (438 kB)
      438.9/438.9 kB 13.2 MB/s eta 0:00:00
Collecting grpcio~=1.43.0
  Downloading grpcio-1.43.0.tar.gz (21.5 MB)
      21.5/21.5 MB 97.6 MB/s eta 0:00:00
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Collecting portpicker~=1.4.0
  Downloading portpicker-1.4.0-py3-none-any.whl (13 kB)
Collecting scipy~=1.7.2
  Downloading scipy-1.7.3-1-cp38-cp38-macosx_12_0_arm64.whl (26.9 MB)
      26.9/26.9 MB 61.7 MB/s eta 0:00:00
Collecting packaging
  Downloading packaging-21.3-py3-none-any.whl (40 kB)
      40.8/40.8 kB 1.1 MB/s eta 0:00:00
Collecting certifi~=2020.12.5
  Downloading certifi-2020.12.5-py2.py3-none-any.whl (147 kB)
      147.5/147.5 kB 4.5 MB/s eta 0:00:00
Collecting twine~=3.6.0
  Downloading twine-3.6.0-py3-none-any.whl (35 kB)
Requirement already satisfied: setuptools in /Users/admin/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from -r ./tensorflow/tools/ci_build/release/requirements_mac.txt (line 8)) (56.0.0)
Collecting jax~=0.2.26
  Downloading jax-0.2.28.tar.gz (887 kB)
      887.3/887.3 kB 17.0 MB/s eta 0:00:00
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
ERROR: Could not find a version that satisfies the requirement jaxlib~=0.1.75 (from versions: 0.3.0, 0.3.2, 0.3.5, 0.3.7, 0.3.10)
ERROR: No matching distribution found for jaxlib~=0.1.75
```
</details>"
56697,The performance is worse after turning on mkldnn,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

2.4.1

### Custom Code

No

### OS Platform and Distribution

centos7

### Mobile device

_No response_

### Python version

3.8.6

### Bazel version

3.1.0

### GCC/Compiler version

10.2

### CUDA/cuDNN version

N

### GPU model and memory

N

### Current Behaviour?

```shell
The performance is worse after turning on mkldnn.
compile command without mkldnn
bazel build --config=nogcp   --config=nohdfs  --config=noaws --config=nonccl -c opt --copt=-march=native //tensorflow:libtensorflow_cc.so 

compile command with mkldnn
bazel build --config=mkl --config=nogcp   --config=nohdfs  --config=noaws --config=nonccl -c opt --copt=-march=native  //tensorflow:libtensorflow_cc.so 

performance with mkldnn
Number of nodes executed: 2628
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	         _MklFusedMatMul	       33	    56.488	    83.143%	    83.143%	    55.356	       33
	            StridedSlice	     1449	     4.799	     7.063%	    90.206%	    66.120	     1449
	            _MklConcatV2	       14	     2.766	     4.071%	    94.277%	   118.188	       14
	                   Const	      737	     1.379	     2.030%	    96.307%	     0.000	      737
	                    NoOp	        1	     0.934	     1.375%	    97.682%	     0.000	        1
	                    _Arg	      303	     0.422	     0.621%	    98.303%	     0.000	      303
	                    Pack	        1	     0.216	     0.318%	    98.621%	     1.080	        1
	             _MklSoftmax	        5	     0.196	     0.288%	    98.909%	     4.724	        5
	                _MklToTf	       31	     0.169	     0.249%	    99.158%	     0.000	       31
	             _MklReshape	        6	     0.136	     0.200%	    99.358%	     5.472	        6
	     _MklInputConversion	        5	     0.105	     0.155%	    99.513%	     9.120	        5
	                     Sum	        5	     0.103	     0.152%	    99.664%	     5.120	        5
	              ExpandDims	       24	     0.097	     0.143%	    99.807%	     0.000	       24
	                 _MklMul	        5	     0.096	     0.141%	    99.948%	    41.984	        5
	                    Sqrt	        2	     0.010	     0.015%	    99.963%	     0.008	        2
	                 Sigmoid	        2	     0.009	     0.013%	    99.976%	     0.000	        2
	               IdentityN	        1	     0.007	     0.010%	    99.987%	     0.000	        1
	                     Mul	        1	     0.005	     0.007%	    99.994%	     0.000	        1
	                 _Retval	        3	     0.004	     0.006%	   100.000%	     0.000	        3

Timings (microseconds): count=1 curr=67941
Memory (bytes): count=1 curr=307172
2628 nodes observed

performance without mkldnn
Number of nodes executed: 1905
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	            StridedSlice	     1449	     8.448	    48.678%	    48.678%	    66.120	     1449
	            _FusedMatMul	       33	     5.554	    32.002%	    80.680%	    25.260	       33
	                ConcatV2	       14	     1.302	     7.502%	    88.182%	   105.420	       14
	                    _Arg	      303	     0.737	     4.247%	    92.429%	     0.000	      303
	                   Const	       50	     0.370	     2.132%	    94.561%	     0.000	       50
	                    Pack	        1	     0.238	     1.371%	    95.932%	     1.080	        1
	                 Reshape	        6	     0.176	     1.014%	    96.946%	     0.000	        6
	                    NoOp	        1	     0.136	     0.784%	    97.730%	     0.000	        1
	                 Softmax	        5	     0.125	     0.720%	    98.450%	     0.000	        5
	              ExpandDims	       24	     0.108	     0.622%	    99.072%	     0.000	       24
	                     Sum	        5	     0.089	     0.513%	    99.585%	     5.120	        5
	                     Mul	        6	     0.040	     0.230%	    99.816%	     0.000	        6
	                    Sqrt	        2	     0.010	     0.058%	    99.873%	     0.008	        2
	                 Sigmoid	        2	     0.009	     0.052%	    99.925%	     0.000	        2
	               IdentityN	        1	     0.009	     0.052%	    99.977%	     0.000	        1
	                 _Retval	        3	     0.004	     0.023%	   100.000%	     0.000	        3

Timings (microseconds): count=1 curr=17355
Memory (bytes): count=1 curr=203008
1905 nodes observed

According to profile above, there are more const operators when mkldnn is enabled, and the performance of many operators becomes worse.
```


### Standalone code to reproduce the issue

```shell
N
```


### Relevant log output

_No response_</details>"
56696,tf.sparse.reshape cannot deal with tf.keras.Input(sparse=True),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.8.0

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tf.sparse.reshape can deal with tf.SparseTensor, however, it cannot deal with tf.keras.Input even if sparse=True is specified for the input.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
inputs = tf.keras.Input(shape=(3, 4), sparse=True, dtype=tf.int64)
output = tf.sparse.reshape(inputs, [-1, 4])
```


### Relevant log output

```shell
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Input In [79], in <cell line: 2>()
      1 inputs = tf.keras.Input(shape=(3, 4), sparse=True, dtype=tf.int64)
----> 2 output = tf.sparse.reshape(inputs, [-1, 4])

File /usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/sparse_ops.py:911, in sparse_reshape(sp_input, shape, name)
    861 @tf_export(""sparse.reshape"", v1=[""sparse.reshape"", ""sparse_reshape""])
    862 @deprecation.deprecated_endpoints(""sparse_reshape"")
    863 def sparse_reshape(sp_input, shape, name=None):
    864   """"""Reshapes a `SparseTensor` to represent values in a new dense shape.
    865 
    866   This operation has the same semantics as `reshape` on the represented dense
   (...)
    909     ValueError:  If `shape` has more than one inferred (== -1) dimension.
    910   """"""
--> 911   sp_input = _convert_to_sparse_tensor(sp_input)
    912   shape = math_ops.cast(shape, dtype=dtypes.int64)
    914   with ops.name_scope(name, ""SparseReshape"", [sp_input]) as name:

File /usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/sparse_ops.py:67, in _convert_to_sparse_tensor(sp_input)
     65   return sparse_tensor.SparseTensor.from_value(sp_input)
     66 if not isinstance(sp_input, sparse_tensor.SparseTensor):
---> 67   raise TypeError(""Input must be a SparseTensor."")
     68 return sp_input

TypeError: Input must be a SparseTensor.
```"
56695,object is not callable,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
56694,`tf.image.rgb_to_hsv` fails on back prop when dtype is double,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Cannot compute the gradient for `tf.image.rgb_to_hsv` with `float64` inputs. It is working fine with forward pass.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
images = tf.random.uniform([5, 5, 3], dtype=tf.float64)
x = tf.image.rgb_to_hsv(images) # pass
with tf.GradientTape(persistent=True,) as g:
  g.watch(images)
  x = tf.image.rgb_to_hsv(images, )
grad = g.gradient(x, images) # InvalidArgumentError
```


### Relevant log output

```shell
InvalidArgumentError: cannot compute Mul as input #1(zero-based) was expected to be a float tensor but is a double tensor [Op:Mul]
```
</details>"
56693,Bug in tf.test.compute_gradient for tf.keras.losses.categorical_hinge,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The numerical gradient and theoretical gradient for `tf.keras.losses.categorical_hinge` is different, one is `0.5`, the other is `0.333`. I think the numerical gradient is wrong.
```


### Standalone code to reproduce the issue

```shell
x = tf.zeros(shape=[2,3], dtype=tf.float64)
y = tf.zeros(shape=[2,3], dtype=tf.float64)
r = tf.keras.losses.categorical_hinge(x,y)
theoretical, numerical = tf.test.compute_gradient(tf.keras.losses.categorical_hinge, [x,y])
print(theoretical[1])
print(numerical[1])
```


### Relevant log output

```shell
[[0.33333333 0.33333333 0.33333333 0.         0.         0.        ]
 [0.         0.         0.         0.33333333 0.33333333 0.33333333]]
[[0.5 0.5 0.5 0.  0.  0. ]
 [0.  0.  0.  0.5 0.5 0.5]]
```
</details>"
56692,Add support for kwargs for tf.recompute_grad API for graph mode," 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

2.9

### Custom Code

No

### OS Platform and Distribution

Linux

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


Currently `tf.recompute_grad()` API do not support keyword arguments  when run in graph mode. It throws ValueError from here: 

https://github.com/tensorflow/tensorflow/blob/d8ce9f9c301d021a69953134185ab728c1c248d3/tensorflow/python/ops/custom_gradient.py#L404-L408 

This will be a useful addition for ease of use.



### Standalone code to reproduce the issue

```shell
This can be reproduced with any example. Even simply passing `training=True` during forward pass will raise this issue when run in graph mode
```


### Relevant log output

"
56690,TF 2.9.1 GPU not detected,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

TF 2.9.1

### Custom Code

No

### OS Platform and Distribution

Win10 21H2 x64

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

cudatoolkit=11.2.2 cudnn=8.1.0.77

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I install TF 2.9.1 by:

conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0
python3 -m pip install tensorflow
# Verify install:
python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""
```

TF did not detect GPU.
```


### Standalone code to reproduce the issue

```shell
`python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""` output empty.
```


### Relevant log output

_No response_</details>"
56689,disable_resource_variables is deprecated,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.6.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 16.04

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
I was running my code normally but then all of sudden I received a warning that disturbing my code from compiling and my program completely stop T.T
My source code was written in tf v1 and I've been running it on tf v2 so apparently I need 'tf.disable_resource_variables()' for my programme to work.
```


### Standalone code to reproduce the issue

```shell
I received this WARNING. 
""WARNING:tensorflow:From /home/mdee/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term""
This is exactly what I received and I read from the tensorflow.org website that if my code needs tf.disable_resource_variables() to be called to work properly, I should file a bug.
Please let me know how can I run my code smoothly again.
```


### Relevant log output

_No response_</details>"
56688,kernel_tests/nn_ops/xent_op_d9m_test.py test fails,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9.1

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04.5 LTS (on colab)

### Mobile device

N/A

### Python version

3.7.13

### Bazel version

N/A

### GCC/Compiler version

N/A

### CUDA/cuDNN version

CUDA V11.1.105

### GPU model and memory

N/A

### Current Behaviour?

7 tests in python/kernal_tests/nn_ops/xent_op_d9m_test.py fail, it seems there is an InvalidArgumentError

```shell
E     tensorflow.python.framework.errors_impl.InvalidArgumentError: labels must be 1-D, but got shape []
E     	 [[{{node SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]
```

Not all failures are due to this error, some failures are already tagged as bug by developers.


### Standalone code to reproduce the issue

Use pytest to run: `pytest sparse_xent_op_d9m_test.py`

Or find the gist [here](https://colab.research.google.com/drive/187TKAxXYtltx-7mY-zm2Z-wWYTiftAmz?usp=sharing)



### Relevant log output


The log is too long, I attach a part of it here, you may find the complete one on colab gist above.
```shell
_______________ SparseXentOpDeterministicTest.testScalarHandling _______________

    def _do_call(self, fn, *args):
      try:
>       return fn(*args)

/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py:1377: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

feed_dict = {<tensorflow.python.client._pywrap_tf_session.TF_Output object at 0x7f2b117351f0>: array([[0]], dtype=int32), <tensorf...t._pywrap_tf_session.TF_Output object at 0x7f2b117356f0>: array([[ 0.5549795, -0.6368797, -0.9563903]], dtype=float32)}
fetch_list = [<tensorflow.python.client._pywrap_tf_session.TF_Output object at 0x7f2b11735030>]
target_list = [], options = None, run_metadata = None

    def _run_fn(feed_dict, fetch_list, target_list, options, run_metadata):
      # Ensure any changes to the graph are reflected in the runtime.
      self._extend_graph()
      return self._call_tf_sessionrun(options, feed_dict, fetch_list,
>                                     target_list, run_metadata)

/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py:1361: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tensorflow.python.framework.test_util.ErrorLoggingSession object at 0x7f2b11734510>
options = None
feed_dict = {<tensorflow.python.client._pywrap_tf_session.TF_Output object at 0x7f2b117351f0>: array([[0]], dtype=int32), <tensorf...t._pywrap_tf_session.TF_Output object at 0x7f2b117356f0>: array([[ 0.5549795, -0.6368797, -0.9563903]], dtype=float32)}
fetch_list = [<tensorflow.python.client._pywrap_tf_session.TF_Output object at 0x7f2b11735030>]
target_list = [], run_metadata = None

    def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,
                            run_metadata):
      return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,
                                              fetch_list, target_list,
>                                             run_metadata)
E     tensorflow.python.framework.errors_impl.InvalidArgumentError: labels must be 1-D, but got shape []
E     	 [[{{node SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]

/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py:1455: InvalidArgumentError

During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py"", line 1377, in _do_call
    return fn(*args)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn
    target_list, run_metadata)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py"", line 1455, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: labels must be 1-D, but got shape []
	 [[{{node SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]

During handling of the above exception, another exception occurred:

tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:
```
</details>"
56687,python/kernal_tests/linalg/tridiagonal_solve_op_test.py test fail,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9.1

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04.5 LTS (on colab)

### Mobile device

N/A

### Python version

3.7.13

### Bazel version

N/A

### GCC/Compiler version

N/A

### CUDA/cuDNN version

CUDA V11.1.105

### GPU model and memory

_No response_

### Current Behaviour?

```shell
6 tests in python/kernel_tests/linalg/tridiagonal_solve_op_test.py fail, the error is:
AttributeError: 'TridiagonalSolveOpTest' object has no attribute 'pivoting'.
```


### Standalone code to reproduce the issue

```shell
Use pytest to run the test.
`pytest tridiagonal_solve_op_test.py`

Or, please find gist here: https://colab.research.google.com/drive/1un1vsyolGCLz6DJLvnR3FHOSRH_i_0id?usp=sharing
```


### Relevant log output

```shell
============================= test session starts ==============================
platform linux -- Python 3.7.13, pytest-3.6.4, py-1.11.0, pluggy-0.7.1
rootdir: /content, inifile:
plugins: typeguard-2.7.1
collected 49 items                                                             

tridiagonal_solve_op_test.py ..............FFFF...............F......F.. [ 87%]
.....s                                                                   [100%]

=================================== FAILURES ===================================
____________ TridiagonalSolveOpTest.testCompactFormatAllDimsUnknown ____________

self = <tridiagonal_solve_op_test.TridiagonalSolveOpTest testMethod=testCompactFormatAllDimsUnknown>
args = (), kwargs = {}

    def decorated(self, *args, **kwargs):
      if context.executing_eagerly():
        with context.graph_mode():
>         return f(self, *args, **kwargs)

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/test_util.py:1625: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tridiagonal_solve_op_test.py:570: in testCompactFormatAllDimsUnknown
    expected=_sample_result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tridiagonal_solve_op_test.TridiagonalSolveOpTest testMethod=testCompactFormatAllDimsUnknown>
diags_shape = [None, None], rhs_shape = [None]
diags_feed = array([[ 2,  1,  4,  0],
       [ 1,  3,  2,  2],
       [ 0,  1, -1,  1]])
rhs_feed = array([1, 2, 3, 4]), expected = array([-9,  5, -4,  4])
diags_format = 'compact'

    def _testWithPlaceholders(self,
                              diags_shape,
                              rhs_shape,
                              diags_feed,
                              rhs_feed,
                              expected,
                              diags_format=""compact""):
      if context.executing_eagerly():
        return
      diags = array_ops.placeholder(dtypes.float64, shape=diags_shape)
      rhs = array_ops.placeholder(dtypes.float64, shape=rhs_shape)
      if test_util.is_xla_enabled() and self.pivoting:
        # Pivoting is not supported by xla backends.
        return
      x = linalg_impl.tridiagonal_solve(
>         diags, rhs, diags_format, partial_pivoting=self.pivoting)
E     AttributeError: 'TridiagonalSolveOpTest' object has no attribute 'pivoting'

tridiagonal_solve_op_test.py:558: AttributeError
___________ TridiagonalSolveOpTest.testCompactFormatUnknownBatchSize ___________

self = <tridiagonal_solve_op_test.TridiagonalSolveOpTest testMethod=testCompactFormatUnknownBatchSize>
args = (), kwargs = {}

    def decorated(self, *args, **kwargs):
      if context.executing_eagerly():
        with context.graph_mode():
>         return f(self, *args, **kwargs)

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/test_util.py:1625: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tridiagonal_solve_op_test.py:597: in testCompactFormatUnknownBatchSize
    expected=np.array([_sample_result, -2 * _sample_result]))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tridiagonal_solve_op_test.TridiagonalSolveOpTest testMethod=testCompactFormatUnknownBatchSize>
diags_shape = [None, 3, 4], rhs_shape = [None, 4]
diags_feed = array([[[ 2,  1,  4,  0],
        [ 1,  3,  2,  2],
        [ 0,  1, -1,  1]],

       [[-2, -1, -4,  0],
        [-1, -3, -2, -2],
        [ 0, -1,  1, -1]]])
rhs_feed = array([[1, 2, 3, 4],
       [2, 4, 6, 8]])
expected = array([[ -9,   5,  -4,   4],
       [ 18, -10,   8,  -8]])
diags_format = 'compact'

    def _testWithPlaceholders(self,
                              diags_shape,
                              rhs_shape,
                              diags_feed,
                              rhs_feed,
                              expected,
                              diags_format=""compact""):
      if context.executing_eagerly():
        return
      diags = array_ops.placeholder(dtypes.float64, shape=diags_shape)
      rhs = array_ops.placeholder(dtypes.float64, shape=rhs_shape)
      if test_util.is_xla_enabled() and self.pivoting:
        # Pivoting is not supported by xla backends.
        return
      x = linalg_impl.tridiagonal_solve(
>         diags, rhs, diags_format, partial_pivoting=self.pivoting)
E     AttributeError: 'TridiagonalSolveOpTest' object has no attribute 'pivoting'

tridiagonal_solve_op_test.py:558: AttributeError
__________ TridiagonalSolveOpTest.testCompactFormatUnknownMatrixSize ___________

self = <tridiagonal_solve_op_test.TridiagonalSolveOpTest testMethod=testCompactFormatUnknownMatrixSize>
args = (), kwargs = {}

    def decorated(self, *args, **kwargs):
      if context.executing_eagerly():
        with context.graph_mode():
>         return f(self, *args, **kwargs)

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/test_util.py:1625: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tridiagonal_solve_op_test.py:579: in testCompactFormatUnknownMatrixSize
    expected=_sample_result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tridiagonal_solve_op_test.TridiagonalSolveOpTest testMethod=testCompactFormatUnknownMatrixSize>
diags_shape = [3, None], rhs_shape = [4]
diags_feed = array([[ 2,  1,  4,  0],
       [ 1,  3,  2,  2],
       [ 0,  1, -1,  1]])
rhs_feed = array([1, 2, 3, 4]), expected = array([-9,  5, -4,  4])
diags_format = 'compact'

    def _testWithPlaceholders(self,
                              diags_shape,
                              rhs_shape,
                              diags_feed,
                              rhs_feed,
                              expected,
                              diags_format=""compact""):
      if context.executing_eagerly():
        return
      diags = array_ops.placeholder(dtypes.float64, shape=diags_shape)
      rhs = array_ops.placeholder(dtypes.float64, shape=rhs_shape)
      if test_util.is_xla_enabled() and self.pivoting:
        # Pivoting is not supported by xla backends.
        return
      x = linalg_impl.tridiagonal_solve(
>         diags, rhs, diags_format, partial_pivoting=self.pivoting)
E     AttributeError: 'TridiagonalSolveOpTest' object has no attribute 'pivoting'

tridiagonal_solve_op_test.py:558: AttributeError
___________ TridiagonalSolveOpTest.testCompactFormatUnknownRhsCount ____________

self = <tridiagonal_solve_op_test.TridiagonalSolveOpTest testMethod=testCompactFormatUnknownRhsCount>
args = (), kwargs = {}

    def decorated(self, *args, **kwargs):
      if context.executing_eagerly():
        with context.graph_mode():
>         return f(self, *args, **kwargs)

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/test_util.py:1625: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tridiagonal_solve_op_test.py:588: in testCompactFormatUnknownRhsCount
    expected=np.transpose([_sample_result, 2 * _sample_result]))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tridiagonal_solve_op_test.TridiagonalSolveOpTest testMethod=testCompactFormatUnknownRhsCount>
diags_shape = [3, 4], rhs_shape = [4, None]
diags_feed = array([[ 2,  1,  4,  0],
       [ 1,  3,  2,  2],
       [ 0,  1, -1,  1]])
rhs_feed = array([[1, 2],
       [2, 4],
       [3, 6],
       [4, 8]])
expected = array([[ -9, -18],
       [  5,  10],
       [ -4,  -8],
       [  4,   8]])
diags_format = 'compact'

    def _testWithPlaceholders(self,
                              diags_shape,
                              rhs_shape,
                              diags_feed,
                              rhs_feed,
                              expected,
                              diags_format=""compact""):
      if context.executing_eagerly():
        return
      diags = array_ops.placeholder(dtypes.float64, shape=diags_shape)
      rhs = array_ops.placeholder(dtypes.float64, shape=rhs_shape)
      if test_util.is_xla_enabled() and self.pivoting:
        # Pivoting is not supported by xla backends.
        return
      x = linalg_impl.tridiagonal_solve(
>         diags, rhs, diags_format, partial_pivoting=self.pivoting)
E     AttributeError: 'TridiagonalSolveOpTest' object has no attribute 'pivoting'

tridiagonal_solve_op_test.py:558: AttributeError
____________ TridiagonalSolveOpTest.testMatrixFormatWithUnknownDims ____________

self = <tridiagonal_solve_op_test.TridiagonalSolveOpTest testMethod=testMatrixFormatWithUnknownDims>
args = (), kwargs = {}

    def decorated(self, *args, **kwargs):
      if context.executing_eagerly():
        with context.graph_mode():
>         return f(self, *args, **kwargs)

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/test_util.py:1625: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tridiagonal_solve_op_test.py:617: in testMatrixFormatWithUnknownDims
    test_with_matrix_shapes(matrix_shape=[4, 4], rhs_shape=[None, None])
tridiagonal_solve_op_test.py:615: in test_with_matrix_shapes
    diags_format=""matrix"")
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tridiagonal_solve_op_test.TridiagonalSolveOpTest testMethod=testMatrixFormatWithUnknownDims>
diags_shape = [4, 4], rhs_shape = [None, None]
diags_feed = array([[ 1,  2,  0,  0],
       [ 1,  3,  1,  0],
       [ 0, -1,  2,  4],
       [ 0,  0,  1,  2]])
rhs_feed = array([[1, 2],
       [2, 4],
       [3, 6],
       [4, 8]])
expected = array([[ -9, -18],
       [  5,  10],
       [ -4,  -8],
       [  4,   8]])
diags_format = 'matrix'

    def _testWithPlaceholders(self,
                              diags_shape,
                              rhs_shape,
                              diags_feed,
                              rhs_feed,
                              expected,
                              diags_format=""compact""):
      if context.executing_eagerly():
        return
      diags = array_ops.placeholder(dtypes.float64, shape=diags_shape)
      rhs = array_ops.placeholder(dtypes.float64, shape=rhs_shape)
      if test_util.is_xla_enabled() and self.pivoting:
        # Pivoting is not supported by xla backends.
        return
      x = linalg_impl.tridiagonal_solve(
>         diags, rhs, diags_format, partial_pivoting=self.pivoting)
E     AttributeError: 'TridiagonalSolveOpTest' object has no attribute 'pivoting'

tridiagonal_solve_op_test.py:558: AttributeError
___________ TridiagonalSolveOpTest.testSequenceFormatWithUnknownDims ___________

self = <tridiagonal_solve_op_test.TridiagonalSolveOpTest testMethod=testSequenceFormatWithUnknownDims>
args = (), kwargs = {}

    def decorated(self, *args, **kwargs):
      if context.executing_eagerly():
        with context.graph_mode():
>         return f(self, *args, **kwargs)

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/test_util.py:1625: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tridiagonal_solve_op_test.TridiagonalSolveOpTest testMethod=testSequenceFormatWithUnknownDims>

    @test_util.run_deprecated_v1
    def testSequenceFormatWithUnknownDims(self):
      if context.executing_eagerly():
        return
      if test_util.is_xla_enabled() and self.pivoting:
        # Pivoting is not supported by xla backends.
        return
      superdiag = array_ops.placeholder(dtypes.float64, shape=[None])
      diag = array_ops.placeholder(dtypes.float64, shape=[None])
      subdiag = array_ops.placeholder(dtypes.float64, shape=[None])
      rhs = array_ops.placeholder(dtypes.float64, shape=[None])
    
      x = linalg_impl.tridiagonal_solve((superdiag, diag, subdiag),
                                        rhs,
                                        diagonals_format=""sequence"",
>                                       partial_pivoting=self.pivoting)
E     AttributeError: 'TridiagonalSolveOpTest' object has no attribute 'pivoting'

tridiagonal_solve_op_test.py:643: AttributeError
=============================== warnings summary ===============================
tridiagonal_solve_op_test.py::TridiagonalSolveOpTest::testAdjointRhs
  /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py:102: ComplexWarning: Casting complex values to real discards the imaginary part
    return ops.EagerTensor(value, ctx.device_name, dtype)

tridiagonal_solve_op_test.py::TridiagonalSolveOpTest::testConjugateRhs
  /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py:102: ComplexWarning: Casting complex values to real discards the imaginary part
    return ops.EagerTensor(value, ctx.device_name, dtype)

tridiagonal_solve_op_test.py::TridiagonalSolveOpTest::testConjugateRhsWithRhsAsVector
  /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py:102: ComplexWarning: Casting complex values to real discards the imaginary part
    return ops.EagerTensor(value, ctx.device_name, dtype)

-- Docs: http://doc.pytest.org/en/latest/warnings.html
========= 6 failed, 42 passed, 1 skipped, 3 warnings in 11.45 seconds ==========
```
</details>"
56686,MBv3 conversion: How to turn off hard swish optimization,"### 1. System information

- Windows 11
- pip 
- version 2.8.0

### 2. Code
```
import numpy as np
import tensorflow as tf
import pathlib
from tensorflow.keras import Model, Input

model = tf.keras.applications.MobileNetV3Small()
model.summary()

dataset = np.random.rand(10, 224, 224, 3).astype(np.float32)
def representative_data_gen():
    for input_value in tf.data.Dataset.from_tensor_slices(dataset).batch(1).take(len(dataset)):
        # Model has only one input so each data point has one element.
        yield [input_value]

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen
tflite_model = converter.convert()

tflite_models_dir = pathlib.Path(""./tflite_models/hard_swish/"")
tflite_models_dir.mkdir(exist_ok=True, parents=True)

# Save the quantized model:
tflite_model_quant_file = tflite_models_dir/""mbv3.tflite""
tflite_model_quant_file.write_bytes(tflite_model)

```
### 3. Failure after conversion
The model is correctly converted. The hard swish layers (multiple layers- ADD, MUL, RELU6 in TF) are combined into one layer- `hard swish` in TFlite.

![image](https://user-images.githubusercontent.com/76564442/177485487-ed7ffa24-b784-4ba7-bc69-79297b05f93a.png)

I understand that this is by design.

#### Question:
I have a use case where I need to turn off this conversion. That is, when the model is converted to tflite, the layers should not be converted to hard swish layer. Please advise on how to turn off this behavior.
"
56685,Can we get PocketFFT ported to Tensorflow?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Is it possible to integrate PocketFFT when using a CPU into Tensorflow, which functions like tf.signal.stft and tf.signal.inverse_stft can leverage? 

Currently, Tensorflow's FFT uses EigenFFT, which is almost 3x slower than Numpy and Jax, which use PocketFFT.

There are heaps more details here: https://github.com/tensorflow/tensorflow/issues/6541

I'm sure many projects would benefit from this investment considering much of what's done for speech and music these days use STFT data.
```


### Standalone code to reproduce the issue

```shell
print(""seconds (lower is better):"")
print(f""Tensorflow {tf.__version__}"", timeit.timeit('X = tf.signal.rfft(x)', setup='import tensorflow as tf; x = tf.random.normal([50000, 512])', number=10))
print(f""Tensorflow {tf.__version__}, double precision"", timeit.timeit('X = tf.cast(tf.signal.rfft(tf.cast(x, tf.float64)), tf.complex64)', setup='import tensorflow as tf; x = tf.random.normal([50000, 512])', number=10))
print(""Numpy: "", timeit.timeit('X = numpy.fft.rfft(x)', setup='import numpy.fft; import tensorflow as tf; x = tf.random.normal([50000, 512])', number=10))
print(""Jax: "", timeit.timeit('jnp.fft.rfft(x).block_until_ready()', setup='import jax.numpy as jnp; import tensorflow as tf; x = tf.random.normal([50000, 512]).numpy()', number=10))
```


### Relevant log output

```shell
seconds (lower is better):
Tensorflow 2.9.1 5.495112890999991
Tensorflow 2.9.1, double precision 7.629201937000033
Numpy:  2.1803204349999987
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
Jax:  1.4081462569999985
```
</details>"
56684,Training with MultiWorkerMirrorredStrategy crashes in TF 2.9,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA=11.2.2 / CUDNN=8.2.4.15-1

### GPU model and memory

Nvidia V100

### Current Behaviour?

```shell
Master worker crashes after training completes
Remote worker exits successfully
```


### Standalone code to reproduce the issue

```shell
git clone https://github.com/tensorflow/models -b v2.9.2
cd models
/usr/local/bin/python3.9 official/vision/train.py --config_file official/vision/configs/experiments/image_classification/imagenet_resnet50_gpu.yaml --experiment resnet_imagenet --mode train --model_dir /opt/ml/model --params_override runtime.enable_xla=False,runtime.num_gpus=1,runtime.distribution_strategy=multi_worker_mirrored,runtime.mixed_precision_dtype=float16,task.train_data.global_batch_size=64,task.train_data.input_path=/opt/ml/input/data/training/train-000*,task.train_data.cache=True,trainer.train_steps=1562,trainer.steps_per_loop=15,trainer.summary_interval=15,trainer.checkpoint_interval=1562,task.model.backbone.type=resnet,task.model.backbone.resnet.model_id=50
```
```


### Relevant log output

```shell
/opt/ml/code/official/vision/tasks/semantic_segmentation.py:169: SyntaxWarning: ""is"" with a literal. Did you mean ""==""?
  if 'mask_scores_mse' is metric.name:
I0701 03:25:47.730769 140097427990336 train_utils.py:356] Final experiment parameters:
{'runtime': {'all_reduce_alg': None,
             'batchnorm_spatial_persistent': False,
             'dataset_num_private_threads': None,
             'default_shard_dim': -1,
             'distribution_strategy': 'multi_worker_mirrored',
             'enable_xla': False,
             'gpu_thread_mode': None,
             'loss_scale': 'dynamic',
             'mixed_precision_dtype': 'float16',
             'num_cores_per_replica': 1,
             'num_gpus': 1,
             'num_packs': 1,
             'per_gpu_thread_count': 0,
             'run_eagerly': False,
             'task_index': -1,
             'tpu': None,
             'tpu_enable_xla_dynamic_padder': None,
             'worker_hosts': None},
 'task': {'differential_privacy_config': None,
          'evaluation': {'top_k': 5},
          'init_checkpoint': None,
          'init_checkpoint_modules': 'all',
          'losses': {'l2_weight_decay': 0.0001,
                     'label_smoothing': 0.1,
                     'loss_weight': 1.0,
                     'one_hot': True,
                     'soft_labels': False},
          'model': {'add_head_batch_norm': False,
                    'backbone': {'resnet': {'bn_trainable': True,
                                            'depth_multiplier': 1.0,
                                            'model_id': 50,
                                            'replace_stem_max_pool': False,
                                            'resnetd_shortcut': False,
                                            'scale_stem': True,
                                            'se_ratio': 0.0,
                                            'stem_type': 'v0',
                                            'stochastic_depth_drop_rate': 0.0},
                                 'type': 'resnet'},
                    'dropout_rate': 0.0,
                    'input_size': [224, 224, 3],
                    'kernel_initializer': 'random_uniform',
                    'norm_activation': {'activation': 'relu',
                                        'norm_epsilon': 1e-05,
                                        'norm_momentum': 0.9,
                                        'use_sync_bn': False},
                    'num_classes': 1001},
          'model_output_keys': [],
          'name': None,
          'train_data': {'aug_policy': None,
                         'aug_rand_hflip': True,
                         'aug_type': None,
                         'block_length': 1,
                         'cache': True,
                         'color_jitter': 0.0,
                         'cycle_length': 10,
                         'decode_jpeg_only': True,
                         'decoder': {'simple_decoder': {'mask_binarize_threshold': None,
                                                        'regenerate_source_id': False},
                                     'type': 'simple_decoder'},
                         'deterministic': None,
                         'drop_remainder': True,
                         'dtype': 'float16',
                         'enable_tf_data_service': False,
                         'file_type': 'tfrecord',
                         'global_batch_size': 64,
                         'image_field_key': 'image/encoded',
                         'input_path': '/opt/ml/input/data/training/train-000*',
                         'is_multilabel': False,
                         'is_training': True,
                         'label_field_key': 'image/class/label',
                         'mixup_and_cutmix': None,
                         'randaug_magnitude': 10,
                         'random_erasing': None,
                         'seed': None,
                         'sharding': True,
                         'shuffle_buffer_size': 10000,
                         'tf_data_service_address': None,
                         'tf_data_service_job_name': None,
                         'tfds_as_supervised': False,
                         'tfds_data_dir': '',
                         'tfds_name': '',
                         'tfds_skip_decoding_feature': '',
                         'tfds_split': ''},
          'validation_data': {'aug_policy': None,
                              'aug_rand_hflip': True,
                              'aug_type': None,
                              'block_length': 1,
                              'cache': False,
                              'color_jitter': 0.0,
                              'cycle_length': 10,
                              'decode_jpeg_only': True,
                              'decoder': {'simple_decoder': {'mask_binarize_threshold': None,
                                                             'regenerate_source_id': False},
                                          'type': 'simple_decoder'},
                              'deterministic': None,
                              'drop_remainder': False,
                              'dtype': 'float16',
                              'enable_tf_data_service': False,
                              'file_type': 'tfrecord',
                              'global_batch_size': 2048,
                              'image_field_key': 'image/encoded',
                              'input_path': 'imagenet-2012-tfrecord/valid*',
                              'is_multilabel': False,
                              'is_training': False,
                              'label_field_key': 'image/class/label',
                              'mixup_and_cutmix': None,
                              'randaug_magnitude': 10,
                              'random_erasing': None,
                              'seed': None,
                              'sharding': True,
                              'shuffle_buffer_size': 10000,
                              'tf_data_service_address': None,
                              'tf_data_service_job_name': None,
                              'tfds_as_supervised': False,
                              'tfds_data_dir': '',
                              'tfds_name': '',
                              'tfds_skip_decoding_feature': '',
                              'tfds_split': ''}},
 'trainer': {'allow_tpu_summary': False,
             'best_checkpoint_eval_metric': '',
             'best_checkpoint_export_subdir': '',
             'best_checkpoint_metric_comp': 'higher',
             'checkpoint_interval': 1562,
             'continuous_eval_timeout': 3600,
             'eval_tf_function': True,
             'eval_tf_while_loop': False,
             'loss_upper_bound': 1000000.0,
             'max_to_keep': 5,
             'optimizer_config': {'ema': None,
                                  'learning_rate': {'stepwise': {'boundaries': [18750,
                                                                                37500,
                                                                                50000],
                                                                 'name': 'PiecewiseConstantDecay',
                                                                 'offset': 0,
                                                                 'values': [0.8,
                                                                            0.08,
                                                                            0.008,
                                                                            0.0008]},
                                                    'type': 'stepwise'},
                                  'optimizer': {'sgd': {'clipnorm': None,
                                                        'clipvalue': None,
                                                        'decay': 0.0,
                                                        'global_clipnorm': None,
                                                        'momentum': 0.9,
                                                        'name': 'SGD',
                                                        'nesterov': False},
                                                'type': 'sgd'},
                                  'warmup': {'linear': {'name': 'linear',
                                                        'warmup_learning_rate': 0,
                                                        'warmup_steps': 3125},
                                             'type': 'linear'}},
             'recovery_begin_steps': 0,
             'recovery_max_trials': 0,
             'steps_per_loop': 15,
             'summary_interval': 15,
             'train_steps': 1562,
             'train_tf_function': True,
             'train_tf_while_loop': True,
             'validation_interval': 625,
             'validation_steps': 25,
             'validation_summary_subdir': 'validation'}}
I0701 03:25:47.731283 140097427990336 train_utils.py:368] Saving experiment configuration to /opt/ml/model/params.yaml
INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK
Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: Tesla V100-SXM2-16GB, compute capability 7.0
I0701 03:25:47.754091 140097427990336 device_compatibility_check.py:117] Mixed precision compatibility check (mixed_float16): OK
Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: Tesla V100-SXM2-16GB, compute capability 7.0
WARNING:tensorflow:From /opt/ml/code/official/common/distribute_utils.py:155: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.
Instructions for updating:
use distribute.MultiWorkerMirroredStrategy instead
W0701 03:25:47.754371 140097427990336 deprecation.py:350] From /opt/ml/code/official/common/distribute_utils.py:155: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.
Instructions for updating:
use distribute.MultiWorkerMirroredStrategy instead
I0701 03:25:49.730333 140259922851648 train_utils.py:356] Final experiment parameters:
{'runtime': {'all_reduce_alg': None,
             'batchnorm_spatial_persistent': False,
             'dataset_num_private_threads': None,
             'default_shard_dim': -1,
             'distribution_strategy': 'multi_worker_mirrored',
             'enable_xla': False,
             'gpu_thread_mode': None,
             'loss_scale': 'dynamic',
             'mixed_precision_dtype': 'float16',
             'num_cores_per_replica': 1,
             'num_gpus': 1,
             'num_packs': 1,
             'per_gpu_thread_count': 0,
             'run_eagerly': False,
             'task_index': -1,
             'tpu': None,
             'tpu_enable_xla_dynamic_padder': None,
             'worker_hosts': None},
 'task': {'differential_privacy_config': None,
          'evaluation': {'top_k': 5},
          'init_checkpoint': None,
          'init_checkpoint_modules': 'all',
          'losses': {'l2_weight_decay': 0.0001,
                     'label_smoothing': 0.1,
                     'loss_weight': 1.0,
                     'one_hot': True,
                     'soft_labels': False},
          'model': {'add_head_batch_norm': False,
                    'backbone': {'resnet': {'bn_trainable': True,
                                            'depth_multiplier': 1.0,
                                            'model_id': 50,
                                            'replace_stem_max_pool': False,
                                            'resnetd_shortcut': False,
                                            'scale_stem': True,
                                            'se_ratio': 0.0,
                                            'stem_type': 'v0',
                                            'stochastic_depth_drop_rate': 0.0},
                                 'type': 'resnet'},
                    'dropout_rate': 0.0,
                    'input_size': [224, 224, 3],
                    'kernel_initializer': 'random_uniform',
                    'norm_activation': {'activation': 'relu',
                                        'norm_epsilon': 1e-05,
                                        'norm_momentum': 0.9,
                                        'use_sync_bn': False},
                    'num_classes': 1001},
          'model_output_keys': [],
          'name': None,
          'train_data': {'aug_policy': None,
                         'aug_rand_hflip': True,
                         'aug_type': None,
                         'block_length': 1,
                         'cache': True,
                         'color_jitter': 0.0,
                         'cycle_length': 10,
                         'decode_jpeg_only': True,
                         'decoder': {'simple_decoder': {'mask_binarize_threshold': None,
                                                        'regenerate_source_id': False},
                                     'type': 'simple_decoder'},
                         'deterministic': None,
                         'drop_remainder': True,
                         'dtype': 'float16',
                         'enable_tf_data_service': False,
                         'file_type': 'tfrecord',
                         'global_batch_size': 64,
                         'image_field_key': 'image/encoded',
                         'input_path': '/opt/ml/input/data/training/train-000*',
                         'is_multilabel': False,
                         'is_training': True,
                         'label_field_key': 'image/class/label',
                         'mixup_and_cutmix': None,
                         'randaug_magnitude': 10,
                         'random_erasing': None,
                         'seed': None,
                         'sharding': True,
                         'shuffle_buffer_size': 10000,
                         'tf_data_service_address': None,
                         'tf_data_service_job_name': None,
                         'tfds_as_supervised': False,
                         'tfds_data_dir': '',
                         'tfds_name': '',
                         'tfds_skip_decoding_feature': '',
                         'tfds_split': ''},
          'validation_data': {'aug_policy': None,
                              'aug_rand_hflip': True,
                              'aug_type': None,
                              'block_length': 1,
                              'cache': False,
                              'color_jitter': 0.0,
                              'cycle_length': 10,
                              'decode_jpeg_only': True,
                              'decoder': {'simple_decoder': {'mask_binarize_threshold': None,
                                                             'regenerate_source_id': False},
                                          'type': 'simple_decoder'},
                              'deterministic': None,
                              'drop_remainder': False,
                              'dtype': 'float16',
                              'enable_tf_data_service': False,
                              'file_type': 'tfrecord',
                              'global_batch_size': 2048,
                              'image_field_key': 'image/encoded',
                              'input_path': 'imagenet-2012-tfrecord/valid*',
                              'is_multilabel': False,
                              'is_training': False,
                              'label_field_key': 'image/class/label',
                              'mixup_and_cutmix': None,
                              'randaug_magnitude': 10,
                              'random_erasing': None,
                              'seed': None,
                              'sharding': True,
                              'shuffle_buffer_size': 10000,
                              'tf_data_service_address': None,
                              'tf_data_service_job_name': None,
                              'tfds_as_supervised': False,
                              'tfds_data_dir': '',
                              'tfds_name': '',
                              'tfds_skip_decoding_feature': '',
                              'tfds_split': ''}},
 'trainer': {'allow_tpu_summary': False,
             'best_checkpoint_eval_metric': '',
             'best_checkpoint_export_subdir': '',
             'best_checkpoint_metric_comp': 'higher',
             'checkpoint_interval': 1562,
             'continuous_eval_timeout': 3600,
             'eval_tf_function': True,
             'eval_tf_while_loop': False,
             'loss_upper_bound': 1000000.0,
             'max_to_keep': 5,
             'optimizer_config': {'ema': None,
                                  'learning_rate': {'stepwise': {'boundaries': [18750,
                                                                                37500,
                                                                                50000],
                                                                 'name': 'PiecewiseConstantDecay',
                                                                 'offset': 0,
                                                                 'values': [0.8,
                                                                            0.08,
                                                                            0.008,
                                                                            0.0008]},
                                                    'type': 'stepwise'},
                                  'optimizer': {'sgd': {'clipnorm': None,
                                                        'clipvalue': None,
                                                        'decay': 0.0,
                                                        'global_clipnorm': None,
                                                        'momentum': 0.9,
                                                        'name': 'SGD',
                                                        'nesterov': False},
                                                'type': 'sgd'},
                                  'warmup': {'linear': {'name': 'linear',
                                                        'warmup_learning_rate': 0,
                                                        'warmup_steps': 3125},
                                             'type': 'linear'}},
             'recovery_begin_steps': 0,
             'recovery_max_trials': 0,
             'steps_per_loop': 15,
             'summary_interval': 15,
             'train_steps': 1562,
             'train_tf_function': True,
             'train_tf_while_loop': True,
             'validation_interval': 625,
             'validation_steps': 25,
             'validation_summary_subdir': 'validation'}}
I0701 03:25:49.730910 140259922851648 train_utils.py:368] Saving experiment configuration to /opt/ml/model/params.yaml
INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK
Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: Tesla V100-SXM2-16GB, compute capability 7.0
I0701 03:25:49.752269 140259922851648 device_compatibility_check.py:117] Mixed precision compatibility check (mixed_float16): OK
Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: Tesla V100-SXM2-16GB, compute capability 7.0
WARNING:tensorflow:From /opt/ml/code/official/common/distribute_utils.py:155: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.
Instructions for updating:
use distribute.MultiWorkerMirroredStrategy instead
W0701 03:25:49.752446 140259922851648 deprecation.py:350] From /opt/ml/code/official/common/distribute_utils.py:155: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.
Instructions for updating:
use distribute.MultiWorkerMirroredStrategy instead
INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:1/device:CPU:0', '/job:worker/replica:0/task:1/device:GPU:0', '/job:worker/replica:0/task:0/device:CPU:0', '/job:worker/replica:0/task:0/device:GPU:0']
I0701 03:25:52.280277 140259922851648 collective_all_reduce_strategy.py:528] Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:1/device:CPU:0', '/job:worker/replica:0/task:1/device:GPU:0', '/job:worker/replica:0/task:0/device:CPU:0', '/job:worker/replica:0/task:0/device:GPU:0']
INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:0/device:CPU:0', '/job:worker/replica:0/task:0/device:GPU:0', '/job:worker/replica:0/task:1/device:CPU:0', '/job:worker/replica:0/task:1/device:GPU:0']
I0701 03:25:52.275715 140097427990336 collective_all_reduce_strategy.py:528] Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:0/device:CPU:0', '/job:worker/replica:0/task:0/device:GPU:0', '/job:worker/replica:0/task:1/device:CPU:0', '/job:worker/replica:0/task:1/device:GPU:0']
INFO:tensorflow:Check health not enabled.
I0701 03:25:52.730556 140259922851648 collective_all_reduce_strategy.py:571] Check health not enabled.
INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['algo-1:8890', 'algo-2:8890']}, task_type = 'worker', task_id = 1, num_workers = 2, local_devices = ('/job:worker/task:1/device:GPU:0',), communication = CommunicationImplementation.AUTO
I0701 03:25:52.730751 140259922851648 collective_all_reduce_strategy.py:573] MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['algo-1:8890', 'algo-2:8890']}, task_type = 'worker', task_id = 1, num_workers = 2, local_devices = ('/job:worker/task:1/device:GPU:0',), communication = CommunicationImplementation.AUTO
I0701 03:25:52.731844 140259922851648 train_utils.py:242] Running default trainer.
INFO:tensorflow:Check health not enabled.
I0701 03:25:52.600587 140097427990336 collective_all_reduce_strategy.py:571] Check health not enabled.
INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['algo-1:8890', 'algo-2:8890']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0/device:GPU:0',), communication = CommunicationImplementation.AUTO
I0701 03:25:52.600898 140097427990336 collective_all_reduce_strategy.py:573] MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['algo-1:8890', 'algo-2:8890']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0/device:GPU:0',), communication = CommunicationImplementation.AUTO
I0701 03:25:52.602246 140097427990336 train_utils.py:242] Running default trainer.
ip-10-0-140-42:46:207 [0] NCCL INFO Bootstrap : Using eth0:10.0.140.42<0>
ip-10-0-140-42:46:207 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.
ip-10-0-140-42:46:207 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.2.0aws
ip-10-0-140-42:46:207 [0] ofi_init:1157 NCCL WARN NET/OFI Only EFA provider is supported
ip-10-0-140-42:46:207 [0] ofi_init:1208 NCCL WARN NET/OFI aws-ofi-nccl initialization failed
ip-10-0-140-42:46:207 [0] NCCL INFO NET/IB : No device found.
ip-10-0-140-42:46:207 [0] NCCL INFO NET/Socket : Using [0]eth0:10.0.140.42<0> [1]veth-pause1:169.254.255.18<0>
ip-10-0-140-42:46:207 [0] NCCL INFO Using network Socket
.
.
.
INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = CommunicationImplementation.AUTO, num_packs = 1
I0701 03:26:01.234215 140259922851648 cross_device_ops.py:1152] Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = CommunicationImplementation.AUTO, num_packs = 1
I0701 03:26:02.539876 140259922851648 controller.py:391] restoring or initializing model...
I0701 03:26:02.540121 140259922851648 controller.py:397] initialized model.
restoring or initializing model...
initialized model.
I0701 03:26:02.540213 140259922851648 train_lib.py:110] Starts to execute mode: train
I0701 03:26:02.541172 140259922851648 controller.py:236] train | step:      0 | training until step 1562...
train | step:      0 | training until step 1562...
Extension horovod.torch has not been built: /usr/local/lib/python3.9/site-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-39-x86_64-linux-gnu.so not found
If this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.
Warning! MPI libs are missing, but python applications are still available.
I0701 03:26:02.633909 140097427990336 controller.py:391] restoring or initializing model...
restoring or initializing model...
I0701 03:26:02.634148 140097427990336 controller.py:397] initialized model.
I0701 03:26:02.634239 140097427990336 train_lib.py:110] Starts to execute mode: train
initialized model.
I0701 03:26:02.635231 140097427990336 controller.py:236] train | step:      0 | training until step 1562...
train | step:      0 | training until step 1562...
Extension horovod.torch has not been built: /usr/local/lib/python3.9/site-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-39-x86_64-linux-gnu.so not found
If this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.
Warning! MPI libs are missing, but python applications are still available.
[2022-07-01 03:26:03.457 ip-10-0-140-42.us-west-2.compute.internal:46 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None
[2022-07-01 03:26:03.368 ip-10-0-179-192.us-west-2.compute.internal:47 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None
/usr/local/lib/python3.9/site-packages/smdebug-1.0.17b20220608-py3.9.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: ""is not"" with a literal. Did you mean ""!=""?
/usr/local/lib/python3.9/site-packages/smdebug-1.0.17b20220608-py3.9.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: ""is not"" with a literal. Did you mean ""!=""?
[2022-07-01 03:26:03.695 ip-10-0-179-192.us-west-2.compute.internal:47 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.
[2022-07-01 03:26:03.822 ip-10-0-179-192.us-west-2.compute.internal:47 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.
[2022-07-01 03:26:03.823 ip-10-0-179-192.us-west-2.compute.internal:47 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.
[2022-07-01 03:26:03.823 ip-10-0-179-192.us-west-2.compute.internal:47 INFO hook.py:254] Saving to /opt/ml/output/tensors
[2022-07-01 03:26:03.824 ip-10-0-179-192.us-west-2.compute.internal:47 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.
[2022-07-01 03:26:03.824 ip-10-0-179-192.us-west-2.compute.internal:47 INFO keras.py:156] Disabling SMDebug as it does not support <tensorflow.python.distribute.collective_all_reduce_strategy._CollectiveAllReduceStrategyExperimental object at 0x7f903c54dca0>
/usr/local/lib/python3.9/site-packages/smdebug-1.0.17b20220608-py3.9.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: ""is not"" with a literal. Did you mean ""!=""?
/usr/local/lib/python3.9/site-packages/smdebug-1.0.17b20220608-py3.9.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: ""is not"" with a literal. Did you mean ""!=""?
[2022-07-01 03:26:03.803 ip-10-0-140-42.us-west-2.compute.internal:46 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.
[2022-07-01 03:26:03.915 ip-10-0-140-42.us-west-2.compute.internal:46 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.
[2022-07-01 03:26:03.915 ip-10-0-140-42.us-west-2.compute.internal:46 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.
[2022-07-01 03:26:03.916 ip-10-0-140-42.us-west-2.compute.internal:46 INFO hook.py:254] Saving to /opt/ml/output/tensors
[2022-07-01 03:26:03.916 ip-10-0-140-42.us-west-2.compute.internal:46 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.
[2022-07-01 03:26:03.917 ip-10-0-140-42.us-west-2.compute.internal:46 INFO keras.py:156] Disabling SMDebug as it does not support <tensorflow.python.distribute.collective_all_reduce_strategy._CollectiveAllReduceStrategyExperimental object at 0x7f6a66e368b0>
INFO:tensorflow:Collective all_reduce tensors: 161 all_reduces, num_devices = 1, group_size = 2, implementation = CommunicationImplementation.AUTO, num_packs = 1
I0701 03:26:06.524687 140259922851648 cross_device_ops.py:1152] Collective all_reduce tensors: 161 all_reduces, num_devices = 1, group_size = 2, implementation = CommunicationImplementation.AUTO, num_packs = 1
.
.
.
train | step:   1562 | steps/sec:    1.7 | output: 
    {'accuracy': 0.0078125,
     'learning_rate': 0.399872,
     'top_5_accuracy': 0.03125,
     'training_loss': 7.470139}
I0701 03:41:26.745199 140097427990336 controller.py:486] saved checkpoint to /opt/ml/model/ckpt-1562.
saved checkpoint to /opt/ml/model/ckpt-1562.
I0701 03:41:26.754823 140097427990336 train_lib.py:137] Number of trainable params in model: 25.612201 Millions.
I0701 03:41:26.743883 140259922851648 controller.py:486] saved checkpoint to /opt/ml/model/ckpt-1562.
saved checkpoint to /opt/ml/model/ckpt-1562.
.
.
.
INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = CommunicationImplementation.AUTO, num_packs = 1
I0701 03:41:28.853216 140097427990336 cross_device_ops.py:1152] Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = CommunicationImplementation.AUTO, num_packs = 1

2022-07-01 03:41:44 Uploading - Uploading generated training modelWARNING:tensorflow:From /usr/local/lib/python3.9/site-packages/tensorflow/python/ops/nn_ops.py:5219: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`
W0701 03:41:36.295221 140259922851648 deprecation.py:350] From /usr/local/lib/python3.9/site-packages/tensorflow/python/ops/nn_ops.py:5219: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`
I0701 03:41:37.068754 140259922851648 train_lib.py:142] FLOPs (multi-adds) in model: 4.090167 Billions.
I0701 03:41:37.069428 140259922851648 train_utils.py:377] Saving gin configurations to /opt/ml/model/operative_config.train.gin
2022-07-01 03:41:37.535436: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
WARNING:tensorflow:From /usr/local/lib/python3.9/site-packages/tensorflow/python/ops/nn_ops.py:5219: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`
W0701 03:41:36.922152 140097427990336 deprecation.py:350] From /usr/local/lib/python3.9/site-packages/tensorflow/python/ops/nn_ops.py:5219: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`
I0701 03:41:37.731269 140097427990336 train_lib.py:142] FLOPs (multi-adds) in model: 4.090167 Billions.
I0701 03:41:37.731925 140097427990336 train_utils.py:377] Saving gin configurations to /opt/ml/model/operative_config.train.gin
terminate called without an active exception
Fatal Python error: Aborted
Thread 0x00007f6af96de740 (most recent call first):
<no Python frame>
Aborted
```
</details>"
56683,tflite_runtime problem on Arm64 linux with multiprocessing,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: no
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux (Arm64)
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: n/a
-   **TensorFlow installed from (source or binary)**: binary
-   **TensorFlow version (use command below)**: 2.4.1
-   **Python version**: 3.6 (PC), 3.7 (embedded Linux)
-   **Bazel version (if compiling from source)**: n/a
-   **GCC/Compiler version (if compiling from source)**: n/a
-   **CUDA/cuDNN version**: 9
-   **GPU model and memory**: Titan xp
-   **Exact command to reproduce**:


### Describe the problem
I trained a neural network using quantization-aware training from tensorflow_model_optimization and converted it to a quantized tflite model. I have a python script that runs on a device with Linux (Arm64, 4 cores) and a NPU. This script uses the tflite_runtime module to make inference on the NPU with the NNAPI delegate:

> INFO: Created TensorFlow Lite delegate for NNAPI.
> Applied NNAPI delegate.

This script uses multiprocessing to perform other operations. The inference is made on the main process while other operations are executed like this:

```
p = Process(target=my_function, args=(my_args,))				
p.start()
```
The problem is that when I start a new process, the inference stops working: the neural network keeps returning the same output, no matter which input I provide. Even when the child process ends, the problem remains. This does not happen if I use threads instead of process.

### Source code / logs

Quantization (on the training PC):
```
import tensorflow as tf
import tensorflow_model_optimization as tfmot
with tfmot.quantization.keras.quantize_scope():
    model = tf.keras.models.load_model(path)

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
quantized_tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(quantized_tflite_model)
```


Interpreter initialization (on the embedded linux):

```
import tflite_runtime.interpreter as tflite
interpreter = tflite.Interpreter(model_path=my_path)
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
input_shape = input_details[0]['shape']
```

Inference:

```
interpreter.set_tensor(input_details[0]['index'], my_input)				
interpreter.invoke()			
output = interpreter.get_tensor(output_details[0]['index'])
```

Pseudo-code of the script:

```
initialize_itnerpreter()
while True:
    acquire_input()
    perform_inference()
    if condition:
        p = Process(target=my_function, args=(my_args,))				
        p.start()
```"
56681,"In tensorflow2.6, when in my keras.model uses yaml to load data, it will turn the array into a listener","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tensorflow 2.6.0

### Custom Code

No

### OS Platform and Distribution

windows 10

### Mobile device

no use phone

### Python version

python3.8.8

### Bazel version

from pip install

### GCC/Compiler version

from pip install

### CUDA/cuDNN version

cuda 11.0

### GPU model and memory

from pip install

### Current Behaviour?

```shell
what bug?:
when i was extends keras.model and in there use yaml load .yaml file, the yaml file array data was changed to listwrapper,but if i not in my model,yaml file was success load array data,if in Same py folder but not in my model class ,yaml file was success load array data
i need what?:
i want in my model the yaml file can be surcess load was array not was listwarpper
```


### Standalone code to reproduce the issue

```shell
class Model(keras.Model):
    def __init__(self,cfg='/models/yolov5s.yaml',ch=3,nc=None):
        super(Model,self).__init__()
        if isinstance(cfg,dict):
            yaml = cfg
        else :
            import yaml as y
            yaml_file=Path(cfg).name
            with open(cfg) as f:
                yaml = y.load(f,Loader=y.FullLoader)
```


### Relevant log output

_No response_</details>"
56680,Conv2D returns empty tensor of type float32 in case of bfloat16 input.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux

### Mobile device

Linux Ubuntu 20.04

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Conv2D returns empty tensor of type float32 in case of bfloat16 input.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

for dtype in [tf.float32, tf.bfloat16]:

    strides = tf.constant([1, 1, 1, 1], dtype=tf.int32)

    input =   tf.constant(np.full(fill_value=1., shape=[1, 56, 56, 1]), dtype=dtype)
    filters = tf.constant(np.full(fill_value=1., shape=[1, 56, 1, 1]), dtype=dtype)

    padding = ""SAME""

    output = tf.nn.conv2d(input, filters, strides, padding)

    print("""")
    print(""dtype:        "", dtype)
    print(""Input size:   "", input.shape.num_elements())
    print(""Output size:  "", output.shape.num_elements())
    print(""Output dtype: "", output.dtype)
    assert output.shape.num_elements() > 0
```


### Relevant log output

```shell
dtype:         <dtype: 'float32'>
Input size:    3136
Output size:   3136
Output dtype:  <dtype: 'float32'>

dtype:         <dtype: 'bfloat16'>
Input size:    3136
Output size:   0
Output dtype:  <dtype: 'float32'>
collected 0 items / 1 errors                                                                                                                                                                                                          

=============================================================================================================== ERRORS ================================================================================================================
______________________________________________________________________________________________________ ERROR collecting repro.py ______________________________________________________________________________________________________
repro.py:20: in <module>
    assert output.shape.num_elements() > 0
E   assert 0 > 0
E    +  where 0 = <bound method TensorShape.num_elements of TensorShape([0])>()
E    +    where <bound method TensorShape.num_elements of TensorShape([0])> = TensorShape([0]).num_elements
E    +      where TensorShape([0]) = <tf.Tensor: shape=(0,), dtype=float32, numpy=array([], dtype=float32)>.shape
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Interrupted: 1 errors during collection !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
======================================================================================================= 1 error in 2.33 seconds =======================================================================================================
```
</details>"
56679,Differing Tensorflow versions when checking,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.8.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I recently installed the latest version of Tensorflow (2.9.1) and when I went to check the version in my local Jupyter Notebook, it constantly said 2.4.0. I expected to see 2.9.1, so I don't understand why it is showing 2.4.0. I tried uninstalling and reinstalling Tensorflow, but still got the same issue.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.version.VERSION
!pip show tensorflow
```


### Relevant log output

```shell
C:\Users\danie>pip show tensorflow
Name: tensorflow
Version: 2.9.1
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author: Google Inc.
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: c:\users\danie\appdata\local\programs\python\python38\lib\site-packages
Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, keras-preprocessing, libclang, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt
Required-by: autokeras, scikeras, tensorflow-federated

C:\Users\danie>python -c ""import tensorflow as tf; print(tf.__version__)""
2.4.0

C:\Users\danie>python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
v2.4.0-0-gbe3d1f7dcd5 2.4.0
```
</details>"
56678,Tensorflow failed to build on MSVC with message Executing genrule //tensorflow:tf_python_api_gen_v2 failed: (Exit 1): bash.exe failed: error executing command ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.9

### Custom Code

No

### OS Platform and Distribution

Windows server 2019

### Mobile device

_No response_

### Python version

3.9.7

### Bazel version

5.2.0

### GCC/Compiler version

msvc latest build

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
ERROR: F:/gitp/tensorflow/tensorflow/tensorflow/BUILD:1393:19: Executing genrule //tensorflow:tf_python_api_gen_v2 failed: (Exit 1): bash.exe failed: error executing command 
ERROR: F:/gitp/tensorflow/tensorflow/tensorflow/BUILD:1393:19: Executing genrule //tensorflow:tf_python_api_gen_v2 failed: (Exit 1): bash.exe failed: error executing command 
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: F:/gitp/tensorflow/tensorflow/tensorflow/python/tools/BUILD:225:10 Middleman _middlemen/tensorflow_Spython_Stools_Soptimize_Ufor_Uinference.exe-runfiles failed: (Exit 1): bash.exe failed: error executing command 
ERROR: F:/gitp/tensorflow/tensorflow/tensorflow/python/tools/BUILD:225:10 Middleman _middlemen/tensorflow_Spython_Stools_Soptimize_Ufor_Uinference.exe-runfiles failed: (Exit 1): bash.exe failed: error executing command 
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
```


### Standalone code to reproduce the issue

```shell
Repro steps:
1. git clone https://github.com/tensorflow/tensorflow.git F:\gitP\tensorflow\tensorflow
2. cd F:\gitP\tensorflow\tensorflow
3. pip3 install six numpy wheel 2>&1
4. pip3 install keras_applications==1.0.6 --no-deps 2>&1
5. pip3 install keras_preprocessing==1.0.5 --no-deps 2>&1
6. set PATH=F:\gitP\tensorflow\tensorflow\..\tools;%path%
   set PATH=F:\gitP\tensorflow\tensorflow\..\tools\msys64\usr\bin;%path%
7. yes """" 2>nul | python ./configure.py 2>&1
8. set BAZEL_VC=C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC
   set BAZEL_VC_FULL_VERSION=14.29.30133
9. bazel --output_user_root F:\bazelTemp build --config=opt --subcommands //tensorflow/tools/pip_package:build_pip_package 2>&1

More info:
We use the master branch and commit is 73c8e20, VS version is VS2019 (VS16.11.11)


Could you help take a look? Thanks. I attached a build.zip file, you can check the details in it.
```


### Relevant log output

_No response_</details>
[build.zip](https://github.com/tensorflow/tensorflow/files/9043565/build.zip)
"
56677,What is the difference between model.__call__ on TensorSpec and Tensor,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Source

binary

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I have a subclassed Keras model which uses a custom layer. 

I find that model.__call__ on TensorSpec behaves differently from model.__call__ on Tensor.
For model.__call__ on TensorSpec, I cannot see the logs from Model.call() and Layer.call().
For model.__call__ on Tensor, I can see these logs.

Please note that I use tf.print() within Model.call() and Layer.call() to print the logs.

The reason why I ask this is that I want to confirm whether the internal forward step is executed when TensorSpec is fed. The use case is that the execution of the custom layer is depending on a service, which is not started before I save the model graph. As you may know, saving the model graph requires model.__call__() first, and I don't want this model.__call__() on TensorSpec to invoke the actual execution of the custom layer.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
import os

os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""

def test(values):
    vector  = tf.zeros_like(values)
    return vector


class TestLayer(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(TestLayer, self).__init__(**kwargs)

    # @tf.function
    def call(self, inputs, training=False):
        tf.print(""=============TestLayer.call()============"")
        emb_vector = test(values = inputs)
        return emb_vector

class Demo(tf.keras.models.Model):
    def __init__(self, **kwargs):
        super(Demo, self).__init__(**kwargs)
        
        self.test_layer = TestLayer()        
        self.dense_layer = tf.keras.layers.Dense(units=1, activation=None,
                                                 kernel_initializer=""ones"",
                                                 bias_initializer=""zeros"")

    def call(self, inputs):
        tf.print(""=============Demo.call()============"")
        vector = self.test_layer(inputs)
        logit = self.dense_layer(vector)
        return logit, vector

    def summary(self):
        inputs = tf.keras.Input(shape=(10,))
        model = tf.keras.models.Model(inputs=inputs, outputs=self.call(inputs))
        return model.summary()

global_batch_size = 16384
input1 = tf.keras.Input(shape=(10,))
input2 = tf.ones(shape=(global_batch_size,10))

model = Demo()
print(""**************Feed TensorSpec*************"")
logit1, vector1 = model(input1)

print(""**************Feed Tensor*************"")
logit2, vector2 = model(input2)
```


### Relevant log output

```shell
2022-07-05 01:40:06.508993: I tensorflow/core/platform/cpu_feature_guard.cc:152] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-07-05 01:40:07.020352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14649 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:06:00.0, compute capability: 7.0
**************Feed TensorSpec*************
**************Feed Tensor*************
=============Demo.call()============
=============TestLayer.call()============
```
</details>"
56676,Save and load model within MirroredStrategy,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18.04.5

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

Tesla V100-SXM2-16GB x 4

### Current Behaviour?

```shell
I try to save and load the model within MirroredStrategy. However, strategy.run() does not work as expected for the loaded model since only one device has been utilized whereas there are four devices.

It can be seen from the log that the directly constructed model works as expected, but the loaded model does not.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import tensorflow.distribute as tf_dist
from tensorflow.python.distribute import distribute_lib
from tensorflow.python.ops import array_ops
import numpy as np
import os

os.environ[""CUDA_VISIBLE_DEVICES""] = ""0,1,2,3""

def _get_current_replica_id_in_group_sync():
    replica_ctx = tf_dist.get_replica_context()
    if replica_ctx:
        replica_id = replica_ctx.replica_id_in_sync_group
    else:
        replica_id = distribute_lib.get_update_replica_id()
    if replica_id is None:
        replica_id = array_ops.constant(0, dtype=array_ops.dtypes.int32)
    return replica_id

def test(values):
    global_replica_id = _get_current_replica_id_in_group_sync()
    tf.print(""global_replica_id: {}"".format(global_replica_id))
    vector  = tf.zeros_like(values)
    return vector


class TestLayer(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(TestLayer, self).__init__(**kwargs)

    # @tf.function
    def call(self, inputs, training=False):
        emb_vector = test(values = inputs)
        return emb_vector

class Demo(tf.keras.models.Model):
    def __init__(self, **kwargs):
        super(Demo, self).__init__(**kwargs)
        
        self.test_layer = TestLayer()        
        self.dense_layer = tf.keras.layers.Dense(units=1, activation=None,
                                                 kernel_initializer=""ones"",
                                                 bias_initializer=""zeros"")

    def call(self, inputs):
        vector = self.test_layer(inputs)
        logit = self.dense_layer(vector)
        return logit, vector

    def summary(self):
        inputs = tf.keras.Input(shape=(10,), dtype=tf.int64)
        model = tf.keras.models.Model(inputs=inputs, outputs=self.call(inputs))
        return model.summary()

@tf.function
def _step(inputs, labels, model):
    logit, vector = model(inputs)
    return logit, vector

def tf_dataset(keys, labels, batchsize, repeat):
    dataset = tf.data.Dataset.from_tensor_slices((keys, labels))
    dataset = dataset.repeat(repeat)
    dataset = dataset.batch(batchsize, drop_remainder=True)
    return dataset

def _dataset_fn(input_context):
    global_batch_size = 16384
    keys = np.ones((global_batch_size, 10))
    labels = np.random.randint(low=0, high=2, size=(global_batch_size, 1))
    replica_batch_size = input_context.get_per_replica_batch_size(global_batch_size)
    dataset = tf_dataset(keys, labels, batchsize=replica_batch_size, repeat=1)
    dataset = dataset.shard(input_context.num_input_pipelines, input_context.input_pipeline_id)
    return dataset

# Save model within MirroredStrategy scope
strategy = tf.distribute.MirroredStrategy([""GPU:0"", ""GPU:1"", ""GPU:2"", ""GPU:3""])
with strategy.scope():
    model = Demo()
model.compile()
model.summary()
dataset = strategy.distribute_datasets_from_function(_dataset_fn)
for i, (key_tensors, replica_labels) in enumerate(dataset):
    print(""-"" * 30, ""step "", str(i), ""-"" * 30)
    logit, vector = strategy.run(_step, args=(key_tensors, replica_labels, model))
# model(tf.keras.Input(shape=(10,), dtype=tf.int64))
model.save(""demo"")

# Load model within MirroredStrategy scope
with strategy.scope():
    model2 = tf.keras.models.load_model(""demo"")
dataset = strategy.distribute_datasets_from_function(_dataset_fn)
for i, (key_tensors, replica_labels) in enumerate(dataset):
    print(""-"" * 30, ""step "", str(i), ""-"" * 30)
    logit, vector = strategy.run(_step, args=(key_tensors, replica_labels, model2))

# print(logit)
```


### Relevant log output

```shell
2022-07-05 00:39:03.555988: I tensorflow/core/platform/cpu_feature_guard.cc:152] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-07-05 00:39:05.281631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14649 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:06:00.0, compute capability: 7.0
2022-07-05 00:39:05.282973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 14649 MB memory:  -> device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:07:00.0, compute capability: 7.0
2022-07-05 00:39:05.284320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 14649 MB memory:  -> device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:0a:00.0, compute capability: 7.0
2022-07-05 00:39:05.285540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 14649 MB memory:  -> device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:0b:00.0, compute capability: 7.0
Model: ""model""
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 input_1 (InputLayer)        [(None, 10)]              0

 test_layer (TestLayer)      (None, 10)                0

 dense (Dense)               (None, 1)                 11

=================================================================
Total params: 11
Trainable params: 11
Non-trainable params: 0
_________________________________________________________________
------------------------------ step  0 ------------------------------
global_replica_id: Tensor(""demo/test_layer/replica_id_in_sync_group:0"", shape=(), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)
global_replica_id: Tensor(""replica_1/demo/test_layer/replica_id_in_sync_group:0"", shape=(), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:1)
global_replica_id: Tensor(""replica_2/demo/test_layer/replica_id_in_sync_group:0"", shape=(), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:2)
global_replica_id: Tensor(""replica_3/demo/test_layer/replica_id_in_sync_group:0"", shape=(), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:3)
2022-07-05 00:39:07.253746: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
------------------------------ step  0 ------------------------------
global_replica_id: 0
```
</details>"
56675,`UniformIntDistribution` is exclusive in its upper bound,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source (I actually use [this binary](https://github.com/elixir-nx/xla/releases/tag/v0.3.0) but that isn't compiled by Google)

### Tensorflow Version

2.8

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

n/a

### Bazel version

2.4.1

### GCC/Compiler version

Unknown, between 7.5 and 9.3

### CUDA/cuDNN version

CUDA Version: 11.6

### GPU model and memory

NVIDIA GeForce GTX 1070

### Current Behaviour?

`UniformIntDistribution` appears to be inclusive in its lower bound, but exclusive in its upper bound. I think this is wrong for a number of reasons:

* There is no simple way to get uniform samples that include the maximum U64.
* Setting both bounds to be the same produces seemingly undefined behaviour*
* Its sister function `UniformFloatingPointDistribution` appears to be _inclusive_ in its upper bound. I came to this conclusion as sampling that function with equal bounds returns the common bound, rather than, say NaN. The two functions thus have incongruous behaviour.

*Example samples for equal bounds of 0 and 0
```
[
      11077253088097075545
    , 13897614985444391724
    , 18164676955841373932
    , 376028057765135569
    , 12082511028297777851
    , 1029834464974463124
    , 12380146588138085609
    , 8561165853724746330
    , 9319215302267380863
    , 16134235052671214906
]
```

### Standalone code to reproduce the issue

```c++
void Test() {
    xla::XlaBuilder builder("""");
    auto zero = xla::ConstantR0<uint64_t>(&builder, 0);
    zero = xla::BroadcastInDim(zero, {1000}, {});
    auto one = xla::ConstantR0<uint64_t>(&builder, 1);
    one = xla::BroadcastInDim(one, {1000}, {});
    auto key = xla::ConstantR0<uint64_t>(&builder, 0);
    auto state = xla::ConstantR0<uint64_t>(&builder, {0});
    auto shape = xla::ShapeUtil::MakeShape(xla::U64, {1000});

    auto sample = xla::UniformIntDistribution(
        key, state, xla::ThreeFryBitGenerator, zero, one, shape
    ).value;
    auto anyEqOne = xla::Any(xla::Eq(sample, one));

    auto computation = builder.Build(anyEqOne).ConsumeValueOrDie();
    auto res =
        xla::ClientLibrary::LocalClientOrDie()
        ->ExecuteAndTransfer(computation, {})
        .ConsumeValueOrDie()
        .ToString();

    std::cout << res << std::endl;
}
```


### Relevant log output

```shell
pred[] false
```
</details>"
56674,Tensorflow predict() is accurate on single sample but giving strange results when predicting on dataset,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

binary

### Tensorflow Version

2.8.0

### Custom Code

No

### OS Platform and Distribution

Issue on Windows 11  (with CUDA) and MacOS 12.4 (CPU)

### Mobile device

_No response_

### Python version

3.8 (Windows) and 3.9.5 (MacOS)

### Bazel version

n/a

### GCC/Compiler version

n/a

### CUDA/cuDNN version

CUDA 11.4, driver version 471.41

### GPU model and memory

RTX3070 / 8GB

### Current Behaviour?


I have trained a model on the Stanford Dogs dataset and get accuracy around 80% (training acc) and 74% (validation acc). 

When I predict using the trained model using `tf_preds = model.predict(test_images)` I get bad predictions (see relevant log output below)

If I iterate through the dataset using a `for` loop, I get very good predictions (see relevant log output below)

I do not understand why this problem occurs. I have raised this on Stackoverflow but had no responses https://stackoverflow.com/questions/72801723/tensorflow-predict-is-accurate-on-single-sample-but-giving-strange-results-whe

I get the same issue when predicting on Windows (env details above) or Mac OS (env details above) with the same trained model.



### Standalone code to reproduce the issue


Here is a link to the notebook in Colab:
 
https://colab.research.google.com/drive/1y2-T_D3bQOXbs3tg_UKyYKZVINxb5vF_?usp=sharing

You can visibly see the issue in the confusion matrices at the bottom of the notebook.

```shell

import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow_datasets as tfds

import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sns

dataset, info = tfds.load(name=""stanford_dogs"", with_info=True)
training_data = dataset['train']
val_data = dataset['test']

IMG_LEN = 224
IMG_SHAPE = (IMG_LEN,IMG_LEN,3)
N_BREEDS = 120

def preprocess(ds_row):
    image = tf.image.convert_image_dtype(ds_row['image'], dtype=tf.float32)
    image = tf.image.resize(image, (IMG_LEN, IMG_LEN), method='nearest')
    
    label = tf.one_hot(ds_row['label'], N_BREEDS)  # TODO: Can remove one_hot and change loss function
    return image, label

def prepare(dataset, batch_size=None):
    ds = dataset.map(preprocess, num_parallel_calls=4)
    ds = ds.shuffle(buffer_size=1000)
    if batch_size:
        ds = ds.batch(batch_size)
        ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
    return ds

train_batches = prepare(training_data, batch_size=32)
val_batches = prepare(val_data, batch_size=32)

base_model = tf.keras.applications.MobileNetV2(input_shape = IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')

base_model.trainable = False

model = tf.keras.Sequential([
  base_model,
  tf.keras.layers.GlobalAveragePooling2D(),
  tf.keras.layers.Dense(N_BREEDS, activation='softmax')
])

model.compile(optimizer=tf.keras.optimizers.Adamax(0.0001),
              loss='categorical_crossentropy',
              metrics=['accuracy', 'top_k_categorical_accuracy'])

history = model.fit(train_batches,
                    epochs=30,
                    validation_data=val_batches)

# Split out test images and test labels
test_images = dataset['test'].map(
    lambda x:  (tf.image.resize(x['image'], (IMG_LEN, IMG_LEN), method='nearest'))
).batch(1)

_test_labels = dataset['test'].map(
    lambda y: (y['label'])
)
test_labels = [l.numpy() for l in _test_labels]

# Use for loop to get good predictions
_preds = []
_argmax = []
_actuals = []

for dog, label in zip(dataset['test'].take(n), test_labels[:n]):

    pic, _ = preprocess(dog)  # Convert to float32 and resize

    img_tensor = tf.expand_dims(pic,0)
    pred = model(img_tensor)
    
    am = tf.argmax(pred, axis=1)
    _argmax.append(am)

    top_components = tf.reshape(tf.math.top_k(pred, k=5).indices,shape=[-1])
    top_matches = [get_name(i) for i in top_components]
    actual = get_name(label)

    _preds.append(top_components[0])
    _actuals.append(label)

conf_matrix = tf.math.confusion_matrix(
    _actuals,
    _argmax
)

plt.figure(figsize=(20,15))
sns.heatmap(conf_matrix)
plt.ylabel('True label')
plt.xlabel('Predicted label')

# Now using model.predict()
tf_preds = model.predict(test_images)
preds = np.array(tf.math.argmax(tf_preds, axis=1))

conf_matrix = tf.math.confusion_matrix(
    _actuals,
    preds
)

plt.figure(figsize=(20,15))
sns.heatmap(conf_matrix)
plt.ylabel('True label')
plt.xlabel('Predicted label')

```

Note it does not matter whether I run `model.predict()` or the `for` loop first, I get the same result. And I have checked that I am not shuffling the data too (shuffling leads to similar results with `model.predict()`, as in it doesn't help)

### Relevant log output

```
# Ground truth labels
print(test_labels)
[67, 84, 57, 12, 88, 32, 55, 9, 68, 99, 10, 1, 60, 52, 96, 33, 108, 71, 11, 75, 77, 9, 50, 19, 41, 118, 30...]

# using model.predict() - low accuracy
print(tf_preds)
[12  65   0  65  56  65  91  65  12  50   0  65   0  65   0   0  65  65   58  65   0  65   0   0  65  65  65  65  65  50  65  65  65  97  65  65...]

# using for loop and predicting one image at a time - high accuracy
print(_preds)
[ 67  84  57   8  88  32  55   9  68  99  15   1  17  52  96  33 108  71...]
```
</details>"
56673,"Unit test fails to build, others fail on AARCH64 after XLA CPU backend enhancements","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

CentOS 7

### Mobile device

n/a

### Python version

3.8.10

### Bazel version

5.1.1

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

```shell
Compiling with --config=mkl_aarch64 the unit test //tensorflow/python/tools:aot_compiled_test fails to build with

ERROR: /tmp/workspace/tensorflow-oneDNN-ACL-git/tensorflow/python/tools/BUILD:498:11: Linking tensorflow/python/tools/aot_compiled_test failed: (Exit 1): gcc failed: error executing command
(cd /root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow &&
exec env -
LD_LIBRARY_PATH=/opt/rh/devtoolset-10/root/usr/lib64:/opt/rh/devtoolset-10/root/usr/lib:/opt/rh/devtoolset-10/root/usr/lib64/dyninst:/opt/rh/devtoolset-10/root/usr/lib/dyninst:/usr/local/lib64
PATH=/root/.cache/bazelisk/downloads/bazelbuild/bazel-5.1.1-linux-arm64/bin:/tmp/workspace/venv-cp38-cp38/bin:/opt/rh/devtoolset-10/root/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
PWD=/proc/self/cwd
PYTHON_BIN_PATH=/tmp/workspace/venv-cp38-cp38/bin/python3
PYTHON_LIB_PATH=/tmp/workspace/venv-cp38-cp38/lib/python3.8/site-packages
TF2_BEHAVIOR=1
/opt/rh/devtoolset-10/root/usr/bin/gcc @bazel-out/aarch64-opt/bin/tensorflow/python/tools/aot_compiled_test-2.params)
Configuration: 3f75d29a2f4aef411fd2f59b82df7437222f52171a4aa87840162de83074dda9
Execution platform: @local_execution_config_platform//:platform

bazel-out/aarch64-opt/bin/_solib_aarch64/libtensorflow_Spython_Stools_Slibaot_Ucompiled_Ux_Umatmul_Uy_Ularge_Umultithreaded.so: error: undefined reference to '__xla_cpu_runtime_ACLMatMulF32'
collect2: error: ld returned 1 exit status
Target //tensorflow/python/tools:aot_compiled_test failed to build
INFO: Elapsed time: 430.002s, Critical Path: 383.37s
INFO: 509 processes: 6 internal, 503 local.
FAILED: Build did NOT complete successfully
//tensorflow/python/tools:aot_compiled_test FAILED TO BUILD

FAILED: Build did NOT complete successfully

Also other unit tests now fail.
//tensorflow/compiler/tests:conv2d_test_cpu_mlir_bridge_test
//tensorflow/compiler/xla/service/cpu/tests:cpu_eigen_dot_operation_test
//tensorflow/compiler/tests:depthwise_conv_op_test_cpu
//tensorflow/compiler/tests:depthwise_conv_op_test_cpu_mlir_bridge_test
//tensorflow/compiler/tests:conv2d_test_cpu
//tensorflow/compiler/xla/tests:convolution_dimension_numbers_test_cpu
//tensorflow/compiler/xla/tests:convolution_variants_test_cpu
//tensorflow/compiler/xla/tests:convolution_test_cpu
//tensorflow/compiler/xla/tests:conv_depthwise_backprop_filter_test_cpu
```


### Standalone code to reproduce the issue

```shell
bazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --noremote_accept_cached --config=nonccl --config=mkl_aarch64 --copt=""-mtune=generic"" --copt=""-march=armv8-a"" --copt=""-O3"" --test_env=TF_ENABLE_ONEDNN_OPTS=1 --copt=""-fopenmp"" --linkopt=""-lgomp"" --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --verbose_failures --jobs=75 --build_tests_only -- //tensorflow/python/tools:aot_compiled_test
```


### Relevant log output

```shell
https://ci.linaro.org/job/ldcg-python-manylinux-tensorflow-onednn-nightly/108/consoleText

ERROR: /tmp/workspace/tensorflow-oneDNN-ACL-git/tensorflow/python/tools/BUILD:498:11: Linking tensorflow/python/tools/aot_compiled_test failed: (Exit 1): gcc failed: error executing command
      (cd /root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow && \
      exec env - \
        LD_LIBRARY_PATH=/opt/rh/devtoolset-10/root/usr/lib64:/opt/rh/devtoolset-10/root/usr/lib:/opt/rh/devtoolset-10/root/usr/lib64/dyninst:/opt/rh/devtoolset-10/root/usr/lib/dyninst:/usr/local/lib64 \
        PATH=/root/.cache/bazelisk/downloads/bazelbuild/bazel-5.1.1-linux-arm64/bin:/tmp/workspace/venv-cp37-cp37m/bin:/opt/rh/devtoolset-10/root/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
        PWD=/proc/self/cwd \
        PYTHON_BIN_PATH=/tmp/workspace/venv-cp37-cp37m/bin/python3 \
        PYTHON_LIB_PATH=/tmp/workspace/venv-cp37-cp37m/lib/python3.7/site-packages \
        TF2_BEHAVIOR=1 \
      /opt/rh/devtoolset-10/root/usr/bin/gcc @bazel-out/aarch64-opt/bin/tensorflow/python/tools/aot_compiled_test-2.params)
    # Configuration: 4e7de1e342fde1b2946298612567ae8e6e1dc07899c92f6eb687f4d6e2fb7650
    # Execution platform: @local_execution_config_platform//:platform
    bazel-out/aarch64-opt/bin/_solib_aarch64/libtensorflow_Spython_Stools_Slibaot_Ucompiled_Ux_Umatmul_Uy_Ularge_Umultithreaded.so: error: undefined reference to '__xla_cpu_runtime_ACLMatMulF32'
    collect2: error: ld returned 1 exit status
    INFO: Elapsed time: 2135.511s, Critical Path: 497.42s
    INFO: 20956 processes: 9447 internal, 11509 local.
    FAILED: Build did NOT complete successfully
    FAILED: Build did NOT complete successfully
```
</details>"
56672,AutoGraph could not transform function,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18 (colab)

### Mobile device

_No response_

### Python version

3.7.13

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2

### GPU model and memory

NVIDIA Tesla T4

### Current Behaviour?

```shell
I get the following warnings: 

WARNING:tensorflow:AutoGraph could not transform <function train at 0x7f0be336b5f0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: name 'fscope' is not defined
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
```


### Standalone code to reproduce the issue

```shell
@tf.function
def contains_pad(inp: tf.Tensor):
    tf.debugging.assert_type(inp, tf.int32)
    bool_ten = tf.math.equal(inp, pad_ten)
    nonzero_count = tf.math.count_nonzero(bool_ten)
    return nonzero_count > 0

@tf.function(input_signature=(tf.TensorSpec(shape=[batch_size, None], dtype=tf.int32),
                              tf.TensorSpec(shape=[batch_size, set_size], dtype=tf.int32)))
def train_step(inp: tf.Tensor, outp: tf.Tensor) -> tf.Tensor:
    with tf.GradientTape() as tape:
        pred: tf.Tensor = model([inp, outp], training=True) 
        loss_val: tf.Tensor = tf.keras.losses.sparse_categorical_crossentropy(y_true = outp, y_pred = pred, from_logits = False)
    grads: tf.RaggedTensor = tape.gradient(loss_val, model.trainable_weights)
    optimizer.apply_gradients(zip(grads, model.trainable_weights))
    return tf.math.reduce_mean(loss_val)


@tf.function(input_signature=[tf.TensorSpec(shape=[batch_size, max_seq_len], dtype=tf.int32)])
def train(batch: tf.Tensor) -> tf.TensorSpec(shape=[], dtype=tf.keras.backend.floatx()):
    per_generation_loss: tf.Tensor = tf.zeros([num_sets], dtype=tf.keras.backend.floatx())
    i = 0
    while i < num_sets:
        # The input is of size set_size-TAKE_TO_ACCOUNT
        already_predicted: int = i * (set_size + 1)
        start_from: int = max(0, already_predicted - max_seq_len)
        inp: tf.Tensor = batch[:, start_from:(i + 1) * set_size]
        tf.debugging.assert_type(inp, tf.int32)
        have_pad = tf.map_fn(contains_pad, inp, dtype=tf.bool, parallel_iterations=16)
        if tf.get_static_value(tf.math.reduce_all(have_pad)):
            break
        outp: tf.TensorSpec(shape=[batch_size, set_size]) = batch[:, (i + 1) * set_size:(i + 2) * set_size]
        loss_val: tf.TensorSpec(shape=[], dtype=tf.keras.backend.floatx()) = val_step(inp, outp)
        one_hot: tf.TensorSpec(shape=[num_sets], dtype=tf.keras.backend.floatx())
        one_hot = tf.one_hot([i], num_sets, dtype=tf.keras.backend.floatx()) * loss_val
        per_generation_loss += one_hot
        i += 1
    return tf.math.reduce_mean(per_generation_loss[:i])

sam = tf.ramndom.uniform(shape=[64, 265], dtype=tf.int32)
train(sam)
```


### Relevant log output

_No response_</details>"
56671,TF's statically linked OpenMP causes performance problems with other libraries,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux CentOS

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hey TF team,

It seems that the PYPI version of TF is statically linked to its own OpenMP
implementation. This however can interfere when using multiple libraries that
rely on OpenMP.

The problem is, that the TF OpenMP does not free the threads immediately, but
spins for a certain amount of time, in case more work is coming in. When the
data is passed onto other libraries, these need to wait till TF put's it's
OpenMP threads to sleep. This causes the other library to wait, ruining the
performance of the entire pipeline. In our case the performance drop was 3-4x
compared to the pure runtime within TF and our library.

You can see that TF comes with it's own OpenMP implementation using
```OMP_DISPLAY_ENV=VERBOSE python3 ``` and the code posted below. It doesn't
matter if you use ```torch``` or any other Python library that uses the system
OpenMP. I removed unnecessary parts from the output.

This might be related to #29968.

A solution to this could be, that TF link to a shared version of OpenMP, i.e.,
as PyTorch and other packages are already doing it.
```

### Standalone code to reproduce the issue

```shell
print("">> SHARED OPENMP"")
import torch
print(""<< SHARED OPENMP"")

import tensorflow
print("">> TF BUNDLED OPENMP"")
with tensorflow.device('cpu'):
	pass
print(""<< TF BUNDLED OPENMP"")
```


### Relevant log output

```shell
>> SHARED OPENMP
	OPENMP DISPLAY ENVIRONMENT BEGIN
	  _OPENMP = '201511'
	  OMP_DYNAMIC = 'FALSE'
	  OMP_NESTED = 'FALSE'
	  OMP_NUM_THREADS = '24'
	  OMP_SCHEDULE = 'DYNAMIC'
	  OMP_PROC_BIND = 'TRUE'
	  OMP_PLACES = '{0},{1},{2},{3},{4},{5},{6},{7},{8},{9},{10},{11},{12},{13},{14},{15},{16},{17},{18},{19},{20},{21},{22},{23}'
	  OMP_STACKSIZE = '140265815980336'
	  OMP_WAIT_POLICY = 'PASSIVE'
	  OMP_THREAD_LIMIT = '4294967295'
	  OMP_MAX_ACTIVE_LEVELS = '2147483647'
	  OMP_CANCELLATION = 'FALSE'
	  OMP_DEFAULT_DEVICE = '0'
	  OMP_MAX_TASK_PRIORITY = '0'
	  GOMP_CPU_AFFINITY = ''
	  GOMP_STACKSIZE = '140265815980336'
	  GOMP_SPINCOUNT = '300000'
	OPENMP DISPLAY ENVIRONMENT END
<< SHARED OPENMP

...

>> TF BUNDLED OPENMP
	2022-07-04 09:47:08.193122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
	To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.

	OPENMP DISPLAY ENVIRONMENT BEGIN
	_OPENMP = 201511
	OMP_NUM_THREADS = 8
	OMP_STACKSIZE = 4194304
	OMP_SCHEDULE = static
	OMP_THREAD_LIMIT = 8
	OMP_MAX_ACTIVE_LEVELS = 1
	OMP_NESTED = false
	OMP_DYNAMIC = false
	OMP_WAIT_POLICY = active
	OMP_TOOL = disabled
	NOMP_MAX_QUEUED_TASKS = 1024
	NOMP_TASK_QUEUE_TYPE = HYBRID
	NOMP_TASK_QUEUE_ORDER = FIFO
	NOMP_TASK_REUSE_MEM = false
	OPENMP DISPLAY ENVIRONMENT END
<< TF BUNDLED OPENMP
```
```
</details>"
56670,can't reduce binary size(tflite),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tensorflow-2.8.2

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.6

### Bazel version

4.2.1

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I reduce the binary size of my model using selective builds https://www.tensorflow.org/lite/guide/reduce_binary_size.
 It can't reduce the binary size.
folder structure:
tmp/
|-- BUILD
`-- model.tflite
BUILD:
load(
    ""@org_tensorflow//tensorflow/lite/delegates/flex:build_def.bzl"",
    ""tflite_flex_shared_library""
)

# Shared lib target for convenience, pulls in the standard set of TensorFlow
# ops and kernels. The output library name is platform dependent:
#   - Linux/Android: `libtensorflowlite_flex.so`
#   - Mac: `libtensorflowlite_flex.dylib`
#   - Windows: `libtensorflowlite_flex.dll`
tflite_flex_shared_library(
  name = ""tensorflowlite_flex"",
  models = [
      "":model.tflite"",
  ],
)
compile command: bazel build -c opt --config=monolithic //tmp:tensorflowlite_flex
bazel-bin/tmp/libtensorflowlite_flex.so 183M
```


### Standalone code to reproduce the issue

```shell
model.tflite: lstm
ops:builtin_ops+tf_select_ops
```


### Relevant log output

_No response_</details>"
56669,_self_tracked_trackables in base_layer.py should not record it self,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.5.1

### Custom Code

No

### OS Platform and Distribution

linux

### Mobile device

_No response_

### Python version

3.6.5

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

My class inherited ""tf.keras.Model""because of some history reason (compatible history code), there is a class variable assigned as ""self"". like:

class Model(tf.keras.Model):
  def __init__(self, name):
    super().__init__()
    self.model = self  # here
    xxxxxxx

but it will occur infinite loop while some function use 'trainable_variables'

the source is in trainning.py/trainable_weights:
    for trackable_obj in self._self_tracked_trackables:
      trainable_variables += trackable_obj.trainable_variables
if self is in _self_tracked_trackables, here will infinite loop
_self_tracked_trackables was constructed in base_layer, maybe the _self_tracked_trackables  should filter it self?


### Standalone code to reproduce the issue

```shell
class Model(tf.keras.Model):
   def __init__(self, name: str = None):
        super().__init__()
        self.dnn_model = self
        print('here', self.trainable_variables)  # here

test = Model(""test"")
```


### Relevant log output

```shell
File ""/usr/local/lib64/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 2021, in trainable_weights
    trainable_variables += trackable_obj.trainable_variables
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow/python/training/tracking/data_structures.py"", line 816, in __getattribute__
    return object.__getattribute__(self, name)
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow/python/training/tracking/data_structures.py"", line 285, in trainable_variables
    return self.trainable_weights
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow/python/training/tracking/data_structures.py"", line 816, in __getattribute__
    return object.__getattribute__(self, name)
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow/python/training/tracking/data_structures.py"", line 242, in trainable_weights
    for obj in self._values:
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow/python/training/tracking/data_structures.py"", line 816, in __getattribute__
    return object.__getattribute__(self, name)
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow/python/training/tracking/data_structures.py"", line 841, in _values
    ordered = list(zip(*sorted(self.items(), key=lambda it: it[0])))
RecursionError: maximum recursion depth exceeded while calling a Python object
```
</details>"
56668,[On device learning] Use model.fit() when pretraining in VGG16,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.7

### GPU model and memory

GTX 1650

### Current Behaviour?


This is version info
```
2022-07-04 13:54:25.617651: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
v2.9.0-18-gd8ce9f9c301 2.9.1
```

According to This Example (https://www.tensorflow.org/lite/examples/on_device_training/overview)
using tf_function custom defined train for pretrain

I tried to Train VGG16 and CIFAR100 while conducting the test, but the training was not proceed,

- It works, but loss and accuracy do not change

--> so I am going to proceed with pretrain using model.fit as below, is this supported by On_device_Learning? <Pretrain seems like Working Well>

--> Or what action should I take if I try to use Gradient Tape in VGG16? (EveryThing Works Well in Lenet5)

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf
from tensorflow.keras import initializers
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

image_shape = (32,32,3)
num_classes = 100

class Model(tf.Module):
    
    def __init__(self):
        
        self.model = tf.keras.Sequential([
        tf.keras.Input(shape=image_shape, name='input_img', dtype = tf.float32),
                
        tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu',bias_initializer=initializers.Zeros()),
        tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu',bias_initializer=initializers.Zeros()),
        tf.keras.layers.MaxPooling2D(pool_size=(2,2), padding='same'),
        
        tf.keras.layers.Conv2D(128, (3,3), padding='same', activation='relu' ,bias_initializer=initializers.Zeros()),
        tf.keras.layers.Conv2D(128, (3,3), padding='same', activation='relu' ,bias_initializer=initializers.Zeros()),
        tf.keras.layers.MaxPooling2D(pool_size=(2,2), padding='same'),
        
        tf.keras.layers.Conv2D(256, (3,3), padding='same', activation='relu' ,bias_initializer=initializers.Zeros()),
        tf.keras.layers.Conv2D(256, (3,3), padding='same', activation='relu' ,bias_initializer=initializers.Zeros()),
        tf.keras.layers.Conv2D(256, (3,3), padding='same', activation='relu' ,bias_initializer=initializers.Zeros()),
        tf.keras.layers.MaxPooling2D(pool_size=(2,2), padding='same'),
        
        tf.keras.layers.Conv2D(512, (3,3), padding='same', activation='relu' ,bias_initializer=initializers.Zeros()),
        tf.keras.layers.Conv2D(512, (3,3), padding='same', activation='relu',bias_initializer=initializers.Zeros() ),
        tf.keras.layers.Conv2D(512, (3,3), padding='same', activation='relu' ,bias_initializer=initializers.Zeros()),
        tf.keras.layers.MaxPooling2D(pool_size=(2,2), padding='same'),
        
        tf.keras.layers.Conv2D(512, (3,3), padding='same', activation='relu' ,bias_initializer=initializers.Zeros()),
        tf.keras.layers.Conv2D(512, (3,3), padding='same', activation='relu' ,bias_initializer=initializers.Zeros()),
        tf.keras.layers.Conv2D(512, (3,3), padding='same', activation='relu' ,bias_initializer=initializers.Zeros()),
        tf.keras.layers.MaxPooling2D(pool_size=(2,2), padding='same'),

        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(512,bias_initializer=initializers.Zeros(), activation = 'relu'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(100,bias_initializer=initializers.Zeros(), activation = 'softmax'),
        ])

        self.model.compile(
            optimizer = 'sgd',
            loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False),
            metrics=['accuracy']
        )
        
    @tf.function(input_signature=[
        tf.TensorSpec([None, image_shape[0], image_shape[1], image_shape[2]], tf.float32),
        tf.TensorSpec([None, 100], tf.float32),
    ])
    def train(self, x, y):
        with tf.GradientTape() as tape:
            prediction = self.model(x) # prediction 
            loss = self.model.loss(y, prediction) #calculate loss
        
        gradients = tape.gradient(loss, self.model.trainable_variables) # calculate gradient
        self.model.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables)) #backpropagation
        
        result = {""loss"": loss, ""prediction"": prediction}
        
        return result

    
    @tf.function(input_signature=[
        tf.TensorSpec([None, image_shape[0], image_shape[1], image_shape[2]], tf.float32),    ])
    def infer(self, x):
        logits = self.model(x)
        probabilities = tf.nn.softmax(logits, axis=-1)
        return {
            ""output"": probabilities,
            ""logits"": logits
        }

    @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])
    def save(self, checkpoint_path):
        tensor_names = [weight.name for weight in self.model.weights]
        tensors_to_save = [weight.read_value() for weight in self.model.weights]
        tf.raw_ops.Save(
            filename=checkpoint_path, tensor_names=tensor_names,
            data=tensors_to_save, name='save')
        return {
            ""checkpoint_path"": checkpoint_path
        }

    @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])
    def restore(self, checkpoint_path):
        restored_tensors = {}
        for var in self.model.weights:
            restored = tf.raw_ops.Restore(
                file_pattern=checkpoint_path, tensor_name=var.name, dt=var.dtype,
                name='restore')
            var.assign(restored)
            restored_tensors[var.name] = restored
        return restored_tensors




m = Model()

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data()

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')

y_train = tf.keras.utils.to_categorical(y_train, num_classes)
y_test = tf.keras.utils.to_categorical(y_test, num_classes)

train_image, val_image, train_label, val_label = train_test_split(x_train, y_train, test_size = 0.15)

history = m.model.fit(x_train, y_train, batch_size = 128, epochs = 3 )

m.save('./model.ckpt')

SAVED_MODEL_DIR = ""saved_model""

tf.saved_model.save(
    m,
    SAVED_MODEL_DIR,
    signatures={
        'train':
            m.train.get_concrete_function() ,
        'infer':
            m.infer.get_concrete_function(),
        'save':
            m.save.get_concrete_function(),
        'restore':
            m.restore.get_concrete_function(),
    })

converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_DIR)
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.
    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.
]
converter.experimental_enable_resource_variables = True
tflite_model = converter.convert()
with open('model_vgg_fit.tflite', 'wb') as f:
    f.write(tflite_model)
```


### Relevant log output

_No response_</details>"
56666,Can't pickle optimizer,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Source

source

### Tensorflow Version

tf 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import pickle
output_file=open('save.dat','wb')
pickle.dump(tf.keras.optimizers.Adam(),output_file)
```


### Relevant log output

_No response_</details>"
56665,tflite_convert error,"### 1. System information

- Ubuntu 20.04
- TensorFlow  built from source
- TensorFlow version 2.9):

### 2. Code

bazel run //tensorflow/lite/python:tflite_convert --

### 3. Failure after conversion
ERROR: /home/tensorflow/BUILD:1355:19: Executing genrule //tensorflow:tf_python_api_gen_v2 failed: (Exit 1): bash failed: error executing command /bin/bash -c ... (remaining 1 argument skipped)

ERROR: /home/tensorflow/lite/python/BUILD:68:10 Middleman _middlemen/tensorflow_Slite_Spython_Stflite_Uconvert-runfiles failed: (Exit 1): bash failed: error executing command /bin/bash -c ... (remaining 1 argument skipped)
INFO: Elapsed time: 2.901s, Critical Path: 0.95s
INFO: 2 processes: 2 internal.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully


"
56663,`UniformFloatingPointDistribution` incorrect behaviour at infinite bounds,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source (I actually use [this binary](https://github.com/elixir-nx/xla/releases/tag/v0.3.0) but that isn't compiled by Google))

### Tensorflow Version

2.8

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

n/a

### Bazel version

2.4.1

### GCC/Compiler version

Unknown, between 7.5 and 9.3

### CUDA/cuDNN version

CUDA Version: 11.6

### GPU model and memory

NVIDIA GeForce GTX 1070

### Current Behaviour?

```shell
`UniformFloatingPointDistribution` produces the following samples for the following bounds

1) -inf 0 -> nan
2) 0 +inf -> inf
3) -inf -inf -> nan
4) +inf +inf -> nan

I believe 1) is incorrect and inconsistent with 2), which is correct. I believe 3) and 4) should be -inf and +inf respectively, since any sample between one +inf and another +inf will be +inf, and since there's no way to specify _different_ +infs (so that bounds are different), I think it makes sense to assume the bounds of +inf and +inf are different, and the same for -inf and -inf.
```


### Standalone code to reproduce the issue

```C++
#include ""tensorflow/compiler/xla/client/xla_builder.h""
#include ""tensorflow/compiler/xla/client/lib/constants.h""
#include ""tensorflow/compiler/xla/client/lib/prng.h""
#include ""tensorflow/compiler/xla/shape.h""
#include ""tensorflow/compiler/xla/client/local_client.h""
#include ""tensorflow/compiler/xla/client/client_library.h""
#include ""tensorflow/core/common_runtime/gpu/gpu_init.h""

void Test() {
    xla::XlaBuilder builder("""");
    auto posinf = xla::MaxValue(&builder, xla::F64);
    auto neginf = xla::MinValue(&builder, xla::F64);
    auto zero = xla::ConstantR0<double>(&builder, 0.0);
    auto key = xla::ConstantR0<uint64_t>(&builder, 0);
    auto state = xla::ConstantR0<uint64_t>(&builder, {0});
    auto shape = xla::ShapeUtil::MakeShape(xla::F64, {});

    auto sample = xla::UniformFloatingPointDistribution(
        key, state, xla::ThreeFryBitGenerator, neginf, 0, shape // replace bounds as appropriate
    ).value;

    auto computation = builder.Build(sample).ConsumeValueOrDie();
    auto res =
        xla::ClientLibrary::GetOrCreateLocalClient(tensorflow::GPUMachineManager())  // I'm also seeing this on CPU
        .ConsumeValueOrDie()
        ->ExecuteAndTransfer(computation, {})
        .ConsumeValueOrDie()
        .ToString();

    std::cout << res << std::endl;
}
```


### Relevant log output

```shell
f64[] -nan
f64[] inf
f64[] -nan
f64[] -nan

for each test case 1-4
```
</details>"
56662,The Height and Width must be changed in tensorflow document on the website ,"Hi,
In the TF documentation there is a mistake, the order of ""Height"" and ""Width"" is wrong, it's (Batch, Width, Height, features) but must be (Batch, Height, Width, features) to comply with channel_last notation (NHWC)

![image](https://user-images.githubusercontent.com/83506388/177032598-e8b01440-5c79-47b2-808d-5cb8df9ad68a.png)
https://www.tensorflow.org/guide/tensor#:~:text=regions%20of%20memory.-,Typical%20axis%20order,-Indexing"
56661,TensorFlow doesn't use all available memory on NVIDIA GPU,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.6

### Custom Code

No

### OS Platform and Distribution

Windows 11 Pro 21H2

### Mobile device

_No response_

### Python version

3.7.13

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.3.1/8.2.1

### GPU model and memory

NVIDIA GeForce RTX 3070 Laptop GPU 8GB

### Current Behaviour?

```shell
When using GPU VRAM, TensorFlow only uses 5.6GB of the 8.0GB available
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
v = tf.Variable(tf.linspace(-10., 10, 10000))
```


### Relevant log output

```shell
2022-07-02 17:25:40.770168: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-07-02 17:25:41.122377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5482 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6

Sat Jul  2 17:26:11 2022
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 516.59       Driver Version: 516.59       CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |
| N/A   52C    P8    12W /  N/A |   5740MiB /  8192MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A     27356      C   ...onda3\envs\tf2\python.exe    N/A      |
+-----------------------------------------------------------------------------+
```
</details>"
56660,"TFLiteConverter.experimental_from_jax on a working jax model: ""flax.errors.JaxTransformError: Jax transforms and Flax models cannot be mixed.""","### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04 (Google Colab)
- TensorFlow installation (pip package or built from source): Colab's version (Pip?)
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.8.0

### 2. Code

The example linked below demonstrates that a working JAX model (which converts and saves successfully with `tf.saved_model.save` throws the error ""flax.errors.JaxTransformError: Jax transforms and Flax models cannot be mixed."" when trying to convert it to TFLite using `TFLiteConverter.experimental_from_jax`.

https://colab.research.google.com/gist/josephrocca/b9b7b4e92cb693772cb937302b4946d9/jax-flax-transformations-cannot-be-mixed.ipynb

The `experimental_from_jax` function is of course experimental, and I understand that the jax2tf --> tf saved model --> tflite is the current recommended route, but just reporting this in case it is not expected behavior.
"
56659,Eree,
56658,OSError: symbolic link privilege not held  when use bazel build tensorflow,"I got a error when i build tensorflow with bazel:
My command is: bazel build --config=opt //tensorflow:tensorflow_cc.dll --local_ram_resources=1024  

And i run it with admin  I got this :

`INFO: Reading 'startup' options from f:\tensorflow\.bazelrc: --output_user_root=F:/tf
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=120
INFO: Reading rc options for 'build' from f:\tensorflow\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/Users/hx/AppData/Local/Programs/Python/Python36/python.exe
INFO: Reading rc options for 'build' from f:\tensorflow\.bazelrc:
  'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false
INFO: Reading rc options for 'build' from f:\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/hx/AppData/Local/Programs/Python/Python36/python.exe --action_env PYTHON_LIB_PATH=C:/Users/hx/AppData/Local/Programs/Python/Python36/Lib/site-packages --python_path=C:/Users/hx/AppData/Local/Programs/Python/Python36/python.exe --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true
INFO: Reading rc options for 'build' from f:\tensorflow\.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file f:\tensorflow\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file f:\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:windows in file f:\tensorflow\.bazelrc: --copt=/W0 --copt=/Zc:__cplusplus --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --features=compiler_param_file --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --cxxopt=/std:c++17 --host_cxxopt=/std:c++17 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file f:\tensorflow\.bazelrc: --define framework_shared_object=false --experimental_link_static_libraries_once=false
INFO: Build options --copt and --host_copt have changed, discarding analysis cache.
WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
INFO: Repository llvm-project instantiated at:
  F:/tensorflow/WORKSPACE:15:14: in <toplevel>
  F:/tensorflow/tensorflow/workspace2.bzl:880:21: in workspace
  F:/tensorflow/tensorflow/workspace2.bzl:518:15: in _tf_repositories
  F:/tensorflow/third_party/llvm/setup.bzl:22:19: in llvm_setup
Repository rule llvm_configure defined at:
  F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/configure.bzl:84:33: in <toplevel>
ERROR: An error occurred during the fetch of repository 'llvm-project':
   Traceback (most recent call last):
        File ""F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/configure.bzl"", line 74, column 25, in _llvm_configure_impl
                _overlay_directories(repository_ctx)
        File ""F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/configure.bzl"", line 63, column 13, in _overlay_directories
                fail((""Failed to execute overlay script: '{cmd}'\n"" +
Error in fail: Failed to execute overlay script: 'C:/Users/hx/AppData/Local/Programs/Python/Python36/python.exe F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/overlay_directories.py --src F:/tf/2qttxlm7/external/llvm-raw --overlay F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/llvm-project-overlay --target .'
Exited with code 1
stdout:

stderr:
Traceback (most recent call last):
  File ""F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/overlay_directories.py"", line 92, in <module>
    main(parse_arguments())
  File ""F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/overlay_directories.py"", line 81, in main
    os.path.join(args.target, relpath))
  File ""F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/overlay_directories.py"", line 64, in _symlink_abs
    os.symlink(os.path.abspath(from_path), os.path.abspath(to_path))
OSError: symbolic link privilege not held

ERROR: F:/tensorflow/WORKSPACE:15:14: fetching llvm_configure rule //external:llvm-project: Traceback (most recent call last):
        File ""F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/configure.bzl"", line 74, column 25, in _llvm_configure_impl
                _overlay_directories(repository_ctx)
        File ""F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/configure.bzl"", line 63, column 13, in _overlay_directories
                fail((""Failed to execute overlay script: '{cmd}'\n"" +
Error in fail: Failed to execute overlay script: 'C:/Users/hx/AppData/Local/Programs/Python/Python36/python.exe F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/overlay_directories.py --src F:/tf/2qttxlm7/external/llvm-raw --overlay F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/llvm-project-overlay --target .'
Exited with code 1
stdout:

stderr:
Traceback (most recent call last):
  File ""F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/overlay_directories.py"", line 92, in <module>
    main(parse_arguments())
  File ""F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/overlay_directories.py"", line 81, in main
    os.path.join(args.target, relpath))
  File ""F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/overlay_directories.py"", line 64, in _symlink_abs
    os.symlink(os.path.abspath(from_path), os.path.abspath(to_path))
OSError: symbolic link privilege not held

INFO: Repository zlib instantiated at:
  F:/tensorflow/WORKSPACE:15:14: in <toplevel>
  F:/tensorflow/tensorflow/workspace2.bzl:880:21: in workspace
  F:/tensorflow/tensorflow/workspace2.bzl:555:20: in _tf_repositories
  F:/tensorflow/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  F:/tensorflow/third_party/repo.bzl:89:35: in <toplevel>
INFO: Repository double_conversion instantiated at:
  F:/tensorflow/WORKSPACE:15:14: in <toplevel>
  F:/tensorflow/tensorflow/workspace2.bzl:880:21: in workspace
  F:/tensorflow/tensorflow/workspace2.bzl:698:20: in _tf_repositories
  F:/tensorflow/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  F:/tensorflow/third_party/repo.bzl:89:35: in <toplevel>
INFO: Repository snappy instantiated at:
  F:/tensorflow/WORKSPACE:15:14: in <toplevel>
  F:/tensorflow/tensorflow/workspace2.bzl:880:21: in workspace
  F:/tensorflow/tensorflow/workspace2.bzl:574:20: in _tf_repositories
  F:/tensorflow/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  F:/tensorflow/third_party/repo.bzl:89:35: in <toplevel>
INFO: Repository org_sqlite instantiated at:
  F:/tensorflow/WORKSPACE:15:14: in <toplevel>
  F:/tensorflow/tensorflow/workspace2.bzl:880:21: in workspace
  F:/tensorflow/tensorflow/workspace2.bzl:299:20: in _tf_repositories
  F:/tensorflow/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  F:/tensorflow/third_party/repo.bzl:89:35: in <toplevel>
INFO: Repository eigen_archive instantiated at:
  F:/tensorflow/WORKSPACE:15:14: in <toplevel>
  F:/tensorflow/tensorflow/workspace2.bzl:873:28: in workspace
  F:/tensorflow/tensorflow/workspace2.bzl:63:11: in _initialize_third_party
  F:/tensorflow/third_party/eigen3/workspace.bzl:14:20: in repo
  F:/tensorflow/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  F:/tensorflow/third_party/repo.bzl:89:35: in <toplevel>
INFO: Repository sobol_data instantiated at:
  F:/tensorflow/WORKSPACE:15:14: in <toplevel>
  F:/tensorflow/tensorflow/workspace2.bzl:873:28: in workspace
  F:/tensorflow/tensorflow/workspace2.bzl:80:15: in _initialize_third_party
  F:/tensorflow/third_party/sobol_data/workspace.bzl:6:20: in repo
  F:/tensorflow/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  F:/tensorflow/third_party/repo.bzl:89:35: in <toplevel>
INFO: Repository com_google_absl instantiated at:
  F:/tensorflow/WORKSPACE:15:14: in <toplevel>
  F:/tensorflow/tensorflow/workspace2.bzl:873:28: in workspace
  F:/tensorflow/tensorflow/workspace2.bzl:58:9: in _initialize_third_party
  F:/tensorflow/third_party/absl/workspace.bzl:39:20: in repo
  F:/tensorflow/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  F:/tensorflow/third_party/repo.bzl:89:35: in <toplevel>
ERROR: F:/tensorflow/tensorflow/compiler/xla/service/cpu/BUILD:173:11: //tensorflow/compiler/xla/service/cpu:cpu_compiler depends on @llvm-project//llvm:Core in repository @llvm-project which failed to fetch. no such package '@llvm-project//llvm': Failed to execute overlay script: 'C:/Users/hx/AppData/Local/Programs/Python/Python36/python.exe F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/overlay_directories.py --src F:/tf/2qttxlm7/external/llvm-raw --overlay F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/llvm-project-overlay --target .'
Exited with code 1
stdout:

stderr:
Traceback (most recent call last):
  File ""F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/overlay_directories.py"", line 92, in <module>
    main(parse_arguments())
  File ""F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/overlay_directories.py"", line 81, in main
    os.path.join(args.target, relpath))
  File ""F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/overlay_directories.py"", line 64, in _symlink_abs
    os.symlink(os.path.abspath(from_path), os.path.abspath(to_path))
OSError: symbolic link privilege not held

ERROR: F:/tensorflow/tensorflow/compiler/xla/service/cpu/BUILD:173:11: //tensorflow/compiler/xla/service/cpu:cpu_compiler depends on @llvm-project//llvm:Object in repository @llvm-project which failed to fetch. no such package '@llvm-project//llvm': Failed to execute overlay script: 'C:/Users/hx/AppData/Local/Programs/Python/Python36/python.exe F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/overlay_directories.py --src F:/tf/2qttxlm7/external/llvm-raw --overlay F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/llvm-project-overlay --target .'
Exited with code 1
stdout:

stderr:
Traceback (most recent call last):
  File ""F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/overlay_directories.py"", line 92, in <module>
    main(parse_arguments())
  File ""F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/overlay_directories.py"", line 81, in main
    os.path.join(args.target, relpath))
  File ""F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/overlay_directories.py"", line 64, in _symlink_abs
    os.symlink(os.path.abspath(from_path), os.path.abspath(to_path))
OSError: symbolic link privilege not held

ERROR: F:/tensorflow/tensorflow/compiler/xla/service/cpu/BUILD:173:11: //tensorflow/compiler/xla/service/cpu:cpu_compiler depends on @llvm-project//llvm:MC in repository @llvm-project which failed to fetch. no such package '@llvm-project//llvm': Failed to execute overlay script: 'C:/Users/hx/AppData/Local/Programs/Python/Python36/python.exe F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/overlay_directories.py --src F:/tf/2qttxlm7/external/llvm-raw --overlay F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/llvm-project-overlay --target .'
Exited with code 1
stdout:

stderr:
Traceback (most recent call last):
  File ""F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/overlay_directories.py"", line 92, in <module>
    main(parse_arguments())
  File ""F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/overlay_directories.py"", line 81, in main
    os.path.join(args.target, relpath))
  File ""F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/overlay_directories.py"", line 64, in _symlink_abs
    os.symlink(os.path.abspath(from_path), os.path.abspath(to_path))
OSError: symbolic link privilege not held

ERROR: F:/tensorflow/tensorflow/compiler/xla/service/cpu/BUILD:173:11: //tensorflow/compiler/xla/service/cpu:cpu_compiler depends on @llvm-project//llvm:Support in repository @llvm-project which failed to fetch. no such package '@llvm-project//llvm': Failed to execute overlay script: 'C:/Users/hx/AppData/Local/Programs/Python/Python36/python.exe F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/overlay_directories.py --src F:/tf/2qttxlm7/external/llvm-raw --overlay F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/llvm-project-overlay --target .'
Exited with code 1
stdout:

stderr:
Traceback (most recent call last):
  File ""F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/overlay_directories.py"", line 92, in <module>
    main(parse_arguments())
  File ""F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/overlay_directories.py"", line 81, in main
    os.path.join(args.target, relpath))
  File ""F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/overlay_directories.py"", line 64, in _symlink_abs
    os.symlink(os.path.abspath(from_path), os.path.abspath(to_path))
OSError: symbolic link privilege not held

ERROR: F:/tensorflow/tensorflow/compiler/xla/service/cpu/BUILD:173:11: //tensorflow/compiler/xla/service/cpu:cpu_compiler depends on @llvm-project//llvm:Target in repository @llvm-project which failed to fetch. no such package '@llvm-project//llvm': Failed to execute overlay script: 'C:/Users/hx/AppData/Local/Programs/Python/Python36/python.exe F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/overlay_directories.py --src F:/tf/2qttxlm7/external/llvm-raw --overlay F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/llvm-project-overlay --target .'
Exited with code 1
stdout:

stderr:
Traceback (most recent call last):
  File ""F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/overlay_directories.py"", line 92, in <module>
    main(parse_arguments())
  File ""F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/overlay_directories.py"", line 81, in main
    os.path.join(args.target, relpath))
  File ""F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/overlay_directories.py"", line 64, in _symlink_abs
    os.symlink(os.path.abspath(from_path), os.path.abspath(to_path))
OSError: symbolic link privilege not held

ERROR: F:/tensorflow/tensorflow/compiler/xla/service/cpu/BUILD:173:11: //tensorflow/compiler/xla/service/cpu:cpu_compiler depends on @llvm-project//llvm:X86CodeGen in repository @llvm-project which failed to fetch. no such package '@llvm-project//llvm': Failed to execute overlay script: 'C:/Users/hx/AppData/Local/Programs/Python/Python36/python.exe F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/overlay_directories.py --src F:/tf/2qttxlm7/external/llvm-raw --overlay F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/llvm-project-overlay --target .'
Exited with code 1
stdout:

stderr:
Traceback (most recent call last):
  File ""F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/overlay_directories.py"", line 92, in <module>
    main(parse_arguments())
  File ""F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/overlay_directories.py"", line 81, in main
    os.path.join(args.target, relpath))
  File ""F:/tf/2qttxlm7/external/llvm-raw/utils/bazel/overlay_directories.py"", line 64, in _symlink_abs
    os.symlink(os.path.abspath(from_path), os.path.abspath(to_path))
OSError: symbolic link privilege not held

ERROR: Analysis of target '//tensorflow:tensorflow_cc.dll' failed; build aborted:
INFO: Elapsed time: 0.741s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (2 packages loaded, 3477 targets configured)
    currently loading: tensorflow/compiler/mlir/hlo ... (5 packages)
    Fetching https://storage.googleapis.com/mirror.tensorflow.org/zlib.net/zlib-1.2.12.tar.gz
    Fetching https://storage.googleapis.com/.../github.com/google/double-conversion/archive/v3.2.0.tar.gz
    Fetching https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/snappy/archive/1.1.8.tar.gz
    Fetching https://storage.googleapis.com/mirror.tensorflow.org/www.sqlite.org/2022/sqlite-amalgamation-3380200.zip
    Fetching https://storage.googleapis.com/.../eigen-b02c384ef4e8eba7b8bdef16f9dc6f8f4d6a6b2b.tar.gz
    Fetching https://storage.googleapis.com/.../sobol_data/archive/835a7d7b1ee3bc83e575e302a985c66ec4b65249.tar.gz`

I am crazy about it , someone can help me? Thank you!

"
56657,Gradient Tape (tf.GradientTape) Returning All 0 Values in GradCam,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Yes (Custom Loss + GradCam)
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS Linux 7
-   **TensorFlow installed from (source or binary)**: PIP
-   **TensorFlow version (use command below)**: 2.9.0
-   **Python version**: 3.7.0
-   **CUDA/cuDNN version**:  11.6
-   **GPU model and memory**: Nvidia RTX 2080Ti

### Describe the problem

I have trained 2 FCN segmentation models, with the same custom loss.  
Model 1 ------>  Trained of 256x512 images, on TF==1.15. 
Model 2 ------> Trained on 512x1024 images, on TF==2.9.0.

**The same GradCam Script works fine on `Model 1`, but returns 0 (and eventually NaN) for `Model 2`.**

**Model1.summary():**

```
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 256, 512, 3  0           []                               
                                )]                                                                
                                                                                                  
 block1_conv2 (Conv2D)          (None, 256, 512, 64  1792        ['input_1[0][0]']                
                                )                                                                 
                                                                                                  
 block1_pool (MaxPooling2D)     (None, 128, 256, 64  0           ['block1_conv2[0][0]']           
                                )                                                                 
                                                                                                  
 block2_conv1 (Conv2D)          (None, 128, 256, 12  73856       ['block1_pool[0][0]']            
                                8)                                                                
                                                                                                  
 block2_conv2 (Conv2D)          (None, 128, 256, 12  147584      ['block2_conv1[0][0]']           
                                8)                                                                
                                                                                                  
 block2_pool (MaxPooling2D)     (None, 64, 128, 128  0           ['block2_conv2[0][0]']           
                                )                                                                 
                                                                                                  
 block3_conv1 (Conv2D)          (None, 64, 128, 256  295168      ['block2_pool[0][0]']            
                                )                                                                 
                                                                                                  
 block3_conv2 (Conv2D)          (None, 64, 128, 256  590080      ['block3_conv1[0][0]']           
                                )                                                                 
                                                                                                  
 block3_conv3 (Conv2D)          (None, 64, 128, 256  590080      ['block3_conv2[0][0]']           
                                )                                                                 
                                                                                                  
 block3_conv4 (Conv2D)          (None, 64, 128, 256  590080      ['block3_conv3[0][0]']           
                                )                                                                 
                                                                                                  
 block3_pool (MaxPooling2D)     (None, 32, 64, 256)  0           ['block3_conv4[0][0]']           
                                                                                                  
 block4_conv1 (Conv2D)          (None, 32, 64, 512)  1180160     ['block3_pool[0][0]']            
                                                                                                  
 block4_conv2 (Conv2D)          (None, 32, 64, 512)  2359808     ['block4_conv1[0][0]']           
                                                                                                  
 block4_conv3 (Conv2D)          (None, 32, 64, 512)  2359808     ['block4_conv2[0][0]']           
                                                                                                  
 block4_conv4 (Conv2D)          (None, 32, 64, 512)  2359808     ['block4_conv3[0][0]']           
                                                                                                  
 block4_pool (MaxPooling2D)     (None, 16, 32, 512)  0           ['block4_conv4[0][0]']           
                                                                                                  
 block5_conv1 (Conv2D)          (None, 16, 32, 512)  2359808     ['block4_pool[0][0]']            
                                                                                                  
 block5_conv2 (Conv2D)          (None, 16, 32, 512)  2359808     ['block5_conv1[0][0]']           
                                                                                                  
 block5_conv3 (Conv2D)          (None, 16, 32, 512)  2359808     ['block5_conv2[0][0]']           
                                                                                                  
 block5_conv4 (Conv2D)          (None, 16, 32, 512)  2359808     ['block5_conv3[0][0]']           
                                                                                                  
 block5_pool (MaxPooling2D)     (None, 8, 16, 512)   0           ['block5_conv4[0][0]']           
                                                                                                  
 conv2d (Conv2D)                (None, 8, 16, 4096)  102764544   ['block5_pool[0][0]']            
                                                                                                  
 re_lu (ReLU)                   (None, 8, 16, 4096)  0           ['conv2d[0][0]']                 
                                                                                                  
 dropout (Dropout)              (None, 8, 16, 4096)  0           ['re_lu[0][0]']                  
                                                                                                  
 conv2d_1 (Conv2D)              (None, 8, 16, 4096)  16781312    ['dropout[0][0]']                
                                                                                                  
 re_lu_1 (ReLU)                 (None, 8, 16, 4096)  0           ['conv2d_1[0][0]']               
                                                                                                  
 dropout_1 (Dropout)            (None, 8, 16, 4096)  0           ['re_lu_1[0][0]']                
                                                                                                  
 conv2d_2 (Conv2D)              (None, 8, 16, 2)     8194        ['dropout_1[0][0]']              
                                                                                                  
 conv2d_transpose (Conv2DTransp  (None, 16, 32, 2)   66          ['conv2d_2[0][0]']               
 ose)                                                                                             
                                                                                                  
 conv2d_3 (Conv2D)              (None, 16, 32, 2)    1026        ['block4_pool[0][0]']            
                                                                                                  
 add (Add)                      (None, 16, 32, 2)    0           ['conv2d_transpose[0][0]',       
                                                                  'conv2d_3[0][0]']               
                                                                                                  
 conv2d_transpose_1 (Conv2DTran  (None, 32, 64, 2)   66          ['add[0][0]']                    
 spose)                                                                                           
                                                                                                  
 conv2d_4 (Conv2D)              (None, 32, 64, 2)    514         ['block3_pool[0][0]']            
                                                                                                  
 add_1 (Add)                    (None, 32, 64, 2)    0           ['conv2d_transpose_1[0][0]',     
                                                                  'conv2d_4[0][0]']               
                                                                                                  
 conv2d_transpose_2 (Conv2DTran  (None, 256, 512, 2)  1026       ['add_1[0][0]']                  
 spose)                                                                                           
                                                                                                  
==================================================================================================
Total params: 139,544,204
Trainable params: 139,544,204
Non-trainable params: 0
```


**Model2.summary():**

```
Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 512, 1024,   0           []                               
                                3)]                                                               
                                                                                                  
 block1_conv1 (Conv2D)          (None, 512, 1024, 6  1792        ['input_1[0][0]']                
                                4)                                                                
                                                                                                  
 block1_conv2 (Conv2D)          (None, 512, 1024, 6  36928       ['block1_conv1[0][0]']           
                                4)                                                                
                                                                                                  
 block1_pool (MaxPooling2D)     (None, 256, 512, 64  0           ['block1_conv2[0][0]']           
                                )                                                                 
                                                                                                  
 block2_conv1 (Conv2D)          (None, 256, 512, 12  73856       ['block1_pool[0][0]']            
                                8)                                                                
                                                                                                  
 block2_conv2 (Conv2D)          (None, 256, 512, 12  147584      ['block2_conv1[0][0]']           
                                8)                                                                
                                                                                                  
 block2_pool (MaxPooling2D)     (None, 128, 256, 12  0           ['block2_conv2[0][0]']           
                                8)                                                                
                                                                                                  
 block3_conv1 (Conv2D)          (None, 128, 256, 25  295168      ['block2_pool[0][0]']            
                                6)                                                                
                                                                                                  
 block3_conv2 (Conv2D)          (None, 128, 256, 25  590080      ['block3_conv1[0][0]']           
                                6)                                                                
                                                                                                  
 block3_conv3 (Conv2D)          (None, 128, 256, 25  590080      ['block3_conv2[0][0]']           
                                6)                                                                
                                                                                                  
 block3_conv4 (Conv2D)          (None, 128, 256, 25  590080      ['block3_conv3[0][0]']           
                                6)                                                                
                                                                                                  
 block3_pool (MaxPooling2D)     (None, 64, 128, 256  0           ['block3_conv4[0][0]']           
                                )                                                                 
                                                                                                  
 block4_conv1 (Conv2D)          (None, 64, 128, 512  1180160     ['block3_pool[0][0]']            
                                )                                                                 
                                                                                                  
 block4_conv2 (Conv2D)          (None, 64, 128, 512  2359808     ['block4_conv1[0][0]']           
                                )                                                                 
                                                                                                  
 block4_conv3 (Conv2D)          (None, 64, 128, 512  2359808     ['block4_conv2[0][0]']           
                                )                                                                 
                                                                                                  
 block4_conv4 (Conv2D)          (None, 64, 128, 512  2359808     ['block4_conv3[0][0]']           
                                )                                                                 
                                                                                                  
 block4_pool (MaxPooling2D)     (None, 32, 64, 512)  0           ['block4_conv4[0][0]']           
                                                                                                  
 block5_conv1 (Conv2D)          (None, 32, 64, 512)  2359808     ['block4_pool[0][0]']            
                                                                                                  
 block5_conv2 (Conv2D)          (None, 32, 64, 512)  2359808     ['block5_conv1[0][0]']           
                                                                                                  
 block5_conv3 (Conv2D)          (None, 32, 64, 512)  2359808     ['block5_conv2[0][0]']           
                                                                                                  
 block5_conv4 (Conv2D)          (None, 32, 64, 512)  2359808     ['block5_conv3[0][0]']           
                                                                                                  
 block5_pool (MaxPooling2D)     (None, 16, 32, 512)  0           ['block5_conv4[0][0]']           
                                                                                                  
 conv2d (Conv2D)                (None, 16, 32, 4096  102764544   ['block5_pool[0][0]']            
                                )                                                                 
                                                                                                  
 re_lu (ReLU)                   (None, 16, 32, 4096  0           ['conv2d[0][0]']                 
                                )                                                                 
                                                                                                  
 dropout (Dropout)              (None, 16, 32, 4096  0           ['re_lu[0][0]']                  
                                )                                                                 
                                                                                                  
 conv2d_1 (Conv2D)              (None, 16, 32, 4096  16781312    ['dropout[0][0]']                
                                )                                                                 
                                                                                                  
 re_lu_1 (ReLU)                 (None, 16, 32, 4096  0           ['conv2d_1[0][0]']               
                                )                                                                 
                                                                                                  
 dropout_1 (Dropout)            (None, 16, 32, 4096  0           ['re_lu_1[0][0]']                
                                )                                                                 
                                                                                                  
 conv2d_2 (Conv2D)              (None, 16, 32, 2)    8194        ['dropout_1[0][0]']              
                                                                                                  
 conv2d_transpose (Conv2DTransp  (None, 32, 64, 2)   66          ['conv2d_2[0][0]']               
 ose)                                                                                             
                                                                                                  
 conv2d_3 (Conv2D)              (None, 32, 64, 2)    1026        ['block4_pool[0][0]']            
                                                                                                  
 add (Add)                      (None, 32, 64, 2)    0           ['conv2d_transpose[0][0]',       
                                                                  'conv2d_3[0][0]']               
                                                                                                  
 conv2d_transpose_1 (Conv2DTran  (None, 64, 128, 2)  66          ['add[0][0]']                    
 spose)                                                                                           
                                                                                                  
 conv2d_4 (Conv2D)              (None, 64, 128, 2)   514         ['block3_pool[0][0]']            
                                                                                                  
 add_1 (Add)                    (None, 64, 128, 2)   0           ['conv2d_transpose_1[0][0]',     
                                                                  'conv2d_4[0][0]']               
                                                                                                  
 conv2d_transpose_2 (Conv2DTran  (None, 512, 1024, 2  1026       ['add_1[0][0]']                  
 spose)                         )                                                                 
                                                                                                  
==================================================================================================
Total params: 139,581,132
Trainable params: 139,581,132
Non-trainable params: 0
```

### Source code / logs

**Custom Loss (common for both models)**

```
def self_balanced_focal_loss(alpha=3, gamma=2.0):
    def loss(y_true, y_pred):
        # cross entropy loss
        y_pred = backend.softmax(y_pred, -1)
        cross_entropy = backend.categorical_crossentropy(y_true, y_pred)

        # sample weights
        sample_weights = backend.max(backend.pow(1.0 - y_pred, gamma) * y_true, axis=-1)

        # class weights
        pixel_rate = backend.sum(y_true, axis=[1, 2], keepdims=True) / backend.sum(backend.ones_like(y_true),
                                                                                   axis=[1, 2], keepdims=True)
        class_weights = backend.max(backend.pow(backend.ones_like(y_true) * alpha, pixel_rate) * y_true, axis=-1)

        # final loss
        final_loss = class_weights * sample_weights * cross_entropy
        return backend.mean(backend.sum(final_loss, axis=[1, 2]))

    return loss
```
**GradCam Script (Common for both models)**

```
def get_img_array(img_path, size):   
    img = keras.preprocessing.image.load_img(img_path, target_size=size)
    array = keras.preprocessing.image.img_to_array(img)
    array = np.expand_dims(array, axis=0)
    return array


def make_gradcam_heatmap(img_array, model, last_conv_layer_name,idx1,idx2):
    # First, we create a model that maps the input image to the activations
    # of the last conv layer as well as the output predictions
    grad_model = tf.keras.models.Model(
        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]
    )
   print(grad_model.outputs)
    # Then, we compute the gradient of the top predicted class for our input image
    # with respect to the activations of the last conv layer
    with tf.GradientTape() as tape:
        last_conv_layer_output, preds = grad_model(img_array)
        class_channel = preds[0][idx1][idx2][1]
        print(""CLASS CHANNEL"", class_channel)

    # This is the gradient of the output neuron (top predicted or chosen)
    # with regard to the output feature map of the last conv layer
    grads = tape.gradient(class_channel, last_conv_layer_output)
    print(""GRADS"", grads)


    # This is a vector where each entry is the mean intensity of the gradient
    # over a specific feature map channel
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

    # We multiply each channel in the feature map array
    # by ""how important this channel is"" with regard to the top predicted class
    # then sum all the channels to obtain the heatmap class activation
    last_conv_layer_output = last_conv_layer_output[0]
    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)

    # For visualization purpose, we will also normalize the heatmap between 0 & 1
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()
```
Predictions from both the model yield useful results. However, the heatmap generated via the `gradcam` script for Model 2 returns 0's, and `anything/max(zeros)=NaN`

**In `Model 2`, there are no other changes, except minor ones, required to make the code compatible with TF>2, from TF==1.X. Why is it that the `tf.GradientTape()` is resulting in 0 values regardless of the image input, when the model (on the same images) is able to generate good predictions?**

Kindly advise and help,
Thanks.


Edit 01
Kindly check a difference I noticed in the GradCam results of the two models.
`Model 1` (Working Gradcam)
```
  [<KerasTensor: shape=(None, 16, 32, 512) dtype=float32 (created by layer 'block5_conv4')>, <KerasTensor: shape=(None, 256, 512, 2) dtype=float32 (created by layer 'conv2d_transpose_2')>]
CLASS CHANNEL tf.Tensor(0.26052475, shape=(), dtype=float32)
GRADS [-0.02966771 -0.02644881 -0.02611654 ...  0.01653964  0.01702563
  0.01976959]
```
`Model 2` (NaN/0 heatmap)
```
[<KerasTensor: shape=(None, 32, 64, 512) dtype=float32 (created by layer 'block5_conv4')>, <KerasTensor: shape=(None, 512, 1024, 2) dtype=float32 (created by layer 'conv2d_transpose_2')>]
CLASS CHANNEL tf.Tensor(-0.15865403, shape=(), dtype=float32)
GRADS [-0.00172401 -0.00150979 -0.00142616 ...  0.00059308  0.00060482
  0.00060655]
```
Thus, the major difference seems to be in the 'class_channel' variable ---> positive in Model 1.
"
56655,`tf.scatter_nd` op is placed on the CPU when values are of boolean type,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

binary

### Tensorflow Version

2.8.2

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04.5

### Mobile device

_No response_

### Python version

3.7.13

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

NVIDISMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The `tf.scatter_nd` op is being placed on the CPU when the values are of type bool. However, the op is placed on the GPU for int type.
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1R41Ta-j8Qh2hLE-l5BBZsmBlg1CxtjcU?usp=sharing
```


### Relevant log output

```shell
indices = tf.constant([[4], [3], [1], [7]])
updates = tf.constant([True, True, False, False])
shape = tf.constant([8])
scatter = tf.scatter_nd(indices, updates, shape)

Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op ScatterNd in device /job:localhost/replica:0/task:0/device:CPU:0

indices = tf.constant([[4], [3], [1], [7]])
updates = tf.constant([9, 10, 11, 12])
shape = tf.constant([8])
scatter = tf.scatter_nd(indices, updates, shape)

Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op ScatterNd in device /job:localhost/replica:0/task:0/device:GPU:0
```
</details>"
56652,Grappler Errors with ConvLSTM2D,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution**: Windows 11 Build 22000
-   **TensorFlow installed from**: pip package
-   **TensorFlow version**: v2.9.0-18-gd8ce9f9c301 2.9.1
-   **Python version**: Python 3.10.4
-   **CUDA/cuDNN version**: CUDA 11.7.0-1
-   **GPU model and memory**: Nvidia 2080 Ti 11GiB Memory
-   **Exact command to reproduce**: `python conv_lstm.py` with [conv_lstm.py](https://github.com/keras-team/keras-io/blob/master/examples/vision/conv_lstm.py) from the official Keras examples

### Describe the problem
Using the ConvLSTM2D layer produces a grappler error: `layout failed: INVALID_ARGUMENT: MutableGraphView::SortTopologically error: detected edge(s) creating cycle(s) ...` when run with a GPU. Interestingly, this does not happen if TensorFlow is forced to run without a GPU.

It is not clear whether this is a problem with grappler, with the implementation of the ConvLSTM2D layer, or a bug in CUDA.

### Source code / logs
I used the example script from the Keras documentation. [[Keras example page](https://keras.io/examples/vision/conv_lstm/)] [[script](https://github.com/keras-team/keras-io/blob/master/examples/vision/conv_lstm.py)]. The logs are here: [output.txt](https://github.com/tensorflow/tensorflow/files/9031236/output.txt).
"
56651,Wrong output Object Detection TFLite on C++ API,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

latest

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

5.1.1

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I trained a tflite model following this tutorial https://colab.research.google.com/github/googlecodelabs/odml-pathways/blob/main/object-detection/codelab2/python/Train_a_salad_detector_with_TFLite_Model_Maker.ipynb#scrollTo=BRd13bfetO7B (kept everything the same except for dataset). Model is trained and gives results as expected using this python code.

However, I would like to use the model in C++. With the very poor support for TFLite C++ API, I was able to run the model following this example https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/label_image.cc

But the output doesn't seem to show an accurate result.

From the Python example, this is the output:
- boxes/location: array of 25 arrays with 4 positions [ymin, xmin, ymax, xmax]
- classes: array of 25 class index
- scores: array of 25 scores (ranging from 0 to 1)
- count: number of detected objects (25)

When running on C++, this is the output:
- locations: 8
- classes: 8
- scores: 8
- count: 25

It'd be great if I could get a sample on how to run this model on C++ (if the generated model on the notebook above works on this sample and is, somehow, giving the same result, then it'd be great)
```


### Standalone code to reproduce the issue

```shell
Build model using first link and run on C++ using second link
```


### Relevant log output

```shell
Using the same image in both environments results in this:

- Python

boxes:  [[0.6845545  0.36612728 0.7911295  0.43582943]
 [0.6968472  0.2782174  0.80486274 0.34516785]
 [0.518698   0.23693119 0.6224493  0.3021077 ]
 [0.37675434 0.40637755 0.48190802 0.4799255 ]
 [0.3602702  0.31977612 0.4757861  0.39137536]
 [0.52311987 0.34146866 0.6311355  0.41024128]
 [0.37212032 0.38038903 0.46354914 0.46545255]
 [0.69832253 0.28683236 0.7704847  0.3361332 ]
 [0.6962269  0.3805097  0.76647705 0.4332339 ]
 [0.543297   0.24946514 0.60724527 0.31224403]
 [0.37587684 0.41691843 0.44803882 0.46621928]
 [0.7220153  0.28947026 0.78770405 0.34977067]
 [0.3777119  0.337319   0.4698854  0.41683683]
 [0.55885607 0.3505047  0.62027985 0.4132836 ]
 [0.39915907 0.33490402 0.4605828  0.39520442]
 [0.53593475 0.3609697  0.60432345 0.41299072]
 [0.53769743 0.25500658 0.6286675  0.3317921 ]
 [0.53600067 0.31216216 0.6224311  0.39518034]
 [0.04420815 0.04183967 0.05564648 0.04938364]
 [0.6869807  0.36310992 0.7525046  0.41888323]
 [0.37572908 0.34481895 0.44411772 0.39546162]
 [0.6779381  0.37233624 0.77865803 0.4795095 ]
 [0.7034475  0.34004462 0.7956472  0.42216188]
 [0.75503016 0.0107995  0.86362123 0.07094846]
 [0.5601154  0.00115304 0.6290807  0.02077294]]
classes:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.
 0.]
scores:  [0.953125   0.953125   0.94140625 0.9375     0.92578125 0.921875
 0.20703125 0.12109375 0.11328125 0.109375   0.0859375  0.078125
 0.06640625 0.0625     0.05859375 0.05859375 0.05859375 0.05859375
 0.05078125 0.05078125 0.046875   0.046875   0.046875   0.046875
 0.0390625 ]
count:  25
y_min:  0.6845545
x_min:  0.36612728
y_max:  0.7911295
x_max:  0.43582943

- C++

output_locations = 8, output_classes = 8, output_scores = 8
num_detections = 25, num_classes = 4
iteration 0
        score = 25
        ymin = 0.953125
        xmin = 0.953125
        ymax = 0.941406
        xmax = 0.933594
iteration 1
        score = -5.58754e-34
        ymin = 0.925781
        xmin = 0.921875
        ymax = 0.15625
        xmax = 0.109375
iteration 2
        score = 2.14789e-08
        ymin = 0.109375
        xmin = 0.0859375
        ymax = 0.078125
        xmax = 0.0664062
iteration 3
        score = -1.38216e-38
        ymin = 0.0625
        xmin = 0.0625
        ymax = 0.0625
        xmax = 0.058593
```
</details>"
56650,Was my TensorFlow installation a success?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

binary

### Tensorflow Version

1.15.0

### Custom Code

No

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I installed Tensorflow 1.15.0 I want to verify my installation. 
So I ran the code as suggested on TensorFlow's webpage:
python -c ""import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))""

I get this:
2022-07-01 12:37:41.303565: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found

2022-07-01 12:37:41.303701: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.

Tensor(""Sum:0"", shape=(), dtype=float32)

Is my tensorflow installed correctly?
```


### Standalone code to reproduce the issue

```shell
After installation run:
python -c ""import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))""
```


### Relevant log output

```shell
(cpu_only) C:\Users\Public\DiplomskaNaloga\deep\keras-yolo3-master\keras-yolo3-master>python -c ""import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))""
2022-07-01 12:37:41.303565: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found
2022-07-01 12:37:41.303701: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Tensor(""Sum:0"", shape=(), dtype=float32)
```
</details>"
56649,"Tensorflow just ""hangs"" on new installation (cpu and gpu)","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux Lubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2/8.1.0

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Any calculation I submit to tensorflow just hangs indefinitely and never completes. This happen even when explicitly using cpu as device.

This happens with the basic example from the tutorial, see below.

Additional when importing tensorflow I get the ""well known"" NUMA error:

> successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 

To add to that this is running on a virtual machine on a server with 2 sockets. SO I wonder if I need to make some configuration to make this warning go away and if it is actually the cause of the ""hanging"". 

There is no other error message or log entry than the NUMA warning.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
mnist = tf.keras.datasets.mnist

(x_train, y_train),(x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

# this causes the issue. it hangs indefinitely
# hanging also happens when using CPU (with tf.device('/CPU:0'):)
model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)
model.evaluate(x_test, y_test)
```


### Relevant log output

_No response_</details>"
56648,How to predict a TPU-distributed dataset?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

binary

### Tensorflow Version

2.4.1

### Custom Code

Yes

### OS Platform and Distribution

kaggle (linux)

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I have some TPU-distributed dataset, created similar to the below. 

    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
    tf.config.experimental_connect_to_cluster(resolver)
    tf.tpu.experimental.initialize_tpu_system(resolver)
    print(""All devices: "", tf.config.list_logical_devices('TPU'))
    strategy = tf.distribute.TPUStrategy(resolver)

___

    2022-07-01 07:06:09.546401: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
    2022-07-01 07:06:09.549373: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib
    2022-07-01 07:06:09.549403: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)
    2022-07-01 07:06:09.549429: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (758f6ae4e228): /proc/driver/nvidia/version does not exist
    2022-07-01 07:06:09.553473: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
    To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
    2022-07-01 07:06:09.555058: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
    2022-07-01 07:06:09.561320: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
    2022-07-01 07:06:09.593314: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}
    2022-07-01 07:06:09.593360: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30042}
    2022-07-01 07:06:09.610681: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}
    2022-07-01 07:06:09.610747: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30042}
    2022-07-01 07:06:09.612180: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:30042
    All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU')]

Create tfrecords and copy data to a google storage bucket.

    dataset = create_tfrecord()
    dataset = strategy.experimental_distribute_dataset(dataset)
    copy_to_gs_bucket(dataset, bucket)

Create keras model:

    with strategy.scope():
        model = create_model()
        model.compile(optimizer, loss)

Training goes fine using `model.fit` however problems arise in inference using `model.predict`:


    ipdb>  model.predict(dataset, steps=10)
    *** TypeError: in user code:
    
        /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1478 predict_function  *
            return step_function(self, iterator)
        /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1468 step_function  **
            outputs = model.distribute_strategy.run(run_step, args=(data,))
        /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/tpu_strategy.py:279 run
            return self.extended.tpu_run(fn, args, kwargs, options)
        /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/tpu_strategy.py:1296 tpu_run
            return func(args, kwargs)
        /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/tpu_strategy.py:1345 tpu_function
            maximum_shape = tensor_shape.TensorShape([None] * rank)
    
        TypeError: can't multiply sequence by non-int of type 'NoneType'

Here's how the first item in dataset looks like:

    ipdb>  x[0]
    {'input_ids': PerReplica:{
      0: <tf.Tensor: shape=(63, 128), dtype=int64, numpy=
    array([[    0, 10431,  1045, ...,     1,     1,     1],
           [    0, 10431, 27480, ...,     1,     1,     1],
           [    0,   442, 12535, ...,     1,     1,     1],
           ...,
           [    0, 41975, 13076, ...,     1,     1,     1],
           [    0, 17265, 45803, ...,     1,     1,     1],
           [    0,  2911,    90, ...,     1,     1,     1]])>,
      1: <tf.Tensor: shape=(63, 128), dtype=int64, numpy=
    array([[    0, 10431,   360, ...,     1,     1,     1],
           [    0, 41975, 13076, ...,     1,     1,     1],
           [    0, 41975, 13076, ...,     1,     1,     1],
           ...,
           [    0,     2,     1, ...,     1,     1,     1],
           [    0,     2,     1, ...,     1,     1,     1],
           [    0,     2,     1, ...,     1,     1,     1]])>,
      2: <tf.Tensor: shape=(63, 128), dtype=int64, numpy=
    array([[0, 2, 1, ..., 1, 1, 1],
           [0, 2, 1, ..., 1, 1, 1],
           [0, 2, 1, ..., 1, 1, 1],
           ...,
           [0, 2, 1, ..., 1, 1, 1],
           [0, 2, 1, ..., 1, 1, 1],
           [0, 2, 1, ..., 1, 1, 1]])>,
      3: <tf.Tensor: shape=(63, 128), dtype=int64, numpy=
    array([[0, 2, 1, ..., 1, 1, 1],
           [0, 2, 1, ..., 1, 1, 1],
           [0, 2, 1, ..., 1, 1, 1],
           ...,
           [0, 2, 1, ..., 1, 1, 1],
           [0, 2, 1, ..., 1, 1, 1],
           [0, 2, 1, ..., 1, 1, 1]])>,
      4: <tf.Tensor: shape=(63, 128), dtype=int64, numpy=
    array([[0, 2, 1, ..., 1, 1, 1],
           [0, 2, 1, ..., 1, 1, 1],
           [0, 2, 1, ..., 1, 1, 1],
           ...,
           [0, 2, 1, ..., 1, 1, 1],
           [0, 2, 1, ..., 1, 1, 1],
           [0, 2, 1, ..., 1, 1, 1]])>,
      5: <tf.Tensor: shape=(63, 128), dtype=int64, numpy=
    array([[0, 2, 1, ..., 1, 1, 1],
           [0, 2, 1, ..., 1, 1, 1],
           [0, 2, 1, ..., 1, 1, 1],
           ...,
           [0, 2, 1, ..., 1, 1, 1],
           [0, 2, 1, ..., 1, 1, 1],
           [0, 2, 1, ..., 1, 1, 1]])>,
      6: <tf.Tensor: shape=(63, 128), dtype=int64, numpy=
    array([[0, 2, 1, ..., 1, 1, 1],
           [0, 2, 1, ..., 1, 1, 1],
           [0, 2, 1, ..., 1, 1, 1],
           ...,
           [0, 2, 1, ..., 1, 1, 1],
           [0, 2, 1, ..., 1, 1, 1],
           [0, 2, 1, ..., 1, 1, 1]])>,
      7: <tf.Tensor: shape=(61, 128), dtype=int64, numpy=
    array([[0, 2, 1, ..., 1, 1, 1],
           [0, 2, 1, ..., 1, 1, 1],
           [0, 2, 1, ..., 1, 1, 1],
           ...,
           [0, 2, 1, ..., 1, 1, 1],
           [0, 2, 1, ..., 1, 1, 1],
           [0, 2, 1, ..., 1, 1, 1]])>
    }, 'attention_mask': PerReplica:{
      0: <tf.Tensor: shape=(63, 128), dtype=int64, numpy=
    array([[1, 1, 1, ..., 0, 0, 0],
           [1, 1, 1, ..., 0, 0, 0],
           [1, 1, 1, ..., 0, 0, 0],
           ...,
           [1, 1, 1, ..., 0, 0, 0],
           [1, 1, 1, ..., 0, 0, 0],
           [1, 1, 1, ..., 0, 0, 0]])>,
      1: <tf.Tensor: shape=(63, 128), dtype=int64, numpy=
    array([[1, 1, 1, ..., 0, 0, 0],
           [1, 1, 1, ..., 0, 0, 0],
           [1, 1, 1, ..., 0, 0, 0],
           ...,
           [1, 1, 0, ..., 0, 0, 0],
           [1, 1, 0, ..., 0, 0, 0],
           [1, 1, 0, ..., 0, 0, 0]])>,
      2: <tf.Tensor: shape=(63, 128), dtype=int64, numpy=
    array([[1, 1, 0, ..., 0, 0, 0],
           [1, 1, 0, ..., 0, 0, 0],
           [1, 1, 0, ..., 0, 0, 0],
           ...,
           [1, 1, 0, ..., 0, 0, 0],
           [1, 1, 0, ..., 0, 0, 0],
           [1, 1, 0, ..., 0, 0, 0]])>,
      3: <tf.Tensor: shape=(63, 128), dtype=int64, numpy=
    array([[1, 1, 0, ..., 0, 0, 0],
           [1, 1, 0, ..., 0, 0, 0],
           [1, 1, 0, ..., 0, 0, 0],
           ...,
           [1, 1, 0, ..., 0, 0, 0],
           [1, 1, 0, ..., 0, 0, 0],
           [1, 1, 0, ..., 0, 0, 0]])>,
      4: <tf.Tensor: shape=(63, 128), dtype=int64, numpy=
    array([[1, 1, 0, ..., 0, 0, 0],
           [1, 1, 0, ..., 0, 0, 0],
           [1, 1, 0, ..., 0, 0, 0],
           ...,
           [1, 1, 0, ..., 0, 0, 0],
           [1, 1, 0, ..., 0, 0, 0],
           [1, 1, 0, ..., 0, 0, 0]])>,
      5: <tf.Tensor: shape=(63, 128), dtype=int64, numpy=
    array([[1, 1, 0, ..., 0, 0, 0],
           [1, 1, 0, ..., 0, 0, 0],
           [1, 1, 0, ..., 0, 0, 0],
           ...,
           [1, 1, 0, ..., 0, 0, 0],
           [1, 1, 0, ..., 0, 0, 0],
           [1, 1, 0, ..., 0, 0, 0]])>,
      6: <tf.Tensor: shape=(63, 128), dtype=int64, numpy=
    array([[1, 1, 0, ..., 0, 0, 0],
           [1, 1, 0, ..., 0, 0, 0],
           [1, 1, 0, ..., 0, 0, 0],
           ...,
           [1, 1, 0, ..., 0, 0, 0],
           [1, 1, 0, ..., 0, 0, 0],
           [1, 1, 0, ..., 0, 0, 0]])>,
      7: <tf.Tensor: shape=(61, 128), dtype=int64, numpy=
    array([[1, 1, 0, ..., 0, 0, 0],
           [1, 1, 0, ..., 0, 0, 0],
           [1, 1, 0, ..., 0, 0, 0],
           ...,
           [1, 1, 0, ..., 0, 0, 0],
           [1, 1, 0, ..., 0, 0, 0],
           [1, 1, 0, ..., 0, 0, 0]])>
    }}

How to obtain the predictions without getting an error?


### Standalone code to reproduce the issue

```shell
Large codebase, I'll include snippets on a need basis.
```


### Relevant log output

_No response_</details>"
56646,"How to convert tf.data.Dataset column which is converted into byte, back to string?","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.8.2

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I converted a Pandas dataframe into Tensorflow Dataset using `ratings = tf.data.Dataset.from_tensor_slices(dataframe.to_dict(orient=""list""))`. Now, all the values which were in string format in dataframe are converted into byte format. How to convert values of a particular column back to string?
```


### Standalone code to reproduce the issue

```shell
ratings = tf.data.Dataset.from_tensor_slices(dataframe.to_dict(orient=""list""))
for x in ratings.take(1).as_numpy_iterator():
  pprint.pprint(x)

Output is : 
'movie_id': b'357',
'movie_title': b""One Flew Over the Cuckoo's Nest (1975)"",
```


### Relevant log output

_No response_</details>"
56644,tf.distribute.MirroredStrategy - suggestion for improving test mean_iou for segmentation network using distributed training,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 20.04
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: No
-   **TensorFlow installed from (source or binary)**: tensorflow docker by tensorflow
-   **TensorFlow version (use command below)**: 2.5
-   **Python version**: 3.7
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**: 4xA100, 40GB
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I am using tensorflow 2.5.0 and implemented semantic segmatation network. used DeepLab_v3_plus network with ResNet101 backbone, adam optimizer and Categorical cross entropy loss to train network. I have first build code for single gpu and achieved test accuracy (mean_iou) of 54% trained for 96 epochs. Then added tf MirroredStrategy (one machine) in code to support for multi gpu training. Surprisingly with 2 gpus, training for 48 epochs, test mean_iou is just 27% and training with 4 gpus, for 24 epochs, test mean_iou can around 12% for same dataset.

* Code I have modified to support multi-gpu training from single-gpu training.
 - By following tensorflow blog for distributed training, created mirrored strategy and created model, model compilation and dataset_generator inside strategy scope. As per my understanding, by doing so, model.fit() method will take care of synchronization of gradients and distributing data on each gpus for training. Though code was running without any error, and also training time reduced compared to single gpu for same number of image training, test mean_iou keep getting worst with more number of gpus.
- Replaced BatchNormalization with SyncBatchNormalization, but no improvement.
used warmup learning rate with linear scaling of learning rate with number of gpus, but no improvement.

- in cross entropy loss, used both losses_utils.ReductionV2.AUTO and losses_utils.ReductionV2.NONE.
```
loss = ce(y_true, y_pred)
# reshape loss for each sample (BxHxWxC -> BxN)
# Normalize loss by number of non zero elements and sum for each sample and mean across all samples.
```
using .AUTO/.NONE options, I am not scaling loss by global_batch_size understanding tf will take care of it and I am already normalizing for each gpus. but with both options, didn't get any luck.

- changed data_generator to tf.data.Dataset obj. Though it has helped in training time, but test mean_iou become even worst.
I would appreciate if any lead or suggestion for improving test_iou in distributed training. let me know if you need any additional details.

Thank you

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
56643,tflite GRU ops not found error,"**System information**
- unity for ios
- TensorFlow installed from (source or binary): https://github.com/asus4/tf-lite-unity-sample
- TensorFlow version (or github SHA if from source): 2.8.0


hello
I use GRU in keras to train a model, then convert it to tflite and use it on mobile (= Unity).
Training and tflite conversion are all successful, but when I inference in Unity it fails with ops not found error.
There seems to be no problem with both input and output when performing inference, but it seems that the GRU layer is not fully supported by tflite.
Can this be solved by changing the model to an RNN-type layer, that is, reconstructing the model using only operations supported by tflite?

keras model code :

```
model = Sequential()
model.add(GRU(units=hidden_units, input_length=window_size, input_dim=input_dim))
model.add(Dense(num_classes, activation='softmax'))

_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 gru (GRU)                   (None, 32)                3456      
                                                                 
 dense (Dense)               (None, 2)                 66        
                                                                 
=================================================================
Total params: 3,522
Trainable params: 3,522
Non-trainable params: 0
_________________________________________________________________

```

keras to tflite convert code :

```
keras_model_name = ""squat_model_rnn_multi_input_dim.h5""
tflite_model_name = ""squat_model_rnn_multi_input_dim.tflite""
keras_model = load_model(keras_model_name)
converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
converter._experimental_lower_tensor_list_ops = False
tflite_model = converter.convert()
f = open(tflite_model_name, 'wb')
f.write(tflite_model)
f.close()
```

unity inference code :

```
var modelBytes = Resources.Load<TextAsset>(""squat_model_rnn_multi_input_dim.tflite"").bytes;
var options = new InterpreterOptions();
options.threads = SystemInfo.processorCount;
var interpreter = new Interpreter(modelBytes, options);
interpreter.LogIOInfo();
_inDim = interpreter.GetInputTensorInfo(0).shape;
_outDim = interpreter.GetOutputTensorInfo(0).shape;
interpreter.ResizeInputTensor(0, _inDim);
interpreter.AllocateTensors();
var inData = new float[50, 2];
var outData = new float[2];

try
{
    interpreter.SetInputTensorData(0, inData);
    interpreter.Invoke();
    interpreter.GetOutputTensorData(0, outData);
}
catch (Exception e)
{
    LogSloth.exception(e);
}
```

unity error log : 

```
System.Exception: Ops not found.
  at TensorFlowLite.Interpreter.ThrowIfError (TensorFlowLite.Interpreter+Status status) [0x00071] in /Users/hhd/project/bluevalley/Packages/com.github.asus4.tflite/Runtime/Interpreter.cs:223 
  at TensorFlowLite.Interpreter.Invoke () [0x00001] in /Users/hhd/project/bluevalley/Packages/com.github.asus4.tflite/Runtime/Interpreter.cs:105 
  at BvSquatNet..ctor () [0x0010f] in /Users/hhd/project/bluevalley/Assets/02Scripts/FilterGraph/BvSquatNet.cs:41
```"
56640,Cannot use LSTM with a photo input,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I need to classify MFCC Spectrogram images from collected sound data using an RNN with an LSTM. The input shape of my image casted to a numpy array is (288, 864, 4). The LSTM Model cannot accept a three dimensional input, apparently. What is the solution to this? How can one classify images using an LSTM? I don't know of a great way to convert the 3-D numpy array of an image to a format that works. However, if someone knows a format that is compatible with LSTMs, that would answer my question, along with any way to have a 3-D input shape.
```


### Standalone code to reproduce the issue

```shell
model.add(LSTM(128,input_shape=(288, 864, 4)))
```


### Relevant log output

```shell
ValueError: Input 0 of layer ""lstm_1"" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 288, 864, 4)
```
</details>"
56639,Build Error: cannot convert float* to float_t* {aka long double*} in assignment,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.9

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

5.0.0

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

11.3.1/8.2.1.32

### GPU model and memory

None, Using cuda stub

### Current Behaviour?

```shell
Bazel errors out while compiling tensorflow/stream_executor/tpu/c_api_conversions.cc with a data type conversion error.  I'm not sure why it's compiling TPU related items at all, since I have not enabled it.

The exact same build environment and settings (it's running in docker) work for TF 2.6, 2.7, and 2.8 (using their own respective versions of Bazel based on https://www.tensorflow.org/install/source#gpu), but do not work for 2.9.0 or 2.9.1.

The reason I'm compiling from source is to be able to use CUDA 11.3 and cuDNN 8.2 for PyTorch compatibility, as well as specify my own cuda architecture list.
```


### Standalone code to reproduce the issue

```shell
git clone https://github.com/tensorflow/tensorflow
cd tensorflow
git checkout ""tags/v2.9.1
cat /dev/null | ./configure
bazel build --config=opt \
    --config=v2 \
    --config=cuda \
    --copt=-mfpmath=both \
    --verbose_failures \
    --define=with_tpu_support=false \
    //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
----------
--- Initial Bazel Start Printout
---------

INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=200
INFO: Reading rc options for 'build' from /usr/local/src/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /usr/local/src/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library
INFO: Reading rc options for 'build' from /usr/local/src/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/bin/python3 --action_env PYTHON_LIB_PATH=/opt/conda/lib/python3.9/site-packages --python_path=/opt/conda/bin/python3 --action_env TF_CUDA_VERSION=11.3 --action_env TF_CUDNN_VERSION=8 --action_env TF_NCCL_VERSION=2 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.3 --action_env NCCL_INSTALL_PATH=/usr/ --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.0,6.1,7.0,7.5,8.0,8.6 --action_env LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/lib64/stubs:/opt/conda/lib:/usr/lib --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-9 --config=cuda
INFO: Reading rc options for 'build' from /usr/local/src/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /usr/local/src/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /usr/local/src/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda in file /usr/local/src/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:opt in file /usr/local/src/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare
INFO: Found applicable config definition build:v2 in file /usr/local/src/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda in file /usr/local/src/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:linux in file /usr/local/src/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /usr/local/src/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS



----------
--- The error itself
---------


ERROR: /usr/local/src/tensorflow/tensorflow/stream_executor/tpu/BUILD:44:11: Compiling tensorflow/stream_executor/tpu/c_api_conversions.cc failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command
  (cd /root/.cache/bazel/_bazel_root/bbcc73fcc5c2b01ab08b6bcf7c29e42e/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda-11.3 \
    GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-9 \
    LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/lib64/stubs:/opt/conda/lib:/usr/lib \
    NCCL_INSTALL_PATH=/usr/ \
    PATH=/opt/conda/bin:/opt/conda/condabin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/opt/conda/bin/python3 \
    PYTHON_LIB_PATH=/opt/conda/lib/python3.9/site-packages \
    TF2_BEHAVIOR=1 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.0,6.1,7.0,7.5,8.0,8.6 \
    TF_CUDA_VERSION=11.3 \
    TF_CUDNN_VERSION=8 \
    TF_NCCL_VERSION=2 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/stream_executor/tpu/_objs/c_api_conversions/c_api_conversions.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/stream_executor/tpu/_objs/c_api_conversions/c_api_conversions.pic.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DLLVM_ON_UNIX=1' '-DHAVE_BACKTRACE=1' '-DBACKTRACE_HEADER=<execinfo.h>' '-DLTDL_SHLIB_EXT="".so""' '-DLLVM_PLUGIN_EXT="".so""' '-DLLVM_ENABLE_THREADS=1' '-DHAVE_DEREGISTER_FRAME=1' '-DHAVE_LIBPTHREAD=1' '-DHAVE_PTHREAD_GETNAME_NP=1' '-DHAVE_PTHREAD_GETSPECIFIC=1' '-DHAVE_PTHREAD_H=1' '-DHAVE_PTHREAD_SETNAME_NP=1' '-DHAVE_REGISTER_FRAME=1' '-DHAVE_SETENV_R=1' '-DHAVE_STRERROR_R=1' '-DHAVE_SYSEXITS_H=1' '-DHAVE_UNISTD_H=1' -D_GNU_SOURCE '-DHAVE_LINK_H=1' '-DHAVE_LSEEK64=1' '-DHAVE_MALLINFO=1' '-DHAVE_SBRK=1' '-DHAVE_STRUCT_STAT_ST_MTIM_TV_NSEC=1' '-DLLVM_NATIVE_ARCH=""X86""' '-DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser' '-DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter' '-DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler' '-DLLVM_NATIVE_TARGET=LLVMInitializeX86Target' '-DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo' '-DLLVM_NATIVE_TARGETMC=LLVMInitializeX86TargetMC' '-DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA' '-DLLVM_HOST_TRIPLE=""x86_64-unknown-linux-gnu""' '-DLLVM_DEFAULT_TARGET_TRIPLE=""x86_64-unknown-linux-gnu""' -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -DBLAKE3_NO_AVX2 -DBLAKE3_NO_AVX512 -DBLAKE3_NO_SSE2 -DBLAKE3_NO_SSE41 '-DBLAKE3_USE_NEON=0' -iquote . -iquote bazel-out/k8-opt/bin -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/gif -iquote bazel-out/k8-opt/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/k8-opt/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/k8-opt/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/k8-opt/bin/external/zlib -iquote external/double_conversion -iquote bazel-out/k8-opt/bin/external/double_conversion -iquote external/local_config_cuda -iquote bazel-out/k8-opt/bin/external/local_config_cuda -iquote external/local_config_rocm -iquote bazel-out/k8-opt/bin/external/local_config_rocm -iquote external/local_config_tensorrt -iquote bazel-out/k8-opt/bin/external/local_config_tensorrt -iquote external/llvm-project -iquote bazel-out/k8-opt/bin/external/llvm-project -iquote external/llvm_terminfo -iquote bazel-out/k8-opt/bin/external/llvm_terminfo -iquote external/llvm_zlib -iquote bazel-out/k8-opt/bin/external/llvm_zlib -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributeInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinDialectIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinLocationAttributesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinTypeInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinTypesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/CallOpInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/CastOpInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/FunctionInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/InferTypeOpInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/OpAsmInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/RegionKindInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SideEffectInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SubElementInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SymbolInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/TensorEncodingIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ControlFlowInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ViewLikeInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/LoopLikeInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLTypesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLInterpOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ConversionPassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/TransformsPassIncGen -isystem external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/k8-opt/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_rocm/rocm -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm -isystem external/local_config_rocm/rocm/rocm/include -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include -isystem external/local_config_rocm/rocm/rocm/include/rocrand -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include/rocrand -isystem external/local_config_rocm/rocm/rocm/include/roctracer -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include/roctracer -isystem external/llvm-project/llvm/include -isystem bazel-out/k8-opt/bin/external/llvm-project/llvm/include -isystem external/llvm-project/mlir/include -isystem bazel-out/k8-opt/bin/external/llvm-project/mlir/include -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -w -DAUTOLOAD_DYNAMIC_KERNELS -Wno-sign-compare '-mfpmath=both' '-std=c++14' -c tensorflow/stream_executor/tpu/c_api_conversions.cc -o bazel-out/k8-opt/bin/tensorflow/stream_executor/tpu/_objs/c_api_conversions/c_api_conversions.pic.o)
# Configuration: 5135a0d180cd03b46d94039319d4ac616ab56e014454d6a1ecebd6ac140e7590
# Execution platform: @local_execution_config_platform//:platform
tensorflow/stream_executor/tpu/c_api_conversions.cc: In instantiation of void ApiConverter::CreateVectorBase(absl::lts_20211102::Span<T>, DstList*) [with Src = const float; Dst = float; DstList = FloatList]:
tensorflow/stream_executor/tpu/c_api_conversions.cc:175:66:   required from here
tensorflow/stream_executor/tpu/c_api_conversions.cc:164:15: error: cannot convert float* to float_t* {aka long double*} in assignment
  164 |     dst->heap = new Dst[dst->size];
      |     ~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~
tensorflow/stream_executor/tpu/c_api_conversions.cc: In instantiation of absl::lts_20211102::Span<const T> ApiConverter::MakeSpanBase(const SrcList&) [with Dst = float; Src = float; SrcList = FloatList]:
tensorflow/stream_executor/tpu/c_api_conversions.cc:214:56:   required from here
tensorflow/stream_executor/tpu/c_api_conversions.cc:203:58: error: cannot convert const float_t* {aka const long double*} to const float* in initialization
  203 |   const Src* src = src_list.size > TPU_C_API_MAX_INLINED ? src_list.heap
      |                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~
      |                                                          |
      |                                                          const float_t* {aka const long double*}
  204 |                                                          : &src_list.inlined[0];
      |                                                          ~~~~~~~~~~~~~~~~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build


----------
--- The tf_env_collect.sh result
---------


== check python ===================================================
python version: 3.9.13
python branch:
python build version: ('main', 'May 27 2022 16:56:21')
python compiler version: GCC 10.3.0
python implementation: CPython


== check os platform ===============================================

== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Copyright (C) 2019 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== check pips ===================================================
numpy                         1.22.4
proto-plus                    1.20.6
protobuf                      3.19.4

== check for virtualenv =========================================
False

== tensorflow import ============================================
    156308:	find library=libpthread.so.0 [0]; searching
    156308:	 search path=/opt/conda/bin/../lib/tls/x86_64/x86_64:/opt/conda/bin/../lib/tls/x86_64:/opt/conda/bin/../lib/tls/x86_64:/opt/conda/bin/../lib/tls:/opt/conda/bin/../lib/x86_64/x86_64:/opt/conda/bin/../lib/x86_64:/opt/conda/bin/../lib/x86_64:/opt/conda/bin/../lib		(RPATH from file /opt/conda/bin/python)
    156308:	  trying file=/opt/conda/bin/../lib/tls/x86_64/x86_64/libpthread.so.0
    156308:	  trying file=/opt/conda/bin/../lib/tls/x86_64/libpthread.so.0
    156308:	  trying file=/opt/conda/bin/../lib/tls/x86_64/libpthread.so.0
    156308:	  trying file=/opt/conda/bin/../lib/tls/libpthread.so.0
    156308:	  trying file=/opt/conda/bin/../lib/x86_64/x86_64/libpthread.so.0
    156308:	  trying file=/opt/conda/bin/../lib/x86_64/libpthread.so.0
    156308:	  trying file=/opt/conda/bin/../lib/x86_64/libpthread.so.0
    156308:	  trying file=/opt/conda/bin/../lib/libpthread.so.0
    156308:	 search path=/usr/local/cuda/extras/CUPTI/lib64/tls/x86_64/x86_64:/usr/local/cuda/extras/CUPTI/lib64/tls/x86_64:/usr/local/cuda/extras/CUPTI/lib64/tls/x86_64:/usr/local/cuda/extras/CUPTI/lib64/tls:/usr/local/cuda/extras/CUPTI/lib64/x86_64/x86_64:/usr/local/cuda/extras/CUPTI/lib64/x86_64:/usr/local/cuda/extras/CUPTI/lib64/x86_64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib64/tls/x86_64/x86_64:/usr/local/nvidia/lib64/tls/x86_64:/usr/local/nvidia/lib64/tls/x86_64:/usr/local/nvidia/lib64/tls:/usr/local/nvidia/lib64/x86_64/x86_64:/usr/local/nvidia/lib64/x86_64:/usr/local/nvidia/lib64/x86_64:/usr/local/nvidia/lib64:/usr/local/cuda/lib64/tls/x86_64/x86_64:/usr/local/cuda/lib64/tls/x86_64:/usr/local/cuda/lib64/tls/x86_64:/usr/local/cuda/lib64/tls:/usr/local/cuda/lib64/x86_64/x86_64:/usr/local/cuda/lib64/x86_64:/usr/local/cuda/lib64/x86_64:/usr/local/cuda/lib64:/usr/local/cuda/lib64/stubs/tls/x86_64/x86_64:/usr/local/cuda/lib64/stubs/tls/x86_64:/usr/local/cuda/lib64/stubs/tls/x86_64:/usr/local/cuda/lib64/stubs/tls:/usr/local/cuda/lib64/stubs/x86_64/x86_64:/usr/local/cuda/lib64/stubs/x86_64:/usr/local/cuda/lib64/stubs/x86_64:/usr/local/cuda/lib64/stubs:/opt/conda/lib/tls/x86_64/x86_64:/opt/conda/lib/tls/x86_64:/opt/conda/lib/tls/x86_64:/opt/conda/lib/tls:/opt/conda/lib/x86_64/x86_64:/opt/conda/lib/x86_64:/opt/conda/lib/x86_64:/opt/conda/lib		(LD_LIBRARY_PATH)
    156308:	  trying file=/usr/local/cuda/extras/CUPTI/lib64/tls/x86_64/x86_64/libpthread.so.0
    156308:	  trying file=/usr/local/cuda/extras/CUPTI/lib64/tls/x86_64/libpthread.so.0
    156308:	  trying file=/usr/local/cuda/extras/CUPTI/lib64/tls/x86_64/libpthread.so.0
    156308:	  trying file=/usr/local/cuda/extras/CUPTI/lib64/tls/libpthread.so.0
    156308:	  trying file=/usr/local/cuda/extras/CUPTI/lib64/x86_64/x86_64/libpthread.so.0
    156308:	  trying file=/usr/local/cuda/extras/CUPTI/lib64/x86_64/libpthread.so.0
    156308:	  trying file=/usr/local/cuda/extras/CUPTI/lib64/x86_64/libpthread.so.0
    156308:	  trying file=/usr/local/cuda/extras/CUPTI/lib64/libpthread.so.0
    156308:	  trying file=/usr/local/nvidia/lib64/tls/x86_64/x86_64/libpthread.so.0
    156308:	  trying file=/usr/local/nvidia/lib64/tls/x86_64/libpthread.so.0
    156308:	  trying file=/usr/local/nvidia/lib64/tls/x86_64/libpthread.so.0
    156308:	  trying file=/usr/local/nvidia/lib64/tls/libpthread.so.0
    156308:	  trying file=/usr/local/nvidia/lib64/x86_64/x86_64/libpthread.so.0
    156308:	  trying file=/usr/local/nvidia/lib64/x86_64/libpthread.so.0
    156308:	  trying file=/usr/local/nvidia/lib64/x86_64/libpthread.so.0
    156308:	  trying file=/usr/local/nvidia/lib64/libpthread.so.0
    156308:	  trying file=/usr/local/cuda/lib64/tls/x86_64/x86_64/libpthread.so.0
    156308:	  trying file=/usr/local/cuda/lib64/tls/x86_64/libpthread.so.0
    156308:	  trying file=/usr/local/cuda/lib64/tls/x86_64/libpthread.so.0
    156308:	  trying file=/usr/local/cuda/lib64/tls/libpthread.so.0
    156308:	  trying file=/usr/local/cuda/lib64/x86_64/x86_64/libpthread.so.0
    156308:	  trying file=/usr/local/cuda/lib64/x86_64/libpthread.so.0
    156308:	  trying file=/usr/local/cuda/lib64/x86_64/libpthread.so.0
    156308:	  trying file=/usr/local/cuda/lib64/libpthread.so.0
    156308:	  trying file=/usr/local/cuda/lib64/stubs/tls/x86_64/x86_64/libpthread.so.0
    156308:	  trying file=/usr/local/cuda/lib64/stubs/tls/x86_64/libpthread.so.0
    156308:	  trying file=/usr/local/cuda/lib64/stubs/tls/x86_64/libpthread.so.0
    156308:	  trying file=/usr/local/cuda/lib64/stubs/tls/libpthread.so.0
    156308:	  trying file=/usr/local/cuda/lib64/stubs/x86_64/x86_64/libpthread.so.0
    156308:	  trying file=/usr/local/cuda/lib64/stubs/x86_64/libpthread.so.0
    156308:	  trying file=/usr/local/cuda/lib64/stubs/x86_64/libpthread.so.0
    156308:	  trying file=/usr/local/cuda/lib64/stubs/libpthread.so.0
    156308:	  trying file=/opt/conda/lib/tls/x86_64/x86_64/libpthread.so.0
    156308:	  trying file=/opt/conda/lib/tls/x86_64/libpthread.so.0
    156308:	  trying file=/opt/conda/lib/tls/x86_64/libpthread.so.0
    156308:	  trying file=/opt/conda/lib/tls/libpthread.so.0
    156308:	  trying file=/opt/conda/lib/x86_64/x86_64/libpthread.so.0
    156308:	  trying file=/opt/conda/lib/x86_64/libpthread.so.0
    156308:	  trying file=/opt/conda/lib/x86_64/libpthread.so.0
    156308:	  trying file=/opt/conda/lib/libpthread.so.0
    156308:	 search path=/usr/lib/tls/x86_64/x86_64:/usr/lib/tls/x86_64:/usr/lib/tls/x86_64:/usr/lib/tls:/usr/lib/x86_64/x86_64:/usr/lib/x86_64:/usr/lib/x86_64:/usr/lib		(system search path)
    156308:	  trying file=/usr/lib/tls/x86_64/x86_64/libpthread.so.0
    156308:	  trying file=/usr/lib/tls/x86_64/libpthread.so.0
    156308:	  trying file=/usr/lib/tls/x86_64/libpthread.so.0
    156308:	  trying file=/usr/lib/tls/libpthread.so.0
    156308:	  trying file=/usr/lib/x86_64/x86_64/libpthread.so.0
    156308:	  trying file=/usr/lib/x86_64/libpthread.so.0
    156308:	  trying file=/usr/lib/x86_64/libpthread.so.0
    156308:	  trying file=/usr/lib/libpthread.so.0
    156308:	 search cache=/etc/ld.so.cache
    156308:	  trying file=/lib/x86_64-linux-gnu/libpthread.so.0
    156308:
    156308:	find library=libdl.so.2 [0]; searching
    156308:	 search path=/opt/conda/bin/../lib		(RPATH from file /opt/conda/bin/python)
    156308:	  trying file=/opt/conda/bin/../lib/libdl.so.2
    156308:	 search path=/usr/local/cuda/lib64:/usr/local/cuda/lib64/stubs:/opt/conda/lib		(LD_LIBRARY_PATH)
    156308:	  trying file=/usr/local/cuda/lib64/libdl.so.2
    156308:	  trying file=/usr/local/cuda/lib64/stubs/libdl.so.2
    156308:	  trying file=/opt/conda/lib/libdl.so.2
    156308:	 search path=/usr/lib		(system search path)
    156308:	  trying file=/usr/lib/libdl.so.2
    156308:	 search cache=/etc/ld.so.cache
    156308:	  trying file=/lib/x86_64-linux-gnu/libdl.so.2
    156308:
    156308:	find library=libutil.so.1 [0]; searching
    156308:	 search path=/opt/conda/bin/../lib		(RPATH from file /opt/conda/bin/python)
    156308:	  trying file=/opt/conda/bin/../lib/libutil.so.1
    156308:	 search path=/usr/local/cuda/lib64:/usr/local/cuda/lib64/stubs:/opt/conda/lib		(LD_LIBRARY_PATH)
    156308:	  trying file=/usr/local/cuda/lib64/libutil.so.1
    156308:	  trying file=/usr/local/cuda/lib64/stubs/libutil.so.1
    156308:	  trying file=/opt/conda/lib/libutil.so.1
    156308:	 search path=/usr/lib		(system search path)
    156308:	  trying file=/usr/lib/libutil.so.1
    156308:	 search cache=/etc/ld.so.cache
    156308:	  trying file=/lib/x86_64-linux-gnu/libutil.so.1
    156308:
    156308:	find library=librt.so.1 [0]; searching
    156308:	 search path=/opt/conda/bin/../lib		(RPATH from file /opt/conda/bin/python)
    156308:	  trying file=/opt/conda/bin/../lib/librt.so.1
    156308:	 search path=/usr/local/cuda/lib64:/usr/local/cuda/lib64/stubs:/opt/conda/lib		(LD_LIBRARY_PATH)
    156308:	  trying file=/usr/local/cuda/lib64/librt.so.1
    156308:	  trying file=/usr/local/cuda/lib64/stubs/librt.so.1
    156308:	  trying file=/opt/conda/lib/librt.so.1
    156308:	 search path=/usr/lib		(system search path)
    156308:	  trying file=/usr/lib/librt.so.1
    156308:	 search cache=/etc/ld.so.cache
    156308:	  trying file=/lib/x86_64-linux-gnu/librt.so.1
    156308:
    156308:	find library=libm.so.6 [0]; searching
    156308:	 search path=/opt/conda/bin/../lib		(RPATH from file /opt/conda/bin/python)
    156308:	  trying file=/opt/conda/bin/../lib/libm.so.6
    156308:	 search path=/usr/local/cuda/lib64:/usr/local/cuda/lib64/stubs:/opt/conda/lib		(LD_LIBRARY_PATH)
    156308:	  trying file=/usr/local/cuda/lib64/libm.so.6
    156308:	  trying file=/usr/local/cuda/lib64/stubs/libm.so.6
    156308:	  trying file=/opt/conda/lib/libm.so.6
    156308:	 search path=/usr/lib		(system search path)
    156308:	  trying file=/usr/lib/libm.so.6
    156308:	 search cache=/etc/ld.so.cache
    156308:	  trying file=/lib/x86_64-linux-gnu/libm.so.6
    156308:
    156308:	find library=libc.so.6 [0]; searching
    156308:	 search path=/opt/conda/bin/../lib		(RPATH from file /opt/conda/bin/python)
    156308:	  trying file=/opt/conda/bin/../lib/libc.so.6
    156308:	 search path=/usr/local/cuda/lib64:/usr/local/cuda/lib64/stubs:/opt/conda/lib		(LD_LIBRARY_PATH)
    156308:	  trying file=/usr/local/cuda/lib64/libc.so.6
    156308:	  trying file=/usr/local/cuda/lib64/stubs/libc.so.6
    156308:	  trying file=/opt/conda/lib/libc.so.6
    156308:	 search path=/usr/lib		(system search path)
    156308:	  trying file=/usr/lib/libc.so.6
    156308:	 search cache=/etc/ld.so.cache
    156308:	  trying file=/lib/x86_64-linux-gnu/libc.so.6
    156308:
    156308:
    156308:	calling init: /lib/x86_64-linux-gnu/libpthread.so.0
    156308:
    156308:
    156308:	calling init: /lib/x86_64-linux-gnu/libc.so.6
    156308:
    156308:
    156308:	calling init: /lib/x86_64-linux-gnu/libm.so.6
    156308:
    156308:
    156308:	calling init: /lib/x86_64-linux-gnu/librt.so.1
    156308:
    156308:
    156308:	calling init: /lib/x86_64-linux-gnu/libutil.so.1
    156308:
    156308:
    156308:	calling init: /lib/x86_64-linux-gnu/libdl.so.2
    156308:
    156308:
    156308:	initialize program: /opt/conda/bin/python
    156308:
    156308:
    156308:	transferring control: /opt/conda/bin/python
    156308:
    156308:
    156308:	calling init: /opt/conda/lib/python3.9/lib-dynload/_heapq.cpython-39-x86_64-linux-gnu.so
    156308:
    156308:	find library=libffi.so.8 [0]; searching
    156308:	 search path=/opt/conda/lib/python3.9/lib-dynload/../../tls/x86_64/x86_64:/opt/conda/lib/python3.9/lib-dynload/../../tls/x86_64:/opt/conda/lib/python3.9/lib-dynload/../../tls/x86_64:/opt/conda/lib/python3.9/lib-dynload/../../tls:/opt/conda/lib/python3.9/lib-dynload/../../x86_64/x86_64:/opt/conda/lib/python3.9/lib-dynload/../../x86_64:/opt/conda/lib/python3.9/lib-dynload/../../x86_64:/opt/conda/lib/python3.9/lib-dynload/../..		(RPATH from file /opt/conda/lib/python3.9/lib-dynload/_ctypes.cpython-39-x86_64-linux-gnu.so)
    156308:	  trying file=/opt/conda/lib/python3.9/lib-dynload/../../tls/x86_64/x86_64/libffi.so.8
    156308:	  trying file=/opt/conda/lib/python3.9/lib-dynload/../../tls/x86_64/libffi.so.8
    156308:	  trying file=/opt/conda/lib/python3.9/lib-dynload/../../tls/x86_64/libffi.so.8
    156308:	  trying file=/opt/conda/lib/python3.9/lib-dynload/../../tls/libffi.so.8
    156308:	  trying file=/opt/conda/lib/python3.9/lib-dynload/../../x86_64/x86_64/libffi.so.8
    156308:	  trying file=/opt/conda/lib/python3.9/lib-dynload/../../x86_64/libffi.so.8
    156308:	  trying file=/opt/conda/lib/python3.9/lib-dynload/../../x86_64/libffi.so.8
    156308:	  trying file=/opt/conda/lib/python3.9/lib-dynload/../../libffi.so.8
    156308:
    156308:
    156308:	calling init: /opt/conda/lib/python3.9/lib-dynload/../../libffi.so.8
    156308:
    156308:
    156308:	calling init: /opt/conda/lib/python3.9/lib-dynload/_ctypes.cpython-39-x86_64-linux-gnu.so
    156308:
    156308:
    156308:	calling init: /opt/conda/lib/python3.9/lib-dynload/_struct.cpython-39-x86_64-linux-gnu.so
    156308:
Traceback (most recent call last):
  File ""/usr/local/src/tensorflow/tensorflow/python/platform/self_check.py"", line 23, in <module>
    from tensorflow.python.platform import build_info
ImportError: cannot import name 'build_info' from 'tensorflow.python.platform' (/usr/local/src/tensorflow/tensorflow/python/platform/__init__.py)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/usr/local/src/tensorflow/tensorflow/__init__.py"", line 20, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/usr/local/src/tensorflow/tensorflow/python/__init__.py"", line 36, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""/usr/local/src/tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 21, in <module>
    from tensorflow.python.platform import self_check
  File ""/usr/local/src/tensorflow/tensorflow/python/platform/self_check.py"", line 25, in <module>
    raise ImportError(""Could not import tensorflow. Do not import tensorflow ""
ImportError: Could not import tensorflow. Do not import tensorflow from its source directory; change directory to outside the TensorFlow source tree, and relaunch your Python interpreter from there.
    156308:
    156308:	calling fini: /opt/conda/bin/python [0]
    156308:
    156308:
    156308:	calling fini: /lib/x86_64-linux-gnu/libutil.so.1 [0]
    156308:
    156308:
    156308:	calling fini: /lib/x86_64-linux-gnu/librt.so.1 [0]
    156308:
    156308:
    156308:	calling fini: /lib/x86_64-linux-gnu/libm.so.6 [0]
    156308:
    156308:
    156308:	calling fini: /opt/conda/lib/python3.9/lib-dynload/_heapq.cpython-39-x86_64-linux-gnu.so [0]
    156308:
    156308:
    156308:	calling fini: /opt/conda/lib/python3.9/lib-dynload/_ctypes.cpython-39-x86_64-linux-gnu.so [0]
    156308:
    156308:
    156308:	calling fini: /lib/x86_64-linux-gnu/libdl.so.2 [0]
    156308:
    156308:
    156308:	calling fini: /lib/x86_64-linux-gnu/libpthread.so.0 [0]
    156308:
    156308:
    156308:	calling fini: /opt/conda/lib/python3.9/lib-dynload/../../libffi.so.8 [0]
    156308:
    156308:
    156308:	calling fini: /opt/conda/lib/python3.9/lib-dynload/_struct.cpython-39-x86_64-linux-gnu.so [0]
    156308:

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/lib64/stubs:/opt/conda/lib:/usr/lib
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tools/tf_env_collect.sh: line 147: nvidia-smi: command not found

== cuda libs  ===================================================
/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudart.so.11.3.109

== tensorflow installed from info ==================

== python version  ==============================================
(major, minor, micro, releaselevel, serial)
(3, 9, 13, 'final', 0)

== bazel version  ===============================================
Build label: 5.0.0
Build time: Wed Jan 19 14:08:54 2022 (1642601334)
Build timestamp: 1642601334
Build timestamp as int: 1642601334
```
</details>"
56638,Keras saved model with custom layer under MirroredStrategy scope cannot be used correctly,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

binary

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18.04.5

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

4  NVIDIA V100 16GB GPUs

### Current Behaviour?

```shell
A Keras model with a custom layer has been saved within MirrorStrategy score. Then I was trying to use another script to load the saved model within a new MirroredStrategy scope, but it did not work work as expected in that only one GPU has been utilized whereas there are 4 GPUs actually.

What I find interestring is that if I save and load the model in the same script using the same MirroredStrategy scope, it works. But it did not work if I use a another script to do so.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import tensorflow.distribute as tf_dist
from tensorflow.python.distribute import distribute_lib
from tensorflow.python.ops import array_ops
import numpy as np
import os

os.environ[""CUDA_VISIBLE_DEVICES""] = ""0,1,2,3""

def _get_current_replica_id_in_group_sync():
    replica_ctx = tf_dist.get_replica_context()
    if replica_ctx:
        replica_id = replica_ctx.replica_id_in_sync_group
    else:
        replica_id = distribute_lib.get_update_replica_id()
    if replica_id is None:
        replica_id = array_ops.constant(0, dtype=array_ops.dtypes.int32)
    return replica_id

def test(values):
    global_replica_id = _get_current_replica_id_in_group_sync()
    tf.print(""global_replica_id: {}"".format(global_replica_id))
    vector  = tf.zeros_like(values)
    return vector


class TestLayer(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(TestLayer, self).__init__(**kwargs)

    # @tf.function
    def call(self, inputs, training=False):
        emb_vector = test(values = inputs)
        return emb_vector

class Demo(tf.keras.models.Model):
    def __init__(self, **kwargs):
        super(Demo, self).__init__(**kwargs)
        
        self.test_layer = TestLayer()        
        self.dense_layer = tf.keras.layers.Dense(units=1, activation=None,
                                                 kernel_initializer=""ones"",
                                                 bias_initializer=""zeros"")

    def call(self, inputs):
        vector = self.test_layer(inputs)
        logit = self.dense_layer(vector)
        return logit, vector

    def summary(self):
        inputs = tf.keras.Input(shape=(10,), dtype=tf.int64)
        model = tf.keras.models.Model(inputs=inputs, outputs=self.call(inputs))
        return model.summary()

@tf.function
def _step(inputs, labels):
    logit, vector = model(inputs)
    return logit, vector

def tf_dataset(keys, labels, batchsize, repeat):
    dataset = tf.data.Dataset.from_tensor_slices((keys, labels))
    dataset = dataset.repeat(repeat)
    dataset = dataset.batch(batchsize, drop_remainder=True)
    return dataset

def _dataset_fn(input_context):
    global_batch_size = 16384
    keys = np.ones((global_batch_size, 10))
    labels = np.random.randint(low=0, high=2, size=(global_batch_size, 1))
    replica_batch_size = input_context.get_per_replica_batch_size(global_batch_size)
    dataset = tf_dataset(keys, labels, batchsize=replica_batch_size, repeat=1)
    dataset = dataset.shard(input_context.num_input_pipelines, input_context.input_pipeline_id)
    return dataset

# Save model within MirroredStrategy scope
strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
    model = Demo()
model.compile()
model.summary()
dataset = strategy.distribute_datasets_from_function(_dataset_fn)
for i, (key_tensors, replica_labels) in enumerate(dataset):
    print(""-"" * 30, ""step "", str(i), ""-"" * 30)
    logit, vector = strategy.run(_step, args=(key_tensors, replica_labels))
model.save(""demo"")

# Load model within MirroredStrategy scope. It works if this part is in the same script of the saving model part. But it did not when using a new script and a new scope to do this part.
with strategy.scope():
    model = tf.keras.models.load_model(""demo"")
dataset = strategy.distribute_datasets_from_function(_dataset_fn)
for i, (key_tensors, replica_labels) in enumerate(dataset):
    print(""-"" * 30, ""step "", str(i), ""-"" * 30)
    logit, vector = strategy.run(_step, args=(key_tensors, replica_labels))
```


### Relevant log output

```shell
If saving and loading model in the same script as above, the log is:

2022-06-30 12:57:46.413337: I tensorflow/core/platform/cpu_feature_guard.cc:152] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-06-30 12:57:48.175920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14649 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:06:00.0, compute capability: 7.0
2022-06-30 12:57:48.177528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 14649 MB memory:  -> device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:07:00.0, compute capability: 7.0
2022-06-30 12:57:48.178765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 14649 MB memory:  -> device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:0a:00.0, compute capability: 7.0
2022-06-30 12:57:48.180070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 14649 MB memory:  -> device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:0b:00.0, compute capability: 7.0
Model: ""model""
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 input_1 (InputLayer)        [(None, 10)]              0

 test_layer (TestLayer)      (None, 10)                0

 dense (Dense)               (None, 1)                 11

=================================================================
Total params: 11
Trainable params: 11
Non-trainable params: 0
_________________________________________________________________
------------------------------ step  0 ------------------------------
global_replica_id: Tensor(""demo/test_layer/replica_id_in_sync_group:0"", shape=(), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)
global_replica_id: Tensor(""replica_1/demo/test_layer/replica_id_in_sync_group:0"", shape=(), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:1)
global_replica_id: Tensor(""replica_2/demo/test_layer/replica_id_in_sync_group:0"", shape=(), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:2)
global_replica_id: Tensor(""replica_3/demo/test_layer/replica_id_in_sync_group:0"", shape=(), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:3)
2022-06-30 12:57:50.089969: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
------------------------------ step  0 ------------------------------
global_replica_id: Tensor(""demo/test_layer/replica_id_in_sync_group:0"", shape=(), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)
global_replica_id: Tensor(""replica_1/demo/test_layer/replica_id_in_sync_group:0"", shape=(), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:1)
global_replica_id: Tensor(""replica_2/demo/test_layer/replica_id_in_sync_group:0"", shape=(), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:2)
global_replica_id: Tensor(""replica_3/demo/test_layer/replica_id_in_sync_group:0"", shape=(), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:3)


If using a new script and a new MirroredStrategy scope to load the saved model and run the steps, then the log is like:
```bash
2022-06-30 12:58:39.305720: I tensorflow/core/platform/cpu_feature_guard.cc:152] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-06-30 12:58:41.151528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14649 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:06:00.0, compute capability: 7.0
2022-06-30 12:58:41.153120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 14649 MB memory:  -> device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:07:00.0, compute capability: 7.0
2022-06-30 12:58:41.154428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 14649 MB memory:  -> device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:0a:00.0, compute capability: 7.0
2022-06-30 12:58:41.155692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 14649 MB memory:  -> device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:0b:00.0, compute capability: 7.0
------------------------------ step  0 ------------------------------
global_replica_id: 0
```
```
</details>"
56637,how to obtain a single pb file in tensorflow 2.x,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

2.8

### Custom Code

No

### OS Platform and Distribution

windows 10

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
i am using tf 2.8, nowadays, the following scrips:
tf.saved_model.save(model, model_save_path)
generates the pb file of a model, along with 2 folders named: assets and variables.
when i use netron to visiualize the pb file, i looks a lot of VarHandleOp.
but when i use netron to visiualize the single pb file, i shows lots of Conv2D and Dense layer which act much more normal.
in my situation, i need single pb file, how can i generate it using tf 2.8? 
thank you
```


### Standalone code to reproduce the issue

```shell
tf.saved_model.save(model, model_save_path)
```


### Relevant log output

_No response_</details>"
56636,Cyclic Python dependency due to tensorflow-io-gcs-filesystem," ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.7 - 2.9 (master)

### Custom Code

No

### OS Platform and Distribution

Any

### Python version

Any

### Current Behaviour?

https://github.com/tensorflow/tensorflow/commit/c2da23e4d8460ebeeda9cf987dc2f11282f80d6e introduced a dependency on `tensorflow-io-gcs-filesystem`.
However that in turn depends on `tensorflow`.

Hence it is impossible to install TensorFlow from source as it requires `tensorflow-io-gcs-filesystem` and that cannot be installed either as it requires TensorFlow.

I would strongly suggest to remove `tensorflow-io-gcs-filesystem` from the `REQUIRED_PACKAGES`

Note that TensorFlow is used on other platforms, e.g. PPC where prebuild wheels are not possible. Also using those platform specific wheels may clash with different compilation options/stdlibs/libraries/...

CC @yongtang"
56635,TF TESTs failures: //tensorflow/lite/kernels:squeeze_test  //tensorflow/lite/kernels:reshape_test,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

latest master branch

### Custom Code

No
</details>

### Current Behaviour?

```shell
TF TESTs failures:
//tensorflow/lite/kernels:squeeze_test
//tensorflow/lite/kernels:reshape_test


tensorflow/lite/kernels/reshape_test.cc:197:1:   required from here
23:16:54  bazel-out/k8-opt/bin/external/flatbuffers/_virtual_includes/flatbuffers/flatbuffers/flatbuffer_builder.h:652:31: error: const class std::initializer_list<int> has no member named data
23:16:54    652 |     return CreateVector(array.data(), array.size());
23:16:54        |                         ~~~~~~^~~~
```
(similar mesg with `squeeze_test`)
```


### Standalone code to reproduce the issue

```shell
bazel test //tensorflow/lite/kernels:squeeze_test
```"
56630,TF build fails with the new cublas_lt commit,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf 2.8, tf 2.9, tf 2.10

### Custom Code

No

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

5.1.1

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

11.7/8.5

### GPU model and memory

V100

### Current Behaviour?

```shell
The TF master cannot build successfully. We hit this err:

ERROR: /home/workspace/image_zoo/gelu_upstream/tensorflow-source/tensorflow/cc/BUILD:677:22: Linking tensorflow/cc/ops/image_ops_gen_cc failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc @bazel-out/k8-opt/bin/tensorflow/cc/ops/image_ops_gen_cc-2.params
/usr/bin/ld: bazel-out/k8-opt/bin/_solib_local/_U_S_Stensorflow_Scc_Cops_Simage_Uops_Ugen_Ucc___Utensorflow/libtensorflow_framework.so.2: undefined reference to `cublasGetStatusString'
collect2: error: ld returned 1 exit status
INFO: Elapsed time: 25.856s, Critical Path: 9.95s
INFO: 209 processes: 196 internal, 13 local.
FAILED: Build did NOT complete successfully
```
I originally thought it was a cublas/cuda version issue, since I see cublasGetStatusString has a cuda version 11.5 macro to protect and we are using the latest CUDA 11.7. So I removed it in cuda_blas_utils.cc (there is only one call site there) to try again but hit this `Unable to regisiter cuBLAS factor` errors.
```
ERROR: /home/workspace/image_zoo/gelu_upstream/tensorflow-source/tensorflow/BUILD:1392:19: Executing genrule //tensorflow:tf_python_api_gen_v2 failed: (Exit 1): bash failed: error executing command /bin/bash -c ... (remaining 1 argument skipped)
2022-06-28 23:43:46.546941: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU
 instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-06-28 23:43:46.827031: E tensorflow/stream_executor/cuda/cuda_blas.cc:3005] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been
 registered
...
```
```


### Standalone code to reproduce the issue

```shell
We have narrowed down that the failure is gone if we revert https://github.com/tensorflow/tensorflow/commit/ba57ae7f24743e684accef3521485a24c1235186.
```


### Relevant log output
#### Original log

```shell
ERROR: /home/workspace/image_zoo/gelu_upstream/tensorflow-source/tensorflow/python/BUILD:557:29: Linking tensorflow/python/gen_boosted_trees_ops_py_wrappers_cc failed: (Exit 1): crosstool_wr
apper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc @bazel-out/k8-opt/bin/tensorflow/python/gen_boosted
_trees_ops_py_wrappers_cc-2.params
/usr/bin/ld: bazel-out/k8-opt/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Uboosted_Utrees_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so.2: undefined reference to `cublas
GetStatusString'
collect2: error: ld returned 1 exit status
INFO: Elapsed time: 67.960s, Critical Path: 15.56s
INFO: 92 processes: 85 internal, 7 local.
FAILED: Build did NOT complete successfully
```

#### After commenting off cublasGetStatusString

```shell
ERROR: /home/workspace/image_zoo/gelu_upstream/tensorflow-source/tensorflow/BUILD:1392:19: Executing genrule //tensorflow:tf_python_api_gen_v2 failed: (Exit 1): bash failed: error executing
command /bin/bash -c ... (remaining 1 argument skipped)
2022-06-29 00:41:00.512392: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU
 instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-06-29 00:41:00.808394: E tensorflow/stream_executor/cuda/cuda_blas.cc:3005] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been
 registered
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/68037c5380866ccba6d41d51f54d109e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org
_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 22, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/root/.cache/bazel/_bazel_root/68037c5380866ccba6d41d51f54d109e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org
_tensorflow/tensorflow/python/__init__.py"", line 42, in <module>
    from tensorflow.python import data
  File ""/root/.cache/bazel/_bazel_root/68037c5380866ccba6d41d51f54d109e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org
_tensorflow/tensorflow/python/data/__init__.py"", line 21, in <module>
    from tensorflow.python.data import experimental
  File ""/root/.cache/bazel/_bazel_root/68037c5380866ccba6d41d51f54d109e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org
_tensorflow/tensorflow/python/data/experimental/__init__.py"", line 95, in <module>
    from tensorflow.python.data.experimental import service
  File ""/root/.cache/bazel/_bazel_root/68037c5380866ccba6d41d51f54d109e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org
_tensorflow/tensorflow/python/data/experimental/service/__init__.py"", line 419, in <module>
    from tensorflow.python.data.experimental.ops.data_service_ops import distribute
  File ""/root/.cache/bazel/_bazel_root/68037c5380866ccba6d41d51f54d109e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org
_tensorflow/tensorflow/python/data/experimental/ops/data_service_ops.py"", line 26, in <module>
    from tensorflow.python.data.ops import dataset_ops
  File ""/root/.cache/bazel/_bazel_root/68037c5380866ccba6d41d51f54d109e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org
_tensorflow/tensorflow/python/data/ops/dataset_ops.py"", line 31, in <module>
    from tensorflow.python.data.ops import iterator_ops
  File ""/root/.cache/bazel/_bazel_root/68037c5380866ccba6d41d51f54d109e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org
_tensorflow/tensorflow/python/data/ops/iterator_ops.py"", line 36, in <module>
    from tensorflow.python.training.saver import BaseSaverBuilder
  File ""/root/.cache/bazel/_bazel_root/68037c5380866ccba6d41d51f54d109e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org
_tensorflow/tensorflow/python/training/saver.py"", line 53, in <module>
    from tensorflow.python.training.saving import saveable_object_util
  File ""/root/.cache/bazel/_bazel_root/68037c5380866ccba6d41d51f54d109e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org
_tensorflow/tensorflow/python/training/saving/saveable_object_util.py"", line 20, in <module>
    from tensorflow.python.eager import def_function
  File ""/root/.cache/bazel/_bazel_root/68037c5380866ccba6d41d51f54d109e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org
_tensorflow/tensorflow/python/eager/def_function.py"", line 75, in <module>
    from tensorflow.python.eager import function as function_lib
  File ""/root/.cache/bazel/_bazel_root/68037c5380866ccba6d41d51f54d109e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org
_tensorflow/tensorflow/python/eager/function.py"", line 30, in <module>
    from tensorflow.core.function.polymorphism import function_cache
  File ""/root/.cache/bazel/_bazel_root/68037c5380866ccba6d41d51f54d109e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org
_tensorflow/tensorflow/core/function/polymorphism/function_cache.py"", line 33, in <module>
    class CaptureSnapshot(trace.TraceType):
  File ""/root/.cache/bazel/_bazel_root/68037c5380866ccba6d41d51f54d109e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org
_tensorflow/tensorflow/core/function/polymorphism/function_cache.py"", line 48, in CaptureSnapshot
    def __init__(self, mapping: dict[Hashable, trace.TraceType]):
TypeError: 'type' object is not subscriptable
ERROR: /home/workspace/image_zoo/gelu_upstream/tensorflow-source/tensorflow/lite/python/BUILD:68:10 Middleman _middlemen/tensorflow_Slite_Spython_Stflite_Uconvert-runfiles failed: (Exit 1):
bash failed: error executing command /bin/bash -c ... (remaining 1 argument skipped)
```
</details>"
56629,"[tflite converter] tflite is empty (zero bytes) after ""successful"" conversion (no errors)","### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04 (Google Colab)
- TensorFlow installation (pip package or built from source): Pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.8.0 

### 2. Code

Please see this Colab for a minimal reproduction: https://colab.research.google.com/gist/josephrocca/201179c22859aacee123e2505f526089/minimal-empty-tflite-file-produced-by-tfliteconverter-converting-dalle-mini-bart-saved-model.ipynb

Here's the code from that Colab:
```py
!wget https://huggingface.co/rocca/dalle-mini-js/resolve/main/debug/dalle-mini-tfsavedmodel.zip
!unzip dalle-mini-tfsavedmodel.zip
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model(""/content/content/dalle-mini-tfsavedmodel"")
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]
tflite_model = converter.convert()
with open('/content/content/dalle-mini.tflite', 'wb') as f:
  f.write(tflite_model)
```

### 3. Failure after conversion
The conversion is ""successful"" (no errors shown), but the tflite file is empty (zero bytes).
"
56628,gradient computation throws InvalidArgumentError in `tf.gather_nd` ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened during gradient computation of `tf.gather_nd` with `indices = [1,1]`. The code below is a minimized snippet. The forward pass of `tf.gather_nd` succeeds and the output `res` is correct. However, if I take the gradient,  it throws `InvalidArgumentError`.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
params = tf.random.uniform([3, 2], dtype=tf.float32)
indices = [1,1]
res = tf.gather_nd(params, indices)
res = tf.reduce_sum(res)
print(res) # success

with tf.GradientTape(persistent=True,) as g:
  g.watch(params)
  res = tf.gather_nd(params, indices)
  res = tf.reduce_sum(res)
grad = g.gradient(res, params)
print(grad) # InvalidArgumentError
```


### Relevant log output

```shell
InvalidArgumentError: Updates shape must have rank at least one. Found:[] [Op:ScatterNd]
```
</details>"
56626,Tensorflow lite for microcontroller not support the model with float32 input/output and int8 weights.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

No

### OS Platform and Distribution

ubuntu20

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Tensorflow lite for microcontroller not support the model with float32 input/output and int8 weights.
```


### Standalone code to reproduce the issue

```shell
Tensorflow lite for microcontroller not support the model with float32 input/output and int8 weights.
```


### Relevant log output

_No response_</details>"
56624,Memory leak when using the optimizer iterations in `tf.data.Dataset`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 16.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am implementing some sort of curriculum learning where I use `tf.data.Dataset.filter(curriculum_fn)` to filter out certain examples during training. This filter depends on the `training_steps` which I get from `optimizer.iterations`. 

The function works fine (see code + colab), however, there's a memory leak (RAM and GPU) when doing this like that. 

Can you please tell me why or show me a better way of how to get the train steps into my `tf.data.Dataset`?
```

See [Colab](https://colab.research.google.com/drive/1b5d1K7G3B8Mzn-q6vYbJjMd6zcnALzCF?usp=sharing)



### Standalone code to reproduce the issue

```shell
import os
from typing import Any, Callable, Dict, Text

import numpy as np
import psutil
import tensorflow as tf
from keras.callbacks import TensorBoard
from keras.optimizers import Optimizer
from tensorflow.python.data.ops.options import AutoShardPolicy

NUM_TRAIN_SAMPLES = 1000
NUM_DEV_SAMPLES = 10


def get_curriculum_fn(optimizer: Optimizer):
    """"""Method to load the curriculum function based on the optimizer iterations.
    :param optimizer: Instance of tf.keras.optimizers.Optimizer.
    :return:
    """"""

    def curriculum_fn(example):
        """"""Creates a function returning True or False depending on the step.

        This function can directly be applied as a predicate in a
        `tf.data.Dataset.filter` method.

        :param example: Dictionary of feature tensors.
        :return: True or False
        """"""
        # Get constants based on train_step
        step = tf.cast(optimizer.iterations, tf.int64)
        max_score = 0.4
        min_score = 0.0
        step_update = 500
        half_life = 1000

        # curriculum_step
        curriculum_step = tf.floor(step / tf.cast(step_update, tf.int64))

        # Get the min score
        delta = max_score - min_score
        weight = 0.5 ** (
                tf.cast(curriculum_step, tf.float32) / tf.cast(half_life, tf.float32)
        )
        min_score = tf.cast(max_score - delta * weight, tf.float32)
        return example[""score""][0] >= min_score

    return curriculum_fn


def get_dataset(split: str, batch_size: int, max_length: int = 64, curriculum_fn: Callable = None):
    # Number generator
    def generator():
        for i in range(num_samples):
            _dims = np.random.randint(low=1, high=max_length, size=1)
            x = np.zeros(_dims, dtype=np.int32) + np.random.randint(low=1, high=5000, size=1)
            y = np.zeros(_dims, dtype=np.int32) + np.random.randint(low=1, high=5000, size=1)
            score = np.random.uniform(low=0, high=1, size=1)
            yield {'sources': x, 'targets': y, 'score': score}

    assert split in (""train"", ""dev"")
    is_training = split == ""train""

    num_samples = NUM_TRAIN_SAMPLES if is_training else NUM_DEV_SAMPLES
    options = tf.data.Options()
    options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA
    dataset = tf.data.Dataset.from_generator(
        lambda: generator(),
        output_signature={
            'sources': tf.TensorSpec(shape=(None,), dtype=tf.int32),
            'targets': tf.TensorSpec(shape=(None,), dtype=tf.int32),
            'score': tf.TensorSpec(shape=(1,), dtype=tf.float32)
        }
    ).with_options(options=options)

    if is_training:
        dataset = dataset.shuffle(buffer_size=256).repeat()

    # Curriculum Learning
    if curriculum_fn is not None and is_training:
        dataset = dataset.filter(curriculum_fn)

    dataset = dataset.padded_batch(batch_size=batch_size)

    def map_to_example(example):
        sources, targets = example['sources'], example['targets']
        return sources, targets

    dataset = dataset.map(map_to_example, num_parallel_calls=tf.data.AUTOTUNE)
    return dataset.prefetch(tf.data.AUTOTUNE)


def memory_usage_process(pid=None) -> float:
    """"""Returns the memory used by a specific process
    :param pid: Process id. If omitted, the default is determined by os.getpid().
    :return: the memory used by the current process in MB.
    """"""
    # return the memory usage in MB
    import psutil

    process = psutil.Process(pid=pid)
    mem = process.memory_full_info().uss / float(2 ** 20)
    return mem


def get_memory_info(logs: Dict[Text, Any] = None) -> Dict[Text, Any]:
    """"""Returns the memory usage of RAM and GPU memory in MB.
    :param logs: (Optional). If provided, all entries will be appended to the
       existing log. Otherwise, a new log will be created.
    :return: A dictionary containing RAM and GPU memory usage.
    """"""
    if logs is None:
        logs = {}

    # Get RAM usage
    logs[""virtual_memory/used_system [MB]""] = psutil.virtual_memory().used / float(2 ** 20)
    logs[""virtual_memory/used_process""] = memory_usage_process(pid=os.getpid())

    # Get GPU memory usage
    physical_devices = tf.config.list_physical_devices(""GPU"")
    if len(physical_devices) == 0:
        return logs

    for n, device in enumerate(physical_devices):
        device_name = ""{}:{}"".format(device.device_type, n)
        logs[""gpu_memory/{device_name} [MB]"".format(device_name=device_name)] = tf.config.experimental.get_memory_info(
            device_name
        )[""current""]
    return logs


class MemoryProfilingCallback(tf.keras.callbacks.Callback):
    def __init__(self, tb: TensorBoard, update_freq: int = 100):
        super().__init__()
        self.tb = tb
        self.update_freq = update_freq
        self.batch_time = 0.0

    def on_train_batch_end(self, batch, logs=None):

        # To account for gradient accumulation we ask the optimizer how many gradient
        # updates happened instead of taking the batch-index
        step = int(self.model.optimizer.iterations.value()) - 1

        if step % self.update_freq == 0:
            metrics = {}
            metrics = get_memory_info(logs=metrics)

            # Write logs
            self._write(metrics, step)

    def _write(self, metrics: dict, step):
        # noinspection PyProtectedMember
        with self.tb._train_writer.as_default():
            for name, value in metrics.items():
                tf.summary.scalar(name, value, step=step)


def get_callbacks(log_dir: str):
    _callbacks = []

    # Tensorboard
    tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=os.path.join(log_dir, ""logs""))
    _callbacks.append(tensorboard_cb)

    # Memory profiler
    _callbacks.append(MemoryProfilingCallback(tensorboard_cb))

    return _callbacks


def main():
    LOG_DIR = ""/tmp/memory_leak_curriculum/""

    max_length = 64

    train_batch_size = 64
    valid_batch_size = 16

    optimizer = tf.keras.optimizers.Adam()

    # Model
    inputs = tf.keras.layers.Input(shape=(None,), dtype=tf.int32)
    x = inputs
    x = tf.keras.layers.Embedding(input_dim=5000,
                                  output_dim=64)(x)
    x = tf.keras.layers.Dense(5000)(x)
    model = tf.keras.Model(inputs=inputs, outputs=x)
    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer)
    model.summary()

    train_data = get_dataset(
        split=""train"",
        batch_size=train_batch_size,
        max_length=max_length,
        curriculum_fn=get_curriculum_fn(optimizer=optimizer)
    )

    valid_data = get_dataset(
        split=""dev"",
        batch_size=valid_batch_size,
        max_length=max_length,
    )

    model.fit(
        train_data,
        epochs=100,
        steps_per_epoch=5000,
        validation_data=valid_data,
        validation_steps=3,
        callbacks=get_callbacks(log_dir=LOG_DIR)
    )


if __name__ == '__main__':
    main()
```


### Relevant log output
<img width=""426"" alt=""image"" src=""https://user-images.githubusercontent.com/60609608/176453696-094ce6f5-ad4a-48df-a6fd-3870f35b564b.png"">"
56623,TF test failure: //tensorflow/lite/kernels/shim:shape_test,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

latest master branch

### Custom Code

No

</details>
### Current Behaviour?

```shell
TEST //tensorflow/lite/kernels/shim:shape_test failing on TF upstream/master:

tensorflow/lite/kernels/shim/shape_test.cc:74: Failure
Expected equality of these values:
  ""[]""
  Shape({}).ToString()
    Which is: ""?""

tensorflow/lite/kernels/shim/shape_test.cc:81: Failure
Value of: Shape({}).FullyDefined()
  Actual: false
Expected: true
```


### Standalone code to reproduce the issue

```shell
bazel test //tensorflow/lite/kernels/shim:shape_test --test_output=all
```"
56622,"Get Input/Output shapes of node (ex: Conv, Add) during the inferencce","Hi,

I would like to know how can we access input and output shapes of a node (ex: Convolution, Add) from the reampper file. 

I am trying to access input shapes in function [IsAddWithNoBroadcast] (https://github.com/tensorflow/tensorflow/blob/d8ce9f9c301d021a69953134185ab728c1c248d3/tensorflow/core/grappler/optimizers/remapper.cc#L843) but I am not successful in retrieving the input shapes of node.

Can someone help me to get the input shapes of node with in a remapper file?

Thanks!!
"
56619,Gradient computation through tf.linalg.eigh does not work with complex-valued input in XLA,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Computing the gradient through tf.linalg.eigh() fails in XLA when the input of the function is complex-valued (tf.complex64).
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

h_real = tf.Variable([[1., 1.], [1., 1.]], dtype=tf.float32, trainable=True)
h_imag = tf.Variable([[0.0, 1.], [-1., 0.0]], dtype=tf.float32, trainable=True)

@tf.function(jit_compile=True)
def test():
    with tf.GradientTape() as tape:
        # Build Hernitian matrix
        h = tf.complex(h_real, h_imag)
        # Compute singular values
        s,_ = tf.linalg.eigh(h)
        loss = tf.math.real(s[0])
    grad = tape.gradient(loss,[h_real,h_imag])
    return grad

g = test()
```


### Relevant log output

```shell
InvalidArgumentError: Binary op add with different element types: f32[2,2] and c64[].
	 [[{{node gradient_tape/add}}]] [Op:__inference_test_245]
```
</details>"
56618,A small typo in `graph_properties.cc`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Source

source

### Tensorflow Version

tf2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/costs/graph_properties.cc#L2145

conflics => conflicts
```


### Standalone code to reproduce the issue

```shell
I think any simple code will output this typo...
```


### Relevant log output

_No response_</details>"
56616,"ImportError: dlopen(/Users/nurujjamanpollob/miniforge3/lib/python3.9/site-packages/tensorflow/python/_pywrap_dtensor_device.so, 0x0002): Symbol not found: __ZN10tensorflow31MaybeRaiseExceptionFromTFStatusEP9TF_StatusP7_object","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Source

binary

### Tensorflow Version

2.7.0

### Custom Code

Yes

### OS Platform and Distribution

macOS 12.4 on M1 Pro ARM64

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
ImportError: dlopen(/Users/nurujjamanpollob/miniforge3/lib/python3.9/site-packages/tensorflow/python/_pywrap_dtensor_device.so, 0x0002): Symbol not found: __ZN10tensorflow31MaybeRaiseExceptionFromTFStatusEP9TF_StatusP7_object
  Referenced from: /Users/nurujjamanpollob/miniforge3/lib/python3.9/site-packages/tensorflow/python/_pywrap_dtensor_device.so
  Expected in: /Users/nurujjamanpollob/miniforge3/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
```


### Standalone code to reproduce the issue

```shell
import tensorflow as ts
import numpy as np

from tensorflow.keras import Sequential as Sq
from tensorflow.keras.layers import Dense as Den
from tensorflow.keras.layers import MaxPooling2D as Max
from tensorflow.keras.layers import Conv2D as conv

# Entry point of python programme
from callbacks.TrainingEpochCallback import EpochCallBack


def program_entry():
    # Get inbuilt dataset
    data = ts.keras.datasets.fashion_mnist

    # Tuples of training data sets by calling load_data() method
    (training_images, training_labels), (test_images, test_labels) = data.load_data()

    # Create a new array, of 'training_images' pixel representation, each pixel range is between 0 - 255, so divide
    # Each of them array element with 255, should make it under zero and one, this is called normalizing the image
    # it will help ML model enhance performance
    training_images = training_images / 255.0

    # Create a new array, of 'training_images' pixel representation, each pixel range is between 0 - 255, so divide
    # Each of them array element with 255, should make it under zero and one, this is called normalizing the image
    # it will help ML model enhance performance
    test_images = test_images / 255.0

    # Create a Sequential model instance with a simple Neural Network Layer
    # That contains 128 layer and 128 neuron
    l0 = [
        conv(64, (3, 3), activation='relu', input_shape=(28, 28, 1)),
        Max(pool_size=(2, 2)),
        conv(64, (3, 3), activation='relu'),
        Max(pool_size=(2, 2)),
        ts.keras.layers.Flatten(),
        Den(128, activation='relu'),
        Den(10, activation='softmax')
    ]

    model = Sq(l0)

    # Configures the model for training
    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    # Trains the model for a fixed number of epochs (iterations on a dataset)
    model.fit(training_images, training_labels, epochs=50)

    # Evaluate the model with test data set
    model.evaluate(test_images, test_labels)

    # Make prediction on test data set
    classifications = model.predict(test_images)
    print(classifications[0])
    print(test_labels[0])


# Press the green button in the gutter to run the script.
if __name__ == '__main__':
    # Entry point to program
    program_entry()
```


### Relevant log output

```shell
Users/nurujjamanpollob/miniforge3/bin/python /Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevconsole.py --mode=client --port=51223
import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/Volumes/Projects/learning/Four-K-Pixel-AI'])
PyDev console: starting.
Python 3.9.13 | packaged by conda-forge | (main, May 27 2022, 17:00:33) 
[Clang 13.0.1 ] on darwin
runfile('/Volumes/Projects/learning/Four-K-Pixel-AI/main.py', wdir='/Volumes/Projects/learning/Four-K-Pixel-AI')
Traceback (most recent call last):
  File ""/Users/nurujjamanpollob/miniforge3/lib/python3.9/code.py"", line 90, in runcode
    exec(code, self.locals)
  File ""<input>"", line 1, in <module>
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydev_bundle/pydev_umd.py"", line 198, in runfile
    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/Volumes/Projects/learning/Four-K-Pixel-AI/main.py"", line 4, in <module>
    from tensorflow.keras import Sequential as Sq
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydev_bundle/pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""/Users/nurujjamanpollob/miniforge3/lib/python3.9/site-packages/keras/api/_v2/keras/__init__.py"", line 12, in <module>
    from keras import __version__
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydev_bundle/pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""/Users/nurujjamanpollob/miniforge3/lib/python3.9/site-packages/keras/__init__.py"", line 24, in <module>
    from keras import models
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydev_bundle/pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""/Users/nurujjamanpollob/miniforge3/lib/python3.9/site-packages/keras/models/__init__.py"", line 18, in <module>
    from keras.engine.functional import Functional
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydev_bundle/pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""/Users/nurujjamanpollob/miniforge3/lib/python3.9/site-packages/keras/engine/functional.py"", line 24, in <module>
    from keras.dtensor import layout_map as layout_map_lib
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydev_bundle/pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""/Users/nurujjamanpollob/miniforge3/lib/python3.9/site-packages/keras/dtensor/__init__.py"", line 22, in <module>
    from tensorflow.compat.v2.experimental import dtensor as dtensor_api  # pylint: disable=g-import-not-at-top
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydev_bundle/pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""/Users/nurujjamanpollob/miniforge3/lib/python3.9/site-packages/tensorflow/_api/v2/compat/v2/experimental/dtensor/__init__.py"", line 8, in <module>
    from tensorflow.dtensor.python.api import call_with_layout
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydev_bundle/pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""/Users/nurujjamanpollob/miniforge3/lib/python3.9/site-packages/tensorflow/dtensor/python/__init__.py"", line 18, in <module>
    from tensorflow.dtensor.python import mesh_util
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydev_bundle/pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""/Users/nurujjamanpollob/miniforge3/lib/python3.9/site-packages/tensorflow/dtensor/python/mesh_util.py"", line 21, in <module>
    from tensorflow.dtensor.python import api
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydev_bundle/pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""/Users/nurujjamanpollob/miniforge3/lib/python3.9/site-packages/tensorflow/dtensor/python/api.py"", line 22, in <module>
    from tensorflow.dtensor.python import dtensor_device
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydev_bundle/pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""/Users/nurujjamanpollob/miniforge3/lib/python3.9/site-packages/tensorflow/dtensor/python/dtensor_device.py"", line 28, in <module>
    from tensorflow.python import _pywrap_dtensor_device
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydev_bundle/pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
ImportError: dlopen(/Users/nurujjamanpollob/miniforge3/lib/python3.9/site-packages/tensorflow/python/_pywrap_dtensor_device.so, 0x0002): Symbol not found: __ZN10tensorflow31MaybeRaiseExceptionFromTFStatusEP9TF_StatusP7_object
  Referenced from: /Users/nurujjamanpollob/miniforge3/lib/python3.9/site-packages/tensorflow/python/_pywrap_dtensor_device.so
  Expected in: /Users/nurujjamanpollob/miniforge3/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
```
</details>"
56615,Allow int16 input/output even when not using 16x8 quantization mode,"Currently the valid types can be either `int16` or `int8`/`uint8`, but not a combination of both. Some models could for example contain a custom op returning an `int16` tensor as model output, and converting such model to TFLite is failing. It looks like always adding `_dtypes.int16` to the list of supported types when [quant_mode.is_integer_quantization()](https://github.com/tensorflow/tensorflow/blob/fd663c96bd88d37174a99a4dbaeb37b9f945505c/tensorflow/lite/python/lite.py#L946) is true would be enough to make it work.
"
56610,Could not find device for node: {{node Qr}} = Qr[T=DT_HALF],"* OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 20.04.2
* TensorFlow installed from (source or binary):Binary
* TensorFlow version (use command below):Tensorflow 2.7
* Python version:3.7



Im running into errors when using `tf.linalg.qr()` with inputs of half precision. The [documentation](https://www.tensorflow.org/api_docs/python/tf/linalg/qr) lists float16 as a supported type. Everything works fine for iterations of single precision.


Testing with this snippet returns a similar error to what happens when I run the source code below.
```
import tensorflow as tf

tn = tf.constant([ [1, 2], [3, 4] ], dtype=tf.half)
[Q, R] = tf.linalg.qr(tn);
```


[Source](https://github.com/cyanguwa/DeepLearningProfiling/blob/master/Basic-Kernels/python/rnn1d_tf2.py)
Output: 
```
  File ""./rnn1d_tf2.py"", line 184, in <module>
    enable_xla=parsed.enable_xla)
  File ""./rnn1d_tf2.py"", line 123, in main
    compfunc(input_image, basic_cell)
  File ""./rnn1d_tf2.py"", line 53, in run_forward
    output_result, states_cur, _ = rnn1d(input_image, basic_cell) 
  File ""./rnn1d_tf2.py"", line 41, in rnn1d
    whole_seq_output, final_memory_state, final_carry_state = tf.keras.layers.RNN(basic_cell, return_sequences=True, return_state=True)(input_data)
  File ""/home/wano/virtual_environments/tf2/lib/python3.7/site-packages/keras/layers/recurrent.py"", line 679, in __call__
    return super(RNN, self).__call__(inputs, **kwargs)
  File ""/home/wano/virtual_environments/tf2/lib/python3.7/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/wano/virtual_environments/tf2/lib/python3.7/site-packages/keras/initializers/initializers_v2.py"", line 620, in __call__
    q, r = tf.linalg.qr(a, full_matrices=False)
tensorflow.python.framework.errors_impl.NotFoundError: Could not find device for node: {{node Qr}} = Qr[T=DT_HALF, full_matrices=false]
All kernels registered for op Qr:
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_COMPLEX128]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_COMPLEX128]
 [Op:Qr]
```"
56608,trap invalid opcode in libtensorflow_io_plugins.so,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

v1.12.1-77459-g4ff6b08be52 2.10.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

5.11

### GCC/Compiler version

8.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Running Tensorflow with the object detection model is causing it to crash.
```


### Standalone code to reproduce the issue

```shell
Following https://github.com/nicknochnack/TFODCourse/blob/main/2.%20Training%20and%20Detection.ipynb section 6. Train the Model

bazel build was simply:
bazel build --jobs=3  //tensorflow/tools/pip_package:build_pip_package --execution_log_json_file=/tmp/exec1.json
```


### Relevant log output

```shell
(tfod) culex@phoebe:~/TFODCourse$ python Tensorflow/models/research/object_detection/model_main_tf2.py --model_dir=Tensorflow/workspace/models/my_ssd_mobnet --pipeline_config_path=Tensorflow/workspace/models/my_ssd_mobnet/pipeline.config --num_train_steps=2000
2022-06-28 06:43:42.949335: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.saving.functional_saver has been moved to tensorflow.python.checkpoint.functional_saver. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.saving.checkpoint_options has been moved to tensorflow.python.checkpoint.checkpoint_options. The old module will be deleted in version 2.11.
Illegal instruction (core dumped)


(tfod) culex@phoebe:~/TFODCourse$ gdb --args python Tensorflow/models/research/object_detection/model_main_tf2.py --model_dir=Tensorflow/workspace/models/my_ssd_mobnet --pipeline_config_path=Tensorflow/workspace/models/my_ssd_mobnet/pipeline.config --num_train_steps=2000
GNU gdb (Ubuntu 9.2-0ubuntu1~20.04.1) 9.2
Copyright (C) 2020 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.
Type ""show copying"" and ""show warranty"" for details.
This GDB was configured as ""x86_64-linux-gnu"".
Type ""show configuration"" for configuration details.
For bug reporting instructions, please see:
<http://www.gnu.org/software/gdb/bugs/>.
Find the GDB manual and other documentation resources online at:
    <http://www.gnu.org/software/gdb/documentation/>.

For help, type ""help"".
Type ""apropos word"" to search for commands related to ""word""...
Reading symbols from python...
Reading symbols from /usr/lib/debug/.build-id/1f/3df9df2b5e575fdee41890fe17f6de614f93f6.debug...
(gdb) run
Starting program: /home/culex/tfod/bin/python Tensorflow/models/research/object_detection/model_main_tf2.py --model_dir=Tensorflow/workspace/models/my_ssd_mobnet --pipeline_config_path=Tensorflow/workspace/models/my_ssd_mobnet/pipeline.config --num_train_steps=2000
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/lib/x86_64-linux-gnu/libthread_db.so.1"".
2022-06-28 06:37:11.608292: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[New Thread 0x7fffdd81d700 (LWP 420425)]
[New Thread 0x7fffdd01c700 (LWP 420426)]
[New Thread 0x7fffd881b700 (LWP 420427)]
[Thread 0x7fffd881b700 (LWP 420427) exited]
[Thread 0x7fffdd01c700 (LWP 420426) exited]
[Thread 0x7fffdd81d700 (LWP 420425) exited]
[Detaching after fork from child process 420428]
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.saving.functional_saver has been moved to tensorflow.python.checkpoint.functional_saver. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.saving.checkpoint_options has been moved to tensorflow.python.checkpoint.checkpoint_options. The old module will be deleted in version 2.11.
[New Thread 0x7fffd881b700 (LWP 420429)]
[New Thread 0x7fffdd01c700 (LWP 420430)]
[New Thread 0x7fffdd81d700 (LWP 420431)]
[New Thread 0x7fffbadab700 (LWP 420432)]

Thread 1 ""python"" received signal SIGILL, Illegal instruction.
0x00007fffa8c7d0dc in __static_initialization_and_destruction_0(int, int) [clone .constprop.19] ()
   from /home/culex/tfod/lib/python3.8/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so
(gdb) backtrace
#0  0x00007fffa8c7d0dc in __static_initialization_and_destruction_0(int, int) [clone .constprop.19] ()
   from /home/culex/tfod/lib/python3.8/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so
#1  0x00007ffff7fe0b9a in call_init (l=<optimized out>, argc=argc@entry=5, argv=argv@entry=0x7fffffffe378, env=env@entry=0x47d9d70) at dl-init.c:72
#2  0x00007ffff7fe0ca1 in call_init (env=0x47d9d70, argv=0x7fffffffe378, argc=5, l=<optimized out>) at dl-init.c:30
#3  _dl_init (main_map=0x5061f80, argc=5, argv=0x7fffffffe378, env=0x47d9d70) at dl-init.c:119
#4  0x00007ffff7f22985 in __GI__dl_catch_exception (exception=<optimized out>, operate=<optimized out>, args=<optimized out>) at dl-error-skeleton.c:182
#5  0x00007ffff7fe50cf in dl_open_worker (a=a@entry=0x7fffffff4520) at dl-open.c:758
#6  0x00007ffff7f22928 in __GI__dl_catch_exception (exception=<optimized out>, operate=<optimized out>, args=<optimized out>) at dl-error-skeleton.c:208
#7  0x00007ffff7fe460a in _dl_open (file=0x4c0f080 ""/home/culex/tfod/lib/python3.8/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so"", mode=-2147483646,
    caller_dlopen=<optimized out>, nsid=-2, argc=5, argv=0x7fffffffe378, env=0x47d9d70) at dl-open.c:837
#8  0x00007ffff7d9a34c in dlopen_doit (a=a@entry=0x7fffffff4740) at dlopen.c:66
#9  0x00007ffff7f22928 in __GI__dl_catch_exception (exception=exception@entry=0x7fffffff46e0, operate=<optimized out>, args=<optimized out>) at dl-error-skeleton.c:208
#10 0x00007ffff7f229f3 in __GI__dl_catch_error (objname=0x9baa40, errstring=0x9baa48, mallocedp=0x9baa38, operate=<optimized out>, args=<optimized out>)
    at dl-error-skeleton.c:227
#11 0x00007ffff7d9ab59 in _dlerror_run (operate=operate@entry=0x7ffff7d9a2f0 <dlopen_doit>, args=args@entry=0x7fffffff4740) at dlerror.c:170
#12 0x00007ffff7d9a3da in __dlopen (file=<optimized out>, mode=<optimized out>) at dlopen.c:87
#13 0x00007ffff2d5da6e in tensorflow::internal::LoadDynamicLibrary(char const*, void**) ()
   from /home/culex/tfod/lib/python3.8/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#14 0x00007fffe177e1cb in tensorflow::(anonymous namespace)::PosixEnv::LoadDynamicLibrary(char const*, void**) ()
   from /home/culex/tfod/lib/python3.8/site-packages/tensorflow/python/../libtensorflow_framework.so.2
#15 0x00007fffe70e4180 in tensorflow::RegisterFilesystemPlugin(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) ()
   from /home/culex/tfod/lib/python3.8/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so

(tfod) culex@phoebe:~$ python3 -c ""import tensorflow as tf; print(tf.reduce_sum(                                                                                                                              tf.random.normal([1000, 1000])))""
2022-06-28 06:24:58.547852: I tensorflow/core/platform/cpu_feature_guard.cc:193]                                                                                                                               This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (on                                                                                                                              eDNN) to use the following CPU instructions in performance-critical operations:                                                                                                                                SSE3 SSE4.1
To enable them in other operations, rebuild TensorFlow with the appropriate comp                                                                                                                              iler flags.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.sa                                                                                                                              ving.functional_saver has been moved to tensorflow.python.checkpoint.functional_                                                                                                                              saver. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.sa                                                                                                                              ving.checkpoint_options has been moved to tensorflow.python.checkpoint.checkpoin                                                                                                                              t_options. The old module will be deleted in version 2.11.
2022-06-28 06:25:02.918331: I tensorflow/core/platform/cpu_feature_guard.cc:193]                                                                                                                               This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (on                                                                                                                              eDNN) to use the following CPU instructions in performance-critical operations:                                                                                                                                SSE3 SSE4.1
To enable them in other operations, rebuild TensorFlow with the appropriate comp                                                                                                                              iler flags.
tf.Tensor(550.23517, shape=(), dtype=float32)
```
</details>"
56607,Request a demo about tensorflow C++ 2.x for inference ,"I found the offical released tensorflow C++ library with version 2.6 on the website: https://tensorflow.google.cn/install/lang_c, However, there is no a complete demo to show how to load and preprocess data (especially for sequential data, not picture), load model and for inference in the C++ environment. Can you help me find several examples?  

Thank you sincerely."
56603,MKL use cannot be disabled ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

TF 2.8.0

### Custom Code

No

### OS Platform and Distribution

Linux CentOS 8

### Mobile device

_No response_

### Python version

3.8.8

### Bazel version

4.2.1

### GCC/Compiler version

8.2.1

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I'm trying to disable the use of JIT kernels entirely. 
Exporting the environment variable TF_ENABLE_ONEDNN_OPTS=0 does not seem to have any effect, TF still calls: jit_avx512_common_gemm_f32.cpp from _pywrap_tensorflow_internal.so.

P.S.: TensorFlow is built without --config=MKL
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

def test():
    x = [1]*100
    W = tf.Variable(tf.ones(shape=(100,100)), name=""W"")
    b = tf.Variable(tf.zeros(shape=(100)), name=""b"")
    return W * x + b

if __name__ == ""__main__"":
    test()
```


### Relevant log output

_No response_</details>"
56600,Poor performance of tensorflow higher versions,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

binary

### Tensorflow Version

2.4.1, 2.3.1, 2.2.0, 2.1.0, 2.0.0, 1.15.2 , 1.14.0, 1.13.1

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 16.04

### Mobile device

_No response_

### Python version

3.7.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?
The detailed experiment results is as follows:
Version | Memory 
-- | -- 
2.7.0 | 142
2.4.1 | 118
2.3.1 | 114
2.2.0 | 108
2.1.0 | 110
1.15.2 | 106
1.14.0 | 104
1.13.1 | 92

According to the above experiment results, when TensorFlow version is higher than 1.13.1, the performance of the model is worse.


### Standalone code to reproduce the issue
[train.csv](https://github.com/tensorflow/tensorflow/files/8997439/train.csv)
[test.csv](https://github.com/tensorflow/tensorflow/files/8997441/test.csv)
```shell
import numpy as np 
import pandas as pd 
import tracemalloc
tracemalloc.start()
train = pd.read_csv(""train.csv"")
test = pd.read_csv(""test.csv"")

train_labels = train.iloc[:,-3:]
features = pd.concat([train.iloc[:,1:-3], test.iloc[:,1:]])

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaled_features =scaler.fit_transform(features)
scaled_train = scaled_features[:train.shape[0]]
scaled_test = scaled_features[train.shape[0]:]
print(scaled_train.shape, scaled_test.shape)
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(scaled_train, train_labels, random_state = 42, test_size = 0.3)
from tensorflow import keras
from tensorflow.keras import layers
model = keras.Sequential([
    layers.Dense(16, activation='relu', input_shape=[8]),
    layers.Dense(64, activation='relu'),
    layers.Dense(64, activation = 'relu'),
    layers.Dense(32, activation = 'relu'),
    layers.Dense(3),
])
model.compile( optimizer = ""adam"", loss = ""mae"")
history = model.fit( x_train, y_train, validation_data=(x_test, y_test), batch_size = 256, epochs = 50, verbose = False)
hstry_df = pd.DataFrame(history.history)

submissions = model.predict(scaled_test)

submission_df = pd.DataFrame(submissions, columns = train_labels.columns)
submission_df['date_time'] = test['date_time']
submission_df.to_csv(""submissions.csv"", index=False)


current3, peak3 = tracemalloc.get_traced_memory()
print(""Get_dummies memory usage is {"",current3 /1024/1024,""}MB; Peak memory was :{"",peak3 / 1024/1024,""}MB"")
```


### Relevant log output

_No response_</details>"
56599,Basic Keras model underperforming against Scikit-Learn MLPRegressor model,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

binary

### Tensorflow Version

2.9.0

### Custom Code

Yes

### OS Platform and Distribution

macOS Monterey

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I've experimented with sklearn's MLPRegressor class and have seen that it does fairly well for the dataset I'm looking at without much tuning. However, I'd like to be able to build out a more complex model in Tensorflow/Keras using a split LSTM and Dense network.

To fulfill that end, I'm trying to first replicate the performance of MLPRegressor in Tensorflow for a very basic architecture, but struggling so far. Here's an attempt to create identical models with each. The parameters in the TF implementation are intended to be based on the MLPRegressor documentation, including certain default values.

Running the code below, there are two noticeable observations:
- The loss during training is roughly half for MLPRegressor versus the TF model. This is also what I've observed on the real dataset.
- The final MSE of the predictions on the training set is always lower for MLPRegressor (note: I'm not sure if the random seeds have the same effect on both models, but if you run it a few times, you should see this).

Just a note - I've posted this on StackOverflow, Reddit, and sklearn's GitHub without any substantive responses, even though the code should be fully reproducible.  This is kind of a last resort, but I'm hoping that someone here might be interested in why TF could potentially underperform against sklearn.


### Standalone code to reproduce the issue

```shell
import numpy as np
import matplotlib.pyplot as plt

from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.wrappers.scikit_learn import KerasRegressor
from tensorflow.keras.regularizers import L2
from tensorflow.random import set_seed

from sklearn.neural_network import MLPRegressor
from sklearn.pipeline import Pipeline
from sklearn.compose import TransformedTargetRegressor
from sklearn.preprocessing import StandardScaler

from sklearn.datasets import make_regression
from sklearn.metrics import mean_squared_error


all_sk = []
all_tf = []

for _ in range(100):

    X, y = make_regression(n_samples=10000, n_features=20, n_informative=10, n_targets=1)
    # y = StandardScaler().fit_transform(y[:,None])[:,0]
    
    use_scaling = True
    seed = np.random.randint(0,1000)
    
    
    def simple_tf_model():
        set_seed(seed)
        dense_input = Input(shape=(X.shape[1],))
        dense = Dense(100, activation=""relu"", kernel_regularizer=L2(l2=0.0001))(dense_input)
        dense = Dense(1, activation=""linear"", kernel_regularizer=L2(l2=0.0001))(dense)     
        tf_model = Model(inputs=[dense_input], outputs=dense)     
        tf_model.compile(loss=""mse"",optimizer=""adam"")
        return tf_model
    
    
    sk_model = MLPRegressor(max_iter=5, hidden_layer_sizes=(100,), batch_size=200, random_state=seed, verbose=1)
    tf_model = KerasRegressor(build_fn=simple_tf_model, batch_size=200, epochs=5, validation_split=0.1)
    
    if use_scaling:
        sk_pipeline = Pipeline([('scaler', StandardScaler()), ('model', sk_model)])
        sk_model = TransformedTargetRegressor(regressor=sk_pipeline, transformer=StandardScaler()) 
        
        tf_pipeline = Pipeline([('scaler', StandardScaler()), ('model', tf_model)])
        tf_model = TransformedTargetRegressor(regressor=tf_pipeline, transformer=StandardScaler()) 
        

    sk_model.fit(X,y)
    tf_model.fit(X,y)
    
    sk_preds = sk_model.predict(X)
    tf_preds = tf_model.predict(X)
    
    
    def get_mse(preds, name):
        print(name, mean_squared_error(preds, y))
        
        if name == ""SK"":
            all_sk.append(mean_squared_error(preds, y))
        else:
            all_tf.append(mean_squared_error(preds, y))
    
    get_mse(sk_preds, ""SK"")
    get_mse(tf_preds, ""TF"")
    

sk_arr = np.array(all_sk)
tf_arr = np.array(all_tf)

print(sk_arr.mean()). # 350.0151514048654
print(tf_arr.mean()) # 382.19699899150226

```


### Relevant log output

_No response_</details>"
56595,InvalidArgumentError: Removed XLA support for adding ragged Tensors in TF 2.9,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Adding two ragged Tensors in XLA mode does not work anymore in TF 2.9 (the same code works until TF 2.8). 

Please note that it seems this is not just related to the addition, the same happens for the element-wise multiplication and a similar behaviour can be even observed for tf.broadcast().

I have the impression this issue only occurs if the ragged dimension is located between two non-ragged dimensions.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
print('Tensorflow version: ', tf.__version__)

outer_dim = 7 # arbitrary
inner_dim = 2 # arbitrary
row_splits = [0, 1, 3, outer_dim] # some random splits
rt_shape = [outer_dim, inner_dim]

def run_graph():   

    x = tf.RaggedTensor.from_row_splits(
                values=tf.ones(rt_shape, dtype=tf.float32),
                row_splits=row_splits)
    
    y = tf.RaggedTensor.from_row_splits(
                values=tf.ones(rt_shape, dtype=tf.float32),
                row_splits=row_splits)

    print(""Shape of x:"", x.shape)
    print(""Shape of y:"", y.shape)

    z = tf.add(x, y) # this add causes the problem (with XLA)
    print(""Shape of z:"", z.shape)
    return z

print(""---Running in Graph mode---"")
tf.function(run_graph)()

print(""\n---Running in XLA mode---"")
tf.function(run_graph, jit_compile=True)()
```


### Relevant log output

```shell
Tensorflow version:  2.9.1
---Running in Graph mode---
Shape of x: (3, None, 2)
Shape of y: (3, None, 2)
Shape of z: (3, None, 2)

---Running in XLA mode---
Shape of x: (3, None, 2)
Shape of y: (3, None, 2)
Shape of z: (3, None, 2)
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/home/projects/XLA_RT_BUG.ipynb Cell 3' in <module>
     24 tf.function(run_graph)()
     26 print(""\n---Running in XLA mode---"")
---> 27 tf.function(run_graph, jit_compile=True)()

[...]

InvalidArgumentError: Detected unsupported operations when trying to compile graph __inference_run_graph_2782[_XlaMustCompile=true,config_proto=9241198235816212909,executor_type=11160318154034397263] on XLA_GPU_JIT: RaggedRange (No registered 'RaggedRange' OpKernel for XLA_GPU_JIT devices compatible with node {{node RaggedRange_2}}){{node RaggedRange_2}}
The op is created at: 

[...]

File ""<ipython-input-4-00ab54ea3ce6>"", line 27, in <module>
  tf.function(run_graph, jit_compile=True)()
File ""<ipython-input-4-00ab54ea3ce6>"", line 19, in run_graph
  z = tf.add(x, y) # this add causes the problem (with XLA) [Op:__inference_run_graph_2782]
```
</details>"
56593,tensorflow:AutoGraph could not transform lambda and will run it as-is.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.4.1

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I zipped 2 tf.data.Dataset and filtered the new dataset based on the first value in the resulting tuples.
This worked, but gave me a warning and asked me to report this to the tensoflow team.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import pandas as pd
import numpy as np


tf.autograph.set_verbosity(10,alsologtostdout=True)
in_size_test = 5
df_test =pd.DataFrame(range(1,18))
df_test_y = pd.DataFrame([range(2,19),range(3,20)]).transpose()

ds_debug1= tf.keras.preprocessing.timeseries_dataset_from_array(
    data=df_test, 
    targets = np.roll(df_test_y, shift=-in_size_test+1,axis=0),
    sequence_length=in_size_test, 
    sequence_stride=1, 
    sampling_rate=1,
    shuffle=False)

ds_debug2= tf.keras.preprocessing.timeseries_dataset_from_array(
    data=df_test, 
    targets = np.roll(df_test_y, shift=-in_size_test+1,axis=0),
    sequence_length=in_size_test, 
    sequence_stride=1, 
    sampling_rate=1,
    shuffle=False)
ds_debug = tf.data.Dataset.zip((ds_debug1, ds_debug2)).unbatch()

ds_debug.filter(lambda z1,z2:tf.math.reduce_any(z1[0][0] == 5))
```


### Relevant log output

```shell
python debug.py 
2022-06-27 15:32:46.330009: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-06-27 15:32:46.330175: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-06-27 15:32:46.330983: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
Converted call: <function timeseries_dataset_from_array.<locals>.<lambda> at 0x7f04d2bae310>
    args: (<tf.Tensor 'args_0:0' shape=() dtype=int64>, <tf.Tensor 'args_1:0' shape=(13,) dtype=int32>)
    kwargs: {}

Allowlisted: <function timeseries_dataset_from_array.<locals>.<lambda> at 0x7f04d2bae310>: DoNotConvert rule for tensorflow
Converted call: <function sequences_from_indices.<locals>.<lambda> at 0x7f04d2baeee0>
    args: (<tf.Tensor 'args_0:0' shape=(17, 1) dtype=int64>, <tf.Tensor 'args_1:0' shape=(None,) dtype=int32>)
    kwargs: {}

Allowlisted: <function sequences_from_indices.<locals>.<lambda> at 0x7f04d2baeee0>: DoNotConvert rule for tensorflow
Converted call: <function timeseries_dataset_from_array.<locals>.<lambda> at 0x7f04cceb5040>
    args: (<tf.Tensor 'args_0:0' shape=() dtype=int64>, <tf.Tensor 'args_1:0' shape=(13,) dtype=int32>)
    kwargs: {}

Allowlisted: <function timeseries_dataset_from_array.<locals>.<lambda> at 0x7f04cceb5040>: DoNotConvert rule for tensorflow
Converted call: <function sequences_from_indices.<locals>.<lambda> at 0x7f04cceb5310>
    args: (<tf.Tensor 'args_0:0' shape=(17, 2) dtype=int64>, <tf.Tensor 'args_1:0' shape=() dtype=int32>)
    kwargs: {}

Allowlisted: <function sequences_from_indices.<locals>.<lambda> at 0x7f04cceb5310>: DoNotConvert rule for tensorflow
Converted call: <function timeseries_dataset_from_array.<locals>.<lambda> at 0x7f04cceb5a60>
    args: (<tf.Tensor 'args_0:0' shape=() dtype=int64>, <tf.Tensor 'args_1:0' shape=(13,) dtype=int32>)
    kwargs: {}

Allowlisted: <function timeseries_dataset_from_array.<locals>.<lambda> at 0x7f04cceb5a60>: DoNotConvert rule for tensorflow
Converted call: <function sequences_from_indices.<locals>.<lambda> at 0x7f04cceb5af0>
    args: (<tf.Tensor 'args_0:0' shape=(17, 1) dtype=int64>, <tf.Tensor 'args_1:0' shape=(None,) dtype=int32>)
    kwargs: {}

Allowlisted: <function sequences_from_indices.<locals>.<lambda> at 0x7f04cceb5af0>: DoNotConvert rule for tensorflow
Converted call: <function timeseries_dataset_from_array.<locals>.<lambda> at 0x7f04ccee3160>
    args: (<tf.Tensor 'args_0:0' shape=() dtype=int64>, <tf.Tensor 'args_1:0' shape=(13,) dtype=int32>)
    kwargs: {}

Allowlisted: <function timeseries_dataset_from_array.<locals>.<lambda> at 0x7f04ccee3160>: DoNotConvert rule for tensorflow
Converted call: <function sequences_from_indices.<locals>.<lambda> at 0x7f04ccee31f0>
    args: (<tf.Tensor 'args_0:0' shape=(17, 2) dtype=int64>, <tf.Tensor 'args_1:0' shape=() dtype=int32>)
    kwargs: {}

Allowlisted: <function sequences_from_indices.<locals>.<lambda> at 0x7f04ccee31f0>: DoNotConvert rule for tensorflow
Converted call: <function normalize_to_dense.<locals>.normalize at 0x7f04ccee35e0>
    args: ((<tf.Tensor 'args_0:0' shape=(None, None, 1) dtype=int64>, <tf.Tensor 'args_1:0' shape=(None, 2) dtype=int64>), (<tf.Tensor 'args_2:0' shape=(None, None, 1) dtype=int64>, <tf.Tensor 'args_3:0' shape=(None, 2) dtype=int64>))
    kwargs: {}

Allowlisted: <function normalize_to_dense.<locals>.normalize at 0x7f04ccee35e0>: DoNotConvert rule for tensorflow
Converted call: <function <lambda> at 0x7f04ccee3820>
    args: ((<tf.Tensor 'args_0:0' shape=(None, 1) dtype=int64>, <tf.Tensor 'args_1:0' shape=(2,) dtype=int64>), (<tf.Tensor 'args_2:0' shape=(None, 1) dtype=int64>, <tf.Tensor 'args_3:0' shape=(2,) dtype=int64>))
    kwargs: {}

Not allowed: <method-wrapper '__call__' of function object at 0x7f04ccee3820>: default rule
Not allowed: <function <lambda> at 0x7f04ccee3820>: default rule
<function <lambda> at 0x7f04ccee3820> is not cached for subkey ConversionOptions[{}]
Source code of <function <lambda> at 0x7f04ccee3820>:

lambda z1,z2:tf.math.reduce_any(z1[0][0] == 5)

Error transforming entity <function <lambda> at 0x7f04ccee3820>
Traceback (most recent call last):
  File ""/home/mspils/anaconda3/envs/wl/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 447, in converted_call
    converted_f = _convert_actual(target_entity, program_ctx)
  File ""/home/mspils/anaconda3/envs/wl/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 284, in _convert_actual
    transformed, module, source_map = _TRANSPILER.transform(entity, program_ctx)
  File ""/home/mspils/anaconda3/envs/wl/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transpiler.py"", line 286, in transform
    return self.transform_function(obj, user_context)
  File ""/home/mspils/anaconda3/envs/wl/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transpiler.py"", line 470, in transform_function
    nodes, ctx = super(PyToPy, self).transform_function(fn, user_context)
  File ""/home/mspils/anaconda3/envs/wl/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transpiler.py"", line 363, in transform_function
    result = self.transform_ast(node, context)
  File ""/home/mspils/anaconda3/envs/wl/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 252, in transform_ast
    node = self.initial_analysis(node, ctx)
  File ""/home/mspils/anaconda3/envs/wl/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 239, in initial_analysis
    node = qual_names.resolve(node)
  File ""/home/mspils/anaconda3/envs/wl/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/qual_names.py"", line 252, in resolve
    return QnResolver().visit(node)
  File ""/home/mspils/anaconda3/envs/wl/lib/python3.9/ast.py"", line 407, in visit
    return visitor(node)
  File ""/home/mspils/anaconda3/envs/wl/lib/python3.9/ast.py"", line 492, in generic_visit
    new_node = self.visit(old_value)
  File ""/home/mspils/anaconda3/envs/wl/lib/python3.9/ast.py"", line 407, in visit
    return visitor(node)
  File ""/home/mspils/anaconda3/envs/wl/lib/python3.9/ast.py"", line 483, in generic_visit
    value = self.visit(value)
  File ""/home/mspils/anaconda3/envs/wl/lib/python3.9/ast.py"", line 407, in visit
    return visitor(node)
  File ""/home/mspils/anaconda3/envs/wl/lib/python3.9/ast.py"", line 492, in generic_visit
    new_node = self.visit(old_value)
  File ""/home/mspils/anaconda3/envs/wl/lib/python3.9/ast.py"", line 407, in visit
    return visitor(node)
  File ""/home/mspils/anaconda3/envs/wl/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/qual_names.py"", line 230, in visit_Subscript
    node = self.generic_visit(node)
  File ""/home/mspils/anaconda3/envs/wl/lib/python3.9/ast.py"", line 492, in generic_visit
    new_node = self.visit(old_value)
  File ""/home/mspils/anaconda3/envs/wl/lib/python3.9/ast.py"", line 407, in visit
    return visitor(node)
  File ""/home/mspils/anaconda3/envs/wl/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/qual_names.py"", line 232, in visit_Subscript
    if not isinstance(s, gast.Index):
AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f04ccee3820> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Index'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function <lambda> at 0x7f04ccee3820> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Index'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
```
</details>"
56592,Micro Speech Example Deploy to STM32F746 error!,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf2.9

### Custom Code

No

### OS Platform and Distribution

ubuntu20

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Following the method to build this example,

https://github.com/tensorflow/tflite-micro/issues/475#issuecomment-915649543

error reported like the following:
xx@xxx:~/code/tflite-micro/test/mbed-os$ mbed compile -m DISCO_F746NG -t GCC_ARM -D CMSIS_NN
[mbed] Working path ""/home/lmk/code/tflite-micro/test/mbed-os"" (library)
[mbed] Program path ""/home/lmk/code/tflite-micro/test""
[mbed] WARNING: Missing Python modules were not auto-installed.
The Mbed OS tools in this program require the following Python modules: future, click
You can install all missing modules by running ""pip install -r requirements.txt"" in ""/home/lmk/code/tflite-micro/test/mbed-os""
On Posix systems (Linux, etc) you might have to switch to superuser account or use ""sudo""

Building project test (DISCO_F746NG, GCC_ARM)
Scan: test
Compile [ 0.2%]: micro_model_settings.cc
Compile [ 0.3%]: command_responder.cc
Compile [ 0.3%]: main.cc
Compile [ 0.4%]: audio_provider.cc
Compile [ 0.5%]: no_micro_features_data.cc
Compile [ 0.6%]: feature_provider.cc
Compile [ 0.7%]: yes_micro_features_data.cc
Compile [ 0.8%]: no_30ms_audio_data.cc
Compile [ 0.8%]: yes_30ms_audio_data.cc
Compile [ 0.9%]: micro_speech_model_data.cc
Compile [ 1.0%]: recognize_commands.cc
Compile [ 1.1%]: RTX_Config.c
Compile [ 1.2%]: no_1000ms_audio_data.cc
Compile [ 1.3%]: cmsis_os1.c
[Fatal Error] base.h@38,10: cstdint: No such file or directory
[ERROR] In file included from ../third_party/flatbuffers/include/flatbuffers/string.h:20,
from ../mbed-os/cmsis/CMSIS_5/CMSIS/RTOS2/RTX/Library/cmsis_os1.c:27:
../third_party/flatbuffers/include/flatbuffers/base.h:38:10: fatal error: cstdint: No such file or directory
38 | #include
| ^~~~~~~~~
compilation terminated.

[mbed] ERROR: ""/usr/bin/python3"" returned error.
Code: 1
Path: ""/home/lmk/code/tflite-micro/test/mbed-os""
Command: ""/usr/bin/python3 -u /home/lmk/code/tflite-micro/test/mbed-os/tools/make.py -D CMSIS_NN -t GCC_ARM -m DISCO_F746NG --source .. --build ../BUILD/DISCO_F746NG/GCC_ARM""
Tip: You could retry the last command with ""-v"" flag for verbose output
```


### Standalone code to reproduce the issue

```shell
Following the codes:
python3 tensorflow/lite/micro/tools/project_generation/create_tflm_tree.py  --makefile_options OPTIMIZED_KERNEL_DIR=cmsis_nn --examples micro_speech <output_dir> 
cd <output_dir>
mbed new .
mbed compile -m DISCO_F746NG -t GCC_ARM -D CMSIS_NN
```


### Relevant log output

_No response_</details>"
56590,Getting different results with cpu and gpu delegate,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf2.8

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

Galaxy Note 10

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I push the model and executable code built with android tflite into my mobile device, but I get different result on cpu and gpu delegate. One of the outputs is all 0.000. And this is my important project, I am expecting your help.
```


### Standalone code to reproduce the issue

```shell
/
```


### Relevant log output

_No response_</details>"
56589,ValueError: Duplicate node name in graph: 'model/lambda_16/zeros/packed',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

1.15

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

-

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

10.0.130

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Getting error ValueError: Duplicate node name in graph: 'model/lambda_16/zeros/packed' weren't able to find a way to fix it on the net
```


### Standalone code to reproduce the issue

```shell
I ran code from https://github.com/ameraner/dsen2-cr. The code with images and prepared csv file can be found here: https://drive.google.com/drive/folders/13gN68q5nH_h6VQtjga992UOXUsWUZV6m?usp=sharing. It is run with  dsen2-cr/Code/dsen2cr_main.py --predict dsen2-cr/model_SARcarl.hdf5 it run for a long time
```


### Relevant log output

_No response_</details>"
56588,How to convert saved model to tflite format model partly?,"### 1. System information

- WSL (Linux Ubuntu 20.04):
- TensorFlow library version  2.7.0

### 2. Code

```
tflite_convert --output_file=./detection.tflite --saved_model_dir=model_dir --input_arryas=input --output_arrays=output --input_shape=1,224,224,3 --experimental_new_converter=true --allow_custom_ops
```

### 3. Failure after conversion
I could pass convert. But I get the whole graph after conversion. I want to set  intermediate node as output node. What should I do?
"
56587,Clip weight values when converting to int8 tflite models,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): TF 1.14

### 2. Code

Provide code to help us reproduce your issues using one of the following options:
```
converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(
            args.pb_path, 
            [args.input_node_name], 
            [args.output_node_name], 
            input_shapes=input_shape
        )
        converter.allow_custom_ops = True
        converter.optimizations = [tf.lite.Optimize.DEFAULT] # require representative dataset
        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
        converter.inference_input_type = tf.int8 
        converter.inference_output_type = tf.int8 
        converter.default_ranges_stats = (2.3283064e-10, 4294967295) # 2022.06.22: weight clipping

        converter.representative_dataset = representative_data_gen
        tflite_int8_model = converter.convert()
```

### 3. Failure after conversion
Hi, 

I'm trying to limit the range the weight values of tflite models because there are limited supported range of weight values on the  hardware I use. Therefore, I have tried to apply `default_ranges_stats` when converting the trained model to an int8 tflite model. However, the weight values of converted model are not changed.

Is there other methods to limit the range of tflite models during the conversion stage?

Best Regards
"
56586,"How to build tensorflow from source, so that it works similar across INTEL and AMD CPUs (in terms of floating point math precision)","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

1.15

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.6.9

### Bazel version

0.26.1

### GCC/Compiler version

7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


I am trying to build tensorflow from source(v1.15) as I was getting a difference in floating point math precision across INTEL and AMD CPUs(refer to issue #56529). What are the possible options to provide for the build command to fix this? I am looking for a solution for either TF v1.15 or TF v2.3. 

I tried the following build commands, 
```shell
1)  bazel --output_base=/local/mnt/workspace/gsaichai build --config=v1 --config=mkl --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --copt=-mfpmath=387 --copt=-mtune=generic --copt=-march=x86-64 --host_copt=-march=x86-64 --verbose_failures //tensorflow/tools/pip_package:build_pip_package

2)  bazel --output_base=/local/mnt/workspace/gsaichai build --config=v1 --config=mkl --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --copt=-mfpmath=387 --copt=-march=x86-64 --host_copt=-march=x86-64 --verbose_failures //tensorflow/tools/pip_package:build_pip_package

```
But the difference was still present.

<s>These commands are resulting in the following error,
```shell
ERROR: /local/mnt/workspace/gsaichai/qnn_src/tensorflow/tensorflow/python/BUILD:329:1: C++ compilation of rule '//tensorflow/python:bfloat16_lib' failed (Exit 1)
tensorflow/python/lib/core/bfloat16.cc: In function 'bool tensorflow::{anonymous}::Initialize()':
tensorflow/python/lib/core/bfloat16.cc:634:36: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [6], <unresolved overloaded function type>, const std::array<int, 3>&)'
                       compare_types)) {
                                    ^
tensorflow/python/lib/core/bfloat16.cc:608:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>
                             const std::array<int, 3>& types) {
                                                            ^
tensorflow/python/lib/core/bfloat16.cc:608:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'
tensorflow/python/lib/core/bfloat16.cc:638:36: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [10], <unresolved overloaded function type>, const std::array<int, 3>&)'
                       compare_types)) {
                                    ^
tensorflow/python/lib/core/bfloat16.cc:608:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>
                             const std::array<int, 3>& types) {
                                                            ^
tensorflow/python/lib/core/bfloat16.cc:608:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'
tensorflow/python/lib/core/bfloat16.cc:641:77: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [5], <unresolved overloaded function type>, const std::array<int, 3>&)'
   if (!register_ufunc(""less"", CompareUFunc<Bfloat16LtFunctor>, compare_types)) {
                                                                             ^
tensorflow/python/lib/core/bfloat16.cc:608:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>
                             const std::array<int, 3>& types) {
                                                            ^
tensorflow/python/lib/core/bfloat16.cc:608:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'
tensorflow/python/lib/core/bfloat16.cc:645:36: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [8], <unresolved overloaded function type>, const std::array<int, 3>&)'
                       compare_types)) {
                                    ^
tensorflow/python/lib/core/bfloat16.cc:608:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>
                             const std::array<int, 3>& types) {
                                                            ^
tensorflow/python/lib/core/bfloat16.cc:608:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'
tensorflow/python/lib/core/bfloat16.cc:649:36: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [11], <unresolved overloaded function type>, const std::array<int, 3>&)'
                       compare_types)) {
                                    ^
tensorflow/python/lib/core/bfloat16.cc:608:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>
                             const std::array<int, 3>& types) {
                                                            ^
tensorflow/python/lib/core/bfloat16.cc:608:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'
tensorflow/python/lib/core/bfloat16.cc:653:36: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [14], <unresolved overloaded function type>, const std::array<int, 3>&)'
                       compare_types)) {
                                    ^
tensorflow/python/lib/core/bfloat16.cc:608:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>
                             const std::array<int, 3>& types) {
                                                            ^
tensorflow/python/lib/core/bfloat16.cc:608:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 232.568s, Critical Path: 63.39s
INFO: 1208 processes: 1208 local.
FAILED: Build did NOT complete successfully

``` 
</s>

<b>The above error got resolved after making some changes to my ./configure file. But the resulting tensorflow packages are still resulting in the difference between AMD and INTEL. Please let me know what are the options that I can provide while building Tensorflow from source(for v1.15 or v2.3), so that the floating point math is consistent across AMD and INTEL</b> 
### Standalone code to reproduce the issue

```shell
None
```


### Relevant log output

_No response_</details>"
56585,build tensorflow from source file get error,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.6.0

### Custom Code

No

### OS Platform and Distribution

ubuntu 18.04

### Mobile device

_No response_

### Python version

python3.6.2

### Bazel version

3.7.2

### GCC/Compiler version

gcc7.5

### CUDA/cuDNN version

cuda10.1

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
I try to build tensorflow-2.6 from source[https://www.tensorflow.org/install/source]
follow the steps until 
'bazel build --config=cuda  //tensorflow/tools/pip_package:build_pip_package'
the terminal logs :
In file included from tensorflow/stream_executor/cuda/cusparse_stub.cc:16:0:
bazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual/third_party/gpus/cuda/include/cusparse.h:7029:1: note: previous declaration 'cusparseStatus_t cusparseDnMatGet(cusparseDnMatDescr_t, size_t*, size_t*, int64_t*, void**, cudaDataType*, cusparseOrder_t*)'
 cusparseDnMatGet(const cusparseDnMatDescr_t dnMatDescr,
 ^~~~~~~~~~~~~~~~
In file included from tensorflow/stream_executor/cuda/cusparse_stub.cc:59:0:
./tensorflow/stream_executor/cuda/cusparse_10_1.inc: In function 'cusparseStatus_t cusparseDnMatSetStridedBatch(cusparseDnMatDescr_t, int, int64_t)':
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8124:30: error: conflicting declaration of C function 'cusparseStatus_t cusparseDnMatSetStridedBatch(cusparseDnMatDescr_t, int, int64_t)'
 cusparseStatus_t CUSPARSEAPI cusparseDnMatSetStridedBatch(
                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from tensorflow/stream_executor/cuda/cusparse_stub.cc:16:0:
bazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual/third_party/gpus/cuda/include/cusparse.h:7038:1: note: previous declaration 'cusparseStatus_t cusparseDnMatSetStridedBatch(cusparseDnMatDescr_t, int, size_t)'
 cusparseDnMatSetStridedBatch(cusparseDnMatDescr_t dnMatDescr,
 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from tensorflow/stream_executor/cuda/cusparse_stub.cc:59:0:
./tensorflow/stream_executor/cuda/cusparse_10_1.inc: In function 'cusparseStatus_t cusparseDnMatGetStridedBatch(cusparseDnMatDescr_t, int*, int64_t*)':
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8134:1: error: conflicting declaration of C function 'cusparseStatus_t cusparseDnMatGetStridedBatch(cusparseDnMatDescr_t, int*, int64_t*)'
 cusparseDnMatGetStridedBatch(const cusparseDnMatDescr_t dnMatDescr,
 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from tensorflow/stream_executor/cuda/cusparse_stub.cc:16:0:
bazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual/third_party/gpus/cuda/include/cusparse.h:7043:1: note: previous declaration 'cusparseStatus_t cusparseDnMatGetStridedBatch(cusparseDnMatDescr_t, int*, size_t*)'
 cusparseDnMatGetStridedBatch(const cusparseDnMatDescr_t dnMatDescr,
 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from tensorflow/stream_executor/cuda/cusparse_stub.cc:59:0:
./tensorflow/stream_executor/cuda/cusparse_10_1.inc: At global scope:
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8145:20: error: 'cusparseSpVecDescr_t' does not name a type; did you mean 'cusparseSpMatDescr_t'?
              const cusparseSpVecDescr_t vecX, const cusparseDnVecDescr_t vecY,
                    ^~~~~~~~~~~~~~~~~~~~
                    cusparseSpMatDescr_t
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8145:53: error: 'cusparseDnVecDescr_t' does not name a type; did you mean 'cusparseDnMatDescr_t'?
              const cusparseSpVecDescr_t vecX, const cusparseDnVecDescr_t vecY,
                                                     ^~~~~~~~~~~~~~~~~~~~
                                                     cusparseDnMatDescr_t
./tensorflow/stream_executor/cuda/cusparse_10_1.inc: In function 'cusparseStatus_t cusparseSpVV(cusparseHandle_t, cusparseOperation_t, int, int, void*, cudaDataType, void*)':
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8148:52: error: 'cusparseSpVecDescr_t' does not name a type; did you mean 'cusparseSpMatDescr_t'?
       cusparseHandle_t, cusparseOperation_t, const cusparseSpVecDescr_t,
                                                    ^~~~~~~~~~~~~~~~~~~~
                                                    cusparseSpMatDescr_t
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8149:13: error: 'cusparseDnVecDescr_t' does not name a type; did you mean 'cusparseDnMatDescr_t'?
       const cusparseDnVecDescr_t, void *, cudaDataType, void *);
             ^~~~~~~~~~~~~~~~~~~~
             cusparseDnMatDescr_t
./tensorflow/stream_executor/cuda/cusparse_10_1.inc: At global scope:
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8157:11: error: 'cusparseSpVecDescr_t' does not name a type; did you mean 'cusparseSpMatDescr_t'?
     const cusparseSpVecDescr_t vecX, const cusparseDnVecDescr_t vecY,
           ^~~~~~~~~~~~~~~~~~~~
           cusparseSpMatDescr_t
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8157:44: error: 'cusparseDnVecDescr_t' does not name a type; did you mean 'cusparseDnMatDescr_t'?
     const cusparseSpVecDescr_t vecX, const cusparseDnVecDescr_t vecY,
                                            ^~~~~~~~~~~~~~~~~~~~
                                            cusparseDnMatDescr_t
./tensorflow/stream_executor/cuda/cusparse_10_1.inc: In function 'cusparseStatus_t cusparseSpVV_bufferSize(cusparseHandle_t, cusparseOperation_t, int, int, const void*, cudaDataType, size_t*)':
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8160:52: error: 'cusparseSpVecDescr_t' does not name a type; did you mean 'cusparseSpMatDescr_t'?
       cusparseHandle_t, cusparseOperation_t, const cusparseSpVecDescr_t,
                                                    ^~~~~~~~~~~~~~~~~~~~
                                                    cusparseSpMatDescr_t
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8161:13: error: 'cusparseDnVecDescr_t' does not name a type; did you mean 'cusparseDnMatDescr_t'?
       const cusparseDnVecDescr_t, const void *, cudaDataType, size_t *);
             ^~~~~~~~~~~~~~~~~~~~
             cusparseDnMatDescr_t
./tensorflow/stream_executor/cuda/cusparse_10_1.inc: At global scope:
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8169:44: error: 'cusparseDnVecDescr_t' does not name a type; did you mean 'cusparseDnMatDescr_t'?
     const cusparseSpMatDescr_t matA, const cusparseDnVecDescr_t vecX,
                                            ^~~~~~~~~~~~~~~~~~~~
                                            cusparseDnMatDescr_t
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8170:29: error: 'cusparseDnVecDescr_t' does not name a type; did you mean 'cusparseDnMatDescr_t'?
     const void *beta, const cusparseDnVecDescr_t vecY, cudaDataType computeType,
                             ^~~~~~~~~~~~~~~~~~~~
                             cusparseDnMatDescr_t
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8171:5: error: 'cusparseSpMVAlg_t' has not been declared
     cusparseSpMVAlg_t alg, void *externalBuffer) {
     ^~~~~~~~~~~~~~~~~
./tensorflow/stream_executor/cuda/cusparse_10_1.inc: In function 'cusparseStatus_t cusparseSpMV(cusparseHandle_t, cusparseOperation_t, const void*, cusparseSpMatDescr_t, int, const void*, int, cudaDataType, int, void*)':
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8174:41: error: 'cusparseDnVecDescr_t' does not name a type; did you mean 'cusparseDnMatDescr_t'?
       const cusparseSpMatDescr_t, const cusparseDnVecDescr_t, const void *,
                                         ^~~~~~~~~~~~~~~~~~~~
                                         cusparseDnMatDescr_t
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8175:13: error: 'cusparseDnVecDescr_t' does not name a type; did you mean 'cusparseDnMatDescr_t'?
       const cusparseDnVecDescr_t, cudaDataType, cusparseSpMVAlg_t, void *);
             ^~~~~~~~~~~~~~~~~~~~
             cusparseDnMatDescr_t
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8175:49: error: 'cusparseSpMVAlg_t' has not been declared
       const cusparseDnVecDescr_t, cudaDataType, cusparseSpMVAlg_t, void *);
                                                 ^~~~~~~~~~~~~~~~~
./tensorflow/stream_executor/cuda/cusparse_10_1.inc: At global scope:
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8184:44: error: 'cusparseDnVecDescr_t' does not name a type; did you mean 'cusparseDnMatDescr_t'?
     const cusparseSpMatDescr_t matA, const cusparseDnVecDescr_t vecX,
                                            ^~~~~~~~~~~~~~~~~~~~
                                            cusparseDnMatDescr_t
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8185:29: error: 'cusparseDnVecDescr_t' does not name a type; did you mean 'cusparseDnMatDescr_t'?
     const void *beta, const cusparseDnVecDescr_t vecY, cudaDataType computeType,
                             ^~~~~~~~~~~~~~~~~~~~
                             cusparseDnMatDescr_t
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8186:5: error: 'cusparseSpMVAlg_t' has not been declared
     cusparseSpMVAlg_t alg, size_t *bufferSize) {
     ^~~~~~~~~~~~~~~~~
./tensorflow/stream_executor/cuda/cusparse_10_1.inc: In function 'cusparseStatus_t cusparseSpMV_bufferSize(cusparseHandle_t, cusparseOperation_t, const void*, cusparseSpMatDescr_t, int, const void*, int, cudaDataType, int, size_t*)':
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8189:41: error: 'cusparseDnVecDescr_t' does not name a type; did you mean 'cusparseDnMatDescr_t'?
       const cusparseSpMatDescr_t, const cusparseDnVecDescr_t, const void *,
                                         ^~~~~~~~~~~~~~~~~~~~
                                         cusparseDnMatDescr_t
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8190:13: error: 'cusparseDnVecDescr_t' does not name a type; did you mean 'cusparseDnMatDescr_t'?
       const cusparseDnVecDescr_t, cudaDataType, cusparseSpMVAlg_t, size_t *);
             ^~~~~~~~~~~~~~~~~~~~
             cusparseDnMatDescr_t
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8190:49: error: 'cusparseSpMVAlg_t' has not been declared
       const cusparseDnVecDescr_t, cudaDataType, cusparseSpMVAlg_t, size_t *);
                                                 ^~~~~~~~~~~~~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 5.609s, Critical Path: 4.92s
INFO: 33 processes: 33 internal.
FAILED: Build did NOT complete successfully
```


### Standalone code to reproduce the issue

```shell
Ubuntu 18.04
tensorflow version:919f693420e (tag: v2.6.0, origin/release-2.6.0, release-2.6.0) Merge pull request #51398 from tensorflow-jenkins/version-numbers-2.6.0-305

cuda 10.1
cudnn 7.6
```


### Relevant log output

```shell
In file included from tensorflow/stream_executor/cuda/cusparse_stub.cc:16:0:
bazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual/third_party/gpus/cuda/include/cusparse.h:7029:1: note: previous declaration 'cusparseStatus_t cusparseDnMatGet(cusparseDnMatDescr_t, size_t*, size_t*, int64_t*, void**, cudaDataType*, cusparseOrder_t*)'
 cusparseDnMatGet(const cusparseDnMatDescr_t dnMatDescr,
 ^~~~~~~~~~~~~~~~
In file included from tensorflow/stream_executor/cuda/cusparse_stub.cc:59:0:
./tensorflow/stream_executor/cuda/cusparse_10_1.inc: In function 'cusparseStatus_t cusparseDnMatSetStridedBatch(cusparseDnMatDescr_t, int, int64_t)':
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8124:30: error: conflicting declaration of C function 'cusparseStatus_t cusparseDnMatSetStridedBatch(cusparseDnMatDescr_t, int, int64_t)'
 cusparseStatus_t CUSPARSEAPI cusparseDnMatSetStridedBatch(
                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from tensorflow/stream_executor/cuda/cusparse_stub.cc:16:0:
bazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual/third_party/gpus/cuda/include/cusparse.h:7038:1: note: previous declaration 'cusparseStatus_t cusparseDnMatSetStridedBatch(cusparseDnMatDescr_t, int, size_t)'
 cusparseDnMatSetStridedBatch(cusparseDnMatDescr_t dnMatDescr,
 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from tensorflow/stream_executor/cuda/cusparse_stub.cc:59:0:
./tensorflow/stream_executor/cuda/cusparse_10_1.inc: In function 'cusparseStatus_t cusparseDnMatGetStridedBatch(cusparseDnMatDescr_t, int*, int64_t*)':
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8134:1: error: conflicting declaration of C function 'cusparseStatus_t cusparseDnMatGetStridedBatch(cusparseDnMatDescr_t, int*, int64_t*)'
 cusparseDnMatGetStridedBatch(const cusparseDnMatDescr_t dnMatDescr,
 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from tensorflow/stream_executor/cuda/cusparse_stub.cc:16:0:
bazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual/third_party/gpus/cuda/include/cusparse.h:7043:1: note: previous declaration 'cusparseStatus_t cusparseDnMatGetStridedBatch(cusparseDnMatDescr_t, int*, size_t*)'
 cusparseDnMatGetStridedBatch(const cusparseDnMatDescr_t dnMatDescr,
 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from tensorflow/stream_executor/cuda/cusparse_stub.cc:59:0:
./tensorflow/stream_executor/cuda/cusparse_10_1.inc: At global scope:
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8145:20: error: 'cusparseSpVecDescr_t' does not name a type; did you mean 'cusparseSpMatDescr_t'?
              const cusparseSpVecDescr_t vecX, const cusparseDnVecDescr_t vecY,
                    ^~~~~~~~~~~~~~~~~~~~
                    cusparseSpMatDescr_t
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8145:53: error: 'cusparseDnVecDescr_t' does not name a type; did you mean 'cusparseDnMatDescr_t'?
              const cusparseSpVecDescr_t vecX, const cusparseDnVecDescr_t vecY,
                                                     ^~~~~~~~~~~~~~~~~~~~
                                                     cusparseDnMatDescr_t
./tensorflow/stream_executor/cuda/cusparse_10_1.inc: In function 'cusparseStatus_t cusparseSpVV(cusparseHandle_t, cusparseOperation_t, int, int, void*, cudaDataType, void*)':
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8148:52: error: 'cusparseSpVecDescr_t' does not name a type; did you mean 'cusparseSpMatDescr_t'?
       cusparseHandle_t, cusparseOperation_t, const cusparseSpVecDescr_t,
                                                    ^~~~~~~~~~~~~~~~~~~~
                                                    cusparseSpMatDescr_t
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8149:13: error: 'cusparseDnVecDescr_t' does not name a type; did you mean 'cusparseDnMatDescr_t'?
       const cusparseDnVecDescr_t, void *, cudaDataType, void *);
             ^~~~~~~~~~~~~~~~~~~~
             cusparseDnMatDescr_t
./tensorflow/stream_executor/cuda/cusparse_10_1.inc: At global scope:
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8157:11: error: 'cusparseSpVecDescr_t' does not name a type; did you mean 'cusparseSpMatDescr_t'?
     const cusparseSpVecDescr_t vecX, const cusparseDnVecDescr_t vecY,
           ^~~~~~~~~~~~~~~~~~~~
           cusparseSpMatDescr_t
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8157:44: error: 'cusparseDnVecDescr_t' does not name a type; did you mean 'cusparseDnMatDescr_t'?
     const cusparseSpVecDescr_t vecX, const cusparseDnVecDescr_t vecY,
                                            ^~~~~~~~~~~~~~~~~~~~
                                            cusparseDnMatDescr_t
./tensorflow/stream_executor/cuda/cusparse_10_1.inc: In function 'cusparseStatus_t cusparseSpVV_bufferSize(cusparseHandle_t, cusparseOperation_t, int, int, const void*, cudaDataType, size_t*)':
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8160:52: error: 'cusparseSpVecDescr_t' does not name a type; did you mean 'cusparseSpMatDescr_t'?
       cusparseHandle_t, cusparseOperation_t, const cusparseSpVecDescr_t,
                                                    ^~~~~~~~~~~~~~~~~~~~
                                                    cusparseSpMatDescr_t
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8161:13: error: 'cusparseDnVecDescr_t' does not name a type; did you mean 'cusparseDnMatDescr_t'?
       const cusparseDnVecDescr_t, const void *, cudaDataType, size_t *);
             ^~~~~~~~~~~~~~~~~~~~
             cusparseDnMatDescr_t
./tensorflow/stream_executor/cuda/cusparse_10_1.inc: At global scope:
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8169:44: error: 'cusparseDnVecDescr_t' does not name a type; did you mean 'cusparseDnMatDescr_t'?
     const cusparseSpMatDescr_t matA, const cusparseDnVecDescr_t vecX,
                                            ^~~~~~~~~~~~~~~~~~~~
                                            cusparseDnMatDescr_t
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8170:29: error: 'cusparseDnVecDescr_t' does not name a type; did you mean 'cusparseDnMatDescr_t'?
     const void *beta, const cusparseDnVecDescr_t vecY, cudaDataType computeType,
                             ^~~~~~~~~~~~~~~~~~~~
                             cusparseDnMatDescr_t
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8171:5: error: 'cusparseSpMVAlg_t' has not been declared
     cusparseSpMVAlg_t alg, void *externalBuffer) {
     ^~~~~~~~~~~~~~~~~
./tensorflow/stream_executor/cuda/cusparse_10_1.inc: In function 'cusparseStatus_t cusparseSpMV(cusparseHandle_t, cusparseOperation_t, const void*, cusparseSpMatDescr_t, int, const void*, int, cudaDataType, int, void*)':
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8174:41: error: 'cusparseDnVecDescr_t' does not name a type; did you mean 'cusparseDnMatDescr_t'?
       const cusparseSpMatDescr_t, const cusparseDnVecDescr_t, const void *,
                                         ^~~~~~~~~~~~~~~~~~~~
                                         cusparseDnMatDescr_t
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8175:13: error: 'cusparseDnVecDescr_t' does not name a type; did you mean 'cusparseDnMatDescr_t'?
       const cusparseDnVecDescr_t, cudaDataType, cusparseSpMVAlg_t, void *);
             ^~~~~~~~~~~~~~~~~~~~
             cusparseDnMatDescr_t
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8175:49: error: 'cusparseSpMVAlg_t' has not been declared
       const cusparseDnVecDescr_t, cudaDataType, cusparseSpMVAlg_t, void *);
                                                 ^~~~~~~~~~~~~~~~~
./tensorflow/stream_executor/cuda/cusparse_10_1.inc: At global scope:
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8184:44: error: 'cusparseDnVecDescr_t' does not name a type; did you mean 'cusparseDnMatDescr_t'?
     const cusparseSpMatDescr_t matA, const cusparseDnVecDescr_t vecX,
                                            ^~~~~~~~~~~~~~~~~~~~
                                            cusparseDnMatDescr_t
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8185:29: error: 'cusparseDnVecDescr_t' does not name a type; did you mean 'cusparseDnMatDescr_t'?
     const void *beta, const cusparseDnVecDescr_t vecY, cudaDataType computeType,
                             ^~~~~~~~~~~~~~~~~~~~
                             cusparseDnMatDescr_t
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8186:5: error: 'cusparseSpMVAlg_t' has not been declared
     cusparseSpMVAlg_t alg, size_t *bufferSize) {
     ^~~~~~~~~~~~~~~~~
./tensorflow/stream_executor/cuda/cusparse_10_1.inc: In function 'cusparseStatus_t cusparseSpMV_bufferSize(cusparseHandle_t, cusparseOperation_t, const void*, cusparseSpMatDescr_t, int, const void*, int, cudaDataType, int, size_t*)':
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8189:41: error: 'cusparseDnVecDescr_t' does not name a type; did you mean 'cusparseDnMatDescr_t'?
       const cusparseSpMatDescr_t, const cusparseDnVecDescr_t, const void *,
                                         ^~~~~~~~~~~~~~~~~~~~
                                         cusparseDnMatDescr_t
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8190:13: error: 'cusparseDnVecDescr_t' does not name a type; did you mean 'cusparseDnMatDescr_t'?
       const cusparseDnVecDescr_t, cudaDataType, cusparseSpMVAlg_t, size_t *);
             ^~~~~~~~~~~~~~~~~~~~
             cusparseDnMatDescr_t
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:8190:49: error: 'cusparseSpMVAlg_t' has not been declared
       const cusparseDnVecDescr_t, cudaDataType, cusparseSpMVAlg_t, size_t *);
                                                 ^~~~~~~~~~~~~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 5.609s, Critical Path: 4.92s
INFO: 33 processes: 33 internal.
FAILED: Build did NOT complete successfully
```
</details>"
56584,disco_f746ng_makefile.inc: No such file or directory,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

No

### OS Platform and Distribution

ubuntu20

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

GCC 9.1.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tensorflow/lite/micro/tools/make/downloads/flatbuffers already exists, skipping the download.
tensorflow/lite/micro/tools/make/downloads/kissfft already exists, skipping the download.
tensorflow/lite/micro/tools/make/downloads/pigweed already exists, skipping the download.
tensorflow/lite/micro/tools/make/downloads/cmsis already exists, skipping the download.
tensorflow/lite/micro/tools/make/Makefile:513: tensorflow/lite/micro/tools/make/targets/disco_f746ng_makefile.inc: No such file or directory
make: *** No rule to make target 'tensorflow/lite/micro/tools/make/targets/disco_f746ng_makefile.inc'.  Stop.
```


### Standalone code to reproduce the issue

```shell
make -f tensorflow/lite/micro/tools/make/Makefile TARGET=disco_f746ng OPTIMIZED_KERNEL_DIR=cmsis_nn generate_micro_speech_mbed_project
```


### Relevant log output

_No response_</details>"
56583,How to convert my object detection dataset to Tensorflow COCO format,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

 3.7.13

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Im running this Tensorflow implementation of Retinanet (https://keras.io/examples/vision/retinanet/ and I want to adapt it to my own Object Detection dataset.

The problem is that the author gets COCO dataset via tfds.load()
I explored these TFRecords files and I noticed that their image annotations are in a different way (different from default COCO annotations) as shown in this image: https://bit.ly/3yjYHDA

Is there a way to generate TFRecords files in this specific format of annotations for my own dataset (not get a ready-to-use dataset from tfds)?

Thanks a lot!
```


### Standalone code to reproduce the issue

```shell
https://keras.io/examples/vision/retinanet/
```


### Relevant log output

_No response_</details>"
56582,SavedModelBundle object instantiation raises protobuf error,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Source

source

### Tensorflow Version

2.9.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

5.0.0

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

CUDA 11.2 / cuDNN 8.1

### GPU model and memory

Tesla P100-PCIE-12GB

### Current Behaviour?

```shell
I built my Tensorflow 2.9.0 from source. During the build process I also installed protobuf-3.9.2. I am trying to load a saved model from: 
https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md

Model: EfficientDet D3 896x896

To do that I created a simple code, during the instantiation of a SavedModelBundle object, I get the following error: 


[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/descriptor_database.cc:118] File already exists in database: google/protobuf/any.proto
[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/descriptor.cc:1379] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): 
terminate called after throwing an instance of 'google::protobuf::FatalException'
  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): 
Aborted (core dumped)
``` 
Could some one please tell me why protobuf is throwing this error and how to resolve this.
```


### Standalone code to reproduce the issue

```shell
MAIN.CPP

#include <iostream>
#include ""tensorflow/cc/client/client_session.h""
#include ""tensorflow/core/framework/tensor.h""
#include ""tensorflow/cc/saved_model/loader.h""

using tensorflow::string;
using tensorflow::Tensor;
using tensorflow::tstring;
using tensorflow::SavedModelBundle;
using tensorflow::SessionOptions;
using tensorflow::RunOptions;
using tensorflow::ClientSession;

int main(int argc, char* argv[]){
    
	if (argc != 2){
		std::cout << ""Error! Usage: <path/to_saved_model> "" << std::endl;
		return 1;
	}

    const std::string model_path = argv[1];
    std::cout << model_path << std::endl; 

    SavedModelBundle bundle;
    SessionOptions session_options;
    RunOptions run_options;
    
    session_options.config.mutable_gpu_options()->set_allow_growth(true);	
	
    auto status = tensorflow::LoadSavedModel(session_options, run_options, model_path, {""serve""}, &bundle);

	if (status.ok()){
		printf(""Model loaded successfully...\n"");
	}
	else {
		printf(""Error in loading model\n"");
	}
    return 0;
}

```

CMAKELISTS.TXT

```
cmake_minimum_required(VERSION 3.16)
project(tensorflow_v2_cpp)

find_package(CUDA REQUIRED)
set(TENSORFLOW_LIB_DIR ""/tensorflow/bazel-bin/tensorflow"")

add_executable(get_prediction main.cpp)
target_include_directories(get_prediction PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/include)
target_include_directories(get_prediction PRIVATE ${TENSORFLOW_LIB_DIR}/include ${OpenCV_INCLUDE_DIRS})

target_link_libraries(get_prediction ${TENSORFLOW_LIB_DIR}/libtensorflow_cc.so ${TENSORFLOW_LIB_DIR}/libtensorflow_framework.so)
```

CLI
```
 ./get_prediction ../../efficientdet_d3_coco17_tpu-32/saved_model/
```
```


### Relevant log output

_No response_</details>"
56579,TPU: unknown service tensorflow.WorkerService,"System information:
Python version: 3.8.8
CUDA/cuDNN version: -
GPU model and memory: TPU v2-8
tensorflow:2.6.2

Here is code:
import tensorflow as tf
print(""Tensorflow version "" + tf.__version__)

try:
  tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='node-14')  # TPU detection
  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])
except ValueError:
  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')

tf.config.experimental_connect_to_cluster(tpu)
tf.tpu.experimental.initialize_tpu_system(tpu)
tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)

print('tpu could work')


But the TPU cannot be make:

ssh://root@34.91.204.189:22/usr/bin/python -u /tmp/pycharm_project_890/cartpo/tputest.py
2022-06-25 20:02:41.660676: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2022-06-25 20:02:41.660879: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Tensorflow version 2.6.2
Running on TPU  ['10.164.0.2:8470']
2022-06-25 20:02:45.423995: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2022-06-25 20:02:45.424056: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2022-06-25 20:02:45.424095: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (instance-5): /proc/driver/nvidia/version does not exist
2022-06-25 20:02:46.788254: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-06-25 20:02:46.792741: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> 10.164.0.2:8470}
2022-06-25 20:02:46.792941: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:54119}
2022-06-25 20:02:46.797212: E tensorflow/core/common_runtime/eager/context_distributed_manager.cc:491] unknown service tensorflow.WorkerService
Additional GRPC error information from remote target /job:worker/replica:0/task:0:
:{""created"":""@1656187366.797041895"",""description"":""Error received from peer ipv4:10.164.0.2:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""unknown service tensorflow.WorkerService"",""grpc_status"":12}
E0625 20:02:46.803456212   21351 completion_queue.cc:244]    assertion failed: queue.num_items() == 0

Process finished with exit code 134"
56577,"    ValueError: Shapes (None, 21) and (None, 1, 1, 21) are incompatible","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.8

### Custom Code

Yes

### OS Platform and Distribution

colab

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When I added keepdim='True' in below code and tried fitting the model it gave the error 

from tensorflow.keras import layers

inputs = tf.keras.Input(shape=(224,224,3))

x = eff_base(inputs)

x = layers.GlobalAveragePooling2D(keepdims=True)(x)

outputs = layers.Dense(21, activation=""softmax"")(x)
print(outputs.shape)

eff_model = tf.keras.Model(inputs, outputs)


Code to fit the model -

# fitting data to the model

eff_model.fit( train_eff,
             epochs = 50,
             steps_per_epoch = len(train_eff),
             validation_data = val_eff,
             validation_steps = len(val_eff)
             )
```


### Standalone code to reproduce the issue

```shell
ValueError: in user code:

    File ""/usr/local/lib/python3.7/dist-packages/keras/engine/training.py"", line 1021, in train_function  *
        return step_function(self, iterator)
    File ""/usr/local/lib/python3.7/dist-packages/keras/engine/training.py"", line 1010, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/usr/local/lib/python3.7/dist-packages/keras/engine/training.py"", line 1000, in run_step  **
        outputs = model.train_step(data)
    File ""/usr/local/lib/python3.7/dist-packages/keras/engine/training.py"", line 860, in train_step
        loss = self.compute_loss(x, y, y_pred, sample_weight)
    File ""/usr/local/lib/python3.7/dist-packages/keras/engine/training.py"", line 919, in compute_loss
        y, y_pred, sample_weight, regularization_losses=self.losses)
    File ""/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py"", line 201, in __call__
        loss_value = loss_obj(y_t, y_p, sample_weight=sw)
    File ""/usr/local/lib/python3.7/dist-packages/keras/losses.py"", line 141, in __call__
        losses = call_fn(y_true, y_pred)
    File ""/usr/local/lib/python3.7/dist-packages/keras/losses.py"", line 245, in call  **
        return ag_fn(y_true, y_pred, **self._fn_kwargs)
    File ""/usr/local/lib/python3.7/dist-packages/keras/losses.py"", line 1790, in categorical_crossentropy
        y_true, y_pred, from_logits=from_logits, axis=axis)
    File ""/usr/local/lib/python3.7/dist-packages/keras/backend.py"", line 5083, in categorical_crossentropy
        target.shape.assert_is_compatible_with(output.shape)

    ValueError: Shapes (None, 21) and (None, 1, 1, 21) are incompatible
```


### Relevant log output

```shell
ValueError: in user code:

    File ""/usr/local/lib/python3.7/dist-packages/keras/engine/training.py"", line 1021, in train_function  *
        return step_function(self, iterator)
    File ""/usr/local/lib/python3.7/dist-packages/keras/engine/training.py"", line 1010, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/usr/local/lib/python3.7/dist-packages/keras/engine/training.py"", line 1000, in run_step  **
        outputs = model.train_step(data)
    File ""/usr/local/lib/python3.7/dist-packages/keras/engine/training.py"", line 860, in train_step
        loss = self.compute_loss(x, y, y_pred, sample_weight)
    File ""/usr/local/lib/python3.7/dist-packages/keras/engine/training.py"", line 919, in compute_loss
        y, y_pred, sample_weight, regularization_losses=self.losses)
    File ""/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py"", line 201, in __call__
        loss_value = loss_obj(y_t, y_p, sample_weight=sw)
    File ""/usr/local/lib/python3.7/dist-packages/keras/losses.py"", line 141, in __call__
        losses = call_fn(y_true, y_pred)
    File ""/usr/local/lib/python3.7/dist-packages/keras/losses.py"", line 245, in call  **
        return ag_fn(y_true, y_pred, **self._fn_kwargs)
    File ""/usr/local/lib/python3.7/dist-packages/keras/losses.py"", line 1790, in categorical_crossentropy
        y_true, y_pred, from_logits=from_logits, axis=axis)
    File ""/usr/local/lib/python3.7/dist-packages/keras/backend.py"", line 5083, in categorical_crossentropy
        target.shape.assert_is_compatible_with(output.shape)

    ValueError: Shapes (None, 21) and (None, 1, 1, 21) are incompatible
```
</details>"
56576,Why does object-detection script require audio software?,"This is from lite/examples/object_detection/raspberry_pi

Is something mixed up with the efficientdet_lite model? It gives me a fatal error for not finding PortAudio library! But i wanted to do object detection with the camera! Earlier, in the setup phase, it seemed satisfied with my installed software.

steve@raspberrypi:~/examples/tensorflow_examples/examples/lite/examples/object_detection/raspberry_pi $ python3 detect.py \
  --model efficientdet_lite0.tflite
Traceback (most recent call last):
  File ""/home/steve/examples/tensorflow_examples/examples/lite/examples/object_detection/raspberry_pi/detect.py"", line 20, in <module>
    from tflite_support.task import core
  File ""/home/steve/.local/lib/python3.9/site-packages/tflite_support/__init__.py"", line 53, in <module>
    from tflite_support import task
  File ""/home/steve/.local/lib/python3.9/site-packages/tflite_support/task/__init__.py"", line 28, in <module>
    from . import audio
  File ""/home/steve/.local/lib/python3.9/site-packages/tflite_support/task/audio/__init__.py"", line 20, in <module>
    from tensorflow_lite_support.python.task.audio import audio_classifier
  File ""/home/steve/.local/lib/python3.9/site-packages/tensorflow_lite_support/python/task/audio/audio_classifier.py"", line 18, in <module>
    from tensorflow_lite_support.python.task.audio.core import audio_record
  File ""/home/steve/.local/lib/python3.9/site-packages/tensorflow_lite_support/python/task/audio/core/audio_record.py"", line 17, in <module>
    import sounddevice as sd
  File ""/home/steve/.local/lib/python3.9/site-packages/sounddevice.py"", line 71, in <module>
    raise OSError('PortAudio library not found')
OSError: PortAudio library not found
"
56575,Add `cdist` - an analog of `scipy.cdist` | `torch.cdist`,"# Feature Request

Source: binary
Tensorflow Version: 2.9
Custom Code: Yes

# Current Behavior

There is no equivalent built-in function of the following two in `TensorFlow`. I need to use this method on large size embedding vectors and require a efficient vectorized implementation.

```python
scipy.spatial.distance.cdist
torch.cdist
```

The same request has been asked several times but closed without any strong reason and no response further to a new commenter. So, kindly **DO NOT CLOSE** without further discussion. 

- https://github.com/tensorflow/tensorflow/issues/30659#
- https://github.com/tensorflow/tensorflow/issues/9745#
"
56574,"when using tflite gpu delegate C++ to inference model on mobile devices, it reports ""Failed to modify graph with delegate""  ","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf2.8

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

Galaxy Note 10/android12

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
when using built tensorflowlite\tensorflowlite_gpu_delegate\EGL\GLESv2(c++ android) to inference the model on mobile device, it reports ""Failed to modify graph with delegate"" . It seems that the delegate can't find gpu. However, I make sure there is gpu on my mobile device.
```


### Standalone code to reproduce the issue




### Relevant log output

```
""Failed to modify graph with delegate""
the log from main.cc/
""""""
    if (interpreter->ModifyGraphWithDelegate(delegate) != kTfLiteOk)
    {
        std::cerr << ""Failed to modify graph with delegate"" << std::endl;
        exit(0);
    }
""""""

"
56571,Auto-Applying XNNPACK Delegate,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf 2.5.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Moving from TFLite 2.4.1 to 2.5.0 and compiling with `TFLITE_ENABLE_XNNPACK` using the Cmake build, it seems that in the latter the tflite delegate is automatically applied to the model. If you attempt to apply the xnnpack delegate explicitly in your own usage of the API, you see the error indicating a delegate has already been applied to the graph, making it immutable if another delegate is applied. 

Is there a way to compile using CMake with xnnpack support such that the xnnpack delegate is not automatically applied starting with tf 2.5.0? I would like to explicitly apply the xnnpack delegate with my own options.


### Standalone code to reproduce the issue

```shell
TfLiteXNNPackDelegateOptions options =
        TfLiteXNNPackDelegateOptionsDefault();

    options.num_threads = 4;

    tflite::Interpreter::TfLiteDelegatePtr xnnpack_delegate(
        TfLiteXNNPackDelegateCreate(&options),
        [](TfLiteDelegate* xnnpack_delegate) {
          TfLiteXNNPackDelegateDelete(xnnpack_delegate);
        });

    // Instruct the Interpreter to use the xnnpack
    if (interpreter_->ModifyGraphWithDelegate(std::move(xnnpack_delegate)) !=
        kTfLiteOk) {
      return TRITONSERVER_ErrorNew(
          TRITONSERVER_ERROR_INTERNAL,
          (""failed to use xnnpack delegate for model "" + Name()).c_str());
    }
  }

  // Allocate memory for input and output tensors
  if (interpreter_->AllocateTensors() != kTfLiteOk) {
    return TRITONSERVER_ErrorNew(
        TRITONSERVER_ERROR_INTERNAL,
        (""TfLite interpreter failed to allocate tensor inputs for model "" +
         Name())
            .c_str());
  }
```


### Relevant log output

```shell
ERROR: ModifyGraphWithDelegate is disallowed when graph is immutable.
ERROR: Ignoring failed application of the default TensorFlow Lite delegate indexed at 0.
```
</details>"
56569,finished,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf2.8

### Custom Code

Yes

### OS Platform and Distribution

Linux 18.04 x86-64

### Mobile device

android 11

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
when I inference the model with tflite gpu delegate c++ on native pc, it works. Because, the opengl is installed on my pc, I link opengl lib(native) into my project. But now, I want build the android version with the android-version lib. Now I have built the libtensorflowlite.so & libtensorflowlite_delegate_gpu.so of android , but I don't know how to build opengl-android.I don't prepare to use the app to test my demo.I only want to build the c++ code into an executable file, and push it into android device to test.
```


### Standalone code to reproduce the issue

```shell
/
```


### Relevant log output

_No response_</details>"
56567,tf.scatter_nd doesn't support int32 updates when some indices are out of bounds,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf2.6

### Custom Code

Yes

### OS Platform and Distribution

win10

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When the type of updates is int32 and some of the indices are out of bound, tf.scatter_nd can't return the expected value.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

print(tf.scatter_nd(tf.constant([[0],[1],[2]]), tf.constant([10,11,12],dtype=tf.int32), [5]))
print(tf.scatter_nd(tf.constant([[-1],[1],[2]]), tf.constant([10,11,12],dtype=tf.float32), [5]))
print(tf.scatter_nd(tf.constant([[-1],[1],[2]]), tf.constant([10,11,12],dtype=tf.int32), [5]))
```


### Relevant log output

```shell
tf.Tensor([10 11 12  0  0], shape=(5,), dtype=int32)
tf.Tensor([ 0. 11. 12.  0.  0.], shape=(5,), dtype=float32)
2022-06-24 23:12:02.730401: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at scatter_nd_op.cc:130 : Invalid argument: indices[0] = [-1] does not index into shape [5]
print(tf.scatter_nd(tf.constant([[-1],[1],[2]]), tf.constant([10,11,12],dtype=tf.int32), [5]))
  File ""D:\miniconda\envs\tf2.6\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 9006, in scatter_nd
    return scatter_nd_eager_fallback(
  File ""D:\miniconda\envs\tf2.6\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 9049, in scatter_nd_eager_fallback   
    _result = _execute.execute(b""ScatterNd"", 1, inputs=_inputs_flat,
  File ""D:\miniconda\envs\tf2.6\lib\site-packages\tensorflow\python\eager\execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[0] = [-1] does not index into shape [5] [Op:ScatterNd]
```
</details>"
56565,Indexing a ragged tensor over a non-ragged index fails in tf.keras.Model,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9.1, nightly

### Custom Code

No

### OS Platform and Distribution

colab

### Mobile device

_No response_

### Python version

3.7

### Bazel version

n/a

### GCC/Compiler version

n/a

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

```shell
Indexing a ragged tensor over a non-ragged dimension throws an error inside a tf.keras.Model. I'm trying to slice a ragged tensor containing a known amount of features, i.e., with shape (batch_size, ragged_dim, known_feature_dim) by the last axis inside a Keras model, hopefully returning a (batch_size, ragged_dim) tensor of features.

This works in eager mode with a tf.ragged.constant, but a model (that works with a tf.constant) fails when passed the same ragged constant.
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1eSEdKidC1A4kw5EYeYimKrSEHkV6EndS?usp=sharing
```


### Relevant log output

```shell
ValueError: Exception encountered when calling layer ""tf.__operators__.getitem"" ""                 f""(type SlicingOpLambda).

TypeError: object of type 'RaggedTensor' has no len()


Call arguments received by layer ""tf.__operators__.getitem"" ""                 f""(type SlicingOpLambda):
   tensor=<tf.RaggedTensor [[[1, 2, 3],
  [4, 5, 6],
  [7, 8, 9]], [[2, 3, 4],
               [5, 6, 7]], [[3, 4, 5]]]>
   slice_spec=({'start': 'None', 'stop': 'None', 'step': 'None'}, {'start': 'None', 'stop': 'None', 'step': 'None'}, '0')
   var=None
```
</details>"
56564,how to close the profiler feature,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Source

binary

### Tensorflow Version

tf2.5

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
how to close profiler
```


### Standalone code to reproduce the issue

```shell
how to close profiler
```


### Relevant log output

```shell
how to close profiler
```
</details>"
56563,"`tf.reduce_mean` returns wrong results for certain 4D tensor sizes, reductions and distributions","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.8.2, 2.9.1

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04.5 LTS

### Mobile device

_No response_

### Python version

3.7 , 3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

for tensor of shape `(1, 256, 256, 256, 4)` generated with `tf.random.normal(size, mean=100, stddev=1)` `tf_reduce_mean(tensor, reduction_axes=[0, 1, 2, 3])` returns wrong results.


### Standalone code to reproduce the issue

```shell
mean_val = 100
std = 1

dimensions = (256,)
channels = (4,)

for d in dimensions:
    for ch in channels:    
        size = (1, d, d, d, ch)
        reduction_axes = [i for i in range(len(size) - 1)]

        tensor = tf.random.normal(size, mean=mean_val, stddev=std)                
        mean = tf.reduce_mean(tensor, reduction_axes)
        print(""shape:"", size, ""per-channel mean:"", mean.numpy())

        for chann_idx in range(ch):
            single_channel = tensor[..., chann_idx]
            mean = tf.reduce_mean(single_channel)
            print(""shape:"", single_channel.shape, ""channel_idx:"", chann_idx, ""mean:"", mean.numpy())
```
This code snippet shows that the results acquired with `reduce_mean` with `reduction_axes` are wrong. Showing that the right mean is calculated when manually iterating over axis slices.

If you replace the search space to e.g.:
```
dimensions = (64,128,256)
channels = (1,4,8)
```
you may notice some other strange behaviours. e.g. for dimension=128 the error happens for channels=4 but not for 1 or 8.

See https://colab.research.google.com/drive/1o4GdZj6ihpQNawj32N0ukYGX4ngviGRG?usp=sharing
```


### Relevant log output

```shell
shape: (1, 256, 256, 256, 4) per-channel mean: [117.75938  117.759384 117.759384 117.759865]
shape: (1, 256, 256, 256) channel_idx: 0 mean: 99.999664
shape: (1, 256, 256, 256) channel_idx: 1 mean: 100.000015
shape: (1, 256, 256, 256) channel_idx: 2 mean: 100.000305
shape: (1, 256, 256, 256) channel_idx: 3 mean: 99.999626
```
```
</details>"
56562,ImportError: ../lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.8

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 16.04

### Mobile device

_No response_

### Python version

3.8.5

### Bazel version

_No response_

### GCC/Compiler version

gcc (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Failed to load the native TensorFlow runtime.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/home/aimluser/miniconda3/envs/application_env/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py"", lin
e 60, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /home/aimluser/miniconda3/envs/application_env/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (requ
ired by /home/aimluser/miniconda3/envs/application_env/lib/python3.8/site-packages/tensorflow/python/_pywrap_tensorflow_interna
l.so)

During handling of the above exception, another exception occurred:

import tensorflow as tf
  File ""/home/aimluser/miniconda3/envs/application_env/lib/python3.8/site-packages/tensorflow/__init__.py"", line 37, in <module
>
    from tensorflow.python.tools import module_util as _module_util
  File ""/home/aimluser/miniconda3/envs/application_env/lib/python3.8/site-packages/tensorflow/python/__init__.py"", line 36, in 
<module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""/home/aimluser/miniconda3/envs/application_env/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py"", lin
e 75, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""/home/aimluser/miniconda3/envs/application_env/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py"", lin
e 60, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /home/aimluser/miniconda3/envs/application_env/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (requ
ired by /home/aimluser/miniconda3/envs/application_env/lib/python3.8/site-packages/tensorflow/python/_pywrap_tensorflow_interna
l.so)


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above t
his error message.
```
</details>"
56561,Issue with Tensorflow Binary and oneAPI while flask deployment,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

binary

### Tensorflow Version

tensorflow-cpu

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

NA

### GPU model and memory

NA

### Current Behaviour?

```shell
I created Flask App with tensorflow model and it is working fine in local machine. However while deploying, it is giving me following warning because of which build is successful but deployment failed. Error shows up with tensorflow and tensorflow-cpu both. I am not sure how to solve the same.
Please help.
```


### Standalone code to reproduce the issue

```shell
Issue happens not due to code but something else.
```


### Relevant log output

```shell
Jun 24 03:47:17 PM  ==> Starting service with 'gunicorn app:app'
Jun 24 03:47:54 PM  2022-06-24 10:17:54.733536: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
Jun 24 03:47:54 PM  To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
```
</details>"
56560,Compiled code executes wrong branch,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 18.04.6 LTS

### Mobile device

_No response_

### Python version

3.10.4

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When I compile my code with `tf.function` it tries to execute the wrong branch of an if-statement, leading to a crash.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf


def inner_dim(x):
    shape = tf.shape(x)
    if tf.size(shape) > 0:
        return shape[-1]
    else:
        return tf.ones((), dtype=shape.dtype)


compiled_inner_dim = tf.function(inner_dim)

small_tensor = tf.ones(())
large_tensor = tf.ones((2,))

print(""--------------------------------------------------"")
print(inner_dim(large_tensor))  # Works.
print(""--------------------------------------------------"")
print(inner_dim(small_tensor))  # Works.
print(""--------------------------------------------------"")
print(compiled_inner_dim(large_tensor))  # Works.
print(""--------------------------------------------------"")
print(compiled_inner_dim(small_tensor))  # slice index -1 of dimension 0 out of bounds.
print(""--------------------------------------------------"")
```


### Relevant log output

```shell
2022-06-24 09:54:35.277941: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2022-06-24 09:54:35.277971: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: jesper-desktop
2022-06-24 09:54:35.277980: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: jesper-desktop
2022-06-24 09:54:35.278030: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 495.29.5
2022-06-24 09:54:35.278050: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 495.29.5
2022-06-24 09:54:35.278057: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 495.29.5
2022-06-24 09:54:35.278256: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
--------------------------------------------------
tf.Tensor(2, shape=(), dtype=int32)
--------------------------------------------------
tf.Tensor(1, shape=(), dtype=int32)
--------------------------------------------------
tf.Tensor(2, shape=(), dtype=int32)
--------------------------------------------------
Traceback (most recent call last):
  File ""/home/jesper/src/GPflow/test_wtf.py"", line 23, in <module>
    print(compiled_inner_dim(small_tensor))
  File ""/home/jesper/src/GPflow/.venv/max310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/tmp/__autograph_generated_fileyqfznmym.py"", line 36, in tf__inner_dim
    ag__.if_stmt(ag__.converted_call(ag__.ld(tf).size, (ag__.ld(shape),), None, fscope) > 0, if_body, else_body, get_state, set_state, ('do_return', 'retval_'), 2)
  File ""/tmp/__autograph_generated_fileyqfznmym.py"", line 23, in if_body
    retval_ = ag__.ld(shape)[-1]
ValueError: in user code:

    File ""/home/jesper/src/GPflow/test_wtf.py"", line 7, in inner_dim  *
        return shape[-1]

    ValueError: slice index -1 of dimension 0 out of bounds. for '{{node cond/strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_INT32, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](cond/strided_slice/Shape, cond/strided_slice/stack, cond/strided_slice/stack_1, cond/strided_slice/stack_2)' with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <-1>, input[2] = <0>, input[3] = <1>.
```
</details>"
56559,how to solve 'TMP' environment variable when compile tensorflow source files,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf 2.6.0

### Custom Code

No

### OS Platform and Distribution

ubuntu 18.04

### Mobile device

_No response_

### Python version

3.6

### Bazel version

3.7.2

### GCC/Compiler version

7.5

### CUDA/cuDNN version

10.1 / 7.6.5

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
DEBUG: /home/wzy/.cache/bazel/_bazel_wzy/c9412f2eef7f926a6cf75da3bd81938f/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:10: 
Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default
INFO: Build options --action_env and --python_path have changed, discarding analysis cache.
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (443 packages loaded, 30534 targets configured).
INFO: Found 1 target...
WARNING: failed to create one or more convenience symlinks for prefix 'bazel-':
  cannot create symbolic link bazel-bin -> /home/wzy/.cache/bazel/_bazel_wzy/c9412f2eef7f926a6cf75da3bd81938f/execroot/org_tensorflow/bazel-out/k8-opt/bin:  /home/wzy/tensorflow/bazel-bin (File exists)
  cannot create symbolic link bazel-testlogs -> /home/wzy/.cache/bazel/_bazel_wzy/c9412f2eef7f926a6cf75da3bd81938f/execroot/org_tensorflow/bazel-out/k8-opt/testlogs:  /home/wzy/tensorflow/bazel-testlogs (File exists)
  cannot create symbolic link bazel-out -> /home/wzy/.cache/bazel/_bazel_wzy/c9412f2eef7f926a6cf75da3bd81938f/execroot/org_tensorflow/bazel-out:  /home/wzy/tensorflow/bazel-out (File exists)
  cannot create symbolic link bazel-tensorflow -> /home/wzy/.cache/bazel/_bazel_wzy/c9412f2eef7f926a6cf75da3bd81938f/execroot/org_tensorflow:  /home/wzy/tensorflow/bazel-tensorflow (File exists)
```


### Standalone code to reproduce the issue

```shell
DEBUG: /home/wzy/.cache/bazel/_bazel_wzy/c9412f2eef7f926a6cf75da3bd81938f/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:10: 
Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default
INFO: Build options --action_env and --python_path have changed, discarding analysis cache.
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (443 packages loaded, 30534 targets configured).
INFO: Found 1 target...
WARNING: failed to create one or more convenience symlinks for prefix 'bazel-':
  cannot create symbolic link bazel-bin -> /home/wzy/.cache/bazel/_bazel_wzy/c9412f2eef7f926a6cf75da3bd81938f/execroot/org_tensorflow/bazel-out/k8-opt/bin:  /home/wzy/tensorflow/bazel-bin (File exists)
  cannot create symbolic link bazel-testlogs -> /home/wzy/.cache/bazel/_bazel_wzy/c9412f2eef7f926a6cf75da3bd81938f/execroot/org_tensorflow/bazel-out/k8-opt/testlogs:  /home/wzy/tensorflow/bazel-testlogs (File exists)
  cannot create symbolic link bazel-out -> /home/wzy/.cache/bazel/_bazel_wzy/c9412f2eef7f926a6cf75da3bd81938f/execroot/org_tensorflow/bazel-out:  /home/wzy/tensorflow/bazel-out (File exists)
  cannot create symbolic link bazel-tensorflow -> /home/wzy/.cache/bazel/_bazel_wzy/c9412f2eef7f926a6cf75da3bd81938f/execroot/org_tensorflow:  /home/wzy/tensorflow/bazel-tensorflow (File exists)
```


### Relevant log output

```shell
DEBUG: /home/wzy/.cache/bazel/_bazel_wzy/c9412f2eef7f926a6cf75da3bd81938f/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:10: 
Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default
INFO: Build options --action_env and --python_path have changed, discarding analysis cache.
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (443 packages loaded, 30534 targets configured).
INFO: Found 1 target...
WARNING: failed to create one or more convenience symlinks for prefix 'bazel-':
  cannot create symbolic link bazel-bin -> /home/wzy/.cache/bazel/_bazel_wzy/c9412f2eef7f926a6cf75da3bd81938f/execroot/org_tensorflow/bazel-out/k8-opt/bin:  /home/wzy/tensorflow/bazel-bin (File exists)
  cannot create symbolic link bazel-testlogs -> /home/wzy/.cache/bazel/_bazel_wzy/c9412f2eef7f926a6cf75da3bd81938f/execroot/org_tensorflow/bazel-out/k8-opt/testlogs:  /home/wzy/tensorflow/bazel-testlogs (File exists)
  cannot create symbolic link bazel-out -> /home/wzy/.cache/bazel/_bazel_wzy/c9412f2eef7f926a6cf75da3bd81938f/execroot/org_tensorflow/bazel-out:  /home/wzy/tensorflow/bazel-out (File exists)
  cannot create symbolic link bazel-tensorflow -> /home/wzy/.cache/bazel/_bazel_wzy/c9412f2eef7f926a6cf75da3bd81938f/execroot/org_tensorflow:  /home/wzy/tensorflow/bazel-tensorflow (File exists)
```
</details>"
56558,some questions about tflite with gpu delegates,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf2.8

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
""interpreter->ModifyGraphWithDelegate(delegate) != kTfLiteOk ""
when will this situation happen?
is it when mydevice does not contain gpu?
```


### Standalone code to reproduce the issue

```shell
/
```


### Relevant log output

_No response_</details>"
56556,BatchNormalization with virtual_batch_size breaks load_model(),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.4.1, 2.8.2, nightly

### Custom Code

No

### OS Platform and Distribution

linux (colab cpu)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Passing `virtual_batch_size=int` to BatchNormalization performs ""Ghost Batch Normalization"", which creates virtual sub-batches which are each normalized separately.

Loading a keras model with such a layer fails with a ValueError.

In BatchNormalizationBase, the `axis` attrib is incremented, e.g. from [3] to [4], I think this happens here: https://github.com/keras-team/keras/blob/07e13740fd181fc3ddec7d9a594d8a08666645f6/keras/layers/normalization/batch_normalization.py#L382

Then, when self.build() is called upon load_model(), `axis` is checked against input rank, which fails. This check has moved in the current version to `validate_axis()` in keras/utils/tf_utils.py:

https://github.com/keras-team/keras/blob/07e13740fd181fc3ddec7d9a594d8a08666645f6/keras/utils/tf_utils.py#L228

It seems there is some confusion in the code, whether or not `axis` is allowed to be larger than rank - 1.
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1QN7EPzr0MaYnHAdzVc2ucaMO544aQYvY?usp=sharing
```


### Relevant log output

```shell
/usr/local/lib/python3.7/dist-packages/keras/layers/normalization/batch_normalization.py in build(self, input_shape)
    311       if x < 0 or x >= ndims:
    312         raise ValueError(
--> 313             f'Invalid axis. Expected 0 <= axis < inputs.rank (with '
    314             f'inputs.rank={ndims}). Received: layer.axis={self.axis}')
    315     if len(self.axis) != len(set(self.axis)):

ValueError: Invalid axis. Expected 0 <= axis < inputs.rank (with inputs.rank=4). Received: layer.axis=ListWrapper([4])
```
</details>"
56555,Different result of cross entropy loss between result from model as function and using predict function,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.8.2

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Firstly, you get a prediction using model as a function, call this predition A; and then you use predict function of the model and get the same seemingly same prediction, call it B.

If you use CategoricalCrossentropy to calculate loss with same label, the loss are different between them.
```


### Standalone code to reproduce the issue

```shell
class NLPModel(keras.Model):
  def __init__(self, **kwargs):
    super().__init__(**kwargs)
    self.hidden_layer1 = tf.keras.layers.GRU(400)
    self.outputs = tf.keras.layers.Dense(3, activation=""sigmoid"")
  def call(self, inputs):
    outputs = self.hidden_layer1(inputs)
    outputs = self.outputs(outputs)
    return outputs
model = NLPModel()
label = tf.constant([[1.,0.,0.]])
prediction = model(tf.constant([[[0.4, 0.5, 0.54,0.4, 0.5, 0.54,0.4, 0.5, 0.54,0.9],
                                  [0.4, 0.5, 0.54,0.4, 0.5, 0.54,0.4, 0.5, 0.54,0.9]]]))
cce = tf.keras.losses.CategoricalCrossentropy()
loss = cce(label, prediction)
def custom_cce(label, prediction):
  return tf.reduce_mean(tf.math.log(tf.matmul(label, tf.transpose(prediction)) / tf.reduce_sum(prediction)))
print(""before numpy"")
print(prediction)
print(loss)
print(""before numpy"")
print(""after custom_cce"")
print(custom_cce(label, prediction))
print(""after custom_cce"")
prediction = prediction.numpy()
loss = cce(label, prediction)
print(""after numpy"")
print(prediction)
print(loss)
print(""after numpy"")
```


### Relevant log output

```shell
before numpy
tf.Tensor([[0.5021686  0.48505145 0.5026401 ]], shape=(1, 3), dtype=float32)
tf.Tensor(1.0769439, shape=(), dtype=float32)
before numpy
after custom_cce
tf.Tensor(-1.0875016, shape=(), dtype=float32)
after custom_cce
after numpy
[[0.5021686  0.48505145 0.5026401 ]]
tf.Tensor(1.0875016, shape=(), dtype=float32)
after numpy
```
</details>"
56554,Issue with building Tensorflow with NCCL from source,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04.4 LTS (Focal Fossa)

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

5.1.1

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

Cuda 11.6 / cuDNN 8.4.1

### GPU model and memory

Tesla P100-PCIE-12GB

### Current Behaviour?

```shell
I am trying to build TensorFlow from source on my ubuntu machine. I installed bazel and configured everything properly for CUDA. When I run 


bazel build -c opt //tensorflow:libtensorflow_cc
```
I get the following error: 

```
INFO: Build option --define has changed, discarding analysis cache.
ERROR: /data/softs/tensorflow/tensorflow/BUILD:1153:21: in cc_shared_library rule //tensorflow:libtensorflow_cc.so.2.10.0: 
Traceback (most recent call last):
        File ""/virtual_builtins_bzl/common/cc/experimental_cc_shared_library.bzl"", line 416, column 105, in _cc_shared_library_impl
        File ""/virtual_builtins_bzl/common/cc/experimental_cc_shared_library.bzl"", line 340, column 37, in _filter_inputs
        File ""/virtual_builtins_bzl/common/cc/experimental_cc_shared_library.bzl"", line 360, column 9, in _throw_error_if_unaccounted_libs
Error in fail: The following libraries cannot be linked either statically or dynamically:
@nccl_archive//:nccl
@nccl_archive//:device
@nccl_archive//:net
@nccl_archive//:include_hdrs
@nccl_archive//:src_hdrs
To ignore which libraries get linked statically for now, add the following to 'static_deps':
        ""@nccl_archive//:__subpackages__"",
ERROR: /data/softs/tensorflow/tensorflow/BUILD:1153:21: Analysis of target '//tensorflow:libtensorflow_cc.so.2.10.0' failed
ERROR: Analysis of target '//tensorflow:tensorflow_cc' failed; build aborted: 
INFO: Elapsed time: 6.706s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded, 28476 targets configured)

```
but when I run with --config=nonccl, everything works fine. Could some one tell me why I getting this error for NCCL and how to fix. 

I also installed nccl-local-repo-ubuntu2004-2.12.12-cuda11.6_1.0-1_amd64 on my machine successfully but still I am not able to build with NCCL.
```


### Standalone code to reproduce the issue

```shell
1) Download Tensorflow and configure it with bazel for CUDA 
2) Run bazel build -c opt //tensorflow:tensorflow_cc
```


### Relevant log output

```shell
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=161
INFO: Reading rc options for 'build' from /data/softs/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /data/softs/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false
INFO: Reading rc options for 'build' from /data/softs/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3.8/dist-packages --python_path=/usr/bin/python3 --config=tensorrt --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.6 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0 --action_env LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-9 --config=cuda
INFO: Reading rc options for 'build' from /data/softs/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /data/softs/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /data/softs/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:tensorrt in file /data/softs/tensorflow/.bazelrc: --repo_env TF_NEED_TENSORRT=1
INFO: Found applicable config definition build:cuda in file /data/softs/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:linux in file /data/softs/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /data/softs/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Build option --define has changed, discarding analysis cache.
ERROR: /data/softs/tensorflow/tensorflow/BUILD:1153:21: in cc_shared_library rule //tensorflow:libtensorflow_cc.so.2.10.0: 
Traceback (most recent call last):
        File ""/virtual_builtins_bzl/common/cc/experimental_cc_shared_library.bzl"", line 416, column 105, in _cc_shared_library_impl
        File ""/virtual_builtins_bzl/common/cc/experimental_cc_shared_library.bzl"", line 340, column 37, in _filter_inputs
        File ""/virtual_builtins_bzl/common/cc/experimental_cc_shared_library.bzl"", line 360, column 9, in _throw_error_if_unaccounted_libs
Error in fail: The following libraries cannot be linked either statically or dynamically:
@nccl_archive//:nccl
@nccl_archive//:device
@nccl_archive//:net
@nccl_archive//:include_hdrs
@nccl_archive//:src_hdrs
To ignore which libraries get linked statically for now, add the following to 'static_deps':
        ""@nccl_archive//:__subpackages__"",
ERROR: /data/softs/tensorflow/tensorflow/BUILD:1153:21: Analysis of target '//tensorflow:libtensorflow_cc.so.2.10.0' failed
ERROR: Analysis of target '//tensorflow:tensorflow_cc' failed; build aborted: 
INFO: Elapsed time: 6.706s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded, 28476 targets configured)
```
</details>"
56553,"CSV generated by the code in the ""Human Pose Classification with MoveNet and TensorFlow Lite"" example inconsistent","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Windows

### Mobile device

_No response_

### Python version

3.8.0

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


Running [part 1](https://www.tensorflow.org/lite/tutorials/pose_classification#part_1_preprocess_the_input_images) here (after running the preparation code) generates CSVs different from the CSVs in the section [here](https://www.tensorflow.org/lite/tutorials/pose_classification#optional_download_the_preprocessed_dataset_if_you_didnt_run_part_1). 

The CSVs should be the same, but the CSVs generated by running the code has discrete keypoint coordinates while the CSVs linked in the above section have decimal keypoint coordinates. I believe that the code in the example is missing some scaling code. In the `def process(self, per_pose_class_limit=None, detection_threshold=0.1):` function, under the `# Get landmarks and scale it to the same size as the input image` comment, I don't think the landmarks are actually scaled:
``` 
pose_landmarks = np.array(
    [[keypoint.coordinate.x, keypoint.coordinate.y, keypoint.score]
        for keypoint in person.keypoints],
    type=np.float32)
```

Yoga CSVs generated by running the code: 
[yoga_train_data.csv](https://github.com/tensorflow/tensorflow/files/8969168/train_data.csv)
[yoga_test_data.csv](https://github.com/tensorflow/tensorflow/files/8969170/test_data.csv)

Yoga CSVs linked in the code for skipping part 1 which claims to be identical:
[yoga_train_data.csv](https://github.com/tensorflow/tensorflow/files/8969172/yoga_train_data.csv)
[yoga_test_data.csv](https://github.com/tensorflow/tensorflow/files/8969171/yoga_test_data.csv)




### Standalone code to reproduce the issue


[https://www.tensorflow.org/lite/tutorials/pose_classification](https://www.tensorflow.org/lite/tutorials/pose_classification)


### Relevant log output

_No response_</details>"
56549,Duplicate logging messages after calling `model.save()`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.3

### GPU model and memory

_No response_

### Current Behaviour?
When using a logger from python's `logging` module, messages are duplicated after a model is saved.

For example:

```python

import logging
import tensorflow as tf

logger = logging.getLogger('test')
logger.setLevel(logging.INFO)
logger.addHandler(logging.StreamHandler())
logger.info(""Message (1) not duplicated"")

model = tf.keras.applications.Xception()
logger.info(""Message (2) not duplicated"")

model.save('test')
logger.info(""Message is duplicated"")
```

Will yield (some output not shown):

```
Message (1) not duplicated
Message (2) not duplicated
WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 40). These functions will not be directly callable after loading.
Message is duplicated
INFO:test:Message is duplicated
```

Logging is not duplicated if `logging.basicConfig()` is called before model saving:

```python
...
logger.setLevel(logging.INFO)
logger.basicConfig()
logger.info(""Message (1) not duplicated"")
...
```

This behavior is only observed in Tensorflow 2.9 and does not occur with version <2.8.

I suspect the issue is in `tensorflow.python.saved_model.save:268` where a module-level `logging.warning()` function is called, which may be unintentionally adding a stream handler to the root logger if none is configured.

### Standalone code to reproduce the issue

```python
import logging
import tensorflow as tf

logger = logging.getLogger('test')
logger.setLevel(logging.INFO)
logger.addHandler(logging.StreamHandler())
logger.info(""Message (1) not duplicated"")

model = tf.keras.applications.Xception()
logger.info(""Message (2) not duplicated"")

model.save('test')
logger.info(""Message is duplicated"")
```


### Relevant log output

```shell
Message (1) not duplicated
Message (2) not duplicated
WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 40). These functions will not be directly callable after loading.
Message is duplicated
INFO:test:Message is duplicated
```
</details>"
56548,iOS uses 100%+ CPU to run tflite,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
MAC OS 12.2
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (or github SHA if from source):
2.9.1

**Provide the text output from tflite_convert**

```
tflite_convert --output_file exported_tflite_0000004/2222od_model.tflite --saved_model_dir exported_tflite_0000004/saved_model
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.

![2022-06-23 21 02 43](https://user-images.githubusercontent.com/7891692/175305025-92f0bbbb-dce1-40a2-96e3-540519192665.png)
is this normal?
"
56547,Reproducible sparse-dense matrix multiplication,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

v2.9.0-18-gd8ce9f9c301 2.9.1

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
There is currently no deterministic sparse-dense matrix multiplication implementation on the GPU. I would like to have one.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.keras.utils.set_random_seed(3948)
tf.config.experimental.enable_op_determinism()

m = tf.SparseTensor(
    indices=[[0, 0], [0, 1]],
    values=np.array([1, 2], dtype=np.float32),
    dense_shape=(3, 2),
)
v = tf.constant([[1], [1]], dtype=tf.float32)

tf.sparse.sparse_dense_matmul(m, v)
```


### Relevant log output

```shell
UnimplementedError: A deterministic GPU implementation of SparseTensorDenseMatmulOp is not currently available. [Op:SparseTensorDenseMatMul]
```
</details>"
56546,Available quantization options in TFL/ TFLM,"Hello,

First, I am not sure where to post this.

Currently, I'm writing my Masters Thesis, where I'm analysing TFLM inference on a Cortex M4 architecture.
Part of it is a comparison between the different available quantization options.

However, in TFL (version 2.8.0) I could not manage to quantize differently than int8, float16 in post training quantization and int8 in QAT.

When I try to run the models on my device using TFLM (source from 2022-05-13), the float16 quantized model is also not running any more (AllocateTensors() failed).
I've seen in the c_api_types.h files that there are technically many tensor types (TfLiteType) like int8, int16, int32, float16, float32 available.

The question is, if I am doing something wrong, or it is simply not supported (jet)?

Thank you in advance,
Marcel

"
56545,Tensorflow lite GPU Delegate error with JNI(C++) on android 10(API 29),"**System information**
- Android Device information (use `adb shell getprop ro.build.fingerprint`
  if possible): Galaxy S10 `(samsung/beyond1lteks/beyond1:10/QP1A.190711.020/G973NKSU4CTE9:user/release-keys)`
- TensorFlow version : 2.9.0
- Bazel version (if compiling from source): 5.2.0

**Describe the problem**
I tried to segmentation inference with tflite model on mobile.
I have placed the [tflite model](https://tfhub.dev/google/lite-model/seefood/segmenter/mobile_food_segmenter_V1/1) in `main_app/src/main/assets` and run my code.
I succeeded in running tflite model with cpu, but I has encountered error when running with GPU delegate.

**Source code / logs**
In Kotlin,
```
# ViewModel
private val context: Context
        get() = getApplication<Application>().applicationContext

private fun init() {
        if (!isInitialized) isInitialized = true else return
        segApi.inferenceNN(context.assets)
```
```
# segApi 
class segApi {

    companion object {
        init {
            System.loadLibrary(""cpp_jni"")
        }
    }

    fun inferenceNN(assetManager: AssetManager) {
        inferenceNNInternal(assetManager)
    }

}

private external fun inferenceNNInternal(assetManager: AssetManager)
```

In C++ JNI
```
JNIEXPORT void JNICALL Java_com_a_b_c_segApi_inferenceNNInternal(JNIEnv *env, jobject thiz, jobject asset_manager) {
    AAssetManager* mgr = AAssetManager_fromJava(env, asset_manager);
    AAssetDir* assetDir = AAssetManager_openDir(mgr, """");
    const char* filename = nullptr;
    while ((filename = AAssetDir_getNextFileName(assetDir)) != nullptr) {
        AAsset* asset = AAssetManager_open(mgr, filename, AASSET_MODE_STREAMING);
        size_t size = AAsset_getLength(asset);
        std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromBuffer((const char*)AAsset_getBuffer(asset), size);
        tflite::ops::builtin::BuiltinOpResolver op_resolver;
        tflite::InterpreterBuilder interpreter_builder(*model, op_resolver);

        auto* delegate = TfLiteGpuDelegateV2Create(/*default options=*/nullptr);
        interpreter_builder.AddDelegate(delegate);

        std::unique_ptr<tflite::Interpreter> interpreter;
        if (interpreter_builder(&interpreter) != kTfLiteOk) {
            __android_log_print(ANDROID_LOG_INFO, ""tag"", ""GPU fail"");
            return;
        }

        cv::Mat image(512, 512, CV_8UC3);
        cv::randu(image, cv::Scalar(1), cv::Scalar(255));

        if (interpreter->AllocateTensors() != kTfLiteOk) {
            __android_log_print(ANDROID_LOG_INFO, ""tag"", ""Allocate tensors fail"");
            return;
        }

        memcpy(interpreter->typed_input_tensor<unsigned char>(0), image.data, image.total() * image.elemSize());

        // Run inference
        std::chrono::steady_clock::time_point start, end;
        start = std::chrono::steady_clock::now();
        interpreter->Invoke();
        end = std::chrono::steady_clock::now();
        auto inference_time = std::chrono::duration_cast<std::chrono::milliseconds>(end - start).count();
        __android_log_print(ANDROID_LOG_INFO, ""tag"", ""inference: %s"", std::to_string(inference_time).c_str());

        float* output = interpreter->typed_output_tensor<float>(0);
        __android_log_print(ANDROID_LOG_INFO, ""tag"", ""output is: %.3f"", *output);

        TfLiteGpuDelegateV2Delete(delegate);

        AAsset_close(asset);
    }
    AAssetDir_close(assetDir);
}
```



Logcat
```
2022-06-23 15:40:15.405 23790-23790/com.a.b.c I/tflite: Created TensorFlow Lite delegate for GPU.
2022-06-23 15:40:15.405 23790-23790/com.a.b.c I/tflite: Initialized TensorFlow Lite runtime.
2022-06-23 15:40:15.411 23790-23790/com.a.b.c E/tflite: Following operations are not supported by GPU delegate:
    CAST: Not supported Cast case. Input type: UINT8 and output type: FLOAT32
    94 operations will run on the GPU, and the remaining 2 operations will run on the CPU.
2022-06-23 15:40:15.411 23790-23790/com.a.b.c I/tflite: Replacing 94 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 2 partitions.
2022-06-23 15:40:16.031 23790-23790/com.a.b.c E/tflite: Unrecognized Read selector
2022-06-23 15:40:16.031 23790-23790/com.a.b.c E/tflite: Falling back to OpenGL
2022-06-23 15:40:16.084 23790-23790/com.a.b.c E/libEGL: call to OpenGL ES API with no current context (logged once per thread)
2022-06-23 15:40:16.110 23790-23790/com.a.b.c E/tflite: TfLiteGpuDelegate Init: Batch size mismatch, expected 512 but got 1
2022-06-23 15:40:16.112 23790-23790/com.a.b.c I/tflite: Created 0 GPU delegate kernels.
2022-06-23 15:40:16.112 23790-23790/com.a.b.c E/tflite: TfLiteGpuDelegate Prepare: delegate is not initialized
2022-06-23 15:40:16.112 23790-23790/com.a.b.c E/tflite: Node number 96 (TfLiteGpuDelegateV2) failed to prepare.
2022-06-23 15:40:16.112 23790-23790/com.a.b.c E/tflite: Restored original execution plan after delegate application failure.
2022-06-23 15:40:16.112 23790-23790/com.a.b.c I/tag: GPU fail

```

Dependcy & Asset
- [tflite model](https://tfhub.dev/google/lite-model/seefood/segmenter/mobile_food_segmenter_V1/1)
"
56544,"ValueError: `validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [<class 'int'>]","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tensorflow version 2.4.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.0

### Mobile device

_No response_

### Python version

python 3.7.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
thesre should be no error because  x_train and y_train both are <class 'numpy.ndarray'>
<class 'numpy.ndarray'>
```


### Standalone code to reproduce the issue

```shell
rint('Build model...')
    # posEmbeddings = np.identity(len(pos2Idx)+3, dtype='float32') 
    # print(""posEmbeddings"")
    # print(posEmbeddings)
    # print(""posEmbeddings shape"")
    # print(posEmbeddings.shape)
    # #print(type(pos_tag_train))
    # word embedding layersplit_data_train_test(X_word, pos_tag, label) 
    print(""shape of train data"")
    print(X_word_train.shape)
    print(""Word input layer"")
    word_in = tf.keras.Input(shape=(max_len,), name='words_input')
    print(""shape of word_in"")
    print(word_in.shape)
    print(""word_in"")
    print(word_in)
    #word_in_1 =tf.keras.layers.Dense(8, activation='relu')(word_in)
    print(""word embedding layer"")
    emb_word = PretrainedwordembedLayer(word_in)
    print(""emd_word"")
    print( emb_word)
    
    # input and embeddings for characters
    print(""Character input layer"")
    char_in = tf.keras.Input(shape=(max_len, max_len_char,),name='char_input')
    print(""character embedding layer"")
    emb_char = TimeDistributed(Embedding(input_dim=n_chars + 2, output_dim=20,input_length=max_len_char, mask_zero=True))(char_in)
    print(""character embedding"")
    print(emb_char.shape)
    # character LSTM to get word encodings by characters
    print(""character LSTM to get word encodings by characters"")
    char_enc = TimeDistributed(LSTM(units=40, return_sequences=False,recurrent_dropout=0))(emb_char)
    print(""char_enc"")
    print(char_enc.shape)
   
    # main LSTM
    print(""main LSTM"")
    x = keras.layers.concatenate([emb_word, char_enc])
    x = SpatialDropout1D(0.3)(x)
    main_lstm = Bidirectional(LSTM(units=200, return_sequences=True, recurrent_dropout=0))(x)
    out = TimeDistributed(Dense(n_tags+1, activation=""softmax""))(main_lstm)
    #crf = CRF(n_tags+1)
    #out1=crf(out)
    model =  tf.compat.v1.keras.Model([word_in, char_in], out)    
    
    #Train the model
    print(""Train the model"")
    model.compile(optimizer=""adam"", loss=""sparse_categorical_crossentropy"", metrics=['accuracy'])
    model.summary()
    

    y_train=np.array(y_train)
    print(y_train[1])
    X_word_train=np.array(X_word_train)
    X_char_train=np.array(X_char_train)
    print(type(X_word_train))
    print(type(X_char_train))


    print("" y_train shape"")
    print(y_train.shape)
    history = model.fit([X_word_train,X_char_train.reshape((len(X_char_train),max_len,max_len_char)), max_len], y_train.reshape(len(y_train),max_len,1), batch_size=5, epochs=50, validation_split=0.2, verbose=1)
    
    plt.show()
    lstm_y_pred = model.predict([np.array(X_word_test),np.array(X_char_test).reshape((len(X_char_test),max_len,max_len_char))])
    print(""LSTM predicted"")
```


### Relevant log output

```shell
<class 'numpy.ndarray'>
<class 'numpy.ndarray'>
 y_train shape
(2745, 100)
Traceback (most recent call last):
  File ""MDLSTM7withoutpos.py"", line 340, in <module>
    model_build(input_file)
  File ""MDLSTM7withoutpos.py"", line 259, in model_build
    history = model.fit([X_word_train,X_char_train.reshape((len(X_char_train),max_len,max_len_char)), max_len], y_train.reshape(len(y_train),max_len,1), batch_size=5, epochs=50, validation_split=0.2, verbose=1, validation_steps=1)
  File ""/home/dr/anaconda3/envs/MENT/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 1041, in fit
    (x, y, sample_weight), validation_split=validation_split))
  File ""/home/dr/anaconda3/envs/MENT/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 1359, in train_validation_split
    ""arrays, found following types in the input: {}"".format(unsplitable))
ValueError: `validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [<class 'int'>]
```
</details>"
56543,"EmitIRForReduction fails with ""Check failed: !reductions.empty()...""","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


When running evaluate on a modified HloModule loaded directly from text (i.e. without going through the OptimizeHloModule pipeline), the evaluation fails at the EmitIRForReduction step (in ir_emitted_unnested.cc), with the error message:

tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc:5025] Check failed: !reductions.empty()  expect at least one reduce instructions.

I managed to reproduce it with a small hlo_module example (see hlo_module text below), and figured out that it was due to an issue in the GroupDisjointReductions function:

```
  std::unique_ptr<HloReachabilityMap> reachability_map =
      HloReachabilityMap::Build(fused_computation);
  for (HloInstruction* instr : fused_computation->instructions()) {
    std::vector<HloInstruction*> reached_output_ids;
    for (HloInstruction* output : roots) {
      if (HloOpcode::kReduce == output->opcode() &&
          (IsBroadcastedConstantOrScalar(*instr))) {
        // Do not group output reduce instructions through broadcasted
        // constants or scalars, as the recomputation should be acceptable.
        VLOG(3) << ""Skip broadcasted constant or scalar "" << instr->ToString();
        continue;
      }
      // Now group output instructions if they have common predecessors.
      if (reachability_map->IsReachable(instr, output)) {
        VLOG(3) << ""Reaching "" << output->ToString() << "" from ""
                << instr->ToString();
        reached_output_ids.push_back(output);
      }
    }
    for (size_t j = 1; j < reached_output_ids.size(); ++j) {
      disjoint_sets[reached_output_ids[0]].Merge(
          &disjoint_sets[reached_output_ids[j]]);
    }
  }

  // Place output instructions in the same set into the same group.
  HloInstructionMap<std::vector<HloInstruction*>> groups;
  for (HloInstruction* root : roots) {
    groups[disjoint_sets[root].Get()].push_back(root);
  }
```
Specifically focusing on the comment ""Do not group output reduce instructions through broadcasted constants or scalars, as the recomputation should be acceptable."". This causes broadcasted constants/scalars to be ignored as predecessors when tracing back from output reduce instructions.

The problem comes when the instruction that shares only broadcasted constant/scalars is itself a root of this computation. This causes that instruction to be ignored for grouping with every other instruction, resulting in a reduction group with just that single output instruction (does not have to be a broadcast instruction, e.g. an output add(constant1, constant2) would also show the same error). This causes the above error since that reduction group has no reduce instruction.

One patch that I've tested that works is to replace the bottom for-loop above with:
```
  // Place output instructions in the same set into the same group.
  HloInstructionMap<std::vector<HloInstruction*>> groups;
  for (HloInstruction* root_instr : roots) {
    if (HloOpcode::kReduce != root_instr->opcode() && IsBroadcastedConstantOrScalar(*root_instr)) {

      // We need to maintain the seen list so we don't add to the group twice
      HloInstructionMap<bool> seen;
      for (HloInstruction* output : roots) {
        if (root_instr->unique_id() != output->unique_id()){
          if (reachability_map->IsReachable(root_instr, output)) {
            HloInstruction* disjoint_set_val = disjoint_sets[output].Get();
            // We haven't seen this disjoint representive val yet
            if (seen.count(disjoint_set_val) == 0) {
              groups[disjoint_set_val].push_back(root_instr);
              seen[disjoint_set_val] = true;
            }
          }
        }
      }
    }
    else {
      groups[disjoint_sets[root_instr].Get()].push_back(root_instr);
    }
  }
```
This ensures that any of these ""isolated"" output instructions get grouped with at least one reduce instruction.



### Standalone code to reproduce the issue

Just load the below HloModule from string and try to Compile and Execute it directly without other changes.
```shell
HloModule input_fused_computation_reduce

%scalar_add_computation (scalar_lhs: f32[], scalar_rhs: f32[]) -> f32[] {
  %scalar_lhs = f32[] parameter(0)
  %scalar_rhs = f32[] parameter(1)
  ROOT %add.1 = f32[] add(f32[] %scalar_lhs, f32[] %scalar_rhs)
}

%fused_reduce.1 () -> (f32[200], f32[200,32]) {
  %constant.0 = f32[] constant(0)
  %broadcast.0 = f32[200,32]{1,0} broadcast(f32[] %constant.0), dimensions={}
  %reduce.0 = f32[200]{0} reduce(f32[200,32]{1,0} %broadcast.0, f32[] %constant.0), dimensions={1}, to_apply=%scalar_add_computation
  ROOT %tuple.0 = (f32[200]{0}, f32[200,32]{1,0}) tuple(f32[200]{0} %reduce.0, f32[200,32]{1,0} %broadcast.0)
}

ENTRY %main.0 (Arg_0.1: f32[200,32]) -> (f32[200], f32[200,32]) {
  %Arg_0.1 = f32[200,32]{1,0} parameter(0)
  ROOT %input_fusion_reduce.9.clone3 = (f32[200]{0}, f32[200,32]{1,0}) fusion(), kind=kInput, calls=%fused_reduce.1
}
```


### Relevant log output

_No response_</details>"
56542,HloReachabilityMap does not update the BitVectors properly when there is a cycle in the HloComputation,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

In the HloReachabilityMap, the BitVectors are updated in topological order starting from the root node. However, if there is a cycle in the graph, e.g. A->B->C->A, with A being closer to the root, the following occurs:

1) A gets updated (OR-ed) with BitVector of C
2) At this point C's BitVector, has not been initialized, so this operation does nothing.

Basically, if there's a cycle whichever instruction in the cycle is processed earlier will not be updated with the actual reachability BitVectors of the later instructions.

Since the HloReachabilityMap is used in ""InstructionFusion::MultiOutputFusionCreatesCycle"" to detect cycles, so I believe it will no work properly in this use case.

A trivial fix would be to just perform the BitVector update twice in HloReachabilityMap::Build, i.e. just repeat the following code twice:

```
  for (const HloInstruction* hlo : all) {
    inputs.clear();
    add_dependencies(hlo);
    switch (hlo->opcode()) {
      case HloOpcode::kRecvDone: {
        auto it = channel_group.find(*hlo->channel_id());
        if (it != channel_group.end()) {
          for (HloInstruction* channel : it->second) {
            if (channel->opcode() == HloOpcode::kSend) {
              add_input(channel);
            }
          }
        }
        break;
      }
      case HloOpcode::kAllReduce:
      case HloOpcode::kReduceScatter: {
        auto channel_id = hlo->channel_id();
        if (channel_id) {
          auto it = channel_group.find(channel_id.value());
          if (it != channel_group.end()) {
            for (HloInstruction* all_reduce : it->second) {
              add_dependencies(all_reduce);
            }
          }
        }
        break;
      }
      default:
        break;
    }

    result->FastSetReachabilityToUnion(inputs, hlo);
  }
```
By running it twice, we can ensure that e.g. in the second loop when we load A's BitMap with C's, C's BitMap already denotes that it's reachable from A, B and C (itself)

### Standalone code to reproduce the issue

```shell
TEST(CycleDetectionTestBase, Basic) {
  Shape r0f32 = ShapeUtil::MakeShape(F32, {});
  auto builder = HloComputation::Builder(""CycleDetection"");
  auto constant1 = builder.AddInstruction(
      HloInstruction::CreateConstant(LiteralUtil::CreateR0<float>(2.0f)));
  auto add1 = builder.AddInstruction(HloInstruction::CreateBinary(
      r0f32, HloOpcode::kAdd, constant1, constant1));
  auto add2 = builder.AddInstruction(
      HloInstruction::CreateBinary(r0f32, HloOpcode::kAdd, add1, add1));
  auto add3 = builder.AddInstruction(
      HloInstruction::CreateBinary(r0f32, HloOpcode::kAdd, add2, add2));
  auto add4 = builder.AddInstruction(
      HloInstruction::CreateBinary(r0f32, HloOpcode::kAdd, add3, add3));
  // Create cycle
  add1->ReplaceOperandWith(0, add3);

  auto module = CreateNewVerifiedModule();
  auto computation =
      module->AddEntryComputation(builder.Build(/*root_instruction=*/add4));

  std::unique_ptr<HloReachabilityMap> reachability =
      HloReachabilityMap::Build(computation);

  EXPECT_TRUE(reachability->IsReachable(add3, add1));
  EXPECT_TRUE(reachability->IsReachable(add1, add3));
  EXPECT_TRUE(reachability->IsReachable(add3, add2));
  EXPECT_TRUE(reachability->IsReachable(add2, add3));
}
```


### Relevant log output

_No response_</details>"
56540,An error occurred during the fetch of repository 'llvm-project':,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.4.1

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

3.1.0

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

11.6/8

### GPU model and memory

RTX3080

### Current Behaviour?

```shell
Bazel-build code fails when trying to retrieve llvm source code. 

Download from primary fails with 404: 
https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/f402e682d0ef5598eeffc9a21a691b03e602ff58.tar.gz

Download from secondary fails with 504:
https://github.com/llvm/llvm-project/archive/f402e682d0ef5598eeffc9a21a691b03e602ff58.tar.gz
```


### Standalone code to reproduce the issue

```shell
https://github.com/tensorflow/tensorflow/archive/v2.4.1.tar.gz
```


### Relevant log output

```shell
ERROR: An error occurred during the fetch of repository 'llvm-project':
   java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/f402e682d0ef5598eeffc9a21a691b03e602ff58.tar.gz, https://github.com/llvm/llvm-project/archive/f402e682d0ef5598eeffc9a21a691b03e602ff58.tar.gz] to /home/dgu/.cache/bazel/_bazel_dgu/bbb96d26b2548d6cbb28158a8986fc7d/external/llvm-project/f402e682d0ef5598eeffc9a21a691b03e602ff58.tar.gz: Read timed out
```
</details>"
56539,"Masking tensor with boolean vector raises an error, while tf.boolean_mask works","### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

We can mask tensors to extract their elements (in the first `k` axes) using a boolean tensor of rank `k`:

```py
maps = tf.reshape(tf.range(4*4*5), (4, 4, 5))
rows = tf.constant([False, True, False, True])
maps[rows].shape            # TensorShape([2, 4, 5])
tf.boolean_mask(maps, rows).shape  # TensorShape([2, 4, 5])
```

However, it's impossible to do so if I want to mask other axis than the first:
```py
maps = tf.reshape(tf.range(4*4*5), (4, 4, 5))
labels = tf.constant([False, True, False, True, False])
tf.boolean_mask(maps, labels, axis=2).shape  # TensorShape([4, 4, 2])
maps[:, :, labels].shape            # TypeError: Only integers, slices (`:`), [...] are valid indices, got <tf.Tensor: shape=(5,), dtype=bool, ...)>
maps[..., labels].shape            # TypeError: Only integers, slices (`:`), [...] are valid indices, got <tf.Tensor: shape=(5,), dtype=bool, ...)>
```

I expected that `maps[:, :, labels]` had the exact same behavior of `tf.boolean_mask(maps, labels, axis=2)`;
and `maps[..., labels] == tf.boolean_mask(maps, labels, axis=-1)`.

### Standalone code to reproduce the issue

```py
import tensorflow as tf

maps = tf.reshape(tf.range(4*4*5), (4, 4, 5))
labels = tf.constant([False, True, False, True, False])
print(tf.boolean_mask(maps, labels, axis=2).shape)  # TensorShape([2, 4, 5])
print(maps[:, :, labels].shape)
```

### Relevant log output

```shell
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/mnt/files/Workspace/envs/tf/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/mnt/files/Workspace/envs/tf/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py"", line 907, in _check_index
    raise TypeError(_SLICE_TYPE_ERROR + "", got {!r}"".format(idx))
TypeError: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got <tf.Tensor: shape=(5,), dtype=bool, numpy=array([False,  True, False,  True, False])>
>>>
```"
56537,6d transpose not supported by TFLite GPU delegate,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 22.04 LTS
- TensorFlow installed from (source or binary): binary (pip)
- TensorFlow version (or github SHA if from source): 2.9.1


**Provide the text output from tflite_convert**

```
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select 
TF Select ops: Transpose
Details:
        tf.Transpose(tensor<1x2x2x7x7x384xf32>, tensor<6xi32>) -> (tensor<1x2x7x2x7x384xf32>) : {device = """"}

```

if SELECT_TF_OPS is enabled:

```
Subgraph#0 main(T#0) -> [T#2]
  Op#0 FlexTranspose(T#0, T#1) -> [T#2]
GPU COMPATIBILITY WARNING: Not supported custom op FlexTranspose

GPU COMPATIBILITY WARNING: Subgraph#0 has GPU delegate compatibility issues at nodes 0 with TFLite runtime version 2.9.1
```

**Standalone code to reproduce the issue** 
```python3
import tensorflow as tf
from tensorflow import keras

input = keras.Input(shape=(2,2,7,7,384), batch_size=1)
output = tf.transpose(a=input, perm=(0, 1, 3, 2, 4, 5))
model = keras.Model(inputs=input, outputs=output, name=""bug_model"")
model.summary()

converter = tf.lite.TFLiteConverter.from_keras_model(model)

converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
]
tflite_model = converter.convert()
tf.lite.experimental.Analyzer.analyze(model_content=tflite_model, gpu_compatibility=True)

open(""bug.tflite"", ""wb"").write(tflite_model)
```

**Any other info / logs**
6D transform is a part of an image transformer model."
56536,Tensorflow unable to find custom protobuf install location on OSX,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.7.2

### Custom Code

No

### OS Platform and Distribution

OSX Monterey

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

4.0.0

### GCC/Compiler version

Apple clang version 13.0.0 (clang-1300.0.29.30)

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
external/local_config_cc/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG '-DNS_BLOCK_ASSERTIONS=1' '-std=c++11' -fdebug-compilation-dir . -iquote . -iquote bazel-out/darwin-opt/bin -iquote external/com_google_absl -iquote bazel-out/darwin-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/darwin-opt/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/darwin-opt/bin/external/eigen_archive -iquote external/gif -iquote bazel-out/darwin-opt/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/darwin-opt/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/darwin-opt/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/darwin-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/darwin-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/darwin-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/darwin-opt/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/darwin-opt/bin/external/zlib -iquote external/snappy -iquote bazel-out/darwin-opt/bin/external/snappy -iquote external/double_conversion -iquote bazel-out/darwin-opt/bin/external/double_conversion -isystem external/nsync/public -isystem bazel-out/darwin-opt/bin/external/nsync/public -isystem external/eigen_archive -isystem bazel-out/darwin-opt/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/darwin-opt/bin/external/gif -isystem external/farmhash_archive/src -isystem bazel-out/darwin-opt/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/darwin-opt/bin/external/zlib -isystem external/double_conversion -isystem bazel-out/darwin-opt/bin/external/double_conversion -MD -MF bazel-out/darwin-opt/bin/tensorflow/c/_objs/env/env.d -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY '-frandom-seed=bazel-out/darwin-opt/bin/tensorflow/c/_objs/env/env.o' -isysroot __BAZEL_XCODE_SDKROOT__ -F__BAZEL_XCODE_SDKROOT__/System/Library/Frameworks -F__BAZEL_XCODE_DEVELOPER_DIR__/Platforms/MacOSX.platform/Developer/Library/Frameworks '-mmacosx-version-min=12.1' -DGRPC_BAZEL_BUILD -w '-march=skylake' '-mtune=skylake' '-std=c++14' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c tensorflow/c/env.cc -o bazel-out/darwin-opt/bin/tensorflow/c/_objs/env/env.o)
Execution platform: @local_execution_config_platform//:platform
In file included from tensorflow/c/env.cc:20:
In file included from ./tensorflow/c/tf_status_helper.h:20:
In file included from ./tensorflow/core/platform/status.h:32:
bazel-out/darwin-opt/bin/tensorflow/core/protobuf/error_codes.pb.h:10:10: fatal error: 'google/protobuf/port_def.inc' file not found
#include <google/protobuf/port_def.inc>
         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
1 error generated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 178.582s, Critical Path: 8.14s
INFO: 108 processes: 54 internal, 54 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
```


### Standalone code to reproduce the issue

```shell
I have a working install of `protobuf` in a custom installation location (not `/usr`). I've specified that install location as:


build:macos --define=PREFIX=/usr
build:macos --define=LIBDIR=$(PREFIX)/lib
build:macos --define=INCLUDEDIR=$(PREFIX)/include
build:macos --define=PROTOBUF_INCLUDE_PATH=/Users/kbeckwi/Documents/spack/opt/spack/darwin-monterey-skylake/apple-clang-13.0.0/protobuf-3.17.3-ohlc5sxkvf5xfvtshtkess3tt72xhk3f/include/google/protobuf/
```

This however does not appear to be sufficient. What else needs to be done to correctly specify the `protobuf` install location?
```


### Relevant log output

_No response_</details>"
56535,New quantizer seems to ignore TFLITE_BUILTINS_INT8,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.7.1 and 2.9.1

### 2. Code

The following script can be run directly to show the issue. A SavedModel is saved before the conversion and a TFLite file afterwards.

```python
import tensorflow as tf
import numpy as np


def create_not_fully_quantizable_model():
    """"""Set up the model""""""
    model = tf.keras.Sequential(
        [
            tf.keras.Input(shape=(5), batch_size=1),
            tf.keras.layers.Dense(5, input_shape=(1, 5)),
            tf.keras.layers.Dense(5),
            tf.keras.layers.Lambda(lambda x: tf.math.floormod(x, 3)),
        ]
    )

    return model


def dataset_generator(shape):
    """"""Set up the training data""""""
    x_train = np.array(
        [
            [0.0, 1.0, 2.0, 3.0, 4.0],
            [2.0, 0.0, 2.0, 3.0, 4.0],
            [0.0, 3.0, 2.0, 3.0, 4.0],
            [4.0, 1.0, 2.0, 3.0, 4.0],
            [5.0, 1.0, 2.0, 3.0, 4.0],
        ],
        dtype=""float32"",
    )

    def generator():
        """"""Dataset generator for post-training calibration.""""""
        for x in x_train:
            yield [np.reshape([x], shape)]

    return generator


def main():
    filename = ""test_partial_quant""

    model = create_not_fully_quantizable_model()
    model.save(filename)

    # Post-training conversion.
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    # converter.experimental_new_quantizer = False
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.representative_dataset = dataset_generator(model.input_shape)
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
    converter.target_spec.supported_types = [tf.int8]
    converter.inference_input_type = tf.int8
    converter.inference_output_type = tf.int8
    tflite_quant_model = converter.convert()

    # Save result
    tflite_file = filename + "".tflite""
    open(tflite_file, ""wb"").write(tflite_quant_model)
    print(f""TFLite file saved to '{tflite_file}'."")


if __name__ == ""__main__"":
    main()
```

### 3. Failure after conversion

I would expect (and want) the above script to fail, because the model intentionally contains ops that can not be quantized and I am using the following options:
- `converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]`
- `converter.target_spec.supported_types = [tf.int8]`

From the [TensorFlow docs](https://github.com/tensorflow/tensorflow/blob/v2.9.1/tensorflow/lite/python/convert.py#L159-L197):
```
  # Convert model using only TensorFlow Lite quantized int8 operations.
  # Specifying this will throw an error for operations that do not yet have
  # quantized implementations.
  TFLITE_BUILTINS_INT8 = ""TFLITE_BUILTINS_INT8""
```

Instead there are dequant/quant nodes inserted around the unsupported operator and the model is converted without any complaints. In other words: The used options seem to have no effect.

### 4. Tests and observations

- When I use the old quantizer (`converter.experimental_new_quantizer = False`) I am getting the expected error.
- After a quick look into the code there seems to be an option `fully_quantize` for `mlir_quantize()` [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python/convert.py#L200), but it seems to be not used [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python/lite.py#L621).
- Might be related to [this TODO](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/quantization/lite/quantize_model.cc#L49) in the cpp code."
56534,Use different Optimizers for two branches of a network. Branches use each other to output results when finally calculating Loss,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

binary

### Tensorflow Version

tf1.11

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.5

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I want to calculate the KLD loss using the output of the two branches, and there will always be an error where one of the results is empty.
```


### Standalone code to reproduce the issue

```shell
self.teacher_loss = self.get_teacher_loss()   #  branch 1 loss
 self.stu_loss = self.get_stu_loss()            # branch 2 loss
     
 with tf.variable_scope('teacher_optimizer'):   #optimizer 1
    ....
 with tf.variable_scope('stu_optimizer'):       #optimizer 2
    ...
```


### Relevant log output

_No response_</details>"
56533,Tensorflow lite GPUDelegate with Android API 31,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Yes 
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Android
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: Pixel 6 pro
-   **TensorFlow installed from (source or binary)**: binary
-   **TensorFlow version (use command below)**: 2.9.0
-   **Python version**:  NA
-   **Bazel version (if compiling from source)**: NA
-   **GCC/Compiler version (if compiling from source)**: NA
-   **CUDA/cuDNN version**: NA
-   **GPU model and memory**: NA
-   **Exact command to reproduce**: NA


### Describe the problem

My model is a classifier trained using EfficientNet and runs on the camera feed using CameraX APIs. It expects a FloatBuffer of size 768 and returns a list of labels. This was working fine upto API 30, but once I update the `targetSdk` to 31 and run on a device with Android 12, I get this crash:

```
    java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Can not open OpenCL library on this device - dlopen failed: library ""libOpenCL.so"" not found
    Falling back to OpenGL
    TfLiteGpuDelegate Init: Batch size mismatch, expected 768 but got 1
    TfLiteGpuDelegate Prepare: delegate is not initialized
    Node number 247 (TfLiteGpuDelegateV2) failed to prepare.
    Restored original execution plan after delegate application failure.
        at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:100)
        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:52)
        at org.tensorflow.lite.NativeInterpreterWrapperExperimental.<init>(NativeInterpreterWrapperExperimental.java:40)
        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:192)
```


### Source code / logs
Initialization code: 

```
fun initialize() {
        val compatList = CompatibilityList()
        val options = Interpreter.Options().apply {
            if (compatList.isDelegateSupportedOnThisDevice) {
                val delegateOptions = compatList.bestOptionsForThisDevice
                addDelegate(GpuDelegate(delegateOptions))
            }
        }
        modelDownloader.download {
            interpreter = Interpreter(it, options)
        }
    }
```

The crash occurs on the line `interpreter = Interpreter(it, options)` 

Inference code (never gets called in case of crash) : 
```
        val outputs: MutableMap<Int?, Any?> = HashMap()
        val result = Array(1) { FloatArray(8) }
        outputs[0] = result
        val input = passportInputImageConverter(imageProxy.image!!)
        interpreter!!.runForMultipleInputsOutputs(
            arrayOf(input), outputs
        )

```
where `passportInputImageConverter` would crop and convert the image to FloatBuffer using :  

```
 private val imageProcessor: ImageProcessor = ImageProcessor.Builder()
            .add(ResizeOp(INPUT_SIZE, INPUT_SIZE, ResizeOp.ResizeMethod.NEAREST_NEIGHBOR))
            .add(NormalizeOp(0f, 255f))
            .build()
```


Dependency list:

```
org.tensorflow:tensorflow-lite:2.9.0
org.tensorflow:tensorflow-lite-gpu:2.9.0
org.tensorflow:tensorflow-lite-gpu-delegate-plugin:0.4.1
org.tensorflow:tensorflow-lite-support:0.4.1
```
"
56532,Conda-forge release of tensorflow 2.x for win-64,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

tf>=2.5

### Custom Code

No

### OS Platform and Distribution

Windows10

### Mobile device

_No response_

### Python version

>=3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The win-64 version of tensorflow on conda-forge is only available at v1.14, please make a release of v2.x, preferable >=2.5.
```


### Standalone code to reproduce the issue

```shell
conda install tensorflow -c conda-forge
```


### Relevant log output

_No response_</details>"
56531,C API Tensor data type doesn't have flat() to convert into a Eigen::Tensor,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Feature Request

### Source

binary

### Tensorflow Version

2.8

### Custom Code

Yes

### OS Platform and Distribution

Linux 

### Mobile device

07010841551

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hi there!
I am trying to convert my TF_Tensor datatype object into a eigen tensor(I am using the tensorflow C API).When I am using the C++ tensorflow API,I use the flat() method but there's no such thing available in the C API TF_Tensor.
Could someone please check and tell me??

Thanks,
Yoga
```


### Standalone code to reproduce the issue

```shell
// In C++:
auto input = input_tensor.flat<int32>();

// In C:
No method available
```


### Relevant log output

_No response_</details>"
56530,Gradients across graphs,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
How can I compute gradients across two `@tf.function`?
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/10V4V5Hr25NZehWE2TjFPD0-jtGpCFDMa?usp=sharing
```


### Relevant log output

_No response_</details>"
56529,Tensor Flow gives different results on INTEL and AMD CPUs,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

<s>source</s>

### Tensorflow Version

2.3

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

>= 3.6 

### Bazel version

<s>0.26.1</s>

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I tried running a sample sqrt computation on INTEL and AMD CPUs. I used a tolerance of 0.0000001 i.e., the values are printed in the log output, only if the difference between corresponding values is >= 0.0000001.
The values obtained on INTEL and AMD CPUs are not matching.

#### Note: 
* Please refer this discussion for more details, https://discuss.tensorflow.org/t/tf-sqrt-computations-are-different-on-two-diff-cpu-architectures-intel-and-amd/10208
* the raw files generated by numpy sqrt computation by the provided script has no difference across both the architectures.

<s> Steps to reproduce building TF from source: ~~

* Used tensorflow 1.15.5 which is built from source ( [Build from source | TensorFlow](https://www.tensorflow.org/install/source#configuration_options)).
* Disabled all supports during ./configure step (XLA, CUDA, etc.,).
* Provided -march=x86-64 for --copt and --host_copt flags while doing bazel build and set --config=v1. </s>

### Standalone code to reproduce the issue

######  test.py
```shell
import numpy as np
import tensorflow.compat.v1 as tf

print(tf.__version__)
tf.disable_v2_behavior()
# float32 NumPy array
a = np.arange(100, dtype=np.float32)
# The same array with the same dtype in TensorFlow
a_tf = tf.constant(a, dtype=tf.float32)
# Square root with NumPy
sqrt = np.sqrt(a)
sqrt.tofile('../np_exp/sqrt.raw')
# Square root with TensorFlow
with tf.Session() as sess:
    sqrt_tf = sess.run(tf.sqrt(a_tf))
    sqrt_tf.tofile('./sqrt.raw')
```

###### compare.py - script to compare raw files

```shell
import csv
import os
import argparse
import json
import numpy as np
import math
# import cv2
result_data = ''
tolrence = 0.0000001

np.set_printoptions(precision=17, suppress=True)

def csv_reader(abc1, abc2, ):
    reader1 = np.fromfile(abc1, dtype=np.float32)
    reader2 = np.fromfile(abc2, dtype=np.float32)
    #print(reader1)
    i = 0
    c = 0
    count = 0
    anti_count = 0
    isNan = 0
    try:
       for (index1, index2) in zip(np.nditer(reader1), np.nditer(reader2)):
           i = i + 1
           if math.isnan(index2):
                continue
           if abs(index1 - index2) >= tolrence:
               count = count + 1
               if count != 0:
                 print(str(index1) + "" "" + str(index2) + "" "" + str(i))
           else:
                anti_count = anti_count + 1
       if count != 0:
             print(abc1)
             print(abc2)
             print(""Total Matches :  "", anti_count, ""Mismatches  : "", count)

    except:
        pass
      
def list_files_from_dir(root):
    all_raw_list = list()
    for path, subdirs, files in os.walk(root):
        for name in files:
            if name not in ['model.cpp', 'model.bin', 'model_net.json']:
                 all_raw_list.append(os.path.join(path, name))

    return all_raw_list


def compare_all(dir1, dir2):
    list1 = list_files_from_dir(dir1)
    list2 = list_files_from_dir(dir2)
    list1.sort()
    list2.sort()
    for i in range(len(list1)):
        csv_reader(list1[i], list2[i])


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(
        description='parser')
    parser.add_argument('-i1', '--input1', help='The path to the directory containing raw files.', required=True)
    parser.add_argument('-i2', '--input2', help='The path to the directory containing raw files', required=True)
    args = parser.parse_args()

    input1 = args.input1
    input2 = args.input2
    compare_all(input1, input2)

```
## Sample commands to run

* python3 test.py                        => script to run the sqrt computation
* python3 compare.py -i1 ./AMD -i2 ./INTEL     => compares the raw files generated by AMD and INTEL (transfer the raw files to one of the machines, ./AMD folder has raw files generated on AMD and ./INTEL has raw files generated on INTEL)

### Relevant log output

```shell
1.4142135 1.4142134 3
2.0 1.9999999 5
2.4494898 2.4494896 7
2.828427 2.8284268 9
3.0 2.9999998 10
3.162278 3.1622777 11
3.6055512 3.6055508 14
4.0 3.9999998 17
4.5825763 4.5825753 22
4.6904163 4.6904154 23
4.795831 4.7958307 24
4.8989797 4.898979 25
5.099019 5.0990186 27
5.1961527 5.196152 28
5.385165 5.3851647 30
5.656854 5.6568537 33
5.7445626 5.744562 34
5.830952 5.830951 35
6.0 5.9999995 37
6.0827627 6.0827622 38
6.324556 6.3245554 41
6.557439 6.557438 44
7.0000005 6.9999995 50
7.0710683 7.0710673 51
7.2111025 7.2111015 53
7.2801104 7.280109 54
7.416199 7.4161983 56
8.0 7.9999995 65
8.062258 8.062257 66
8.124039 8.124038 67
8.306624 8.306623 70
8.42615 8.426149 72
8.5440035 8.544003 74
8.6602545 8.660254 76
9.0 8.999999 82
9.055386 9.055385 83
9.110435 9.110433 84
9.165153 9.165151 85
9.273619 9.273618 87
9.380833 9.380831 89
9.486833 9.486834 91
9.539392 9.5393915 92
9.591662 9.591661 93
./AMD/sqrt.raw
./INTEL/sqrt.raw
Total Matches :   57 Mismatches  :  43
```
</details>"
56527,"""Cannot convert a symbolic Tensor ({}) to numpy array""","I tried to use the interface from DeepXDE as mentioned here- ""https://github.com/tensorflow/tensorflow/issues/48167#issuecomment-966596156"" but encountered an error.  While I understand the error, I'm not sure how to resolve it. Any help in this regards is appreciated. 

I'm using:
```
dde.optimizers.config.set_LBFGS_options(maxcor=50,ftol=1*np.finfo(float).eps,maxiter=100000,maxfun=100000,maxls=50)
self.optimizer = dde.optimizers.tfp_optimizer.lbfgs_minimize(self.trainable_var,self.loss)
```                                                     

The error:
""Cannot convert a symbolic Tensor ({}) to numpy array""

It seems like the function only accepts numpy arrays but I have a TF symbolic tensor. Any way to resolve this?

_Originally posted by @rohitvuppala in https://github.com/tensorflow/tensorflow/issues/48167#issuecomment-1162519845_"
56522,error: use of undeclared identifier 'posix_fallocate' when cross compiling from darwin_arm64 to darwin_x64_86,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

OSX Darwin Kernel Version 21.5.

### Mobile device

_No response_

### Python version

3.9

### Bazel version

5.1.1

### GCC/Compiler version

Apple clang version 13.1.6 (clang-1316.0.21.2.5)

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Compilation fail with these flags:

bazel build --config opt --config macos --cpu=darwin_x86_64 //tensorflow/tools/lib_package:libtensorflow

external/org_sqlite/sqlite3.c:35245:42: error: use of undeclared identifier 'posix_fallocate'
```


### Standalone code to reproduce the issue

```shell
bazel build --config opt --config macos --cpu=darwin_x86_64 //tensorflow/tools/lib_package:libtensorflow
```


### Relevant log output

```shell
ERROR: [...]/external/org_sqlite/BUILD.bazel:34:11: Compiling sqlite3.c failed: (Aborted): wrapped_clang failed: error executing command external/local_config_cc/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 -DNDEBUG '-DNS_BLOCK_ASSERTIONS=1' ... (remaining 45 arguments skipped)
external/org_sqlite/sqlite3.c:35245:42: error: use of undeclared identifier 'posix_fallocate'
  { ""fallocate"",    (sqlite3_syscall_ptr)posix_fallocate,  0 },
                                         ^
external/org_sqlite/sqlite3.c:35367:22: error: invalid application of 'sizeof' to an incomplete type 'struct unix_syscall []'
    for(i=0; i<sizeof(aSyscall)/sizeof(aSyscall[0]); i++){
                     ^~~~~~~~~~
external/org_sqlite/sqlite3.c:35376:22: error: invalid application of 'sizeof' to an incomplete type 'struct unix_syscall []'
    for(i=0; i<sizeof(aSyscall)/sizeof(aSyscall[0]); i++){
                     ^~~~~~~~~~
external/org_sqlite/sqlite3.c:35403:20: error: invalid application of 'sizeof' to an incomplete type 'struct unix_syscall []'
  for(i=0; i<sizeof(aSyscall)/sizeof(aSyscall[0]); i++){
                   ^~~~~~~~~~
external/org_sqlite/sqlite3.c:35420:16: error: invalid application of 'sizeof' to an incomplete type 'struct unix_syscall []'
    for(i=0; i<ArraySize(aSyscall)-1; i++){
               ^~~~~~~~~~~~~~~~~~~
external/org_sqlite/sqlite3.c:14509:38: note: expanded from macro 'ArraySize'
#define ArraySize(X)    ((int)(sizeof(X)/sizeof(X[0])))
                                     ^~~
external/org_sqlite/sqlite3.c:35424:14: error: invalid application of 'sizeof' to an incomplete type 'struct unix_syscall []'
  for(i++; i<ArraySize(aSyscall); i++){
             ^~~~~~~~~~~~~~~~~~~
external/org_sqlite/sqlite3.c:14509:38: note: expanded from macro 'ArraySize'
#define ArraySize(X)    ((int)(sizeof(X)/sizeof(X[0])))
                                     ^~~
6 errors generated
```
</details>"
56521,batch_scatter_ops_test.py fails on GPU,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.8.2

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18.04.5 (on Google Colab)

### Mobile device

_No response_

### Python version

3.7.13

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA V11.1.105

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Test `scatterTest.testVariableRankUpdate()` in `kernel_tests/array_ops/batch_scatter_ops_test.py` cannot pass on GPU, while on CPU is ok. A large difference is noticed.
```


### Standalone code to reproduce the issue

```shell
Colab link: https://colab.research.google.com/drive/1fpTSa3Uhmz-NgGztvPjWbt2uYg76ftRu?usp=sharing
```


### Relevant log output

```shell
VType: <class 'numpy.float32'>, IType: <class 'numpy.int32'>
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-10-47ce3e6b7fbd> in <module>()
      1 scatterTest = ScatterTest()
----> 2 scatterTest.testVariableRankUpdate()

7 frames
<ipython-input-7-810e700a9bcb> in testVariableRankUpdate(self)
     67         print(f""VType: {vtype}, IType: {itype}"")
     68         self._VariableRankTest(
---> 69             state_ops.batch_scatter_update, vtype, itype)
     70     print(""Test Pass!"")

<ipython-input-7-810e700a9bcb> in _VariableRankTest(self, tf_scatter, vtype, itype, repeat_indices, updates_are_scalar, method)
     59           else:
     60             self.evaluate(tf_scatter(ref, indices, updates))
---> 61           self.assertAllClose(ref, new)
     62 
     63   def testVariableRankUpdate(self):

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/test_util.py in decorated(*args, **kwds)
   1446   def decorated(*args, **kwds):
   1447     if not ops.inside_function():
-> 1448       return f(*args, **kwds)
   1449 
   1450     tensor_args = []

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/test_util.py in assertAllClose(self, a, b, rtol, atol, msg)
   3021           `[p] = [1]['d']`, then `a[p] = (6, 7)`.
   3022     """"""
-> 3023     self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)
   3024 
   3025   @py_func_if_in_function

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/test_util.py in _assertAllCloseRecursive(self, a, b, rtol, atol, path, msg)
   2983             atol=atol,
   2984             msg=(""Mismatched value: a%s is different from b%s. %s"" %
-> 2985                  (path_str, path_str, msg)))
   2986       except TypeError as e:
   2987         msg = (""Error: a%s has %s, but b%s has %s. %s"" %

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/test_util.py in _assertArrayLikeAllClose(self, a, b, rtol, atol, msg)
   2915       # nan even though the equal_nan is False by default internally.
   2916       np.testing.assert_allclose(
-> 2917           a, b, rtol=rtol, atol=atol, err_msg=""\n"".join(msgs), equal_nan=True)
   2918 
   2919   def _assertAllCloseRecursive(self,

/usr/local/lib/python3.7/dist-packages/numpy/testing/_private/utils.py in assert_allclose(actual, desired, rtol, atol, equal_nan, err_msg, verbose)
   1529     header = f'Not equal to tolerance rtol={rtol:g}, atol={atol:g}'
   1530     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
-> 1531                          verbose=verbose, header=header, equal_nan=equal_nan)
   1532 
   1533 

/usr/local/lib/python3.7/dist-packages/numpy/testing/_private/utils.py in assert_array_compare(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)
    842                                 verbose=verbose, header=header,
    843                                 names=('x', 'y'), precision=precision)
--> 844             raise AssertionError(msg)
    845     except ValueError:
    846         import traceback

AssertionError: 
Not equal to tolerance rtol=1e-06, atol=1e-06
Mismatched value: a is different from b. 
not close where = (array([1, 1, 1, 1, 1]), array([0, 1, 2, 3, 4]))
not close lhs = [-2.2964916   2.4098344   1.7278361   2.2045562   0.79482764]
not close rhs = [ 0.9764211 -1.1834271  1.9163636 -1.1233268 -0.6640355]
not close dif = [3.2729127  3.5932615  0.18852746 3.327883   1.4588631 ]
not close tol = [1.9764211e-06 2.1834271e-06 2.9163634e-06 2.1233268e-06 1.6640355e-06]
dtype = float32, shape = (2, 5)
Mismatched elements: 5 / 10 (50%)
Max absolute difference: 3.5932615
Max relative difference: 3.351948
 x: array([[-0.378359, -0.791615,  0.859548, -0.230789, -0.065661],
       [-2.296492,  2.409834,  1.727836,  2.204556,  0.794828]],
      dtype=float32)
 y: array([[-0.378359, -0.791615,  0.859548, -0.230789, -0.065661],
       [ 0.976421, -1.183427,  1.916364, -1.123327, -0.664035]],
      dtype=float32)
```
</details>"
56520,[TF 1.x] Cannot print tensors whose operations have been executed from modules in anotehr file,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.2.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.01

### Mobile device

_No response_

### Python version

3.6.13

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

N/A

### GPU model and memory

N/A

### Current Behaviour?
Apparently, using TF `1.x` modules imported from another files output tensors which can't be `tf.Print`ed. Everything uses `compat.v1`, base version specified above.

- There's **no** `stdout`, and scripts are executed from the terminal so `stderr` should be visible. The corresponding string 'message' alongside with `tf.print`/`tf.Print` command is also not printed

- with no `stdout`, the program would simply *hang* until CTRL+Z killed. Using `sess.close()` and other variations do not help at all.

### Standalone code to reproduce the issue

This is a snippet of the code I'm using:

```py
import batching #module from another file
import nls #same as above

sess = tf.compat.v1.Session()

with sess.as_default():
    #Executing custom ops
    data_obj = nls.MyDataset('train')
    inp, out = batching.batch_inputs(data_obj, train=False, batch_size=1, num_preprocess_threads=8, num_readers=4)
    #ops computation finished

    a = out[0] #out is a LIST of 4 tensors - none of them can be printed
    print_op = tf.print(""\n---\ntensors:"", a, output_stream=sys.stdout)
    
    with tf.control_dependencies([print_op]): #dummy operation to maintain graph
      d_tensor = a * 1
    sess.run(d_tensor)

sess.close() #This seems to be largely irrelevant, changing no behavior whatsoever
```

vanilla `print()`-ing tensors leads to their shapes and types, but not the contents. So the tensors are well formed atleast - just not accessible for some reason.

These computations aren't intermediary between layers - its simply vanilla operations to *preprocess* tensors from a `TFRecord` and output the data for future training in the model. That stage hasn't been implemented yet.


### Relevant log output

```shell
2022-06-21 12:43:08.150767: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
/opt/awesome/data/sample_tfrecs IN DATA_FILES
Glob Files Done...
WARNING:tensorflow:From /opt/awesome/scripts/BDD_Driving_Model/batching.py:156: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.
WARNING:tensorflow:From /home/awesome/anaconda3/envs/bdd100k_prepro/lib/python3.6/site-packages/tensorflow/python/training/input.py:277: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.
WARNING:tensorflow:From /home/awesome/anaconda3/envs/bdd100k_prepro/lib/python3.6/site-packages/tensorflow/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.
WARNING:tensorflow:From /home/awesome/anaconda3/envs/bdd100k_prepro/lib/python3.6/site-packages/tensorflow/python/training/input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
WARNING:tensorflow:From /home/awesome/anaconda3/envs/bdd100k_prepro/lib/python3.6/site-packages/tensorflow/python/training/input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
using non random shuffle queue
WARNING:tensorflow:From /opt/awesome/scripts/BDD_Driving_Model/dataset.py:80: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:From /opt/awesome/scripts/BDD_Driving_Model/data_providers/nexar_large_speed.py:817: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)
WARNING:tensorflow:From /opt/awesome/scripts/BDD_Driving_Model/batching.py:242: batch_join (from tensorflow.python.training.input) is deprecated and will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.interleave(...).batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).
4  batch_joins, each of them capacity is,  2  instances  Warning: using this might be quite slow!
buffer queue capacity is:  4  batches

---------------------------
Freezes here, no further stdout/stderr - nor the corresponding message with tf.print
---------------------------
```
</details>"
56519,"tf.linalg.inv does not support ""half"" datatype","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.8.2

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18.04.5 (on Google Colab)

### Mobile device

_No response_

### Python version

3.7.13

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The `tf.linalg.inv` does not support `half` data type, while the doc on [https://www.tensorflow.org/versions/r2.8/api_docs/python/tf/linalg/inv](https://www.tensorflow.org/versions/r2.8/api_docs/python/tf/linalg/inv) said `half` can be used.

The latest version (tf 2.9) also has this issue.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

x = np.array([[1, 2], [3, 4]], dtype=np.half)
tf.linalg.inv(x)
```


### Relevant log output

```shell
---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
<ipython-input-1-22e20a75db78> in <module>()
      3 
      4 x = np.array([[1, 2], [3, 4]], dtype=np.half)
----> 5 tf.linalg.inv(x)

2 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     53     ctx.ensure_initialized()
     54     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---> 55                                         inputs, attrs, num_outputs)
     56   except core._NotOkStatusException as e:
     57     if name is not None:

NotFoundError: Could not find device for node: {{node MatrixInverse}} = MatrixInverse[T=DT_HALF, adjoint=false]
All kernels registered for op MatrixInverse:
  device='GPU'; T in [DT_COMPLEX128]
  device='GPU'; T in [DT_COMPLEX64]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
 [Op:MatrixInverse]
```
</details>"
56518,error: class tflite::Interpreter has no member named UseNNAPI               ^~~~~~~~,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf2.8

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
in tflite 2.8 , how to use nnapi C++ api in native linux-x86
I use it like 
interpreter->UseNNAPI(true);
but it is not exists.
""Interpreter has no member named UseNNAPI ""
```


### Standalone code to reproduce the issue

```shell
/
```


### Relevant log output

_No response_</details>"
56517,UNKNOWN: KeyError: 331 in `tf.data.Dataset`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2

### GPU model and memory

NVIDIA GTX 2080Ti

### Current Behaviour?

```shell
During training using a data pipeline constructed with `tf.data.Dataset.from_generator(...)` an error occurs stochastically. The error does not interrupt the training but still I cannot find the issue in my code.
```


### Standalone code to reproduce the issue

```shell
import functools
import os

import numpy as np
import tensorflow as tf
from itranslate.text_models.utility.data_reading import load_records, parse_example
from tensorflow.python.data.ops.options import AutoShardPolicy

NUM_TRAIN_SAMPLES = 1000
NUM_DEV_SAMPLES = 10

def get_dataset(split: str, batch_size: int, max_length: int = 64):
    # Number generator
    def generator():
        for i in range(num_samples):
            _dims = np.random.randint(low=1, high=max_length, size=1)
            x = np.zeros(_dims, dtype=np.int32) + np.random.randint(low=1, high=5000, size=1)
            y = np.zeros(_dims, dtype=np.int32) + np.random.randint(low=1, high=5000, size=1)
            yield {'sources': x, 'targets': y}

    assert split in (""train"", ""dev"")
    is_training = split == ""train""

    num_samples = NUM_TRAIN_SAMPLES if is_training else NUM_DEV_SAMPLES
    options = tf.data.Options()
    options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA
    options.experimental_deterministic = False
    dataset = tf.data.Dataset.from_generator(
        lambda: generator(),
        output_signature={
            'sources': tf.TensorSpec(shape=(None,), dtype=tf.int32),
            'targets': tf.TensorSpec(shape=(None,), dtype=tf.int32)
        }
    ).with_options(options=options)

    dataset = dataset.padded_batch(batch_size=batch_size)

    def map_to_example(example):
        sources, targets = example['sources'], example['targets']
        return sources, targets

    dataset = dataset.map(map_to_example, num_parallel_calls=tf.data.AUTOTUNE)

    if is_training:
        dataset = dataset.shuffle(buffer_size=256)

    return dataset.repeat().prefetch(tf.data.AUTOTUNE)


def main():
    max_length = 64

    train_batch_size = 64
    valid_batch_size = 16

    # noinspection PyArgumentEqualDefault
    strategy = tf.distribute.MirroredStrategy(devices=[""GPU:0"", ""GPU:1""])
    with strategy.scope():
        optimizer = tf.keras.optimizers.Adam()
        # Model
        inputs = tf.keras.layers.Input(shape=(None,), dtype=tf.int32)
        x = inputs
        x = tf.keras.layers.Embedding(input_dim=5000,
                                      output_dim=64)(x)
        x = tf.keras.layers.Dense(5000)(x)
        model = tf.keras.Model(inputs=inputs, outputs=x)
        model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer)
        model.summary()

        train_data = get_dataset(
            split=""train"",
            batch_size=train_batch_size,
            max_length=max_length
        )

        valid_data = get_dataset(
            split=""dev"",
            batch_size=valid_batch_size,
            max_length=max_length,
        )

        model.fit(
            train_data,
            epochs=200,
            steps_per_epoch=5000,
            validation_data=valid_data,
            validation_steps=3
        )


if __name__ == '__main__':
    main()
```


### Relevant log output

```shell
5000/5000 [==============================] - ETA: 0s - loss: 5.93492022-06-21 07:14:06.835116: W tensorflow/core/framework/op_kernel.cc:1733] UNKNOWN: KeyError: 331
Traceback (most recent call last):

  File ""/home/gc/miniconda3/envs/itranslate-translation-v2/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py"", line 270, in __call__
    ret = func(*args)

  File ""/home/gc/miniconda3/envs/itranslate-translation-v2/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py"", line 642, in wrapper
    return func(*args, **kwargs)

  File ""/home/gc/miniconda3/envs/itranslate-translation-v2/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1132, in finalize_py_func
    generator_state.iterator_completed(iterator_id)

  File ""/home/gc/miniconda3/envs/itranslate-translation-v2/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 853, in iterator_completed
    del self._iterators[self._normalize_id(iterator_id)]

KeyError: 331
```
</details>"
56515,Can you help me find the links about the calling of pretrained moder for inference under C++ environment,"Can you help me find the resource about the demo showing the calling of pretrained moder for inference under C++ environment?

I have training a model using Tensorflow2.4.x, and want to use it for inference in C++ environment, but I cannot find related resource in this project.
"
56513,Data Sharding doesn't work with MultiworkerMirroredStrategy for distributed training,"While using distributed training, the data sharding doesn't seem to work properly.
        options = tf.data.Options()
        options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA
        ds_train = ds_train.with_options(options)
Even after setting, AutoShardPolicy.DATA my workers work on same chunk of data. It gives the same metrics at each epoch

Chief Logs:
 [==============================] - 4s 52ms/step - loss: 0.2569 - accuracy: 0.9237 - lr: 0.0010
Epoch 3/5
1/70 [..............................] - ETA: 1s - loss: 0.1778 - accuracy: 0.9531 3/70 [>.............................] - ETA: 2s - loss: 0.1472 - accuracy: 0.9583 4/70 [>.............................] - ETA: 2s - loss: 0.1550 - accuracy: 0.9512 6/70 [=>............................] - ETA: 2s - loss: 0.1757 - accuracy: 0.9440 7/70 [==>...........................] - ETA: 2s - loss: 0.1703 - accuracy: 0.9475 9/70 [==>...........................] - ETA: 2s - loss: 0.1669 - accuracy: 0.9514 [===>..........................] - ETA: 2s - loss: 0.1679 - accuracy: 0.9503 [====>.........................] - ETA: 2s - loss: 0.1648 - accuracy: 0.9513 [=====>........................] - ETA: 2s - loss: 0.1605 - accuracy: 0.9526 [======>.......................] - ETA: 2s - loss: 0.1615 - accuracy: 0.9531 [=======>......................] - ETA: 2s - loss: 0.1605 - accuracy: 0.9515 [=======>......................] - ETA: 2s - loss: 0.1616 - accuracy: 0.9508 [========>.....................] - ETA: 2s - loss: 0.1583 - accuracy: 0.9513 [=========>....................] - ETA: 2s - loss: 0.1603 - accuracy: 0.9508 [==========>...................] - ETA: 1s - loss: 0.1603 - accuracy: 0.9501 [===========>..................] - ETA: 1s - loss: 0.1598 - accuracy: 0.9506 [===========>..................] - ETA: 1s - loss: 0.1596 - accuracy: 0.9510 [============>.................] - ETA: 1s - loss: 0.1614 - accuracy: 0.9509 [=============>................] - ETA: 1s - loss: 0.1596 - accuracy: 0.9517 [==============>...............] - ETA: 1s - loss: 0.1578 - accuracy: 0.9527 [==============>...............] - ETA: 1s - loss: 0.1608 - accuracy: 0.9523 [===============>..............] - ETA: 1s - loss: 0.1598 - accuracy: 0.9531 [================>.............] - ETA: 1s - loss: 0.1579 - accuracy: 0.9537 [=================>............] - ETA: 1s - loss: 0.1590 - accuracy: 0.9526 [==================>...........] - ETA: 1s - loss: 0.1585 - accuracy: 0.9530 [==================>...........] - ETA: 1s - loss: 0.1573 - accuracy: 0.9533 [===================>..........] - ETA: 0s - loss: 0.1599 - accuracy: 0.9526 [====================>.........] - ETA: 0s - loss: 0.1593 - accuracy: 0.9523 [=====================>........] - ETA: 0s - loss: 0.1574 - accuracy: 0.9528 [======================>.......] - ETA: 0s - loss: 0.1551 - accuracy: 0.9540 [======================>.......] - ETA: 0s - loss: 0.1538 - accuracy: 0.9544 [=======================>......] - ETA: 0s - loss: 0.1538 - accuracy: 0.9546 [========================>.....] - ETA: 0s - loss: 0.1542 - accuracy: 0.9544 [=========================>....] - ETA: 0s - loss: 0.1533 - accuracy: 0.9547 [==========================>...] - ETA: 0s - loss: 0.1526 - accuracy: 0.9550 [==========================>...] - ETA: 0s - loss: 0.1510 - accuracy: 0.9555 [===========================>..] - ETA: 0s - loss: 0.1508 - accuracy: 0.9556 [============================>.] - ETA: 0s - loss: 0.1504 - accuracy: 0.9557 [==============================] - ETA: 0s - loss: 0.1491 - accuracy: 0.9560
Learning rate for epoch 3 is 0.0010000000474974513
 [==============================] - 4s 55ms/step - loss: 0.1491 - accuracy: 0.9560 - lr: 0.0010
Epoch 4/5
1/70 [..............................] - ETA: 2s - loss: 0.1289 - accuracy: 0.9609 3/70 [>.............................] - ETA: 1s - loss: 0.1608 - accuracy: 0.9531 4/70 [>.............................] - ETA: 2s - loss: 0.1567 - accuracy: 0.9590 6/70 [=>............................] - ETA: 2s - loss: 0.1553 - accuracy: 0.9583 8/70 [==>...........................] - ETA: 2s - loss: 0.1443 - accuracy: 0.9590 [===>..........................] - ETA: 2s - loss: 0.1307 - accuracy: 0.9625 [====>.........................] - ETA: 2s - loss: 0.1301 - accuracy: 0.9622 [=====>........................] - ETA: 2s - loss: 0.1256 - accuracy: 0.9604 [======>.......................] - ETA: 2s - loss: 0.1240 - accuracy: 0.9609 [=======>......................] - ETA: 2s - loss: 0.1201 - accuracy: 0.9634 [========>.....................] - ETA: 2s - loss: 0.1209 - accuracy: 0.9617 [========>.....................] - ETA: 2s - loss: 0.1210 - accuracy: 0.9626 [=========>....................] - ETA: 1s - loss: 0.1198 - accuracy: 0.9631 [==========>...................] - ETA: 1s - loss: 0.1216 - accuracy: 0.9627 [===========>..................] - ETA: 1s - loss: 0.1204 - accuracy: 0.9634 [===========>..................] - ETA: 1s - loss: 0.1194 - accuracy: 0.9641 [============>.................] - ETA: 1s - loss: 0.1189 - accuracy: 0.9641 [=============>................] - ETA: 1s - loss: 0.1186 - accuracy: 0.9642 [==============>...............] - ETA: 1s - loss: 0.1200 - accuracy: 0.9640 [===============>..............] - ETA: 1s - loss: 0.1205 - accuracy: 0.9640 [===============>..............] - ETA: 1s - loss: 0.1213 - accuracy: 0.9635 [================>.............] - ETA: 1s - loss: 0.1205 - accuracy: 0.9636 [=================>............] - ETA: 1s - loss: 0.1206 - accuracy: 0.9637 [==================>...........] - ETA: 1s - loss: 0.1220 - accuracy: 0.9635 [===================>..........] - ETA: 1s - loss: 0.1218 - accuracy: 0.9634 [===================>..........] - ETA: 0s - loss: 0.1233 - accuracy: 0.9635 [====================>.........] - ETA: 0s - loss: 0.1217 - accuracy: 0.9639 [=====================>........] - ETA: 0s - loss: 0.1213 - accuracy: 0.9641 [======================>.......] - ETA: 0s - loss: 0.1233 - accuracy: 0.9637 [=======================>......] - ETA: 0s - loss: 0.1228 - accuracy: 0.9636 [=======================>......] - ETA: 0s - loss: 0.1219 - accuracy: 0.9639 [========================>.....] - ETA: 0s - loss: 0.1200 - accuracy: 0.9646 [=========================>....] - ETA: 0s - loss: 0.1189 - accuracy: 0.9650 [==========================>...] - ETA: 0s - loss: 0.1180 - accuracy: 0.9653 [==========================>...] - ETA: 0s - loss: 0.1175 - accuracy: 0.9650 [===========================>..] - ETA: 0s - loss: 0.1166 - accuracy: 0.9650 [============================>.] - ETA: 0s - loss: 0.1156 - accuracy: 0.9649
Learning rate for epoch 4 is 9.999999747378752e-05
 [==============================] - 4s 53ms/step - loss: 0.1147 - accuracy: 0.9652 - lr: 1.0000e-04
Epoch 5/5
1/70 [..............................] - ETA: 2s - loss: 0.0874 - accuracy: 0.9766 3/70 [>.............................] - ETA: 1s - loss: 0.0866 - accuracy: 0.9792 4/70 [>.............................] - ETA: 2s - loss: 0.1148 - accuracy: 0.9668 6/70 [=>............................] - ETA: 2s - loss: 0.1110 - accuracy: 0.9674 8/70 [==>...........................] - ETA: 2s - loss: 0.1168 - accuracy: 0.9619 [===>..........................] - ETA: 2s - loss: 0.1121 - accuracy: 0.9656 [====>.........................] - ETA: 2s - loss: 0.1062 - accuracy: 0.9674 [=====>........................] - ETA: 2s - loss: 0.1061 - accuracy: 0.9671 [======>.......................] - ETA: 2s - loss: 0.1127 - accuracy: 0.9665 [=======>......................] - ETA: 2s - loss: 0.1109 - accuracy: 0.9675 [========>.....................] - ETA: 2s - loss: 0.1077 - accuracy: 0.9676 [========>.....................] - ETA: 2s - loss: 0.1040 - accuracy: 0.9688 [=========>....................] - ETA: 2s - loss: 0.1027 - accuracy: 0.9694 [==========>...................] - ETA: 1s - loss: 0.1029 - accuracy: 0.9688 [===========>..................] - ETA: 1s - loss: 0.1044 - accuracy: 0.9688 [============>.................] - ETA: 1s - loss: 0.1054 - accuracy: 0.9685 [============>.................] - ETA: 1s - loss: 0.1056 - accuracy: 0.9690 [=============>................] - ETA: 1s - loss: 0.1035 - accuracy: 0.9692 [==============>...............] - ETA: 1s - loss: 0.1044 - accuracy: 0.9683 [===============>..............] - ETA: 1s - loss: 0.1038 - accuracy: 0.9690 [================>.............] - ETA: 1s - loss: 0.1026 - accuracy: 0.9691 [=================>............] - ETA: 1s - loss: 0.1014 - accuracy: 0.9693 [=================>............] - ETA: 1s - loss: 0.1009 - accuracy: 0.9698 [==================>...........] - ETA: 1s - loss: 0.1023 - accuracy: 0.9691 [===================>..........] - ETA: 1s - loss: 0.1037 - accuracy: 0.9684 [====================>.........] - ETA: 0s - loss: 0.1048 - accuracy: 0.9681 [====================>.........] - ETA: 0s - loss: 0.1045 - accuracy: 0.9683 [=====================>........] - ETA: 0s - loss: 0.1056 - accuracy: 0.9673 [======================>.......] - ETA: 0s - loss: 0.1066 - accuracy: 0.9668 [=======================>......] - ETA: 0s - loss: 0.1068 - accuracy: 0.9667 [=======================>......] - ETA: 0s - loss: 0.1058 - accuracy: 0.9671 [========================>.....] - ETA: 0s - loss: 0.1072 - accuracy: 0.9673 [=========================>....] - ETA: 0s - loss: 0.1073 - accuracy: 0.9672 [==========================>...] - ETA: 0s - loss: 0.1069 - accuracy: 0.9673 [===========================>..] - ETA: 0s - loss: 0.1073 - accuracy: 0.9674 [============================>.] - ETA: 0s - loss: 0.1073 - accuracy: 0.9671 [============================>.] - ETA: 0s - loss: 0.1075 - accuracy: 0.9669
Learning rate for epoch 5 is 9.999999747378752e-05
 [==============================] - 4s 60ms/step - loss: 0.1068 - accuracy: 0.9672 - lr: 1.0000e-04
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.
2022-06-20 16:59:48.538693: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
2022-06-20 16:59:48.602809: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.
2022-06-20 16:59:50.517948: E tensorflow/core/common_runtime/base_collective_executor.cc:249] BaseCollectiveExecutor::StartAbort UNAVAILABLE: Error reported from /job:worker/task:0: Task /job:worker/replica:0/task:0 heartbeat timeout. This indicates that the remote task has failed, got preempted, or crashed unexpectedly. [type.googleapis.com/tensorflow.CoordinationServiceError='\""\x08\n\x06worker']
2022-06-20 16:59:50.519603: E tensorflow/core/common_runtime/ring_alg.cc:290] Aborting RingReduce with UNAVAILABLE: Collective ops is aborted by: Error reported from /job:worker/task:0: Task /job:worker/replica:0/task:0 heartbeat timeout. This indicates that the remote task has failed, got preempted, or crashed unexpectedly.
The error could be from a previous operation. Restart your program to reset. [type.googleapis.com/tensorflow.DerivedStatus='']



Woker Logs:
Epoch 1/5
1/70 [..............................] - ETA: 3:40 - loss: 2.3142 - accuracy: 0.0703 3/70 [>.............................] - ETA: 2s - loss: 2.2875 - accuracy: 0.1615  4/70 [>.............................] - ETA: 3s - loss: 2.2796 - accuracy: 0.1758 6/70 [=>............................] - ETA: 3s - loss: 2.2574 - accuracy: 0.2096 8/70 [==>...........................] - ETA: 2s - loss: 2.2298 - accuracy: 0.2217 [===>..........................] - ETA: 2s - loss: 2.1922 - accuracy: 0.2641 [====>.........................] - ETA: 2s - loss: 2.1497 - accuracy: 0.2995 [=====>........................] - ETA: 2s - loss: 2.1005 - accuracy: 0.3276 [=====>........................] - ETA: 2s - loss: 2.0723 - accuracy: 0.3396 [======>.......................] - ETA: 2s - loss: 2.0074 - accuracy: 0.3755 [=======>......................] - ETA: 2s - loss: 1.9356 - accuracy: 0.4042 [========>.....................] - ETA: 2s - loss: 1.8638 - accuracy: 0.4308 [========>.....................] - ETA: 2s - loss: 1.7819 - accuracy: 0.4606 [=========>....................] - ETA: 2s - loss: 1.7005 - accuracy: 0.4872 [==========>...................] - ETA: 2s - loss: 1.6295 - accuracy: 0.5078 [===========>..................] - ETA: 1s - loss: 1.5604 - accuracy: 0.5277 [============>.................] - ETA: 1s - loss: 1.5001 - accuracy: 0.5436 [=============>................] - ETA: 1s - loss: 1.4447 - accuracy: 0.5616 [==============>...............] - ETA: 1s - loss: 1.3976 - accuracy: 0.5759 [==============>...............] - ETA: 1s - loss: 1.3543 - accuracy: 0.5883 [===============>..............] - ETA: 1s - loss: 1.3339 - accuracy: 0.5956 [================>.............] - ETA: 1s - loss: 1.2934 - accuracy: 0.6076 [=================>............] - ETA: 1s - loss: 1.2606 - accuracy: 0.6176 [=================>............] - ETA: 1s - loss: 1.2274 - accuracy: 0.6268 [==================>...........] - ETA: 1s - loss: 1.1959 - accuracy: 0.6360 [===================>..........] - ETA: 1s - loss: 1.1820 - accuracy: 0.6408 [====================>.........] - ETA: 1s - loss: 1.1469 - accuracy: 0.6511 [====================>.........] - ETA: 0s - loss: 1.1240 - accuracy: 0.6596 [=====================>........] - ETA: 0s - loss: 1.1105 - accuracy: 0.6641 [======================>.......] - ETA: 0s - loss: 1.0850 - accuracy: 0.6725 [=======================>......] - ETA: 0s - loss: 1.0613 - accuracy: 0.6791 [=======================>......] - ETA: 0s - loss: 1.0397 - accuracy: 0.6851 [========================>.....] - ETA: 0s - loss: 1.0182 - accuracy: 0.6911 [=========================>....] - ETA: 0s - loss: 1.0000 - accuracy: 0.6968 [==========================>...] - ETA: 0s - loss: 0.9812 - accuracy: 0.7023 [==========================>...] - ETA: 0s - loss: 0.9741 - accuracy: 0.7048 [===========================>..] - ETA: 0s - loss: 0.9530 - accuracy: 0.7115 [============================>.] - ETA: 0s - loss: 0.9376 - accuracy: 0.7164
Learning rate for epoch 1 is 0.0010000000474974513
 [==============================] - 7s 50ms/step - loss: 0.9276 - accuracy: 0.7196 - lr: 0.0010
Epoch 2/5
1/70 [..............................] - ETA: 2s - loss: 0.4861 - accuracy: 0.8438 3/70 [>.............................] - ETA: 2s - loss: 0.3831 - accuracy: 0.8932 5/70 [=>............................] - ETA: 2s - loss: 0.3363 - accuracy: 0.9047 7/70 [==>...........................] - ETA: 2s - loss: 0.3478 - accuracy: 0.8996 9/70 [==>...........................] - ETA: 2s - loss: 0.3354 - accuracy: 0.9054 [===>..........................] - ETA: 2s - loss: 0.3156 - accuracy: 0.9098 [====>.........................] - ETA: 2s - loss: 0.3102 - accuracy: 0.9087 [=====>........................] - ETA: 2s - loss: 0.3115 - accuracy: 0.9057 [======>.......................] - ETA: 2s - loss: 0.3115 - accuracy: 0.9041 [=======>......................] - ETA: 2s - loss: 0.3077 - accuracy: 0.9043 [========>.....................] - ETA: 2s - loss: 0.3063 - accuracy: 0.9034 [=========>....................] - ETA: 2s - loss: 0.3023 - accuracy: 0.9053 [==========>...................] - ETA: 1s - loss: 0.3004 - accuracy: 0.9062 [===========>..................] - ETA: 1s - loss: 0.2929 - accuracy: 0.9089 [============>.................] - ETA: 1s - loss: 0.2900 - accuracy: 0.9100 [=============>................] - ETA: 1s - loss: 0.2925 - accuracy: 0.9096 [==============>...............] - ETA: 1s - loss: 0.2919 - accuracy: 0.9103 [==============>...............] - ETA: 1s - loss: 0.2908 - accuracy: 0.9111 [===============>..............] - ETA: 1s - loss: 0.2920 - accuracy: 0.9127 [================>.............] - ETA: 1s - loss: 0.2892 - accuracy: 0.9143 [=================>............] - ETA: 1s - loss: 0.2850 - accuracy: 0.9163 [=================>............] - ETA: 1s - loss: 0.2825 - accuracy: 0.9167 [==================>...........] - ETA: 1s - loss: 0.2807 - accuracy: 0.9166 [===================>..........] - ETA: 0s - loss: 0.2778 - accuracy: 0.9178 [====================>.........] - ETA: 0s - loss: 0.2747 - accuracy: 0.9187 [=====================>........] - ETA: 0s - loss: 0.2718 - accuracy: 0.9187 [=====================>........] - ETA: 0s - loss: 0.2717 - accuracy: 0.9188 [======================>.......] - ETA: 0s - loss: 0.2709 - accuracy: 0.9196 [=======================>......] - ETA: 0s - loss: 0.2678 - accuracy: 0.9204 [========================>.....] - ETA: 0s - loss: 0.2679 - accuracy: 0.9199 [=========================>....] - ETA: 0s - loss: 0.2643 - accuracy: 0.9211 [==========================>...] - ETA: 0s - loss: 0.2626 - accuracy: 0.9215 [==========================>...] - ETA: 0s - loss: 0.2616 - accuracy: 0.9219 [===========================>..] - ETA: 0s - loss: 0.2602 - accuracy: 0.9229 [============================>.] - ETA: 0s - loss: 0.2584 - accuracy: 0.9234 [==============================] - ETA: 0s - loss: 0.2569 - accuracy: 0.9237
Learning rate for epoch 2 is 0.0010000000474974513
 [==============================] - 3s 46ms/step - loss: 0.2569 - accuracy: 0.9237 - lr: 0.0010
Epoch 3/5
1/70 [..............................] - ETA: 2s - loss: 0.1778 - accuracy: 0.9531 3/70 [>.............................] - ETA: 1s - loss: 0.1472 - accuracy: 0.9583 4/70 [>.............................] - ETA: 2s - loss: 0.1550 - accuracy: 0.9512 6/70 [=>............................] - ETA: 2s - loss: 0.1757 - accuracy: 0.9440 7/70 [==>...........................] - ETA: 2s - loss: 0.1703 - accuracy: 0.9475 9/70 [==>...........................] - ETA: 2s - loss: 0.1669 - accuracy: 0.9514 [===>..........................] - ETA: 2s - loss: 0.1679 - accuracy: 0.9503 [====>.........................] - ETA: 2s - loss: 0.1648 - accuracy: 0.9513 [=====>........................] - ETA: 2s - loss: 0.1605 - accuracy: 0.9526 [======>.......................] - ETA: 2s - loss: 0.1615 - accuracy: 0.9531 [=======>......................] - ETA: 2s - loss: 0.1605 - accuracy: 0.9515 [=======>......................] - ETA: 2s - loss: 0.1616 - accuracy: 0.9508 [========>.....................] - ETA: 2s - loss: 0.1583 - accuracy: 0.9513 [=========>....................] - ETA: 2s - loss: 0.1603 - accuracy: 0.9508 [==========>...................] - ETA: 1s - loss: 0.1603 - accuracy: 0.9501 [===========>..................] - ETA: 1s - loss: 0.1598 - accuracy: 0.9506 [===========>..................] - ETA: 1s - loss: 0.1596 - accuracy: 0.9510 [============>.................] - ETA: 1s - loss: 0.1614 - accuracy: 0.9509 [=============>................] - ETA: 1s - loss: 0.1596 - accuracy: 0.9517 [==============>...............] - ETA: 1s - loss: 0.1578 - accuracy: 0.9527 [==============>...............] - ETA: 1s - loss: 0.1608 - accuracy: 0.9523 [===============>..............] - ETA: 1s - loss: 0.1598 - accuracy: 0.9531 [================>.............] - ETA: 1s - loss: 0.1579 - accuracy: 0.9537 [=================>............] - ETA: 1s - loss: 0.1590 - accuracy: 0.9526 [==================>...........] - ETA: 1s - loss: 0.1585 - accuracy: 0.9530 [==================>...........] - ETA: 1s - loss: 0.1573 - accuracy: 0.9533 [===================>..........] - ETA: 0s - loss: 0.1599 - accuracy: 0.9526 [====================>.........] - ETA: 0s - loss: 0.1593 - accuracy: 0.9523 [=====================>........] - ETA: 0s - loss: 0.1574 - accuracy: 0.9528 [======================>.......] - ETA: 0s - loss: 0.1551 - accuracy: 0.9540 [======================>.......] - ETA: 0s - loss: 0.1538 - accuracy: 0.9544 [=======================>......] - ETA: 0s - loss: 0.1538 - accuracy: 0.9546 [========================>.....] - ETA: 0s - loss: 0.1542 - accuracy: 0.9544 [=========================>....] - ETA: 0s - loss: 0.1533 - accuracy: 0.9547 [==========================>...] - ETA: 0s - loss: 0.1526 - accuracy: 0.9550 [==========================>...] - ETA: 0s - loss: 0.1510 - accuracy: 0.9555 [===========================>..] - ETA: 0s - loss: 0.1508 - accuracy: 0.9556 [============================>.] - ETA: 0s - loss: 0.1504 - accuracy: 0.9557 [==============================] - ETA: 0s - loss: 0.1491 - accuracy: 0.9560
Learning rate for epoch 3 is 0.0010000000474974513
 [==============================] - 3s 46ms/step - loss: 0.1491 - accuracy: 0.9560 - lr: 0.0010
Epoch 4/5
1/70 [..............................] - ETA: 2s - loss: 0.1289 - accuracy: 0.9609 3/70 [>.............................] - ETA: 1s - loss: 0.1608 - accuracy: 0.9531 4/70 [>.............................] - ETA: 2s - loss: 0.1567 - accuracy: 0.9590 6/70 [=>............................] - ETA: 2s - loss: 0.1553 - accuracy: 0.9583 8/70 [==>...........................] - ETA: 2s - loss: 0.1443 - accuracy: 0.9590 [===>..........................] - ETA: 2s - loss: 0.1307 - accuracy: 0.9625 [====>.........................] - ETA: 2s - loss: 0.1301 - accuracy: 0.9622 [=====>........................] - ETA: 2s - loss: 0.1256 - accuracy: 0.9604 [======>.......................] - ETA: 2s - loss: 0.1240 - accuracy: 0.9609 [=======>......................] - ETA: 2s - loss: 0.1201 - accuracy: 0.9634 [========>.....................] - ETA: 2s - loss: 0.1209 - accuracy: 0.9617 [========>.....................] - ETA: 2s - loss: 0.1210 - accuracy: 0.9626 [=========>....................] - ETA: 1s - loss: 0.1198 - accuracy: 0.9631 [==========>...................] - ETA: 1s - loss: 0.1216 - accuracy: 0.9627 [===========>..................] - ETA: 1s - loss: 0.1204 - accuracy: 0.9634 [===========>..................] - ETA: 1s - loss: 0.1194 - accuracy: 0.9641 [============>.................] - ETA: 1s - loss: 0.1189 - accuracy: 0.9641 [=============>................] - ETA: 1s - loss: 0.1186 - accuracy: 0.9642 [==============>...............] - ETA: 1s - loss: 0.1200 - accuracy: 0.9640 [===============>..............] - ETA: 1s - loss: 0.1205 - accuracy: 0.9640 [===============>..............] - ETA: 1s - loss: 0.1213 - accuracy: 0.9635 [================>.............] - ETA: 1s - loss: 0.1205 - accuracy: 0.9636 [=================>............] - ETA: 1s - loss: 0.1206 - accuracy: 0.9637 [==================>...........] - ETA: 1s - loss: 0.1220 - accuracy: 0.9635 [===================>..........] - ETA: 1s - loss: 0.1218 - accuracy: 0.9634 [===================>..........] - ETA: 0s - loss: 0.1233 - accuracy: 0.9635 [====================>.........] - ETA: 0s - loss: 0.1217 - accuracy: 0.9639 [=====================>........] - ETA: 0s - loss: 0.1213 - accuracy: 0.9641 [======================>.......] - ETA: 0s - loss: 0.1233 - accuracy: 0.9637 [=======================>......] - ETA: 0s - loss: 0.1228 - accuracy: 0.9636 [=======================>......] - ETA: 0s - loss: 0.1219 - accuracy: 0.9639 [========================>.....] - ETA: 0s - loss: 0.1200 - accuracy: 0.9646 [=========================>....] - ETA: 0s - loss: 0.1189 - accuracy: 0.9650 [==========================>...] - ETA: 0s - loss: 0.1180 - accuracy: 0.9653 [==========================>...] - ETA: 0s - loss: 0.1175 - accuracy: 0.9650 [===========================>..] - ETA: 0s - loss: 0.1166 - accuracy: 0.9650 [============================>.] - ETA: 0s - loss: 0.1156 - accuracy: 0.9649
Learning rate for epoch 4 is 9.999999747378752e-05
 [==============================] - 3s 46ms/step - loss: 0.1147 - accuracy: 0.9652 - lr: 1.0000e-04
Epoch 5/5
1/70 [..............................] - ETA: 2s - loss: 0.0874 - accuracy: 0.9766 3/70 [>.............................] - ETA: 1s - loss: 0.0866 - accuracy: 0.9792 4/70 [>.............................] - ETA: 2s - loss: 0.1148 - accuracy: 0.9668 6/70 [=>............................] - ETA: 2s - loss: 0.1110 - accuracy: 0.9674 8/70 [==>...........................] - ETA: 2s - loss: 0.1168 - accuracy: 0.9619 [===>..........................] - ETA: 2s - loss: 0.1121 - accuracy: 0.9656 [====>.........................] - ETA: 2s - loss: 0.1062 - accuracy: 0.9674 [=====>........................] - ETA: 2s - loss: 0.1061 - accuracy: 0.9671 [======>.......................] - ETA: 2s - loss: 0.1127 - accuracy: 0.9665 [=======>......................] - ETA: 2s - loss: 0.1109 - accuracy: 0.9675 [========>.....................] - ETA: 2s - loss: 0.1077 - accuracy: 0.9676 [========>.....................] - ETA: 2s - loss: 0.1040 - accuracy: 0.9688 [=========>....................] - ETA: 2s - loss: 0.1027 - accuracy: 0.9694 [==========>...................] - ETA: 1s - loss: 0.1029 - accuracy: 0.9688 [===========>..................] - ETA: 1s - loss: 0.1044 - accuracy: 0.9688 [============>.................] - ETA: 1s - loss: 0.1054 - accuracy: 0.9685 [============>.................] - ETA: 1s - loss: 0.1056 - accuracy: 0.9690 [=============>................] - ETA: 1s - loss: 0.1037 - accuracy: 0.9695 [==============>...............] - ETA: 1s - loss: 0.1029 - accuracy: 0.9692 [==============>...............] - ETA: 1s - loss: 0.1044 - accuracy: 0.9683 [===============>..............] - ETA: 1s - loss: 0.1038 - accuracy: 0.9690 [================>.............] - ETA: 1s - loss: 0.1026 - accuracy: 0.9691 [=================>............] - ETA: 1s - loss: 0.1014 - accuracy: 0.9693 [=================>............] - ETA: 1s - loss: 0.1009 - accuracy: 0.9698 [==================>...........] - ETA: 1s - loss: 0.1023 - accuracy: 0.9691 [===================>..........] - ETA: 1s - loss: 0.1037 - accuracy: 0.9684 [====================>.........] - ETA: 0s - loss: 0.1048 - accuracy: 0.9681 [====================>.........] - ETA: 0s - loss: 0.1045 - accuracy: 0.9683 [=====================>........] - ETA: 0s - loss: 0.1056 - accuracy: 0.9673 [======================>.......] - ETA: 0s - loss: 0.1066 - accuracy: 0.9668 [=======================>......] - ETA: 0s - loss: 0.1068 - accuracy: 0.9667 [=======================>......] - ETA: 0s - loss: 0.1058 - accuracy: 0.9671 [========================>.....] - ETA: 0s - loss: 0.1072 - accuracy: 0.9673 [=========================>....] - ETA: 0s - loss: 0.1073 - accuracy: 0.9672 [==========================>...] - ETA: 0s - loss: 0.1069 - accuracy: 0.9673 [===========================>..] - ETA: 0s - loss: 0.1073 - accuracy: 0.9674 [============================>.] - ETA: 0s - loss: 0.1073 - accuracy: 0.9671 [============================>.] - ETA: 0s - loss: 0.1075 - accuracy: 0.9669
Learning rate for epoch 5 is 9.999999747378752e-05
 [==============================] - 3s 46ms/step - loss: 0.1068 - accuracy: 0.9672 - lr: 1.0000e-04
2022-06-20 16:59:37.985293: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
2022-06-20 16:59:38.034121: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading."
56512,"What is the role of ""WriteToInputTensor"" function in tf lite gpu delegate example?","Hi,
I saw the tensorflow lite [gpu delegate tutorial](https://www.tensorflow.org/lite/performance/gpu_advanced?hl=ko) providing gpu delegate inference on android C++ code snippet:
```
// Set up interpreter.
auto model = FlatBufferModel::BuildFromFile(model_path);
if (!model) return false;
ops::builtin::BuiltinOpResolver op_resolver;
std::unique_ptr<Interpreter> interpreter;
InterpreterBuilder(*model, op_resolver)(&interpreter);

// NEW: Prepare GPU delegate.
auto* delegate = TfLiteGpuDelegateV2Create(/*default options=*/nullptr);
if (interpreter->ModifyGraphWithDelegate(delegate) != kTfLiteOk) return false;

// Run inference.
WriteToInputTensor(interpreter->typed_input_tensor<float>(0));
if (interpreter->Invoke() != kTfLiteOk) return false;
ReadFromOutputTensor(interpreter->typed_output_tensor<float>(0));

// NEW: Clean up.
TfLiteGpuDelegateV2Delete(delegate);
```
But I don't understand the fuction `WriteToInputTensor` & `ReadFromOutputTensor`. What is the role of these function? And where can I find it? I couldn't find any code or information in tenforflow repo."
56511,XLA doesn't have a scatter emitter on CPU,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

master

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

XLA doesn't has a CPU emitter for scatter so it will emit a slow serial XLA while on CPU:
https://github.com/tensorflow/tensorflow/blob/a94cff5861a7560768faa798d7150cff35e9472a/tensorflow/compiler/xla/service/cpu/ir_emitter.cc#L1931-L1933


### Standalone code to reproduce the issue

```python
import tensorflow as tf

@tf.function(jit_compile=True)
def test_scatter(indices, updates, shape):
  scatter = tf.scatter_nd(indices, updates, shape)
  return scatter
indices = tf.constant([[0], [2]])
updates = tf.constant([[[5, 5, 5, 5], [6, 6, 6, 6],
                        [7, 7, 7, 7], [8, 8, 8, 8]],
                        [[5, 5, 5, 5], [6, 6, 6, 6],
                        [7, 7, 7, 7], [8, 8, 8, 8]]])
shape = tf.constant([4, 4, 4])
print(test_scatter.experimental_get_compiler_ir(indices, updates, shape)('optimized_hlo'))
```


### Relevant log output

CPU 

```mlir
HloModule a_inference_test_scatter_113__.17, alias_passthrough_params=true

%fused_computation (param_0: s32[4,4,4], param_1.3: s32[2,4,4], param_2.4: s32[], param_3.4: pred[], param_4.6: s32[2]) -> s32[4,4,4] {
  %param_0 = s32[4,4,4]{2,1,0} parameter(0)
  %param_3.4 = pred[] parameter(3)
  %broadcast.14 = pred[1,4,4]{2,1,0} broadcast(pred[] %param_3.4), dimensions={}
  %param_4.6 = s32[2]{0} parameter(4)
  %param_2.4 = s32[] parameter(2)
  %dynamic-slice.5 = s32[1]{0} dynamic-slice(s32[2]{0} %param_4.6, s32[] %param_2.4), dynamic_slice_sizes={1}
  %reshape.26 = s32[] reshape(s32[1]{0} %dynamic-slice.5)
  %constant.21 = s32[] constant(0)
  %dynamic-slice.4 = s32[1,4,4]{2,1,0} dynamic-slice(s32[4,4,4]{2,1,0} %param_0, s32[] %reshape.26, s32[] %constant.21, s32[] %constant.21), dynamic_slice_sizes={1,4,4}
  %param_1.3 = s32[2,4,4]{2,1,0} parameter(1)
  %dynamic-slice.3 = s32[1,4,4]{2,1,0} dynamic-slice(s32[2,4,4]{2,1,0} %param_1.3, s32[] %param_2.4, s32[] %constant.21, s32[] %constant.21), dynamic_slice_sizes={1,4,4}
  %add.2 = s32[1,4,4]{2,1,0} add(s32[1,4,4]{2,1,0} %dynamic-slice.4, s32[1,4,4]{2,1,0} %dynamic-slice.3)
  %select.1 = s32[1,4,4]{2,1,0} select(pred[1,4,4]{2,1,0} %broadcast.14, s32[1,4,4]{2,1,0} %add.2, s32[1,4,4]{2,1,0} %dynamic-slice.4)
  ROOT %dynamic-update-slice.1 = s32[4,4,4]{2,1,0} dynamic-update-slice(s32[4,4,4]{2,1,0} %param_0, s32[1,4,4]{2,1,0} %select.1, s32[] %reshape.26, s32[] %constant.21, s32[] %constant.21)
}

%and.reduce_sub_computation (lhs: pred[], rhs: pred[]) -> pred[] {
  %lhs = pred[] parameter(0)
  %rhs = pred[] parameter(1)
  ROOT %and = pred[] and(pred[] %lhs, pred[] %rhs)
}

%fused_computation.1 (param_0.4: s32[3]) -> pred[] {
  %constant.24 = s32[] constant(0)
  %broadcast.15 = s32[3]{0} broadcast(s32[] %constant.24), dimensions={}
  %param_0.4 = s32[3]{0} parameter(0)
  %compare.4 = pred[3]{0} compare(s32[3]{0} %broadcast.15, s32[3]{0} %param_0.4), direction=LE
  %constant.23 = s32[3]{0} constant({3, 0, 0})
  %compare.3 = pred[3]{0} compare(s32[3]{0} %constant.23, s32[3]{0} %param_0.4), direction=GE
  %and.2 = pred[3]{0} and(pred[3]{0} %compare.4, pred[3]{0} %compare.3)
  %constant.22 = pred[] constant(true)
  ROOT %reduce.1 = pred[] reduce(pred[3]{0} %and.2, pred[] %constant.22), dimensions={0}, to_apply=%and.reduce_sub_computation
}

%fused_computation.2 (param_0.7: s32[2], param_1.13: s32[]) -> s32[3] {
  %param_0.7 = s32[2]{0} parameter(0)
  %param_1.13 = s32[] parameter(1)
  %dynamic-slice.6 = s32[1]{0} dynamic-slice(s32[2]{0} %param_0.7, s32[] %param_1.13), dynamic_slice_sizes={1}
  %constant.25 = s32[1]{0} constant({0})
  ROOT %concatenate.1 = s32[3]{0} concatenate(s32[1]{0} %dynamic-slice.6, s32[1]{0} %constant.25, s32[1]{0} %constant.25), dimensions={0}
}

%while_body (param.0: (s32[], s32[4,4,4], s32[2], s32[2,4,4])) -> (s32[], s32[4,4,4], s32[2], s32[2,4,4]) {
  %param.0 = (s32[], s32[4,4,4]{2,1,0}, s32[2]{0}, s32[2,4,4]{2,1,0}) parameter(0)
  %get-tuple-element.12 = s32[] get-tuple-element((s32[], s32[4,4,4]{2,1,0}, s32[2]{0}, s32[2,4,4]{2,1,0}) %param.0), index=0
  %copy.3 = s32[] copy(s32[] %get-tuple-element.12)
  %constant.4 = s32[] constant(1)
  %add = s32[] add(s32[] %copy.3, s32[] %constant.4)
  %get-tuple-element.13 = s32[4,4,4]{2,1,0} get-tuple-element((s32[], s32[4,4,4]{2,1,0}, s32[2]{0}, s32[2,4,4]{2,1,0}) %param.0), index=1
  %get-tuple-element.20 = s32[2,4,4]{2,1,0} get-tuple-element((s32[], s32[4,4,4]{2,1,0}, s32[2]{0}, s32[2,4,4]{2,1,0}) %param.0), index=3
  %get-tuple-element.19 = s32[2]{0} get-tuple-element((s32[], s32[4,4,4]{2,1,0}, s32[2]{0}, s32[2,4,4]{2,1,0}) %param.0), index=2
  %fusion.2 = s32[3]{0} fusion(s32[2]{0} %get-tuple-element.19, s32[] %copy.3), kind=kLoop, calls=%fused_computation.2
  %fusion.1 = pred[] fusion(s32[3]{0} %fusion.2), kind=kLoop, calls=%fused_computation.1
  %fusion = s32[4,4,4]{2,1,0} fusion(s32[4,4,4]{2,1,0} %get-tuple-element.13, s32[2,4,4]{2,1,0} %get-tuple-element.20, s32[] %copy.3, pred[] %fusion.1, s32[2]{0} %get-tuple-element.19), kind=kLoop, calls=%fused_computation
  ROOT %tuple.5 = (s32[], s32[4,4,4]{2,1,0}, s32[2]{0}, s32[2,4,4]{2,1,0}) tuple(s32[] %add, s32[4,4,4]{2,1,0} %fusion, s32[2]{0} %get-tuple-element.19, s32[2,4,4]{2,1,0} %get-tuple-element.20)
}

%while_cond (param.1: (s32[], s32[4,4,4], s32[2], s32[2,4,4])) -> pred[] {
  %param.1 = (s32[], s32[4,4,4]{2,1,0}, s32[2]{0}, s32[2,4,4]{2,1,0}) parameter(0)
  %get-tuple-element.4 = s32[] get-tuple-element((s32[], s32[4,4,4]{2,1,0}, s32[2]{0}, s32[2,4,4]{2,1,0}) %param.1), index=0
  %constant.13 = s32[] constant(2)
  ROOT %compare.2 = pred[] compare(s32[] %get-tuple-element.4, s32[] %constant.13), direction=LT
}

ENTRY %a_inference_test_scatter_113__.17 (arg0.1: s32[2,1], arg1.2: s32[2,4,4]) -> s32[4,4,4] {
  %constant.7 = s32[] constant(0), metadata={op_type=""ScatterNd"" op_name=""ScatterNd"" source_file=""<ipython-input-10-8db6e7aac50a>"" source_line=5}
  %copy.7 = s32[] copy(s32[] %constant.7)
  %broadcast.8 = s32[4,4,4]{2,1,0} broadcast(s32[] %copy.7), dimensions={}, metadata={op_type=""ScatterNd"" op_name=""ScatterNd"" source_file=""<ipython-input-10-8db6e7aac50a>"" source_line=5}
  %arg0.1 = s32[2,1]{1,0} parameter(0), parameter_replication={false}, metadata={op_name=""XLA_Args""}
  %bitcast = s32[2]{0} bitcast(s32[2,1]{1,0} %arg0.1)
  %arg1.2 = s32[2,4,4]{2,1,0} parameter(1), parameter_replication={false}, metadata={op_name=""XLA_Args""}
  %tuple.3 = (s32[], s32[4,4,4]{2,1,0}, s32[2]{0}, s32[2,4,4]{2,1,0}) tuple(s32[] %copy.7, s32[4,4,4]{2,1,0} %broadcast.8, s32[2]{0} %bitcast, s32[2,4,4]{2,1,0} %arg1.2)
  %while = (s32[], s32[4,4,4]{2,1,0}, s32[2]{0}, s32[2,4,4]{2,1,0}) while((s32[], s32[4,4,4]{2,1,0}, s32[2]{0}, s32[2,4,4]{2,1,0}) %tuple.3), condition=%while_cond, body=%while_body, metadata={op_type=""ScatterNd"" op_name=""ScatterNd"" source_file=""<ipython-input-10-8db6e7aac50a>"" source_line=5}
  ROOT %get-tuple-element.16 = s32[4,4,4]{2,1,0} get-tuple-element((s32[], s32[4,4,4]{2,1,0}, s32[2]{0}, s32[2,4,4]{2,1,0}) %while), index=1, metadata={op_type=""ScatterNd"" op_name=""ScatterNd"" source_file=""<ipython-input-10-8db6e7aac50a>"" source_line=5}
}
```

GPU 
```mlir
HloModule a_inference_test_scatter_11__.17, alias_passthrough_params=true

%scatter-combiner.9 (p0.10: s32[], p1.11: s32[]) -> s32[] {
  %p0.10 = s32[] parameter(0)
  %p1.11 = s32[] parameter(1)
  ROOT %add.12 = s32[] add(s32[] %p0.10, s32[] %p1.11)
}

%fused_computation (param_0.1: s32[2,1], param_1.1: s32[2,4,4]) -> s32[4,4,4] {
  %constant_0 = s32[] constant(0), metadata={op_type=""ScatterNd"" op_name=""ScatterNd"" source_file=""<ipython-input-1-8db6e7aac50a>"" source_line=5}
  %broadcast.0 = s32[4,4,4]{2,1,0} broadcast(s32[] %constant_0), dimensions={}, metadata={op_type=""ScatterNd"" op_name=""ScatterNd"" source_file=""<ipython-input-1-8db6e7aac50a>"" source_line=5}
  %param_0.1 = s32[2,1]{1,0} parameter(0)
  %param_1.1 = s32[2,4,4]{2,1,0} parameter(1)
  ROOT %scatter.0 = s32[4,4,4]{2,1,0} scatter(s32[4,4,4]{2,1,0} %broadcast.0, s32[2,1]{1,0} %param_0.1, s32[2,4,4]{2,1,0} %param_1.1), update_window_dims={1,2}, inserted_window_dims={0}, scatter_dims_to_operand_dims={0}, index_vector_dim=1, to_apply=%scatter-combiner.9, metadata={op_type=""ScatterNd"" op_name=""ScatterNd"" source_file=""<ipython-input-1-8db6e7aac50a>"" source_line=5}
}

ENTRY %a_inference_test_scatter_11__.17 (arg0.1: s32[2,1], arg1.2: s32[2,4,4]) -> s32[4,4,4] {
  %arg0.1 = s32[2,1]{1,0} parameter(0), parameter_replication={false}, metadata={op_name=""XLA_Args""}
  %arg1.2 = s32[2,4,4]{2,1,0} parameter(1), parameter_replication={false}, metadata={op_name=""XLA_Args""}
  ROOT %fusion = s32[4,4,4]{2,1,0} fusion(s32[2,1]{1,0} %arg0.1, s32[2,4,4]{2,1,0} %arg1.2), kind=kInput, calls=%fused_computation, metadata={op_type=""ScatterNd"" op_name=""ScatterNd"" source_file=""<ipython-input-1-8db6e7aac50a>"" source_line=5}
}
```
</details>"
56509,Create Tensor from even and odd Tensors,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

binary

### Tensorflow Version

2.9.0

### Custom Code

Yes

### OS Platform and Distribution

macOS 12.4

### Mobile device

_No response_

### Python version

Conda Python 3.9.10

### Bazel version

_No response_

### GCC/Compiler version

Clang 11.1.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

Apple M1

### Current Behaviour?

```shell
Hello,

how can I create full Tensor from even and odd Tensors? I don't know the batch_size, tf.shape(inputs)[0] == None, because I use Dataset API. I cannot create tf.Variable and use assign() to do it. I know that Tensor does not support assignment, but exist any alternative
```


### Standalone code to reproduce the issue

```shell
class PositionalEmbedding(Layer):
    def __init__(self, units, dropout_rate, **kwargs):
        super(PositionalEmbedding, self).__init__(**kwargs)

        self.units = units

        self.projection = Dense(units, kernel_initializer=TruncatedNormal(stddev=0.02))
        self.dropout = Dropout(rate=dropout_rate)

    def call(self, inputs, training):
        x = self.projection(inputs[:, :, :, 3:])

        A = inputs[:, :, :, 2]
        x = x + self.positional_encoding(A, self.units)

        B = inputs[:, :, :, 0]
        C = inputs[:, :, :, 1]
        x = x + self.positional_encoding(B, self.units)
        x = x + self.positional_encoding(C, self.units)

        return self.dropout(x, training=training)

    def positional_encoding(self, position, d_model, n=10000):
        angle_rads = position[:, :, :, tf.newaxis] / (
            tf.math.pow(n, (
                2 * (
                    tf.range(d_model, dtype=tf.float32)[tf.newaxis, tf.newaxis, tf.newaxis, :]//2
                )
            ) / d_model)
        )

        # apply sin to even indices in the array; 2i
        angle_rads[:, 0::2] = tf.sin(angle_rads[:, 0::2])

        # apply cos to odd indices in the array; 2i+1
        angle_rads[:, 1::2] = tf.cos(angle_rads[:, 1::2])

        return angle_rads

x = PositionalEmbedding(32, 0.1)
print(x(tf.random.normal((8, 7, 15, 4))))
```


### Relevant log output

```shell
TypeError: Exception encountered when calling layer ""positional_embedding_55"" (type PositionalEmbedding).

'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment
```
</details>"
56508,Dataset loses cardinality/length after prefetch to gpu ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.8.2

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hi Guys,

I've noticed that I lose dataset cardinality when prefetching data to gpu. 

This forces me to call assert_cardinality, which is a problem because:
- operation runs on CPU, so instead of buffering data for training in GPU, I'll be buffering data that is sent to CPU for assert_cardinality and then being send on demand to GPU
- if I don't call assert_cardinality, model.fit runs endlessly since it can't determine the number of steps per epoch
```


### Standalone code to reproduce the issue

```shell
I've created this colab that reproduces the issue I'm telling
https://colab.research.google.com/drive/1I7Z4BwTuVe4qc_TrtfzdcdUNrBw9vWnB?usp=sharing

There's also an unexpected problem regarding the length without GPU (it's returning 24 when 100 was the expected). 

Thanks for the help in advance!
```


### Relevant log output

_No response_</details>"
56507,Create Tensor from even and odd Tensors,"Hello,

  how can I create full Tensor from even and odd Tensors? I don't know the batch_size, `tf.shape(inputs)[0] == None`, because I use Dataset API. I cannot create `tf.Variable` and use `assign()` to do it.

Here is the reproducible code:
```python
class PositionalEmbedding(Layer):
    def __init__(self, units, dropout_rate, **kwargs):
        super(PositionalEmbedding, self).__init__(**kwargs)

        self.units = units

        self.projection = Dense(units, kernel_initializer=TruncatedNormal(stddev=0.02))
        self.dropout = Dropout(rate=dropout_rate)

    def call(self, inputs, training):
        x = self.projection(inputs[:, :, :, 3:])

        A = inputs[:, :, :, 2]
        x = x + self.positional_encoding(A, self.units)

        B = inputs[:, :, :, 0]
        C = inputs[:, :, :, 1]
        x = x + self.positional_encoding(B, self.units)
        x = x + self.positional_encoding(C, self.units)

        return self.dropout(x, training=training)

    def positional_encoding(self, position, d_model, n=10000):
        angle_rads = position[:, :, :, tf.newaxis] / (
            tf.math.pow(n, (
                2 * (
                    tf.range(d_model, dtype=tf.float32)[tf.newaxis, tf.newaxis, tf.newaxis, :]//2
                )
            ) / d_model)
        )

        # apply sin to even indices in the array; 2i
        angle_rads[:, 0::2] = tf.sin(angle_rads[:, 0::2])

        # apply cos to odd indices in the array; 2i+1
        angle_rads[:, 1::2] = tf.cos(angle_rads[:, 1::2])

        return angle_rads

x = PositionalEmbedding(32, 0.1)
print(x(tf.random.normal((8, 7, 15, 4))))
```

Now, the issue is - (I know that Tensor does not support assignment):
```
TypeError: Exception encountered when calling layer ""positional_embedding_55"" (type PositionalEmbedding).

'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment
```

Thanks."
56506,tf.keras.backend.ctc_decode merging repeated characters by default,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.7.0

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The current built-in keras wrapper function does not have an optional agrument ""merge_repeated"" to pass along to the underlying tensorflow function. I believe this issue was solved in standalone keras as per https://github.com/keras-team/keras/issues/12238
```


### Standalone code to reproduce the issue

```shell
Check https://github.com/keras-team/keras/issues/12238 for the reproduction code
```


### Relevant log output

_No response_</details>"
56505,Type INT16 is unsupported by op Rsqrt.Node number 12 (RSQRT) failed to prepare.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows10
- TensorFlow installed from (source or binary):                   tensorflow 2.9.1
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

```
RuntimeError: tensorflow/lite/kernels/elementwise.cc:88 Type INT16 is unsupported by op Rsqrt.Node number 12 (RSQRT) failed to prepare.
```

**Standalone code to reproduce the issue** 
First,I convert the input/output of the model from float32 into int16. Second,I load the new model.Third,I allocate tensors.Then the error reports after run the code ""interpreter_2.allocate_tensors()"" in real_time_processing_tf_lite.py.
```
interpreter_2 = tf.lite.Interpreter(model_path='F:\\testmodel_2.tflite')
interpreter_2.allocate_tensors()
```
The reported error is as following:
`RuntimeError: tensorflow/lite/kernels/elementwise.cc:88 Type INT16 is unsupported by op Rsqrt.Node number 12 (RSQRT) failed to prepare.`


**Also, please include a link to a GraphDef or the model if possible.**
The target model is here https://github.com/breizhn/DTLN
The inputs format of the model in the link are float32.

**Any other info / logs**

The following are my codes for converting.It has no problem:

In DTLN_model.py:

```
def representative_dataset():
    for _ in range(100):
        data1 = np.random.uniform(-1000, 1000, [1, 2, 128, 2]).astype(np.float32)
        data2 = np.random.uniform(-250, 250, [1, 1, 257]).astype(np.float32)
        #data3 = {'input_2': [data2.astype(np.float32)], 'input_3': [data1.astype(np.float32)]}
        yield [data1.astype(np.float32), data2.astype(np.float32)]

def representative_dataset2():
    for _ in range(100):
        data1 = np.random.uniform(-1000, 1000, [1, 2, 128, 2]).astype(np.float32)
        data2 = np.random.uniform(-250, 250, [1, 1, 512]).astype(np.float32)
        #data3 = {'input_2': [data2.astype(np.float32)], 'input_3': [data1.astype(np.float32)]}
        yield [data2.astype(np.float32), data1.astype(np.float32)]
```
```
if use_dynamic_range_quant:
    #+++++++++++++++++++++++++++++++++++++++++++++++++++++++++
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.target_spec.supported_ops = [
        tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]
    converter.inference_input_type = tf.int16
    converter.inference_output_type = tf.int16
    converter.representative_dataset = representative_dataset
    #++++++++++++++++++++++++++++++++++++++++++++++++++++++++
tflite_model = converter.convert()
with tf.io.gfile.GFile(target_name + '_1.tflite', 'wb') as f:
      f.write(tflite_model)
# convert second model    
converter = tf.lite.TFLiteConverter.from_keras_model(model_2)
if use_dynamic_range_quant:
    # 
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.target_spec.supported_ops = [
        tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]
    converter.inference_input_type = tf.int16
    converter.inference_output_type = tf.int16
    converter.representative_dataset = representative_dataset2
    #
tflite_model = converter.convert() 
```

"
56504,35x difference in performance between windows and linux version,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux, Windows

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

cuda_11.2.r11.2 (windows)

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The inference time of my tflite model is 35 times faster in linux (0.4 sec) than in windows (14 sec).
In both cases the same hardware is used. 
Drivers configuration: GPU is available in windows and not available in linux.
In both cases tensorflow 2.9.1 is used.
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/18AWls7w_oWdWk3bRlTdkG2U7pm_w8rn6
```


### Relevant log output

_No response_</details>"
56503,Tensorflow-lite breaks after installing Flatbuffers,"### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Ubuntu 21.04

### Bazel version

5.1.1

I installed Tensorflow lite using this guide: https://www.tensorflow.org/lite/guide/build_cmake

My TensorFlow-lite files are in `/usr/local/include` to make them available for all projects. However, Tensorflow-lite required FlatBuffers, which were not a part of the installation:
```
In file included from /usr/local/include/tensorflow/lite/model.h:21,
                 from /usr/local/include/tensorflow/lite/kernels/register.h:18,
                 from main.cpp:7:
/usr/local/include/tensorflow/lite/interpreter_builder.h:26:10: fatal error: flatbuffers/flatbuffers.h: No such file or directory
   26 | #include ""flatbuffers/flatbuffers.h""  // from @flatbuffers
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
```
After building FlatBuffers from source using CMAKE, Now I'm getting this error:
```
/usr/bin/ld: /tmp/ccH6EykD.o: warning: relocation against `_ZTVN6tflite17MutableOpResolverE' in read-only section `.text._ZN6tflite17MutableOpResolverD2Ev[_ZN6tflite17MutableOpResolverD5Ev]'
/usr/bin/ld: /tmp/ccH6EykD.o: in function `main':
main.cpp:(.text+0x34c): undefined reference to `tflite::DefaultErrorReporter()'
/usr/bin/ld: main.cpp:(.text+0x368): undefined reference to `tflite::FlatBufferModel::BuildFromFile(char const*, tflite::ErrorReporter*)'
/usr/bin/ld: main.cpp:(.text+0x377): undefined reference to `tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()'
/usr/bin/ld: main.cpp:(.text+0x3af): undefined reference to `tflite::InterpreterBuilder::InterpreterBuilder(tflite::FlatBufferModel const&, tflite::OpResolver const&, tflite::InterpreterOptions const*)'
/usr/bin/ld: main.cpp:(.text+0x3c8): undefined reference to `tflite::InterpreterBuilder::operator()(std::unique_ptr<tflite::Interpreter, std::default_delete<tflite::Interpreter> >*)'
/usr/bin/ld: main.cpp:(.text+0x3d7): undefined reference to `tflite::InterpreterBuilder::~InterpreterBuilder()'
/usr/bin/ld: main.cpp:(.text+0x5b3): undefined reference to `tflite::InterpreterBuilder::~InterpreterBuilder()'
/usr/bin/ld: /tmp/ccH6EykD.o: in function `tflite::MutableOpResolver::~MutableOpResolver()':
main.cpp:(.text._ZN6tflite17MutableOpResolverD2Ev[_ZN6tflite17MutableOpResolverD5Ev]+0x13): undefined reference to `vtable for tflite::MutableOpResolver'
/usr/bin/ld: /tmp/ccH6EykD.o: in function `std::default_delete<tflite::FlatBufferModel>::operator()(tflite::FlatBufferModel*) const':
main.cpp:(.text._ZNKSt14default_deleteIN6tflite15FlatBufferModelEEclEPS1_[_ZNKSt14default_deleteIN6tflite15FlatBufferModelEEclEPS1_]+0x22): undefined reference to `tflite::FlatBufferModel::~FlatBufferModel()'
/usr/bin/ld: /tmp/ccH6EykD.o: in function `std::default_delete<tflite::Interpreter>::operator()(tflite::Interpreter*) const':
main.cpp:(.text._ZNKSt14default_deleteIN6tflite11InterpreterEEclEPS1_[_ZNKSt14default_deleteIN6tflite11InterpreterEEclEPS1_]+0x22): undefined reference to `tflite::Interpreter::~Interpreter()'
/usr/bin/ld: /tmp/ccH6EykD.o:(.data.rel.ro._ZTVN6tflite3ops7builtin17BuiltinOpResolverE[_ZTVN6tflite3ops7builtin17BuiltinOpResolverE]+0x10): undefined reference to `tflite::MutableOpResolver::FindOp(tflite::BuiltinOperator, int) const'
/usr/bin/ld: /tmp/ccH6EykD.o:(.data.rel.ro._ZTVN6tflite3ops7builtin17BuiltinOpResolverE[_ZTVN6tflite3ops7builtin17BuiltinOpResolverE]+0x18): undefined reference to `tflite::MutableOpResolver::FindOp(char const*, int) const'
/usr/bin/ld: /tmp/ccH6EykD.o:(.data.rel.ro._ZTVN6tflite3ops7builtin17BuiltinOpResolverE[_ZTVN6tflite3ops7builtin17BuiltinOpResolverE]+0x48): undefined reference to `tflite::MutableOpResolver::MayContainUserDefinedOps() const'
/usr/bin/ld: /tmp/ccH6EykD.o:(.data.rel.ro._ZTIN6tflite3ops7builtin17BuiltinOpResolverE[_ZTIN6tflite3ops7builtin17BuiltinOpResolverE]+0x10): undefined reference to `typeinfo for tflite::MutableOpResolver'
/usr/bin/ld: warning: creating DT_TEXTREL in a PIE
collect2: error: ld returned 1 exit status
```
This is the command I use to run my code:
```
g++ -std=c++17 main.cpp src/VideoProcessing.cpp `pkg-config --libs --cflags opencv4 flatbuffers` -o result
```



### Standalone code to reproduce the issue

```shell
#include <iostream>
#include <cstdio>
#include <opencv2/opencv.hpp>
#include <opencv2/videoio.hpp>
#include ""tensorflow/lite/interpreter.h""
#include ""tensorflow/lite/kernels/register.h""
#include ""tensorflow/lite/model.h""
#include ""tensorflow/lite/model_builder.h""
#include ""tensorflow/lite/interpreter_builder.h""
#include ""tensorflow/lite/optional_debug_tools.h""


int main() {
    cv::VideoCapture cap(""trainer.mp4"");

    //Check if input video exists
    if(!cap.isOpened()){
        std::cout<<""Error opening video stream or file""<<std::endl;
        return -1;
    }

    //Create a window to show input video
    cv::namedWindow(""input video"", cv::WINDOW_NORMAL);

    //Keep playing video until video is completed
    while(true){
        cv::Mat frame;

        //Capture frame by frame
        cap >> frame;

        //If frame is empty then break the loop
        if(frame.empty()){break;}

        //Show the current frame
        imshow(""input video"", frame);

    }

    //Close window after input video is completed
    cap.release();

    //Destroy all the opened windows
    cv::destroyAllWindows();

    std::cout << ""Video file FPS: "" << fps << std::endl;
    std::cout << ""Video file size: "" << size << std::endl;

    // Load the model
    std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(""pose_landmark_full.tflite"");

    // Build the interpreter
    tflite::ops::builtin::BuiltinOpResolver resolver;
    std::unique_ptr<tflite::Interpreter> interpreter;
    tflite::InterpreterBuilder(*model, resolver)(&interpreter);

    if (interpreter == nullptr)
    {
        fprintf(stderr, ""Failed to initiate the interpreter\n"");
        exit(-1);
    }

    return 0;
}
```


### Relevant log output

_No response_</details>"
56502,Deleted question,
56501,Question on the implementation of BMM (batched matmul),"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Irrelevant
- TensorFlow installation (pip package or built from source): Irrelevant
- TensorFlow library (version, if pip package or github SHA, if built from source): Irrelevant

### 2. Code

Hi, I am currently working on the transformer models on mobile GPUs.
Because transformer-based models have multi-headed attention, batched matrix multiplication is crucial.

However, I found out that TOCO simply replaces such BMM operators into a bunch of matmul operators.
Is there any rationale behind this? If there is, can you provide some benchmark or evaluation on this?

My guess is that the capacity of the mobile GPU cache or register file is not enough to perform BMM efficiently without register spill or something, so performance would be degraded, but I have not tried such experiments so cannot be sure on this.

Thanks in advance."
56500, OverflowError: int too large to convert to float,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Windows 1-

### Mobile device

_No response_

### Python version

3.9.5

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I wanted to train my machine learning model on this environment, unfortunately this bug appeared. I don't know if this is my issue or bug in Tensorflow's code
```


### Standalone code to reproduce the issue

```shell
from collections import Counter
from keras import layers
import numpy as np
import tensorflow as tf
from rl.agents.dqn import DQNAgent
from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy
from rl.memory import SequentialMemory
import keras
import random
import gym

class Env(gym.Env):
    def __init__(self):
        self.observation_space = gym.spaces.MultiDiscrete([50 for _ in range(9)] + [60000, 60000, 60000])
        self.action_space = gym.spaces.Discrete(12)
        self.stepsLeft = 20000
        self.state = self.reset()
        # Step mapping is a varible showing what each action does, this prevents wall of 12 if statements
        # first 3 numbers are the row you take, second 3 is the one you perform the operation on, 7th number means either adding or substacting
        self.StepMapping = {0: [0, 1, 2, 3, 4, 5, -1], 1: [0, 1, 2, 6, 7, 8, -1], 2: [3, 4, 5, 0, 1, 2, -1],
                            3: [3, 4, 5, 6, 7, 8, -1], 4: [6, 7, 8, 0, 1, 2, -1], 5: [6, 7, 8, 3, 4, 5, 1],
                            6: [0, 1, 2, 3, 4, 5, 1], 7: [0, 1, 2, 6, 7, 8, 1], 8: [3, 4, 5, 0, 1, 2, 1],
                            9: [3, 4, 5, 6, 7, 8, 1], 10: [6, 7, 8, 0, 1, 2, 1], 11: [6, 7, 8, 3, 4, 5, 1]}

    def step(self, action):
        reward = -1
        done = False
        self.stepsLeft -= 1
        self.state[self.StepMapping[action][3]] += self.state[self.StepMapping[action][0]] * self.StepMapping[action][6]
        self.state[self.StepMapping[action][4]] += self.state[self.StepMapping[action][1]] * self.StepMapping[action][6]
        self.state[self.StepMapping[action][5]] += self.state[self.StepMapping[action][2]] * self.StepMapping[action][6]
        # this wierd reward system should make AI lean towards the solution and not astronomically big numbers
        wholeSystem = abs(self.state[0]) + abs(self.state[1]) + abs(self.state[2]) + abs(self.state[3]) + abs(
            self.state[4]) + abs(self.state[5]) + abs(self.state[6]) + abs(self.state[7]) + abs(self.state[8])
        if wholeSystem > 10000: reward = -10
        if wholeSystem < 400: reward = 1
        if wholeSystem < 200: reward = 5
        if wholeSystem < 100: reward = 10
        if wholeSystem < 50: reward = 20
        if self.stepsLeft == 0:
            done = True
        if self.done():
            done = True
            reward = 100000
            print(""holy shit"")
        return self.state, reward, done, {}

    def reset(self):
        # in case you don't understand my genius way of calling varibles:
        # lsq - left side of quation, rsq - right side of quation
        x, y, z = random.randint(0, 200), random.randint(0, 200), random.randint(0, 200)
        lsq = [random.randint(0, 49), random.randint(0, 49), random.randint(0, 49),
               random.randint(0, 49), random.randint(0, 49), random.randint(0, 49),
               random.randint(0, 49), random.randint(0, 49), random.randint(0, 49)]
        rsq = [lsq[0] * x + lsq[1] * y + lsq[2] * z,
               lsq[3] * x + lsq[4] * y + lsq[5] * z,
               lsq[6] * x + lsq[7] * y + lsq[8] * z]
        return lsq + rsq

    def done(self):
        state = [[self.state[0], self.state[1], self.state[2]], [self.state[3], self.state[4], self.state[5]], [self.state[6], self.state[7], self.state[8]]]
        found = [0, 1, 2]
        savedIndexes = []
        try:
            for i in range(3):
                if Counter(state[i])[0] == 2:
                    found.pop(found.index(2))
                    savedIndexes = np.where(np.array(state[i]) == 0)[0].tolist()
            for i in range(3):
                if 0 not in state[i]: found.pop(found.index(0))
                if Counter(state[i])[0] == 1:
                    if state[i].index(0) in savedIndexes:
                        found.pop(found.index(1))
                    else: return False
        except ValueError: return False
        if not found: return True
        else: return False

env = Env()

model = keras.Sequential()
model.add(layers.Flatten(input_shape=(1, 12)))
model.add(layers.Dense(256, activation='relu', batch_input_shape=(12,)))
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dense(12))

model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),
              optimizer=keras.optimizers.Adam(lr=0.001), metrics=[""accuracy""])

policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1.0, value_min=0.1, value_test=0.2, nb_steps=10000)
memory = SequentialMemory(limit=50000, window_length=1)
DQN = DQNAgent(model=model, memory=memory, policy=policy, nb_actions=12, nb_steps_warmup=10, target_model_update=1e-2)

DQN.compile(tf.keras.optimizers.Adam(learning_rate=1e-4), metrics=['mae'])
DQN.fit(env, nb_steps=2000000, visualize=False, verbose=1)

model.save('workingmodel.h5')
scores = DQN.test(env, nb_episodes=1000000, visualize=True)
print(np.mean(scores.history['episode_reward']))
```


### Relevant log output

```shell
D:\PycharmProjects\AI\venv\lib\site-packages\keras\optimizers\optimizer_v2\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  super(Adam, self).__init__(name, **kwargs)
Training for 2000000 steps ...
Interval 1 (0 steps performed)
D:\PycharmProjects\AI\venv\lib\site-packages\keras\engine\training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
    1/10000 [..............................] - ETA: 14:50 - reward: 1.0000D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!
  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')
D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 10 + 1) instead
  batch_idxs = np.random.random_integers(low, high - 1, size=size)
   12/10000 [..............................] - ETA: 6:11 - reward: -0.6667D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 11 + 1) instead
  batch_idxs = np.random.random_integers(low, high - 1, size=size)
D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 12 + 1) instead
  batch_idxs = np.random.random_integers(low, high - 1, size=size)
D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 13 + 1) instead
  batch_idxs = np.random.random_integers(low, high - 1, size=size)
D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 14 + 1) instead
  batch_idxs = np.random.random_integers(low, high - 1, size=size)
D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 15 + 1) instead
  batch_idxs = np.random.random_integers(low, high - 1, size=size)
D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 16 + 1) instead
  batch_idxs = np.random.random_integers(low, high - 1, size=size)
D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 17 + 1) instead
  batch_idxs = np.random.random_integers(low, high - 1, size=size)
   29/10000 [..............................] - ETA: 3:25 - reward: -0.8621D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 18 + 1) instead
  batch_idxs = np.random.random_integers(low, high - 1, size=size)
D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 19 + 1) instead
  batch_idxs = np.random.random_integers(low, high - 1, size=size)
D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 20 + 1) instead
  batch_idxs = np.random.random_integers(low, high - 1, size=size)
D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 21 + 1) instead
  batch_idxs = np.random.random_integers(low, high - 1, size=size)
D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 22 + 1) instead
  batch_idxs = np.random.random_integers(low, high - 1, size=size)
D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 23 + 1) instead
  batch_idxs = np.random.random_integers(low, high - 1, size=size)
D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 24 + 1) instead
  batch_idxs = np.random.random_integers(low, high - 1, size=size)
D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 25 + 1) instead
  batch_idxs = np.random.random_integers(low, high - 1, size=size)
D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 26 + 1) instead
  batch_idxs = np.random.random_integers(low, high - 1, size=size)
D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 27 + 1) instead
  batch_idxs = np.random.random_integers(low, high - 1, size=size)
D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 28 + 1) instead
  batch_idxs = np.random.random_integers(low, high - 1, size=size)
D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 29 + 1) instead
  batch_idxs = np.random.random_integers(low, high - 1, size=size)
D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 30 + 1) instead
  batch_idxs = np.random.random_integers(low, high - 1, size=size)
D:\PycharmProjects\AI\venv\lib\site-packages\rl\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 31 + 1) instead
  batch_idxs = np.random.random_integers(low, high - 1, size=size)
 6720/10000 [===================>..........] - ETA: 25s - reward: -9.9231Traceback (most recent call last):
File ""D:\PycharmProjects\AI\venv\lib\site-packages\IPython\core\interactiveshell.py"", line 3398, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-3-0e6266aa34dd>"", line 1, in <cell line: 1>
    runfile('D:/PycharmProjects/AI/Linear Algebra AI.py', wdir='D:/PycharmProjects/AI')
  File ""D:\JetBrains\PyCharm Community Edition 2021.2\plugins\python-ce\helpers\pydev\_pydev_bundle\pydev_umd.py"", line 198, in runfile
    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script
  File ""D:\JetBrains\PyCharm Community Edition 2021.2\plugins\python-ce\helpers\pydev\_pydev_imps\_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""D:/PycharmProjects/AI/Linear Algebra AI.py"", line 95, in <module>
    DQN.fit(env, nb_steps=2000000, visualize=False, verbose=1)
  File ""D:\PycharmProjects\AI\venv\lib\site-packages\rl\core.py"", line 168, in fit
    action = self.forward(observation)
  File ""D:\PycharmProjects\AI\venv\lib\site-packages\rl\agents\dqn.py"", line 224, in forward
    q_values = self.compute_q_values(state)
  File ""D:\PycharmProjects\AI\venv\lib\site-packages\rl\agents\dqn.py"", line 68, in compute_q_values
    q_values = self.compute_batch_q_values([state]).flatten()
  File ""D:\PycharmProjects\AI\venv\lib\site-packages\rl\agents\dqn.py"", line 63, in compute_batch_q_values
    q_values = self.model.predict_on_batch(batch)
  File ""D:\PycharmProjects\AI\venv\lib\site-packages\keras\engine\training_v1.py"", line 1200, in predict_on_batch
    outputs = self.predict_function(inputs)
  File ""D:\PycharmProjects\AI\venv\lib\site-packages\keras\backend.py"", line 4269, in __call__
    array_vals.append(np.asarray(value,
OverflowError: int too large to convert to float
```
</details>"
56499,Bincount doesn't check the tensor type,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

master

### Custom Code

Yes

### OS Platform and Distribution

linux

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

With `axis=None` and `binary_output=False` it doesn't check the input tensor type so it is failing calling directly the `bincount` kernel that it doesn't support sparse and ragged inputs:

https://github.com/tensorflow/tensorflow/blob/d8ce9f9c301d021a69953134185ab728c1c248d3/tensorflow/python/ops/bincount_ops.py#L137


### Standalone code to reproduce the issue

```python
import tensorflow as tf
values = tf.constant([1,1,2,3,2,4,4,5])
values = tf.sparse.from_dense(values)
print(tf.math.bincount(values, binary_output=False, axis=0)) # Ok -> specialized sparse kernel
print(tf.math.bincount(values, binary_output=False, axis=None)) # Fail -> bincount kernel
```
```


### Relevant log output

_No response_</details>"
56498,tf.raw_ops.DepthwiseConv2dNative ignores dilation values,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

No

### OS Platform and Distribution

Linux Debian 11

### Mobile device

_No response_

### Python version

3.9.2

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
As per the raw_ops documentation https://www.tensorflow.org/api_docs/python/tf/raw_ops/DepthwiseConv2dNative
the DepthWiseConv2dNative op supports dilated convolutions. 

However, in practice, the operator seems to ignore the dilation values. Given any value for dilations, the output corresponds to dilation = [1, 1, 1, 1].
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

# CustomLayer using DepthwiseConv2dNative operator
class CustomDepthwiseConv2DLayer(tf.keras.layers.Layer):
    def __init__(self):
        super(CustomDepthwiseConv2DLayer, self).__init__()
    def build(self, input_shape):
        pass
    def call(self, input, filter, strides, padding, explicit_paddings, dilations):
        return tf.raw_ops.DepthwiseConv2dNative(
    input=input,
    filter=filter,
    strides=strides,
    padding=padding,
    explicit_paddings=explicit_paddings,
    dilations=dilations
)

# Model
inputs = tf.keras.Input(shape=(3, 3, 3))

filter_shape = (2, 2, 3, 1)
filter = tf.ones(filter_shape)
strides = [1, 1, 1, 1]
padding = ""VALID""
explicit_paddings = []

# Enter any dilation value here. It has no effect!
dilations = [1, 1, 1, 1]
#dilations = [1, 2, 2, 1]
operator = CustomDepthwiseConv2DLayer()
outputs = operator(inputs, filter, strides, padding, explicit_paddings, dilations)

model = tf.keras.Model(inputs=inputs, outputs=outputs)

# Prediction
x = np.arange(1, 28)
x = np.reshape(x, (1, 3, 3, 3))
x = tf.convert_to_tensor(x, dtype=tf.float32)

y = model.predict(x)

print(y.shape)
print(y)
```


### Relevant log output

_No response_</details>"
56496,TensorFlow Lite in Play Services issue,"**System information**
- Android Device information (use `adb shell getprop ro.build.fingerprint`
  if possible):
- TensorFlow Lite in Play Services SDK version (found in `build.gradle`):
- Google Play Services version
  (`Settings` > `Apps` > `Google Play Services` > `App details`):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to or attach code demonstrating
the problem.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
56492,Error: Convert a SavedModel,"### 1. System information

- Window 10 pro 21H2
- TensorFlow 2
- TensorFlow library : tf.lite package

### 2. Code

# %%
import tensorflow as tf

model = tf.lite.TFLiteConverter.from_saved_model(
    '../ImageRecognitionWithTensorFlow2/fashion-2'
)

model.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]

model.convert()

with open('converted-fashion.tflite', 'wb') as file:
  file.write(model)
# %%

#### Reference colab notebooks
1)  Reference [TensorFlow Lite converter]
(https://www.tensorflow.org/lite/models/convert/convert_models)

### 3. Failure after conversion
The conversion is successful, but the generated model is wrong, the state what is wrong:
- Model produces wrong results and the file's size = 0

### 5.  Info / logs

(2022-06-17 16:52:23.053795: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1777] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following flex op(s):
Flex ops: FlexCast
Details:
        tf.Cast {Truncate = false, device = """"}
Traceback (most recent call last):
  File ""convert-fashion-mnist.py"", line 17, in <module>
    file.write(model)
TypeError: a bytes-like object is required, not 'TFLiteSavedModelConverterV2')"
56491,"TypeError: a bytes-like object is required, not 'TFLiteSavedModelConverterV2'",
56490,tf.data.experimental.service does not stop with repeated dataset,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.7.0 and 2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.8.13

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

After consuming a repeated dataset, the `DispatchServer` and `WorkerServer` are shutdown, but some thread keeps running that tries to reconnect to either the dispatcher or worker. This prevents the entire process from terminating (thread seems not to be a daemon thread, likely the gRPC thread: https://github.com/grpc/grpc-java/issues/80).

Setting `WorkerConfig(dispatcher_timeout_ms=1000)` (timeout is 1s) makes the thread giving up at some point, but shutdown still takes 20 seconds.

### Standalone code to reproduce the issue

```python
import threading
from itertools import islice

import tensorflow as tf


def in_thread(fn, args=()):
    bg = threading.Thread(target=fn, args=args)
    bg.daemon = True
    bg.start()
    return bg

def consume(ds):
    for _ in islice(ds.as_numpy_iterator(), 10):
        print('.', end='')
    print()

dispatcher = tf.data.experimental.service.DispatchServer()
dispatcher_address = dispatcher.target.split(""://"")[1]

worker = tf.data.experimental.service.WorkerServer(
    tf.data.experimental.service.WorkerConfig(dispatcher_address=dispatcher_address)
)

print('distribute dataset')
dataset = tf.data.Dataset.range(1024).repeat().batch(128).apply(
    tf.data.experimental.service.distribute(processing_mode='distributed_epoch', service=dispatcher.target)
)

print('consume dataset')
thread = in_thread(consume, (dataset, ))
thread.join()
print('dataset consumed')
```


### Relevant log output

```
2022-06-17 11:53:03.722050: I tensorflow/core/data/service/dispatcher_impl.cc:189] Running with fault_tolerant_mode=False. The dispatcher will not be able to recover its state on restart.
2022-06-17 11:53:03.722067: I tensorflow/core/data/service/server_lib.cc:64] Started tf.data DispatchServer running at 0.0.0.0:33963
2022-06-17 11:53:03.723049: I tensorflow/core/data/service/worker_impl.cc:147] Worker registered with dispatcher running at localhost:33963
2022-06-17 11:53:03.723113: I tensorflow/core/data/service/server_lib.cc:64] Started tf.data WorkerServer running at 0.0.0.0:38313
2022-06-17 11:53:03.723501: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
distribute dataset
consume dataset
..........
2022-06-17 11:53:03.889622: I tensorflow/core/data/service/server_lib.cc:76] Shut down DispatchServer server running at port 33963
2022-06-17 11:53:03.889894: I tensorflow/core/data/service/server_lib.cc:76] Shut down WorkerServer server running at port 38313
dataset consumed
2022-06-17 11:53:04.192806: I tensorflow/core/data/service/grpc_util.cc:68] Failed to get next split: UNAVAILABLE: Failed to get split: failed to connect to all addresses. Will retry in 100ms.
2022-06-17 11:53:04.293259: I tensorflow/core/data/service/grpc_util.cc:68] Failed to get next split: UNAVAILABLE: Failed to get split: failed to connect to all addresses. Will retry in 118ms.
2022-06-17 11:53:04.411498: I tensorflow/core/data/service/grpc_util.cc:68] Failed to get next split: UNAVAILABLE: Failed to get split: failed to connect to all addresses. Will retry in 129ms.
2022-06-17 11:53:04.540736: I tensorflow/core/data/service/grpc_util.cc:68] Failed to get next split: UNAVAILABLE: Failed to get split: failed to connect to all addresses. Will retry in 215ms.
```
</details>"
56489,Tensorflow model input data type is not available to train it.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.9.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
ValueError: Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {'(<class \'list\'> containing values of types {""<class \'int\'>""})'})
```
Got a problem with like this, and X data type is a ndarray that I converted from list. Each size are like this.

```

[SIZE]		NpX lenght : (112599, 428)
		NpY length : (112599, 1)
```
```


### Standalone code to reproduce the issue

```shell
# Dataload part
le = preprocessing.LabelEncoder()

record_list = []
pickle_input = dict()
X, y = [], []

print(""[INFO] Read records file from "", DATA_PATH)
with open(DATA_PATH + 'RECORDS') as f:
    record_lines = f.readlines()

for i in range(len(record_lines)):
    record_list.append(str(record_lines[i].strip()))

for i in range(len(record_list)):
    temp_path = DATA_PATH + ""mit"" + record_list[i] + "".pkl""
    with open(temp_path, 'rb') as f:
        pickle_input = pickle.load(f)
        for i in range(len(pickle_input[0])):
            X.append(pickle_input[0][i])

        for i in range(len(pickle_input[1])):
            check_ann = pickle_input[1][i]
            temp_ann_list = list()
            if check_ann == ""N"":            # Normal
                temp_ann_list.append(0)

            elif check_ann == ""S"":          # Supra-ventricular
                temp_ann_list.append(1)

            elif check_ann == ""V"":          # Ventricular
                temp_ann_list.append(2)

            elif check_ann == ""F"":          # False alarm
                temp_ann_list.append(3)

            else:                           # Unclassed 
                temp_ann_list.append(4)
            y.append(temp_ann_list)

print(""[SIZE]\t\tX length : {}\n\t\ty length : {}"".format(len(X), len(y)))

npx = np.array(X)
npy = np.array(y)
print(""[SIZE]\t\tNpX lenght : {}\n\t\tNpY length : {}"".format(npx.shape, npy.shape))
input_size = layers.Input(shape=(npx.shape))

# First Input
model = keras.Sequential([
        layers.Conv1D(428, 9, padding=""same""),
        layers.BatchNormalization(),
        layers.Activation('relu')
])

model.compile(optimizer='sgd', loss='mse')
model.fit(X_np, y, batch_size=BATCH_SIZE, epochs=EPOCH)
model.summary()
```


### Relevant log output

```shell
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
C:\Users\HILAB_~1\AppData\Local\Temp/ipykernel_21332/3822957398.py in <module>
      9 
     10 model.compile(optimizer='sgd', loss='mse')
---> 11 model.fit(X_np, y, batch_size=BATCH_SIZE, epochs=EPOCH)
     12 model.summary()

c:\Users\HILAB_Labtop_02\anaconda3\lib\site-packages\keras\utils\traceback_utils.py in error_handler(*args, **kwargs)
     65     except Exception as e:  # pylint: disable=broad-except
     66       filtered_tb = _process_traceback_frames(e.__traceback__)
---> 67       raise e.with_traceback(filtered_tb) from None
     68     finally:
     69       del filtered_tb

c:\Users\HILAB_Labtop_02\anaconda3\lib\site-packages\keras\engine\data_adapter.py in select_data_adapter(x, y)
    983   if not adapter_cls:
    984     # TODO(scottzhu): This should be a less implementation-specific error.
--> 985     raise ValueError(
    986         ""Failed to find data adapter that can handle ""
    987         ""input: {}, {}"".format(

ValueError: Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {'(<class \'list\'> containing values of types {""<class \'int\'>""})'})
```
</details>"
56482,[RNN] LSTM after conversion to TFLite not run on GPU. ,"### 1. System information
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 11
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): tensorflow 2.5, tensorflow 2.6, tensorflow 2.9, tf-nightly
- Mobile device: Huawei Mate 10 Pro with Android 10.


### 2. Code
Provide code to help us reproduce your issues using one of the following options:
You can see the attched files. 


### 3. Failure after conversion
In most cases I can complete the conversion to TFLite, with and without Quantization.


### 4. Any other info / logs
The problem exists after the conversion; I am currently unable to understand if LSTM or GRU layers are actually supported on GPU and NNAPI_GPU Delegates. I test my models using the BenchmarkTools through the Andoid apk, setting the --use_gpu or use_nnapi flags to True. 

From my tests, I was able to find two different cases:

1) When I set in conversion as supported_ops  tf.lite.OpsSet.TFLITE_BUILTINS e tf.lite.OpsSet.SELECT_TF_OPS which leads to 

UNPACK: Operation is not supported.
06-16 17:31:52.176 27077 27077 E tflite  : 294 operations will run on the GPU, and the remaining 296 operations will run on the CPU.
06-16 17:31:52.177 27077 27077 I tflite  : Replacing 294 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 2 partitions.
06-16 17:31:53.098 27077 27077 I tflite  : Initialized OpenCL-based API.
06-16 17:31:53.242 27077 27077 I tflite  : Created 1 GPU delegate kernels.
06-16 17:31:53.245 27077 27077 I tflite  : Explicitly applied GPU delegate, and the model graph will be partially executed by the delegate w/ 1 delegate kernels.

2) When I set only tf.lite.OpsSet.SELECT_TF_OPS in supported_ops, which leads to
Created TensorFlow Lite delegate for select TF ops.
06-16 17:39:13.016 27417 27417 I tflite  : TfLiteFlexDelegate delegate: 609 nodes delegated out of 609 nodes with 1 partitions.
06-16 17:39:13.016 27417 27417 I tflite  : Replacing 609 node(s) with delegate (TfLiteFlexDelegate) node, yielding 1 partitions.
06-16 17:39:13.152 27417 27417 I tflite  : Created TensorFlow Lite delegate for GPU.
06-16 17:39:13.152 27417 27417 I tflite  : GPU delegate created.
06-16 17:39:13.178 27417 27417 E tflite  : Following operations are not supported by GPU delegate:
06-16 17:39:13.178 27417 27417 E tflite  : DELEGATE TfLiteFlexDelegate: Operation is not supported.
06-16 17:39:13.178 27417 27417 E tflite  : No operations will run on the GPU, and all 1 operations will run on the CPU.
06-16 17:39:13.178 27417 27417 I tflite  : Created 0 GPU delegate kernels.
06-16 17:39:13.198 27417 27417 I tflite  : Explicitly applied GPU delegate, and the model graph will be completely executed by the delegate.

In any case, however, I note that indeed, the values for the Inference (avg) time are extremely higher than in the case in which I use the CPU directly, without either of the two delegates.
This is probably due to a Fallback to CPU, which results in worse performance.

I have tested all possible settings configurations in conversion as you can see below, even with tensorflow 2.9 and tf-nightly, but I couldn't do better.
   
    converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])  
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.representative_dataset = representative_data_gen
    converter.experimental_new_converter = True
    converter.experimental_new_quantizer = True
    converter.experimental_enable_resource_variables = True
    converter.allow_custom_ops = True
    converter.target_spec.supported_types = [tf.float16]
tf.lite.OpsSet.TFLITE_BUILTINS]
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
    converter.inference_input_type = [tf.float32]
    converter.inference_output_type = [tf.float32]
    tflite_quantized_model = converter.convert()
    

The ops that cause problems during the performance of the Benchmark are as follows:

06-16 17:31:54.781 27077 27077 I tflite  :                       RESHAPE            0.019           0.017         0.112%          0.112%         0.000              1       [sequential/lstm/transpose1]:0
06-16 17:31:54.781 27077 27077 I tflite  :                        UNPACK            0.032           0.029         0.199%          0.311%         0.000              1       [sequential/lstm/unstack, sequential/lstm/unstack1, sequential/lstm/unstack2, sequential/lstm/unstack3, sequential/lstm/unstack4, sequential/lstm/unstack5, sequential/lstm/unstack6, sequential/lstm/unstack7, sequential/lstm/unstack8, sequential/lstm/unstack9, sequential/lstm/unstack10, sequential/lstm/unstack11, sequential/lstm/unstack12, sequential/lstm/unstack13, sequential/lstm/unstack14, sequential/lstm/unstack15, sequential/lstm/unstack16, sequential/lstm/unstack17, sequential/lstm/unstack18, sequential/lstm/unstack19, sequential/lstm/unstack20]:1

although as indicated in this page the unpack operator is supported through TF Ops
https://www.tensorflow.org/lite/guide/op_select_allowlist

and the Reshape Layer is supported on GPU as here
https://www.tensorflow.org/lite/performance/gpu_advanced

Also, on this page it is indicated that LSTM is supported on GPU with float16, but also specifying the type in the conevrsion
converter.target_spec.supported_types = [tf.float16]

Also, on this page
https://www.tensorflow.org/lite/performance/gpu_advanced
it is indicated that LSTM is supported on GPU with float16

converter.target_spec.supported_types = [tf.float16]

Also specifying the type in the conversion, I didn't get any changes.


Basically, can you tell me if RNN can run on GPU and NNAPI-GPU on TFLite. If so can you provide me with support to do so. I have already taken a long time, with no solution or support."
56481,Wrong is_gpu flag computation in graph_memory.cc,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.9.1

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I work for Sonar and as part of our internal tests, we run our static analyzers on multiple open source codes. While reviewing our tests we saw the following code that is almost certainly not intended:
On the following line: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/costs/graph_memory.cc#L175
the method `std::string::find` will return `std::string::npos` if the searched substring is not present. Because `std::string::npos` is different from 0, casting `std::string::find` as a boolean does not return what one would expect and there is no way the test on this line returns anything other than true, whether there is a GPU or not.


### Standalone code to reproduce the issue

I don't know how this bug impacts the functional behavior of the tool. It might be completely innocuous, given its age.
I am also unsure whether it should be fixed by removing the flag altogether to keep the current years-old behavior or fixed to actually handle non-GPU devices.


### Relevant log output

_No response_</details>"
56476,Estimator Training Data Reading Incorrect Values,"  ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.5+

### Custom Code

No

### OS Platform and Distribution

Mac and Linux

### Python version

3.8


### Current Behaviour?

Starting with tensorflow 2.5, I've encountered my training pipeline having wrong values for certain intermediate calculation. It looks like the data pipeline is reading wrong value in tf.train.Example. I made toy dataset with 1 feature (all -5) and 1 label (always 1) and I print the label, but instead it shows value of the feature.

I've been able to reproduce it down to small estimator training loop although it's still ~50 lines. The model here is trivial fake model as focus is on the value of label tensor passed to model_fn. Dataset pipeline is short but still several steps. If I remove some steps the bug disappears. Also batching uses,

`dataset = dataset.batch(batch_size, drop_remainder=True)`

The drop_remainder is required for the bug. Changing that somehow changes how the data is read.

In the model_fn, minor what should be equivalent variations of this line effect whether bug happens.

`tf.stack([features[key] for key in label_names])`

It's also not specific only to tf.stack. tf.reduce_mean also leads to negative values even though features[key] is always positive. One key element is that this list has same tensor multiple times because label is repeated across different model heads. If label_names is unique the bug disappears.

I've tested this on 2.4, 2.5, 2.6, 2.7, 2.8, 2.9. All of 2.5+ consistently show same wrong behavior for value of intermediate printed. I am working on bisecting to exact commit, but building tensorflow is slow. So far I've narrowed it down to bug being introduced in between March 18 commit [fd020fff99a0](https://github.com/tensorflow/tensorflow/commit/fd020fff99a0ad5e0293e43a77df54dd9efedce7) and March 24 commit [83a8259081](https://github.com/tensorflow/tensorflow/commits/b9e31e669e454b185a100802b5d6cb1a6f59d74b?before=b9e31e669e454b185a100802b5d6cb1a6f59d74b+140&branch=b9e31e669e454b185a100802b5d6cb1a6f59d74b&qualified_name=b9e31e669e454b185a100802b5d6cb1a6f59d74b).

I've also tested on both my laptop (mac) and a linux gcp VM. All tests were done with CPU only. They show same behavior.

My suspicion is some memory aliasing is happening when non unique tensor appears for some of these operations. I'm confused why this is estimator specific. Maybe it's possible to reproduce this without estimator, but I've been unsuccessful with that.


### Standalone code to reproduce the issue

Here's a colab link, https://colab.research.google.com/drive/11-7vyU4cc_3BGXudUEuT12bbVS-JJQPH?usp=sharing It is fully self contained with only dependency being tensorflow. If you run notebook in different tensorflow versions <=2.4 vs >=2.5 it produces different behavior.


### Relevant log output

If you run notebook on a good tensorflow version (2.4) you will see 1 for label. If you run it on any bad version (2.5+) -5 is result.
"
56474,Tensorflow precompiled AOT XLA compiler.,"I am trying to compile a Saved Model to a linkable library. But compiling the compiler takes forever on my computer. Is it possible to provide the compiler binaries and possibly have it in the releases. Perhaps also make it freeze the model automatically for simplicity.

Thank you!"
56473,Tensorflow-Macos 2.9.2 Missing OP files,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.2

### Custom Code

No

### OS Platform and Distribution

MacOS 13

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
On trying to import horovod with tensorflow, the following bug shows up which states that Tensorflow 2.9.2 is missing some ops when it built.
```


### Standalone code to reproduce the issue

```shell
`>>> import horovod.tensorflow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/bradbury-11/miniforge3/lib/python3.9/site-packages/horovod/tensorflow/__init__.py"", line 26, in <module>
    from horovod.tensorflow import elastic
  File ""/Users/bradbury-11/miniforge3/lib/python3.9/site-packages/horovod/tensorflow/elastic.py"", line 24, in <module>
    from horovod.tensorflow.functions import broadcast_object, broadcast_object_fn, broadcast_variables
  File ""/Users/bradbury-11/miniforge3/lib/python3.9/site-packages/horovod/tensorflow/functions.py"", line 24, in <module>
    from horovod.tensorflow.mpi_ops import allgather, broadcast, broadcast_
  File ""/Users/bradbury-11/miniforge3/lib/python3.9/site-packages/horovod/tensorflow/mpi_ops.py"", line 53, in <module>
    raise e
  File ""/Users/bradbury-11/miniforge3/lib/python3.9/site-packages/horovod/tensorflow/mpi_ops.py"", line 50, in <module>
    MPI_LIB = _load_library('mpi_lib' + get_ext_suffix())
  File ""/Users/bradbury-11/miniforge3/lib/python3.9/site-packages/horovod/tensorflow/mpi_ops.py"", line 45, in _load_library
    library = load_library.load_op_library(filename)
  File ""/Users/bradbury-11/miniforge3/lib/python3.9/site-packages/tensorflow/python/framework/load_library.py"", line 54, in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename)
tensorflow.python.framework.errors_impl.NotFoundError: dlopen(/Users/bradbury-11/miniforge3/lib/python3.9/site-packages/horovod/tensorflow/mpi_lib.cpython-39-darwin.so, 0x0006): weak-def symbol not found '__ZN3xla14HloInstruction5VisitIPKS0_EEN10tensorflow6StatusEPNS_17DfsHloVisitorBaseIT_EE'`
```


### Relevant log output

_No response_</details>"
56470,Different results on 2.9.1 and 2.8.0,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.8.0, tf 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Windows 11 x86

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.3, cuDNN 8.2.1

### GPU model and memory

NVIDIA GeForce RTX 3060 Laptop GPU, 6gb

### Current Behaviour?

```shell
The model has different results on 2.8.0 and 2.9.1 versions:
model = keras.applications.EfficientNetB0(classes=1000)

-----2.8.0-----
print(model.predict(np.zeros((1, 224, 224, 3)))[0][0])
### result is 0.0010812443

-----2.9.1-----
print(model.predict(np.zeros((1, 224, 224, 3)))[0][0])
### result is 0.00066214177

The same happens with other models
```


### Standalone code to reproduce the issue

```shell
import numpy as np
from tensorflow import keras

model = keras.applications.EfficientNetB0(classes=1000)
print(model.predict(np.zeros((1, 224, 224, 3)))[0][0])
```


### Relevant log output

_No response_</details>"
56469,Tensorflow Gfile raises wrong error when trying to delete non existing file on HDFS,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.5.3

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When I use tf.io.gfile.remove to remove a file which does not exist the error raised is  `tf.errors.UnknownError` instead of `tf.errors.NotFoundError`.

It seems to come from a wrong interpretation of hadoop error code.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.io.gfile.remove(""hdfs://myhdfs:9000/NONEXISTENTFILE.txt"")
```


### Relevant log output

```shell
22/06/15 15:01:18 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 330, in delete_file_v2
    _pywrap_file_io.DeleteFile(compat.path_to_bytes(path))
tensorflow.python.framework.errors_impl.UnknownError: hdfs://myhdfs:9000/NONEXISTENTFILE.txt; Input/output error
```
</details>"
56698,"InactiveRpcError of RPC that terminated with: status = StatusCode.FAILED_PRECONDITION, details = ""Attempting to use uninitialized value dense_2/bias","
-----------------------
### [InactiveRpcError of RPC that terminated with: status = StatusCode.FAILED_PRECONDITION, details = ""Attempting to use uninitialized value dense_2/bias]


Getting following error from my python grpc client when trying predicting wuth the tf-serving server:
```
_InactiveRpcError: <_InactiveRpcError of RPC that terminated with:
    status = StatusCode.FAILED_PRECONDITION
    details = ""Attempting to use uninitialized value dense_2/bias
     [[{{node dense_2/bias/read}}]]""
    debug_error_string = ""{""created"":""@1654790154.977771219"",""description"":""Error received from peer ipv4:10.1.25.14:9002"",""file"":""src/core/lib/surface/call.cc"",""file_line"":903,""grpc_message"":""Attempting to use uninitialized value dense_2/bias\n\t [[{{node dense_2/bias/read}}]]"",""grpc_status"":9}""
>
```



I found description of problem but it is not informative for me
Operation was rejected because the system is not in a state required for the operations execution. (https://grpc.github.io/grpc/python/grpc.html#grpc-status-code)

### System information
python 3.9 
tensorflow-serving-api==2.8.0
tensorflow==2.8.0


What can be the reason for this error and how it could be fix?
"
56467,Tensorflow GPU Delegate Limit resources (Feature Request),"Hello, sometimes when I'm running the tflite model on gpu delegate it use almost all gpu resources while execution and this behaviour freeze UI because of lack resources. If it possible, will be good to have some initial option with percentage of GPU use. 

OS Android

For example - 
GpuDelegate.Options().gpuUsePercentage = 0.8

This feature will provide more control over resources."
56466,Keras model Can't pickle _thread.RLock objects,"`from __future__ import absolute_import
from __future__ import print_function

import numpy as np
from keras.applications.vgg16 import VGG16
from keras.layers import *
from keras.models import Model, load_model
from keras.optimizers import SGD
from keras.preprocessing.image import load_img, img_to_array
import tensorflow as tf
from keras import backend as K

config = tf.ConfigProto()
config.gpu_options.allow_growth = True
sess = tf.Session(config=config)
K.set_session(sess)


def convnet_model_():
    vgg_model = VGG16(weights=None, include_top=False)
    x = vgg_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(4096, activation='relu')(x)
    x = Dropout(0.6)(x)
    x = Dense(4096, activation='relu')(x)
    x = Dropout(0.6)(x)
    x = Lambda(lambda x_: K.l2_normalize(x, axis=1))(x)
    convnet_model = Model(inputs=vgg_model.input, outputs=x)
    return convnet_model


def deep_rank_model():
    convnet_model = convnet_model_()
    first_input = Input(shape=(224, 224, 3))
    first_conv = Conv2D(96, kernel_size=(8, 8), strides=(16, 16), padding='same')(first_input)
    first_max = MaxPool2D(pool_size=(3, 3), strides=(4, 4), padding='same')(first_conv)
    first_max = Flatten()(first_max)
    first_max = Lambda(lambda x: K.l2_normalize(x, axis=1))(first_max)

    second_input = Input(shape=(224, 224, 3))
    second_conv = Conv2D(96, kernel_size=(8, 8), strides=(32, 32), padding='same')(second_input)
    second_max = MaxPool2D(pool_size=(7, 7), strides=(2, 2), padding='same')(second_conv)
    second_max = Flatten()(second_max)
    second_max = Lambda(lambda x: K.l2_normalize(x, axis=1))(second_max)

    merge_one = concatenate([first_max, second_max])

    merge_two = concatenate([merge_one, convnet_model.output])
    emb = Dense(4096)(merge_two)
    l2_norm_final = Lambda(lambda x: K.l2_normalize(x, axis=1))(emb)

    final_model = Model(inputs=[first_input, second_input, convnet_model.input], outputs=l2_norm_final)

    return final_model


As can see this code having lambda, which may fail when it try to serialize an unserializable object.
My question is what should I add to my lambda expression to avoid this.
i must save the model in term of using in web.
`"
56463,Reduction in tflite performance between v2.5 and v2.9,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

tflite 2.5.0-rc3 to 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

n/a

### Bazel version

4.2.1 for earlier tflite versions, 5.1.1 for v2.9.1

### GCC/Compiler version

9.4.0-1ubuntu1~20.04.1

### CUDA/cuDNN version

n/a

### GPU model and memory

NVIDIA RTX 2070

### Current Behaviour?

```shell
We are seeing a decline in the inferencing performance of tflite using the opencl gpu delegate on an x86_64 platform.

The log output below shows the performance of each version we've tested. The classifier, hardware and our software is identical in each case - the only change is the version of tflite. The results are the average time per sample over several thousand inferences.

The classifier model is inception-v1 based.

Since batch processing is still not possible in the tflite gpu delegate then single inference performance is critical.

We're still using 2.5.0-rc3 in production due to this performance issue.
```


### Standalone code to reproduce the issue

```shell
Since the classifier was trained for a client on proprietary data we are unable to post the classifier and the test samples here. 
However the second part of the log shows the results using the tflite benchmark_model tool built for x64 with opencl delegate support. It shows the same increase in average inference time and weirdly the 2.8 and 2.9 versions also segfaulted after the benchmark.
```


### Relevant log output

```shell
## Using our framework
Version    Time to load (ms)	Time/sample (ms)
2.5.0-rc3  215                  1.76
2.5.3      222                  1.83
2.6.5      289                  2.08
2.8.2      2173                 3.35
2.9.1      1727                 3.33

---

## Using benchmark_model
dan@dodeca:[local *]:~/code/external$ ./bin/benchmark_model_v2.5.0-rc3 --use_gpu=true --graph=/home/dan/code/data-eagle/libeagle-data-cig12/usr/share/libeagle/normal/model.tflite
STARTING!
Log parameter values verbosely: [0]
Graph: [/home/dan/code/data-eagle/libeagle-data-cig12/usr/share/libeagle/normal/model.tflite]
Use gpu: [1]
Loaded model /home/dan/code/data-eagle/libeagle-data-cig12/usr/share/libeagle/normal/model.tflite
INFO: Created TensorFlow Lite delegate for GPU.
INFO: Initialized OpenCL-based API.
INFO: Created 1 GPU delegate kernels.
Explicitly applied GPU delegate, and the model graph will be completely executed by the delegate.
The input model file size (MB): 22.452
Initialized session in 154.524ms.
Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
count=237 first=8216 curr=1821 min=1723 max=8216 avg=2096.54 std=583

Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.
count=526 first=2277 curr=1732 min=1725 max=2765 avg=1884.21 std=212

Inference timings in us: Init: 154524, First inference: 8216, Warmup (avg): 2096.54, Inference (avg): 1884.21
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Peak memory footprint (MB): init=212.746 overall=212.746

---

dan@dodeca:[local *]:~/code/external$ ./bin/benchmark_model_v2.5.3 --use_gpu=true --graph=/home/dan/code/data-eagle/libeagle-data-cig12/usr/share/libeagle/normal/model.tflite
STARTING!
Log parameter values verbosely: [0]
Graph: [/home/dan/code/data-eagle/libeagle-data-cig12/usr/share/libeagle/normal/model.tflite]
Use gpu: [1]
Loaded model /home/dan/code/data-eagle/libeagle-data-cig12/usr/share/libeagle/normal/model.tflite
INFO: Created TensorFlow Lite delegate for GPU.
INFO: Initialized OpenCL-based API.
INFO: Created 1 GPU delegate kernels.
Explicitly applied GPU delegate, and the model graph will be completely executed by the delegate.
The input model file size (MB): 22.452
Initialized session in 168.928ms.
Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
count=233 first=10964 curr=1728 min=1728 max=10964 avg=2127.98 std=719

Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.
count=523 first=1749 curr=1735 min=1727 max=2719 avg=1896.62 std=214

Inference timings in us: Init: 168928, First inference: 10964, Warmup (avg): 2127.98, Inference (avg): 1896.62
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Peak memory footprint (MB): init=211.449 overall=211.449

---

dan@dodeca:[local *]:~/code/external$ ./bin/benchmark_model_v2.6.5 --use_gpu=true --graph=/home/dan/code/data-eagle/libeagle-data-cig12/usr/share/libeagle/normal/model.tflite
STARTING!
Log parameter values verbosely: [0]
Graph: [/home/dan/code/data-eagle/libeagle-data-cig12/usr/share/libeagle/normal/model.tflite]
Use gpu: [1]
Loaded model /home/dan/code/data-eagle/libeagle-data-cig12/usr/share/libeagle/normal/model.tflite
INFO: Created TensorFlow Lite delegate for GPU.
GPU delegate created.
Going to apply 1 delegates one after another.
INFO: Initialized OpenCL-based API.
INFO: Created 1 GPU delegate kernels.
Explicitly applied GPU delegate, and the model graph will be completely executed by the delegate.
The input model file size (MB): 22.452
Initialized session in 163.602ms.
Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
count=228 first=8352 curr=1788 min=1780 max=8352 avg=2106.11 std=585

Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.
count=500 first=1799 curr=1802 min=1777 max=7597 avg=1911.31 std=309

Inference timings in us: Init: 163602, First inference: 8352, Warmup (avg): 2106.11, Inference (avg): 1911.31
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Peak memory footprint (MB): init=211.082 overall=211.082

---

dan@dodeca:[local *]:~/code/external$ ./bin/benchmark_model_v2.8.2 --use_gpu=true --graph=/home/dan/code/data-eagle/libeagle-data-cig12/usr/share/libeagle/normal/model.tflite
STARTING!
Log parameter values verbosely: [0]
Graph: [/home/dan/code/data-eagle/libeagle-data-cig12/usr/share/libeagle/normal/model.tflite]
Use gpu: [1]
Loaded model /home/dan/code/data-eagle/libeagle-data-cig12/usr/share/libeagle/normal/model.tflite
INFO: Created TensorFlow Lite delegate for GPU.
GPU delegate created.
INFO: Initialized OpenCL-based API.
INFO: Created 1 GPU delegate kernels.
Explicitly applied GPU delegate, and the model graph will be completely executed by the delegate.
The input model file size (MB): 22.452
Initialized session in 179.387ms.
Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
count=132 first=14658 curr=3492 min=3165 max=14658 avg=3706.23 std=1029

Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.
count=280 first=3877 curr=3433 min=3149 max=8949 avg=3494.44 std=450

Inference timings in us: Init: 179387, First inference: 14658, Warmup (avg): 3706.23, Inference (avg): 3494.44
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Memory footprint delta from the start of the tool (MB): init=211.52 overall=211.52
Segmentation fault (core dumped)

---

dan@dodeca:[local *]:~/code/external$ ./bin/benchmark_model_v2.9.1 --use_gpu=true --graph=/home/dan/code/data-eagle/libeagle-data-cig12/usr/share/libeagle/normal/model.tflite
STARTING!
Log parameter values verbosely: [0]
Graph: [/home/dan/code/data-eagle/libeagle-data-cig12/usr/share/libeagle/normal/model.tflite]
Use gpu: [1]
Loaded model /home/dan/code/data-eagle/libeagle-data-cig12/usr/share/libeagle/normal/model.tflite
INFO: Created TensorFlow Lite delegate for GPU.
GPU delegate created.
INFO: Initialized OpenCL-based API.
INFO: Created 1 GPU delegate kernels.
Explicitly applied GPU delegate, and the model graph will be completely executed by the delegate.
The input model file size (MB): 22.452
Initialized session in 154.443ms.
Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
count=127 first=13799 curr=3222 min=3179 max=13799 avg=3866.95 std=1039

Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.
count=279 first=3525 curr=3185 min=3174 max=4197 avg=3510.01 std=271

Inference timings in us: Init: 154443, First inference: 13799, Warmup (avg): 3866.95, Inference (avg): 3510.01
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Memory footprint delta from the start of the tool (MB): init=211.57 overall=211.57
Segmentation fault (core dumped)
```
</details>"
56462,ktonthat - tech writer testing this template,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Source

source

### Tensorflow Version

2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Ignore - testing template for documentation
```


### Standalone code to reproduce the issue

```shell
Ignore - testing template for documentation
```


### Relevant log output

_No response_</details>"
56459,"assemble two model into one, but encount  ValueError: Graph disconnected","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.8

### Custom Code

Yes

### OS Platform and Distribution

windows 10

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
i have two models:
model_0 assign with input shape of 6,6,1280, which has two layers of GlobalAveragePooling2D and Dense.
mobile_net is mobilenet assign with input shape of 192,192,3, and output shape of 6,6,1280
now i connect the model_1 and model_0, hoping they will connected together.
I finally assembled the two model successfully, the assembled model is named model_1
But when i extract some layer outputs of the model_1, wired things happened~~
i can get the -4 layer output successfully
i can get the -3 layer output successfully
i can not get the -2 layer output 
i can not get the -1 layer output
```


### Standalone code to reproduce the issue

```shell
inputs=tf.keras.layers.Input([6,6,1280])
x=tf.keras.layers.GlobalAveragePooling2D(
    name='global_average_pooling2d_0')(inputs)
y=tf.keras.layers.Dense(5,name='dense_0')(x)
model_0 = tf.keras.Model(inputs,y,name='model_0')

mobile_net = tf.keras.applications.MobileNetV2(
    input_shape=(192,192,3),
    include_top=False,
    weights='imagenet')
assert mobile_net.output_shape==(None, 6, 6, 1280)
y1=model_0.get_layer('global_average_pooling2d_0')(mobile_net.output)
y2=model_0.get_layer('dense_0')(y1)
model_1 = tf.keras.Model(mobile_net.input,y2,name='assembled_model')


tmp=tf.keras.Model(model_1.input,model_1.layers[-3].output) # successfully
tmp=tf.keras.Model(model_1.input,model_1.layers[-2].output) # failed
tmp=tf.keras.Model(model_1.input,model_1.layers[-1].output) # failed
```


### Relevant log output

```shell
ValueError: Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 6, 6, 1280), dtype=tf.float32, name='input_12'), name='input_12', description=""created by layer 'input_12'"") at layer ""global_average_pooling2d_0"". The following previous layers were accessed without issue: []

ValueError: Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 6, 6, 1280), dtype=tf.float32, name='input_12'), name='input_12', description=""created by layer 'input_12'"") at layer ""global_average_pooling2d_0"". The following previous layers were accessed without issue: []
```
</details>"
56458,Demo example Segmentation fault with tf version 2.9.1,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

Python 3.10.4

### Bazel version

_No response_

### GCC/Compiler version

11.2.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
TF_SessionRun has Segmentation fault
```


### Standalone code to reproduce the issue

```shell
Code based on:
https://github.com/AmirulOm/tensorflow_capi_sample

TF Version:

root@2ea05d554c95:~/demo# python3 -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
v2.9.0-18-gd8ce9f9c301 2.9.1
```

Installed python packages:
```
root@2ea05d554c95:~/demo# pip3 list
Package                      Version
---------------------------- -----------
absl-py                      1.1.0
astunparse                   1.6.3
cachetools                   5.2.0
certifi                      2022.5.18.1
charset-normalizer           2.0.12
dbus-python                  1.2.18
flatbuffers                  1.12
gast                         0.4.0
google-auth                  2.7.0
google-auth-oauthlib         0.4.6
google-pasta                 0.2.0
grpcio                       1.46.3
h5py                         3.7.0
idna                         3.3
keras                        2.9.0
Keras-Preprocessing          1.1.2
libclang                     14.0.1
Markdown                     3.3.7
numpy                        1.22.4
oauthlib                     3.2.0
opt-einsum                   3.3.0
packaging                    21.3
pandas                       1.4.2
pip                          22.0.2
protobuf                     3.19.4
pyasn1                       0.4.8
pyasn1-modules               0.2.8
PyGObject                    3.42.0
pyparsing                    3.0.9
python-dateutil              2.8.2
pytz                         2022.1
requests                     2.28.0
requests-oauthlib            1.3.1
rsa                          4.8
setuptools                   59.6.0
six                          1.16.0
tensorboard                  2.9.1
tensorboard-data-server      0.6.1
tensorboard-plugin-wit       1.8.1
tensorflow                   2.9.1
tensorflow-decision-forests  0.2.6
tensorflow-estimator         2.9.0
tensorflow-io-gcs-filesystem 0.26.0
termcolor                    1.1.0
typing_extensions            4.2.0
urllib3                      1.26.9
Werkzeug                     2.1.2
wheel                        0.37.1
wrapt                        1.14.1
wurlitzer                    3.0.2
```

Installed c library:
```
# Install Tensorflow for C: https://www.tensorflow.org/install/lang_c
ENV FILENAME libtensorflow-cpu-linux-x86_64-2.9.1.tar.gz
RUN wget -c https://storage.googleapis.com/tensorflow/libtensorflow/${FILENAME}
RUN tar -C /usr/local -xzf ${FILENAME} 
RUN ldconfig /usr/local/lib
```

Created model:
```
root@2ea05d554c95:~/demo# saved_model_cli show --dir ./model --tag_set serve --signature_def serving_default
The given SavedModel SignatureDef contains the following input(s):
  inputs['input_1'] tensor_info:
      dtype: DT_INT64
      shape: (-1, 1)
      name: serving_default_input_1:0
The given SavedModel SignatureDef contains the following output(s):
  outputs['output_1'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1, 1)
      name: StatefulPartitionedCall:0
Method name is: tensorflow/serving/predict
```

Code:
```
#include <stdlib.h>
#include <stdio.h>
#include ""tensorflow/c/c_api.h""

void NoOpDeallocator(void* data, size_t a, void* b) {}

int main()
{
    //********* Read model
    TF_Graph* Graph = TF_NewGraph();
    TF_Status* Status = TF_NewStatus();

    TF_SessionOptions* SessionOpts = TF_NewSessionOptions();
    TF_Buffer* RunOpts = NULL;

    const char* saved_model_dir = ""model/"";
    const char* tags = ""serve""; // default model serving tag; can change in future
    int ntags = 1;

    TF_Session* Session = TF_LoadSessionFromSavedModel(SessionOpts, RunOpts, saved_model_dir, &tags, ntags, Graph, NULL, Status);
    if(TF_GetCode(Status) == TF_OK)
    {
        printf(""TF_LoadSessionFromSavedModel OK\n"");
    }
    else
    {
        printf(""%s"",TF_Message(Status));
    }

    //****** Get input tensor
    int NumInputs = 1;
    TF_Output* Input = malloc(sizeof(TF_Output) * NumInputs);

    TF_Output t0 = {TF_GraphOperationByName(Graph, ""serving_default_input_1""), 0};
    if(t0.oper == NULL)
        printf(""ERROR: Failed TF_GraphOperationByName serving_default_input_1\n"");
    else
	    printf(""TF_GraphOperationByName serving_default_input_1 is OK\n"");
    
    Input[0] = t0;
    
    //********* Get Output tensor
    int NumOutputs = 1;
    TF_Output* Output = malloc(sizeof(TF_Output) * NumOutputs);

    TF_Output t2 = {TF_GraphOperationByName(Graph, ""StatefulPartitionedCall""), 0};
    if(t2.oper == NULL)
        printf(""ERROR: Failed TF_GraphOperationByName StatefulPartitionedCall\n"");
    else	
	printf(""TF_GraphOperationByName StatefulPartitionedCall is OK\n"");
    
    Output[0] = t2;

    //********* Allocate data for inputs & outputs
    TF_Tensor** InputValues = (TF_Tensor**)malloc(sizeof(TF_Tensor*)*NumInputs);
    TF_Tensor** OutputValues = (TF_Tensor**)malloc(sizeof(TF_Tensor*)*NumOutputs);

    int ndims = 2;
    int64_t dims[] = {1,1};
    int64_t data[] = {20};
    int ndata = sizeof(int64_t); // This is tricky, it number of bytes not number of element

    TF_Tensor* int_tensor = TF_NewTensor(TF_FLOAT, dims, ndims, data, ndata, &NoOpDeallocator, 0);
    if (int_tensor != NULL)
    {
        printf(""TF_NewTensor is OK\n"");
    }
    else
	printf(""ERROR: Failed TF_NewTensor\n"");
    
    InputValues[0] = int_tensor;
    
    // //Run the Session
    TF_SessionRun(Session, NULL, Input, InputValues, NumInputs, Output, OutputValues, NumOutputs, NULL, 0,NULL , Status);

    if(TF_GetCode(Status) == TF_OK)
    {
        printf(""Session is OK\n"");
    }
    else
    {
        printf(""%s"",TF_Message(Status));
    }

    // //Free memory
    TF_DeleteGraph(Graph);
    TF_DeleteSession(Session, Status);
    TF_DeleteSessionOptions(SessionOpts);
    TF_DeleteStatus(Status);


    void* buff = TF_TensorData(OutputValues[0]);
    float* offsets = buff;
    printf(""Result Tensor :\n"");
    printf(""%f\n"",offsets[0]);

    return 0;
}
```
```


### Relevant log output

```shell
root@2ea05d554c95:~/demo# gcc -I/usr/local/include -L/usr/local/lib main.c -ltensorflow -o main; ./main
2022-06-14 15:26:32.875436: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: model/
2022-06-14 15:26:32.876544: I tensorflow/cc/saved_model/reader.cc:81] Reading meta graph with tags { serve }
2022-06-14 15:26:32.876598: I tensorflow/cc/saved_model/reader.cc:122] Reading SavedModel debug info (if present) from: model/
2022-06-14 15:26:32.876660: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-06-14 15:26:32.886525: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled
2022-06-14 15:26:32.887156: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.
2022-06-14 15:26:32.903110: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: model/
2022-06-14 15:26:32.909285: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 33857 microseconds.
TF_LoadSessionFromSavedModel OK
TF_GraphOperationByName serving_default_input_1 is OK
TF_GraphOperationByName StatefulPartitionedCall is OK
TF_NewTensor is OK
Segmentation fault
```
</details>"
56457,TF Lite NNAPI fails to load a model,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9.0

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18.04 (host), Android 11 (mobile device)

### Mobile device

Device based on Qualcomm Snapdragon 6490

### Python version

3.8

### Bazel version

4.2.1

### GCC/Compiler version

8.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When selecting TFLite's NNAPI delegate with the accelerator 'qti-dsp', it fails with an error during the model loading stage. For more information on the error please see the log below.
Expected behaviour is the model to be successfully loaded.
```


### Standalone code to reproduce the issue

```shell
Select TFLite's NNAPI delegate with the accelerator 'qti-dsp', while loading the model available at: https://github.com/tensorflow/tensorflow/files/8118439/test_model.zip
```


### Relevant log output

```shell
E/VersionedInterfaces: DeathHandler::serviceDied -- service unexpectedly died!
E/VersionedInterfaces: prepareModel_1_3 on qti-dsp failed because the PreparedModel object is dead
E/tflite: NN API returned error ANEURALNETWORKS_DEAD_OBJECT at line 4662 while completing NNAPI compilation.
```
</details>"
56456,"I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Source

binary

### Tensorflow Version

tf 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu18.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

cuda11.6/cuDNN8.4.1.50

### GPU model and memory

RTX2060

### Current Behaviour?

```shell
`v2.9.0-18-gd8ce9f9c301 2.9.1
2022-06-14 21:22:40.505725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
Num GPUs: 1
2022-06-14 21:22:40.527915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-06-14 21:22:40.528046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-06-14 21:22:40.528624: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-06-14 21:22:40.529244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-06-14 21:22:40.529347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-06-14 21:22:40.529434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-06-14 21:22:40.833024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-06-14 21:22:40.833140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-06-14 21:22:40.833228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-06-14 21:22:40.833306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4264 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5
2
b'Hello, TensorFlow!'
`
```


### Standalone code to reproduce the issue

```shell
`import tensorflow as tf

print(tf.version.GIT_VERSION, tf.version.VERSION)
physical_devices = tf.config.list_physical_devices('GPU')
print(""Num GPUs:"", len(physical_devices))

# time.sleep(2)
print(tf.add(1, 1).numpy())

hello = tf.constant('Hello, TensorFlow!')
print(hello.numpy())
`
```


### Relevant log output

_No response_</details>"
56454,XLA performance drop when use --xla_dump_to  flag,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9.0-rc0

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

Linux Ubuntu 18.04

### Python version

3.8.8

### Bazel version

5.0.0

### GCC/Compiler version

7.5.0

### CUDA/cuDNN version

11.1

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When use --xla_dump_tothe performance drop to 50% of original TensorFlow runtime

Environment: Tesla T4


1. python interpreter
Command: python3 test_mlp.py
Average time per iteration: 1.379ms

2. XLA auto clustering
Command: TF_XLA_FLAGS=""--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit"" python3 test_mlp.py
Average time per iteration: 1.395ms

3. XLA auto clustering with --xla_dump_to
Command: XLA_FLAGS=""--xla_dump_to=/tmp/test_mlp"" TF_XLA_FLAGS=""--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit"" python3 test_mlp.py
Average time per iteration: 2.52ms
```


### Standalone code to reproduce the issue

```shell
import numpy as np
import time
import tensorflow.compat.v1 as tf
tf.disable_eager_execution()

num_iter = 20
weights = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1), name=""weights"")
bias = tf.Variable(tf.random_normal([1, 3]), name=""bias"")
w = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))

x = tf.placeholder(tf.float32, shape=(None, 2), name='x')
labels = tf.placeholder(tf.float32, shape=(None, 1), name='labels')

mul = tf.matmul(x, weights, name=""matmul"")
pred = tf.add(mul, bias, name=""add"")
logits = tf.nn.relu(pred, name=""Relu"")
loss = tf.nn.softmax_cross_entropy_with_logits(
             labels=labels, logits=logits, name=""cross_entropy""
             )
optimizer = tf.train.AdamOptimizer(1e-3)
opt = optimizer.minimize(loss)

bs = 20480
avg_time = 0
warmup = 2
with tf.Session() as sess:
    sess.run(tf.initializers.global_variables())
    for i in range(num_iter):
        start = time.time()
        _, res_loss = sess.run([opt, loss],
                feed_dict={x: np.random.rand(bs, 2),
                           labels: np.random.randint(0, 1, (bs, 1))})
        end = time.time()
        if i >= warmup:
            avg_time += end - start
        print(""loss: "", res_loss, "", duration: "", (end - start) * 1000, "" (ms)"")

    print(""avg duration: "", (avg_time / (num_iter - warmup)) * 1000, "" (ms)"")
```


### Relevant log output

```shell
# XLA_FLAGS=""--xla_dump_to=/tmp/test_mlp"" TF_XLA_FLAGS=""--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit"" python3 test_mlp.py
2022-06-14 16:03:10.484916: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
WARNING:tensorflow:From /root/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

2022-06-14 16:03:11.907873: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-06-14 16:03:13.115903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13793 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:3b:00.0, compute capability: 7.5
2022-06-14 16:03:13.117128: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13793 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:af:00.0, compute capability: 7.5
2022-06-14 16:03:13.125430: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled
2022-06-14 16:03:13.397154: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f8ad400de60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-06-14 16:03:13.397189: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5
2022-06-14 16:03:13.397195: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5
2022-06-14 16:03:13.549700: I tensorflow/compiler/jit/xla_compilation_cache.cc:478] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-06-14 16:03:13.599047: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  1600.0151634216309  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  3.8709640502929688  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.8984546661376953  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.435445785522461  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.755403518676758  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.538919448852539  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.468109130859375  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.3980140686035156  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.3653507232666016  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.3200511932373047  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.5146007537841797  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.523183822631836  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.428770065307617  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.515077590942383  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.5157928466796875  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.872943878173828  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.4328231811523438  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.4433135986328125  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.4378299713134766  (ms)
loss:  [0. 0. 0. ... 0. 0. 0.] , duration:  2.5124549865722656  (ms)
avg duration:  2.520918846130371  (ms)
```
</details>"
56451,Failed to load delegate from libedgetpu.so.1 M.2 Accelerator with Dual Edge TPU," ### Issue Type
Support
### Source
binary
### Tensorflow Version
2.9.1
### Custom Code
No
### OS Platform and Distribution
Debian GNU/Linux 11 (bullseye)
### Mobile device
_No response_
### Python version
3.9.2
### Bazel version
_No response_
### GCC/Compiler version
_No response_
### CUDA/cuDNN version
_No response_
### GPU model and memory
_No response_

### Current Behaviour?
The image classifier test from https://www.coral.ai/docs/m2/get-started/ fails. I had this coral device installed and working in Frigate for months before it quit.

### Standalone code to reproduce the issue
```shell
user@nvr:~/coral/pycoral$ python3 examples/classify_image.py --model test_data/mobilenet_v2_1.0_224_inat_bird_quant_edgetpu.tflite --labels test_data/inat_bird_labels.txt --input test_data/parrot.jpg
Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/tflite_runtime/interpreter.py"", line 160, in load_delegate
    delegate = Delegate(library, options)
  File ""/usr/lib/python3/dist-packages/tflite_runtime/interpreter.py"", line 119, in __init__
    raise ValueError(capture.message)
ValueError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/user/coral/pycoral/examples/classify_image.py"", line 121, in <module>
    main()
  File ""/home/user/coral/pycoral/examples/classify_image.py"", line 71, in main
    interpreter = make_interpreter(*args.model.split('@'))
  File ""/usr/lib/python3/dist-packages/pycoral/utils/edgetpu.py"", line 87, in make_interpreter
    delegates = [load_edgetpu_delegate({'device': device} if device else {})]
  File ""/usr/lib/python3/dist-packages/pycoral/utils/edgetpu.py"", line 52, in load_edgetpu_delegate
    return tflite.load_delegate(_EDGETPU_SHARED_LIB, options or {})
  File ""/usr/lib/python3/dist-packages/tflite_runtime/interpreter.py"", line 162, in load_delegate
    raise ValueError('Failed to load delegate from {}\n{}'.format(
ValueError: Failed to load delegate from libedgetpu.so.1
```

### Any other info / logs
```shell
user@nvr:~/coral/pycoral$ ls /dev/apex_0
/dev/apex_0
```
```shell
user@nvr:/root$ lspci -nn | grep 089a
01:00.0 System peripheral [0880]: Global Unichip Corp. Coral Edge TPU [1ac1:089a]
```
```shell
root@nvr:~# dmesg | grep apex
[    3.313709] apex 0000:01:00.0: enabling device (0100 -> 0102)
[    3.314038] apex 0000:01:00.0: Couldn't initialize interrupts: -22
[    8.471106] apex 0000:01:00.0: Apex performance not throttled due to temperature
```
```shell
root@nvr:~# dmesg | grep 089a
[    0.315384] pci 0000:01:00.0: [1ac1:089a] type 00 class 0x0000ff
```
```shell
root@nvr:~# id user
uid=1000(user) gid=1000(user) groups=1000(user),24(cdrom),25(floppy),27(sudo),29(audio),30(dip),44(video),46(plugdev),107(render),109(netdev),1001(apex)
```"
56450,Add ParametricScalar Layer,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

TF 2.9

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
It would be nice to add a ParametricScalar layer where learnable scalars are multiplied element-wise with the input similar to the PReLU activation function. Although this layer can be implemented as a custom layer, it is a commonly used module to merge multiple results together with learnable weights. A sample implementation is provided in the ""Standalone Code"" section.
```


### Standalone code to reproduce the issue

```shell
class ParametricScalar(keras.layers.Layer):
    """"""combine multiple activations weighted by learnable variables""""""
    def __init__(self, alpha_initializer='ones', shared_axes=None, **kwargs):
        super().__init__(**kwargs)
        self.alpha_initializer = keras.initializers.get(alpha_initializer)
        if shared_axes is None:
            self.shared_axes = None
        elif not isinstance(shared_axes, (list, tuple)):
            self.shared_axes = [shared_axes]
        else:
            self.shared_axes = list(shared_axes)

    def get_config(self):
        return {'act_set': self.act_set}

    def build(self, input_shape):
        param_shape = list(input_shape[1:])
        if self.shared_axes is not None:
            for i in self.shared_axes:
                param_shape[i - 1] = 1
        self.alpha = self.add_weight(
            shape=param_shape,
            name='alpha',
            initializer=self.alpha_initializer)
        
    def get_config(self):
        config = {
            'alpha_initializer': keras.initializers.serialize(self.alpha_initializer),
            'shared_axes': self.shared_axes
        }
        base_config = super().get_config()
        return dict(list(base_config.items()) + list(config.items()))

    def call(self, inputs):
        return inputs * self.alpha
```


### Relevant log output

_No response_</details>"
56449,Lite Model Maker Object Detection not accepting background data,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Source

binary

### Tensorflow Version

tf 2.8.0

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 16.04

### Mobile device

_No response_

### Python version

3.9.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I cannot figure out how background data is supposed to be configured for TensorFlow Lite Model Maker Object Detection. I followed the tutorial found here (https://www.tensorflow.org/lite/models/modify/model_maker/object_detection), only replacing the tutorial dataset with my custom dataset. Everything is working correctly, and the model loads data and trains fine when all background data is removed, except I cannot figure out how background image data, which are the jpeg files in the dataset that have no associated bounding boxes, are supposed to be formatted in the .CSV file. This has resulted in a large number of false positives in my model.
```


### Standalone code to reproduce the issue

```shell
Following the formatting found here (https://cloud.google.com/vision/automl/object-detection/docs/csv-format), lines of background data in my .csv file are formatted as follows.

TEST,/folder/im3.jpeg,,,,,,,,,

However, when I try to load the CSV file into the dataset as per the tutorial, the program throws an error. It seems to be treating the CSV lines containing background data as it would treat a CSV line containing a bounding box, and I am not sure why it is not recognizing the CSV line as a line of background data.
```


### Relevant log output

```shell
Traceback (most recent call last):
File makemodel.py, line 20, in
train_data, validation_data, test_data = object_detector.DataLoader.from_csv(filename=os.getcwd() + /csvfile.csv, )
File /home/m/.local/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/data_util/object_detector_dataloader.py, line 292, in from_csv
cache_writer.write_files(cache_files, csv_lines=csv_lines)
File /home/m/.local/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/data_util/object_detector_dataloader_util.py, line 247, in write_files
for idx, xml_dict in enumerate(self._get_xml_dict(*args, **kwargs)):
File /home/m/.local/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/data_util/object_detector_dataloader_util.py, line 381, in _get_xml_dict
lines)
File /home/m/.local/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/data_util/object_detector_dataloader_util.py, line 337, in _get_xml_dict_from_csv_lines
xmin, ymin = float(line[3]) * width, float(line[4]) * height
ValueError: could not convert string to float:
```
</details>"
56448,Saved h5 failed to load with custom layers using hdf5_format,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.6 - 2.9

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Model loading fails with custom layer using hdf5_format lib.
It works with keras.models.load_model or with version TF 2.5.x, but starts failing from TF 2.6+. See log for details.
```


### Standalone code to reproduce the issue

```shell
from tensorflow import keras
import h5py
from tensorflow.python.keras.saving import hdf5_format


class CustomLayer(keras.layers.Layer):
    """"""combine multiple activations weighted by learnable variables""""""
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def get_config(self):
        return {}

    def build(self, input_shape):
        return

    def call(self, inputs):
        return inputs
    
path = 'test.h5'
    
x = keras.Input((5))
y = CustomLayer()(x)
model = keras.Model(x, y)
model.build(x)
model.save(path)

# this works ok
custom_objects = {'CustomLayer': CustomLayer}
model = keras.models.load_model(path, custom_objects=custom_objects)

# this fails
with h5py.File('test.h5', mode='r') as f:
    saved_model = hdf5_format.load_model_from_hdf5(
        f, custom_objects=custom_objects)
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""model_save.py"", line 36, in <module>
    saved_model = hdf5_format.load_model_from_hdf5(
  File ""/opt/conda/envs/iris/lib/python3.8/site-packages/tensorflow/python/keras/saving/hdf5_format.py"", line 180, in load_model_from_hdf5
    model = model_config_lib.model_from_config(model_config,
  File ""/opt/conda/envs/iris/lib/python3.8/site-packages/tensorflow/python/keras/saving/model_config.py"", line 52, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File ""/opt/conda/envs/iris/lib/python3.8/site-packages/tensorflow/python/keras/layers/serialization.py"", line 163, in deserialize
    return generic_utils.deserialize_keras_object(
  File ""/opt/conda/envs/iris/lib/python3.8/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 674, in deserialize_keras_object
    deserialized_obj = cls.from_config(
  File ""/opt/conda/envs/iris/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py"", line 667, in from_config
    input_tensors, output_tensors, created_layers = reconstruct_from_config(
  File ""/opt/conda/envs/iris/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py"", line 1310, in reconstruct_from_config
    layer_output_tensors = layer._inbound_nodes[node_index].output_tensors
IndexError: list index out of range
```
</details>"
56447,Tensorflow load_weights(0 function working in jupyter ntoebook but not in a script,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9.1

### OS Platform and Distribution

Windows

### Python version

3.8.10

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The above code when run in Jupyter notebook inside vscode runs as intended
but fails when run inside vscode as a .py file. I am using a virtual environment 
and only installed tensorflow, transformers and ipykernel for using as a jupyter kernel.

I get the following error when running it in a .py file.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from transformers import AutoTokenizer ,TFAutoModelForSequenceClassification, DataCollatorWithPadding


model_ckpt = 'distilbert-base-uncased'
num_labels = 6
batch_size = 16
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)


def predict(text:str)->int:


    tf_model = (TFAutoModelForSequenceClassification
        .from_pretrained(model_ckpt, num_labels=num_labels))

    tf_model.load_weights('./checkpoints/my_checkpoint')

    input_text_tokenized = tokenizer.encode(text,
                                            truncation=True,
                                            padding=True,
                                            return_tensors=""tf"")
    prediction = tf_model(input_text_tokenized)
    prediction_logits = prediction[0]
    emotion = np.argmax(prediction_logits[0])
    return prediction


x=predict(text='I am despair')
x
```


### Relevant log output

```shell
raise errors_impl.NotFoundError(None, None, error_message)
tensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./checkpoints/my_checkpoint
```
</details>

**Solved**
Tensorflow does not work well with relative paths. The solution was to provide an absolute path to the checkpoint when running it in the terminal. "
56444,Installing Python editable modules in Tensorflow Docker containers," ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.6.1

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Python version

3.8.10

### Current Behaviour?

Hi all,

I found a very intriguing bug in TF docker images. I've spent a couple of days on it but I'm still clueless.

When you install a Python module in TF docker images in **editable** mode (`-e` flag), the module is not listed in pip. The module however is still importable from python (`python3 -c 'import samplemod'`). Another (related) symptom that something is wrong is that the entrypoints defined in that module are not found either (which is the reason I found the bug in the first place).

When the module is installed normally (no `-e` flag), everything works as usual, and pip finds the module.

The bug first appeared in TF 2.6.1 and has remained since. TF 2.6.1 was the first version that upgraded to Ubuntu 20.04 (and python 3.8) so I suspected that it could be related to that. So I checked by building directly from Ubuntu base images. Here are the results:

* ubuntu:18.04 :heavy_check_mark:
```
Ubuntu 18.04.6 LTS
Python 3.6.9
pip 21.3.1 from /usr/local/lib/python3.6/dist-packages/pip (python 3.6)

$ pip3 list -e
Package Version Editable project location
------- ------- -------------------------
sample  0.1.0   /samplemod
```

* ubuntu:20.04 :x:
```
Ubuntu 20.04.4 LTS
Python 3.8.10
pip 22.1.2 from /usr/local/lib/python3.8/dist-packages/pip (python 3.8)

$ pip3 list -e
```

* tensorflow/tensorflow:2.6.0 :heavy_check_mark:
```
Ubuntu 18.04.5
Python 3.6.9
pip 21.3.1 from /usr/local/lib/python3.6/dist-packages/pip (python 3.6)

$ pip3 list -e
Package Version Editable project location
------- ------- -------------------------
sample  0.1.0   /samplemod
```

* tensorflow/tensorflow:2.6.1 :x:
```
Ubuntu 20.04.4 LTS
Python 3.8.10
pip 22.1.2 from /usr/local/lib/python3.8/dist-packages/pip (python 3.8)

$ pip3 list -e
```

So it looks indeed that is is related to Ubuntu/python, but I'm still opening an issue here because it looks like something that needs to be fixed when using TF docker images. You can find here the issues I opened in [AskUbuntu](https://askubuntu.com/questions/1413875/installing-python-editable-modules-in-ubuntu-20-04-docker-containers) and [pip](https://github.com/pypa/pip/issues/11189).

Thanks,

Ignacio

### Standalone code to reproduce the issue
I attach a _minimal_ Dockerfile to reproduce the issue:

```dockerfile
#ARG base=ubuntu:18.04  # WORKS
#ARG base=ubuntu:20.04  # DOES NOT WORK
#ARG base=tensorflow/tensorflow:2.6.0  # WORKS
ARG base=tensorflow/tensorflow:2.6.1  # DOES NOT WORK

FROM ${base}

# Refresh cache and install git
RUN DEBIAN_FRONTEND=noninteractive apt-get update && \
    apt-get install -y --no-install-recommends git

# Install Python for Ubuntu images
# Python versions mimic corresponding TF images
ARG base  # https://stackoverflow.com/a/56748289
RUN if [ ""$base"" = ""ubuntu:18.04"" ]; then \
        apt-get install -y --no-install-recommends python3.6 python3 python3-pip; \
    elif [ ""$base"" = ""ubuntu:20.04"" ]; then \
        apt-get install -y --no-install-recommends python3.8 python3 python3-pip; \
    fi
         
# Update pip
RUN pip3 install --upgrade pip setuptools wheel

# Install sample Python module in **editable** (-e) mode
RUN git clone https://github.com/navdeep-G/samplemod && \
    cd  samplemod && \
    pip3 install -e . && \
    cd ..

# Check behaviour
RUN cat /etc/issue
RUN python3 --version
RUN pip3 --version
RUN python3 -c ""import samplemod""
RUN pip3 list -e
```"
56443,"Adding additional image color and packing conversions for row images in order to save effort, space and improve performance ","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

tf 2.8

### Custom Code

No

### OS Platform and Distribution

any

### Mobile device

any

### Python version

n/a

### Bazel version

n/a

### GCC/Compiler version

n/a

### CUDA/cuDNN version

n/a

### GPU model and memory

any

### Current Behaviour?

```shell
Presently there are  only limited  amount of color and packing conversion for images . 
example : https://www.tensorflow.org/api_docs/python/tf/image/yuv_to_rgb   and  the packing  of the images is usually 1:1 type. 
Majority of test images from cameras are originally decoded as I420, or NV12 packing  (4:2:0) and  they need to get converted into RGB (or BGR) 4:4:4. 
It would be beneficial if https://www.tensorflow.org/api_docs/python/tf/image/ has a  NV12 (4:2:0) to RGB  layer more than YUV --> RGB (4:4:4 to 4:4:4). 
There are multiple reason for it:
   1 - Both Intel and nVidia (and many others) decode videos into NV12 
   2 - Majority of Images and Videos are stored in 4:2:0 packing  .
   3 - Reduce memory (or disk) footprint of using other tools to convert images  outside of TF (like FFMPEG, OpenCV...) 
   4 - Most importantly, it will allow for quick/significant bandwidth reduction by fussing first layer of many NN with the color-conversion+packing, considering that all of them use RGB (4:4:4) images as input . 
   5 - Finally, the user will be  using TF
```


### Standalone code to reproduce the issue

```shell
There are many color/packing converter examples  of Images  available in many other places. 
Find one wo  royalties.
```


### Relevant log output

```shell
n/a
```
</details>"
56442,Improvement of the training follow-up on the progress bar (i did a improvement),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Source

binary

### Tensorflow Version

2.4.0 to 2.9

### Custom Code

No

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.7.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
During training, a progress bar appears for monitoring.

However, we are mainly interested in the loss value, (the *loss* field) and the remaining time value (*ETA* field).

But it is not possible to have an estimate of the total time of the training in order to know when this stage is finished.
```


### Standalone code to reproduce the issue

```shell
trainer = DetectionModelTrainer()
trainer.setDataDirectory(anyFolder)
trainer.setModelTypeAsYOLOv3() 
trainer.setTrainConfig(object_names_array=[""anyClass""], batch_size=BATCH_SIZE, num_experiments=NUMBER_EXP)
trainer.trainModel()
```


### Relevant log output

_No response_</details>"
56441,"tf.linalg.qr does not support ""half"" datatype in the latest version","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Source

binary

### Tensorflow Version

tf 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 16.04

### Mobile device

_No response_

### Python version

3.7.13

### Bazel version

N/A

### GCC/Compiler version

N/A

### CUDA/cuDNN version

N/A

### GPU model and memory

N/A

### Current Behaviour?

```shell
`tf.linalg.qr` raise an exception when the input is in `half` data type as follows:

---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
<ipython-input-2-25e9e29d1dbe> in <module>()
      1 import tensorflow as tf
      2 a = tf.random.uniform((10,10), dtype=""half"")
----> 3 tf.linalg.qr(a)

1 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   7162 def raise_from_not_ok_status(e, name):
   7163   e.message += ("" name: "" + name if name is not None else """")
-> 7164   raise core._status_to_exception(e) from None  # pylint: disable=protected-access
   7165 
   7166 

NotFoundError: Could not find device for node: {{node Qr}} = Qr[T=DT_HALF, full_matrices=false]
All kernels registered for op Qr:
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_COMPLEX128]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_COMPLEX128]
 [Op:Qr]
```

But the documentation describes that this API supports `half` datatype: https://www.tensorflow.org/api_docs/python/tf/linalg/qr
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
a = tf.random.uniform((10,10), dtype=""half"")
tf.linalg.qr(a)
```
```


### Relevant log output

```shell
---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
<ipython-input-2-25e9e29d1dbe> in <module>()
      1 import tensorflow as tf
      2 a = tf.random.uniform((10,10), dtype=""half"")
----> 3 tf.linalg.qr(a)

1 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   7162 def raise_from_not_ok_status(e, name):
   7163   e.message += ("" name: "" + name if name is not None else """")
-> 7164   raise core._status_to_exception(e) from None  # pylint: disable=protected-access
   7165 
   7166 

NotFoundError: Could not find device for node: {{node Qr}} = Qr[T=DT_HALF, full_matrices=false]
All kernels registered for op Qr:
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_COMPLEX128]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_COMPLEX128]
 [Op:Qr]
```
</details>"
56439,How to upgrade Tensorflow-gpu code from 1.14 into Tensorflow-gpu code 2.8.0?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

binary

### Tensorflow Version

2.8.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 16.04

### Mobile device

Linux Ubuntu 16.04

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

cuda11.1

### GPU model and memory

32g

### Current Behaviour?

```shell
A bug happened!
Now, I want to upgrade and run the code of tensorflow-gpu==1.14 with cuda90 with a new cuda11.1 environment with tensorflow-gpu==2.8.0. But I met the following errors like this.

How to replace the ltensorflow_framework with the corresponding code in tensorflow 2.*? 

Thanks a lot.
```


### Standalone code to reproduce the issue

```shell
The code is here.

https://github.com/yanx27/SensatUrban_sol_tf

Error occurs in compiling sh init.sh.
```


### Relevant log output

```shell
Successfully opened dynamic library libcudart.so.11.0
/usr/bin/ld: cannot find -ltensorflow_framework
collect2: error: ld returned 1 exit status
/usr/bin/ld: cannot find -ltensorflow_framework
collect2: error: ld returned 1 exit status
/usr/bin/ld: cannot find -ltensorflow_framework
collect2: error: ld returned 1 exit status
/usr/bin/ld: cannot find -ltensorflow_framework
collect2: error: ld returned 1 exit status
```
</details>"
56438,Take-dataset,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

clang version 13.1.6

### CUDA/cuDNN version

_No response_

### GPU model and memory

Tesla 

### Current Behaviour?

```shell
A bug happened!
In take dataset when taking dataset more then the limit doesnt show error meanwhile if we train from that dataset it shows error
```


### Standalone code to reproduce the issue

```shell
test_dataset = data_set.take(1000) 
train_dataset = data_set.skip(1000)
lets assume data_set is the batch-dataset of batch size=32 and when we take 1000 then it shows green signal but when someone will try to fit the model with this it will an error
```


### Relevant log output

```shell
ValueError: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.
```
</details>"
56435,Tensorflow not working with oneDNN at Win10?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Win10

### Mobile device

_No response_

### Python version

python-3.7.9-amd64.exe

### Bazel version

bazel-5.0.0-windows-x86_64.exe

### GCC/Compiler version

vs2019

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I had build tensorflow lite c, But seem not include oneDNN.

bazel build //tensorflow/lite/c:tensorflowlite_c.dll -c opt --define tflite_with_xnnpack=true --config=mkl
```


### Standalone code to reproduce the issue

```shell
Nothing
```


### Relevant log output

_No response_</details>"
56434,Core Dump in model.save_weights() (ARM),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.8.0

### Custom Code

Yes

### OS Platform and Distribution

ARM8 (Raspberry Pi 4) with 8GB RAM

### Mobile device

Raspberry Pi 4 Bullseye

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
While using model.save_weights() a core dump is produced when it's used 2 times, or sometimes at first if training steps are not so much.

This behaviour can be reproduced using ModelIntervalCheckpoint() with a low interval, the first time the weights are saved fine the second time, it usually crashes (producint a core dump) or hangs up.

This has been tested on JupyterLab and also as a Python file with errors like:

terminate called after throwing an instance of 'std::system_error'
  what():  Invalid argument
Aborted (core dumped)

python3: pthread_mutex_lock.c:81: __pthread_mutex_lock: Assertion `mutex->__data.__owner == 0' failed.
Aborted (core dumped)
```


### Standalone code to reproduce the issue

```shell
# This code uses intentionally a low interval log to reproduce the error
from tensorflow.keras.optimizers import SGD, RMSprop

memory = SequentialMemory(limit=500000, window_length=WINDOW_LENGTH)
processor = AtariProcessor()
policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), 
                              attr='eps',
                              value_max=1.,
                              value_min=.1, 
                              value_test=.05,
                              nb_steps=1000000)

dqn = DQNAgent(model=model,
               nb_actions=nb_actions, 
               policy=policy,
               memory=memory, 
               processor=processor,
               nb_steps_warmup=50000,
               enable_double_dqn = True,
               enable_dueling_network = True,
               dueling_type = 'avg',
               gamma=.99,
               target_model_update=10000,
               delta_clip=1.,
               train_interval=32)

dqn.compile(RMSprop(lr=.001, momentum=0.9, epsilon=1e-06, rho=0.8), metrics=['mae'])

weights_filename = 'dqn_{}_weights.h5f'.format(env_name)
checkpoint_weights_filename = 'dqn_' + env_name + '_weights_{step}.h5f'
log_filename = 'dqn_{}_log.json'.format(env_name)
callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=100)]

dqn.fit(env, callbacks=callbacks, nb_steps=150, log_interval=10000, visualize=False, verbose=2)
```


### Relevant log output

```shell
Errors

terminate called after throwing an instance of 'std::system_error'
  what():  Invalid argument
Aborted (core dumped)

python3: pthread_mutex_lock.c:81: __pthread_mutex_lock: Assertion `mutex->__data.__owner == 0' failed.
Aborted (core dumped)
```
</details>"
56432,`crosstool_wrapper_driver_is_not_gcc` error while building tf v2.8.0 and v2.9.0,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

v2.8.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04 LTS

### Mobile device

--

### Python version

3.8.10

### Bazel version

4.2.1

### GCC/Compiler version

GCC 7.5.0

### CUDA/cuDNN version

11.2/8.1

### GPU model and memory

GTX 1050 2GB Mobile

### Current Behaviour?

```shell
Well, I expected the build to complete successfully from source. I had already tried with v2.9.0 without success and I have already followed [this](https://www.tensorflow.org/install/source) guide.
```


### Standalone code to reproduce the issue

```shell
`
$ git clone https://github.com/tensorflow/tensorflow
$ cd tensorflow
$ git checkout v2.8.0
$ ./configure  # CUDA support -> Y, CUDA compiler -> nvcc, GCC -> /usr/bin/gcc-7
$ bazel build --config=opt -c opt //tensorflow/tools/pip_package:build_pip_package
`
```


### Relevant log output

```shell
Extracting Bazel installation...
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=130
INFO: Reading rc options for 'build' from /home/edo/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/edo/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library
INFO: Reading rc options for 'build' from /home/edo/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1 --action_env LD_LIBRARY_PATH=/usr/local/cuda-11.2/lib64 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-7 --config=cuda
INFO: Reading rc options for 'build' from /home/edo/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /home/edo/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/edo/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda in file /home/edo/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:opt in file /home/edo/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare
INFO: Found applicable config definition build:linux in file /home/edo/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/edo/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/c3e082762b7664bbc7ffd2c39e86464928e27c0c.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
DEBUG: /home/edo/.cache/bazel/_bazel_edo/3e7782b7c78299bfd079af691b99f92d/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:10: 
Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1596824487 -0400""
DEBUG: Repository io_bazel_rules_docker instantiated at:
  /home/edo/tensorflow/WORKSPACE:23:14: in <toplevel>
  /home/edo/tensorflow/tensorflow/workspace0.bzl:108:34: in workspace
  /home/edo/.cache/bazel/_bazel_edo/3e7782b7c78299bfd079af691b99f92d/external/bazel_toolchains/repositories/repositories.bzl:35:23: in repositories
Repository rule git_repository defined at:
  /home/edo/.cache/bazel/_bazel_edo/3e7782b7c78299bfd079af691b99f92d/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/XNNPACK/archive/113092317754c7dea47bfb3cb49c4f59c3c1fa10.zip failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (450 packages loaded, 36243 targets configured).
INFO: Found 1 target...
ERROR: /home/edo/.cache/bazel/_bazel_edo/3e7782b7c78299bfd079af691b99f92d/external/llvm-project/mlir/BUILD.bazel:3841:11: Compiling mlir/lib/Dialect/SPIRV/IR/SPIRVDialect.cpp failed: (Exit 4): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt-exec-50AE0418/bin/external/llvm-project/mlir/_objs/SPIRVDialect/SPIRVDialect.d ... (remaining 136 argument(s) skipped)
x86_64-linux-gnu-gcc-7: internal compiler error: Killed (program cc1plus)
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 2909.274s, Critical Path: 230.50s
INFO: 13150 processes: 5685 internal, 7465 local.
FAILED: Build did NOT complete successfully
```
</details>"
56430,macos build error: no matching function for call to 'LsbMask'     return LsbMask<uint64_t>(bits);,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf 2.9.1

### Custom Code

No

### OS Platform and Distribution

macos 

### Mobile device

n/a

### Python version

3.7, 3.8, 3.9, 3.10

### Bazel version

4.2.2

### GCC/Compiler version

clang 13

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

```shell
./tensorflow/compiler/xla/util.h:568:12: error: no matching function for call to 'LsbMask'     return LsbMask<uint64_t>(bits);
```


### Standalone code to reproduce the issue

```shell
build like: https://github.com/ngam/tensorflow-feedstock/blob/v2.9.0/recipe/build.sh 



#!/bin/bash

set -ex

export PATH=""$PWD:$PATH""
export CC=$(basename $CC)
export CXX=$(basename $CXX)
export LIBDIR=$PREFIX/lib
export INCLUDEDIR=$PREFIX/include

export TF_IGNORE_MAX_BAZEL_VERSION=""1""

# Upstream docstring for TF_SYSTEM_LIBS in:
# https://github.com/tensorflow/tensorflow/blob/v{{ version }}/third_party/systemlibs/syslibs_configure.bzl
#   * `TF_SYSTEM_LIBS`: list of third party dependencies that should use
#      the system version instead
#
# To avoid bazel installing lots of vendored (python) packages,
# we need to install these packages through meta.yaml and then
# tell bazel to use them. Note that the names don't necessarily
# match PyPI or conda, but are defined in:
# https://github.com/tensorflow/tensorflow/blob/v{{ version }}/tensorflow/workspace<i>.bzl

# Exceptions and TODOs:
# Needs a bazel build:
# com_google_absl
# Build failures in tensorflow/core/platform/s3/aws_crypto.cc
# boringssl (i.e. system openssl)
# Most importantly: Write a patch that uses system LLVM libs for sure as well as MLIR and oneDNN/mkldnn
# TODO(check):
# absl_py
# com_github_googleapis_googleapis
# com_github_googlecloudplatform_google_cloud_cpp
# Needs c++17, try on linux
#  com_googlesource_code_re2
export TF_SYSTEM_LIBS=""
  absl_py
  astor_archive
  astunparse_archive
  boringssl
  com_github_googlecloudplatform_google_cloud_cpp
  com_github_grpc_grpc
  com_google_protobuf
  curl
  cython
  dill_archive
  flatbuffers
  gast_archive
  gif
  icu
  libjpeg_turbo
  org_sqlite
  png
  pybind11
  snappy
  zlib
  ""
sed -i -e ""s/GRPCIO_VERSION/${grpc_cpp}/"" tensorflow/tools/pip_package/setup.py

# do not build with MKL support
export TF_NEED_MKL=0
export BAZEL_MKL_OPT=""""

mkdir -p ./bazel_output_base
export BAZEL_OPTS=""""
# Set this to something as otherwise, it would include CFLAGS which itself contains a host path and this then breaks bazel's include path validation.
export CC_OPT_FLAGS=""-O2""

# Quick debug:
# cp -r ${RECIPE_DIR}/build.sh . && bazel clean && bash -x build.sh --logging=6 | tee log.txt
# Dependency graph:
# bazel query 'deps(//tensorflow/tools/lib_package:libtensorflow)' --output graph > graph.in
if [[ ""${target_platform}"" == osx-* ]]; then
  export LDFLAGS=""${LDFLAGS} -lz -framework CoreFoundation -Xlinker -undefined -Xlinker dynamic_lookup""
else
  export LDFLAGS=""${LDFLAGS} -lrt""
fi

source ${RECIPE_DIR}/gen-bazel-toolchain.sh

if [[ ""${target_platform}"" == ""osx-64"" ]]; then
  # Tensorflow doesn't cope yet with an explicit architecture (darwin_x86_64) on osx-64 yet.
  TARGET_CPU=darwin
fi

# If you really want to see what is executed, add --subcommands
BUILD_OPTS=""
    --crosstool_top=//custom_toolchain:toolchain
    --logging=6
    --verbose_failures
    --config=opt
    --define=PREFIX=${PREFIX}
    --define=PROTOBUF_INCLUDE_PATH=${PREFIX}/include
    --config=noaws
    --cpu=${TARGET_CPU}
    --local_cpu_resources=${CPU_COUNT}""

if [[ ""${target_platform}"" == ""osx-arm64"" ]]; then
  BUILD_OPTS=""${BUILD_OPTS} --config=macos_arm64""
fi
export TF_ENABLE_XLA=1
export BUILD_TARGET=""//tensorflow/tools/pip_package:build_pip_package //tensorflow/tools/lib_package:libtensorflow //tensorflow:libtensorflow_cc${SHLIB_EXT}""

# Python settings
export PYTHON_BIN_PATH=${PYTHON}
export PYTHON_LIB_PATH=${SP_DIR}
export USE_DEFAULT_PYTHON_LIB_PATH=1

# additional settings
export TF_NEED_OPENCL=0
export TF_NEED_OPENCL_SYCL=0
export TF_NEED_COMPUTECPP=0
export TF_NEED_CUDA=0
export TF_CUDA_CLANG=0
export TF_NEED_TENSORRT=0
export TF_NEED_ROCM=0
export TF_NEED_MPI=0
export TF_DOWNLOAD_CLANG=0
export TF_SET_ANDROID_WORKSPACE=0
export TF_CONFIGURE_IOS=0

# Get rid of unwanted defaults
sed -i -e ""/PROTOBUF_INCLUDE_PATH/c\ "" .bazelrc
sed -i -e ""/PREFIX/c\ "" .bazelrc


if [[ ${cuda_compiler_version} != ""None"" ]]; then
    export GCC_HOST_COMPILER_PATH=""${GCC}""
    export GCC_HOST_COMPILER_PREFIX=""$(dirname ${GCC})""

    export TF_CUDA_PATHS=""${PREFIX},${CUDA_HOME}""
    export TF_NEED_CUDA=1
    export TF_CUDA_VERSION=""${cuda_compiler_version}""
    export TF_CUDNN_VERSION=""${cudnn}""
    export TF_NCCL_VERSION=$(pkg-config nccl --modversion | grep -Po '\d+\.\d+')

    export LDFLAGS=""${LDFLAGS//-Wl,-z,now/-Wl,-z,lazy}""
    export CC_OPT_FLAGS=""-march=nocona -mtune=haswell""

    if [[ ${cuda_compiler_version} == 10.* ]]; then
        export TF_CUDA_COMPUTE_CAPABILITIES=sm_35,sm_50,sm_60,sm_62,sm_70,sm_72,sm_75,compute_75
    elif [[ ${cuda_compiler_version} == 11.0* ]]; then
        export TF_CUDA_COMPUTE_CAPABILITIES=sm_35,sm_50,sm_60,sm_62,sm_70,sm_72,sm_75,sm_80,compute_80
    elif [[ ${cuda_compiler_version} == 11.1 ]]; then
        export TF_CUDA_COMPUTE_CAPABILITIES=sm_35,sm_50,sm_60,sm_62,sm_70,sm_72,sm_75,sm_80,sm_86,compute_86
    elif [[ ${cuda_compiler_version} == 11.2 ]]; then
        export TF_CUDA_COMPUTE_CAPABILITIES=sm_35,sm_50,sm_60,sm_62,sm_70,sm_72,sm_75,sm_80,sm_86,compute_86
    else
        echo ""unsupported cuda version.""
        exit 1
    fi
fi

bazel clean --expunge
bazel shutdown

./configure

# build using bazel
bazel ${BAZEL_OPTS} build ${BUILD_OPTS} ${BUILD_TARGET}

# build a whl file
mkdir -p $SRC_DIR/tensorflow_pkg
bash -x bazel-bin/tensorflow/tools/pip_package/build_pip_package $SRC_DIR/tensorflow_pkg

# Build libtensorflow(_cc)
cp $SRC_DIR/bazel-bin/tensorflow/tools/lib_package/libtensorflow.tar.gz $SRC_DIR
mkdir -p $SRC_DIR/libtensorflow_cc_output/lib
if [[ ""${target_platform}"" == osx-* ]]; then
  cp -RP bazel-bin/tensorflow/libtensorflow_cc.* $SRC_DIR/libtensorflow_cc_output/lib/
  cp -RP bazel-bin/tensorflow/libtensorflow_framework.* $SRC_DIR/libtensorflow_cc_output/lib/
else
  cp -d bazel-bin/tensorflow/libtensorflow_cc.so* $SRC_DIR/libtensorflow_cc_output/lib/
  cp -d bazel-bin/tensorflow/libtensorflow_framework.so* $SRC_DIR/libtensorflow_cc_output/lib/
  cp -d $SRC_DIR/libtensorflow_cc_output/lib/libtensorflow_framework.so.2 $SRC_DIR/libtensorflow_cc_output/lib/libtensorflow_framework.so
fi
# Make writable so patchelf can do its magic
chmod u+w $SRC_DIR/libtensorflow_cc_output/lib/libtensorflow*

mkdir -p $SRC_DIR/libtensorflow_cc_output/include/tensorflow
rsync -r --chmod=D777,F666 --exclude '_solib*' --exclude '_virtual_includes/' --exclude 'pip_package/' --exclude 'lib_package/' --include '*/' --include '*.h' --include '*.inc' --exclude '*' bazel-bin/ $SRC_DIR/libtensorflow_cc_output/include
rsync -r --chmod=D777,F666 --include '*/' --include '*.h' --include '*.inc' --exclude '*' tensorflow/cc $SRC_DIR/libtensorflow_cc_output/include/tensorflow/
rsync -r --chmod=D777,F666 --include '*/' --include '*.h' --include '*.inc' --exclude '*' tensorflow/core $SRC_DIR/libtensorflow_cc_output/include/tensorflow/
rsync -r --chmod=D777,F666 --include '*/' --include '*' --exclude '*.cc' third_party/ $SRC_DIR/libtensorflow_cc_output/include/third_party/
rsync -r --chmod=D777,F666 --include '*/' --include '*' --exclude '*.txt' bazel-work/external/eigen_archive/Eigen/ $SRC_DIR/libtensorflow_cc_output/include/Eigen/
rsync -r --chmod=D777,F666 --include '*/' --include '*' --exclude '*.txt' bazel-work/external/eigen_archive/unsupported/ $SRC_DIR/libtensorflow_cc_output/include/unsupported/
pushd $SRC_DIR/libtensorflow_cc_output
  tar cf ../libtensorflow_cc_output.tar .
popd
rm -r $SRC_DIR/libtensorflow_cc_output

bazel clean

```
```


### Relevant log output

```shell
./tensorflow/compiler/xla/util.h:502:5: error: variable of non-literal type '::tensorflow::internal::CheckOpString' cannot be defined in a constexpr function
    DCHECK_GE(exponent, 0);
    ^
./tensorflow/core/platform/default/logging.h:472:31: note: expanded from macro 'DCHECK_GE'
#define DCHECK_GE(val1, val2) CHECK_GE(val1, val2)
                              ^
./tensorflow/core/platform/default/logging.h:459:30: note: expanded from macro 'CHECK_GE'
#define CHECK_GE(val1, val2) CHECK_OP(Check_GE, >=, val1, val2)
                             ^
./tensorflow/core/platform/default/logging.h:452:40: note: expanded from macro 'CHECK_OP'
#define CHECK_OP(name, op, val1, val2) CHECK_OP_LOG(name, op, val1, val2)
                                       ^
./tensorflow/core/platform/default/logging.h:445:48: note: expanded from macro 'CHECK_OP_LOG'
  while (::tensorflow::internal::CheckOpString _result{        \
                                               ^
./tensorflow/core/platform/default/logging.h:306:8: note: 'CheckOpString' is not literal because it is not an aggregate and has no constexpr constructors other than copy or move constructors
struct CheckOpString {
       ^
In file included from tensorflow/compiler/xla/util.cc:16:
./tensorflow/compiler/xla/util.h:559:12: error: no matching function for call to 'LsbMask'
    return LsbMask<uint64_t>(bits);
           ^~~~~~~~~~~~~~~~~
./tensorflow/compiler/xla/util.h:455:20: note: candidate template ignored: substitution failure [with T = unsigned long long]
constexpr inline T LsbMask(int width)
                   ^
3 errors generated.
CONDA_BUILD_SYSROOT=/Applications/Xcode_12.4.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.13.sdk x86_64-apple-darwin13.4.0-clang -MD -MF bazel-out/darwin-opt/bin/tensorflow/compiler/xla/_objs/util/util.pic.d -frandom-seed=bazel-out/darwin-opt/bin/tensorflow/compiler/xla/_objs/util/util.pic.o -fPIC -DEIGEN_MPL2_ONLY -DEIGEN_MAX_ALIGN_BYTES=64 -iquote . -iquote bazel-out/darwin-opt/bin -iquote external/com_google_absl -iquote bazel-out/darwin-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/darwin-opt/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/darwin-opt/bin/external/eigen_archive -iquote external/gif -iquote bazel-out/darwin-opt/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/darwin-opt/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/darwin-opt/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/darwin-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/darwin-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/darwin-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/darwin-opt/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/darwin-opt/bin/external/zlib -iquote external/double_conversion -iquote bazel-out/darwin-opt/bin/external/double_conversion -isystem external/nsync/public -isystem bazel-out/darwin-opt/bin/external/nsync/public -isystem external/eigen_archive -isystem bazel-out/darwin-opt/bin/external/eigen_archive -isystem external/farmhash_archive/src -isystem bazel-out/darwin-opt/bin/external/farmhash_archive/src -isystem $BUILD_PREFIX/include/c++/v1 -isystem $BUILD_PREFIX/lib/clang/13.0.1/include -isystem /Applications/Xcode_12.4.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.13.sdk/usr/include -isystem /Applications/Xcode_12.4.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.13.sdk/System/Library/Frameworks -isystem $PREFIX/include -march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O2 -pipe -isystem $PREFIX/include -fdebug-prefix-map=$SRC_DIR=/usr/local/src/conda/tensorflow-split-2.9.1 -fdebug-prefix-map=$PREFIX=/usr/local/src/conda-prefix -D_FORTIFY_SOURCE=2 -isystem $PREFIX/include -mmacosx-version-min=10.9 -march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O2 -pipe -stdlib=libc++ -fvisibility-inlines-hidden -fmessage-length=0 -isystem $PREFIX/include -fdebug-prefix-map=$SRC_DIR=/usr/local/src/conda/tensorflow-split-2.9.1 -fdebug-prefix-map=$PREFIX=/usr/local/src/conda-prefix -D_FORTIFY_SOURCE=2 -isystem $PREFIX/include -mmacosx-version-min=10.9 -DGRPC_BAZEL_BUILD -w -O2 -std=c++14 -c tensorflow/compiler/xla/util.cc -o bazel-out/darwin-opt/bin/tensorflow/compiler/xla/_objs/util/util.pic.o
INFO: Elapsed time: 16821.056s, Critical Path: 247.98s
INFO: 7663 processes: 795 internal, 6868 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
Traceback (most recent call last):
```
```
</details>"
56429,resource_variable_ops.variable_accessed can not watch a values.DistributedVariable,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

binary

### Tensorflow Version

tf 2.6

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

x

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tensorflow.python.ops.resource_variable_ops.variable_accessed can not watch a tensorflow.python.distribute.values.DistributedVariable

I try to make a custom custom variable and let it could be accessed when distributed training, but fail. Here is the code, and var create from a class inheriting resource_variable_ops.ResourceVariable. Besides, strategy is Mirror.


var_dist = values.DistributedVariable(strategy, var, VariableAggregation.ONLY_FIRST_REPLICA)
resource_variable_ops.variable_accessed(var_dist)
```

And it throw when ran in tape.py: *** SystemError: <built-in method TFE_Py_TapeVariableAccessed of PyCapsule object at 0x7fe053a75480> returned a result with an error set
```


### Standalone code to reproduce the issue

```shell
Sorry, I try.
```


### Relevant log output

_No response_</details>"
56428,ModuleNotFoundError: No module named 'tensorflow',"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
56427,Confusing wranings when `tf.data.dataset`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf2.9

### Custom Code

No

### OS Platform and Distribution

Windows 11

### Mobile device

N/A

### Python version

3.10.4

### Bazel version

N/A

### GCC/Compiler version

N/A

### CUDA/cuDNN version

11.7/8.4

### GPU model and memory

RTX3090 24GB

### Current Behaviour?

```shell
Just follow `tf.data.Dataset`'s documents, using 
the `tf.data.Dataset.cache`'s example code' to test the `cache()` function
but there will be some confusing warnings arising, such as 

   1. when establish the cache file for the first time, I got:

`W tensorflow/core/kernels/data/cache_dataset_ops.cc:296] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.`

   2. when use the cache file, I got:

`W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled`

I want to know why these wranings appera even completely consistent with the document code, or 
how can I use `tf.data.Dataset.cache` correctly. Thanks!
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
physical_devices = tf.config.experimental.list_physical_devices(device_type='GPU')
tf.config.experimental.set_memory_growth(physical_devices[0], True)
dataset = tf.data.Dataset.range(5)
dataset = dataset.map(lambda x: x**2)
dataset = dataset.cache(tf.constant(""./cache/file""))
# The first time reading through the data will generate the data using
# `range` and `map`.
list(dataset.as_numpy_iterator())

# Subsequent iterations read from the cache.
list(dataset.as_numpy_iterator())
```


### Relevant log output

```shell
W tensorflow/core/kernels/data/cache_dataset_ops.cc:296] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.

or

2022-06-11 20:45:07.212334: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled
2022-06-11 20:45:07.216874: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled
```
</details>"
56426,Executing genrule //tensorflow:tf_python_api_gen_v2 failed: (Exit 1): bash.exe failed: error executing command,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Windows 11 10.0.22000

### Mobile device

_No response_

### Python version

3.10.5

### Bazel version

5.0.0

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
intel-tensorflow failed to build. I followed the intel guide at https://www.intel.com/content/www/us/en/developer/articles/guide/optimization-for-tensorflow-installation-guide.html to no avail multiple times. I have been trying for days to get it to build. I'm sorry if this issue is not in a proper format.
```


### Standalone code to reproduce the issue

```shell
Follow the intel guide at https://www.intel.com/content/www/us/en/developer/articles/guide/optimization-for-tensorflow-installation-guide.html
Python version - 3.10.5
Bazel version - 5.0.0
pip version - 22.1.2
bash version - 5.1.016-1
cpu - i5 - 1135g7 (supports AVX instructions as far as I'm aware)
release version used - 2.9.1(intel tensorflow)
build command used -  --output_base=""C:/one_dnn_dir_final"" build --announce_rc --config=opt --config=mkl --action_env=PATH=""""C:/Python3/Scripts/"";""C:/Python3/"";""D:/output_dir/external/mkl/windows/lib"";""C:/ProgramData/chocolatey/bin"";""C:/msys64/usr/bin;C:/msys64"";'C:/Program Files/Git/cmd';'C:/Program Files/Git/usr/bin';'C:/Program Files (x86)/Intel/oneAPI/tbb/latest/redist/intel64/vc_mt/';'C:/Program Files (x86)/Intel/oneAPI/tbb/latest/redist/ia32/vc_mt/';'C:/Program Files (x86)/Intel/oneAPI/compiler/latest/windows/redist/intel64_win/compiler';'C:/Program Files (x86)/Intel/oneAPI/compiler/latest/windows/redist/ia32_win/compiler';'C:/Program Files/Eclipse Adoptium/jdk-17.0.1.12-hotspot/bin';'C:/Program Files/Microsoft/jdk-11.0.12.7-hotspot/bin';'C:/Program Files (x86)/Common Files/Oracle/Java/javapath';""C:/WINDOWS/system32"";""C:/WINDOWS"";""C:/WINDOWS/System32/Wbem"";""C:/WINDOWS/System32/WindowsPowerShell/v1.0/"";""C:/WINDOWS/System32/OpenSSH/"";'C:/Program Files/Microsoft SQL Server/150/Tools/Binn/';'C:/Program Files/Microsoft SQL Server/Client SDK/ODBC/170/Tools/Binn/';'C:/Program Files/dotnet/';'C:/Program Files/PowerShell/7/';""C:/one_dnn_dir_final"";""C:/Python2/"";""C:/Users/aryan/AppData/Local/Microsoft/WindowsApps"";'C:/Users/aryan/AppData/Local/Programs/Microsoft VS Code/bin';""C:/Users/aryan/.dotnet/tools"";'C:/Program Files/JetBrains/IntelliJ IDEA Community Edition 2021.3.1/bin';"" --noincompatible_do_not_split_linking_cmdline --define=no_tensorflow_py_deps=true tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
ERROR: C:/users/aryan/tensorflow/tensorflow/BUILD:1263:19: Executing genrule //tensorflow:tf_python_api_gen_v2 failed: (Exit 1): bash.exe failed: error executing command
  cd /d C:/one_dnn_dir_final/execroot/org_tensorflow
  SET PATH=C:/Python3/Scripts/;C:/Python3/;D:/output_dir/external/mkl/windows/lib;C:/ProgramData/chocolatey/bin;C:/msys64/usr/bin;C:/msys64;'C:/Program Files/Git/cmd';'C:/Program Files/Git/usr/bin';'C:/Program Files (x86)/Intel/oneAPI/tbb/latest/redist/intel64/vc_mt/';'C:/Program Files (x86)/Intel/oneAPI/tbb/latest/redist/ia32/vc_mt/';'C:/Program Files (x86)/Intel/oneAPI/compiler/latest/windows/redist/intel64_win/compiler';'C:/Program Files (x86)/Intel/oneAPI/compiler/latest/windows/redist/ia32_win/compiler';'C:/Program Files/Eclipse Adoptium/jdk-17.0.1.12-hotspot/bin';'C:/Program Files/Microsoft/jdk-11.0.12.7-hotspot/bin';'C:/Program Files (x86)/Common Files/Oracle/Java/javapath';C:/WINDOWS/system32;C:/WINDOWS;C:/WINDOWS/System32/Wbem;C:/WINDOWS/System32/WindowsPowerShell/v1.0/;C:/WINDOWS/System32/OpenSSH/;'C:/Program Files/Microsoft SQL Server/150/Tools/Binn/';'C:/Program Files/Microsoft SQL Server/Client SDK/ODBC/170/Tools/Binn/';'C:/Program Files/dotnet/';'C:/Program Files/PowerShell/7/';C:/one_dnn_dir_final;C:/Python2/;C:/Users/aryan/AppData/Local/Microsoft/WindowsApps;'C:/Users/aryan/AppData/Local/Programs/Microsoft VS Code/bin';C:/Users/aryan/.dotnet/tools;'C:/Program Files/JetBrains/IntelliJ IDEA Community Edition 2021.3.1/bin';
    SET PYTHON_BIN_PATH=C:/Python3/python.exe
    SET PYTHON_LIB_PATH=C:/Python3/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TF2_BEHAVIOR=1
  C:\msys64\usr\bin\bash.exe bazel-out/x64_windows-opt/bin/tensorflow/tf_python_api_gen_v2.genrule_script.sh
# Configuration: b2a8c312d15b397779224698fb37130236d93adde6a3e7a989797461b4f2abae
# Execution platform: @local_execution_config_platform//:platform
Traceback (most recent call last):
  File ""\\?\C:\Users\aryan\AppData\Local\Temp\Bazel.runfiles_qkyau8g0\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""\\?\C:\Users\aryan\AppData\Local\Temp\Bazel.runfiles_qkyau8g0\runfiles\org_tensorflow\tensorflow\python\tools\api\generator\create_python_api.py"", line 22, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""\\?\C:\Users\aryan\AppData\Local\Temp\Bazel.runfiles_qkyau8g0\runfiles\org_tensorflow\tensorflow\python\__init__.py"", line 36, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""\\?\C:\Users\aryan\AppData\Local\Temp\Bazel.runfiles_qkyau8g0\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow.py"", line 77, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""\\?\C:\Users\aryan\AppData\Local\Temp\Bazel.runfiles_qkyau8g0\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: C:/users/aryan/tensorflow/tensorflow/lite/python/BUILD:68:10 Middleman _middlemen/tensorflow_Slite_Spython_Stflite_Uconvert.exe-runfiles failed: (Exit 1): bash.exe failed: error executing command
  cd /d C:/one_dnn_dir_final/execroot/org_tensorflow
  SET PATH=C:/Python3/Scripts/;C:/Python3/;D:/output_dir/external/mkl/windows/lib;C:/ProgramData/chocolatey/bin;C:/msys64/usr/bin;C:/msys64;'C:/Program Files/Git/cmd';'C:/Program Files/Git/usr/bin';'C:/Program Files (x86)/Intel/oneAPI/tbb/latest/redist/intel64/vc_mt/';'C:/Program Files (x86)/Intel/oneAPI/tbb/latest/redist/ia32/vc_mt/';'C:/Program Files (x86)/Intel/oneAPI/compiler/latest/windows/redist/intel64_win/compiler';'C:/Program Files (x86)/Intel/oneAPI/compiler/latest/windows/redist/ia32_win/compiler';'C:/Program Files/Eclipse Adoptium/jdk-17.0.1.12-hotspot/bin';'C:/Program Files/Microsoft/jdk-11.0.12.7-hotspot/bin';'C:/Program Files (x86)/Common Files/Oracle/Java/javapath';C:/WINDOWS/system32;C:/WINDOWS;C:/WINDOWS/System32/Wbem;C:/WINDOWS/System32/WindowsPowerShell/v1.0/;C:/WINDOWS/System32/OpenSSH/;'C:/Program Files/Microsoft SQL Server/150/Tools/Binn/';'C:/Program Files/Microsoft SQL Server/Client SDK/ODBC/170/Tools/Binn/';'C:/Program Files/dotnet/';'C:/Program Files/PowerShell/7/';C:/one_dnn_dir_final;C:/Python2/;C:/Users/aryan/AppData/Local/Microsoft/WindowsApps;'C:/Users/aryan/AppData/Local/Programs/Microsoft VS Code/bin';C:/Users/aryan/.dotnet/tools;'C:/Program Files/JetBrains/IntelliJ IDEA Community Edition 2021.3.1/bin';
    SET PYTHON_BIN_PATH=C:/Python3/python.exe
    SET PYTHON_LIB_PATH=C:/Python3/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TF2_BEHAVIOR=1
  C:\msys64\usr\bin\bash.exe bazel-out/x64_windows-opt/bin/tensorflow/tf_python_api_gen_v2.genrule_script.sh
# Configuration: b2a8c312d15b397779224698fb37130236d93adde6a3e7a989797461b4f2abae
# Execution platform: @local_execution_config_platform//:platform
INFO: Elapsed time: 11069.283s, Critical Path: 848.02s
INFO: 11287 processes: 24 internal, 11263 local.
FAILED: Build did NOT complete successfully
```
</details>"
56425,TFLite - different inference results between Android and python,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

python: 2.7.0; android: 2.4.0

### Custom Code

No

### OS Platform and Distribution

Windows 10; Android 10

### Mobile device

Redmi Note 9

### Python version

3.7.9


### CUDA/cuDNN version

cuda_11.2.r11.2


### Current Behaviour?

```shell
The same tflite model gives different results on Android and in python using the same input data.
Expected behaviour: the results are same.
```


### Standalone code to reproduce the issue

```shell
Google Colab code:
https://colab.research.google.com/drive/1J_gE3JYiqS9MNml9phFiryznI-Bcc3-M

Android code: 
https://github.com/ivankrylatskoe/tfbug

Model file and input data file can be downloaded from here:
https://drive.google.com/file/d/1Ed07YKIZh69Rmso3EL9a7t4dCacJqEs2/view?usp=sharing
https://drive.google.com/file/d/1p2iAJVT8Wc46ho5Hh29O7n4bwLD38h3O/view?usp=sharing

or from here:
https://github.com/ivankrylatskoe/tfbug/tree/main/app/src/main/assets
```


### Relevant log output

```shell
Python output:

File input.dat checksum: 99cc12fcae36b2b07ce8e3afe8c1ed86
File model.tflite checksum: eb302aa27bd246fb2759e768ad3c3bc0
Model output:
[[ 12  13  14  16  10 253]
 [ 12  13  15  17  10 253]
 [ 12  13  15  17  10 253]
 [ 18  17  16  18  10 253]
 [ 18  17  18  21  10 253]
 [ 18  15  20  22  10 253]
 [ 25  18  15  16  10 253]
 [ 25  18  16  18  10 253]
 [ 25  18  17  19  10 253]
 [ 32  18  15  17  10 253]]

Android output:

2022-06-10 23:52:46.146 20317-20317/com.android.tfbug I/TFBug: File input.dat md5: 99cc12fcae36b2b07ce8e3afe8c1ed86
2022-06-10 23:52:46.252 20317-20317/com.android.tfbug I/TFBug: File model.tflite md5: eb302aa27bd246fb2759e768ad3c3bc0
2022-06-10 23:52:46.587 20317-20317/com.android.tfbug I/TFBug: Model output: 
    12 14 14 16 10 253 
    12 14 15 17 10 253 
    12 14 15 17 10 253 
    18 17 16 18 10 253 
    18 17 18 21 10 253 
    18 15 20 23 10 253 
    25 18 15 16 10 253 
    25 18 16 18 10 253 
    25 18 17 19 10 253 
    32 18 15 16 10 253
```


</details>"
56424,tensorflow.python.framework.errors_impl.NotFoundError: Failed to create a NewWriteableFile:,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf2.5

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.8.5

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
for name, model in models.items():
                                if model is not None:

                                    name_ = ""best_model_"" + name
                                    # model.save_weights(os.path.join(weights_dir, name_), save_format='tf')
                                    model.save(os.path.join(weights_dir, name_), save_format=""tf"")

The above is the code I used to save my model, and there is a problem named a subclassing model. It seems that TensorFlow did not solve it, and gives the error below,
```shell
tensorflow.python.framework.errors_impl.NotFoundError: Failed to create a NewWriteableFile: with_embedded_layer\sub_data\best_model_label_embedder\variables\variables_temp/part-00000-of-00001.data-00000-of-00001.tempstate7900589316842660676 : The system cannot find the path specified.
```
I did not know why Tensorflow introduces a wrong path delimiter in windows, i.e., `/` in the above path.
```


### Standalone code to reproduce the issue

```shell
tensorflow.python.framework.errors_impl.NotFoundError: Failed to create a NewWriteableFile: with_embedded_layer\sub_data\best_model_label_embedder\variables\variables_temp/part-00000-of-00001.data-00000-of-00001.tempstate7900589316842660676 : The system cannot find the path specified.
```


### Relevant log output

_No response_</details>"
56423,Use of Keras `jit_compile` in a distribution strategy causes a `std::system_error`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9.1

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2/8.1.1.33

### GPU model and memory

_No response_

### Current Behaviour?


The following error is thrown during training after a number of steps / epochs 
```shell
terminate called after throwing an instance of 'std::system_error'
what():  Resource temporarily unavailable
```
I am able to reproduce this error in colab with my sample code



### Standalone code to reproduce the issue

```python
import keras
import tensorflow as tf

def build_model_() -> keras.Model:
    input = tf.keras.layers.Input(shape=(5,), name='input_a')
    x = tf.keras.layers.Dense(512, activation = 'relu')(input)
    x = tf.keras.layers.Dense(512, activation = 'relu')(x)
    output = tf.keras.layers.Dense(1, name='output')(x)
    model = tf.keras.models.Model(inputs=input, outputs=output)
    return model


strategy = tf.distribute.MirroredStrategy()
print(f""Can see {strategy.num_replicas_in_sync} gpus"")
with strategy.scope():
  model = build_model_()
  model.compile(loss = 'mse', jit_compile=True)

BATCH_SIZE_PER_REPLICA = 1024
GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync

options = tf.data.Options()
options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA

dataset = tf.data.Dataset.from_tensors(
    (tf.ones(5), 1)
).repeat(10_000_000).batch(GLOBAL_BATCH_SIZE).with_options(options)

history = model.fit(
    x = dataset,
    epochs=7,
    verbose = 1,
)
```


### Relevant log output

_No response_</details>"
56422,Issue on building tensorflow-2.9.0 on centos 7.3,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.9.0

### Custom Code

No

### OS Platform and Distribution

centos 7.3.1

### Mobile device

_No response_

### Python version

3.8

### Bazel version

5.0.0

### GCC/Compiler version

9.3.1

### CUDA/cuDNN version

11.2/8.1

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Trying to build tensorflow 2.9.0 with bazel 5.0.0 from source. But always getting same issue while bulding.
I have downloaded the zip file of 2.9.0 and also tried with cloning the repo and checkout 2.9.0. But from both I see the same behaviour.

Command used to build the repo.

bazel build --config=cuda --config=v2 --config=opt --verbose_failures -c opt --cxxopt=-std=c++17 --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow:tensorflow_cc //tensorflow:install_headers tensorflow:tensorflow_framework //tensorflow/tools/..
```


### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

```shell
bazel build --config=cuda --config=v2 --config=opt --verbose_failures -c opt --cxxopt=-std=c++17 --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow:tensorflow_cc //tensorflow:install_headers tensorflow:tensorflow_framework //tensorflow/tools/...

Complete Logs:
Include Complete Log information or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached:

WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/093ed77f7d50f75b376f40a71ea86e08cedb8b80.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found

DEBUG: 
.cache/bazel/74ec22257a67dd1b8da1cbc24b2c24fb/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:10: 
Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default

WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found

ERROR: /remote/home/rrajeev/tensorflow/tensorflow/tools/toolchains/win_1803/py38/BUILD:9:11: in interpreter_path attribute of py_runtime rule //tensorflow/tools/toolchains/win_1803/py38:py3_runtime: must be an absolute path.

ERROR: /remote/home/rrajeev/tensorflow/tensorflow/tools/toolchains/win_1803/py38/BUILD:9:11: Analysis of target '//tensorflow/tools/toolchains/win_1803/py38:py3_runtime' failed

 Elapsed time: 80.921s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (95 packages loaded, 1584 targets configured)
    currently loading: @com_google_protobuf// ... (8 packages)
    Fetching ...zel_rules_docker; Cloning 9bfcd7dbf0294ed9d11a99da6363fc28df904502 of https://github.com/bazelbuild/rules_docker.git
    Fetching @local_execution_config_python; fetching
    Fetching @local_config_python; fetching
    Fetching @pasta; fetching
    Fetching @speech_commands; fetching
    Fetching https://storage.googleapis.com/download.tensorflow.org/models/inception_v1.zip
    Fetching .../pasta; Extracting .cache/bazel/external/pasta/temp27256293479294320\
98/v0.1.8.tar.gz
    Fetching https://dl.google.com/go/go1.12.5.linux-amd64.tar.gz ... (10 fetches)
```
```
</details>"
56420,Configuration Error: --define PYTHON_BIN_PATH='C:/Users/user/AppData/Local/Programs/Python/Python39/python.exe' is not executable. Is it the python binary?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf2.9.1

### Custom Code

No

### OS Platform and Distribution

win10

### Mobile device

_No response_

### Python version

3.9

### Bazel version

5.1.1

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
bazel build -c opt --config=monolithic --define=with_select_tf_ops=true tensorflow/lite/delegates/flex:tensorflowlite_flex
WARNING: The following configs were expanded more than once: [monolithic]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=190
INFO: Reading rc options for 'build' from d:\ai_framework\tensorflow_src\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/Users/user/AppData/Local/Programs/Python/Python39/python.exe
INFO: Reading rc options for 'build' from d:\ai_framework\tensorflow_src\.bazelrc:
  'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library
INFO: Reading rc options for 'build' from d:\ai_framework\tensorflow_src\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/user/AppData/Local/Programs/Python/Python39/python.exe --action_env PYTHON_LIB_PATH=C:/Users/user/AppData/Local/Programs/Python/Python39/lib/site-packages --python_path=C:/Users/user/AppData/Local/Programs/Python/Python39/python.exe --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true
INFO: Reading rc options for 'build' from d:\ai_framework\tensorflow_src\.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file d:\ai_framework\tensorflow_src\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file d:\ai_framework\tensorflow_src\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:monolithic in file d:\ai_framework\tensorflow_src\.bazelrc: --define framework_shared_object=false
INFO: Found applicable config definition build:windows in file d:\ai_framework\tensorflow_src\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file d:\ai_framework\tensorflow_src\.bazelrc: --define framework_shared_object=false
INFO: Repository local_execution_config_python instantiated at:
  D:/ai_framework/tensorflow_src/WORKSPACE:15:14: in <toplevel>
  D:/ai_framework/tensorflow_src/tensorflow/workspace2.bzl:870:19: in workspace
  D:/ai_framework/tensorflow_src/tensorflow/workspace2.bzl:91:27: in _tf_toolchains
  D:/ai_framework/tensorflow_src/tensorflow/tools/toolchains/remote_config/configs.bzl:6:28: in initialize_rbe_configs
  D:/ai_framework/tensorflow_src/tensorflow/tools/toolchains/remote_config/rbe_config.bzl:158:27: in _tensorflow_local_config
Repository rule local_python_configure defined at:
  D:/ai_framework/tensorflow_src/third_party/py/python_configure.bzl:280:41: in <toplevel>
ERROR: An error occurred during the fetch of repository 'local_execution_config_python':
   Traceback (most recent call last):
        File ""D:/ai_framework/tensorflow_src/third_party/py/python_configure.bzl"", line 213, column 22, in _create_local_python_repository
                _check_python_bin(repository_ctx, python_bin)
        File ""D:/ai_framework/tensorflow_src/third_party/py/python_configure.bzl"", line 145, column 25, in _check_python_bin
                auto_config_fail(""--define %s='%s' is not executable. Is it the python binary? %s"" % (
        File ""D:/ai_framework/tensorflow_src/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail
                fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg))
Error in fail: Configuration Error: --define PYTHON_BIN_PATH='C:/Users/user/AppData/Local/Programs/Python/Python39/python.exe' is not executable. Is it the python binary? <unknown object com.google.devtools.build.lib.bazel.repository.starlark.StarlarkRepositoryContext>
ERROR: D:/ai_framework/tensorflow_src/WORKSPACE:15:14: fetching local_python_configure rule //external:local_execution_config_python: Traceback (most recent call last):
        File ""D:/ai_framework/tensorflow_src/third_party/py/python_configure.bzl"", line 213, column 22, in _create_local_python_repository
                _check_python_bin(repository_ctx, python_bin)
        File ""D:/ai_framework/tensorflow_src/third_party/py/python_configure.bzl"", line 145, column 25, in _check_python_bin
                auto_config_fail(""--define %s='%s' is not executable. Is it the python binary? %s"" % (
        File ""D:/ai_framework/tensorflow_src/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail
                fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg))
Error in fail: Configuration Error: --define PYTHON_BIN_PATH='C:/Users/user/AppData/Local/Programs/Python/Python39/python.exe' is not executable. Is it the python binary? <unknown object com.google.devtools.build.lib.bazel.repository.starlark.StarlarkRepositoryContext>
ERROR: Analysis of target '//tensorflow/lite/delegates/flex:tensorflowlite_flex' failed; build aborted: Configuration Error: --define PYTHON_BIN_PATH='C:/Users/user/AppData/Local/Programs/Python/Python39/python.exe' is not executable. Is it the python binary? <unknown object com.google.devtools.build.lib.bazel.repository.starlark.StarlarkRepositoryContext>
INFO: Elapsed time: 0.226s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)
    Fetching @local_config_python; fetching
```


### Standalone code to reproduce the issue

```shell
python configure.py
bazel build -c opt --config=monolithic --define=with_select_tf_ops=true tensorflow/lite/delegates/flex:tensorflowlite_flex
```


### Relevant log output

_No response_</details>"
56419,error! when inferencing my model with tf-lite c++Api. it is well with tf-lite pythonApi,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

cApi: tf2.8    pythonApi:tf2.8

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.6

### Bazel version

_No response_

### GCC/Compiler version

7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
when I inferencing my model with tflite python, it could run successfully!
But when I inferencing my model with tflite c, it reported:
""""""
ERROR: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select
ERROR: Node number 333 (FlexConv2D) failed to prepare.
""""""
```


### Standalone code to reproduce the issue

```shell
const char* filename = ""./rvm_f16.tflite"";
const char* image_path = ""./meeting.png"";
    int src_h = 144;
    int src_w = 256;
    int r1i_h = 72;
    int r1i_w = 128;
    int r2i_h = 36;
    int r2i_w = 64;
    int r3i_h = 18;
    int r3i_w = 32;
    int r4i_h = 9;
    int r4i_w = 16;
    //read image
     cv::Mat srcImage = cv::imread(image_path, 1);
     cv::Mat priImage;
     cv::resize(srcImage, priImage, cv::Size(src_w, src_h), 0, 0, cv::INTER_NEAREST);
     priImage.convertTo(priImage, CV_32FC3, 1.0f/255);

    // Load model
    std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(filename);
    TFLITE_RVM_CHECK(model != nullptr);
    // Build the interpreter
    tflite::ops::builtin::BuiltinOpResolver resolver;
    tflite::InterpreterBuilder builder(*model, resolver);
    std::unique_ptr<tflite::Interpreter> interpreter;
    builder(&interpreter);
    TFLITE_RVM_CHECK(interpreter != nullptr);
    // Allocate tensor buffers.
    std::vector<int> src_size = {1, src_h, src_w, 3};
    std::vector<int> r1i_size = {1, r1i_h, r1i_w, 16};
    std::vector<int> r2i_size = {1, r2i_h, r2i_w, 20};
    std::vector<int> r3i_size = {1, r3i_h, r3i_w, 40};
    std::vector<int> r4i_size = {1, r4i_h, r4i_w, 64};
    interpreter->ResizeInputTensor(interpreter->inputs()[0], r2i_size);//2
    interpreter->ResizeInputTensor(interpreter->inputs()[1], r4i_size);//4
    interpreter->ResizeInputTensor(interpreter->inputs()[2], src_size);//0
    interpreter->ResizeInputTensor(interpreter->inputs()[3], r1i_size);//1
    interpreter->ResizeInputTensor(interpreter->inputs()[4], r3i_size);//3
    interpreter->AllocateTensors();
    float* src = interpreter->typed_input_tensor<float>(2);// std::cout<<interpreter->inputs().size()<<std::endl;
    float* r1i = interpreter->typed_input_tensor<float>(3);
    float* r2i = interpreter->typed_input_tensor<float>(0);
    float* r3i = interpreter->typed_input_tensor<float>(4);
    float* r4i = interpreter->typed_input_tensor<float>(1);
    memcpy(src, priImage.data, src_h*src_w*3*sizeof(float));
    memset(r1i, 0.0f, sizeof(float)*r1i_h*r1i_w*16);
    memset(r2i, 0.0f, sizeof(float)*r2i_h*r2i_w*20);
    memset(r3i, 0.0f, sizeof(float)*r3i_h*r3i_w*40);
    memset(r4i, 0.0f, sizeof(float)*r4i_h*r4i_w*64);
    interpreter->Invoke();
```


### Relevant log output

_No response_</details>"
56418,`tflite.invoke()` restart the kernel for transformer model (vision),"Source: binary
Tensorflow Version: 2.8
Custom Code: Yes

# Current Behaviour?

I've trained a vision transformer model and after training, I saved the model in `.tflite` format. Everything works without any issue until I reload the tf-lite model and try to make a prediction with it. It just restarts the kernel without showing any particular error message.


# Standalone code to reproduce the issue

HERE IS THE [GIST](https://colab.research.google.com/drive/1LMighhfcdlmK8jpy6_CJDfz7lC14QfSu?usp=sharing).
"
56416,tf.device('cpu') does not work under distributed strategies,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.10.0-dev20220520

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

N/A

### Python version

3.8

### Bazel version

N/A

### GCC/Compiler version

N/A

### CUDA/cuDNN version

11.2/8.1

### GPU model and memory

T4/16GB

### Current Behaviour?

```shell
I was trying to put a very large embedding on CPU under MirroredStrategy, but using tf.device to place the variable on CPU does not work.
```


### Standalone code to reproduce the issue

```shell
tf.config.set_soft_device_placement(True)
tf.debugging.set_log_device_placement(True)
strategy = tf.distribute.MirroredStrategy()

with tf.device('cpu'):
    var = tf.Variable(1.)

# Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
# Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0

with strategy.scope():
    with tf.device('cpu'):
        var = tf.Variable(1.)

# Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
# Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
# Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
# Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:1
# Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:1
# Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:1
# Executing op NoOp in device /job:localhost/replica:0/task:0/device:CPU:0
# Executing op NoOp in device /job:localhost/replica:0/task:0/device:CPU:0

```
```


### Relevant log output

_No response_</details>"
56415,ForwardAccumulator: The dtype of the watched primal must be floating,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hi! I noticed that `tf.ForwardAccumulator` is currently issuing a warning when it is used to compute a jvp of a real-valued function for complex input. I'm not sure if this is a historic artifact since the result seems correct despite the warning as the code example below demonstrates. I would appreciate if someone could clarify why the warning exists.

The example below implements two Hessian-vector product (hvp) functions, one based on reverse-over-reverse mode using two `tf.GradientTape` instances (i.e., the gradient of the directional derivative), and one based on forward-over-reverse mode using `tf.ForwardAccumulator` and `tf.GradientTape`. The latter issues the warning `WARNING:tensorflow:The dtype of the watched primal must be floating (e.g. tf.float32), got tf.complex64` which originates here: https://github.com/tensorflow/tensorflow/blame/d8ce9f9c301d021a69953134185ab728c1c248d3/tensorflow/python/eager/forwardprop.py#L399-L402.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf


def hessian_vector_product_forward_over_reverse(function, primals, tangents):
    with tf.GradientTape() as tape, tf.autodiff.ForwardAccumulator(
        primals, tangents
    ) as accumulator:
        for primal in primals:
            tape.watch(primal)
        gradients = tape.gradient(function(*primals), primals)
    return accumulator.jvp(gradients)


def hessian_vector_product_reverse_over_reverse(function, primals, tangents):
    with tf.GradientTape() as hvp_tape, tf.GradientTape() as gradient_tape:
        for primal in primals:
            gradient_tape.watch(primal)
            hvp_tape.watch(primal)
        gradients = gradient_tape.gradient(function(*primals), primals)
        dot_product = sum(
            [
                tf.math.real(
                    tf.tensordot(
                        tf.math.conj(gradient), tangent, axes=gradient.ndim
                    )
                )
                for gradient, tangent in zip(gradients, tangents)
            ]
        )
    return hvp_tape.gradient(dot_product, primals)


if __name__ == ""__main__"":
    tf.random.set_seed(42)

    def function(x):
        return tf.math.real(tf.reduce_sum(x**2))

    n = 10
    x = tf.complex(tf.random.normal((n,)), tf.random.normal((n,)))
    v = tf.complex(tf.random.normal((n,)), tf.random.normal((n,)))

    print(""Expected hvp:"", (2 * tf.math.conj(v),))
    print(
        ""Forward-over-reverse hvp:"",
        hessian_vector_product_forward_over_reverse(function, (x,), (v,)),
    )
    print(
        ""Reverse-over-reverse hvp:"",
        hessian_vector_product_reverse_over_reverse(function, (x,), (v,)),
    )
```


### Relevant log output

```shell
...
Expected hvp: (<tf.Tensor: shape=(10,), dtype=complex64, numpy=
array([-1.1181947 -1.3129735j, -1.0694427 +0.8261034j,
        4.7460666 -0.6799501j, -3.1451862 +2.0112543j,
        1.6110111 -1.4053247j, -1.6677539 +2.8017285j,
        0.61222446+1.7956151j,  4.532099  +0.6971235j,
        0.5712828 +1.9178125j, -3.1072311 -2.3896964j], dtype=complex64)>,)
WARNING:tensorflow:The dtype of the watched primal must be floating (e.g. tf.float32), got tf.complex64
Forward-over-reverse hvp: (<tf.Tensor: shape=(10,), dtype=complex64, numpy=
array([-1.1181947 -1.3129735j, -1.0694427 +0.8261034j,
        4.7460666 -0.6799501j, -3.1451862 +2.0112543j,
        1.6110111 -1.4053247j, -1.6677539 +2.8017285j,
        0.61222446+1.7956151j,  4.532099  +0.6971235j,
        0.5712828 +1.9178125j, -3.1072311 -2.3896964j], dtype=complex64)>,)
Reverse-over-reverse hvp: (<tf.Tensor: shape=(10,), dtype=complex64, numpy=
array([-1.1181947 -1.3129735j, -1.0694427 +0.8261034j,
        4.7460666 -0.6799501j, -3.1451862 +2.0112543j,
        1.6110111 -1.4053247j, -1.6677539 +2.8017285j,
        0.61222446+1.7956151j,  4.532099  +0.6971235j,
        0.5712828 +1.9178125j, -3.1072311 -2.3896964j], dtype=complex64)>,)
```
</details>"
56414,"`reduce_retracing` causing: ""Called a function referencing variables which have been deleted...""","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 18.04.6 LTS

### Mobile device

_No response_

### Python version

3.10.4

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When I run the below code I get the error:
""Called a function referencing variables which have been deleted. This likely means that function-local variables were created and not referenced elsewhere in the program. This is generally a mistake; consider storing variables in an object attribute on first call.""

I do not get the error if:
1: I use an older version of TensorFlow (with experimental_relax_shapes instead of reduce_retracing)
2: I set `reduce_retracing=False`.
3: I remove one of the `tf.function` calls.

I'm not sure exactly what's going on here, but there is some kind of interaction that is unstable.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

reduce_retracing = True
v = tf.Variable(0.0)

@tf.function(reduce_retracing=reduce_retracing)
def f(x: tf.Tensor) -> tf.Tensor:
    return x

@tf.function(reduce_retracing=reduce_retracing)
def loss() -> tf.Tensor:
    return f(v)

optimiser = tf.keras.optimizers.SGD()
optimiser.minimize(loss, var_list=[v])
optimiser.minimize(loss, var_list=[v])
```


### Relevant log output

```shell
2022-06-09 15:38:18.788974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-06-09 15:38:18.792122: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory
2022-06-09 15:38:18.792632: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2022-06-09 15:38:18.792867: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Traceback (most recent call last):
  File ""/home/jesper/src/GPflow/tf29_error.py"", line 16, in <module>
    optimiser.minimize(loss, var_list=[v])
  File ""/home/jesper/src/GPflow/.venv/max310/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py"", line 537, in minimize
    grads_and_vars = self._compute_gradients(
  File ""/home/jesper/src/GPflow/.venv/max310/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py"", line 581, in _compute_gradients
    loss = loss()
  File ""/home/jesper/src/GPflow/.venv/max310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/jesper/src/GPflow/.venv/max310/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py"", line 587, in deref
    raise AssertionError(
AssertionError: Called a function referencing variables which have been deleted. This likely means that function-local variables were created and not referenced elsewhere in the program. This is generally a mistake; consider storing variables in an object attribute on first call.
```
</details>"
56413,Could not load dynamic library 'libcudart.so.11.0',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.2, CUDNN 8.1.0

### GPU model and memory

GRID T4-8Q 

### Current Behaviour?

```shell
I followed the steps mentioned in the https://www.tensorflow.org/install/pip to install tensorflow 2.9. But unfortunately getting an warning as follows:

2022-06-09 15:50:37.711160: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2022-06-09 15:50:37.711178: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.

I further investigate and found that ""libcudart.so.11.0"" is present in /anaconda3/envs/<ENVIRONMENT NAME>/lib. But tensorflow is unable to find it. I tried to export the path to ./bashrc and to verify I ran command ""echo $PATH"" which return many paths and it include path /home/CAMPUS/<USER NAME>/anaconda3/envs/<ENVIRONMENT NAME>/lib. But still tensorflow is unable to find the ""libcudart.so.11.0"".

Please help !!!
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
```


### Relevant log output

_No response_</details>"
56412,tf.distribute.MultiWorkerMirroredStrategy documentation is vague for HPC clusters,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Feature Request

### Source

binary

### Tensorflow Version

tf 2.7

### Custom Code

No

### OS Platform and Distribution

Linux cluster

### Mobile device

Linux cluster

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2/8.2.1

### GPU model and memory

_No response_

### Current Behaviour?

Your current documentation on how to run ""tf.distribute.MultiWorkerMirroredStrategy"" with multiple nodes(machines) on a linux cluster is vague. It appears tf_config needs to be set manually on each machine. This can be troublesome if you have many machines. Is there a tensorflow native method to automate the submission of jobs using the multi worker strategy? 

I think you need to provide more documentation and examples for high performance linux clusters considering many users of distributed learning use HPC clusters.



### Standalone code to reproduce the issue

```shell
No specific code is required.
```


### Relevant log output

_No response_</details>"
56411,it is time-consuming to input the image data with tflite c-api,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

tf2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I find the official demo for tf-lite inference with c-api:
""""""
interpreter->AllocateTensors();
float* input = interpreter->typed_input_tensor<float>(0);
for(int i=0;i<921600;i++) input[i]=features[i];
interpreter->Invoke();
""""""
if an input is an image, traversal assignment like this is time-consuming. are there some other ways to input the image data.
```


### Standalone code to reproduce the issue

```shell
not yet
```


### Relevant log output

_No response_</details>"
56410,Tanh giving out of range output.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.8.2

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The tanh activation function is supposed to give an output in range [-1,1] but in this instance the output would sometimes have value in range [-9.999...,9.9999...]. Below I have attached a link to a repo where you can see the model architecture, the input and the output of the same, the model is also saved and uploaded in the same repo. The notebook was ran in colab and local environment where I faced similar result
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/github/AdityaPunetha/TensorFlow-TanH-Issue/blob/main/TanH_issue.ipynb
https://github.com/AdityaPunetha/TensorFlow-TanH-Issue
```


### Relevant log output

_No response_</details>"
56409,ios TensorFlowLiteCMetal link error,"### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

ios

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

5.0.0

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I compiled a TensorFlowLiteCMetal framework and when I finally link to it, it reports the following error

```shell

Showing All Messages
Undefined symbol: absl::lts_20211102::strings_internal::CatPieces(std::initializer_list<std::__1::basic_string_view<char, std::__1::char_traits<char> > >)

Undefined symbol: absl::lts_20211102::substitute_internal::SubstituteAndAppendArray(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >*, std::__1::basic_string_view<char, std::__1::char_traits<char> >, std::__1::basic_string_view<char, std::__1::char_traits<char> > const*, unsigned long)

Undefined symbol: absl::lts_20211102::NotFoundError(std::__1::basic_string_view<char, std::__1::char_traits<char> >)

Undefined symbol: absl::lts_20211102::UnknownError(std::__1::basic_string_view<char, std::__1::char_traits<char> >)

Undefined symbol: absl::lts_20211102::InvalidArgumentError(std::__1::basic_string_view<char, std::__1::char_traits<char> >)

Undefined symbol: absl::lts_20211102::InternalError(std::__1::basic_string_view<char, std::__1::char_traits<char> >)

Undefined symbol: absl::lts_20211102::ResourceExhaustedError(std::__1::basic_string_view<char, std::__1::char_traits<char> >)

```


### Standalone code to reproduce the issue

```shell
No standalone code.
```


### Relevant log output

_No response_</details>"
56406,Is it possible to sync tensorflow stream_executor's cuda stream with cuda stream outside?,"In tensorflow, streams including cuda streams are handled by stream_executer. Now I make some modification to tensorflow and create an independent cuda stream outside stream_executer. I was wondering is it possible to sync the independent cuda stream with the streams that wrap a cuda stream in stream_executer? 

I really appreciate if someone has advices."
56405,Error Running TFLite Mobile Benchmark Tool for iOS,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9

### Custom Code

No

### OS Platform and Distribution

Mac OS 12.4

### Mobile device

_No response_

### Python version

3.10.4

### Bazel version

5.1.1-homebrew

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I'm trying to run the TFLite Model Benchmark Tool for iOS devices.
I'm following the following instructions:
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark/ios
I've been able to do the following:
- download and configure the TensorFlow repo from source
- install and configure the Bazel workspace

But when I run the ""build_benchmark_framework.sh"", the shell file doesn't build the benchmark framework for iOS under TFLiteBenchmark/TFLiteBenchmark/Frameworks as expected.

Because of this, when I try to run the Xcode project, it gives the error:
""TensorFlowLiteBenchMarkC/TensorFlowLiteBenchmarkC.h file not found""
```


### Standalone code to reproduce the issue

```shell
I'm following the following instructions:
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark/ios
- download and configure the TensorFlow repo from source
- install and configure the Bazel workspace
```


### Relevant log output

_No response_</details>"
56404,TF.dataset.cache(path) still uses memory despite the cache file path,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.9.0-rc2-42-g8a20d54a3c1 2.9.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2 8.1

### GPU model and memory

_No response_

### Current Behaviour?

```shell
dataset.cache(path) uses the internal ram memory, which keeps increasing. At the same time, it also uses a local cache. In the discussion: https://discuss.tensorflow.org/t/tf-dataset-cache-path-still-uses-memory-despite-the-cache-file-path-is-given-in-tf2-5-2-8/8176?u=roshan
```


### Standalone code to reproduce the issue

```shell
dataset = tf.data.Dataset.from_generator(generator,output_types=(tf.string, tf.float32),
                                            output_shapes=((),(num_cls,)))
dataset = dataset.map(parse_function, num_parallel_calls=AUTOTUNE)
dataset = dataset.cache(path)
dataset = dataset.map(applyMask, num_parallel_calls=AUTOTUNE)
dataset = dataset.shuffle(SHUFFLE_BUFFER_SIZE)
```


### Relevant log output

_No response_</details>"
56403,Bug: bad marshal data (unknown type code),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I can load my model without any problem in the tf==2.9 and lower and python 3.7, but when I want to load the trained model in the python 3.9 with the same version of tf it gets a `bad marshal data (unknown type code)` error.
The model was trained in a lower version of python.
```


### Standalone code to reproduce the issue

```shell
# Download Model
! wget http://predimportance.mit.edu/data/xception_cl_fus_aspp_imp1k_10kl-3cc0.1mse-5nss5bc_bs4_ep30_valloss-2.5774_full.h5

# Imports 
from tensorflow import keras
from tensorflow.keras import backend as K

# Code:
ckpt_path = 'xception_cl_fus_aspp_imp1k_10kl-3cc0.1mse-5nss5bc_bs4_ep30_valloss-2.5774_full.h5'
model = keras.models.load_model(ckpt_path, custom_objects={""K"": K})
```


### Relevant log output

_No response_</details>"
56402,//tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu fails on manylinux2014_aarch64,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

CentOS 7

### Mobile device

n/a

### Python version

3.8.13

### Bazel version

5.1.1

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

```shell
Test fails with an abort from the C++ code. This started to fail after the move to C++17
```


### Standalone code to reproduce the issue

```shell
bazel test -s --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --jobs=75 --config=nonccl --strip=never --copt=-mtune=generic --copt=-march=armv8-a --copt=-Og --copt=-ggdb --verbose_failures --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --build_tests_only -- //tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu
```


### Relevant log output

```shell
bazel test -s --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --jobs=75 --config=nonccl --strip=never --copt=-mtune=generic --copt=-march=armv8-a --copt=-Og --copt=-ggdb --verbose_failures --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --build_tests_only -- //tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=131
INFO: Reading rc options for 'test' from /tmp/workspace/tensorflow-git/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'test' from /tmp/workspace/tensorflow-git/.bazelrc:
  Inherited 'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false
INFO: Reading rc options for 'test' from /tmp/workspace/tensorflow-git/.tf_configure.bazelrc:
  Inherited 'build' options: --action_env PYTHON_BIN_PATH=/tmp/workspace/venv-cp37-cp37/bin/python3 --action_env PYTHON_LIB_PATH=/tmp/workspace/venv-cp37-cp37/lib/python3.7/site-packages --python_path=/tmp/workspace/venv-cp37-cp37/bin/python3
INFO: Reading rc options for 'test' from /tmp/workspace/tensorflow-git/.bazelrc:
  Inherited 'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Reading rc options for 'test' from /tmp/workspace/tensorflow-git/.tf_configure.bazelrc:
  'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium
INFO: Found applicable config definition build:short_logs in file /tmp/workspace/tensorflow-git/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /tmp/workspace/tensorflow-git/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition test:v2 in file /tmp/workspace/tensorflow-git/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only
INFO: Found applicable config definition build:nonccl in file /tmp/workspace/tensorflow-git/.bazelrc: --define=no_nccl_support=true
INFO: Found applicable config definition build:linux in file /tmp/workspace/tensorflow-git/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /tmp/workspace/tensorflow-git/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Analyzed target //tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu (273 packages loaded, 20072 targets configured).
INFO: Found 1 test target...
SUBCOMMAND: # //tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu [action 'Testing //tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu', configuration: 999d8fdbd492fbab7145c4b176f7b45f3a833d5a353f2920c6f3f7f187db0233, execution platform: @local_execution_config_platform//:platform]
(cd /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow && \
  exec env - \
    EXPERIMENTAL_SPLIT_XML_GENERATION=1 \
    JAVA_RUNFILES=bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu.runfiles \
    LD_LIBRARY_PATH=/opt/rh/devtoolset-10/root/usr/lib64:/opt/rh/devtoolset-10/root/usr/lib:/opt/rh/devtoolset-10/root/usr/lib64/dyninst:/opt/rh/devtoolset-10/root/usr/lib/dyninst:/usr/local/lib64 \
    PATH=/root/.cache/bazelisk/downloads/bazelbuild/bazel-5.1.1-linux-arm64/bin:/tmp/workspace/venv-cp310-cp310/bin:/opt/rh/devtoolset-10/root/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PYTHON_BIN_PATH=/tmp/workspace/venv-cp37-cp37/bin/python3 \
    PYTHON_LIB_PATH=/tmp/workspace/venv-cp37-cp37/lib/python3.7/site-packages \
    PYTHON_RUNFILES=bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu.runfiles \
    RUNFILES_DIR=bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu.runfiles \
    RUN_UNDER_RUNFILES=1 \
    TEST_BINARY=tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu \
    TEST_INFRASTRUCTURE_FAILURE_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.infrastructure_failure \
    TEST_LOGSPLITTER_OUTPUT_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.raw_splitlogs/test.splitlogs \
    TEST_PREMATURE_EXIT_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.exited_prematurely \
    TEST_SIZE=small \
    TEST_SRCDIR=bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu.runfiles \
    TEST_TARGET=//tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu \
    TEST_TIMEOUT=300 \
    TEST_TMPDIR=_tmp/5071cc0f2378db80c84f18d85ecc569a \
    TEST_UNDECLARED_OUTPUTS_ANNOTATIONS=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.outputs_manifest/ANNOTATIONS \
    TEST_UNDECLARED_OUTPUTS_ANNOTATIONS_DIR=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.outputs_manifest \
    TEST_UNDECLARED_OUTPUTS_DIR=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.outputs \
    TEST_UNDECLARED_OUTPUTS_MANIFEST=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.outputs_manifest/MANIFEST \
    TEST_UNDECLARED_OUTPUTS_ZIP=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.outputs/outputs.zip \
    TEST_UNUSED_RUNFILES_LOG_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.unused_runfiles_log \
    TEST_WARNINGS_OUTPUT_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.warnings \
    TEST_WORKSPACE=org_tensorflow \
    TF2_BEHAVIOR=1 \
    TZ=UTC \
    XML_OUTPUT_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.xml \
  external/bazel_tools/tools/test/test-setup.sh tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu)
# Configuration: 999d8fdbd492fbab7145c4b176f7b45f3a833d5a353f2920c6f3f7f187db0233
# Execution platform: @local_execution_config_platform//:platform
SUBCOMMAND: # //tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu [action 'Testing //tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu', configuration: 999d8fdbd492fbab7145c4b176f7b45f3a833d5a353f2920c6f3f7f187db0233, execution platform: @local_execution_config_platform//:platform]
(cd /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow && \
  exec env - \
    EXPERIMENTAL_SPLIT_XML_GENERATION=1 \
    JAVA_RUNFILES=bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu.runfiles \
    LD_LIBRARY_PATH=/opt/rh/devtoolset-10/root/usr/lib64:/opt/rh/devtoolset-10/root/usr/lib:/opt/rh/devtoolset-10/root/usr/lib64/dyninst:/opt/rh/devtoolset-10/root/usr/lib/dyninst:/usr/local/lib64 \
    PATH=/root/.cache/bazelisk/downloads/bazelbuild/bazel-5.1.1-linux-arm64/bin:/tmp/workspace/venv-cp310-cp310/bin:/opt/rh/devtoolset-10/root/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PYTHON_BIN_PATH=/tmp/workspace/venv-cp37-cp37/bin/python3 \
    PYTHON_LIB_PATH=/tmp/workspace/venv-cp37-cp37/lib/python3.7/site-packages \
    PYTHON_RUNFILES=bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu.runfiles \
    RUNFILES_DIR=bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu.runfiles \
    RUN_UNDER_RUNFILES=1 \
    TEST_BINARY=tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu \
    TEST_INFRASTRUCTURE_FAILURE_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.infrastructure_failure \
    TEST_LOGSPLITTER_OUTPUT_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.raw_splitlogs/test.splitlogs \
    TEST_NAME=//tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu \
    TEST_PREMATURE_EXIT_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.exited_prematurely \
    TEST_SHARD_INDEX=0 \
    TEST_SIZE=small \
    TEST_SRCDIR=bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu.runfiles \
    TEST_TARGET=//tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu \
    TEST_TIMEOUT=300 \
    TEST_TMPDIR=_tmp/5071cc0f2378db80c84f18d85ecc569a \
    TEST_TOTAL_SHARDS=0 \
    TEST_UNDECLARED_OUTPUTS_ANNOTATIONS=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.outputs_manifest/ANNOTATIONS \
    TEST_UNDECLARED_OUTPUTS_ANNOTATIONS_DIR=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.outputs_manifest \
    TEST_UNDECLARED_OUTPUTS_DIR=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.outputs \
    TEST_UNDECLARED_OUTPUTS_MANIFEST=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.outputs_manifest/MANIFEST \
    TEST_UNDECLARED_OUTPUTS_ZIP=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.outputs/outputs.zip \
    TEST_UNUSED_RUNFILES_LOG_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.unused_runfiles_log \
    TEST_WARNINGS_OUTPUT_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.warnings \
    TEST_WORKSPACE=org_tensorflow \
    TF2_BEHAVIOR=1 \
    TZ=UTC \
    XML_OUTPUT_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.xml \
  external/bazel_tools/tools/test/generate-xml.sh bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.log bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.xml 14 134)
# Configuration: 999d8fdbd492fbab7145c4b176f7b45f3a833d5a353f2920c6f3f7f187db0233
# Execution platform: @local_execution_config_platform//:platform
FAIL: //tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu (see /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test_attempts/attempt_1.log)
SUBCOMMAND: # //tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu [action 'Testing //tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu', configuration: 999d8fdbd492fbab7145c4b176f7b45f3a833d5a353f2920c6f3f7f187db0233, execution platform: @local_execution_config_platform//:platform]
(cd /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow && \
  exec env - \
    EXPERIMENTAL_SPLIT_XML_GENERATION=1 \
    JAVA_RUNFILES=bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu.runfiles \
    LD_LIBRARY_PATH=/opt/rh/devtoolset-10/root/usr/lib64:/opt/rh/devtoolset-10/root/usr/lib:/opt/rh/devtoolset-10/root/usr/lib64/dyninst:/opt/rh/devtoolset-10/root/usr/lib/dyninst:/usr/local/lib64 \
    PATH=/root/.cache/bazelisk/downloads/bazelbuild/bazel-5.1.1-linux-arm64/bin:/tmp/workspace/venv-cp310-cp310/bin:/opt/rh/devtoolset-10/root/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PYTHON_BIN_PATH=/tmp/workspace/venv-cp37-cp37/bin/python3 \
    PYTHON_LIB_PATH=/tmp/workspace/venv-cp37-cp37/lib/python3.7/site-packages \
    PYTHON_RUNFILES=bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu.runfiles \
    RUNFILES_DIR=bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu.runfiles \
    RUN_UNDER_RUNFILES=1 \
    TEST_BINARY=tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu \
    TEST_INFRASTRUCTURE_FAILURE_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.infrastructure_failure \
    TEST_LOGSPLITTER_OUTPUT_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.raw_splitlogs/test.splitlogs \
    TEST_PREMATURE_EXIT_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.exited_prematurely \
    TEST_SIZE=small \
    TEST_SRCDIR=bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu.runfiles \
    TEST_TARGET=//tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu \
    TEST_TIMEOUT=300 \
    TEST_TMPDIR=_tmp/5071cc0f2378db80c84f18d85ecc569a \
    TEST_UNDECLARED_OUTPUTS_ANNOTATIONS=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.outputs_manifest/ANNOTATIONS \
    TEST_UNDECLARED_OUTPUTS_ANNOTATIONS_DIR=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.outputs_manifest \
    TEST_UNDECLARED_OUTPUTS_DIR=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.outputs \
    TEST_UNDECLARED_OUTPUTS_MANIFEST=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.outputs_manifest/MANIFEST \
    TEST_UNDECLARED_OUTPUTS_ZIP=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.outputs/outputs.zip \
    TEST_UNUSED_RUNFILES_LOG_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.unused_runfiles_log \
    TEST_WARNINGS_OUTPUT_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.warnings \
    TEST_WORKSPACE=org_tensorflow \
    TF2_BEHAVIOR=1 \
    TZ=UTC \
    XML_OUTPUT_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.xml \
  external/bazel_tools/tools/test/test-setup.sh tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu)
# Configuration: 999d8fdbd492fbab7145c4b176f7b45f3a833d5a353f2920c6f3f7f187db0233
# Execution platform: @local_execution_config_platform//:platform
SUBCOMMAND: # //tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu [action 'Testing //tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu', configuration: 999d8fdbd492fbab7145c4b176f7b45f3a833d5a353f2920c6f3f7f187db0233, execution platform: @local_execution_config_platform//:platform]
(cd /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow && \
  exec env - \
    EXPERIMENTAL_SPLIT_XML_GENERATION=1 \
    JAVA_RUNFILES=bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu.runfiles \
    LD_LIBRARY_PATH=/opt/rh/devtoolset-10/root/usr/lib64:/opt/rh/devtoolset-10/root/usr/lib:/opt/rh/devtoolset-10/root/usr/lib64/dyninst:/opt/rh/devtoolset-10/root/usr/lib/dyninst:/usr/local/lib64 \
    PATH=/root/.cache/bazelisk/downloads/bazelbuild/bazel-5.1.1-linux-arm64/bin:/tmp/workspace/venv-cp310-cp310/bin:/opt/rh/devtoolset-10/root/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PYTHON_BIN_PATH=/tmp/workspace/venv-cp37-cp37/bin/python3 \
    PYTHON_LIB_PATH=/tmp/workspace/venv-cp37-cp37/lib/python3.7/site-packages \
    PYTHON_RUNFILES=bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu.runfiles \
    RUNFILES_DIR=bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu.runfiles \
    RUN_UNDER_RUNFILES=1 \
    TEST_BINARY=tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu \
    TEST_INFRASTRUCTURE_FAILURE_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.infrastructure_failure \
    TEST_LOGSPLITTER_OUTPUT_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.raw_splitlogs/test.splitlogs \
    TEST_NAME=//tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu \
    TEST_PREMATURE_EXIT_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.exited_prematurely \
    TEST_SHARD_INDEX=0 \
    TEST_SIZE=small \
    TEST_SRCDIR=bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu.runfiles \
    TEST_TARGET=//tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu \
    TEST_TIMEOUT=300 \
    TEST_TMPDIR=_tmp/5071cc0f2378db80c84f18d85ecc569a \
    TEST_TOTAL_SHARDS=0 \
    TEST_UNDECLARED_OUTPUTS_ANNOTATIONS=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.outputs_manifest/ANNOTATIONS \
    TEST_UNDECLARED_OUTPUTS_ANNOTATIONS_DIR=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.outputs_manifest \
    TEST_UNDECLARED_OUTPUTS_DIR=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.outputs \
    TEST_UNDECLARED_OUTPUTS_MANIFEST=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.outputs_manifest/MANIFEST \
    TEST_UNDECLARED_OUTPUTS_ZIP=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.outputs/outputs.zip \
    TEST_UNUSED_RUNFILES_LOG_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.unused_runfiles_log \
    TEST_WARNINGS_OUTPUT_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.warnings \
    TEST_WORKSPACE=org_tensorflow \
    TF2_BEHAVIOR=1 \
    TZ=UTC \
    XML_OUTPUT_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.xml \
  external/bazel_tools/tools/test/generate-xml.sh bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.log bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.xml 14 134)
# Configuration: 999d8fdbd492fbab7145c4b176f7b45f3a833d5a353f2920c6f3f7f187db0233
# Execution platform: @local_execution_config_platform//:platform
FAIL: //tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu (see /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test_attempts/attempt_2.log)
SUBCOMMAND: # //tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu [action 'Testing //tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu', configuration: 999d8fdbd492fbab7145c4b176f7b45f3a833d5a353f2920c6f3f7f187db0233, execution platform: @local_execution_config_platform//:platform]
(cd /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow && \
  exec env - \
    EXPERIMENTAL_SPLIT_XML_GENERATION=1 \
    JAVA_RUNFILES=bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu.runfiles \
    LD_LIBRARY_PATH=/opt/rh/devtoolset-10/root/usr/lib64:/opt/rh/devtoolset-10/root/usr/lib:/opt/rh/devtoolset-10/root/usr/lib64/dyninst:/opt/rh/devtoolset-10/root/usr/lib/dyninst:/usr/local/lib64 \
    PATH=/root/.cache/bazelisk/downloads/bazelbuild/bazel-5.1.1-linux-arm64/bin:/tmp/workspace/venv-cp310-cp310/bin:/opt/rh/devtoolset-10/root/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PYTHON_BIN_PATH=/tmp/workspace/venv-cp37-cp37/bin/python3 \
    PYTHON_LIB_PATH=/tmp/workspace/venv-cp37-cp37/lib/python3.7/site-packages \
    PYTHON_RUNFILES=bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu.runfiles \
    RUNFILES_DIR=bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu.runfiles \
    RUN_UNDER_RUNFILES=1 \
    TEST_BINARY=tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu \
    TEST_INFRASTRUCTURE_FAILURE_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.infrastructure_failure \
    TEST_LOGSPLITTER_OUTPUT_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.raw_splitlogs/test.splitlogs \
    TEST_PREMATURE_EXIT_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.exited_prematurely \
    TEST_SIZE=small \
    TEST_SRCDIR=bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu.runfiles \
    TEST_TARGET=//tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu \
    TEST_TIMEOUT=300 \
    TEST_TMPDIR=_tmp/5071cc0f2378db80c84f18d85ecc569a \
    TEST_UNDECLARED_OUTPUTS_ANNOTATIONS=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.outputs_manifest/ANNOTATIONS \
    TEST_UNDECLARED_OUTPUTS_ANNOTATIONS_DIR=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.outputs_manifest \
    TEST_UNDECLARED_OUTPUTS_DIR=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.outputs \
    TEST_UNDECLARED_OUTPUTS_MANIFEST=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.outputs_manifest/MANIFEST \
    TEST_UNDECLARED_OUTPUTS_ZIP=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.outputs/outputs.zip \
    TEST_UNUSED_RUNFILES_LOG_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.unused_runfiles_log \
    TEST_WARNINGS_OUTPUT_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.warnings \
    TEST_WORKSPACE=org_tensorflow \
    TF2_BEHAVIOR=1 \
    TZ=UTC \
    XML_OUTPUT_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.xml \
  external/bazel_tools/tools/test/test-setup.sh tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu)
# Configuration: 999d8fdbd492fbab7145c4b176f7b45f3a833d5a353f2920c6f3f7f187db0233
# Execution platform: @local_execution_config_platform//:platform
SUBCOMMAND: # //tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu [action 'Testing //tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu', configuration: 999d8fdbd492fbab7145c4b176f7b45f3a833d5a353f2920c6f3f7f187db0233, execution platform: @local_execution_config_platform//:platform]
(cd /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow && \
  exec env - \
    EXPERIMENTAL_SPLIT_XML_GENERATION=1 \
    JAVA_RUNFILES=bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu.runfiles \
    LD_LIBRARY_PATH=/opt/rh/devtoolset-10/root/usr/lib64:/opt/rh/devtoolset-10/root/usr/lib:/opt/rh/devtoolset-10/root/usr/lib64/dyninst:/opt/rh/devtoolset-10/root/usr/lib/dyninst:/usr/local/lib64 \
    PATH=/root/.cache/bazelisk/downloads/bazelbuild/bazel-5.1.1-linux-arm64/bin:/tmp/workspace/venv-cp310-cp310/bin:/opt/rh/devtoolset-10/root/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PYTHON_BIN_PATH=/tmp/workspace/venv-cp37-cp37/bin/python3 \
    PYTHON_LIB_PATH=/tmp/workspace/venv-cp37-cp37/lib/python3.7/site-packages \
    PYTHON_RUNFILES=bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu.runfiles \
    RUNFILES_DIR=bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu.runfiles \
    RUN_UNDER_RUNFILES=1 \
    TEST_BINARY=tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu \
    TEST_INFRASTRUCTURE_FAILURE_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.infrastructure_failure \
    TEST_LOGSPLITTER_OUTPUT_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.raw_splitlogs/test.splitlogs \
    TEST_NAME=//tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu \
    TEST_PREMATURE_EXIT_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.exited_prematurely \
    TEST_SHARD_INDEX=0 \
    TEST_SIZE=small \
    TEST_SRCDIR=bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu.runfiles \
    TEST_TARGET=//tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu \
    TEST_TIMEOUT=300 \
    TEST_TMPDIR=_tmp/5071cc0f2378db80c84f18d85ecc569a \
    TEST_TOTAL_SHARDS=0 \
    TEST_UNDECLARED_OUTPUTS_ANNOTATIONS=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.outputs_manifest/ANNOTATIONS \
    TEST_UNDECLARED_OUTPUTS_ANNOTATIONS_DIR=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.outputs_manifest \
    TEST_UNDECLARED_OUTPUTS_DIR=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.outputs \
    TEST_UNDECLARED_OUTPUTS_MANIFEST=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.outputs_manifest/MANIFEST \
    TEST_UNDECLARED_OUTPUTS_ZIP=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.outputs/outputs.zip \
    TEST_UNUSED_RUNFILES_LOG_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.unused_runfiles_log \
    TEST_WARNINGS_OUTPUT_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.warnings \
    TEST_WORKSPACE=org_tensorflow \
    TF2_BEHAVIOR=1 \
    TZ=UTC \
    XML_OUTPUT_FILE=bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.xml \
  external/bazel_tools/tools/test/generate-xml.sh bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.log bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.xml 14 134)
# Configuration: 999d8fdbd492fbab7145c4b176f7b45f3a833d5a353f2920c6f3f7f187db0233
# Execution platform: @local_execution_config_platform//:platform
FAIL: //tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu (see /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.log)

FAILED: //tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu (Summary)
      /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.log
      /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test_attempts/attempt_1.log
      /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test_attempts/attempt_2.log
INFO: From Testing //tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu:
==================== Test output for //tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu:
[==========] Running 2 tests from 1 test suite.
[----------] Global test environment set-up.
[----------] 2 tests from SessionCoordinationTests/SingleClientTest
[ RUN      ] SessionCoordinationTests/SingleClientTest.SetGetConfigInOpTest/EnableClusterSpecPropagation
2022-06-07 21:47:24.274859: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job server_init -> {0 -> localhost:37979, 1 -> localhost:33977, 2 -> localhost:36596}
2022-06-07 21:47:24.275393: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:37979
2022-06-07 21:47:24.301207: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job server_init -> {0 -> localhost:37979, 1 -> localhost:33977, 2 -> localhost:36596}
2022-06-07 21:47:24.301658: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:33977
2022-06-07 21:47:24.327811: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job server_init -> {0 -> localhost:37979, 1 -> localhost:33977, 2 -> localhost:36596}
2022-06-07 21:47:24.328239: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:36596
terminate called after throwing an instance of 'std::out_of_range'
  what():  basic_string::replace
*** Received signal 6 ***
*** BEGIN MANGLED STACK TRACE ***
/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu.runfiles/org_tensorflow/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu[0x79a198]
[0xffffb69c07a0]
/lib64/libc.so.6(gsignal+0x4c)[0xffff956b5238]
/lib64/libc.so.6(abort+0x11c)[0xffff956b68b0]
/lib64/libstdc++.so.6(_ZN9__gnu_cxx27__verbose_terminate_handlerEv+0x16c)[0xffff958b16ec]
/lib64/libstdc++.so.6(+0x5f540)[0xffff958af540]
/lib64/libstdc++.so.6(+0x5f564)[0xffff958af564]
/lib64/libstdc++.so.6(__cxa_rethrow+0x0)[0xffff958af800]
/lib64/libstdc++.so.6(_ZSt20__throw_out_of_rangePKc+0x7c)[0xffff95902c60]
/lib64/libstdc++.so.6(_ZNSs7replaceEmmPKcm+0x1b0)[0xffff9590ea90]
/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/../../../../_solib_aarch64/_U_S_Stensorflow_Score_Sdistributed_Uruntime_Sintegration_Utest_Cc_Uapi_Usession_Ucoordination_Utest_Ucpu___Utensorflow/libtensorflow_framework.so.2(_ZN3re23RE27ReplaceEPSsRKS0_St17basic_string_viewIcSt11char_traitsIcEE+0x13c)[0xffffb5205c58]
/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/../../../../_solib_aarch64/libtensorflow_Score_Sdistributed_Uruntime_Slibmaster.so(+0x1a110)[0xffff9eafa110]
/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/../../../../_solib_aarch64/libtensorflow_Score_Sdistributed_Uruntime_Slibmaster.so(+0x1b2c4)[0xffff9eafb2c4]
/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu.runfiles/org_tensorflow/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu(_ZNKSt8functionIFvvEEclEv+0x18)[0x78e3e0]
/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/../../../../_solib_aarch64/_U_S_Stensorflow_Score_Sdistributed_Uruntime_Sintegration_Utest_Cc_Uapi_Usession_Ucoordination_Utest_Ucpu___Utensorflow/libtensorflow_framework.so.2(_ZNSt6thread11_State_implINS_8_InvokerISt5tupleIJSt8functionIFvvEEEEEEE6_M_runEv+0x10)[0xffffb5dca7d4]
/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/../../../../_solib_aarch64/_U_S_Stensorflow_Score_Sdistributed_Uruntime_Sintegration_Utest_Cc_Uapi_Usession_Ucoordination_Utest_Ucpu___Utensorflow/libtensorflow_framework.so.2(+0x1b48d7c)[0xffffb63f8d7c]
/lib64/libpthread.so.0(+0x7d38)[0xffff95aa7d38]
/lib64/libc.so.6(+0xdf690)[0xffff9575f690]
*** END MANGLED STACK TRACE ***

*** Begin stack trace ***
	tensorflow::CurrentStackTrace()
	
	__kernel_rt_sigreturn
	gsignal
	abort
	__gnu_cxx::__verbose_terminate_handler()
	
	
	__cxa_rethrow
	std::__throw_out_of_range(char const*)
	std::string::replace(unsigned long, unsigned long, char const*, unsigned long)
	re2::RE2::Replace(std::string*, re2::RE2 const&, std::basic_string_view<char, std::char_traits<char> >)
	
	
	std::function<void ()>::operator()() const
	std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::function<void ()> > > >::_M_run()
	
	
	
*** End stack trace ***
================================================================================
==================== Test output for //tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu:
[==========] Running 2 tests from 1 test suite.
[----------] Global test environment set-up.
[----------] 2 tests from SessionCoordinationTests/SingleClientTest
[ RUN      ] SessionCoordinationTests/SingleClientTest.SetGetConfigInOpTest/EnableClusterSpecPropagation
2022-06-07 21:47:39.039928: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job server_init -> {0 -> localhost:39379, 1 -> localhost:41961, 2 -> localhost:50214}
2022-06-07 21:47:39.040489: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:39379
2022-06-07 21:47:39.067278: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job server_init -> {0 -> localhost:39379, 1 -> localhost:41961, 2 -> localhost:50214}
2022-06-07 21:47:39.067802: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:41961
2022-06-07 21:47:39.094810: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job server_init -> {0 -> localhost:39379, 1 -> localhost:41961, 2 -> localhost:50214}
2022-06-07 21:47:39.095263: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:50214
terminate called after throwing an instance of 'std::out_of_range'
  what():  basic_string::replace
*** Received signal 6 ***
*** BEGIN MANGLED STACK TRACE ***
/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu.runfiles/org_tensorflow/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu[0x79a198]
[0xffffae5f07a0]
/lib64/libc.so.6(gsignal+0x4c)[0xffff8d2e5238]
/lib64/libc.so.6(abort+0x11c)[0xffff8d2e68b0]
/lib64/libstdc++.so.6(_ZN9__gnu_cxx27__verbose_terminate_handlerEv+0x16c)[0xffff8d4e16ec]
/lib64/libstdc++.so.6(+0x5f540)[0xffff8d4df540]
/lib64/libstdc++.so.6(+0x5f564)[0xffff8d4df564]
/lib64/libstdc++.so.6(__cxa_rethrow+0x0)[0xffff8d4df800]
/lib64/libstdc++.so.6(_ZSt20__throw_out_of_rangePKc+0x7c)[0xffff8d532c60]
/lib64/libstdc++.so.6(_ZNSs7replaceEmmPKcm+0x1b0)[0xffff8d53ea90]
/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/../../../../_solib_aarch64/_U_S_Stensorflow_Score_Sdistributed_Uruntime_Sintegration_Utest_Cc_Uapi_Usession_Ucoordination_Utest_Ucpu___Utensorflow/libtensorflow_framework.so.2(_ZN3re23RE27ReplaceEPSsRKS0_St17basic_string_viewIcSt11char_traitsIcEE+0x13c)[0xfffface35c58]
/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/../../../../_solib_aarch64/libtensorflow_Score_Sdistributed_Uruntime_Slibmaster.so(+0x1a110)[0xffff9672a110]
/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/../../../../_solib_aarch64/libtensorflow_Score_Sdistributed_Uruntime_Slibmaster.so(+0x1b2c4)[0xffff9672b2c4]
/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu.runfiles/org_tensorflow/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu(_ZNKSt8functionIFvvEEclEv+0x18)[0x78e3e0]
/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/../../../../_solib_aarch64/_U_S_Stensorflow_Score_Sdistributed_Uruntime_Sintegration_Utest_Cc_Uapi_Usession_Ucoordination_Utest_Ucpu___Utensorflow/libtensorflow_framework.so.2(_ZNSt6thread11_State_implINS_8_InvokerISt5tupleIJSt8functionIFvvEEEEEEE6_M_runEv+0x10)[0xffffad9fa7d4]
/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/../../../../_solib_aarch64/_U_S_Stensorflow_Score_Sdistributed_Uruntime_Sintegration_Utest_Cc_Uapi_Usession_Ucoordination_Utest_Ucpu___Utensorflow/libtensorflow_framework.so.2(+0x1b48d7c)[0xffffae028d7c]
/lib64/libpthread.so.0(+0x7d38)[0xffff8d6d7d38]
/lib64/libc.so.6(+0xdf690)[0xffff8d38f690]
*** END MANGLED STACK TRACE ***

*** Begin stack trace ***
	tensorflow::CurrentStackTrace()
	
	__kernel_rt_sigreturn
	gsignal
	abort
	__gnu_cxx::__verbose_terminate_handler()
	
	
	__cxa_rethrow
	std::__throw_out_of_range(char const*)
	std::string::replace(unsigned long, unsigned long, char const*, unsigned long)
	re2::RE2::Replace(std::string*, re2::RE2 const&, std::basic_string_view<char, std::char_traits<char> >)
	
	
	std::function<void ()>::operator()() const
	std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::function<void ()> > > >::_M_run()
	
	
	
*** End stack trace ***
================================================================================
==================== Test output for //tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu:
[==========] Running 2 tests from 1 test suite.
[----------] Global test environment set-up.
[----------] 2 tests from SessionCoordinationTests/SingleClientTest
[ RUN      ] SessionCoordinationTests/SingleClientTest.SetGetConfigInOpTest/EnableClusterSpecPropagation
2022-06-07 21:47:53.786623: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job server_init -> {0 -> localhost:40779, 1 -> localhost:50024, 2 -> localhost:33378}
2022-06-07 21:47:53.787166: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:40779
2022-06-07 21:47:53.814843: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job server_init -> {0 -> localhost:40779, 1 -> localhost:50024, 2 -> localhost:33378}
2022-06-07 21:47:53.815308: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:50024
2022-06-07 21:47:53.843121: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job server_init -> {0 -> localhost:40779, 1 -> localhost:50024, 2 -> localhost:33378}
2022-06-07 21:47:53.843579: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:33378
terminate called after throwing an instance of 'std::out_of_range'
  what():  basic_string::replace
*** Received signal 6 ***
*** BEGIN MANGLED STACK TRACE ***
/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu.runfiles/org_tensorflow/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu[0x79a198]
[0xffffaf8e07a0]
/lib64/libc.so.6(gsignal+0x4c)[0xffff8e5d5238]
/lib64/libc.so.6(abort+0x11c)[0xffff8e5d68b0]
/lib64/libstdc++.so.6(_ZN9__gnu_cxx27__verbose_terminate_handlerEv+0x16c)[0xffff8e7d16ec]
/lib64/libstdc++.so.6(+0x5f540)[0xffff8e7cf540]
/lib64/libstdc++.so.6(+0x5f564)[0xffff8e7cf564]
/lib64/libstdc++.so.6(__cxa_rethrow+0x0)[0xffff8e7cf800]
/lib64/libstdc++.so.6(_ZSt20__throw_out_of_rangePKc+0x7c)[0xffff8e822c60]
/lib64/libstdc++.so.6(_ZNSs7replaceEmmPKcm+0x1b0)[0xffff8e82ea90]
/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/../../../../_solib_aarch64/_U_S_Stensorflow_Score_Sdistributed_Uruntime_Sintegration_Utest_Cc_Uapi_Usession_Ucoordination_Utest_Ucpu___Utensorflow/libtensorflow_framework.so.2(_ZN3re23RE27ReplaceEPSsRKS0_St17basic_string_viewIcSt11char_traitsIcEE+0x13c)[0xffffae125c58]
/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/../../../../_solib_aarch64/libtensorflow_Score_Sdistributed_Uruntime_Slibmaster.so(+0x1a110)[0xffff97a1a110]
/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/../../../../_solib_aarch64/libtensorflow_Score_Sdistributed_Uruntime_Slibmaster.so(+0x1b2c4)[0xffff97a1b2c4]
/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu.runfiles/org_tensorflow/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu(_ZNKSt8functionIFvvEEclEv+0x18)[0x78e3e0]
/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/../../../../_solib_aarch64/_U_S_Stensorflow_Score_Sdistributed_Uruntime_Sintegration_Utest_Cc_Uapi_Usession_Ucoordination_Utest_Ucpu___Utensorflow/libtensorflow_framework.so.2(_ZNSt6thread11_State_implINS_8_InvokerISt5tupleIJSt8functionIFvvEEEEEEE6_M_runEv+0x10)[0xffffaecea7d4]
/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/integration_test/../../../../_solib_aarch64/_U_S_Stensorflow_Score_Sdistributed_Uruntime_Sintegration_Utest_Cc_Uapi_Usession_Ucoordination_Utest_Ucpu___Utensorflow/libtensorflow_framework.so.2(+0x1b48d7c)[0xffffaf318d7c]
/lib64/libpthread.so.0(+0x7d38)[0xffff8e9c7d38]
/lib64/libc.so.6(+0xdf690)[0xffff8e67f690]
*** END MANGLED STACK TRACE ***

*** Begin stack trace ***
	tensorflow::CurrentStackTrace()
	
	__kernel_rt_sigreturn
	gsignal
	abort
	__gnu_cxx::__verbose_terminate_handler()
	
	
	__cxa_rethrow
	std::__throw_out_of_range(char const*)
	std::string::replace(unsigned long, unsigned long, char const*, unsigned long)
	re2::RE2::Replace(std::string*, re2::RE2 const&, std::basic_string_view<char, std::char_traits<char> >)
	
	
	std::function<void ()>::operator()() const
	std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::function<void ()> > > >::_M_run()
	
	
	
*** End stack trace ***
================================================================================
Target //tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu up-to-date:
  bazel-bin/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu
INFO: Elapsed time: 76.383s, Critical Path: 63.64s
INFO: 2 processes: 6 local.
INFO: Build completed, 1 test FAILED, 2 total actions
//tensorflow/core/distributed_runtime/integration_test:c_api_session_coordination_test_cpu FAILED in 3 out of 3 in 14.8s
  Stats over 3 runs: max = 14.8s, min = 14.6s, avg = 14.7s, dev = 0.1s
  /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test.log
  /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test_attempts/attempt_1.log
  /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/core/distributed_runtime/integration_test/c_api_session_coordination_test_cpu/test_attempts/attempt_2.log

INFO: Build completed, 1 test FAILED, 2 total actions
```
</details>"
56399,Warnings (or any stderr writes) cause Python configuration to fail,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

v2.5.0 (and many others)

### Custom Code

No

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

N/A

### Python version

3.10

### Bazel version

4.2.2

### GCC/Compiler version

11.2.0

### CUDA/cuDNN version

N/A

### GPU model and memory

N/A

### Current Behaviour?

The [current definition](https://github.com/tensorflow/tensorflow/blob/7fd6bc43e4d2ba1ee06ae80316e24ec43e997987/third_party/remote_config/common.bzl#L229) of the `execute()` helper for TensorFlow treats any writes to `stderr` as a fatal error.

This causes #56373, and can also cause the `third_party/py/python_configure.bzl` helpers [`_get_python_include(), _get_python_import_lib_name()`](https://github.com/tensorflow/tensorflow/blob/7fd6bc43e4d2ba1ee06ae80316e24ec43e997987/third_party/py/python_configure.bzl#L150-L186) to fail if the Python runtime issues any warnings, which is not an uncommon event. The `Dockerfile` attached to this issue illustrates a downstream failure caused by a warning issued in Python 3.10 because of the [deprecation of `distutils` (PEP 632)](https://www.python.org/dev/peps/pep-0632/)

I have observed this behavior in 2.5.0, but this failure mode should be common to most extant versions of TensorFlow.

### Standalone code to reproduce the issue

Sorry about the state of this reproduction, I did try to make a simpler one that depended only on the `tensorflow` repo but couldn't whittle it down any further.

```Dockerfile
FROM ubuntu:22.04 as repro

ENV DEBIAN_FRONTEND=noninteractive
ENV LANG C.UTF-8

# install Python (and numpy)
RUN apt update && \
    apt install -y \
        git \
        wget \
        python3 python3-pip && \
    python3 -m pip install numpy==1.22.4

# install bazel
RUN apt install -y apt-transport-https curl gnupg && \
    curl -fsSL https://bazel.build/bazel-release.pub.gpg | gpg --dearmor >bazel-archive-keyring.gpg && \
    mv bazel-archive-keyring.gpg /usr/share/keyrings && \
    echo ""deb [arch=amd64 signed-by=/usr/share/keyrings/bazel-archive-keyring.gpg] https://storage.googleapis.com/bazel-apt stable jdk1.8"" | tee /etc/apt/sources.list.d/bazel.list && \
    apt update && \
    apt install -y bazel-4.2.2

RUN git clone --branch 0.2.4 --depth=1 https://github.com/google/yggdrasil-decision-forests/ /tmp/yggdrasil-decision-forests

WORKDIR /tmp/yggdrasil-decision-forests
CMD [""/bin/sh"", ""-c"", ""bazel-4.2.2 build //yggdrasil_decision_forests/cli:all""]
```


### Relevant log output

```shell
$ docker build -t tfrepro:bug .
# ... snip ...
Successfully tagged tfrepro:bug
$ docker run -it tfrepro:bug
Extracting Bazel installation...
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=0
INFO: Reading rc options for 'build' from /tmp/yggdrasil-decision-forests/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec --incompatible_restrict_string_escapes=false
INFO: Reading rc options for 'build' from /tmp/yggdrasil-decision-forests/.bazelrc:
  'build' options: -c opt --spawn_strategy=standalone --announce_rc --noincompatible_strict_action_env --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --define=grpc_no_ares=true --color=yes
DEBUG: /root/.cache/bazel/_bazel_root/46fba50116469a0e63844a2e4aed5fee/external/org_tensorflow/third_party/repo.bzl:108:14: 
Warning: skipping import of repository 'com_google_absl' because it already exists.
DEBUG: /root/.cache/bazel/_bazel_root/46fba50116469a0e63844a2e4aed5fee/external/org_tensorflow/third_party/repo.bzl:108:14: 
Warning: skipping import of repository 'farmhash_archive' because it already exists.
DEBUG: /root/.cache/bazel/_bazel_root/46fba50116469a0e63844a2e4aed5fee/external/org_tensorflow/third_party/repo.bzl:108:14: 
Warning: skipping import of repository 'com_google_protobuf' because it already exists.
DEBUG: /root/.cache/bazel/_bazel_root/46fba50116469a0e63844a2e4aed5fee/external/org_tensorflow/third_party/repo.bzl:108:14: 
Warning: skipping import of repository 'com_google_googletest' because it already exists.
DEBUG: /root/.cache/bazel/_bazel_root/46fba50116469a0e63844a2e4aed5fee/external/org_tensorflow/third_party/repo.bzl:108:14: 
Warning: skipping import of repository 'zlib' because it already exists.
DEBUG: /root/.cache/bazel/_bazel_root/46fba50116469a0e63844a2e4aed5fee/external/org_tensorflow/third_party/repo.bzl:108:14: 
Warning: skipping import of repository 'rules_cc' because it already exists.
DEBUG: /root/.cache/bazel/_bazel_root/46fba50116469a0e63844a2e4aed5fee/external/org_tensorflow/third_party/repo.bzl:108:14: 
Warning: skipping import of repository 'rules_python' because it already exists.
DEBUG: /root/.cache/bazel/_bazel_root/46fba50116469a0e63844a2e4aed5fee/external/org_tensorflow/third_party/repo.bzl:108:14: 
Warning: skipping import of repository 'bazel_skylib' because it already exists.
INFO: Repository local_execution_config_python instantiated at:
  /tmp/yggdrasil-decision-forests/WORKSPACE:38:4: in <toplevel>
  /root/.cache/bazel/_bazel_root/46fba50116469a0e63844a2e4aed5fee/external/org_tensorflow/tensorflow/workspace2.bzl:1108:19: in workspace
  /root/.cache/bazel/_bazel_root/46fba50116469a0e63844a2e4aed5fee/external/org_tensorflow/tensorflow/workspace2.bzl:84:27: in _tf_toolchains
  /root/.cache/bazel/_bazel_root/46fba50116469a0e63844a2e4aed5fee/external/tf_toolchains/toolchains/remote_config/configs.bzl:6:28: in initialize_rbe_configs
  /root/.cache/bazel/_bazel_root/46fba50116469a0e63844a2e4aed5fee/external/tf_toolchains/toolchains/remote_config/rbe_config.bzl:158:27: in _tensorflow_local_config
Repository rule local_python_configure defined at:
  /root/.cache/bazel/_bazel_root/46fba50116469a0e63844a2e4aed5fee/external/org_tensorflow/third_party/py/python_configure.bzl:275:41: in <toplevel>
INFO: Repository local_config_python instantiated at:
  /tmp/yggdrasil-decision-forests/WORKSPACE:38:4: in <toplevel>
  /root/.cache/bazel/_bazel_root/46fba50116469a0e63844a2e4aed5fee/external/org_tensorflow/tensorflow/workspace2.bzl:1108:19: in workspace
  /root/.cache/bazel/_bazel_root/46fba50116469a0e63844a2e4aed5fee/external/org_tensorflow/tensorflow/workspace2.bzl:94:21: in _tf_toolchains
Repository rule python_configure defined at:
  /root/.cache/bazel/_bazel_root/46fba50116469a0e63844a2e4aed5fee/external/org_tensorflow/third_party/py/python_configure.bzl:294:35: in <toplevel>
ERROR: An error occurred during the fetch of repository 'local_execution_config_python':
   Traceback (most recent call last):
	File ""/root/.cache/bazel/_bazel_root/46fba50116469a0e63844a2e4aed5fee/external/org_tensorflow/third_party/py/python_configure.bzl"", line 212, column 41, in _create_local_python_repository
		python_include = _get_python_include(repository_ctx, python_bin)
	File ""/root/.cache/bazel/_bazel_root/46fba50116469a0e63844a2e4aed5fee/external/org_tensorflow/third_party/py/python_configure.bzl"", line 152, column 21, in _get_python_include
		result = execute(
	File ""/root/.cache/bazel/_bazel_root/46fba50116469a0e63844a2e4aed5fee/external/org_tensorflow/third_party/remote_config/common.bzl"", line 219, column 13, in execute
		fail(
Error in fail: Problem getting python include path.
<string>:1: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives
<string>:1: DeprecationWarning: The distutils.sysconfig module is deprecated, use sysconfig instead
Is the Python binary path set up right? (See ./configure or PYTHON_BIN_PATH.) Is distutils installed?
ERROR: Error fetching repository: Traceback (most recent call last):
	File ""/root/.cache/bazel/_bazel_root/46fba50116469a0e63844a2e4aed5fee/external/org_tensorflow/third_party/py/python_configure.bzl"", line 212, column 41, in _create_local_python_repository
		python_include = _get_python_include(repository_ctx, python_bin)
	File ""/root/.cache/bazel/_bazel_root/46fba50116469a0e63844a2e4aed5fee/external/org_tensorflow/third_party/py/python_configure.bzl"", line 152, column 21, in _get_python_include
		result = execute(
	File ""/root/.cache/bazel/_bazel_root/46fba50116469a0e63844a2e4aed5fee/external/org_tensorflow/third_party/remote_config/common.bzl"", line 219, column 13, in execute
		fail(
Error in fail: Problem getting python include path.
<string>:1: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives
<string>:1: DeprecationWarning: The distutils.sysconfig module is deprecated, use sysconfig instead
Is the Python binary path set up right? (See ./configure or PYTHON_BIN_PATH.) Is distutils installed?
INFO: Repository remote_coverage_tools instantiated at:
  /DEFAULT.WORKSPACE.SUFFIX:11:13: in <toplevel>
Repository rule http_archive defined at:
  /root/.cache/bazel/_bazel_root/46fba50116469a0e63844a2e4aed5fee/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>
INFO: Repository go_sdk instantiated at:
  /tmp/yggdrasil-decision-forests/WORKSPACE:42:4: in <toplevel>
  /root/.cache/bazel/_bazel_root/46fba50116469a0e63844a2e4aed5fee/external/org_tensorflow/tensorflow/workspace0.bzl:117:20: in workspace
  /root/.cache/bazel/_bazel_root/46fba50116469a0e63844a2e4aed5fee/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps
  /root/.cache/bazel/_bazel_root/46fba50116469a0e63844a2e4aed5fee/external/io_bazel_rules_go/go/toolchain/toolchains.bzl:379:28: in go_register_toolchains
  /root/.cache/bazel/_bazel_root/46fba50116469a0e63844a2e4aed5fee/external/io_bazel_rules_go/go/private/sdk.bzl:65:21: in go_download_sdk
Repository rule _go_download_sdk defined at:
  /root/.cache/bazel/_bazel_root/46fba50116469a0e63844a2e4aed5fee/external/io_bazel_rules_go/go/private/sdk.bzl:53:35: in <toplevel>
ERROR: Analysis of target '//yggdrasil_decision_forests/cli:infer_dataspec' failed; build aborted: Problem getting python include path.
<string>:1: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives
<string>:1: DeprecationWarning: The distutils.sysconfig module is deprecated, use sysconfig instead
Is the Python binary path set up right? (See ./configure or PYTHON_BIN_PATH.) Is distutils installed?
INFO: Elapsed time: 54.150s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (16 packages loaded, 172 targets c\
onfigured)
    Fetching ...docker; Cloning 251f6a68b439744094faff800cd029798edf9faa of ht\
tps://github.com/bazelbuild/rules_docker.git
```
</details>"
56398,"put two models in another model, the two models variable names aren't under the another model name scope","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.5.0

### Custom Code

Yes

### OS Platform and Distribution

mac os/ linux

### Mobile device

_No response_

### Python version

3.8.5

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
put two models in another model, the two models variable names aren't under the another model name scope.

1. class clip inherit from keras.Model
2. class CLIPTextTransformer inherit from keras.Model
3. class CLIPVisionTrransformer inherit from keras.Model
```


### Standalone code to reproduce the issue

```shell
the source code as follow

class CLIP(PreTrainModel):
    """"""
    CLIP
    """"""
    config_cls = CLIPConfig

    def __init__(self, name: str = 'clip123', **kwargs):
        super(CLIP, self).__init__(name=name, **kwargs)

        self.text_transformer = CLIPTextTransformer(config=self.config.text_transformer, name='text_transformer')
        self.vision_transformer = CLIPVisionTrransformer(config=self.config.vision_transformer,
                                                         name='vision_transformer')

        self.visual_projection = tf.keras.layers.Dense(units=self.config.projection_dim,
                                                       kernel_initializer=get_initializer(
                                                           self.vision_transformer.config.hidden_size ** -0.5),
                                                       use_bias=False,
                                                       name='visual_projection')
        self.text_projection = tf.keras.layers.Dense(units=self.config.projection_dim,
                                                     kernel_initializer=get_initializer(
                                                         self.text_transformer.config.hidden_size ** -0.5),
                                                     use_bias=False,
                                                     name='text_projection')
```
```


### Relevant log output

_No response_</details>"
56397,tf.nn.max_pool2d data_format check is wrong for GPU,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

11.2/8.0

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Even though tf.nn.max_pool2d is supposed to run on GPU, the operation always fails with the data_format check that says 'NCHW' is not supposed for CPU.
```


### Standalone code to reproduce the issue

```shell
Python 3.8.10 (default, Mar 15 2022, 12:22:08) 
[GCC 9.4.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> tf.debugging.set_log_device_placement(True)
>>> print(tf.config.list_physical_devices('GPU'))
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
>>> test = tf.constant([[[[1,1,1], [1,2,1], [1,1,1]]]])
2022-06-07 19:48:03.741097: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-06-07 19:48:04.333505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10341 MB memory:  -> device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1
2022-06-07 19:48:04.336454: I tensorflow/core/common_runtime/eager/execute.cc:1323] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
>>> tf.nn.max_pool2d(test, 2, 1, 'SAME', 'NCHW')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/jq/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/jq/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 7164, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: Default MaxPoolingOp only supports NHWC on device type CPU [Op:MaxPool]
>>> with tf.device(""GPU:0""):
...     tf.nn.max_pool2d(test, 2, 1, 'SAME', 'NCHW')
... 
Traceback (most recent call last):
  File ""<stdin>"", line 2, in <module>
  File ""/home/jq/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/jq/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 7164, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: Default MaxPoolingOp only supports NHWC on device type CPU [Op:MaxPool]
```


### Relevant log output

_No response_</details>"
56396,No gradients provided for any variable,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

macOS Monterey 12.4

### Mobile device

_No response_

### Python version

3.10.4

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

[distilbert-base-uncased.txt](https://huggingface.co/distilbert-base-uncased/blob/main/vocab.txt)

Loss is provided and I get an error. To make it work, modify the commented line and replace `return loss` with `return 'mse'` and it should work fine.



### Standalone code to reproduce the issue

```shell
import random
import string

import numpy as np
import tensorflow as tf
import tensorflow_models as tfm
from keras import Model
from keras.layers import Dense, Dropout, Input
from transformers import TFAutoModel


def calculate_loss(pad_val, max_val):
    def loss(y_true, y_pred):
        pad_indices = tf.where(y_true == pad_val)
        y_true = tf.tensor_scatter_nd_update(
            y_true,
            pad_indices,
            tf.ones(tf.shape(pad_indices)[0], y_true.dtype) * max_val,
        )
        actual_orders = tf.argsort(tf.argsort(y_true))
        predicted_orders = tf.argsort(tf.argsort(y_pred))
        return tf.reduce_sum(tf.abs(actual_orders - predicted_orders))

    return loss  # change this with 'mse' and the error is gone


def create_model(input_shape, bert_head):
    ids = Input(input_shape, dtype='int32', name='input_ids')
    masks = Input(input_shape, dtype='int32', name='attention_mask')
    x0 = dict(input_ids=ids, attention_mask=masks)
    x = bert_head(x0)[0]
    x = Dropout(0.2)(x)
    x = Dense(64)(x)
    x = Dropout(0.2)(x)
    output = Dense(1, 'sigmoid')(x)
    return Model(x0, output)


class DatasetGen:
    def __init__(self, tokenizer, packer, min_seq_length, max_seq_length):
        self.tokenizer = tokenizer
        self.packer = packer
        self.min_seq_length = min_seq_length
        self.max_seq_length = max_seq_length

    def process_xy(self, x, y):
        tokens = self.tokenizer(x)
        x = self.packer(tokens)
        x['input_ids'] = x['input_word_ids']
        x['attention_mask'] = x['input_mask']
        del x['input_word_ids']
        del x['input_type_ids']
        del x['input_mask']
        return x, tf.expand_dims(y, -1)

    def dummy_dataset(self, batch_size=4, size=1000):
        s = string.ascii_letters + 10 * ' '
        xs = []
        ys = []
        for _ in range(size):
            x, y = [], []
            seq_length = random.randint(self.min_seq_length, self.max_seq_length)
            for i in range(seq_length):
                x.append(''.join(random.choice(s) for _ in range(50, 150)))
                y.append(i)
            pad_size = self.max_seq_length - seq_length
            xs.append(np.pad(np.array(x), [0, pad_size]))
            ys.append(np.pad(np.array(y), [0, pad_size]))
        return (
            tf.data.Dataset.from_tensor_slices((xs, ys))
            .batch(batch_size)
            .map(self.process_xy)
        )


if __name__ == '__main__':
    pretrained_model = 'distilbert-base-uncased'
    cache_dir = 'transformers-cache'
    head = TFAutoModel.from_pretrained(pretrained_model, cache_dir=cache_dir)
    tokenizer = tfm.nlp.layers.BertTokenizer(
        vocab_file='distilbert-base-uncased.txt'
    )
    packer = tfm.nlp.layers.BertPackInputs(
        50, special_tokens_dict=tokenizer.get_special_tokens_dict()
    )
    data_gen = DatasetGen(tokenizer, packer, 10, 50)
    dataset = data_gen.dummy_dataset()
    model = create_model((None,), head)
    model.compile('adam', calculate_loss(0, 1))
    model.fit(dataset, batch_size=4)
```


### Relevant log output

```shell
/usr/local/opt/python@3.10/bin/python3.10 /Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py --multiprocess --qt-support=auto --client 127.0.0.1 --port 49349 --file ""/Users/user/Library/Application Support/JetBrains/PyCharm2022.1/scratches/scratch_1.py""
Connected to pydev debugger (build 221.5787.24)
2022-06-08 04:01:25.576194: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_transform', 'vocab_layer_norm', 'vocab_projector', 'activation_13']
- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.
WARNING:tensorflow:The dtype of the target tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32
WARNING:tensorflow:The dtype of the target tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32
Traceback (most recent call last):
  File ""/usr/local/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/var/folders/_0/8cr6pwvs1jg99fgtf2_33lsm0000gn/T/__autograph_generated_file56mk7ojv.py"", line 15, in tf__train_function
    retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
ValueError: in user code:

    File ""/usr/local/lib/python3.10/site-packages/keras/engine/training.py"", line 1051, in train_function  *
        return step_function(self, iterator)
    File ""/usr/local/lib/python3.10/site-packages/keras/engine/training.py"", line 1040, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/usr/local/lib/python3.10/site-packages/keras/engine/training.py"", line 1030, in run_step  **
        outputs = model.train_step(data)
    File ""/usr/local/lib/python3.10/site-packages/keras/engine/training.py"", line 893, in train_step
        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File ""/usr/local/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py"", line 539, in minimize
        return self.apply_gradients(grads_and_vars, name=name)
    File ""/usr/local/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py"", line 640, in apply_gradients
        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)
    File ""/usr/local/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/utils.py"", line 73, in filter_empty_gradients
        raise ValueError(f""No gradients provided for any variable: {variable}. ""

    ValueError: No gradients provided for any variable: (['tf_distil_bert_model/distilbert/embeddings/word_embeddings/weight:0', 'tf_distil_bert_model/distilbert/embeddings/position_embeddings/embeddings:0', 'tf_distil_bert_model/distilbert/embeddings/LayerNorm/gamma:0', 'tf_distil_bert_model/distilbert/embeddings/LayerNorm/beta:0', 'tf_distil_bert_model/distilbert/transformer/layer_._0/attention/q_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._0/attention/q_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._0/attention/k_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._0/attention/k_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._0/attention/v_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._0/attention/v_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._0/attention/out_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._0/attention/out_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._0/sa_layer_norm/gamma:0', 'tf_distil_bert_model/distilbert/transformer/layer_._0/sa_layer_norm/beta:0', 'tf_distil_bert_model/distilbert/transformer/layer_._0/ffn/lin1/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._0/ffn/lin1/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._0/ffn/lin2/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._0/ffn/lin2/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._0/output_layer_norm/gamma:0', 'tf_distil_bert_model/distilbert/transformer/layer_._0/output_layer_norm/beta:0', 'tf_distil_bert_model/distilbert/transformer/layer_._1/attention/q_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._1/attention/q_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._1/attention/k_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._1/attention/k_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._1/attention/v_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._1/attention/v_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._1/attention/out_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._1/attention/out_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._1/sa_layer_norm/gamma:0', 'tf_distil_bert_model/distilbert/transformer/layer_._1/sa_layer_norm/beta:0', 'tf_distil_bert_model/distilbert/transformer/layer_._1/ffn/lin1/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._1/ffn/lin1/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._1/ffn/lin2/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._1/ffn/lin2/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._1/output_layer_norm/gamma:0', 'tf_distil_bert_model/distilbert/transformer/layer_._1/output_layer_norm/beta:0', 'tf_distil_bert_model/distilbert/transformer/layer_._2/attention/q_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._2/attention/q_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._2/attention/k_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._2/attention/k_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._2/attention/v_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._2/attention/v_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._2/attention/out_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._2/attention/out_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._2/sa_layer_norm/gamma:0', 'tf_distil_bert_model/distilbert/transformer/layer_._2/sa_layer_norm/beta:0', 'tf_distil_bert_model/distilbert/transformer/layer_._2/ffn/lin1/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._2/ffn/lin1/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._2/ffn/lin2/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._2/ffn/lin2/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._2/output_layer_norm/gamma:0', 'tf_distil_bert_model/distilbert/transformer/layer_._2/output_layer_norm/beta:0', 'tf_distil_bert_model/distilbert/transformer/layer_._3/attention/q_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._3/attention/q_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._3/attention/k_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._3/attention/k_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._3/attention/v_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._3/attention/v_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._3/attention/out_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._3/attention/out_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._3/sa_layer_norm/gamma:0', 'tf_distil_bert_model/distilbert/transformer/layer_._3/sa_layer_norm/beta:0', 'tf_distil_bert_model/distilbert/transformer/layer_._3/ffn/lin1/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._3/ffn/lin1/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._3/ffn/lin2/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._3/ffn/lin2/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._3/output_layer_norm/gamma:0', 'tf_distil_bert_model/distilbert/transformer/layer_._3/output_layer_norm/beta:0', 'tf_distil_bert_model/distilbert/transformer/layer_._4/attention/q_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._4/attention/q_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._4/attention/k_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._4/attention/k_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._4/attention/v_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._4/attention/v_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._4/attention/out_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._4/attention/out_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._4/sa_layer_norm/gamma:0', 'tf_distil_bert_model/distilbert/transformer/layer_._4/sa_layer_norm/beta:0', 'tf_distil_bert_model/distilbert/transformer/layer_._4/ffn/lin1/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._4/ffn/lin1/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._4/ffn/lin2/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._4/ffn/lin2/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._4/output_layer_norm/gamma:0', 'tf_distil_bert_model/distilbert/transformer/layer_._4/output_layer_norm/beta:0', 'tf_distil_bert_model/distilbert/transformer/layer_._5/attention/q_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._5/attention/q_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._5/attention/k_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._5/attention/k_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._5/attention/v_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._5/attention/v_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._5/attention/out_lin/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._5/attention/out_lin/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._5/sa_layer_norm/gamma:0', 'tf_distil_bert_model/distilbert/transformer/layer_._5/sa_layer_norm/beta:0', 'tf_distil_bert_model/distilbert/transformer/layer_._5/ffn/lin1/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._5/ffn/lin1/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._5/ffn/lin2/kernel:0', 'tf_distil_bert_model/distilbert/transformer/layer_._5/ffn/lin2/bias:0', 'tf_distil_bert_model/distilbert/transformer/layer_._5/output_layer_norm/gamma:0', 'tf_distil_bert_model/distilbert/transformer/layer_._5/output_layer_norm/beta:0', 'dense/kernel:0', 'dense/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'tf_distil_bert_model/distilbert/embeddings/word_embeddings/weight:0' shape=(30522, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/embeddings/position_embeddings/embeddings:0' shape=(512, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._0/attention/q_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._0/attention/q_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._0/attention/k_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._0/attention/k_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._0/attention/v_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._0/attention/v_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._0/attention/out_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._0/attention/out_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._0/sa_layer_norm/gamma:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._0/sa_layer_norm/beta:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._0/ffn/lin1/kernel:0' shape=(768, 3072) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._0/ffn/lin1/bias:0' shape=(3072,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._0/ffn/lin2/kernel:0' shape=(3072, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._0/ffn/lin2/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._0/output_layer_norm/gamma:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._0/output_layer_norm/beta:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._1/attention/q_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._1/attention/q_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._1/attention/k_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._1/attention/k_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._1/attention/v_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._1/attention/v_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._1/attention/out_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._1/attention/out_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._1/sa_layer_norm/gamma:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._1/sa_layer_norm/beta:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._1/ffn/lin1/kernel:0' shape=(768, 3072) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._1/ffn/lin1/bias:0' shape=(3072,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._1/ffn/lin2/kernel:0' shape=(3072, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._1/ffn/lin2/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._1/output_layer_norm/gamma:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._1/output_layer_norm/beta:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._2/attention/q_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._2/attention/q_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._2/attention/k_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._2/attention/k_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._2/attention/v_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._2/attention/v_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._2/attention/out_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._2/attention/out_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._2/sa_layer_norm/gamma:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._2/sa_layer_norm/beta:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._2/ffn/lin1/kernel:0' shape=(768, 3072) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._2/ffn/lin1/bias:0' shape=(3072,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._2/ffn/lin2/kernel:0' shape=(3072, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._2/ffn/lin2/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._2/output_layer_norm/gamma:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._2/output_layer_norm/beta:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._3/attention/q_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._3/attention/q_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._3/attention/k_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._3/attention/k_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._3/attention/v_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._3/attention/v_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._3/attention/out_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._3/attention/out_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._3/sa_layer_norm/gamma:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._3/sa_layer_norm/beta:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._3/ffn/lin1/kernel:0' shape=(768, 3072) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._3/ffn/lin1/bias:0' shape=(3072,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._3/ffn/lin2/kernel:0' shape=(3072, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._3/ffn/lin2/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._3/output_layer_norm/gamma:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._3/output_layer_norm/beta:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._4/attention/q_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._4/attention/q_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._4/attention/k_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._4/attention/k_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._4/attention/v_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._4/attention/v_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._4/attention/out_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._4/attention/out_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._4/sa_layer_norm/gamma:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._4/sa_layer_norm/beta:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._4/ffn/lin1/kernel:0' shape=(768, 3072) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._4/ffn/lin1/bias:0' shape=(3072,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._4/ffn/lin2/kernel:0' shape=(3072, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._4/ffn/lin2/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._4/output_layer_norm/gamma:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._4/output_layer_norm/beta:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._5/attention/q_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._5/attention/q_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._5/attention/k_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._5/attention/k_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._5/attention/v_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._5/attention/v_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._5/attention/out_lin/kernel:0' shape=(768, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._5/attention/out_lin/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._5/sa_layer_norm/gamma:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._5/sa_layer_norm/beta:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._5/ffn/lin1/kernel:0' shape=(768, 3072) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._5/ffn/lin1/bias:0' shape=(3072,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._5/ffn/lin2/kernel:0' shape=(3072, 768) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._5/ffn/lin2/bias:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._5/output_layer_norm/gamma:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'tf_distil_bert_model/distilbert/transformer/layer_._5/output_layer_norm/beta:0' shape=(768,) dtype=float32>), (None, <tf.Variable 'dense/kernel:0' shape=(768, 64) dtype=float32>), (None, <tf.Variable 'dense/bias:0' shape=(64,) dtype=float32>), (None, <tf.Variable 'dense_1/kernel:0' shape=(64, 1) dtype=float32>), (None, <tf.Variable 'dense_1/bias:0' shape=(1,) dtype=float32>)).

python-BaseException

Process finished with exit code 137 (interrupted by signal 9: SIGKILL)
```
</details>"
56393,DTensor pack/unpack APIs used high device memory,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf.2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04.4

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

V100

### Current Behaviour?

Hi, DTensor developers

I was testing the DTensor features on ResNet-50 on imagenet. I started with the simple data parallel. To do data parallel, I fully replicated the weights and only shard the input data. The input batch sharding is handled by  method. However, I can only run the model with a very small global batch, i.e., 64. If I increased the #step-per-epoch, this number will be further reduced to 32. 

When I used a larger batch size, i.e. 128, from the TF-profiler, I saw that the used device memory kept accumulating and then it hit the OOM within 110 steps.

I spent some times and figured out that ```repack_batch``` method could cause high device memory. If I moved the below [code](https://github.com/Young768/test-dtensor/blob/main/resnet-opt.py#L757-L762) out from the for loop and used a fixed sharded input batch, I can successfully run the model with a global batch size of 1024. I would think that TF would reuse the allocation for the every new batch. But here it seems TF didn't do that. Would you please have a look? Thanks.

```
x = next(train_iter)
images, labels = x
images, labels = repack_batch(
        images, labels, image_layout, label_layout)
t_x = (images, labels)
total_loss += train_step(t_x)
```



### Standalone code to reproduce the issue

This is the [repo](https://github.com/Young768/test-dtensor/blob/main/resnet-opt.py) I used.



### Relevant log output

```shell
2022-05-26 19:20:17.146997: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] total_region_allocated_bytes_: 15348793344 memory_limit_: 15348793344 available bytes: 0 curr_region_allocation_bytes_: 17179869184
2022-05-26 19:20:17.147010: I tensorflow/core/common_runtime/bfc_allocator.cc:1103] Stats: 
Limit:                     15348793344
InUse:                     15344515072
MaxInUse:                  15347394560
NumAllocs:                       23834
MaxAllocSize:               2512388096
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0
```
</details>"
56391,tf.data.experimental.load() no longer working,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

2.8

### Custom Code

No

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2

### GPU model and memory

NVIDIA Quadro T2000

### Current Behaviour?

```shell
Hi, I've been trying to load my own tf.data dataset using the tf.data.experimental.load() command.
The data being loaded has been created in a different script using the tf.data.experimental.save() command
When I attempt to load the data it gives me an error and I cannot seem to be able to fix it.
Any help appreciated.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

tf_train = tf.data.experimental.load(
    r'D:/data/train_w512_small',
    element_spec=None,
    compression=None,
    reader_func=None
)
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""C:\Users\CORRELU\AppData\Roaming\Python\Python39\site-packages\IPython\core\interactiveshell.py"", line 3251, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-11-4ae4a880fc8d>"", line 1, in <module>
    tf_train = tf.data.experimental.load(
  File ""C:\Users\CORRELU\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\data\experimental\ops\io.py"", line 310, in load
    return _LoadDataset(
  File ""C:\Users\CORRELU\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\data\experimental\ops\io.py"", line 216, in __init__
    encoded_spec = f.read()
  File ""C:\Users\CORRELU\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\lib\io\file_io.py"", line 114, in read
    self._preread_check()
  File ""C:\Users\CORRELU\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\lib\io\file_io.py"", line 76, in _preread_check
    self._read_buf = _pywrap_file_io.BufferedInputStream(
tensorflow.python.framework.errors_impl.NotFoundError: NewRandomAccessFile failed to Create/Open: D:/data/train_w512_small\dataset_spec.pb : The system cannot find the path specified.
; No such process
```
</details>"
56389,AttributeError: 'Tensor' object has no attribute 'numpy' while  defining a custom loss,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Yes, a custom loss metric.
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS Linux 7
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: NA
-   **TensorFlow installed from (source or binary)**: pip
-   **TensorFlow version (use command below)**: 2.0.0
-   **Python version**: 3.6.13
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:  CUDA 10.2/ cuDNN 7.6.5
-   **GPU model and memory**:  Nvidia GTX 1080 Ti 12 GB


### Describe the problem
I'm creating a custom loss function in Keras, and am getting the error `AttributeError: 'Tensor' object has no attribute 'numpy'`. As per discussion on [StackOverflow](https://stackoverflow.com/questions/52357542/attributeerror-tensor-object-has-no-attribute-numpy), and Github([#27519](https://github.com/tensorflow/tensorflow/issues/27519),[#35393](https://github.com/tensorflow/tensorflow/issues/35393)), this is due to the code not running in eager execution mode. I have checked, with

`tf.executing_eagerly()`

and my code is indeed runnning in eager execution.
Also, these discussion do not help me solve my issue.

### Source code / logs
My custom metric has these two additional operations -->

```
 y_pred_copy = tf.math.reduce_max(y_pred_copy, axis=-1, keepdims=True)
 y_pred_copy = tf.image.resize(y_pred_copy, (8, 16), method=""bilinear"")
```
(The loss metric defined without these works without the error. However, my custom loss metric require these operations (calculating max val to reduce tensor channels from 2 to 1, and to resize the tensor using interpolation) to be performed.

Kindly help me out,
thanks. 

**Edit 01 (Adding my Custom loss for further clarity along with the other custom loss that works without error)**

**Custom Metric (Produces Error)**
```
def self_balanced_focal_loss_attention(attention_mask,alpha=3, gamma=2.0):
    
    def loss(y_true, y_pred):
    
    	#loss between predicted_mask and ground_truth mask
        y_pred = backend.softmax(y_pred, -1)
        cross_entropy = backend.categorical_crossentropy(y_true, y_pred)
        
        sample_weights = backend.max(backend.pow(1.0 - y_pred, gamma) * y_true, axis=-1)
        
        pixel_rate = backend.sum(y_true, axis=[1, 2], keepdims=True) / backend.sum(backend.ones_like(y_true),
                                                                                   axis=[1, 2], keepdims=True)
        class_weights = backend.max(backend.pow(backend.ones_like(y_true) * alpha, pixel_rate) * y_true, axis=-1)
        
        final_loss = class_weights * sample_weights * cross_entropy
        loss_gt =  backend.mean(backend.sum(final_loss, axis=[1, 2]))
        
        #loss between predicted_mask and attention_mask        
        y_pred_copy = y_pred

        y_pred_copy = tf.math.reduce_max(y_pred_copy, axis=-1, keepdims=True)
        y_pred_copy = tf.image.resize(y_pred_copy, (8, 16), method=""bilinear"")

        cross_entropy = backend.categorical_crossentropy(attention_mask, y_pred_copy)

        sample_weights = backend.max(backend.pow(1.0 - y_pred_copy, gamma) * attention_mask, axis=-1)

        pixel_rate = backend.sum(attention_mask, axis=[1, 2], keepdims=True) / backend.sum(backend.ones_like(attention_mask),
                                                                                   axis=[1, 2], keepdims=True)
        class_weights = backend.max(backend.pow(backend.ones_like(attention_mask) * alpha, pixel_rate) * attention_mask, axis=-1)

        final_loss = class_weights * sample_weights * cross_entropy
        loss_attention =  (-1)*backend.mean(backend.sum(final_loss, axis=[1, 2]))
        #combining the two losses
        return ((loss_attention+loss_gt)/2)

    return loss
```
**Custom Metric (Runs without Error)**

```
def self_balanced_focal_loss(alpha=3, gamma=2.0):
    def loss(y_true, y_pred):
        # cross entropy loss
        y_pred = backend.softmax(y_pred, -1)
        cross_entropy = backend.categorical_crossentropy(y_true, y_pred)

        sample_weights = backend.max(backend.pow(1.0 - y_pred, gamma) * y_true, axis=-1)
        pixel_rate = backend.sum(y_true, axis=[1, 2], keepdims=True) / backend.sum(backend.ones_like(y_true),
                                                                                   axis=[1, 2], keepdims=True)
        class_weights = backend.max(backend.pow(backend.ones_like(y_true) * alpha, pixel_rate) * y_true, axis=-1)
        final_loss = class_weights * sample_weights * cross_entropy
        
        return backend.mean(backend.sum(final_loss, axis=[1, 2]))

    return loss
```



"
56387,VRAM-hungry LSTM monster,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.8

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.6

### GPU model and memory

3x RTX 3090 (24Gb of VRAM each)

### Current Behaviour?

```shell
A bug happened!
Compiling a simple model with an lstm cell fills the VRAM to the maximum, before even fitting.
```


### Standalone code to reproduce the issue

```shell
https://github.com/MovsisyanM/sandbox/blob/main/random/LSTM-hell.ipynb
```


### Relevant log output

_No response_</details>"
56386,tf.data Service Dispatcher/ Worker - how to shutdown/stop?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Source

binary

### Tensorflow Version

tf 2.8

### Custom Code

No

### OS Platform and Distribution

Linux

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
How can I shutdown/stop tf.data.service dispatcher and workerServer processes when training completes?  
I'm running tf.data.service on two nodes: data-node, GPU-node. 
On the data-node I'm starting: `tf.data.experimental.service.DispatchServer` and a `tf.data.experimental.service.WorkerServer`.
On the GPU-node I'm consuming a dataset which connects to the DispatcherServer
.apply(tf.data.experimental.service.distribute(
        processing_mode=""parallel_epochs"",
        service=f'grpc://{dispatcher_host}:6000',),
    )
All is well, but I found there's no built-in way to signal the dispatcher and workerserver to exit when training completes on the GPU-node. I need them to exit gracefully as I'm running in a job orchestrator that waits with terminating the overall job until all processes exit.  
I would expect to have a shutdown/close/stop method with tf.data.Dataset, or in data_service_ops.py.
```


### Standalone code to reproduce the issue

```shell
I just followed the built-in example for tf.data.service. Asking a what is the expected behavior question.
```


### Relevant log output

_No response_</details>"
56385,"Dataset Pipeline determinism when reload, using `batch` `prefetch`","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf2.9

### Custom Code

No

### OS Platform and Distribution

Windows 11

### Mobile device

No

### Python version

3.10.4

### Bazel version

N/A

### GCC/Compiler version

N/A

### CUDA/cuDNN version

11.7/8.4

### GPU model and memory

RTX3090 24GB

### Current Behaviour?

```shell
For some reason, I have to built a cascade tf.data.dataset like `dataset_B(dataset_A)`, where the dataset_A deal with `tf.string` datas (contains description of data, such as paths, operands for data augmentation), saving the iter state into checkpoint, dataset_B do some mapping (such as getting real data arrays from paths, augmenting data by operands). In this way, I can easily condouct `shuffle` in dataset_A and without loading whole and huge real data into `shuffle buffsize`. I can easily  save the data's iter state by `https://www.tensorflow.org/guide/data#iterator_checkpointing` and reload the data with `checkpoint` to deal with any unexpected stop of my algorithmmy. 

It looked perfect, and I did it. But there are still shortcomings. If I use `batch` or `prefetch` to deal with dataset_B, and if I do not use `tf.config.experimental.enable_op_determinism()`, the reload behavior will be non-deterministic. 

I will show this bug in following simple codes. If there is no bug, 
`buf1`, `buf2` and `buf3` should have the same output. But unfortunately, since `buf2` is the output by using `batch` and `perfetch` and without  `tf.config.experimental.enable_op_determinism()`, it did more iterations before unexpected termination, leading data shift when reload. But this bug disappear when use `tf.config.experimental.enable_op_determinism()`

I hope this bug will be fixed, i.e., I want data will not shift after reload, even thouth I do not use `tf.config.experimental.enable_op_determinism()`.
```


### Standalone code to reproduce the issue

```shell
import tempfile
import tensorflow as tf

physical_devices = tf.config.experimental.list_physical_devices(device_type='GPU')
tf.config.experimental.set_memory_growth(physical_devices[0], True)

datas = {""test"":[str(item) for item in  range(100)]} 
def mapfunc(x):
    def py_func(inp):
        return int(inp.numpy())
    k = list(x.keys())
    v = list(x.values())
    y = tf.py_function(py_func,inp=v,Tout=[tf.float32])
    return dict(zip(k,y))

# buf1-(without batch,perfetch)
with tempfile.TemporaryDirectory() as dir_name:
    dataset_A = tf.data.Dataset.from_tensor_slices(datas)
    def wrapper(iterator):
        def gen():
            yield from iterator
        dataset_B = tf.data.Dataset.from_generator(gen,output_signature=({""test"":tf.TensorSpec(shape=[],dtype=tf.string)})).map(map_func=mapfunc)
        return dataset_B # without batch,perfetch
    step = tf.Variable(0)
    iterator = iter(dataset_A) 
    checkpoint = tf.train.Checkpoint(iterator=iterator,step=step)
    ckpt_manager = tf.train.CheckpointManager(checkpoint=checkpoint,directory=dir_name,max_to_keep=3,step_counter=step,checkpoint_interval=10)
    buf1 = []
    for s,item  in zip(range(step.numpy()+1,8+1),wrapper(iterator)):
        step.assign(s)
        buf1.append((step.numpy(),tf.reduce_mean(item[""test""]).numpy()))
        ckpt_manager.save(check_interval=True,checkpoint_number=step)
        if step.numpy()>=5: # mimic unexpected stop
            break
    ckpt_manager.restore_or_initialize()
    for s,item  in zip(range(step.numpy()+1,8+1),wrapper(iterator)):
        step.assign(s)
        buf1.append((step.numpy(),tf.reduce_mean(item[""test""]).numpy()))
        ckpt_manager.save(check_interval=True,checkpoint_number=step)
    

#buf2-(with batch,perfetch)
with tempfile.TemporaryDirectory() as dir_name:
    dataset_A = tf.data.Dataset.from_tensor_slices(datas)
    def wrapper(iterator):
        def gen():
            yield from iterator
        dataset_B = tf.data.Dataset.from_generator(gen,output_signature=({""test"":tf.TensorSpec(shape=[],dtype=tf.string)})).map(map_func=mapfunc)
        return dataset_B.batch(1).prefetch(tf.data.AUTOTUNE) # with batch,perfetch
    step = tf.Variable(0)
    iterator = iter(dataset_A) 
    checkpoint = tf.train.Checkpoint(iterator=iterator,step=step)
    ckpt_manager = tf.train.CheckpointManager(checkpoint=checkpoint,directory=dir_name,max_to_keep=3,step_counter=step,checkpoint_interval=10)
    buf2 = []
    for s,item  in zip(range(step.numpy()+1,8+1),wrapper(iterator)):
        step.assign(s)
        buf2.append((step.numpy(),tf.reduce_mean(item[""test""]).numpy()))
        ckpt_manager.save(check_interval=True,checkpoint_number=step)
        if step.numpy()>=5: # mimic unexpected stop
            break
    ckpt_manager.restore_or_initialize()
    for s,item  in zip(range(step.numpy()+1,8+1),wrapper(iterator)):
        step.assign(s)
        buf2.append((step.numpy(),tf.reduce_mean(item[""test""]).numpy()))
        ckpt_manager.save(check_interval=True,checkpoint_number=step)
    

#buf3-(use determinism and with batch,perfetch)
tf.keras.utils.set_random_seed(0)
tf.config.experimental.enable_op_determinism() # determinism
with tempfile.TemporaryDirectory() as dir_name:
    dataset_A = tf.data.Dataset.from_tensor_slices(datas)
    def wrapper(iterator):
        def gen():
            yield from iterator
        dataset_B = tf.data.Dataset.from_generator(gen,output_signature=({""test"":tf.TensorSpec(shape=[],dtype=tf.string)})).map(map_func=mapfunc)
        return dataset_B.batch(1).prefetch(tf.data.AUTOTUNE) # with batch,perfetch
    step = tf.Variable(0)
    iterator = iter(dataset_A) 
    checkpoint = tf.train.Checkpoint(iterator=iterator,step=step)
    ckpt_manager = tf.train.CheckpointManager(checkpoint=checkpoint,directory=dir_name,max_to_keep=3,step_counter=step,checkpoint_interval=10)
    buf3 = []
    for s,item  in zip(range(step.numpy()+1,8+1),wrapper(iterator)):
        step.assign(s)
        buf3.append((step.numpy(),tf.reduce_mean(item[""test""]).numpy()))
        ckpt_manager.save(check_interval=True,checkpoint_number=step)
        if step.numpy()>=5: # mimic unexpected stop
            break
    ckpt_manager.restore_or_initialize()
    for s,item  in zip(range(step.numpy()+1,8+1),wrapper(iterator)):
        step.assign(s)
        buf3.append((step.numpy(),tf.reduce_mean(item[""test""]).numpy()))
        ckpt_manager.save(check_interval=True,checkpoint_number=step)

print(""buf1 without batch,perfetch (index,data):"")
print(buf1)
print(""buf2 with batch,perfetch (index,data):"")
print(buf2)
print(""buf3 use determinism and with batch,perfetch (index,data):"")
print(buf3)
```


### Relevant log output

```shell
buf1 without batch,perfetch (index,data):
[(1, 0.0), (2, 1.0), (3, 2.0), (4, 3.0), (5, 4.0), (2, 1.0), (3, 2.0), (4, 3.0), (5, 4.0), (6, 5.0), (7, 6.0), (8, 7.0)]
buf2 with batch,perfetch (index,data):
[(1, 0.0), (2, 1.0), (3, 2.0), (4, 3.0), (5, 4.0), (2, 3.0), (3, 4.0), (4, 5.0), (5, 6.0), (6, 7.0), (7, 8.0), (8, 9.0)]
buf3 use determinism and with batch,perfetch (index,data):
[(1, 0.0), (2, 1.0), (3, 2.0), (4, 3.0), (5, 4.0), (2, 1.0), (3, 2.0), (4, 3.0), (5, 4.0), (6, 5.0), (7, 6.0), (8, 7.0)]
```
</details>"
56384,TensorFlow Lite label_image build fails,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

master

### Custom Code

No

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

gcc version 11.2.0 (Ubuntu 11.2.0-19ubuntu1) 

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Build tensorflow-lite and label_image from source (with CMake).


cmake --build . -j$(nproc) -t label_image

...
[100%] Linking CXX executable label_image
/usr/bin/ld: ../../libtensorflow-lite.a(interpreter.cc.o): in function `tflite::Interpreter::SetProfilerImpl(std::unique_ptr<tflite::Profiler, std::default_delete<tflite::Profiler> >)':
interpreter.cc:(.text+0x28d5): undefined reference to `tflite::profiling::RootProfiler::RemoveChildProfilers()'
/usr/bin/ld: interpreter.cc:(.text+0x28e4): undefined reference to `tflite::profiling::RootProfiler::AddProfiler(std::unique_ptr<tflite::Profiler, std::default_delete<tflite::Profiler> >&&)'
/usr/bin/ld: interpreter.cc:(.text+0x2926): undefined reference to `vtable for tflite::profiling::RootProfiler'
/usr/bin/ld: interpreter.cc:(.text+0x29e9): undefined reference to `vtable for tflite::profiling::RootProfiler'
/usr/bin/ld: ../../libtensorflow-lite.a(interpreter.cc.o): in function `tflite::Interpreter::~Interpreter()':
interpreter.cc:(.text+0x2fcd): undefined reference to `vtable for tflite::profiling::RootProfiler'
/usr/bin/ld: ../../libtensorflow-lite.a(interpreter.cc.o): in function `tflite::profiling::RootProfiler::~RootProfiler()':
interpreter.cc:(.text._ZN6tflite9profiling12RootProfilerD0Ev[_ZN6tflite9profiling12RootProfilerD5Ev]+0x3): undefined reference to `vtable for tflite::profiling::RootProfiler'
/usr/bin/ld: ../../libtensorflow-lite.a(interpreter.cc.o): in function `tflite::profiling::RootProfiler::~RootProfiler()':
interpreter.cc:(.text._ZN6tflite9profiling12RootProfilerD2Ev[_ZN6tflite9profiling12RootProfilerD5Ev]+0x3): undefined reference to `vtable for tflite::profiling::RootProfiler'
/usr/bin/ld: ../../libtensorflow-lite.a(interpreter_experimental.cc.o): in function `tflite::Interpreter::AddProfiler(tflite::Profiler*)':
interpreter_experimental.cc:(.text+0x613): undefined reference to `tflite::profiling::RootProfiler::AddProfiler(tflite::Profiler*)'
/usr/bin/ld: interpreter_experimental.cc:(.text+0x661): undefined reference to `vtable for tflite::profiling::RootProfiler'
/usr/bin/ld: ../../libtensorflow-lite.a(interpreter_experimental.cc.o): in function `tflite::Interpreter::SetProfiler(tflite::Profiler*)':
interpreter_experimental.cc:(.text+0x7ac): undefined reference to `tflite::profiling::RootProfiler::RemoveChildProfilers()'
/usr/bin/ld: interpreter_experimental.cc:(.text+0x7e9): undefined reference to `vtable for tflite::profiling::RootProfiler'
collect2: error: ld returned 1 exit status
gmake[3]: *** [examples/label_image/CMakeFiles/label_image.dir/build.make:370: examples/label_image/label_image] Error 1
gmake[2]: *** [CMakeFiles/Makefile2:5635: examples/label_image/CMakeFiles/label_image.dir/all] Error 2
gmake[1]: *** [CMakeFiles/Makefile2:5642: examples/label_image/CMakeFiles/label_image.dir/rule] Error 2
gmake: *** [Makefile:1879: label_image] Error 2
```
```


### Standalone code to reproduce the issue

```shell
git clone https://github.com/tensorflow/tensorflow.git
mkdir build && cd build
cmake ../tensorflow/tensorflow/lite/
cmake --build . -j$(nproc)
cmake --build . -j$(nproc) -t label_image
```


### Relevant log output

_No response_</details>"
56383,"Add SimpleMLCreateModelResource, SimpleMLInferenceOpWithHandle to allowed List (Lite Converter)","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
I am using Lite Converter to convert a Tensorflow Model (Random Forest from the Decision Forest Library) into a Tensorflow Lite Model.
I receive the folllowing error:  SimpleMLCreateModelResource, SimpleMLInferenceOpWithHandleop is neither a custom op nor a flex op.
The Troubleshooting page says to add these to the 'allowed list'.
```


### Standalone code to reproduce the issue

```shell
model = tdf.keras.RandomForestModel()
model.fit(train_data)
model.save('/project/first_model')
converter = tf.lite.TFLiteConverter.from_saved_model('/project/first_model')
tflite_model = converter.convert()
```


### Relevant log output

```shell
ConverterError: <unknown>:0: error: loc(fused[""SimpleMLCreateModelResource:"", ""SimpleMLCreateModelResource""]): 'tf.SimpleMLCreateModelResource' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""SimpleMLCreateModelResource:"", ""SimpleMLCreateModelResource""]): Error code: ERROR_NEEDS_CUSTOM_OPS
<unknown>:0: error: loc(callsite(callsite(callsite(fused[""SimpleMLInferenceOpWithHandle:"", ""inference_op@__inference_call_591""] at fused[""StatefulPartitionedCall:"", ""random_forest_model/StatefulPartitionedCall@__inference__wrapped_model_596""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_833""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): 'tf.SimpleMLInferenceOpWithHandle' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""]): called from
<unknown>:0: note: loc(callsite(callsite(callsite(fused[""SimpleMLInferenceOpWithHandle:"", ""inference_op@__inference_call_591""] at fused[""StatefulPartitionedCall:"", ""random_forest_model/StatefulPartitionedCall@__inference__wrapped_model_596""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_833""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): Error code: ERROR_NEEDS_CUSTOM_OPS
<unknown>:0: error: failed while converting: 'main': 
Some ops in the model are custom ops, See instructions to implement custom ops: https://www.tensorflow.org/lite/guide/ops_custom 
Custom ops: SimpleMLCreateModelResource, SimpleMLInferenceOpWithHandle
Details:
	tf.SimpleMLCreateModelResource() -> (tensor<!tf_type.resource>) : {container = """", device = """", shared_name = ""simple_ml_model_0c74f688-489e-46e7-aa58-983fddef4f14""}
	tf.SimpleMLInferenceOpWithHandle(tensor<?x8xf32>, tensor<0x0xf32>, tensor<0x0xi32>, tensor<0xi32>, tensor<1xi64>, tensor<1xi64>, tensor<!tf_type.resource>) -> (tensor<?x6xf32>, tensor<6x!tf_type.string>) : {dense_output_dim = 6 : i64, device = """"}
```
</details>"
56382,Custom Op written in C API compilation Issue,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Feature Request

### Source

binary

### Tensorflow Version

2.8

### Custom Code

Yes

### OS Platform and Distribution

Linux 

### Mobile device

07010841551

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hi there!
Is there any example where Custom OP written using C API was compiled successfully and executed after tf_load_library()?
I have referred the entire kernels.h ,ops.h and c_api.h files of the official github tf repo but I am not able to figure it out.
Could someone give the command to execute a C API custom op file along with the code?

Thanks,
Yoga
```


### Standalone code to reproduce the issue

```shell
#include ""tensorflow/c/kernels.h""
#include ""tensorflow/c/ops.h""
#include ""tensorflow/c/tf_tensor.h""
#include ""tensorflow/core/framework/common_shape_fns.h""
#include ""tensorflow/core/framework/op.h""
#include ""tensorflow/core/framework/registration/registration.h""
#include ""tensorflow/core/framework/shape_inference.h""
#include ""tensorflow/core/platform/macros.h""

// shape inference
void basic_shape_inference_fn(TF_ShapeInferenceContext* ctx,TF_Status* status) {
  TF_ShapeHandle* handle = TF_NewShapeHandle();
  TF_ShapeInferenceContextGetInput(ctx, 0, handle, status);
  TF_ShapeInferenceContextSetOutput(ctx, 0, handle, status);
  TF_DeleteShapeHandle(handle);
  assert(TF_GetCode(status) != TF_OK);
}

typedef struct Basic {
  TF_Tensor* input;
  TF_Tensor* output;
} Basic;

static void* Basic_Create(TF_OpKernelConstruction* context) {
  Basic* k = (Basic*) calloc(1, sizeof(Basic));
  TF_Status* status = TF_NewStatus();
  /* initialize the fields of k as needed */
  assert(TF_GetCode(status) != TF_OK);
  TF_DeleteStatus(status);
  return (void*) k;
  
}

static void Basic_Compute(void* k, TF_OpKernelContext* ctx) { 
  /* compute the result */
  TF_Tensor* input_tensor=NULL;
  TF_Status* status = TF_NewStatus();
  TF_GetInput(ctx, 0, &input_tensor, status);
  TF_SetOutput(ctx, 0, input_tensor, status);
  assert(TF_GetCode(status) != TF_OK);
  TF_DeleteStatus(status);
}

static void Basic_Delete(void* kernel) {
  delete static_cast<Basic*>(kernel);
}

// Op and Kernel Registration

void InitPlugin(){
  TF_Status* status = TF_NewStatus();
  TF_OpDefinitionBuilder* op_builder = TF_NewOpDefinitionBuilder(""Basic"");
  TF_OpDefinitionBuilderAddInput(op_builder, ""to_zero: int32"");
  TF_OpDefinitionBuilderAddOutput(op_builder, ""zeroed: int32"");
  TF_OpDefinitionBuilderSetShapeInferenceFunction(op_builder,&basic_shape_inference_fn);
  TF_RegisterOpDefinition(op_builder, status);
  assert(TF_GetCode(status) != TF_OK);
  TF_DeleteStatus(status);
  TF_Status* status1 = TF_NewStatus();
    auto* builder = TF_NewKernelBuilder(""Basic"", tensorflow::DEVICE_CPU,
                                        &Basic_Create, &Basic_Compute,
                                        &Basic_Delete);
    TF_RegisterKernelBuilder(""Basic"", builder, status1);
    assert(TF_OK != TF_GetCode(status1));
  TF_DeleteStatus(status1);
}

// TF_ATTRIBUTE_UNUSED static bool IsBasicKernelRegistered = []() {
//   if (SHOULD_REGISTER_OP_KERNEL(""Basic"")) {
//     RegisterBasicKernel();
//   }
//   return true;
// }();
```


### Relevant log output

```shell
module '012ff3e36e3c24aefc4a3a7b68a03fedd1e7a7e1' has no attribute 'basic'
```
</details>"
56381,Support item assignment in tensor object.,"Issue Type: Feature Request
Source: binary
Tensorflow Version: 2.9
Custom Code: Yes

# Current Behaviour?

Item assignment is not supported.

Discuss more here. https://github.com/tensorflow/tensorflow/issues/33131

# Standalone code to reproduce the issue

```python
outputs = tf.zeros(..)

for step_index in range(5):
    outputs[step_index,...] = step_index + 1
```

Please support this and make a roadmap for the next version of tensorflow. And kindly don't suggest to cast to `tf.Variable` and use `assign/assgn_add` etc or any other non-intuitive workaround."
56378,ImportError: cannot import name 'full_type_pb2',"### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf.2.6

### Custom Code

Yes

### OS Platform and Distribution

Windows10 x64

### Mobile device

_No response_

### Python version

3.6.8


### Current Behaviour?

```shell
When import tensorflow raise ImportError: cannot import name 'full_type_pb2'
```


### Standalone code to reproduce the issue

```shell
use anaconda-4.12.0 to create python env.
Package                 Version
----------------------- ------------
absl-py                 0.15.0
astunparse              1.6.3
atomicwrites            1.4.0
attrs                   21.4.0
cached-property         1.5.2
cachetools              4.2.4
certifi                 2021.10.8
cffi                    1.15.0
charset-normalizer      2.0.10
clang                   5.0
cloudpickle             2.0.0
colorama                0.4.4
configparser            3.7.5
cycler                  0.11.0
dataclasses             0.8
deap                    1.3.1
decorator               4.4.2
enum34                  1.1.10
esdk-obs-python         3.21.8
flatbuffers             1.12
future                  0.18.2
gast                    0.4.0
google-auth             1.35.0
google-auth-oauthlib    0.4.6
google-pasta            0.2.0
grpcio                  1.43.0
h5py                    3.1.0
hyperopt                0.2.7
idna                    3.3
importlib-metadata      4.8.3
importlib-resources     5.4.0
joblib                  1.1.0
jsonschema              3.2.0
keras                   2.6.0
Keras-Preprocessing     1.1.2
kiwisolver              1.3.1
lightgbm                3.3.2
lxml                    4.9.0
Markdown                3.3.6
matplotlib              3.3.4
mip                     1.13.0
more-itertools          8.12.0
mpmath                  1.2.1
networkx                2.5.1
numpy                   1.19.5
oauthlib                3.1.1
opt-einsum              3.3.0
packaging               21.3
pandas                  1.1.5
Pillow                  8.4.0
pip                     21.3.1
pluggy                  0.13.1
protobuf                4.21.0
py                      1.11.0
py4j                    0.10.9.5
pyaml                   21.10.1
pyasn1                  0.4.8
pyasn1-modules          0.2.8
pycparser               2.21
pycryptodome            3.10.1
pyparsing               3.0.7
pyrsistent              0.18.0
pytest                  4.6.11
python-dateutil         2.8.2
python-docx             0.8.11
python-intervals        1.10.0.post1
pytz                    2021.3
PyWavelets              1.1.1
PyYAML                  6.0
requests                2.26.0
requests-oauthlib       1.3.0
rsa                     4.8
ruamel.yaml             0.16.13
ruamel.yaml.clib        0.2.6
scikit-learn            0.23.2
scikit-optimize         0.8.1
scipy                   1.4.1
setuptools              47.1.1
simon                   1.0.8
six                     1.15.0
sympy                   1.9
tensorboard             2.6.0
tensorboard-data-server 0.6.1
tensorboard-plugin-wit  1.8.1
tensorflow              2.6.2
tensorflow-estimator    2.6.0
termcolor               1.1.0
threadpoolctl           3.1.0
tqdm                    4.64.0
typing-extensions       3.7.4.3
urllib3                 1.26.8
wcwidth                 0.2.5
Werkzeug                2.0.2
wheel                   0.37.1
wincertstore            0.2
wrapt                   1.12.1
xgboost                 1.4.2
zipp                    3.6.0
```


### Relevant log output

Traceback (most recent call last):
  File ""D:\solution_utils.py"", line 17, in <module>
    import tensorflow as tf
  File ""C:\Users\jq\AppData\Roaming\Python\Python36\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\jq\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\__init__.py"", line 46, in <module>
    from tensorflow.python import data
  File ""C:\Users\jq\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\data\__init__.py"", line 25, in <module>
    from tensorflow.python.data import experimental
  File ""C:\Users\jq\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\data\experimental\__init__.py"", line 97, in <module>
    from tensorflow.python.data.experimental import service
  File ""C:\Users\jq\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\data\experimental\service\__init__.py"", line 353, in <module>
    from tensorflow.python.data.experimental.ops.data_service_ops import distribute
  File ""C:\Users\jq\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\data\experimental\ops\data_service_ops.py"", line 26, in <module>
    from tensorflow.python.data.experimental.ops import compression_ops
  File ""C:\Users\jq\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\data\experimental\ops\compression_ops.py"", line 20, in <module>
    from tensorflow.python.data.util import structure
  File ""C:\Users\jq\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\data\util\structure.py"", line 26, in <module>
    from tensorflow.python.data.util import nest
  File ""C:\Users\jq\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\data\util\nest.py"", line 40, in <module>
    from tensorflow.python.framework import sparse_tensor as _sparse_tensor
  File ""C:\Users\jq\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\framework\sparse_tensor.py"", line 28, in <module>
    from tensorflow.python.framework import constant_op
  File ""C:\Users\jq\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\framework\constant_op.py"", line 29, in <module>
    from tensorflow.python.eager import execute
  File ""C:\Users\jq\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\eager\execute.py"", line 28, in <module>
    from tensorflow.python.framework import ops
  File ""C:\Users\jq\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\framework\ops.py"", line 54, in <module>
    from tensorflow.python.framework import cpp_shape_inference_pb2
  File ""C:\Users\jq\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\framework\cpp_shape_inference_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import full_type_pb2 as tensorflow_dot_core_dot_framework_dot_full__type__pb2
ImportError: cannot import name 'full_type_pb2'"
56377,No mechanism to ensure nightly ARM64 CD pipeline passes,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

master

### Custom Code

No

### OS Platform and Distribution

manylinux2014 aarch64

### Mobile device

_No response_

### Python version

3.7, 3.8, 3.9, 3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Nightly CD pipeline for Linux ARM64 wheels is now enabled on master branch. But currently we do not have a mechanism to make sure that the pipeline is not broken by new commits.

We do have CI (same build/test stage as CD) pipelines running for PR commits and all master branch commits. These pipelines can be referred to in case there is a failure in nightly CD pipeline to find the root cause. But this effort is currently manual.

We need to have some automation here to make sure the nightly pipeline is not broken and we have nightly wheels published to PyPI everyday to help the developer community.

One easy way to achieve this is by making CI checks a hard requirement before committing to master branch.
```


### Standalone code to reproduce the issue

```shell
Not required.
```


### Relevant log output

_No response_</details>"
56376,How to install tensorflow v<2.0?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.?

### Custom Code

No

### OS Platform and Distribution

mac 12.4

### Mobile device

_No response_

### Python version

3.8.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
For code I am running, I need tensorflow<2.0, how do I install this? pip will not let me install <2.2.0
```


### Standalone code to reproduce the issue

```shell
pip install tensorflow==1.4
ERROR: Could not find a version that satisfies the requirement tensorflow==1.4 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.4.0, 2.4.1, 2.4.2, 2.4.3, 2.4.4, 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.5, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 2.7.2, 2.7.3, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1)
ERROR: No matching distribution found for tensorflow==1.4
```


### Relevant log output

_No response_</details>"
56374,Add tf.Complex to tflite ops. Support padding and slicing of complex tensors,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
It would be great if we could get support in tflite for the `tf.Complex` op as well as support for slicing and padding complex numbers.

This is very important for audio use cases as it often involves running STFTs and working with complex numbers.

Here's the error I currently get when I try to convert my model:


Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select
TF Select ops: Complex, Pad, StridedSlice
Details:
	tf.Complex(tensor<2x128x2048x2xf32>, tensor<2x128x2048x2xf32>) -> (tensor<2x128x2048x2xcomplex<f32>>) : {device = """"}
	tf.Pad(tensor<2x128x2048x2xcomplex<f32>>, tensor<4x2xi32>) -> (tensor<2x128x2049x2xcomplex<f32>>) : {device = """"}
	tf.StridedSlice(tensor<2x128x2049x2xcomplex<f32>>, tensor<4xi32>, tensor<4xi32>, tensor<4xi32>) -> (tensor<2x128x2048x2xcomplex<f32>>) : {begin_mask = 15 : i64, ellipsis_mask = 0 : i64, end_mask = 11 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 0 : i64}
```
```


### Standalone code to reproduce the issue

```shell
NA
```


### Relevant log output

_No response_</details>"
56373,Locating the numpy include path can fail incorrectly,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

Many versions. Affects 2.9.1 and HEAD (as of ee96662)

### Custom Code

Yes

### OS Platform and Distribution

Most. Reproduced on Ubuntu 20.04

### Mobile device

N/A

### Python version

3.8.10

### Bazel version

5.1.1

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

N/A

### GPU model and memory

N/A

### Current Behaviour?

### NOTE: This is a re-filing of #56346 to use the issue template and add a complete reproduction

[`_get_numpy_include()`](https://github.com/tensorflow/tensorflow/blob/d7e521a1ad21681855b439b9c2a05837c804e488/third_party/py/python_configure.bzl#L188-L201) will fail if importing `numpy` causes _anything_ to be written to stderr, even if the command succeeds. I found this while helping a user in #python on the Libera.chat IRC network debug [a failure](https://github.com/google/yggdrasil-decision-forests/issues/19) when building `yggdrasil-decision-forests`. When they import `numpy` on their system, they get the following non-fatal warning:

```
OpenBLAS WARNING - could not determine the L2 cache size on this system, assuming 256k
```

Which causes `execute()` to fail. It looks like the `allow_failure` parameter for that function is meant to address this, so adding this argument should fix the issue in this specific case, but I'm not sure what the other implications might be. (as an aside, this parameter is confusingly documented, I have opened #56345 to address that)

Insight from someone who is more familiar with TF's build system would be appreciated, but inspecting stdout/stderr strikes me as a very strange way to work out whether or not the executed process failed. Is there a reason this bit of code does not check the [`return_code`](https://bazel.build/rules/lib/exec_result) of the raw execution?


### Standalone code to reproduce the issue

Below is a `Dockerfile` that should completely reproduce the problem.

**NOTE** that this example does modify `numpy` to issue a guaranteed warning, but as linked above, warnings _can_ occur in `numpy` and its dependencies.

```Dockerfile
FROM ubuntu:20.04 as repro                                                        
                                                                                  
ENV DEBIAN_FRONTEND=noninteractive                                                
ENV LANG C.UTF-8                                                                  
                                                                                  
RUN apt update && \                                                               
    apt install -y \                                                              
        curl \                                                                    
        git \                                                                     
        python3 python3-pip                                                       
                                                                                  
# install bazel                                                                   
RUN apt install apt-transport-https curl gnupg -y && \                            
    curl -fsSL https://bazel.build/bazel-release.pub.gpg | gpg --dearmor > bazel.g
pg && \                                                                           
    mv bazel.gpg /etc/apt/trusted.gpg.d/ && \                                     
    echo ""deb [arch=amd64] https://storage.googleapis.com/bazel-apt stable jdk1.8"" | tee /etc/apt/sources.list.d/bazel.list && \                                    
    apt update && apt install bazel -y && \                                       
    apt update && apt full-upgrade -y && \                                        
    apt install bazel-1.0.0 -y                                                    
                                                                                  
RUN python3 -m pip install numpy==1.22.4 && \                                     
    # NOTE: the below modifies numpy in-place to reliably simulate warnings       
    # that might be emitted in normal operation of numpy (e.g. OpenBLAS can       
    # issue warnings). This can cause a build of TensorFlow to fail incorrectly   
    sed -i ""38i\    warnings.warn('example warning (written to stderr)')"" /usr/local/lib/python3.8/dist-packages/numpy/lib/utils.py                                 
                                                                                  
RUN git clone --depth=1 https://github.com/tensorflow/tensorflow/ /tmp/tensorflow 
                                                                                  
WORKDIR /tmp/tensorflow                                                           
CMD [""/bin/sh"", ""-c"", ""bazel build //tensorflow""]
```


### Relevant log output

```shell
$ docker build -t tfbug:repro .
# ... snip ...
Successfully tagged tfbug:repro

$ docker run -it tfbug:repro
Extracting Bazel installation...
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=0
INFO: Reading rc options for 'build' from /tmp/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /tmp/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /tmp/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /tmp/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:linux in file /tmp/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /tmp/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/62a3081341bdbe5614899de6a1aa3ec183810229.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
INFO: Repository local_config_python instantiated at:
  /tmp/tensorflow/WORKSPACE:15:14: in <toplevel>
  /tmp/tensorflow/tensorflow/workspace2.bzl:870:19: in workspace
  /tmp/tensorflow/tensorflow/workspace2.bzl:100:21: in _tf_toolchains
Repository rule python_configure defined at:
  /tmp/tensorflow/third_party/py/python_configure.bzl:298:35: in <toplevel>
INFO: Repository local_execution_config_python instantiated at:
  /tmp/tensorflow/WORKSPACE:15:14: in <toplevel>
  /tmp/tensorflow/tensorflow/workspace2.bzl:870:19: in workspace
  /tmp/tensorflow/tensorflow/workspace2.bzl:90:27: in _tf_toolchains
  /tmp/tensorflow/tensorflow/tools/toolchains/remote_config/configs.bzl:6:28: in initialize_rbe_configs
  /tmp/tensorflow/tensorflow/tools/toolchains/remote_config/rbe_config.bzl:158:27: in _tensorflow_local_config
Repository rule local_python_configure defined at:
  /tmp/tensorflow/third_party/py/python_configure.bzl:279:41: in <toplevel>
ERROR: An error occurred during the fetch of repository 'local_config_python':
   Traceback (most recent call last):
        File ""/tmp/tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl
                _create_local_python_repository(repository_ctx)
        File ""/tmp/tensorflow/third_party/py/python_configure.bzl"", line 216, column 39, in _create_local_python_repository
                numpy_include = _get_numpy_include(repository_ctx, python_bin) + ""/numpy""
        File ""/tmp/tensorflow/third_party/py/python_configure.bzl"", line 190, column 19, in _get_numpy_include
                return execute(
        File ""/tmp/tensorflow/third_party/remote_config/common.bzl"", line 230, column 13, in execute
                fail(
Error in fail: Problem getting numpy include path.
/usr/local/lib/python3.8/dist-packages/numpy/lib/utils.py:38: UserWarning: example warning (written to stderr)
  warnings.warn('example warning (written to stderr)')
Is numpy installed?
ERROR: /tmp/tensorflow/WORKSPACE:15:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):
        File ""/tmp/tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl
                _create_local_python_repository(repository_ctx)
        File ""/tmp/tensorflow/third_party/py/python_configure.bzl"", line 216, column 39, in _create_local_python_repository
                numpy_include = _get_numpy_include(repository_ctx, python_bin) + ""/numpy""
        File ""/tmp/tensorflow/third_party/py/python_configure.bzl"", line 190, column 19, in _get_numpy_include
                return execute(
        File ""/tmp/tensorflow/third_party/remote_config/common.bzl"", line 230, column 13, in execute
                fail(
Error in fail: Problem getting numpy include path.
/usr/local/lib/python3.8/dist-packages/numpy/lib/utils.py:38: UserWarning: example warning (written to stderr)
  warnings.warn('example warning (written to stderr)')
Is numpy installed?
INFO: Repository go_sdk instantiated at:
  /tmp/tensorflow/WORKSPACE:23:14: in <toplevel>
  /tmp/tensorflow/tensorflow/workspace0.bzl:134:20: in workspace
  /root/.cache/bazel/_bazel_root/e5cce820cc082410b4fcc604db349066/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps
  /root/.cache/bazel/_bazel_root/e5cce820cc082410b4fcc604db349066/external/io_bazel_rules_go/go/toolchain/toolchains.bzl:379:28: in go_register_toolchains
  /root/.cache/bazel/_bazel_root/e5cce820cc082410b4fcc604db349066/external/io_bazel_rules_go/go/private/sdk.bzl:65:21: in go_download_sdk
Repository rule _go_download_sdk defined at:
  /root/.cache/bazel/_bazel_root/e5cce820cc082410b4fcc604db349066/external/io_bazel_rules_go/go/private/sdk.bzl:53:35: in <toplevel>
ERROR: Analysis of target '//tensorflow:tensorflow' failed; build aborted: Problem getting numpy include path.
/usr/local/lib/python3.8/dist-packages/numpy/lib/utils.py:38: UserWarning: example warning (written to stderr)
  warnings.warn('example warning (written to stderr)')
Is numpy installed?
INFO: Elapsed time: 116.976s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (33 packages loaded, 15 targets co\
nfigured)
$ docker run -it tfbug:repro python3 -c ""import numpy; print(numpy.get_include())""  # demonstration: get_include() succeeds just fine
/usr/local/lib/python3.8/dist-packages/numpy/lib/utils.py:38: UserWarning: example warning (written to stderr)
  warnings.warn('example warning (written to stderr)')
/usr/local/lib/python3.8/dist-packages/numpy/core/include
$ echo $?  # the return code of the process is zero, there was no failure
0
```
</details>"
56372,`DistributedIterator` cannot be checkpointed because it is not trackable,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

2.10.0-dev20220520

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

N/A

### Python version

3.8

### Bazel version

N/A

### GCC/Compiler version

N/A

### CUDA/cuDNN version

11.2/8.1

### GPU model and memory

N/A

### Current Behaviour?

```shell
`DistributedIterator` cannot be checkpointed because it is not trackable
```


### Standalone code to reproduce the issue

```shell
ds = tf.data.Dataset.range(100).shuffle(8).batch(7)
ds_it = iter(ds)
print(type(ds_it))
ckpt = tf.train.Checkpoint(ds_it=ds_it)

strategy = tf.distribute.OneDeviceStrategy('cpu')
dist_ds = strategy.experimental_distribute_dataset(ds)
dist_ds_it = iter(dist_ds)
print(type(dist_ds_it))
ckpt = tf.train.Checkpoint(dist_ds_it=dist_ds_it)
```


### Relevant log output

```shell
<class 'tensorflow.python.data.ops.iterator_ops.OwnedIterator'>
<class 'tensorflow.python.distribute.input_lib.DistributedIterator'>
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-5-cabcd820a79a> in <module>()
      8 dist_ds_it = iter(dist_ds)
      9 print(type(dist_ds_it))
---> 10 ckpt = tf.train.Checkpoint(dist_ds_it=dist_ds_it)

/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/util.py in __init__(self, root, **kwargs)
   2015       # v to a Trackable data structure when v is a list/dict/tuple.
   2016       converted_v = getattr(self, k)
-> 2017       _assert_trackable(converted_v, k)
   2018 
   2019       if root:

/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/util.py in _assert_trackable(obj, name)
   1462       obj, (base.Trackable, def_function.Function)):
   1463     raise ValueError(
-> 1464         f""`Checkpoint` was expecting {name} to be a trackable object (an ""
   1465         f""object derived from `Trackable`), got {obj}. If you believe this ""
   1466         ""object should be trackable (i.e. it is part of the ""

ValueError: `Checkpoint` was expecting dist_ds_it to be a trackable object (an object derived from `Trackable`), got <tensorflow.python.distribute.input_lib.DistributedIterator object at 0x7fb291849610>. If you believe this object should be trackable (i.e. it is part of the TensorFlow Python API and manages state), please open an issue.
```
</details>"
56370,tf.saved_model.save() excessive function tracing,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

binary

### Tensorflow Version

tf 2.8

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3,8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
This is filed against TF 2.8 but the behavior described appeared when upgrading from TF 2.3 to TF 2.4.

Create a tf.Module subclass containing a tf.keras.models.Model subclass.
Add tf.functions to the tf.Module that call the model.
Saved the tf.Module with tf.saved_model.save() in order to reload and use it in a language other than python.

With TF2.3, the functions are traced only once.
With TF2.4, the functions are traced twice.

With large models, this can almost double the model save time (e.g. up to 40s for a distilbert model with 6 layers).
```


### Standalone code to reproduce the issue

```shell
Execute this same code with TF2.3.2 and TF2.4.4 (output with TF2.8 is the same as for 2.4.4)


import tensorflow as tf
print('tf version', tf.__version__)

import os

F1_SIG=[tf.TensorSpec([None, None], tf.float32, name='inputs_1')]
F2_SIG=[tf.TensorSpec([None], tf.float32, name='inputs_2')]

class CustomModel(tf.keras.models.Model):
    def __init__(self, **kwargs):
        super().__init__(**kwargs) # handles standard args (e.g., name)

    def call(self, x):
        _x = tf.random.normal((2,3,4)) + x 
        return _x

class CustomModule(tf.Module):
    def __init__(self):
        self.model = CustomModel()

    @tf.function(input_signature=F1_SIG)
    def f1(self, x):
        print('tracing f1')
        return self.model(x[0])

    @tf.function(input_signature=F2_SIG)
    def f2(self, x):
        print('tracing f2')
        return self.model(x)

x = tf.random.uniform((2,4))*10

saved_model_dir='saved_model'
os.makedirs(saved_model_dir, exist_ok=True)

module = CustomModule()
signatures = {""serving_default"": module.f1.get_concrete_function(*F1_SIG),
              ""serve_sig_v2""   : module.f2.get_concrete_function(*F2_SIG),
              }
tf.saved_model.save(module, saved_model_dir, signatures=signatures)
```

Output with tensorflow 2.3.2:
```
tf version 2.3.2
tracing f1
tracing f2
INFO:tensorflow:Assets written to: saved_model/assets
```

Output with tensorflow 2.4.4:
```
tf version 2.4.4
tracing f1
tracing f2
tracing f1
tracing f2
INFO:tensorflow:Assets written to: saved_model/assets
```

Why are the functions traced a second time with TF>=2.4 ?

TF2.3 colab:
https://colab.research.google.com/drive/19iniJKEcFmhkNDVe6sbseOyASWb3oCYC?usp=sharing

TF2.4 colab: 
https://colab.research.google.com/drive/1NVCSiSednJnnZqk_s1VmOsIMSUluaLP7?usp=sharing


### Relevant log output

_No response_</details>"
56367,[TensorFlow Lite label_image]  Abort occurs with xnnpack_delegate option.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

v2.9.1 (or master)

### Custom Code

No

### OS Platform and Distribution

Raspberry Pi OS (bullseye)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

gcc version 10.2.1 20210110 (Debian 10.2.1-6) 

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Build tensorflow-lite and label_image from source (with CMake).
Execute with the `--xnnpack_delegate` option.

label_image \
  --tflite_model /tmp/mobilenet_v1_1.0_224.tflite \
  --labels /tmp/labels.txt \
  --image tensorflow/lite/examples/label_image/testdata/grace_hopper.bmp \
  --xnnpack_delegate 1
Type mismatch while accessing parameter.
Abort
```
```


### Standalone code to reproduce the issue

```shell
Clone repository.

git clone -b v2.9.1 https://github.com/tensorflow/tensorflow.git
```

Build tensorflow-lite and label_image
```
mkdir build && cd build
cmake ../tensorflow/tensorflow/lite/
cmake --build . -j$(nproc)
cmake --build . -j$(nproc) -t label_image
```

Download tflite model
```
wget https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_1.0_224.tgz
tar xf mobilenet_v1_1.0_224.tgz
wget  https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_1.0_224_frozen.tgz
tar xf mobilenet_v1_1.0_224_frozen.tgz
cp mobilenet_v1_1.0_224/labels.txt ./
```

Exec label_image (with use_xnnpack option)
```
./examples/label_image/label_image \
  --tflite_model ./mobilenet_v1_1.0_224.tflite \
  --labels ./labels.txt \
  --image ../tensorflow/tensorflow/lite/examples/label_image/testdata/grace_hopper.bmp
```
```


### Relevant log output

_No response_</details>"
56365,There was a problem quantifying the model with two inputs which have different dimensions.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

windows10

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
My model have two inputs.The first input have dimension [1,1,257],the second input have dimension [1,2,128,2].I want to quantify my model with int16 input/output type and int8 weights type.When I use my
code to quantify my model, Error occurred.
```


### Standalone code to reproduce the issue

```shell
def representative_dataset():
    for _ in range(100):
      data1 = np.random.rand(1, 2, 128,2)
      data2 = np.random.rand(1, 1, 257)
      #data3 = {'input_2': [data2.astype(np.float32)], 'input_3': [data1.astype(np.float32)]}
      data3 = [[data2.astype(np.float32)], [data1.astype(np.float32)]]
      yield data3

if use_dynamic_range_quant:
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.target_spec.supported_ops =\
      [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]
    converter.inference_input_type = tf.int16
    converter.inference_output_type = tf.int16
    converter.representative_dataset = representative_dataset
tflite_model = converter.convert()
```


### Relevant log output

```shell
The error like this:
  File ""D:\ProgramFiles\Anaconda3\lib\site-packages\tensorflow\lite\python\optimize\calibrator.py"", line 129, in <listcomp>
    self._calibrator.Prepare([list(s.shape) for s in input_array])
AttributeError: 'list' object has no attribute 'shape'
```
</details>"
56363,Keras/tensorflow runs only one epoch,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.6.0

### Custom Code

Yes

### OS Platform and Distribution

Windows 10 21H2

### Mobile device

_No response_

### Python version

3.9.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

Cuda 11.2.2_461.33/ cuDNN v8.1.1

### GPU model and memory

GeForce NVidia RTX 3060  16GB RAM

### Current Behaviour?

```shell
My tensforflow 2 is gpu-enabled and had been running model training with no issues until last week. Then the issue occurred - it runs only one epoch and stops.

I have all libraries installed via one of Anaconda's virtual environments. when I ran 'install -c anaconda tensorflow-gpu' , I actually got installed the following versions: tensorflow: 2.6.0,  cudatoolkit-11.3.1, cudnn-8.2.1

I have tried rolling back to the previous Nvidia driver version (from 512.95 to 512.15), and restoring to an earlier point in Windows when it worked.
```


### Standalone code to reproduce the issue

```shell
tf.test.is_built_with_cuda()
Out: True
physical_devices = tf.config.list_physical_devices('GPU')
physical_devices
Out: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]

with open('./Data/train.csv') as f:
    data = f.readlines()   

def read_csv(filename):
    sentences = []
    labels = []
    with open(filename, 'r') as csvfile:        
        reader = csv.reader(csvfile, delimiter=',')
        for row in reader:
            sentences.append(row[0])
            labels.append(row[1])

    return sentences, labels

## some data preprocessing
...
## glove embeddings

GLOVE_EMBEDDINGS = {}

with open(GLOVE_FILE) as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        GLOVE_EMBEDDINGS[word] = coefs
EMBEDDING_DIM = 50
# Initialize an empty numpy array with the appropriate size
EMBEDDINGS_MATRIX = np.zeros((VOCAB_SIZE+1, EMBEDDING_DIM))

# Iterate all of the words in the vocabulary and if the vector representation for 
# each word exists within GloVe's representations, save it in the EMBEDDINGS_MATRIX array
for word, i in word_index.items():
    embedding_vector = GLOVE_EMBEDDINGS.get(word)
    if embedding_vector is not None:
        EMBEDDINGS_MATRIX[i] = embedding_vector

## hyperparameters
EMBEDDING_DIM = 50
MAXLEN = 500 #1000, 1400
VOCAB_SIZE =  33713

DENSE1_DIM = 64
DENSE2_DIM = 32
DENSE3_DIM = 16

LSTM1_DIM = 32 
LSTM2_DIM = 16
WD = 0.001
FILTERS = 64  
KERNEL_SIZE = 5

## Model Definition 
model_lstm = tf.keras.Sequential([
    tf.keras.layers.Embedding(VOCAB_SIZE+1, EMBEDDING_DIM, input_length=MAXLEN,weights=[EMBEDDINGS_MATRIX], trainable=False),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM1_DIM, dropout=0.5, kernel_regularizer = regularizers.l2(WD),return_sequences=True)),
    tf.keras.layers.Dense(DENSE2_DIM, activation='relu'),    
    tf.keras.layers.Conv1D(FILTERS, KERNEL_SIZE, activation='relu', ),
    tf.keras.layers.Dropout(0.1),
    tf.keras.layers.GlobalAveragePooling1D(), 
    tf.keras.layers.Dense(DENSE3_DIM, activation='relu'),  
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Set the training parameters
model_lstm.compile(loss='binary_crossentropy',
                   optimizer=tf.keras.optimizers.Adam(), 
                   metrics=[tf.keras.metrics.BinaryAccuracy()])

# Print the model summary
model_lstm.summary()

## training the model
history_lstm = model_lstm.fit(sent_tok_train, labels_train,  validation_data=(sent_tok_val, labels_val), verbose =2)

Out: 
44/44 - 8s - loss: 0.7946 - binary_accuracy: 0.5629 - val_loss: 0.7613 - val_binary_accuracy: 0.5250
```


### Relevant log output

```shell
None. no error messages at all
```
</details>"
56362,Build from source with cuda support fails,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

master branch 

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

5.1.1

### GCC/Compiler version

gcc version 11.2.0

### CUDA/cuDNN version

cuda 10.2 cuddn 8.4

### GPU model and memory

quadro k1100m

### Current Behaviour?

```shell
Building from source with cuda support fails with error:

ERROR: /home/tensorflow/tensorflow/core/kernels/BUILD:3581:18: Compiling tensorflow/core/kernels/cast_op_gpu.cu.cc failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/cast_op_gpu/cast_op_gpu.cu.pic.d ... (remaining 452 arguments skipped)
nvcc fatal   : Value 'c++17' is not defined for option 'std'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```


### Standalone code to reproduce the issue

```shell
Cuda 10.2 toolkit and cudnn 8.4 installed.

Configuration file[.tf_configure.bazel] as follows:

build --action_env PYTHON_BIN_PATH=""/usr/bin/python3""
build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages""
build --python_path=""/usr/bin/python3""
build --action_env CUDA_TOOLKIT_PATH=""/usr/local/cuda-10.2""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""3.0""

run following command:
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=183
INFO: Reading rc options for 'build' from /home/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false
INFO: Reading rc options for 'build' from /home/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-10.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.0 --action_env LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64:/home/vulkan/1.2.176.1/x86_64/lib:/home/vulkan/1.2.176.1/x86_64/lib: --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-11 --config=cuda
INFO: Reading rc options for 'build' from /home/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /home/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda in file /home/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:opt in file /home/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare
INFO: Found applicable config definition build:linux in file /home/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: /home/.cache/bazel/_bazel_bourbaki/8677db6468c86623aec25477afdca2f5/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:10: 
Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: /home/tensorflow/tensorflow/core/kernels/linalg/BUILD:128:18: Compiling tensorflow/core/kernels/linalg/matrix_set_diag_op_gpu.cu.cc failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF ... (remaining 487 arguments skipped)
nvcc fatal   : Value 'c++17' is not defined for option 'std'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 427.075s, Critical Path: 36.23s
INFO: 1420 processes: 267 internal, 1153 local.
FAILED: Build did NOT complete successfully
```
</details>"
56359,Simple audio recognition: Recognizing keywords -  'images' must have either 3 or 4 dimensions.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.7.0

### Custom Code

No

### OS Platform and Distribution

MacOS Monterey 12.4

### Mobile device

_No response_

### Python version

Python 3.10.4

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I'm following the TensorFlow tutorial on Simple audio recognition: Recognizing keywords
and when training the model, after the first EPOCH, I get the following error:

Epoch 1/10
100/100 [==============================] - ETA: 0s - loss: 1.7273 - accuracy: 0.3780WARNING:tensorflow:Model was constructed with shape (None, 124, 129, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 124, 129, 1), dtype=tf.float32, name='input_1'), name='input_1', description=""created by layer 'input_1'""), but it was called on an input with incompatible shape (None, None).
Traceback (most recent call last):
  File ""/Users/feiticeir0/scripts/audio_recognition/audio_classification.py"", line 283, in <module>
    history = model.fit(
  File ""/opt/miniconda3/envs/audio_recognition/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/opt/miniconda3/envs/audio_recognition/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py"", line 1129, in autograph_handler
    raise e.ag_error_metadata.to_exception(e)
ValueError: in user code:

    File ""/opt/miniconda3/envs/audio_recognition/lib/python3.10/site-packages/keras/engine/training.py"", line 1366, in test_function  *
        return step_function(self, iterator)
    File ""/opt/miniconda3/envs/audio_recognition/lib/python3.10/site-packages/keras/engine/training.py"", line 1356, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/opt/miniconda3/envs/audio_recognition/lib/python3.10/site-packages/keras/engine/training.py"", line 1349, in run_step  **
        outputs = model.test_step(data)
    File ""/opt/miniconda3/envs/audio_recognition/lib/python3.10/site-packages/keras/engine/training.py"", line 1303, in test_step
        y_pred = self(x, training=False)
    File ""/opt/miniconda3/envs/audio_recognition/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler
        raise e.with_traceback(filtered_tb) from None

    ValueError: Exception encountered when calling layer ""resizing"" (type Resizing).
    
    'images' must have either 3 or 4 dimensions.
    
    Call arguments received:
       inputs=tf.Tensor(shape=(None, None), dtype=float32)
```


### Standalone code to reproduce the issue

```shell
Just follow the code in the tutorial.
```


### Relevant log output

```shell
Epoch 1/10
100/100 [==============================] - ETA: 0s - loss: 1.7273 - accuracy: 0.3780WARNING:tensorflow:Model was constructed with shape (None, 124, 129, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 124, 129, 1), dtype=tf.float32, name='input_1'), name='input_1', description=""created by layer 'input_1'""), but it was called on an input with incompatible shape (None, None).
Traceback (most recent call last):
  File ""/Users/feiticeir0/scripts/audio_recognition/audio_classification.py"", line 283, in <module>
    history = model.fit(
  File ""/opt/miniconda3/envs/audio_recognition/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/opt/miniconda3/envs/audio_recognition/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py"", line 1129, in autograph_handler
    raise e.ag_error_metadata.to_exception(e)
ValueError: in user code:

    File ""/opt/miniconda3/envs/audio_recognition/lib/python3.10/site-packages/keras/engine/training.py"", line 1366, in test_function  *
        return step_function(self, iterator)
    File ""/opt/miniconda3/envs/audio_recognition/lib/python3.10/site-packages/keras/engine/training.py"", line 1356, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/opt/miniconda3/envs/audio_recognition/lib/python3.10/site-packages/keras/engine/training.py"", line 1349, in run_step  **
        outputs = model.test_step(data)
    File ""/opt/miniconda3/envs/audio_recognition/lib/python3.10/site-packages/keras/engine/training.py"", line 1303, in test_step
        y_pred = self(x, training=False)
    File ""/opt/miniconda3/envs/audio_recognition/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler
        raise e.with_traceback(filtered_tb) from None

    ValueError: Exception encountered when calling layer ""resizing"" (type Resizing).
    
    'images' must have either 3 or 4 dimensions.
    
    Call arguments received:
       inputs=tf.Tensor(shape=(None, None), dtype=float32)
```
</details>"
56358,F1 score metric,"### Issue Type

<kbd>Feature Request</kbd>

---

### Tensorflow Version

2.9.1

---

### Current Behaviour?

No F1 score metric. It's a holistic measure for classification.

https://torchmetrics.readthedocs.io/en/stable/classification/f1_score.html

https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score

---

### Standalone code to reproduce the issue

https://www.tensorflow.org/api_docs/python/tf/keras/metrics
"
56356,Division not allowed in graph for MacOS with M1 Max,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.8.0

### Custom Code

No

### OS Platform and Distribution

MaxOS 12.3.1 

### Mobile device

-

### Python version

3.9

### Bazel version

-

### GCC/Compiler version

-

### CUDA/cuDNN version

-

### GPU model and memory

M1 Max 26 cores GPU 32 GB ram

### Current Behaviour?

```shell
The same code runs on Colab but not on MacOS if using GPU.

Please note that the same exact code uploaded on Colab works perfectly, and same for this code on MacOS using CPU as device
```


### Standalone code to reproduce the issue

```shell
# take the VAE example on Keras doc and instead of the KL div use the following
kl_loss = tf.reduce_mean(tf.reduce_sum(tf.square(z_mean)/2, axis=1) + tf.reduce_sum(tf.square(tf.exp(z_var/2) - 1)/4, axis=1))
```


### Relevant log output

```shell
NotFoundError: Graph execution error:

Detected at node 'sub' defined at (most recent call last):
    File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal/lib/python3.9/runpy.py"", line 197, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal/lib/python3.9/runpy.py"", line 87, in _run_code
      exec(code, run_globals)
    File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal/lib/python3.9/site-packages/ipykernel_launcher.py"", line 17, in <module>
      app.launch_new_instance()
    File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal/lib/python3.9/site-packages/traitlets/config/application.py"", line 972, in launch_instance
      app.start()
    File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal/lib/python3.9/site-packages/ipykernel/kernelapp.py"", line 712, in start
      self.io_loop.start()
    File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal/lib/python3.9/site-packages/tornado/platform/asyncio.py"", line 199, in start
      self.asyncio_loop.run_forever()
    File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal/lib/python3.9/asyncio/base_events.py"", line 601, in run_forever
      self._run_once()
    File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal/lib/python3.9/asyncio/base_events.py"", line 1905, in _run_once
      handle._run()
    File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal/lib/python3.9/asyncio/events.py"", line 80, in _run
      self._context.run(self._callback, *self._args)
    File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal/lib/python3.9/site-packages/ipykernel/kernelbase.py"", line 504, in dispatch_queue
      await self.process_one()
    File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal/lib/python3.9/site-packages/ipykernel/kernelbase.py"", line 493, in process_one
      await dispatch(*args)
    File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal/lib/python3.9/site-packages/ipykernel/kernelbase.py"", line 400, in dispatch_shell
      await result
    File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal/lib/python3.9/site-packages/ipykernel/kernelbase.py"", line 724, in execute_request
      reply_content = await reply_content
    File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal/lib/python3.9/site-packages/ipykernel/ipkernel.py"", line 383, in do_execute
      res = shell.run_cell(
    File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal/lib/python3.9/site-packages/ipykernel/zmqshell.py"", line 528, in run_cell
      return super().run_cell(*args, **kwargs)
    File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal/lib/python3.9/site-packages/IPython/core/interactiveshell.py"", line 2881, in run_cell
      result = self._run_cell(
    File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal/lib/python3.9/site-packages/IPython/core/interactiveshell.py"", line 2936, in _run_cell
      return runner(coro)
    File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal/lib/python3.9/site-packages/IPython/core/async_helpers.py"", line 129, in _pseudo_sync_runner
      coro.send(None)
    File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal/lib/python3.9/site-packages/IPython/core/interactiveshell.py"", line 3135, in run_cell_async
      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
    File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal/lib/python3.9/site-packages/IPython/core/interactiveshell.py"", line 3338, in run_ast_nodes
      if await self.run_code(code, result, async_=asy):
    File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal/lib/python3.9/site-packages/IPython/core/interactiveshell.py"", line 3398, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File ""/var/folders/6f/83fhb735331g631bmd84c_xc0000gn/T/ipykernel_52845/3646646759.py"", line 4, in <cell line: 4>
      history = vae.fit(x_train, epochs=30, batch_size=128)
    File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal/lib/python3.9/site-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
      return fn(*args, **kwargs)
    File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal/lib/python3.9/site-packages/keras/engine/training.py"", line 1409, in fit
      tmp_logs = self.train_function(iterator)
    File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal/lib/python3.9/site-packages/keras/engine/training.py"", line 1051, in train_function
      return step_function(self, iterator)
    File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal/lib/python3.9/site-packages/keras/engine/training.py"", line 1040, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal/lib/python3.9/site-packages/keras/engine/training.py"", line 1030, in run_step
      outputs = model.train_step(data)
    File ""/var/folders/6f/83fhb735331g631bmd84c_xc0000gn/T/ipykernel_52845/2081516424.py"", line 21, in train_step
      kl_loss = tf.reduce_mean(tf.reduce_sum(tf.square(z_mean)/2, axis=1) + tf.reduce_sum(tf.square(tf.exp(z_var/2) - 1)/4, axis=1))
Node: 'sub'
No registered 'Expm1' OpKernel for 'GPU' devices compatible with node {{node sub}}
	.  Registered:  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_BFLOAT16]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_COMPLEX128]

	 [[sub]] [Op:__inference_train_function_136152]
```
</details>"
56355,Seeing E tensorflow/core/grappler/costs/op_level_cost_estimator.cc:1104] Incompatible Matrix dimensions ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.8

### Custom Code

No

### OS Platform and Distribution

Linux

### Mobile device

_No response_

### Python version

3.9.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.6.2

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Seeing the following errors on enabling profiler. 

2022-06-04 01:32:28.169748: E tensorflow/core/grappler/costs/op_level_cost_estimator.cc:1104] Incompatible Matrix dimensions
```


### Standalone code to reproduce the issue

```shell
Happens with some gemm/mtmul based models.
```


### Relevant log output

_No response_</details>"
56350,"[intel cpu(Sapphire Rapids) ] TF_ENABLE_ONEDNN_OPTS=0 performance is better(intra_op_parallelism_threads=1,  inter_op_parallelism_threads=1)","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04.4 LTS

### Mobile device

_No response_

### Python version

Python 3.8.10

### Bazel version

_No response_

### GCC/Compiler version

GCC 9.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
kernel version : Linux version 5.16.0-gx-test-new  (gcc (GCC) 9.2.0, GNU ld version 2.30-93.el8)
doker image : tensorflow/tensorflow:latest
cpu: intel cpu(Sapphire Rapids) 

Points to note:
test based on single thread mode,  intra_op_parallelism_threads=1, inter_op_parallelism_threads=1.
[if not set intra_op_parallelism_threads, inter_op_parallelism_threadsTF_ENABLE_ONEDNN_OPTS=1 performance will be significantly better than TF_ENABLE_ONEDNN_OPTS=0]

performance test (tried multiple times basically the same):

1
intra_op_parallelism_threads=1, inter_op_parallelism_threads=1

TF_ENABLE_ONEDNN_OPTS=0

cost time:16.9s (session run 1000 times %CPU 100[top command])

TF_ENABLE_ONEDNN_OPTS=1

cost time:31.1s (session run 1000 times %CPU 100[top command])


2
intra_op_parallelism_threads=10, inter_op_parallelism_threads=1
TF_ENABLE_ONEDNN_OPTS=0

cost time:9.0s (session run 1000 times, %CPU 650[top command])

TF_ENABLE_ONEDNN_OPTS=1

cost time:5.6 s (session run 1000 times, %CPU 530[top command])


[oneDnn]: under single thread, the performance is too poor, but under multi-threading, the cpu load and performance are linearly proportional.
[no oneDnn]: under multi-threadingthe cpu load and performance are  is not linearly proportional


What is the problem with the program?

```


### Standalone code to reproduce the issue

```shell
cpu info:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Byte Order:                      Little Endian
Address sizes:                   52 bits physical, 57 bits virtual
CPU(s):                          224
On-line CPU(s) list:             0-223
Thread(s) per core:              2
Core(s) per socket:              56
Socket(s):                       2
NUMA node(s):                    2
Vendor ID:                       GenuineIntel
CPU family:                      6
Model:                           143
Model name:                      Genuine Intel(R) CPU 0000%@
Stepping:                        3
CPU MHz:                         1900.000
CPU max MHz:                     3700.0000
CPU min MHz:                     800.0000
BogoMIPS:                        3800.00
Virtualization:                  VT-x
L1d cache:                       5.3 MiB
L1i cache:                       3.5 MiB
L2 cache:                        224 MiB
L3 cache:                        210 MiB
NUMA node0 CPU(s):               0-55,112-167
NUMA node1 CPU(s):               56-111,168-223
Vulnerability Itlb multihit:     Not affected
Vulnerability L1tf:              Not affected
Vulnerability Mds:               Not affected
Vulnerability Meltdown:          Not affected
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Not affected
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe s
                                 yscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf t
                                 sc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic
                                  movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3
                                  invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsg
                                 sbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb i
                                 ntel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_loca
                                 l split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vb
                                 mi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus
                                 _lock_detect cldemote movdiri movdir64b enqcmd fsrm avx512_vp2intersect md_clear serialize tsxldtrk pconfig arch_lbr avx5
                                 12_fp16 amx_tile flush_l1d arch_capabilities
```


### Relevant log output

```shell

intra_op_parallelism_threads=10, inter_op_parallelism_threads=1

detail[TF_ENABLE_ONEDNN_OPTS=0]:

tf-docker ~/tf_perf_test > export TF_ENABLE_ONEDNN_OPTS=0
tf-docker ~/tf_perf_test > DNNL_VERBOSE=1 python tf_session_run_v2.py 
2022-06-03 15:00:25.676937: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-06-03 15:00:25.780887: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled
1.981963

detail[TF_ENABLE_ONEDNN_OPTS=1]:
tf-docker ~/tf_perf_test > export TF_ENABLE_ONEDNN_OPTS=1
tf-docker ~/tf_perf_test > DNNL_VERBOSE=1 python tf_session_run_v2.py 
2022-06-03 14:59:34.833958: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-06-03 14:59:35.602788: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-06-03 14:59:35.707948: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled
onednn_verbose,info,oneDNN v2.6.0 (commit N/A)
onednn_verbose,info,cpu,runtime:threadpool,nthr:56
onednn_verbose,info,cpu,isa:Intel AVX-512 with Intel DL Boost and bfloat16 support and Intel AMX with bfloat16 and 8-bit integer support
onednn_verbose,info,gpu,runtime:none
onednn_verbose,info,prim_template:operation,engine,primitive,implementation,prop_kind,memory_descriptors,attributes,auxiliary,problem_desc,exec_time
onednn_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:ba:f0 dst_f32::blocked:AB16b64a:f0,,,512x15680,10.978
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic15680oc512,32.1628
onednn_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:ba:f0 dst_f32::blocked:AB16b64a:f0,,,128x512,0.072998
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic512oc128,0.249023
onednn_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:ba:f0 dst_f32::blocked:AB16b64a:f0,,,64x128,0.0270996
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic128oc64,0.0480957
onednn_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:ba:f0 dst_f32:p:blocked:AB16b16a:f0,,,31x64,0.0249023
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32:p:blocked:AB16b16a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user ,,mb200ic64oc31,0.0339355
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic15680oc512,29.4858
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic512oc128,0.175049
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic128oc64,0.0410156
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32:p:blocked:AB16b16a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user ,,mb200ic64oc31,0.0300293
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic15680oc512,28.623
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic512oc128,0.194092
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic128oc64,0.0461426
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32:p:blocked:AB16b16a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user ,,mb200ic64oc31,0.032959
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic15680oc512,28.0022
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic512oc128,0.193115
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic128oc64,0.0478516
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32:p:blocked:AB16b16a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user ,,mb200ic64oc31,0.0319824
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic15680oc512,28.6951
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic512oc128,0.193115
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic128oc64,0.0461426
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32:p:blocked:AB16b16a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user ,,mb200ic64oc31,0.0310059
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic15680oc512,28.6072
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic512oc128,0.194092
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic128oc64,0.0458984
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32:p:blocked:AB16b16a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user ,,mb200ic64oc31,0.0300293
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic15680oc512,28.6541
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic512oc128,0.192139
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic128oc64,0.0480957
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32:p:blocked:AB16b16a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user ,,mb200ic64oc31,0.0332031
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic15680oc512,27.8789
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic512oc128,0.193848
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic128oc64,0.0480957
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32:p:blocked:AB16b16a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user ,,mb200ic64oc31,0.0319824
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic15680oc512,27.937
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic512oc128,0.191895
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic128oc64,0.0461426
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32:p:blocked:AB16b16a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user ,,mb200ic64oc31,0.032959
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic15680oc512,30.3501
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic512oc128,0.544922
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic128oc64,0.0620117
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32:p:blocked:AB16b16a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user ,,mb200ic64oc31,0.0361328
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic15680oc512,30.0071
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic512oc128,0.194092
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic128oc64,0.0471191
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32:p:blocked:AB16b16a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user ,,mb200ic64oc31,0.0319824
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic15680oc512,27.877
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic512oc128,0.185059
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic128oc64,0.0471191
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32:p:blocked:AB16b16a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user ,,mb200ic64oc31,0.0319824
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic15680oc512,28.156
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic512oc128,0.162109
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic128oc64,0.0429688
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32:p:blocked:AB16b16a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user ,,mb200ic64oc31,0.0300293
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic15680oc512,28.6611
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic512oc128,0.191162
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user attr-post-ops:eltwise_relu ,,mb200ic128oc64,0.0461426
onednn_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_inference,src_f32::blocked:ab:f0 wei_f32:p:blocked:AB16b16a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,attr-scratchpad:user ,,mb200ic64oc31,0.0310059
```
</details>"
56349,Tensorflow argsort sometimes produces non-deterministic results,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.8

### Custom Code

No

### OS Platform and Distribution

Mac OS Monterey 12.3.1 with M1 Pro CPU 

### Mobile device

_No response_

### Python version

3.9.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hey there!

I am currently dealing with a very weird tensorflow problem, which I think is a bug. I am trying to develop an inference application and for that purpose I have to apply the `tf.argsort` function. 

I fetch a slice from a 2D tensor from an Embedding layer:


embedding_layer_tensor[index]
```

Which gives me a 1D tensor with shape (n,). Then I want to sort those values in descending order and return their indexes in sort order - hence I use the `tf.argsort function`.

But I sometimes get very weird results. Sometimes it works, sometimes the results just don't make any sense.

Here's an example:

```
# Below is my 1D tensor:
sample_constant = tf.constant([1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 9, 9, 9, 9, 5.171316e-32, 0.000000e+00, 0.000000e+00, 0.000000e+00, 2, 3, 4, 1.000000e+00, 0.000000e+00, 0.000000e+00, 1.000000e+00])

# Now I apply the argsort function on it:
tf.argsort(sample_constant, axis=0)
```

Sometimes the argsort function returns:
```
array([19, 13,  5, 14,  6, 12, 20,  4, 11,  3, 21,  1, 18,  2,  0, 15,  0,
        0,  0,  0,  0,  0], dtype=int32)>
```

But sometimes it also returns this which makes no sense:

```
<tf.Tensor: shape=(22,), dtype=int32, numpy=
array([          7,           9,          10,           8,          17,
                16,          15,           3,          21,           1,
                18,           2,           0,          11,          19,
                13, -1069547520, -1065353216, -1082130432, -2147483648,
       -2147483648, -1082130432], dtype=int32)>
```

Can someone help me out with this or try if you can reproduce this error? 

Thanks! :)
```


### Standalone code to reproduce the issue

```shell
sample_constant = tf.constant([1.000000e+00, 1.000000e+00, 1.000000e+00, 1.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 9, 9, 9, 9, 5.171316e-32, 0.000000e+00, 0.000000e+00, 0.000000e+00, 2, 3, 4, 1.000000e+00, 0.000000e+00, 0.000000e+00, 1.000000e+00])

# Needs to be performed many times:
for i in range(0,10):
   tf.argsort(sample_constant, axis=0)
```


### Relevant log output

_No response_</details>"
56348,tensorflow has no gfile,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
56346,"Locating the numpy include path can ""fail"" incorrectly","### Description

[`_get_numpy_include()`](https://github.com/tensorflow/tensorflow/blob/d7e521a1ad21681855b439b9c2a05837c804e488/third_party/py/python_configure.bzl#L188-L201) will fail if importing `numpy` causes _anything_ to be written to stderr, even if the command succeeds. I found this while helping a user in #python on the Libera.chat IRC network debug [a failure](https://github.com/google/yggdrasil-decision-forests/issues/19) when building `yggdrasil-decision-forests`. When they import `numpy` on their system, they get the following non-fatal warning:

```
OpenBLAS WARNING - could not determine the L2 cache size on this system, assuming 256k
```

Which causes `execute()` to fail. It looks like the `allow_failure` parameter for that function is meant to address this, so adding this argument should fix the issue in this specific case, but I'm not sure what the other implications might be. (as an aside, this parameter is confusingly documented, I have opened #56345 to address that)

Insight from someone who is more familiar with TF's build system would be appreciated, but inspecting stdout/stderr strikes me as a very strange way to work out whether or not the executed process failed. Is there a reason this bit of code does not check the [`return_code`](https://bazel.build/rules/lib/exec_result) of the raw execution?"
56343,How to use tf.data API with multiple inputs of different shapes,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


I am trying to make a tensorflow model with two different inputs, one will have shape [9,10], the other will just have shape [8].

I am furthermore trying to use tf.dataset to iterate over my inputs. However, whenever I try to do so it fails with the following error:

```
tensorflow.python.framework.errors_impl.InvalidArgumentError: Shapes of all inputs must match: values[0].shape = [9,10] != values[1].shape = [8]
     [[{{node packed}}]] [Op:IteratorGetNext]
```

But surely it is possible to have differently shaped inputs into different branches! This is exactly the case in the example in tensorflow's functional API guide, however they do not use tf.dataset so I can't simply follow their example.

To give a little more specifics into the problem I am trying to solve and why I am using the tf.dataset api:

I am doing a time-series problem over multiple sites where my inputs are of two types: those that vary with time, and those that do not but do vary by site. For the time being, I'm just trying to estimate the next time step.

First, I get my dynamic covariates and targets in a sliding window using the timeseries_dataset_from_array util.

```
    train_ds = tf.keras.preprocessing.timeseries_dataset_from_array(
        input_data, targets, sequence_length=window_size, batch_size=256)
```

This works perfectly and I can train models using this dataset as is.

However, I want to also use the static covariates from the specific site that the time series data is coming from. The site id is included in the window input data in its own column, though it gets removed before training. Thus, what I am trying to do is grab the static covariates for the site and attach it as a separate input to my dataset.

```
    train_ds = train_ds.map(lambda x, y: (tf.py_function(attach_static_covariates, [x, idindex, colnames], [tf.float64, tf.float64]), y))

    train_ds = train_ds.map(lambda x, y: ({'dynamic': x[0], 'static': x[1]}, y))

```
The code for the attach_static_covariates method is:

```
def attach_static_covariates(x, idindex, colnames):
    id = x[0, idindex].numpy()
    static_cov = static_df.iloc[int(id)]
    #This just filters out the id column, now that it has served its purpose
    x = tf.gather(x, [i for i in range(len(colnames)) if i != idindex])

    return (x, static_cov)
```

I've confirmed that my code can run and train on multiple inputs provided by the above method provided they are the same size (e.g. if I return (x, x) I can run my model on two copies of the dynamic covariates inputted into two different branches of my model). The problem is not due to a mismatch or a bad model definition because I get the same error from the following code:

```
    for feature_batch, label_batch in train_ds.take(1):
        print(feature_batch)
```

I've looked everywhere on google and the tensorflow git and I can't find anyone else with this problem, except for https://github.com/tensorflow/tensorflow/issues/48003, which points to ragged tensors which I don't believe is what I am looking for.  The ragged tensor use case seems to be for variable length entries, but I'm just trying to input two different, but fixed, shapes. And yet it surely MUST be possible to have differently shaped inputs using tf.dataset! I can't imagine that such an incredibly common use case would be completely unsupported. However, I can't find any examples online where someone has multiple inputs of different shapes and uses tf.dataset api. Any help or links to such examples would be greatly appreciated.
```

EDIT: I did find a workaround, which is in the colab, where you embed the py_function step into another mapped function returning the dictionaries, so that no step ever produces a dataset with two different shaped inputs instead of a dict.  Still, I'd be interested in knowing if there is some other canonical way to get past this.


### Standalone code to reproduce the issue


https://colab.research.google.com/drive/1EnaJoUULl-fyAwlcG_5tcWfsRFVCOMtv#scrollTo=PHvsIOx6-Uux
```


### Relevant log output

_No response_</details>"
56341,Incorrect gradient of model output with respect to inputs using GradientTape,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.1.3

### Custom Code

No

### OS Platform and Distribution

Windows 10 Home

### Mobile device

_No response_

### Python version

3.7

### Bazel version

n/a

### GCC/Compiler version

n/a

### CUDA/cuDNN version

CPU

### GPU model and memory

n/a

### Current Behaviour?

```shell
I am trying to take the gradient/jacobian of my model output with respect to the inputs to study the derivatives of my parameter space. This issue has come up several times in the past but I do not seem to be able to reproduce their results or solutions, if any were suggested. It seems that I am able to apply gradients a single variable output, single variable input model, however my problem is multivariate with respect to both. I am using an older version of TF for work purposes but the issue was the same for TF 2.6 as well.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from numpy import *
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

x1 = linspace(0,4*pi,100000)
x2 = linspace(2,20,100000)
y = cos(x1) + x2/5

model = Sequential()
model.add(Dense(100, activation='elu', input_shape=(2,)))  # elu used so that smooth, continuous functions are differentiated
model.add(Dense(100, activation='elu'))
model.add(Dense(50, activation='elu'))
model.add(Dense(50, activation='elu'))
model.add(Dense(10, activation='elu'))
model.add(Dense(10, activation='elu'))
model.add(Dense(1, activation='linear'))
model.compile(loss='MSE', optimizer='adam', metrics=['accuracy'])

x_in = vstack((x1,x2)).T
y_out = y

x_train, x_valid, y_train, y_valid = train_test_split(x_in, y_out, test_size=0.3, shuffle= True)

epochs = 100
batch_size = 110

h0 = model.fit(x_train, y_train, validation_data=(x_valid,y_valid), batch_size=batch_size,epochs=epochs)

grado1 = zeros((100000,2))
grado2 = zeros((100000,2))

for i in range(0,100000):
    x = tf.Variable([[x_in[i,0],x_in[i,1]]])
    
    with tf.GradientTape(persistent=True,watch_accessed_variables=True) as t:
        t.watch(x)  
        out1 = model(x)
        out2 = tf.math.cos(x[:,0]) + x[:,1]/5  # this works using both a Variable array as well as separated taped variables, i.e., x1t, x2t
    gradients1 = t.gradient(out1, x)
    gradients2 = t.gradient(out2, x)
    if i % 100 == 0:
        print(i)
    grado1[i,:] = gradients1.numpy()
    grado2[i,:] = gradients2.numpy()


ymodel = model.predict([x_in])

for i in range(2):
    plt.figure(figsize=(15,4))
    ax = plt.subplot(111)
    ax.plot(x_in[:,i],y_out,label='data',linewidth=5)
    ax.plot(x_in[:,i],ymodel,'--',label='model',linewidth=5)
    plt.ylabel('y')
    plt.xlabel('x'+str(i+1))
    plt.legend(loc='lower left')
    
    ax2 = ax.twinx()
    ax2.plot(x_in[:,i],grado1[:,i],'b--',label='grad1_x'+str(i+1)+' model')
    ax2.plot(x_in[:,i],grado2[:,i],'g',label='grad2_x'+str(i+1)+' autodiff formula')
    if i == 0:
        ax2.plot(x_in[:,0],-sin(x_in[:,0]),'r--',label='grad2_x'+str(i+1)+' explicit formula')
    elif i == 1:
        ax2.plot(x_in[:,1],1/5*ones(len(x_in[:,1])),'r--',label='grad2_x'+str(i+1)+' explicit formula')
    plt.ylabel('dy/dx'+str(i+1))
    plt.xlabel('x'+str(i+1))

    plt.legend()
```


### Relevant log output

_No response_</details>"
56337,Character-level seq2seq model for translation and beam search. ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Feature Request

### Source

source

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

Colab GPU

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I was trying to implement seq2seq translation model at character level along with beam search by referring the tensorflow documentation.

https://colab.research.google.com/github/tensorflow/addons/blob/master/docs/tutorials/networks_seq2seq_nmt.ipynb

For this, I changed only one parameter -- 'char_level = True' in tf.keras tokenizer. There was no issue during model training, but I'm getting error for inferences.
```


### Standalone code to reproduce the issue

```shell
# Step 3 and Step 4
    def tokenize(self, lang):
        # lang = list of sentences in a language
        
        # print(len(lang), ""example sentence: {}"".format(lang[0]))
        lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>', char_level = True)
        lang_tokenizer.fit_on_texts(lang)

        ## tf.keras.preprocessing.text.Tokenizer.texts_to_sequences converts string (w1, w2, w3, ......, wn) 
        ## to a list of correspoding integer ids of words (id_w1, id_w2, id_w3, ...., id_wn)
        tensor = lang_tokenizer.texts_to_sequences(lang) 

        ## tf.keras.preprocessing.sequence.pad_sequences takes argument a list of integer id sequences 
        ## and pads the sequences to match the longest sequences in the given input
        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')

        return tensor, lang_tokenizer
```


### Relevant log output

```shell
translate(u'hace mucho frio aqui.')

---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-19-a7e085e16f9f> in <module>()
----> 1 translate(u'hace mucho frio aqui.')

2 frames
<ipython-input-17-d0c0d138384e> in <listcomp>(.0)
      2   sentence = dataset_creator.preprocess_sentence(sentence)
      3 
----> 4   inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]
      5   inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],
      6                                                           maxlen=max_length_input,

KeyError: '<start>'
```
</details>"
56335,ZeroDivisionError on GradientTape.gradient when computing a loss from concatting scalar values,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.8.0

### Custom Code

Yes

### OS Platform and Distribution

18.04.1-Ubuntu

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

-

### GCC/Compiler version

-

### CUDA/cuDNN version

cuda11.3/cudnn8.2.1

### GPU model and memory

Tesla V100

### Current Behaviour?

```shell
a ZeroDivisionError on GradientTape.gradient when computing a loss from concatting scalar values using for example loss=tf.math.reduce_mean(tf.concat([self.trainable_var]*10,0))
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

class CustomModel(tf.keras.models.Model):
    def __init__(self):
        super(CustomModel, self).__init__()
        self.var = tf.Variable(initial_value=0.1, dtype=tf.float32, trainable=True)

    def loss(self):
        return tf.math.reduce_mean(tf.concat([self.var]*10,0))


model = CustomModel()

with tf.GradientTape() as gt:
    loss = model.loss()
    gt.gradient(loss, model.trainable_variables)
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""tensorflow_crash.py.py"", line 18, in <module>
    gt.gradient(loss, model.trainable_variables)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/backprop.py"", line 1081, in gradient
    flat_grad = imperative_grad.imperative_grad(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/imperative_grad.py"", line 67, in imperative_grad
    return pywrap_tfe.TFE_Py_TapeGradient(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/backprop.py"", line 156, in _gradient_function
    return grad_fn(mock_op, *out_grads)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/array_grad.py"", line 228, in _ConcatGradV2
    return _ConcatGradHelper(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/array_grad.py"", line 116, in _ConcatGradHelper
    concat_dim._numpy().item(0) % input_values[0]._rank())  # pylint: disable=protected-access
ZeroDivisionError: integer division or modulo by zero
```
</details>"
56333,`tf.image.resize_with_pad` raises `InvalidArgumentError` but the behavior is not documented,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Source

binary

### Tensorflow Version

v2.9.0-18-gd8ce9f9c301 2.9.1

### Custom Code

No

### OS Platform and Distribution

macOS 12.4

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The documentation only states `tf.image.resize_with_pad` raises `ValueError`.
https://www.tensorflow.org/api_docs/python/tf/image/resize_with_pad#raises

The function actually raises `InvalidArgumentError` if the resized image becomes too small (zero width or zero height). The function should raise a `ValueError` corresponding to this situation, or at least document that the function raises `InvalidArgumentError`.

The same applies `tf.image.resize` with `preserve_aspect_ratio=True`.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.image.resize_with_pad(tf.ones((1, 100, 1)), target_height=10, target_width=10)
```
```


### Relevant log output

```shell
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/var/folders/xx/0bq30ljn7dz9055_2f5kd2csqm1fwc/T/ipykernel_41817/1900687440.py in <module>
----> 1 tf.image.resize_with_pad(tf.ones((1, 100, 1)), target_height=10, target_width=10)

~/projects/ailab-psd-processing/.venv/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs)
    151     except Exception as e:
    152       filtered_tb = _process_traceback_frames(e.__traceback__)
--> 153       raise e.with_traceback(filtered_tb) from None
    154     finally:
    155       del filtered_tb

~/projects/ailab-psd-processing/.venv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   7162 def raise_from_not_ok_status(e, name):
   7163   e.message += ("" name: "" + name if name is not None else """")
-> 7164   raise core._status_to_exception(e) from None  # pylint: disable=protected-access
   7165 
   7166 

InvalidArgumentError: output dimensions must be positive [Op:ResizeBilinear]
```
```
</details>"
56332,Is it possible to change colour of specific keypoint in iOS,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

pod 'TensorFlowLiteSwift', '~> 0.0.1-nightly', :subspecs => ['CoreML', 'Metal']

### Custom Code

Yes

### OS Platform and Distribution

MacOS

### Mobile device

iPhone X

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I want to change colour of specific body landmarks. Is it possible to achieve in iOS using Swift
```


### Standalone code to reproduce the issue

```shell
Multicolor on overlay image
```


### Relevant log output

_No response_</details>"
56331,Can Tensorflow provide a hardware training/inference standard for hardware accelerator developers?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

any

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
It is difficult to compile and run common machine learning libraries on custom SIMD accelerators.
```


### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

_No response_</details>"
56330,Unix wheel URLs are incorrect in TF2.9,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Source

binary

### Tensorflow Version

2.9

### Custom Code

No

### OS Platform and Distribution

Ubuntu

### Mobile device

NA

### Python version

3.7, 3.8, 3.9, 3.10

### Bazel version

NA

### GCC/Compiler version

NA

### CUDA/cuDNN version

NA

### GPU model and memory

NA

### Current Behaviour?

```shell
Unix wheel URLs are incorrect in TF2.9 at https://www.tensorflow.org/install/pip#package_location
```


### Standalone code to reproduce the issue

```shell
Linux
Python 3.7 GPU support	https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-2.9.0-cp37-cp37m-manylinux2014.whl
Python 3.7 CPU-only	https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow_cpu-2.9.0-cp37-cp37m-manylinux2014.whl
Python 3.8 GPU support	https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-2.9.0-cp38-cp38-manylinux2014.whl
Python 3.8 CPU-only	https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow_cpu-2.9.0-cp38-cp38-manylinux2014.whl
Python 3.9 GPU support	https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-2.9.0-cp39-cp39-manylinux2014.whl
Python 3.9 CPU-only	https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow_cpu-2.9.0-cp39-cp39-manylinux2014.whl
Python 3.10 GPU support	https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-2.9.0-cp310-cp310-manylinux2014.whl
Python 3.10 CPU-only	https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow_cpu-2.9.0-cp310-cp310-manylinux2014.whl
```


### Relevant log output

```shell
<Error>
<Code>NoSuchKey</Code>
<Message>The specified key does not exist.</Message>
<Details>No such object: tensorflow/linux/gpu/tensorflow_gpu-2.9.0-cp37-cp37m-manylinux2014.whl</Details>
</Error>
```
```
</details>"
56329,Generate SSD anchor box aspect ratios using k-means clustering - Tutorial,"Does anyone has experience with it? Is it working for TensorFlow 1 or only for 2. Is the point ""Upload your own dataset"" the point where I'm starting now?

https://colab.research.google.com/github/tensorflow/models/blob/master/research/object_detection/colab_tutorials/generate_ssd_anchor_box_aspect_ratios_using_k_means_clustering.ipynb"
56328,clEnqueueNDRangeKernel failed on TfLiteGpuDelegate for split operation,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

Galaxy S10 (w/ Mali G76)

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When executing TensorFlow Lite benchmark [binary](https://www.tensorflow.org/lite/performance/measurement#native_benchmark_binary) (built from tf 2.9.1 source) with TfLiteGpuDelegate for certain models on Galaxy S10 (with Mali G76 GPU), I got the following error:
> ERROR: TfLiteGpuDelegate Invoke: Failed to clEnqueueNDRangeKernel - Out of resources
> ERROR: Node number 44 (TfLiteGpuDelegateV2) failed to invoke.
> count=1 curr=238
> Benchmarking failed.

I found that [clEnqueueNDRangeKernel](https://github.com/tensorflow/tensorflow/blob/d8ce9f9c301d021a69953134185ab728c1c248d3/tensorflow/lite/delegates/gpu/cl/cl_command_queue.cc#L72) returns CL_OUT_OF_RESOURCES only for **split** operation (but working well for other operations). 

For example, the sixth operation of [resnext14_16x4d.tflite](https://github.com/165749/tflite-models/raw/main/models/resnext14_16x4d.tflite) is a split layer (which splits a tensor in shape [1, 58, 58, 64] into 16 tensors in shape [1, 58, 58, 4]). The clEnqueueNDRangeKernel failed for this operation, and I observed 
work_groups_count [2, 5, 1] and work_group_size [29, 13, 1] (which is under the limit of CL_DEVICE_MAX_WORK_GROUP_SIZE 384).  

I have uploaded a bunch of tflite models with the same issue [here](https://github.com/165749/tflite-models/tree/main/models); they are actually executed smoothly on Pixel 4 (Adreno) and Galaxy A03s (PowerVR). I havent confirmed on other devices with Mali GPUs.

I appreciate it if any insights can be provided!


### Standalone code to reproduce the issue

```shell
/data/local/tmp/benchmark_model --warmup_runs=1 --num_runs=1 --use_gpu=true --graph=/data/local/tmp/resnext14_16x4d.tflite
```


### Relevant log output

The OpenCL information of the Galaxy S10 (Android 12, Mali G76) is attached:

```shell

CL_PLATFORM_NAME: ARM Platform
CL_PLATFORM_VENDOR: ARM
CL_PLATFORM_VERSION: OpenCL 3.0 v1.r32p1-01bet2-mbs2v39_0.131801e953429f661ecce1d5e1d2b3ef
CL_PLATFORM_PROFILE: FULL_PROFILE
CL_PLATFORM_EXTENSIONS: cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_byte_addressable_store cl_khr_3d_image_writes cl_khr_int64_base_atomics cl_khr_int64_extended_atomics cl_khr_fp16 cl_khr_icd cl_khr_egl_image cl_khr_image2d_from_buffer cl_khr_depth_images cl_khr_subgroups cl_khr_subgroup_extended_types cl_khr_subgroup_non_uniform_vote cl_khr_subgroup_ballot cl_khr_il_program cl_khr_priority_hints cl_khr_create_command_queue cl_khr_spirv_no_integer_wrap_decoration cl_khr_extended_versioning cl_khr_device_uuid cl_arm_core_id cl_arm_printf cl_arm_non_uniform_work_group_size cl_arm_import_memory cl_arm_import_memory_dma_buf cl_arm_import_memory_host cl_arm_import_memory_android_hardware_buffer cl_arm_integer_dot_product_int8 cl_arm_job_slot_selection cl_arm_scheduling_controls cl_arm_controlled_kernel_termination cl_ext_cxx_for_opencl
Number of devices available: 1 
CL_DEVICE_NAME: Mali-G76 r0p0
CL_DEVICE_TYPE: CL_DEVICE_TYPE_GPU
CL_DEVICE_VENDOR: ARM
CL_DEVICE_VERSION: OpenCL 3.0 v1.r32p1-01bet2-mbs2v39_0.131801e953429f661ecce1d5e1d2b3ef
CL_DRIVER_VERSION: 3.0
CL_DEVICE_OPENCL_C_VERSION: OpenCL C 3.0 v1.r32p1-01bet2-mbs2v39_0.131801e953429f661ecce1d5e1d2b3ef
CL_DEVICE_MAX_CLOCK_FREQUENCY: 5 MHz
CL_DEVICE_MAX_COMPUTE_UNITS: 12
CL_DEVICE_GLOBAL_MEM_SIZE: 6969 MB
CL_DEVICE_GLOBAL_MEM_CACHE_SIZE: 1024 KB
CL_DEVICE_GLOBAL_MEM_CACHE_TYPE: CL_READ_WRITE_CACHE
CL_DEVICE_GLOBAL_MEM_CACHELINE_SIZE: 64
CL_DEVICE_MAX_MEM_ALLOC_SIZE: 6969 MB
CL_DEVICE_LOCAL_MEM_SIZE: 32 KB
CL_DEVICE_LOCAL_MEM_TYPE: CL_GLOBAL
CL_DEVICE_MAX_CONSTANT_ARGS: 128
CL_DEVICE_MAX_CONSTANT_BUFFER_SIZE: 7137008 KB
CL_DEVICE_MAX_WORK_GROUP_SIZE: 384
CL_DEVICE_MAX_WORK_ITEM_DIMENSIONS: 3
CL_DEVICE_MAX_WORK_ITEM_SIZES 0: 384
CL_DEVICE_MAX_WORK_ITEM_SIZES 1: 384
CL_DEVICE_MAX_WORK_ITEM_SIZES 2: 384
CL_DEVICE_PROFILE: FULL_PROFILE
CL_DEVICE_PROFILING_TIMER_RESOLUTION: 1000 ns
CL_DEVICE_QUEUE_PROPERTIES: CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE
CL_DEVICE_QUEUE_PROPERTIES: CL_QUEUE_PROFILING_ENABLE
CL_DEVICE_IMAGE_SUPPORT: 1
CL_DEVICE_IMAGE2D_MAX_WIDTH: 65536
CL_DEVICE_IMAGE2D_MAX_HEIGHT: 65536
CL_DEVICE_IMAGE3D_MAX_WIDTH: 65536
CL_DEVICE_IMAGE3D_MAX_HEIGHT: 65536
CL_DEVICE_IMAGE3D_MAX_DEPTH: 65536
CL_DEVICE_MAX_READ_IMAGE_ARGS: 128
CL_DEVICE_MAX_WRITE_IMAGE_ARGS: 64
CL_DEVICE_MAX_SAMPLERS: 16
```
</details>"
56327,Character-level seq2seq model for translation and beam search.,"I was trying to implement seq2seq translation model at character level along with beam search by referring the tensorflow documentation.


[[https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt](https://github.com/tensorflow/addons/issues/url)](url)

For this, I tried to change parameter, 'char_level = True' in tf.keras tokenizer. There was no issue during model training, but I'm getting error for inferences. 

Can someone please help me to solve this issue.

Thank you in advance

![image](https://user-images.githubusercontent.com/25157856/171536625-f0bf3ad4-e51a-40db-a6ae-3e825f95c875.png)
"
56326,"`model.fit(..., verbose=0)` still prints stuff","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Windows 10 Build 17763

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I expected to see nothing when calling `model.fit(..., verbose=0)`
```


### Standalone code to reproduce the issue

```python
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
from tensorflow import keras
from keras.layers import InputLayer, Dense

data = [
	[ [0,0], [0] ],
	[ [0,1], [1] ],
	[ [1,0], [1] ],
	[ [1,1], [0] ],
]
x_train = [i[0] for i in data]
y_train = [i[1] for i in data]

model = keras.Sequential([
	InputLayer(2),
	Dense(units=8, activation='sigmoid'),
	Dense(units=1, activation='sigmoid'),
])
model.compile(optimizer=keras.optimizers.SGD(0.6), loss='mean_squared_error')

model.fit(x_train, y_train, batch_size=1, epochs=500, verbose=0)

print(
	model.predict(  [ [0,0], [0,1], [1,0], [1,1] ]  )
)
```
![Untitled](https://user-images.githubusercontent.com/16363767/171522020-7946b036-376b-4ca0-8a55-5150263b5a76.png)


### Relevant log output

_No response_</details>"
56325,Linking to static libtensorflow-lite.a gives linking errors on Debian x64,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Debian 11 x64

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

g++ (Debian 10.2.1-6) 10.2.1 20210110

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I build the static tf lite library following documentation here perfectly: https://www.tensorflow.org/lite/guide/build_cmake


sudo apt-get install cmake
git clone https://github.com/tensorflow/tensorflow.git tensorflow_src
mkdir tflite_build
cd tflite_build
cmake ../tensorflow_src/tensorflow/lite
cmake --build . -j


I see that the folder contains a resulting libtensorflow-lite.a indicating success

I then link my existing application to this library and build it. The result is a disappointing list of link errors (see log below)
```


### Standalone code to reproduce the issue

```shell
There is no code, just linking errors.
```


### Relevant log output

```shell
g++ -Wl,--allow-multiple-definition -Wl,--whole-archive -Wl,-rpath,/home/leo/Qt/6.3.0/gcc_64/lib -Wl,-rpath-link,/home/leo/Qt/6.3.0/gcc_64/lib -o Karaoke   main.o [....]   -L/home/leo/Desktop/Projects/Singularity/libs/tflite_build -ltensorflow-lite -ldl /home/leo/Qt/6.3.0/gcc_64/lib/libQt6MultimediaWidgets.so /home/leo/Qt/6.3.0/gcc_64/lib/libQt6Multimedia.so -pthread /home/leo/Qt/6.3.0/gcc_64/lib/libQt6OpenGLWidgets.so /home/leo/Qt/6.3.0/gcc_64/lib/libQt6Widgets.so /home/leo/Qt/6.3.0/gcc_64/lib/libQt6OpenGL.so /home/leo/Qt/6.3.0/gcc_64/lib/libQt6Gui.so /home/leo/Qt/6.3.0/gcc_64/lib/libQt6Network.so /home/leo/Qt/6.3.0/gcc_64/lib/libQt6Sql.so /home/leo/Qt/6.3.0/gcc_64/lib/libQt6Core.so -lpthread -lGL   
/usr/bin/ld: /home/leo/Desktop/Projects/Singularity/libs/tflite_build/libtensorflow-lite.a(serialization.cc.o): in function `tflite::delegates::StrFingerprint[abi:cxx11](void const*, unsigned long)':
serialization.cc:(.text+0x7c1): undefined reference to `util::Fingerprint64(char const*, unsigned long)'
/usr/bin/ld: /home/leo/Desktop/Projects/Singularity/libs/tflite_build/libtensorflow-lite.a(serialization.cc.o): in function `tflite::delegates::Serialization::GetEntryImpl(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, TfLiteContext*, TfLiteDelegateParams const*)':
serialization.cc:(.text+0x1760): undefined reference to `util::Fingerprint64(char const*, unsigned long)'
/usr/bin/ld: serialization.cc:(.text+0x1770): undefined reference to `util::Fingerprint64(char const*, unsigned long)'
/usr/bin/ld: serialization.cc:(.text+0x18a4): undefined reference to `util::Fingerprint64(char const*, unsigned long)'
/usr/bin/ld: serialization.cc:(.text+0x1ab7): undefined reference to `util::Fingerprint64(char const*, unsigned long)'
/usr/bin/ld: /home/leo/Desktop/Projects/Singularity/libs/tflite_build/libtensorflow-lite.a(xnnpack_delegate.cc.o): in function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitReluNode(xnn_subgraph*, tflite::xnnpack::(anonymous namespace)::Delegate const&, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, float, float, std::vector<unsigned int, std::allocator<unsigned int> > const&) [clone .constprop.0]':
xnnpack_delegate.cc:(.text+0x104c): undefined reference to `xnn_define_clamp'
/usr/bin/ld: /home/leo/Desktop/Projects/Singularity/libs/tflite_build/libtensorflow-lite.a(xnnpack_delegate.cc.o): in function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitConcatenationNode(xnn_subgraph*, tflite::xnnpack::(anonymous namespace)::Delegate const&, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, TfLiteConcatenationParams const*, std::vector<unsigned int, std::allocator<unsigned int> > const&)':
xnnpack_delegate.cc:(.text+0x1594): undefined reference to `xnn_define_concatenate4'
/usr/bin/ld: xnnpack_delegate.cc:(.text+0x15d7): undefined reference to `xnn_define_concatenate2'
/usr/bin/ld: xnnpack_delegate.cc:(.text+0x15f1): undefined reference to `xnn_define_concatenate3'
/usr/bin/ld: /home/leo/Desktop/Projects/Singularity/libs/tflite_build/libtensorflow-lite.a(xnnpack_delegate.cc.o): in function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitMaxPool2DNode(xnn_subgraph*, tflite::xnnpack::(anonymous namespace)::Delegate const&, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, TfLitePoolParams const*, std::vector<unsigned int, std::allocator<unsigned int> > const&)':
xnnpack_delegate.cc:(.text+0x1a58): undefined reference to `xnn_define_max_pooling_2d'
/usr/bin/ld: xnnpack_delegate.cc:(.text+0x1c8e): undefined reference to `xnn_define_clamp'
/usr/bin/ld: /home/leo/Desktop/Projects/Singularity/libs/tflite_build/libtensorflow-lite.a(xnnpack_delegate.cc.o): in function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitMediaPipeUnpoolingNode(xnn_subgraph*, tflite::xnnpack::(anonymous namespace)::Delegate const&, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, TfLitePoolParams const*, std::vector<unsigned int, std::allocator<unsigned int> > const&) [clone .constprop.0]':
xnnpack_delegate.cc:(.text+0x2242): undefined reference to `xnn_define_unpooling_2d'
/usr/bin/ld: /home/leo/Desktop/Projects/Singularity/libs/tflite_build/libtensorflow-lite.a(xnnpack_delegate.cc.o): in function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitMediaPipeMaxPoolingNode(xnn_subgraph*, tflite::xnnpack::(anonymous namespace)::Delegate const&, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, TfLitePoolParams const*, std::vector<unsigned int, std::allocator<unsigned int> > const&) [clone .constprop.0]':
xnnpack_delegate.cc:(.text+0x2940): undefined reference to `xnn_define_argmax_pooling_2d'
/usr/bin/ld: /home/leo/Desktop/Projects/Singularity/libs/tflite_build/libtensorflow-lite.a(xnnpack_delegate.cc.o): in function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitReshapeNode(xnn_subgraph*, tflite::xnnpack::(anonymous namespace)::Delegate const&, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, TfLiteReshapeParams const*, std::vector<unsigned int, std::allocator<unsigned int> > const&) [clone .constprop.0]':
xnnpack_delegate.cc:(.text+0x2ded): undefined reference to `xnn_define_static_reshape'
/usr/bin/ld: /home/leo/Desktop/Projects/Singularity/libs/tflite_build/libtensorflow-lite.a(xnnpack_delegate.cc.o): in function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitResizeBilinearNode(xnn_subgraph*, tflite::xnnpack::(anonymous namespace)::Delegate const&, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, TfLiteResizeBilinearParams const*, std::vector<unsigned int, std::allocator<unsigned int> > const&)':
xnnpack_delegate.cc:(.text+0x3635): undefined reference to `xnn_define_static_resize_bilinear_2d'
/usr/bin/ld: /home/leo/Desktop/Projects/Singularity/libs/tflite_build/libtensorflow-lite.a(xnnpack_delegate.cc.o): in function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitPadNode(xnn_subgraph*, tflite::xnnpack::(anonymous namespace)::Delegate const&, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, std::vector<unsigned int, std::allocator<unsigned int> > const&)':
xnnpack_delegate.cc:(.text+0x3e5a): undefined reference to `xnn_define_static_constant_pad'
/usr/bin/ld: /home/leo/Desktop/Projects/Singularity/libs/tflite_build/libtensorflow-lite.a(xnnpack_delegate.cc.o): in function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitPreluNode(xnn_subgraph*, tflite::xnnpack::(anonymous namespace)::Delegate const&, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, std::unordered_set<int, std::hash<int>, std::equal_to<int>, std::allocator<int> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&) [clone .constprop.0]':
xnnpack_delegate.cc:(.text+0x433c): undefined reference to `xnn_define_prelu'
/usr/bin/ld: /home/leo/Desktop/Projects/Singularity/libs/tflite_build/libtensorflow-lite.a(xnnpack_delegate.cc.o): in function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitSplitNode(xnn_subgraph*, tflite::xnnpack::(anonymous namespace)::Delegate const&, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, TfLiteSplitParams const*, std::vector<unsigned int, std::allocator<unsigned int> > const&) [clone .isra.0]':
xnnpack_delegate.cc:(.text+0x4a88): undefined reference to `xnn_define_even_split2'
/usr/bin/ld: xnnpack_delegate.cc:(.text+0x4ace): undefined reference to `xnn_define_even_split4'
/usr/bin/ld: xnnpack_delegate.cc:(.text+0x4b01): undefined reference to `xnn_define_even_split3'
/usr/bin/ld: /home/leo/Desktop/Projects/Singularity/libs/tflite_build/libtensorflow-lite.a(xnnpack_delegate.cc.o): in function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitFullyConnectedNode(xnn_subgraph*, tflite::xnnpack::(anonymous namespace)::Delegate const&, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, TfLiteFullyConnectedParams const*, std::unordered_set<int, std::hash<int>, std::equal_to<int>, std::allocator<int> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&)':
xnnpack_delegate.cc:(.text+0x4cac): undefined reference to `xnn_define_fully_connected'
/usr/bin/ld: /home/leo/Desktop/Projects/Singularity/libs/tflite_build/libtensorflow-lite.a(xnnpack_delegate.cc.o): in function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitTransposeConvNode(xnn_subgraph*, tflite::xnnpack::(anonymous namespace)::Delegate const&, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, TfLiteTransposeConvParams const*, std::unordered_set<int, std::hash<int>, std::equal_to<int>, std::allocator<int> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&)':
xnnpack_delegate.cc:(.text+0x5f87): undefined reference to `xnn_define_deconvolution_2d'
/usr/bin/ld: /home/leo/Desktop/Projects/Singularity/libs/tflite_build/libtensorflow-lite.a(xnnpack_delegate.cc.o): in function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitMediaPipeDeconvolutionNode(xnn_subgraph*, tflite::xnnpack::(anonymous namespace)::Delegate const&, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, TfLiteTransposeConvParams const*, std::unordered_set<int, std::hash<int>, std::equal_to<int>, std::allocator<int> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&) [clone .constprop.0]':
xnnpack_delegate.cc:(.text+0x67ed): undefined reference to `xnn_define_deconvolution_2d'
/usr/bin/ld: /home/leo/Desktop/Projects/Singularity/libs/tflite_build/libtensorflow-lite.a(xnnpack_delegate.cc.o): in function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitMeanNode(xnn_subgraph*, tflite::xnnpack::(anonymous namespace)::Delegate const&, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, TfLiteReducerParams const*, std::vector<unsigned int, std::allocator<unsigned int> > const&)':
xnnpack_delegate.cc:(.text+0x6fa1): undefined reference to `xnn_define_global_average_pooling_2d'
/usr/bin/ld: /home/leo/Desktop/Projects/Singularity/libs/tflite_build/libtensorflow-lite.a(xnnpack_delegate.cc.o): in function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitConv2DNode(xnn_subgraph*, tflite::xnnpack::(anonymous namespace)::Delegate const&, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, TfLiteConvParams const*, std::unordered_set<int, std::hash<int>, std::equal_to<int>, std::allocator<int> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&)':
xnnpack_delegate.cc:(.text+0x7b13): undefined reference to `xnn_define_convolution_2d'
/usr/bin/ld: /home/leo/Desktop/Projects/Singularity/libs/tflite_build/libtensorflow-lite.a(xnnpack_delegate.cc.o): in function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitDepthwiseConv2DNode(xnn_subgraph*, tflite::xnnpack::(anonymous namespace)::Delegate const&, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, TfLiteDepthwiseConvParams const*, std::unordered_set<int, std::hash<int>, std::equal_to<int>, std::allocator<int> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&)':
xnnpack_delegate.cc:(.text+0x866d): undefined reference to `xnn_define_depthwise_convolution_2d'
/usr/bin/ld: /home/leo/Desktop/Projects/Singularity/libs/tflite_build/libtensorflow-lite.a(xnnpack_delegate.cc.o): in function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitNode(xnn_subgraph*, tflite::xnnpack::(anonymous namespace)::Delegate const&, TfLiteContext*, TfLiteRegistration*, TfLiteNode*, int, std::unordered_set<int, std::hash<int>, std::equal_to<int>, std::allocator<int> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&)':
xnnpack_delegate.cc:(.text+0x88ec): undefined reference to `xnn_define_hardswish'
/usr/bin/ld: xnnpack_delegate.cc:(.text+0x89e0): undefined reference to `xnn_define_bankers_rounding'
/usr/bin/ld: xnnpack_delegate.cc:(.text+0x8afd): undefined reference to `xnn_define_convert'
/usr/bin/ld: xnnpack_delegate.cc:(.text+0x8c24): undefined reference to `xnn_define_elu'
/usr/bin/ld: xnnpack_delegate.cc:(.text+0x8d28): undefined reference to `xnn_define_ceiling'
/usr/bin/ld: xnnpack_delegate.cc:(.text+0x8e30): undefined reference to `xnn_define_abs'
/usr/bin/ld: xnnpack_delegate.cc:(.text+0x8f8a): undefined reference to 


Too long so truncating
```
</details>"
56324,Can't build tensorflow with cuda_clang ubuntu20,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf2.10

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04 (from devel-gpu docker)

### Mobile device

_No response_

### Python version

3.8

### Bazel version

5.1.1

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

11.2

### GPU model and memory

RTX2080Ti

### Current Behaviour?

```shell
A bug happened!When using clang as compiler, setting its path as:

build --action_env CLANG_CUDA_COMPILER_PATH=""/usr/bin/clang-14""
```
and running command
```
bazel build --config=cuda --config=opt //tensorflow/tools/pip_package:build_pip_package
```

Got the fallowing error:

```
ERROR: /mnt/Documents/eduardss/tensorflow/tensorflow/lite/python/BUILD:68:10 Middleman _middlemen/tensorflow_Slite_Spython_Stflite_Uconvert-runfiles failed: undeclared inclusion(s) in rule '@com_google_absl//absl/base:log_severity':
this rule is missing dependency declarations for the following files included by 'absl/base/log_severity.cc':
  '/usr/lib/clang/14.0.5/include/stddef.h'
  '/usr/lib/clang/14.0.5/include/__stddef_max_align_t.h'
  '/usr/lib/clang/14.0.5/include/stdarg.h'
  '/usr/lib/clang/14.0.5/include/stdint.h'
  '/usr/lib/clang/14.0.5/include/limits.h'
INFO: Elapsed time: 8.262s, Critical Path: 2.47s
INFO: 71 processes: 54 internal, 17 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
```

This seems to be problem with installing clang from apt and has been reported before without any other solution as building clang from source. When build clang-12 from source it didn't have an error, but that version does not support sm75+ architectures. 

Downloading clang was also not an option as chromium versions are not built against `nvptx` target and produces error during build with sm75+.

How to properly link clang13/14 as cuda compiler to build tensorflow?
```


### Standalone code to reproduce the issue

```shell
Building from source at master branch and in devel-gpu docker image.
Using instructions from https://apt.llvm.org/ to get clang-14.
Adding to tf_configure_bazel.rc: 
 --action_env CLANG_CUDA_COMPILER_PATH=""/usr/bin/clang-14""
```


### Relevant log output

```shell
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=0 --terminal_columns=80
INFO: Reading rc options for 'build' from /mnt/Documents/eduardss/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /mnt/Documents/eduardss/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=true
INFO: Reading rc options for 'build' from /mnt/Documents/eduardss/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --config=tensorrt --action_env TF_CUDA_VERSION=11.2 --action_env TF_CUDNN_VERSION=8 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=7.5,8.6 --action_env LD_LIBRARY_PATH=/usr/local/cuda-11.0/targets/x86_64-linux/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/include/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64/stubs:/usr/local/cuda-11.0/lib64:/usr/local/cuda-11.2/lib64 --config=cuda_clang --action_env CLANG_CUDA_COMPILER_PATH=/usr/bin/clang-14
INFO: Reading rc options for 'build' from /mnt/Documents/eduardss/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /mnt/Documents/eduardss/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /mnt/Documents/eduardss/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:tensorrt in file /mnt/Documents/eduardss/tensorflow/.bazelrc: --repo_env TF_NEED_TENSORRT=1
INFO: Found applicable config definition build:cuda_clang in file /mnt/Documents/eduardss/tensorflow/.bazelrc: --config=cuda --repo_env TF_CUDA_CLANG=1 --@local_config_cuda//:cuda_compiler=clang
INFO: Found applicable config definition build:cuda in file /mnt/Documents/eduardss/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:cuda in file /mnt/Documents/eduardss/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:opt in file /mnt/Documents/eduardss/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare
INFO: Found applicable config definition build:linux in file /mnt/Documents/eduardss/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /mnt/Documents/eduardss/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/75569959891573459f33b5ff530388e15b55c5c8.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
Loading: 
Loading: 0 packages loaded
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured)
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/XNNPACK/archive/da533e0114f2bf730f17853ae10556d84a3d1e89.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
[1 / 1] checking cached actions
[32 / 3,620] Compiling src/google/protobuf/compiler/main.cc; 0s local ... (8 actions, 7 running)
[48 / 3,676] Compiling src/google/protobuf/compiler/main.cc; 2s local ... (24 actions running)
ERROR: /root/.cache/bazel/_bazel_root/f5d1fdb6f5ab96afbd54f50def1c6c1a/external/com_google_absl/absl/base/BUILD.bazel:53:11: Compiling absl/base/log_severity.cc failed: undeclared inclusion(s) in rule '@com_google_absl//absl/base:log_severity':
this rule is missing dependency declarations for the following files included by 'absl/base/log_severity.cc':
  '/usr/lib/clang/14.0.5/include/stddef.h'
  '/usr/lib/clang/14.0.5/include/__stddef_max_align_t.h'
  '/usr/lib/clang/14.0.5/include/stdarg.h'
  '/usr/lib/clang/14.0.5/include/stdint.h'
  '/usr/lib/clang/14.0.5/include/limits.h'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /mnt/Documents/eduardss/tensorflow/tensorflow/lite/python/BUILD:68:10 Middleman _middlemen/tensorflow_Slite_Spython_Stflite_Uconvert-runfiles failed: undeclared inclusion(s) in rule '@com_google_absl//absl/base:log_severity':
this rule is missing dependency declarations for the following files included by 'absl/base/log_severity.cc':
  '/usr/lib/clang/14.0.5/include/stddef.h'
  '/usr/lib/clang/14.0.5/include/__stddef_max_align_t.h'
  '/usr/lib/clang/14.0.5/include/stdarg.h'
  '/usr/lib/clang/14.0.5/include/stdint.h'
  '/usr/lib/clang/14.0.5/include/limits.h'
INFO: Elapsed time: 8.262s, Critical Path: 2.47s
INFO: 71 processes: 54 internal, 17 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
```
</details>"
56323,Can't build tflite for Java,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

macos

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I've been trying to build tflite for Java using the [build for android guide](https://www.tensorflow.org/lite/android/lite_build). When I try to run 

bazelisk build -c opt --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \
  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
  //tensorflow/lite/java:tensorflow-lite
```
I get the error
```
ERROR: tensorflow/tensorflow/lite/java/BUILD:187:28: While resolving toolchains for target //tensorflow/lite/java:tensorflowlite: no matching toolchains found for types @bazel_tools//tools/android:sdk_toolchain_type
```

Not sure what it means though a similar issu?e I saw in mediapipe claims it has to do with my ANDROID_HOME and ANDROID_NDK_HOME. Any ideas?


### Standalone code to reproduce the issue

```shell
Build command is in the current behavior section.
```


### Relevant log output

_No response_</details>"
56322,Unable to use tflite java on macos,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

MacOS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Currently TFLite on Maven only distributes .so files for JNI which are not usable on macos (as it's not a mach-o file). I am trying to run some tests on a library I'm working on outside of the android emulator, but can't run the tests due to the lack of .dylib files. Is it possible to add a configuration for mac builds or possibly add a .jar artifact to maven with .dylib files for jni?
```


### Standalone code to reproduce the issue

```shell
Built it locally and tried to run tests with JUnit.
```


### Relevant log output

_No response_</details>"
56320,Unit test atrous_conv2d_test is flaky with oneDNN on AARCH64,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

CentOS 7

### Mobile device

n/a

### Python version

3.8.13

### Bazel version

5.1.1

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

```shell
The unit test is flaky and sometimes that is enough to fail the test even with the retries.
```


### Standalone code to reproduce the issue

```shell
bazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --jobs=75 --config=nonccl --config=mkl_aarch64 --copt=-fopenmp --linkopt=-lgomp --copt=-mtune=generic --copt=-march=armv8-a --copt=-O3 --verbose_failures --test_env=TF_ENABLE_ONEDNN_OPTS=1 --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --build_tests_only -- //tensorflow/python/kernel_tests/nn_ops:atrous_conv2d_test_cpu
```


### Relevant log output

```shell
INFO: From Testing //tensorflow/python/kernel_tests/nn_ops:atrous_conv2d_test_cpu (shard 1 of 2):
==================== Test output for //tensorflow/python/kernel_tests/nn_ops:atrous_conv2d_test_cpu (shard 1 of 2):
2022-06-01 09:32:57.698421: I tensorflow/core/util/util.cc:175] Experimental oneDNN custom operations are on. If you experience issues, please turn them off by setting the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base has been moved to tensorflow.python.trackable.base. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.checkpoint_management has been moved to tensorflow.python.checkpoint.checkpoint_management. The old module will be deleted in version 2.9.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.resource has been moved to tensorflow.python.trackable.resource. The old module will be deleted in version 2.11.
Running tests under Python 3.8.13: /tmp/workspace/venv-cp38-cp38/bin/python3
[ RUN      ] AtrousConv2DTest.testAtrousConv2DForward
2022-06-01 09:32:58.982617: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled
INFO:tensorflow:time(__main__.AtrousConv2DTest.testAtrousConv2DForward): 5.91s
I0601 09:33:04.815157 281473122258016 test_util.py:2458] time(__main__.AtrousConv2DTest.testAtrousConv2DForward): 5.91s
[       OK ] AtrousConv2DTest.testAtrousConv2DForward
[ RUN      ] AtrousConv2DTest.testGradient
WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/nn_ops/atrous_conv2d_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/nn_ops/atrous_conv2d_test.py:153: compute_gradient_error (from tensorflow.python.ops.gradient_checker) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.test.compute_gradient in 2.0, which has better support for functions. Note that the two versions have different usage, so code change is needed.
W0601 09:33:04.821886 281473122258016 deprecation.py:350] From /root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/nn_ops/atrous_conv2d_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/nn_ops/atrous_conv2d_test.py:153: compute_gradient_error (from tensorflow.python.ops.gradient_checker) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.test.compute_gradient in 2.0, which has better support for functions. Note that the two versions have different usage, so code change is needed.
WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/nn_ops/atrous_conv2d_test_cpu.runfiles/org_tensorflow/tensorflow/python/ops/gradient_checker.py:390: compute_gradient (from tensorflow.python.ops.gradient_checker) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.test.compute_gradient in 2.0, which has better support for functions. Note that the two versions have different usage, so code change is needed.
W0601 09:33:04.822114 281473122258016 deprecation.py:350] From /root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/nn_ops/atrous_conv2d_test_cpu.runfiles/org_tensorflow/tensorflow/python/ops/gradient_checker.py:390: compute_gradient (from tensorflow.python.ops.gradient_checker) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.test.compute_gradient in 2.0, which has better support for functions. Note that the two versions have different usage, so code change is needed.
INFO:tensorflow:time(__main__.AtrousConv2DTest.testGradient): 0.74s
I0601 09:33:05.552801 281473122258016 test_util.py:2458] time(__main__.AtrousConv2DTest.testGradient): 0.74s
[  FAILED  ] AtrousConv2DTest.testGradient
[ RUN      ] AtrousConv2DTransposeTest.testAtrousConv2DTransposeForward
INFO:tensorflow:time(__main__.AtrousConv2DTransposeTest.testAtrousConv2DTransposeForward): 5.66s
I0601 09:33:11.212162 281473122258016 test_util.py:2458] time(__main__.AtrousConv2DTransposeTest.testAtrousConv2DTransposeForward): 5.66s
[       OK ] AtrousConv2DTransposeTest.testAtrousConv2DTransposeForward
[ RUN      ] AtrousDepthwiseConv2DTest.testAtrousDepthwiseConv2DForward
INFO:tensorflow:time(__main__.AtrousDepthwiseConv2DTest.testAtrousDepthwiseConv2DForward): 6.57s
I0601 09:33:17.780310 281473122258016 test_util.py:2458] time(__main__.AtrousDepthwiseConv2DTest.testAtrousDepthwiseConv2DForward): 6.57s
[       OK ] AtrousDepthwiseConv2DTest.testAtrousDepthwiseConv2DForward
======================================================================
FAIL: testGradient (__main__.AtrousConv2DTest)
AtrousConv2DTest.testGradient
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/nn_ops/atrous_conv2d_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1625, in decorated
    return f(self, *args, **kwargs)
  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/nn_ops/atrous_conv2d_test_cpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/nn_ops/atrous_conv2d_test.py"", line 158, in testGradient
    self.assertLess(err, err_tolerance)
AssertionError: 0.9331526160240173 not less than 0.001

----------------------------------------------------------------------
Ran 4 tests in 18.879s

FAILED (failures=1)
atrous_conv2d gradient err = 0.933153 
================================================================================
```
</details>"
56319,Dataset created from large ragged tensor doesn't copy values,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Linux

### Mobile device

_No response_

### Python version

3.9.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
<tf.Tensor: shape=(256,), dtype=float32, numpy=
array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0.], dtype=float32)>
```


### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

x = np.random.randn(425000, 32, 256).astype('float32')
l = [250] * 1700
ds = tf.data.Dataset.from_tensor_slices(tf.RaggedTensor.from_row_lengths(x, l))

display(next(iter(ds))[0][0])
```


### Relevant log output

_No response_</details>"
56318,Best image input size for SSD-Models,"Hello,

I am currently doing a student research project and am concerned with the question of which input variable would be the best to train my object detector.

**SSD models**, according to other users, have a problem detecting smaller objects (too many convolutional layers in the SSD). Therefore, I assumed a relatively good resolution like 512,720 or 800 as a square image. The SSD structure contains a resizer of **300x300 (ssd_mobilenet_v2)**, whereby one should also be careful not to get too strong defomations of the objects if the image is unfavourably chosen, e.g. 600x1024.

Questions:
**Is it important to have a large input size (more information)?**
**Does a smaller one work well, even the same as the resizer of the architecture?**
**What advantages/disadvantages do I have with larger or smaller images?
Is it a problem to train with a fixed size and then apply it to smaller or larger resolutions?
300x300 -> 512x512 or 720x720?**

I would be grateful if someone could tell me good sources to justify the decision.

P.S Enclosed is an example picture (720x720) 2cm cell_size
![image_19](https://user-images.githubusercontent.com/62354721/171372228-966ccf20-79f2-4526-9d28-59e0b523c34b.jpg)

"
56317,"Unit test //tensorflow/compiler/mlir/quantization/tensorflow/python:concurrency_test fails on Python 3.7, 3.8","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.8.13

### Bazel version

5.1.1

### GCC/Compiler version

10.3.0

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

```shell
The test fails due to Python code that is not supported in 3.7 or 3.8
```


### Standalone code to reproduce the issue

```shell
bazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --noremote_accept_cached --config=nonccl --copt=""-mtune=generic"" --copt=""-march=armv8-a"" --copt=""-O3"" --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --verbose_failures --build_tests_only -- //tensorflow/compiler/mlir/quantization/tensorflow/python:concurrency_test
```


### Relevant log output

```shell
bazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --noremote_accept_cached --config=nonccl --copt=""-mtune=generic"" --copt=""-march=armv8-a"" --copt=""-O3"" --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --verbose_failures --build_tests_only -- //tensorflow/compiler/mlir/quantization/tensorflow/python:concurrency_test
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=143
INFO: Reading rc options for 'test' from /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'test' from /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc:
  Inherited 'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=true
INFO: Reading rc options for 'test' from /home/builder/1/tensorflow_build/tensorflow-git/.tf_configure.bazelrc:
  Inherited 'build' options: --action_env PYTHON_BIN_PATH=/home/builder/1/tensorflow_build/venv_py38/bin/python3 --action_env PYTHON_LIB_PATH=/home/builder/1/tensorflow_build/venv_py38/lib/python3.8/site-packages --python_path=/home/builder/1/tensorflow_build/venv_py38/bin/python3
INFO: Reading rc options for 'test' from /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc:
  Inherited 'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Reading rc options for 'test' from /home/builder/1/tensorflow_build/tensorflow-git/.tf_configure.bazelrc:
  'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium
INFO: Found applicable config definition build:short_logs in file /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition test:v2 in file /home/builder/1/tensorflow_build/tensorflow-git/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only
INFO: Found applicable config definition build:nonccl in file /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc: --define=no_nccl_support=true
INFO: Found applicable config definition build:linux in file /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Analyzed target //tensorflow/compiler/mlir/quantization/tensorflow/python:concurrency_test (474 packages loaded, 28581 targets configured).
INFO: Found 1 test target...
FAIL: //tensorflow/compiler/mlir/quantization/tensorflow/python:concurrency_test (see /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/compiler/mlir/quantization/tensorflow/python/concurrency_test/test_attempts/attempt_1.log)
FAIL: //tensorflow/compiler/mlir/quantization/tensorflow/python:concurrency_test (see /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/compiler/mlir/quantization/tensorflow/python/concurrency_test/test_attempts/attempt_2.log)
FAIL: //tensorflow/compiler/mlir/quantization/tensorflow/python:concurrency_test (see /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/compiler/mlir/quantization/tensorflow/python/concurrency_test/test.log)

FAILED: //tensorflow/compiler/mlir/quantization/tensorflow/python:concurrency_test (Summary)
      /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/compiler/mlir/quantization/tensorflow/python/concurrency_test/test.log
      /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/compiler/mlir/quantization/tensorflow/python/concurrency_test/test_attempts/attempt_1.log
      /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/compiler/mlir/quantization/tensorflow/python/concurrency_test/test_attempts/attempt_2.log
INFO: From Testing //tensorflow/compiler/mlir/quantization/tensorflow/python:concurrency_test:
==================== Test output for //tensorflow/compiler/mlir/quantization/tensorflow/python:concurrency_test:
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base has been moved to tensorflow.python.trackable.base. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.checkpoint_management has been moved to tensorflow.python.checkpoint.checkpoint_management. The old module will be deleted in version 2.9.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.resource has been moved to tensorflow.python.trackable.resource. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.util has been moved to tensorflow.python.checkpoint.checkpoint. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base_delegate has been moved to tensorflow.python.trackable.base_delegate. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.graph_view has been moved to tensorflow.python.checkpoint.graph_view. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.python_state has been moved to tensorflow.python.trackable.python_state. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.saving.functional_saver has been moved to tensorflow.python.checkpoint.functional_saver. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.saving.checkpoint_options has been moved to tensorflow.python.checkpoint.checkpoint_options. The old module will be deleted in version 2.11.
Traceback (most recent call last):
  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/quantization/tensorflow/python/concurrency_test.runfiles/org_tensorflow/tensorflow/compiler/mlir/quantization/tensorflow/python/integration_test/concurrency_test.py"", line 22, in <module>
    from tensorflow.compiler.mlir.quantization.tensorflow.python import quantize_model
  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/quantization/tensorflow/python/concurrency_test.runfiles/org_tensorflow/tensorflow/compiler/mlir/quantization/tensorflow/python/quantize_model.py"", line 312, in <module>
    signature_keys: list[str],
TypeError: 'type' object is not subscriptable
================================================================================
==================== Test output for //tensorflow/compiler/mlir/quantization/tensorflow/python:concurrency_test:
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base has been moved to tensorflow.python.trackable.base. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.checkpoint_management has been moved to tensorflow.python.checkpoint.checkpoint_management. The old module will be deleted in version 2.9.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.resource has been moved to tensorflow.python.trackable.resource. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.util has been moved to tensorflow.python.checkpoint.checkpoint. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base_delegate has been moved to tensorflow.python.trackable.base_delegate. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.graph_view has been moved to tensorflow.python.checkpoint.graph_view. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.python_state has been moved to tensorflow.python.trackable.python_state. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.saving.functional_saver has been moved to tensorflow.python.checkpoint.functional_saver. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.saving.checkpoint_options has been moved to tensorflow.python.checkpoint.checkpoint_options. The old module will be deleted in version 2.11.
Traceback (most recent call last):
  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/quantization/tensorflow/python/concurrency_test.runfiles/org_tensorflow/tensorflow/compiler/mlir/quantization/tensorflow/python/integration_test/concurrency_test.py"", line 22, in <module>
    from tensorflow.compiler.mlir.quantization.tensorflow.python import quantize_model
  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/quantization/tensorflow/python/concurrency_test.runfiles/org_tensorflow/tensorflow/compiler/mlir/quantization/tensorflow/python/quantize_model.py"", line 312, in <module>
    signature_keys: list[str],
TypeError: 'type' object is not subscriptable
================================================================================
==================== Test output for //tensorflow/compiler/mlir/quantization/tensorflow/python:concurrency_test:
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base has been moved to tensorflow.python.trackable.base. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.checkpoint_management has been moved to tensorflow.python.checkpoint.checkpoint_management. The old module will be deleted in version 2.9.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.resource has been moved to tensorflow.python.trackable.resource. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.util has been moved to tensorflow.python.checkpoint.checkpoint. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base_delegate has been moved to tensorflow.python.trackable.base_delegate. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.graph_view has been moved to tensorflow.python.checkpoint.graph_view. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.python_state has been moved to tensorflow.python.trackable.python_state. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.saving.functional_saver has been moved to tensorflow.python.checkpoint.functional_saver. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.saving.checkpoint_options has been moved to tensorflow.python.checkpoint.checkpoint_options. The old module will be deleted in version 2.11.
Traceback (most recent call last):
  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/quantization/tensorflow/python/concurrency_test.runfiles/org_tensorflow/tensorflow/compiler/mlir/quantization/tensorflow/python/integration_test/concurrency_test.py"", line 22, in <module>
    from tensorflow.compiler.mlir.quantization.tensorflow.python import quantize_model
  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/quantization/tensorflow/python/concurrency_test.runfiles/org_tensorflow/tensorflow/compiler/mlir/quantization/tensorflow/python/quantize_model.py"", line 312, in <module>
    signature_keys: list[str],
TypeError: 'type' object is not subscriptable
================================================================================
Target //tensorflow/compiler/mlir/quantization/tensorflow/python:concurrency_test up-to-date:
  bazel-bin/tensorflow/compiler/mlir/quantization/tensorflow/python/concurrency_test
INFO: Elapsed time: 112.798s, Critical Path: 85.50s
INFO: 274 processes: 19 internal, 255 local.
INFO: Build completed, 1 test FAILED, 274 total actions
//tensorflow/compiler/mlir/quantization/tensorflow/python:concurrency_test FAILED in 3 out of 3 in 8.4s
  Stats over 3 runs: max = 8.4s, min = 3.8s, avg = 5.3s, dev = 2.2s
  /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/compiler/mlir/quantization/tensorflow/python/concurrency_test/test.log
  /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/compiler/mlir/quantization/tensorflow/python/concurrency_test/test_attempts/attempt_1.log
  /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/compiler/mlir/quantization/tensorflow/python/concurrency_test/test_attempts/attempt_2.log

INFO: Build completed, 1 test FAILED, 274 total actions
```
</details>"
56316,Loading tflite model from a buffer instead of .tflite file,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 16.04.2 LTS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
In the example https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/minimal.cc I can see that we are loading model from a file.
Is it possible to load the model from a character buffer (e.g. xxd -i minimal.tflite > minimal_data.cc) instead of from a file? If so, could you please provide an example?

Thanks
```


### Standalone code to reproduce the issue

```shell
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/minimal.cc:

int main(int argc, char* argv[]) {
  if (argc != 2) {
    fprintf(stderr, ""minimal <tflite model>\n"");
    return 1;
  }
  const char* filename = argv[1];

  // Load model
  std::unique_ptr<tflite::FlatBufferModel> model =
      tflite::FlatBufferModel::BuildFromFile(filename);
  TFLITE_MINIMAL_CHECK(model != nullptr);
...
}
```


### Relevant log output

_No response_</details>"
56315,Building Tflite compiling out pthread,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf2.9

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 16.04.2 LTS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am trying to build Tflite example ""minimal"" for a device which does not support Posix Thread.
I know one can disabled multi-threading by setting num_threads = 1 in the Interpreter, but what I am trying to get is disabling PTHREAD fully during compilation.

Is there a way to achieve this?

Thanks
```


### Standalone code to reproduce the issue

```shell
You can run the https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/minimal test.
```


### Relevant log output

_No response_</details>"
56313,How to full int 8 quantize a yamnet model?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.8

### Custom Code

Yes

### OS Platform and Distribution

Linux, Mac

### Mobile device

Linux Ubuntu, Mac m1, Mac intel

### Python version

3.7, 3.10, 3.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
How to full int 8 QAT quantisation
```


### Standalone code to reproduce the issue

```shell
import tensorflow_model_optimization as tfmot

LastValueQuantizer = tfmot.quantization.keras.quantizers.LastValueQuantizer
MovingAverageQuantizer = tfmot.quantization.keras.quantizers.MovingAverageQuantizer

class DefaultDenseQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):
    # List all of your weights
    weights = {
        ""kernel"": LastValueQuantizer(num_bits=8, symmetric=True, narrow_range=False, per_axis=False)
    }

    # List of all your activations
    activations = {
        ""activation"": MovingAverageQuantizer(num_bits=8, symmetric=False, narrow_range=False, per_axis=False)
    }

    # Configure how to quantize weights.
    def get_weights_and_quantizers(self, layer):
        output = []
        for attribute, quantizer in self.weights.items():
            if hasattr(layer, attribute):
                output.append((getattr(layer, attribute), quantizer))

        return output

    # Configure how to quantize activations.
    def get_activations_and_quantizers(self, layer):
        output = []
        for attribute, quantizer in self.activations.items():
            if hasattr(layer, attribute):
                output.append((getattr(layer, attribute), quantizer))

        return output

    def set_quantize_weights(self, layer, quantize_weights):
        # Add this line for each item returned in `get_weights_and_quantizers`
        # , in the same order

        count = 0
        for attribute in self.weights.keys():
            if hasattr(layer, attribute):
                setattr(layer, attribute, quantize_weights[count])
                count += 1

    def set_quantize_activations(self, layer, quantize_activations):
        # Add this line for each item returned in `get_activations_and_quantizers`
        # , in the same order.
        count = 0
        for attribute in self.activations.keys():
            if hasattr(layer, attribute):
                setattr(layer, attribute, quantize_activations[count])
                count += 1

    # Configure how to quantize outputs (may be equivalent to activations).
    def get_output_quantizers(self, layer):
        return []

    def get_config(self):
        return {}


from quant import DefaultDenseQuantizeConfig
from tensorflow_model_optimization.python.core.quantization.keras.quantize import quantize_scope, quantize_apply
import tensorflow_model_optimization as tfmot


with quantize_scope({
    ""DefaultDenseQuantizeConfig"": DefaultDenseQuantizeConfig,
    ""CustomLayer"": CustomLayer
}):
    def apply_quantization_to_layer(layer):
        return tfmot.quantization.keras.quantize_annotate_layer(layer, DefaultDenseQuantizeConfig())

    annotated_model = tf.keras.models.clone_model(
        tflite_model,
        clone_function=apply_quantization_to_layer,
    )

    qat_model = tfmot.quantization.keras.quantize_apply(annotated_model)

    qat_model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
        loss=""categorical_crossentropy"",
        metrics=['accuracy']
    )

    qat_model.summary()
```


### Relevant log output

```shell
When I test the tflite model it predicts completely randomly
```
</details>"
56312,Training the same model on the same data yielding extremely different test accuracy,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.8.0 (From Google Colab)

### Custom Code

No

### OS Platform and Distribution

Google Colab

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

Google Colab

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I wrote a function to load datasets (train, val, and test), create a model, train the model on the dataset train partition, and compute its accuracy on the test partition.

I expected that running this function multiple times in a row would yield similar results, but they are extremely different, ranging from .20 to .70 accuracy.

I tried to remove most sources of randomness, such as using the same numpy rng seed, TensorFlow rng seed, resetting the TensorFlow backend between runs, and using the same seed for shuffling the dataset, etc. The issue persists.

The issue occurs both in my local machine running Tensorflow 2.10.0-dev20220424 and in the Google Colab running TensorFlow 2.8.
```


### Standalone code to reproduce the issue

```shell
I created a Google Colab notebook that reproduces the issue: 

https://colab.research.google.com/drive/1b2DZaAIjqqXDlQ_uJyinVxOn1z_cbAoT?usp=sharing

The dataset I am using is just CIFAR-10 with the training partition split into train and validation:

https://drive.google.com/file/d/1UXJOv9MJ7bu_0woiVVp0BmcnfDzo0hpf/view?usp=sharing
```


### Relevant log output

_No response_</details>"
56311,SAMPLED_ADDMM,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

2.8

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hi TF team,

Is there a plan to add dense-dense matrix multiplication at sparse locations? Pytorch has such support: https://pytorch.org/docs/1.11/generated/torch.sparse.sampled_addmm.html

It is quite useful in ML modules where using sparse connection accelerates the model and helps prevent overfitting. For example, a multi-head attention module can be more efficient if in certain cases we already know that a sparse attention is needed. Currently, the same effect can be achieved by adding the ""attention_mask"". But there's no performance benefit when the mask is sparse.
```


### Standalone code to reproduce the issue

```shell
layers.MultiHeadAttention(num_heads=num_heads, key_dim=projection_dim, dropout=0.1)(x, x, attention_mask=mask)

Ideally, mask can be both sparse or dense tensor and there's performance improvement when mask's non-zero elements is sparse.
```


### Relevant log output

_No response_</details>"
56310,Tensorflow 2.2 build from source: Executing genrule @local_config_python//:numpy_include failed (Exit 1),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.2

### Custom Code

No

### OS Platform and Distribution

Ubuntu 

### Mobile device

Ubuntu 20.04.4

### Python version

3.8.10

### Bazel version

Bazel 2.0.0

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I get a numpy_include_failed error. I do have a correct version of numpy installed in the environment that I'm using.
```


### Standalone code to reproduce the issue

```shell
bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.2 --action_env PYTHON_BIN_PATH=/home/brent/Documents/Projects/BenchmarkRCStrategiesSOTA/venv_temp/bin/python  //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
ERROR: /home/brent/.cache/bazel/_bazel_brent/f717514c870df2340554ff84ed4b3c4d/external/local_config_python/BUILD:241:1: Executing genrule @local_config_python//:numpy_include failed (Exit 1)
cp: cannot stat '/home/brent/Documents/Projects/BenchmarkRCStrategiesSOTA/venv_temp/lib/python3.8/site-packages/numpy/core/include/numpy/.doxyfile': No such file or directory
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /home/brent/Documents/Projects/BenchmarkRCStrategiesSOTA/tensorflow/tensorflow/python/tools/BUILD:82:1 Executing genrule @local_config_python//:numpy_include failed (Exit 1)
INFO: Elapsed time: 272.539s, Critical Path: 68.77s
INFO: 1569 processes: 1569 local.
FAILED: Build did NOT complete successfully
```
</details>"
56309,Add gdb to development docker?,"Following arguments similar to https://github.com/tensorflow/io/pull/460, and given splendid @mrry advice here, https://stackoverflow.com/questions/40889221/debugging-tensorflow-tests-pdb-or-gdb, it would be nice to have GDB in the devel image.  I am happy to create a PR given very brief pointers on what files to change.


<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

head

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.8.3

### Bazel version

5.0.0

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
$ docker pull tensorflow/tensorflow:devel
$ docker run tensorflow/tensorflow:devel gdb
docker: Error response from daemon: OCI runtime create failed: container_linux.go:367: starting container process caused: exec: ""gdb"": executable file not found in $PATH: unknown.
ERRO[0000] error waiting for container: context canceled 


Ideally there would not be an error.

```


### Standalone code to reproduce the issue

```shell
$ docker pull tensorflow/tensorflow:devel
$ docker run tensorflow/tensorflow:devel gdb
docker: Error response from daemon: OCI runtime create failed: container_linux.go:367: starting container process caused: exec: ""gdb"": executable file not found in $PATH: unknown.
ERRO[0000] error waiting for container: context canceled
```


### Relevant log output

_No response_</details>"
56308,Fails to import tensorflow,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When try to import it has a circular import error.
```


### Standalone code to reproduce the issue

```shell
$ python
Python 3.9.13 (main, May 23 2022, 22:01:06) 
[GCC 9.4.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
2022-05-31 12:09:07.100479: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2022-05-31 12:09:07.100505: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/michael/.venv/ml/lib/python3.9/site-packages/tensorflow/__init__.py"", line 37, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/home/michael/.venv/ml/lib/python3.9/site-packages/tensorflow/python/__init__.py"", line 37, in <module>
    from tensorflow.python.eager import context
  File ""/home/michael/.venv/ml/lib/python3.9/site-packages/tensorflow/python/eager/context.py"", line 25, in <module>
    from absl import logging
  File ""/home/michael/.venv/ml/lib/python3.9/site-packages/absl/logging/__init__.py"", line 88, in <module>
    import timeit
  File ""/home/michael/timeit.py"", line 1, in <module>
    from timeit import timeit
ImportError: cannot import name 'timeit' from partially initialized module 'timeit' (most likely due to a circular import) (/home/michael/timeit.py)
>>>
```


### Relevant log output

```shell
See above
```
</details>"
56307,tensorflow.lite.python.convert_phase.ConverterError: device assign error,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

v2.9.0-rc2-42-g8a20d54a3c1 2.9.0

### Custom Code

No

### OS Platform and Distribution

Linux CentOS 7

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2 / 8.1.1

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I tried to convert the tf saved_mode(.pb) to tflite model. my computer can use gpu device, but I have error that some operation cannot assigned to gpu device. How can I fix it?
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1kDOUXMha543jKJxMl0w4CEExx7G461Q6?usp=sharing
```


### Relevant log output

```shell
2022-05-31 17:55:48.396106: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-31 17:55:54.142361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30983 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:15:00.0, compute capability: 7.0
2022-05-31 17:55:54.144955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30983 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:16:00.0, compute capability: 7.0
2022-05-31 17:55:54.147135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 30983 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:3a:00.0, compute capability: 7.0
2022-05-31 17:55:54.149256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 30983 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:3b:00.0, compute capability: 7.0
2022-05-31 17:55:54.151379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 30983 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0
2022-05-31 17:55:54.153498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 30983 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:8a:00.0, compute capability: 7.0
2022-05-31 17:55:54.155614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 30983 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:b2:00.0, compute capability: 7.0
2022-05-31 17:55:54.157735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 30983 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:b3:00.0, compute capability: 7.0
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1a/kernel:0' shape=(3, 3, 3, 16) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1a/bias:0' shape=(16,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1aa/kernel:0' shape=(3, 3, 16, 16) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1aa/bias:0' shape=(16,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1b/kernel:0' shape=(3, 3, 16, 16) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1a/kernel:0' shape=(3, 3, 3, 16) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1a/bias:0' shape=(16,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1aa/kernel:0' shape=(3, 3, 16, 16) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1aa/bias:0' shape=(16,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1b/kernel:0' shape=(3, 3, 16, 16) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1a/kernel:0' shape=(3, 3, 3, 16) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1a/bias:0' shape=(16,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1aa/kernel:0' shape=(3, 3, 16, 16) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1aa/bias:0' shape=(16,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1b/kernel:0' shape=(3, 3, 16, 16) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1a/kernel:0' shape=(3, 3, 3, 16) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1a/bias:0' shape=(16,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1aa/kernel:0' shape=(3, 3, 16, 16) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1aa/bias:0' shape=(16,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'pwcnet/featpyr/conv1b/kernel:0' shape=(3, 3, 16, 16) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
2022-05-31 17:56:01.538586: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.
2022-05-31 17:56:01.538654: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.
2022-05-31 17:56:01.539779: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: ./models/saved_model
2022-05-31 17:56:01.562091: I tensorflow/cc/saved_model/reader.cc:81] Reading meta graph with tags { serve }
2022-05-31 17:56:01.562154: I tensorflow/cc/saved_model/reader.cc:122] Reading SavedModel debug info (if present) from: ./models/saved_model
2022-05-31 17:56:01.640074: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled
2022-05-31 17:56:01.711920: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: fail: INVALID_ARGUMENT: Cannot assign a device for operation pwcnet/featpyr/strided_slice: {{node pwcnet/featpyr/strided_slice}} was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.
         [[pwcnet/featpyr/strided_slice]]. Took 172109 microseconds.
2022-05-31 17:56:01.712003: E tensorflow/compiler/mlir/tensorflow/translate/tf_mlir_translate.cc:199] Failed to load saved model v1 './models/saved_model': INVALID_ARGUMENT: Cannot assign a device for operation pwcnet/featpyr/strided_slice: {{node pwcnet/featpyr/strided_slice}} was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.
         [[pwcnet/featpyr/strided_slice]]
Traceback (most recent call last):
  File ""/home1/irteam/users/yeonghwa.jin/tfoptflow/tfoptflow/export_tflite.py"", line 51, in <module>
    export_tflite(saved_model_dir)
  File ""/home1/irteam/users/yeonghwa.jin/tfoptflow/tfoptflow/export_tflite.py"", line 42, in export_tflite
    tflite_model = converter.convert()
  File ""/home1/irteam/.pyenv/versions/tf2.9.0_cuda11.2_cudnn8.1.1/lib/python3.9/site-packages/tensorflow/lite/python/lite.py"", line 929, in wrapper
    return self._convert_and_export_metrics(convert_func, *args, **kwargs)
  File ""/home1/irteam/.pyenv/versions/tf2.9.0_cuda11.2_cudnn8.1.1/lib/python3.9/site-packages/tensorflow/lite/python/lite.py"", line 908, in _convert_and_export_metrics
    result = convert_func(self, *args, **kwargs)
  File ""/home1/irteam/.pyenv/versions/tf2.9.0_cuda11.2_cudnn8.1.1/lib/python3.9/site-packages/tensorflow/lite/python/lite.py"", line 1212, in convert
    return self._convert_from_saved_model(graph_def)
  File ""/home1/irteam/.pyenv/versions/tf2.9.0_cuda11.2_cudnn8.1.1/lib/python3.9/site-packages/tensorflow/lite/python/lite.py"", line 1095, in _convert_from_saved_model
    result = _convert_saved_model(**converter_kwargs)
  File ""/home1/irteam/.pyenv/versions/tf2.9.0_cuda11.2_cudnn8.1.1/lib/python3.9/site-packages/tensorflow/lite/python/convert_phase.py"", line 212, in wrapper
    raise converter_error from None  # Re-throws the exception.
  File ""/home1/irteam/.pyenv/versions/tf2.9.0_cuda11.2_cudnn8.1.1/lib/python3.9/site-packages/tensorflow/lite/python/convert_phase.py"", line 205, in wrapper
    return func(*args, **kwargs)
  File ""/home1/irteam/.pyenv/versions/tf2.9.0_cuda11.2_cudnn8.1.1/lib/python3.9/site-packages/tensorflow/lite/python/convert.py"", line 809, in convert_saved_model
    data = convert(
  File ""/home1/irteam/.pyenv/versions/tf2.9.0_cuda11.2_cudnn8.1.1/lib/python3.9/site-packages/tensorflow/lite/python/convert.py"", line 311, in convert
    raise converter_error
tensorflow.lite.python.convert_phase.ConverterError: Cannot assign a device for operation pwcnet/featpyr/strided_slice: {{node pwcnet/featpyr/strided_slice}} was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.
         [[pwcnet/featpyr/strided_slice]]
```
</details>"
56306,tf.data shard malfunction (drops data) when using from_generator as source,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.8.0-rc1-32-g3f878cff5b6 2.8.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.6 / cuDNN 8.3.2 (using 22.02 TensorRT NGC container)

### GPU model and memory

Quadro RTX 4000 8GB

### Current Behaviour?

For prediction pipelines I am using tf.data.Dataset fed from queues by using from_generator. I chose this way since I get data from queues and I want the dataset never to terminate.

I need to shard datasets to feed parallel prediction instances and tried using the shard function. It does not work as expected when using from_generator as source. It drops some data and fails with indices. 
It works perfectly fine when using e.g. from_tensor_slices or range to define the dataset.

Minimal example in colab: https://colab.research.google.com/drive/18SLBxl2Qws1GdA7YJ9GOI8DyPNgv_LTn?usp=sharing

I am currently working around this by extracting the tf.data.Dataset and putting it into multiple queues to manually shard and putting it back into tf.data.Dataset - this works but feels very wrong and produces overhead as I assume.

Is this behaviour desired? 
Am I maybe missing something?
Are there other solutions getting infinite Datasets from Queue with shard working?
Would appreciate some help 

### Standalone code to reproduce the issue

```python
import time
from threading import Thread
from queue import Queue
import tensorflow as tf

# Helper function
def iter_data(ds, prefix):
    for data in ds.as_numpy_iterator():
        print(f'{prefix}: {data}')
        

############################################
########## Works as expected ###############
############################################
time.sleep(.1)
print('\nSharding from range as source')

# define dataset
ds = tf.data.Dataset.range(10)
# ds.map(...) # Some processing ...

# shard dataset
ds_even = ds.shard(2, 0)
ds_odd = ds.shard(2, 1)

# extract dataset
t1 = Thread(target=iter_data, args=(ds_even,'even from range'), daemon=True)
t2 = Thread(target=iter_data, args=(ds_odd,'odd from range'), daemon=True)
t1.start()
t2.start()
time.sleep(1)


############################################
########## Works as expected ###############
############################################
time.sleep(.1)
print('\nSharding from from_tensor_slices as source')

# define dataset
ds = tf.data.Dataset.from_tensor_slices(list(range(10)))
# ds.map(...) # Some processing ...

# shard
ds_even = ds.shard(2, 0)
ds_odd = ds.shard(2, 1)

# extract
t1 = Thread(target=iter_data, args=(ds_even,'even from tensor'), daemon=True)
t2 = Thread(target=iter_data, args=(ds_odd,'odd from tensor'), daemon=True)
t1.start()
t2.start()
time.sleep(1)


############################################
########## Some data will be ommited #######
############################################

# create Queue source and fill it with elements as above
q_src = Queue()
for i in range(10):
    q_src.put(i)

# define generator function to bring the queue into tf Dataset
def get_q_gen(q):
    def q_gen():
        while True:
            yield q.get()
    return q_gen
 
# define dataset
ds = tf.data.Dataset.from_generator(get_q_gen(q_src), output_signature=(tf.TensorSpec((None), dtype=tf.int32)))
# ds.map(...) # Some processing ...

# shard - Not working as expected
ds_even = ds.shard(2, 0)
ds_odd = ds.shard(2, 1)

print('\nSharding from generator as source')

# extract
t1 = Thread(target=iter_data, args=(ds_even,'even from gen'), daemon=True)
t2 = Thread(target=iter_data, args=(ds_odd,'odd from gen'), daemon=True)
t1.start()
t2.start()
time.sleep(1)



############################################
########## My currently used workaround ####
############################################

q_src = Queue()
for i in range(10):
    q_src.put(i)
        
ds = tf.data.Dataset.from_generator(get_q_gen(q_src), output_signature=(tf.TensorSpec((None), dtype=tf.int32)))
# ds.map(...) # Some processing ...

# workaround - manually sharding dataset into multiple queues
def to_queue(ds:tf.data.Dataset, queues):
    for i, data in enumerate(ds.as_numpy_iterator()):
        q = queues[i%len(queues)]
        q.put(data)

q1, q2 = Queue(), Queue()
t0 = Thread(target=to_queue, args=(ds, [q1, q2]), daemon=True)
t0.start()

# sharded dataset from queues
ds_even = tf.data.Dataset.from_generator(get_q_gen(q1), output_signature=(tf.TensorSpec((None), dtype=tf.int32)))
ds_odd = tf.data.Dataset.from_generator(get_q_gen(q2), output_signature=(tf.TensorSpec((None), dtype=tf.int32)))

print('\nWorkaround')

t1 = Thread(target=iter_data, args=(ds_even,'even from gen'), daemon=True)
t2 = Thread(target=iter_data, args=(ds_odd,'odd from gen'), daemon=True)
t1.start()
t2.start()
time.sleep(1)

```


### Relevant log output


Sharding from range as source
even from range: 0
even from range: 2
odd from range: 1
even from range: 4
odd from range: 3
even from range: 6
odd from range: 5
odd from range: 7
even from range: 8
odd from range: 9

Sharding from from_tensor_slices as source
even from tensor: 0
odd from tensor: 1
even from tensor: 2
odd from tensor: 3
odd from tensor: 5
odd from tensor: 7
even from tensor: 4
odd from tensor: 9
even from tensor: 6
even from tensor: 8

Sharding from generator as source
even from gen: 0
odd from gen: 2
even from gen: 4
odd from gen: 6
odd from gen: 9

Workaround
odd from gen: 1
even from gen: 0
odd from gen: 3
even from gen: 2
odd from gen: 5
even from gen: 4
even from gen: 6
odd from gen: 7
even from gen: 8
odd from gen: 9

</details>"
56305,`tf.keras.losses.cosine_similarity` generates unreasonably large gradients for zero-masked & broadcasted tensor,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.9.0-18-gd8ce9f9c301 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 21.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
For a zero-masked, broadcasted tensor, autograd generates very large gradients for zero-masked parts.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

def test():
    a = tf.random.normal([3, 5])
    with tf.GradientTape(persistent=True) as tape:
        tape.watch(a)
        a = a * tf.constant([[1],[0],[0]], dtype=tf.float32)
        cos_score = tf.keras.losses.cosine_similarity(a[:,None], a[None,:])
        loss = tf.reduce_sum(cos_score)

    grad = tape.gradient(loss, a)
    print(grad)

test()
```


### Relevant log output

```shell
A sample output from the above code

tf.Tensor(
[[-5.9604645e-08  5.9604645e-08 -4.4703484e-08 -2.9802322e-08
   8.9406967e-08]
 [ 9.4966488e+05 -9.1792988e+05  6.5334194e+05  3.2174541e+05
  -1.3134559e+06]
 [ 9.4966488e+05 -9.1792988e+05  6.5334194e+05  3.2174541e+05
  -1.3134559e+06]], shape=(3, 5), dtype=float32)
```
</details>"
56304,How to display custom label with angles in iOS?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

pod 'TensorFlowLiteSwift', '~> 0.0.1-nightly', :subspecs => ['CoreML', 'Metal']

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
How to display degree of angle on every keyPoint.
```


### Standalone code to reproduce the issue

```shell
Below code for to create degree of angle I used

func angle(
              firstLandmark: CGPoint,
              midLandmark: CGPoint,
              lastLandmark: CGPoint
          ) -> CGFloat {
              let radians: CGFloat =
                  atan2(lastLandmark.y - midLandmark.y,
                            lastLandmark.x - midLandmark.x) -
                    atan2(firstLandmark.y - midLandmark.y,
                            firstLandmark.x - midLandmark.x)
              
              
               var degrees = radians * 180.0 / .pi
              //var degrees = radians * .pi / 180 /// .pi
                degrees = abs(degrees) // Angle should never be negative
               if degrees > 180.0 {
                  degrees = 360.0 - degrees // Always get the acute representation of the angle
               }
              
               let roundedValue1 = round(degrees)
              
              return roundedValue1
          }
```


### Relevant log output

_No response_</details>"
56303,tf.keras.layers.Reshape throws segfault ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf2.1.0

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.6.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
segmentation fault. reproducible in the nightly version. 
https://colab.research.google.com/drive/1pRSpvs2vlCQyH5HIa8BjAbovIfqs9Dvr?usp=sharing
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
tf.keras.layers.Reshape(target_shape=np.array([21943, 45817, 30516, 61760, 38987], dtype=np.uint16))(np.ones((0,1,1)))
```


### Relevant log output

_No response_</details>"
56302,Fail to link unit tests for AARCH64 with --config=mkl_aarch64,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

CentOS 7

### Mobile device

n/a

### Python version

3.8.13

### Bazel version

5.1.1

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

```shell
Some unit tests in //tensorflow/compiler/mlir/tensorflow/tests/... fail to link tf-reduce
```


### Standalone code to reproduce the issue

```shell
bazel test --test_timeout=300,500,-1,-1 -k --flaky_test_attempts=3 --test_output=all --cache_test_results=no --noremote_accept_cached --config=nonccl --config=mkl_aarch64 --copt=""-mtune=generic"" --copt=""-march=armv8-a"" --copt=""-O3"" --test_env=TF_ENABLE_ONEDNN_OPTS=1 --copt=""-fopenmp"" --linkopt=""-lgomp"" --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --verbose_failures --build_tests_only --jobs=75 -- //tensorflow/compiler/mlir/tensorflow/tests/...
```


### Relevant log output

```shell
bazel test --test_timeout=300,500,-1,-1 -k --flaky_test_attempts=3 --test_output=all --cache_test_results=no --noremote_accept_cached --config=nonccl --config=mkl_aarch64 --copt=""-mtune=generic"" --copt=""-march=armv8-a"" --copt=""-O3"" --test_env=TF_ENABLE_ONEDNN_OPTS=1 --copt=""-fopenmp"" --linkopt=""-lgomp"" --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --verbose_failures --build_tests_only --jobs=75 -- //tensorflow/compiler/mlir/tensorflow/tests/...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=140
INFO: Reading rc options for 'test' from /tmp/workspace/tensorflow-oneDNN-ACL-git/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'test' from /tmp/workspace/tensorflow-oneDNN-ACL-git/.bazelrc:
  Inherited 'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=true
INFO: Reading rc options for 'test' from /tmp/workspace/tensorflow-oneDNN-ACL-git/.tf_configure.bazelrc:
  Inherited 'build' options: --action_env PYTHON_BIN_PATH=/tmp/workspace/venv-cp38-cp38/bin/python3 --action_env PYTHON_LIB_PATH=/tmp/workspace/venv-cp38-cp38/lib/python3.8/site-packages --python_path=/tmp/workspace/venv-cp38-cp38/bin/python3
INFO: Reading rc options for 'test' from /tmp/workspace/tensorflow-oneDNN-ACL-git/.bazelrc:
  Inherited 'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Reading rc options for 'test' from /tmp/workspace/tensorflow-oneDNN-ACL-git/.tf_configure.bazelrc:
  'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium
INFO: Found applicable config definition build:short_logs in file /tmp/workspace/tensorflow-oneDNN-ACL-git/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /tmp/workspace/tensorflow-oneDNN-ACL-git/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition test:v2 in file /tmp/workspace/tensorflow-oneDNN-ACL-git/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only
INFO: Found applicable config definition build:nonccl in file /tmp/workspace/tensorflow-oneDNN-ACL-git/.bazelrc: --define=no_nccl_support=true
INFO: Found applicable config definition build:mkl_aarch64 in file /tmp/workspace/tensorflow-oneDNN-ACL-git/.bazelrc: --define=build_with_mkl_aarch64=true --define=build_with_openmp=true -c opt
INFO: Found applicable config definition build:linux in file /tmp/workspace/tensorflow-oneDNN-ACL-git/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /tmp/workspace/tensorflow-oneDNN-ACL-git/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/d6709ddccb7d6fd1c30144ae7a515956dd470e9f.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
INFO: Analyzed 282 targets (7 packages loaded, 1406 targets configured).
INFO: Found 282 test targets...
ERROR: /tmp/workspace/tensorflow-oneDNN-ACL-git/tensorflow/compiler/mlir/BUILD:194:13: Linking tensorflow/compiler/mlir/tf-reduce failed: (Exit 1): gcc failed: error executing command 
  (cd /root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/opt/rh/devtoolset-10/root/usr/lib64:/opt/rh/devtoolset-10/root/usr/lib:/opt/rh/devtoolset-10/root/usr/lib64/dyninst:/opt/rh/devtoolset-10/root/usr/lib/dyninst:/usr/local/lib64 \
    PATH=/root/.cache/bazelisk/downloads/bazelbuild/bazel-5.1.1-linux-arm64/bin:/tmp/workspace/venv-cp38-cp38/bin:/opt/rh/devtoolset-10/root/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/tmp/workspace/venv-cp38-cp38/bin/python3 \
    PYTHON_LIB_PATH=/tmp/workspace/venv-cp38-cp38/lib/python3.8/site-packages \
    TF2_BEHAVIOR=1 \
  /opt/rh/devtoolset-10/root/usr/bin/gcc @bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/tf-reduce-2.params)
# Configuration: 399000c0582e812b9aaea4d84675b9962333bd9c4e42025bb240c359ddc70077
# Execution platform: @local_execution_config_platform//:platform
/opt/rh/devtoolset-10/root/usr/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
collect2: error: ld returned 1 exit status
```
</details>"
56301,Tflite model not work correctly on NNAPI-GPU delegate,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

2.5, 2.6

### Custom Code

Yes

### OS Platform and Distribution

Windows 11

### Mobile device

Huawei Mate 10 Pro

### Python version

3.6.0

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

Non disponibile 

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I have converted a GRU or LSTM based RNN model to TFLite. But I have some issues with using NNAPI delegate on GPU and GPU delegate when using model on device.

In particular I cannot use the GPU for all the operations of the Model Graph, although all the operations (Reshape and LSTM layer)are actually supported as indicated at this link:

https://www.tensorflow.org/lite/performance/gpu_advanced

I tested this working using the apk I downloaded from this page:

https://www.tensorflow.org/lite/performance/measurement

both the ""nightly pre-built"" version and the version that support TF ops via Flex delegate.

I have experimented with different conversion options such as:
- converter.representative_dataset = representative_data_gen
- converter.experimental_new_quantizer = True
- converter.experimental_enable_resource_variables = True


I expect that if the LSTM, Reshape and Dense layers are usable on GPU, it is possible to use NNAPI delegates on GPU and GPU.
```


### Standalone code to reproduce the issue

```shell
import os
import numpy as np
import tensorflow as tf


def representative_data_gen():
    for input_value in tf.data.Dataset.from_tensor_slices(X_train).batch(1).take(X_train.shape[0]):
    # Model has only one input so each data point has one element.
        yield [input_value]


training = """"
model_save_path = """"

X_train = np.loadtxt(training, delimiter=',', dtype='float32', usecols=list(range(1, (21 * 3 * 2 * 21) + 1)))
y_train = np.loadtxt(training, delimiter=',', dtype='int32', usecols=(0))
    
model = tf.keras.models.Sequential([                            
          tf.keras.layers.InputLayer(input_shape=(21 * 3 * 2 * 21, )),
          tf.keras.layers.Reshape((21, 3 * 2 * 21)),        
          tf.keras.layers.LSTM(8, return_sequences=True, unroll=True),
          tf.keras.layers.Dropout(0.40, seed=42),
          tf.keras.layers.LSTM(8, unroll=True),        
          tf.keras.layers.Dropout(0.50, seed=42),             
          tf.keras.layers.Dense(27, activation='softmax')])    
               
           
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

history = model.fit(
    X_train,
    y_train,
    epochs=100,
    batch_size=256,
)

model.save(model_save_path, include_optimizer=False)   
model = tf.keras.models.load_model(model_save_path)

run_model = tf.function(lambda x: model(x))
concrete_func = run_model.get_concrete_function(tf.TensorSpec([1, model.inputs[0].shape[1]], model.inputs[0].dtype))

converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])  
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.experimental_new_converter = True
#converter.representative_dataset = representative_data_gen
converter.experimental_new_quantizer = True
converter.experimental_enable_resource_variables = True
converter.allow_custom_ops = True
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
tflite_quantized_model = converter.convert()


open("""", 'wb').write(tflite_quantized_model)

interpreter = tf.lite.Interpreter(model_path="""")
interpreter.allocate_tensors()
```


### Relevant log output

```shell
With tf 2.5 without rappresentative dataset

05-30 13:04:37.451 25935 25935 I tflite_BenchmarkModelActivity: Running TensorFlow Lite benchmark with args: --graph=/data/local/tmp/model.tflite --warmup_runs=1 --num_runs=50 --enable_op_profiling=true --use_xnnpack=false --use_nnapi=true
05-30 13:04:37.455 25935 25935 I tflite  : Log parameter values verbosely: [0]
05-30 13:04:37.455 25935 25935 I tflite  : Min num runs: [50]
05-30 13:04:37.455 25935 25935 I tflite  : Min warmup runs: [1]
05-30 13:04:37.455 25935 25935 I tflite  : Graph: [/data/local/tmp/model.tflite]
05-30 13:04:37.455 25935 25935 I tflite  : Enable op profiling: [1]
05-30 13:04:37.455 25935 25935 I tflite  : Use NNAPI: [1]
05-30 13:04:37.460 25935 25935 I tflite  : NNAPI accelerators available: [ipuadaptor,nnapi-reference]
05-30 13:04:37.460 25935 25935 I tflite  : Use xnnpack: [0]
05-30 13:04:37.460 25935 25935 I tflite  : Loaded model /data/local/tmp/model.tflite
05-30 13:04:37.460 25935 25935 I tflite  : Initialized TensorFlow Lite runtime.
05-30 13:04:37.462 25935 25935 I tflite  : Created TensorFlow Lite delegate for NNAPI.
05-30 13:04:37.462 25935 25935 I tflite  : NNAPI delegate created.
05-30 13:04:37.516 25935 25935 I tflite  : Though NNAPI delegate is explicitly applied, the model graph will not be executed by the delegate.
05-30 13:04:37.516 25935 25935 I tflite  : The input model file size (MB): 0.141312
05-30 13:04:37.516 25935 25935 I tflite  : Initialized session in 56.16ms.
05-30 13:04:37.519 25935 25935 I tflite  : Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
05-30 13:04:38.019 25935 25935 I tflite  : count=1156 first=4645 curr=433 min=424 max=4645 avg=431.215 std=124
05-30 13:04:38.019 25935 25935 I tflite  : Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.
05-30 13:04:39.020 25935 25935 I tflite  : count=586 first=564 curr=522 min=515 max=949 avg=520.706 std=27
05-30 13:04:39.021 25935 25935 I tflite  : Inference timings in us: Init: 56160, First inference: 4645, Warmup (avg): 431.215, Inference (avg): 520.706
05-30 13:04:39.021 25935 25935 I tflite  : Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
05-30 13:04:39.021 25935 25935 I tflite  : Memory footprint delta from the start of the tool (MB): init=2.03516 overall=2.03516
05-30 13:04:39.021 25935 25935 I tflite  : Profiling Info for Benchmark Initialization:
05-30 13:04:39.021 25935 25935 I tflite  : ============================== Run Order ==============================
05-30 13:04:39.021 25935 25935 I tflite  :                   [node type]          [first]        [avg ms]            [%]          [cdf%]          [mem KB] [times called]   [Name]
05-30 13:04:39.021 25935 25935 I tflite  :       ModifyGraphWithDelegate           54.571          54.571        95.055%         95.055%          1132.000         1        ModifyGraphWithDelegate/0
05-30 13:04:39.021 25935 25935 I tflite  :               AllocateTensors            2.833           1.419         4.945%        100.000%             0.000         2        AllocateTensors/0
05-30 13:04:39.021 25935 25935 I tflite  :
05-30 13:04:39.021 25935 25935 I tflite  : ============================== Top by Computation Time ==============================
05-30 13:04:39.021 25935 25935 I tflite  :                   [node type]          [first]        [avg ms]            [%]          [cdf%]          [mem KB] [times called]   [Name]
05-30 13:04:39.021 25935 25935 I tflite  :       ModifyGraphWithDelegate           54.571          54.571        95.055%         95.055%          1132.000         1        ModifyGraphWithDelegate/0
05-30 13:04:39.021 25935 25935 I tflite  :               AllocateTensors            2.833           1.419         4.945%        100.000%             0.000         2        AllocateTensors/0
05-30 13:04:39.021 25935 25935 I tflite  :
05-30 13:04:39.021 25935 25935 I tflite  : Number of nodes executed: 2
05-30 13:04:39.021 25935 25935 I tflite  : ============================== Summary by node type ==============================
05-30 13:04:39.021 25935 25935 I tflite  :                   [Node type]          [count]         [avg ms]          [avg %]         [cdf %]       [mem KB] [ti
05-30 13:04:39.026 25935 25935 I tflite  : Operator-wise Profiling Info for Regular Benchmark Runs:
05-30 13:04:39.026 25935 25935 I tflite  : ============================== Run Order ==============================
05-30 13:04:39.026 25935 25935 I tflite  :                   [node type]          [first]        [avg ms]            [%]          [cdf%]          [mem KB] [times called]   [Name]
05-30 13:04:39.026 25935 25935 I tflite  :                       RESHAPE            0.001           0.001         0.264%          0.264%             0.000         1        [sequential/lstm/transpose1]:0
05-30 13:04:39.026 25935 25935 I tflite  :                        UNPACK            0.004           0.003         0.651%          0.915%             0.000         1        [sequential/lstm/unstack, sequential/lstm/unstack1, sequential/lstm/unstack2, sequential/lstm/unstack3, sequential/lstm/unstack4, sequential/lstm/unstack5, sequential/lstm/unstack6, sequential/lstm/unstack7, sequential/lstm/unstack8, sequential/lstm/unstack9, sequential/lstm/unstack10, sequential/lstm/unstack11, sequential/lstm/unstack12, sequential/lstm/unstack13, sequential/lstm/unstack14, sequential/lstm/unstack15, sequential/lstm/unstack16, sequential/lstm/unstack17, sequential/lstm/unstack18, sequential/lstm/unstack19, sequential/lstm/unstack20]:1
05-30 13:04:39.026 25935 25935 I tflite  :               FULLY_CONNECTED            0.008

========================================================================================================
========================================================================================================
========================================================================================================
========================================================================================================
========================================================================================================
With tf 2.6.x and full-integer quantization(rappresentative dataset)

05-30 15:16:44.706  3380  3380 I tflite_BenchmarkModelActivity: Running TensorFlow Lite benchmark with args: --graph=/data/local/tmp/model.tflite --warmup_runs=1 --num_runs=50 --enable_op_profiling=true --use_xnnpack=false --use_nnapi=true --disable_nnapi_cpu=false
05-30 15:16:44.719  3380  3380 I tflite  : Log parameter values verbosely: [0]
05-30 15:16:44.719  3380  3380 I tflite  : Min num runs: [50]
05-30 15:16:44.719  3380  3380 I tflite  : Min warmup runs: [1]
05-30 15:16:44.719  3380  3380 I tflite  : Graph: [/data/local/tmp/model.tflite]
05-30 15:16:44.719  3380  3380 I tflite  : Enable op profiling: [1]
05-30 15:16:44.720  3380  3380 I tflite  : Use NNAPI: [1]
05-30 15:16:44.731  3380  3380 I tflite  : NNAPI accelerators available: [ipuadaptor,nnapi-reference]
05-30 15:16:44.731  3380  3380 I tflite  : Disable NNAPI cpu: [0]
05-30 15:16:44.731  3380  3380 I tflite  : Use xnnpack: [0]
05-30 15:16:44.732  3380  3380 I tflite  : Loaded model /data/local/tmp/model.tflite
05-30 15:16:44.733  3380  3380 I tflite  : Initialized TensorFlow Lite runtime.
05-30 15:16:44.737  3380  3380 I tflite  : Created TensorFlow Lite delegate for NNAPI.
05-30 15:16:44.737  3380  3380 I tflite  : NNAPI delegate created.
05-30 15:16:44.748  3380  3380 I tflite  : Replacing 504 node(s) with delegate (TfLiteNnapiDelegate) node, yielding 87 partitions.
05-30 15:16:44.857  3380  3380 I tflite  : Explicitly applied NNAPI delegate, and the model graph will be partially executed by the delegate w/ 44 delegate kernels.
05-30 15:16:44.858  3380  3380 I tflite  : The input model file size (MB): 0.113392
05-30 15:16:44.858  3380  3380 I tflite  : Initialized session in 126.059ms.
05-30 15:16:44.858  3380  3380 I tflite  : Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
05-30 15:16:45.362  3380  3380 I tflite  : count=40 first=32942 curr=12594 min=11340 max=32942 avg=12581.9 std=3331
05-30 15:16:45.362  3380  3380 I tflite  : Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.
05-30 15:16:46.365  3380  3380 I tflite  : count=75 first=12840 curr=11853 min=11787 max=13649 avg=12167.6 std=325
05-30 15:16:46.366  3380  3380 I tflite  : Inference timings in us: Init: 126059, First inference: 32942, Warmup (avg): 12581.9, Inference (avg): 12167.6
05-30 15:16:46.366  3380  3380 I tflite  : Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
05-30 15:16:46.366  3380  3380 I tflite  : Memory footprint delta from the start of the tool (MB): init=2.01562 overall=4.23438
05-30 15:16:46.366  3380  3380 I tflite  : Profiling Info for Benchmark Initialization:
05-30 15:16:46.366  3380  3380 I tflite  : ============================== Run Order ==============================
05-30 15:16:46.366  3380  3380 I tflite  :                   [node type]          [first]        [avg ms]            [%]          [cdf%]          [mem KB] [times called]   [Name]
05-30 15:16:46.366  3380  3380 I tflite  :       ModifyGraphWithDelegate          119.932         119.932        63.300%         63.300%          1116.000         1        ModifyGraphWithDelegate/0
05-30 15:16:46.366  3380  3380 I tflite  :               AllocateTensors           69.517          34.767        36.700%        100.000%             0.000         2        AllocateTensors/0
05-30 15:16:46.366  3380  3380 I tflite  :
05-30 15:16:46.366  3380  3380 I tflite  : ============================== Top by Computation Time ==============================
05-30 15:16:46.366  3380  3380 I tflite  :                   [node type]          [first]        [avg ms]            [%]          [cdf%]          [mem KB] [times called]   [Name]
05-30 15:16:46.366  3380  3380 I tflite  :       ModifyGraphWithDelegate          119.932         119.932        63.300%         63.300%          1116.000         1        ModifyGraphWithDelegate/0
05-30 15:16:46.366  3380  3380 I tflite  :               AllocateTensors           69.517          34.767        36.700%        100.000%             0.000         2        AllocateTensors/0
05-30 15:16:46.366  3380  3380 I tflite  :
05-30 15:16:46.366  3380  3380 I tflite  : Number of nodes executed: 2
05-30 15:16:46.366  3380  3380 I tflite  : ============================== Summary by node type ==============================
05-30 15:16:46.366  3380  3380 I tflite  :                   [Node type]          [count]         [avg ms]          [avg %]         [cdf %]       [mem KB] [ti
05-30 15:16:46.370  3380  3380 I tflite  : Operator-wise Profiling Info for Regular Benchmark Runs:
05-30 15:16:46.370  3380  3380 I tflite  : ============================== Run Order ==============================
05-30 15:16:46.370  3380  3380 I tflite  :                   [node type]          [first]        [avg ms]            [%]          [cdf%]          [mem KB] [times called]   [Name]
05-30 15:16:46.370  3380  3380 I tflite  :           TfLiteNnapiDelegate            0.228           0.213         1.773%          1.773%             0.000         1        [sequential/lstm/transpose1]:590
05-30 15:16:46.370  3380  3380 I tflite  :                        UNPACK            0.033           0.020         0.168%          1.941%             0.000         1        [sequential/lstm/unstack, sequential/lstm/unstack1, sequential/lstm/unstack2, sequential/lstm/unstack3, sequential/lstm/unstack4, sequential/lstm/unstack5, sequential/lstm/unstack6, sequential/lstm/unstack7, sequential/lstm/unstack8, sequential/lstm/unstack9, sequential/lstm/unstack10, sequential/lstm/unstack11, sequential/lstm/unstack12, sequential/lstm/unstack13, sequential/lstm/unstack14, sequential/lstm/unstack15, sequential/lstm/unstack16, sequential/lstm/unstack17, sequential/lstm/unstack18, sequential/lstm/unstack19, sequential/lstm/unstack20]:1
05-30 15:16:46.370  3380  3380 I tflite  :               FULLY_CONNECTED            0.011
```
</details>"
56300,Ragged Tensor in 'batch_jacobian',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.9.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The function `tape.batch_jacobian` does support ragged tensors. 
Can something like the code below be done using `tape.batch_jacobian`, or did I do something wrong?
For normal tensor input, the code works just fine.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

x_ragged = tf.RaggedTensor.from_row_lengths([[1.0], [2.0], [3.0], [4.0]], [2, 1, 1])
x_tensor = tf.constant([[[1.0]], [[3.0]], [[4.0]]])
print(x_ragged.shape, x_tensor.shape)

x = x_ragged

with tf.GradientTape(persistent=True) as tape:
    tape.watch(x)
    y = x*x + x
    y = tf.reduce_sum(y, axis=(-2))
out = tape.batch_jacobian(y, x, experimental_use_pfor=True)
print(y.shape)
print(out)
```


### Relevant log output

```shell
ValueError: in user code:
    ValueError: No pfor vectorization defined for RaggedTensorToVariant
    name: ""loop_body/RaggedToVariant/RaggedTensorToVariant""
    op: ""RaggedTensorToVariant""
    input: ""loop_body/RaggedToVariant/RaggedTensorToVariant/rt_nested_splits_0""
    input: ""loop_body/AddN""
    attr {
      key: ""RAGGED_RANK""
      value {
        i: 1
      }
    }
    attr {
      key: ""Tsplits""
      value {
        type: DT_INT64
      }
    }
    attr {
      key: ""Tvalues""
      value {
        type: DT_FLOAT
      }
    }
    attr {
      key: ""batched_input""
      value {
        b: false
      }
    }
    experimental_type {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_RAGGED
        args {
          type_id: TFT_FLOAT
        }
      }
    }
    
     inputs: [WrappedTensor(t=<tf.Tensor 'loop_body/RaggedToVariant/RaggedTensorToVariant/rt_nested_splits_0:0' shape=(4,) dtype=int64>, is_stacked=False, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'loop_body/AddN/pfor/AddN:0' shape=(1, 4, 1) dtype=float32>, is_stacked=True, is_sparse_stacked=False)].
Encountered an exception while vectorizing the batch_jacobian computation. Vectorization can be disabled by setting experimental_use_pfor to False.
```
</details>"
56299,Matmul not working on ragged tensors,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.8.0-1

### GPU model and memory

_No response_

### Current Behaviour?


The matmul function on ragged tensors gets the row splits dtype inferred incorrectly, leading to unreadable errors with Keras.

I suppose this could be fixed somehow like this:
```python
b/tensorflow/python/ops/ragged/ragged_math_ops.py
index 189ebf1e..36fd5232 100644
--- a/tensorflow/python/ops/ragged/ragged_math_ops.py
+++ b/tensorflow/python/ops/ragged/ragged_math_ops.py
@@ -958,16 +958,17 @@ def _matmul_3d_with_map_fn(a, b, **kwargs):
   else:
     output_ragged_rank = 1
 
+  fn_out_shape = None  # Figure out proper shape.
+  row_splits_dtype = (
+      a.row_splits.dtype
+      if isinstance(a, ragged_tensor.RaggedTensor) else b.row_splits.dtype)
+
   def single_batch_matmul(x):
     out = _matmul_2d(x[0], x[1], **kwargs)
     if output_ragged_rank == 2:
-      out = ragged_tensor.RaggedTensor.from_tensor(out)
+      out = ragged_tensor.RaggedTensor.from_tensor(out, row_splits_dtype=row_splits_dtype)
     return out
 
-  fn_out_shape = None  # Figure out proper shape.
-  row_splits_dtype = (
-      a.row_splits.dtype
-      if isinstance(a, ragged_tensor.RaggedTensor) else b.row_splits.dtype)
   output_type = kwargs['output_type']
   if output_type is None:
     output_type = a.dtype
```
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import keras

from tensorflow.python.ops.numpy_ops import np_config
from scipy.optimize import linear_sum_assignment

np_config.enable_numpy_behavior()

def apply_normalization(layer, x):
    """"""
    Supports ragged tensors
    """"""
    s = tf.shape(x)
    x = tf.reshape(x, [-1, tf.shape(x)[-1]])
    y = layer(x)
    return tf.reshape(y, s)


def foo(*args):
    n_elements = tf.reduce_prod(args)
    return tf.range(n_elements, dtype=tf.float32).reshape(args)


class Attention(keras.layers.Layer):
    def __init__(self):
        super().__init__()

    def build(self, input_shape):
        queries, keys, values = input_shape
        assert queries[-1] == keys[-1] == values[-1]
        model_dim = queries[-1]
        self.keys_projector = keras.layers.Dense(model_dim)
        self.query_projector = keras.layers.Dense(model_dim)
        self.value_projector = keras.layers.Dense(model_dim)

    def call(self, x, *args, **kwargs) -> tf.Tensor:
        queries, keys, values = x
        queries = self.query_projector(queries)
        keys = self.keys_projector(keys)
        values = self.value_projector(values)
        print('keys, queries', tf.shape(keys), tf.shape(queries))
        print(keys.row_splits.dtype, queries.row_splits.dtype)
        logits = tf.matmul(keys, queries, transpose_b=True)
        print('logits', tf.shape(logits))
        weights = tf.nn.softmax(logits)
        return tf.matmul(weights, values)


class MultiHeadAttention(keras.layers.Layer):
    def __init__(self, n_heads: int = 6):
        super().__init__()
        self.heads = [Attention() for _ in range(n_heads)]

    def build(self, input_shape):
        queries, keys, values = input_shape
        assert queries[-1] == keys[-1] == values[-1]
        model_dim = queries[-1]
        self.final_projector = keras.layers.Dense(model_dim)

    def call(self, x, *args, **kwargs) -> tf.Tensor:
        print('x', x)
        head_outputs = []
        for head in self.heads:
            head_outputs.append(head(x))
        return self.final_projector(tf.concat(head_outputs, axis=-1))


class ResidualAttentionBlock(keras.layers.Layer):
    def __init__(self, mlp_factor: float = 4.0):
        super().__init__()
        self.mlp_factor = mlp_factor
        self.attention = MultiHeadAttention()
        self.norm1 = keras.layers.LayerNormalization()
        self.norm2 = keras.layers.LayerNormalization()

    def build(self, input_shape):
        model_dim = input_shape[-1]
        mlp_width = tf.cast(tf.cast(model_dim, tf.float32) * self.mlp_factor, tf.int32)
        self.mlp = keras.Sequential(
            (keras.layers.Dense(mlp_width, 'swish'), keras.layers.Dense(model_dim))
        )

    def attend(self, x):
        return self.attention((x, x, x))

    def call(self, x):
        print('x', tf.shape(x))
        normalized = apply_normalization(self.norm1, x)
        print('normalized', tf.shape(normalized))
        residuals = self.attend(normalized)
        print('residuals', tf.shape(residuals))
        x = x + residuals
        x = x + self.mlp(apply_normalization(self.norm2, x))
        return residuals

x = tf.RaggedTensor.from_row_lengths(foo(6, 2), [1, 2, 3])
block(x)
```


### Relevant log output

```shell
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Input In [166], in <cell line: 2>()
      1 x = tf.RaggedTensor.from_row_lengths(foo(6, 2), [1, 2, 3])
----> 2 block(x)

File ~/miniconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     65 except Exception as e:  # pylint: disable=broad-except
     66   filtered_tb = _process_traceback_frames(e.__traceback__)
---> 67   raise e.with_traceback(filtered_tb) from None
     68 finally:
     69   del filtered_tb

File ~/my_super_project/models.py:91, in ResidualAttentionBlock.call(self, x)
     89 normalized = apply_normalization(self.norm1, x)
     90 print('normalized', tf.shape(normalized))
---> 91 residuals = self.attend(normalized)
     92 print('residuals', tf.shape(residuals))
     93 x = x + residuals

File ~/my_super_project/models.py:85, in ResidualAttentionBlock.attend(self, x)
     84 def attend(self, x):
---> 85     return self.attention((x, x, x))

File ~/my_super_project/models.py:66, in MultiHeadAttention.call(self, x, *args, **kwargs)
     65 def call(self, x, *args, **kwargs) -> tf.Tensor:
---> 66     return self.final_projector(tf.concat([head(x) for head in self.heads], axis=-1))

File ~/my_super_project/models.py:66, in <listcomp>(.0)
     65 def call(self, x, *args, **kwargs) -> tf.Tensor:
---> 66     return self.final_projector(tf.concat([head(x) for head in self.heads], axis=-1))

File ~/my_super_project/models.py:48, in Attention.call(self, x, *args, **kwargs)
     46 values = self.value_projector(values)
     47 print('keys, queries', tf.shape(keys), tf.shape(queries))
---> 48 logits = tf.matmul(keys, queries, transpose_b=True)
     49 print('logits', tf.shape(logits))
     50 weights = tf.nn.softmax(logits)

ValueError: Exception encountered when calling layer ""attention_52"" (type Attention).

Error in map_fn:
  Expected `fn` to return a:
    RaggedTensorSpec(TensorShape(None), tf.float32, 1, tf.int32)
  But it returned a:
    RaggedTensorSpec(TensorShape([1, 1]), tf.float32, 1, tf.int64)
    (value=<tf.RaggedTensor [[-0.036843408]]>)
  To fix, update the `fn_output_signature` (or `dtype`) argument to `map_fn`.

Call arguments received by layer ""attention_52"" (type Attention):
   x=('<tf.RaggedTensor [[[-0.99800605, 0.99800605]], [[-0.99800605, 0.99800605],\n                               [-0.99800605, 0.99800605]],\n [[-0.99800605, 0.99800605],\n  [-0.99800605, 0.99800605],\n  [-0.99800605, 0.99800605]]]>', '<tf.RaggedTensor [[[-0.99800605, 0.99800605]], [[-0.99800605, 0.99800605],\n                               [-0.99800605, 0.99800605]],\n [[-0.99800605, 0.99800605],\n  [-0.99800605, 0.99800605],\n  [-0.99800605, 0.99800605]]]>', '<tf.RaggedTensor [[[-0.99800605, 0.99800605]], [[-0.99800605, 0.99800605],\n                               [-0.99800605, 0.99800605]],\n [[-0.99800605, 0.99800605],\n  [-0.99800605, 0.99800605],\n  [-0.99800605, 0.99800605]]]>')
   args=<class 'inspect._empty'>
   kwargs={'training': 'None'}
```
</details>"
56298,tensorflow cannot import name 'dtensor',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

binary

### Tensorflow Version

tf 2.6.0

### Custom Code

No

### OS Platform and Distribution

Linux 

### Mobile device

CentOS Linux release 7.9.2009 (Core)

### Python version

3.6

### Bazel version

4.2

### GCC/Compiler version

9.3

### CUDA/cuDNN version

11.2

### GPU model and memory

d

### Current Behaviour?

```shell
A bug happened!

import tensorflow as tf
print(tf.__version__)
help(tf.keras)
```


### Standalone code to reproduce the issue

```shell
/home/wzy/anaconda3/envs/tensorflow-2.6/bin/python
2.6.0
cannot import name 'dtensor'
```


### Relevant log output

```shell
/home/wzy/anaconda3/envs/tensorflow-2.6/bin/python
2.6.0
cannot import name 'dtensor'
```
</details>"
56297,Extract some of the weights in the saved model and assign them to another,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

2.5

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
How to extract the weights of part of the specified network from the saved model, and assign the extracted weights to another saved model with the same structure.

Also, the point is, how to write the extracted weight into the [variables.data-00000-of-00001] and [variables.index] files.

Thank you very much.
```


### Standalone code to reproduce the issue

```shell
model_1 = tf.keras.models.load_model(model_dir1, compile=False)

for layer in model_1.layers:
    print(layer.name)
    layer.set_weights([weight, bias])
```


### Relevant log output

_No response_</details>"
56296,tf.experimental.set_loop_options - shape invariants fail with TPU strategy,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.8.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tf.experimental.set_loop_options - shape invariants do not work with TPU strategy
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.experimental.TPUStrategy(resolver)

@tf.function(autograph=True)
def f():
  v = tf.constant((0,))
  for i in tf.range(3):
    tf.autograph.experimental.set_loop_options(
        shape_invariants=[(v, tf.TensorShape([None]))]
    )
    v = tf.concat((v, [i]), 0)
  return v

strategy.run(f)
```


### Relevant log output

```shell
InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py in __call__(self, obj)
    697                 type_pprinters=self.type_printers,
    698                 deferred_pprinters=self.deferred_printers)
--> 699             printer.pretty(obj)
    700             printer.flush()
    701             return stream.getvalue()

6 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in shape(self)
   1325         # `_tensor_shape` is declared and defined in the definition of
   1326         # `EagerTensor`, in C.
-> 1327         self._tensor_shape = tensor_shape.TensorShape(self._shape_tuple())
   1328       except core._NotOkStatusException as e:
   1329         raise core._status_to_exception(e) from None

InvalidArgumentError: 9 root error(s) found.
  (0) INVALID_ARGUMENT: {{function_node __inference_tpu_function_230}} Input and output shapes of loop body do not match: (s32[], s32[], s32[], s32[1], s32[], /*index=5*/s32[]) vs. (s32[], s32[], s32[], s32[2], s32[], /*index=5*/s32[])

	 [[{{function_node __inference_f_202}}{{node while}}]]
	 [[TPUReplicate/_compile/_17449455679645184466/_2]]
  (1) INVALID_ARGUMENT: {{function_node __inference_tpu_function_230}} Input and output shapes of loop body do not match: (s32[], s32[], s32[], s32[1], s32[], /*index=5*/s32[]) vs. (s32[], s32[], s32[], s32[2], s32[], /*index=5*/s32[])

	 [[{{function_node __inference_f_202}}{{node while}}]]
	 [[TPUReplicate/_compile/_17449455679645184466/_2]]
	 [[tpu_compile_succeeded_assert/_5804319525153755267/_3/_35]]
  (2) INVALID_ARGUMENT: {{function_node __inference_tpu_function_230}} Input and output shapes of loop body do not match: (s32[], s32[], s32[], s32[1], s32[], /*index=5*/s32[]) vs. (s32[], s32[], s32[], s32[2], s32[], /*index=5*/s32[])

	 [[{{function_node __inference_f_202}}{{node while}}]]
	 [[TPUReplicate/_compile/_17449455679645184466/_2]]
	 [[cluster_tpu_function/control_after/_1/_99]]
  (3) INVALID_ARGUMENT: {{function_node __inference_tpu_function_230}} Input and output shapes of loop body do not match: (s32[], s32[], s32[], s32[1], s32[], /*index=5*/s32[]) vs. (s32[], s32[], s32[], s32[2], s32[], /*index=5*/s32[])

	 [[{{function_node __inference_f_202}}{{node while}}]]
	 [[TPUReplicate/_compile/_17449455679645184466/_2]]
	 [[cluster_tpu_function/control_after/_1/_95]]
  (4) INVALID_ARGUMENT: {{function_node __inference_tpu_function_230}} Input and output shapes of loop body do not match: (s32[], s32[], s32[], s32[1], s32[], /*index=5*/s32[]) vs. (s32[], s32[], s32[], s32[2], s32[], /*index=5*/s32[])

	 [[{{function_node __inference_f_202}}{{node while}}]]
	 [[TPUReplicate/_compile/_17449455679645184466/_2]]
	 [[cluster_tpu_function/control_after/_1/_91]]
  (5) INVALID_ARGUMENT: {{function_node __inference_tpu_function_230}} Input and output shapes of loop body do not match: (s32[], s32[], s32[], s32[1], s32[], /*index=5*/s32[]) vs. (s32[], s32[], s32[], s32[2], s32[], /*index=5*/s32[])

	 [[{{function_node __inference_f_202}}{{node while}}]]
	 [[TPUReplicate/_compile/_17449455679645184466/_2]]
	 [[cluster_tpu_function/control_after/_1/_83]]
  (6) INVALID_ARGUMENT: {{function_node __inference_tpu_function_230}} Input and output shapes of loop body do not match: (s32[], s32[], s32[], s32[1], s32[], /*index=5*/s32[]) vs. (s32[], s32[], s32[], s32[2], s32[], /*index=5*/s32[])

	 [[{{function_node __inference_f_202}}{{node while}}]]
	 [[TPUReplicate/_compile/_17449455679645184466/_2]]
	 [[cluster_tpu_function/control_after/_1/_111]]
  (7) INVALID_ARGUMENT: {{function_node __inference_tpu_function_230}} Input and output shapes of loop body do not match: (s32[], s32[], s32[], s32[1], s32[], /*index=5*/s32[]) vs. (s32[], s32[], s32[], s32[2], s32[], /*index=5*/s32[])

	 [[{{function_node __inference_f_202}}{{node while}}]]
	 [[TPUReplicate/_compile/_174 ... [truncated]
```
</details>"
56294,How to calculate 45 degree standing position of body from camera in swift (Pose estimation),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

pod 'TensorFlowLiteSwift', '~> 0.0.1-nightly', :subspecs => ['CoreML', 'Metal']

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
How to calculate 45 degree standing position of body from camera in swift.
```


### Standalone code to reproduce the issue

```shell
How to calculate 45 degree standing position of body from camera in swift using the body keypoints. (Pose estimation)
```


### Relevant log output

_No response_</details>"
56293,Unable to batch dataset using `.batch` and `.padded_batch`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

macOS Monterey 12.3.1

### Mobile device

_No response_

### Python version

3.10.4

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I'm writing some variable length string feature to tfrecord. If the feature has the same shape for all examples, it runs perfectly fine without problems. If the shape varies, the error below is raised whenever the created tfrecord is being read.
```


### Standalone code to reproduce the issue

```shell
import random

import numpy as np
import tensorflow as tf


def serialize_example(writer):
    # s = np.array(['aaa' for _ in range(10)])  # this works fine
    s = np.array(['aaa' for _ in range(random.randint(1, 100))])
    features = {
        'f1': tf.train.Feature(
            bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(s).numpy()])
        )
    }
    example = tf.train.Example(features=tf.train.Features(feature=features))
    writer.write(example.SerializeToString())


def create_tfrecord(output_path):
    with tf.io.TFRecordWriter(output_path) as writer:
        for i in range(total := 100):
            print(f'\rWriting example: {i + 1}/{total}', end='')
            serialize_example(writer)


def read_example(example, feature_map):
    features = tf.io.parse_single_example(example, feature_map)
    f1 = tf.sparse.to_dense(features['f1'])
    f1 = tf.io.parse_tensor(f1[0], tf.string)
    return f1


def read_tfrecord(fp, batch_size):
    files = tf.data.Dataset.list_files(fp)
    dataset = files.flat_map(tf.data.TFRecordDataset)
    feature_map = {
        'f1': tf.io.VarLenFeature(tf.string),
    }
    return dataset.map(
        lambda x: read_example(x, feature_map),
        tf.data.experimental.AUTOTUNE,
    ).batch(batch_size)  # if this is removed, both cases work fine


if __name__ == '__main__':
    create_tfrecord('xyz.tfrecord')
    dataset = read_tfrecord('xyz.tfrecord', 8)
    sample = dataset.take(1).as_numpy_iterator().next()
```


### Relevant log output

```shell
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [83], [batch]: [32] [Op:IteratorGetNext]
```
</details>"
56292,Error in Ragged Tensor Documentation,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Source

source

### Tensorflow Version

2.9

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When I run the example in the doc to create a RaggedTensor t4 with shape [3, None, 4, 8, None, 2] I get an error
```


### Standalone code to reproduce the issue

```shell
t0 = tf.zeros([1000, 2])                           # Shape:         [1000, 2]
t1 = RaggedTensor.from_row_lengths(t0, [...])      #           [160, None, 2]
t2 = RaggedTensor.from_uniform_row_length(t1, 8)   #         [20, 8, None, 2]
t3 = RaggedTensor.from_uniform_row_length(t2, 4)   #       [5, 4, 8, None, 2]
t4 = RaggedTensor.from_row_lengths(t3, [...])      # [3, None, 4, 8, None, 2]
```
```


### Relevant log output


ValueError: Attempt to convert a value (Ellipsis) with an unsupported type (<class 'ellipsis'>) to a Tensor.

</details>"
56291,Using mkl on the tensorflow-2.8.0,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf 2.8

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu-20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

4.2.1

### GCC/Compiler version

gcc 9

### CUDA/cuDNN version

N/A

### GPU model and memory

Intel UHD grahics

### Current Behaviour?

```shell
I built tensorflow-2.8.0 with bazel to get libtensorflow.tar.gz in the Ubuntu-20.04 and I select mkl when config tensorflow to improve performance when I run my application with CPU ONLY. My target system is Intel N5095 CPU computor I think mkl would support this CPU also.

When I run my application, I got an messages as following which I did not see it till now when I deselect mkl option in the config stage.

Once I ignored this message and run my application, processing time that I got result is nearly twice.

Can you let me know following message means and how can I avoid it?
```


### Standalone code to reproduce the issue

```shell
If I did not use mkl, such a message are gone. 

I can reproduce it with my application but, I did not refine it to reproduce. so, If you need it exactly, I will find a way.

BR,
Mark
```


### Relevant log output

```shell
2022-05-27 10:08:06.978017: E tensorflow/core/framework/node_def_util.cc:630] NodeDef mentions attribute epsilon which is not in the op definition: Op<name=_MklFusedBatchMatMulV2; signature=x:T, y:T, args:num_args*T -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_FLOAT]; attr=adj_x:bool,default=false; attr=adj_y:bool,default=false; attr=num_args:int,min=0; attr=fused_ops:list(string),default=[]> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node encoder/layer_._0/attention/self/add}}
2022-05-27 10:08:12.289880: E tensorflow/core/framework/node_def_util.cc:630] NodeDef mentions attribute epsilon which is not in the op definition: Op<name=_MklFusedBatchMatMulV2; signature=x:T, y:T, args:num_args*T -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_FLOAT]; attr=adj_x:bool,default=false; attr=adj_y:bool,default=false; attr=num_args:int,min=0; attr=fused_ops:list(string),default=[]> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node encoder/layer_._0/attention/self/add}}
```
</details>"
56290,Import Locally Cause Resource Leak,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.6.3

### Custom Code

No

### OS Platform and Distribution

X86 linux

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When the tensorflow is imported locally in a function, it seems the traceback will be stored in tensorflow, and the local variable is not released after the function exit, we found this issue in our RPC test case, it will cause the connection timeout actually the job is already completed.
```


### Standalone code to reproduce the issue

```shell
class A:
    def __init__(self, name):
        self._name = name
    def __del__(self):
        print(f""A obj {self._name} is delete"")


def _run_rpc():
    a = A(""a"")
    import tensorflow
    b = A(""b"")


if __name__ == ""__main__"":
    _run_rpc()
    print(""another start."")
```


### Relevant log output

```shell
another start.
A obj a is delete
A obj b is delete

The expect ouput:
A obj a is delete
A obj b is delete
another start.
```
</details>"
56289,Issue created for Rollback of PR #55619: Use faster sort in TopK kernel when num_rows==1,"Merged PR #55619 is rolled back in 01ae282c1858667bd2ef56415557fd1a2ad64949.
    Please follow up with the reviewer and close this issue once its resolved."
56288,How to decode numpy ndarray stored as `tf.train.BytesList`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

macOS Monterey 12.3.1

### Mobile device

_No response_

### Python version

3.10.4

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I have a numpy array saved as bytes in tf.train.BytesList, I need to read it back to its initial form. In the code below, I need to modify `read_example` to restore and return string tensors in their initial form. In other words, the 2 print statements below should print the same thing.
```


### Standalone code to reproduce the issue

```shell
import random
import string

import numpy as np
import tensorflow as tf


def read_example(example, feature_map):
    features = tf.io.parse_single_example(example, feature_map)
    return tf.sparse.to_dense(features['example'])


if __name__ == '__main__':
    s = string.ascii_letters + '0123456789'
    a = np.array(
        [
            [''.join(random.sample(s, random.randint(10, 20))) for _ in range(3)]
            for _ in range(5)
        ]
    )
    print(a)
    with tf.io.TFRecordWriter(fp := 'example.tfrecord') as writer:
        features = {
            'example': tf.train.Feature(
                bytes_list=tf.train.BytesList(value=[a.tobytes()])
            )
        }
        example = tf.train.Example(features=tf.train.Features(feature=features))
        writer.write(example.SerializeToString())

    files = tf.data.Dataset.list_files(fp)
    dataset = files.flat_map(tf.data.TFRecordDataset)
    feature_map = {
        'example': tf.io.VarLenFeature(tf.string),
    }
    dataset = dataset.map(
        lambda x: read_example(x, feature_map),
        tf.data.experimental.AUTOTUNE,
    )
    sample = dataset.take(1).as_numpy_iterator().next()
    print(sample)
```


### Relevant log output

```shell
[['KxaGbDU591MFClZh3A' 'jak041rhv6gfzt' 'X9mHsecvJlg']
 ['Z5hpUVt09GSYfqIuMH' 'mFT7MNs0qZI' 'zFP2AjxGdlfEu7y05TU']
 ['2IeMXK6iP1bCBZUT' 'J2qoWiNIkXrT84FpKhQm' 'vgrpTP5Qis8yLGDMFRWd']
 ['iwIjS8Q1JW2pkTg' '2Ul3vufwy5XbaGSxo9ph' 'tquXfzbA5lKgpaC']
 ['0nFTbRrSkBpsDVMYi4' 'vqbf6x3Dm54wYlR' 'QUA3vPoZSha']]
[b'K\x00\x00\x00x\x00\x00\x00a\x00\x00\x00G\x00\x00\x00b\x00\x00\x00D\x00\x00\x00U\x00\x00\x005\x00\x00\x009\x00\x00\x001\x00\x00\x00M\x00\x00\x00F\x00\x00\x00C\x00\x00\x00l\x00\x00\x00Z\x00\x00\x00h\x00\x00\x003\x00\x00\x00A\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00j\x00\x00\x00a\x00\x00\x00k\x00\x00\x000\x00\x00\x004\x00\x00\x001\x00\x00\x00r\x00\x00\x00h\x00\x00\x00v\x00\x00\x006\x00\x00\x00g\x00\x00\x00f\x00\x00\x00z\x00\x00\x00t\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00X\x00\x00\x009\x00\x00\x00m\x00\x00\x00H\x00\x00\x00s\x00\x00\x00e\x00\x00\x00c\x00\x00\x00v\x00\x00\x00J\x00\x00\x00l\x00\x00\x00g\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00Z\x00\x00\x005\x00\x00\x00h\x00\x00\x00p\x00\x00\x00U\x00\x00\x00V\x00\x00\x00t\x00\x00\x000\x00\x00\x009\x00\x00\x00G\x00\x00\x00S\x00\x00\x00Y\x00\x00\x00f\x00\x00\x00q\x00\x00\x00I\x00\x00\x00u\x00\x00\x00M\x00\x00\x00H\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00m\x00\x00\x00F\x00\x00\x00T\x00\x00\x007\x00\x00\x00M\x00\x00\x00N\x00\x00\x00s\x00\x00\x000\x00\x00\x00q\x00\x00\x00Z\x00\x00\x00I\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00z\x00\x00\x00F\x00\x00\x00P\x00\x00\x002\x00\x00\x00A\x00\x00\x00j\x00\x00\x00x\x00\x00\x00G\x00\x00\x00d\x00\x00\x00l\x00\x00\x00f\x00\x00\x00E\x00\x00\x00u\x00\x00\x007\x00\x00\x00y\x00\x00\x000\x00\x00\x005\x00\x00\x00T\x00\x00\x00U\x00\x00\x00\x00\x00\x00\x002\x00\x00\x00I\x00\x00\x00e\x00\x00\x00M\x00\x00\x00X\x00\x00\x00K\x00\x00\x006\x00\x00\x00i\x00\x00\x00P\x00\x00\x001\x00\x00\x00b\x00\x00\x00C\x00\x00\x00B\x00\x00\x00Z\x00\x00\x00U\x00\x00\x00T\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00J\x00\x00\x002\x00\x00\x00q\x00\x00\x00o\x00\x00\x00W\x00\x00\x00i\x00\x00\x00N\x00\x00\x00I\x00\x00\x00k\x00\x00\x00X\x00\x00\x00r\x00\x00\x00T\x00\x00\x008\x00\x00\x004\x00\x00\x00F\x00\x00\x00p\x00\x00\x00K\x00\x00\x00h\x00\x00\x00Q\x00\x00\x00m\x00\x00\x00v\x00\x00\x00g\x00\x00\x00r\x00\x00\x00p\x00\x00\x00T\x00\x00\x00P\x00\x00\x005\x00\x00\x00Q\x00\x00\x00i\x00\x00\x00s\x00\x00\x008\x00\x00\x00y\x00\x00\x00L\x00\x00\x00G\x00\x00\x00D\x00\x00\x00M\x00\x00\x00F\x00\x00\x00R\x00\x00\x00W\x00\x00\x00d\x00\x00\x00i\x00\x00\x00w\x00\x00\x00I\x00\x00\x00j\x00\x00\x00S\x00\x00\x008\x00\x00\x00Q\x00\x00\x001\x00\x00\x00J\x00\x00\x00W\x00\x00\x002\x00\x00\x00p\x00\x00\x00k\x00\x00\x00T\x00\x00\x00g\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x002\x00\x00\x00U\x00\x00\x00l\x00\x00\x003\x00\x00\x00v\x00\x00\x00u\x00\x00\x00f\x00\x00\x00w\x00\x00\x00y\x00\x00\x005\x00\x00\x00X\x00\x00\x00b\x00\x00\x00a\x00\x00\x00G\x00\x00\x00S\x00\x00\x00x\x00\x00\x00o\x00\x00\x009\x00\x00\x00p\x00\x00\x00h\x00\x00\x00t\x00\x00\x00q\x00\x00\x00u\x00\x00\x00X\x00\x00\x00f\x00\x00\x00z\x00\x00\x00b\x00\x00\x00A\x00\x00\x005\x00\x00\x00l\x00\x00\x00K\x00\x00\x00g\x00\x00\x00p\x00\x00\x00a\x00\x00\x00C\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x000\x00\x00\x00n\x00\x00\x00F\x00\x00\x00T\x00\x00\x00b\x00\x00\x00R\x00\x00\x00r\x00\x00\x00S\x00\x00\x00k\x00\x00\x00B\x00\x00\x00p\x00\x00\x00s\x00\x00\x00D\x00\x00\x00V\x00\x00\x00M\x00\x00\x00Y\x00\x00\x00i\x00\x00\x004\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00v\x00\x00\x00q\x00\x00\x00b\x00\x00\x00f\x00\x00\x006\x00\x00\x00x\x00\x00\x003\x00\x00\x00D\x00\x00\x00m\x00\x00\x005\x00\x00\x004\x00\x00\x00w\x00\x00\x00Y\x00\x00\x00l\x00\x00\x00R\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00Q\x00\x00\x00U\x00\x00\x00A\x00\x00\x003\x00\x00\x00v\x00\x00\x00P\x00\x00\x00o\x00\x00\x00Z\x00\x00\x00S\x00\x00\x00h\x00\x00\x00a\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00']
```
</details>"
56287,Leak when using Metal delegate,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.7.3

### Custom Code

Yes

### OS Platform and Distribution

iOS 15.5

### Mobile device

iPad

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Whenever we enable Metal delegate, a fast memory leak is observed. Using just CoreML delegate, and the same exact app/code, the memory use is steady.

Most of the leaked memory looks like

_objc_rootAllocWithZone		
-[MTLDebugCommandBuffer computeCommandEncoder]		
0x102775ec8		
0x11379ec3c		
tflite::Subgraph::Invoke()		
tflite::Interpreter::Invoke()		
```

The app is a Swift app, that calls into native C/C++ code. Native C++ threads then run inference calling into TFLite C API.

Any insight into what can be tried is appreciated!
```


### Standalone code to reproduce the issue

```shell
n/a
```


### Relevant log output

_No response_</details>"
56286,Library .so file path Custom Ops using C API,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.8

### Custom Code

Yes

### OS Platform and Distribution

Linux 

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Where is the .so plugin file for ops.h and kernels.h in C API? I am trying to build custom OP using tensorflow C API's and I am facing compilation issues due to path of the headers.
Where are all the headers located?
```


### Standalone code to reproduce the issue

```shell
#include <stdio.h>
#include <tensorflow/c/c_api.h>
#include ""tensorflow/c/kernels.h""x
#include ""tensorflow/c/ops.h""
int main() {
  printf(""Hello from TensorFlow C library version %s\n"", TF_Version());
  return 0;
}
```


### Relevant log output

_No response_</details>"
56285,Tensorflow lite bazel build in docker containers fails,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

Commit b9799cc27df90f05ef30b029f794e962730635e7

### Custom Code

No

### OS Platform and Distribution

Made from [docker file](https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/tools/dockerfiles/tflite-android.Dockerfile) provided in the [official android lite build guide](https://www.tensorflow.org/lite/android/lite_build)

### Mobile device

Building for android_arm64

### Python version

Not relevant

### Bazel version

5.1.1

### GCC/Compiler version

Clang from Android NDK

### CUDA/cuDNN version

Not relevant

### GPU model and memory

Not relevant

### Current Behaviour?

```shell
I am following https://www.tensorflow.org/lite/android/lite_build to build TFLite for Android.

To ensure a reproducible build, I am using the [Docker file provided there](https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/tools/dockerfiles/tflite-android.Dockerfile) to make a container to build in.

After executing `./configure` (see output below), I execute `bazel build -c opt --config android_arm64 //tensorflow/lite:tensorflowlite` to build the C++ library only as I do not need the JNI interface.

I would expect this to produce a `libtensorflowlite.so` file, but the build fails.
```


### Standalone code to reproduce the issue

```shell
Follow https://www.tensorflow.org/lite/android/lite_build:
 - Download https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/tools/dockerfiles/tflite-android.Dockerfile
 - Build image: `docker build . -t tflite-builder -f tflite-android.Dockerfile`
 - Run image: `docker run -it -v $PWD:/host_dir tflite-builder bash`
 - In container: `sdkmanager ""build-tools;${ANDROID_BUILD_TOOLS_VERSION}"" ""platform-tools"" ""platforms;android-${ANDROID_API_LEVEL}""`
 - In container `/tensorflow_src`: `./configure`
  - Please specify the location of python. [Default is /usr/bin/python3]: 
  - Please input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]
  - Do you wish to build TensorFlow with ROCm support? [y/N]: n
  - Do you wish to build TensorFlow with CUDA support? [y/N]: n
  - Do you wish to download a fresh release of clang? (Experimental) [y/N]: n
  - Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: 
  - Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: y

 - In container `/tensorflow_src`: `bazel build -c opt --config android_arm64 //tensorflow/lite:tensorflowlite`

The build (the last command) fails, with the log listed below.

Note that the source code in the Docker container is at commit b9799cc27df. Switching to tag v2.9.1 (d8ce9f9c301) and repeating above steps leads to a **successful* build.
```


### Relevant log output

```shell
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=239
INFO: Reading rc options for 'build' from /tensorflow_src/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /tensorflow_src/.bazelrc:
  'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=true
INFO: Reading rc options for 'build' from /tensorflow_src/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --action_env ANDROID_NDK_HOME=/android/ndk --action_env ANDROID_NDK_API_LEVEL=21 --action_env ANDROID_BUILD_TOOLS_VERSION=30.0.0 --action_env ANDROID_SDK_API_LEVEL=23 --action_env ANDROID_SDK_HOME=/android/sdk
INFO: Reading rc options for 'build' from /tensorflow_src/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /tensorflow_src/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /tensorflow_src/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:android_arm64 in file /tensorflow_src/.bazelrc: --config=android --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a
INFO: Found applicable config definition build:android in file /tensorflow_src/.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --noenable_platform_specific_config --copt=-w --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --define=with_xla_support=false
INFO: Analyzed target //tensorflow/lite:tensorflowlite (83 packages loaded, 8872 targets configured).
INFO: Found 1 target...
ERROR: /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/cpuinfo/BUILD.bazel:100:11: Compiling src/arm/linux/chipset.c failed: (Exit 1): clang failed: error executing command external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin/clang -gcc-toolchain external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/linux-x86_64 -target ... (remaining 51 arguments skipped)
external/cpuinfo/src/arm/linux/chipset.c:330:11: error: implicitly declaring library function 'tolower' with type 'int (int)' [-Werror,-Wimplicit-function-declaration]
                int d = tolower((unsigned char)*a) - tolower((unsigned char)*b);
                        ^
external/cpuinfo/src/arm/linux/chipset.c:330:11: note: include the header <ctype.h> or explicitly provide a declaration for 'tolower'
1 error generated.
Target //tensorflow/lite:tensorflowlite failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 181.172s, Critical Path: 67.90s
INFO: 894 processes: 92 internal, 802 local.
FAILED: Build did NOT complete successfully
```
</details>"
56284,bazel build tensorflow but failed,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf 2.10.0

### Custom Code

No

### OS Platform and Distribution

ubuntu 20.04

### Mobile device

none

### Python version

3.6.9

### Bazel version

5.1.1

### GCC/Compiler version

9.3.0

### CUDA/cuDNN version

none

### GPU model and memory

none

### Current Behaviour?

```shell
A bug happened!
I git clone a copy of latest tensorflow. Then I run 
""bazel build -c opt --config=android_arm64   //tensorflow/lite/examples/label_image:label_image""
Then I encounterd some errors.
```


### Standalone code to reproduce the issue

```shell
I didn't make any change to source code.
```


### Relevant log output

```shell
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/1f06398e96d4508d22f42b760f70eb5d4e7b1dc9.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://github.com/llvm/llvm-project/archive/1f06398e96d4508d22f42b760f70eb5d4e7b1dc9.tar.gz failed: class javax.net.ssl.SSLHandshakeException PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
ERROR: An error occurred during the fetch of repository 'llvm-raw':
   Traceback (most recent call last):
        File ""/home/lvhang/tensorflow/tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl
                ctx.download_and_extract(
Error in download_and_extract: java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/1f06398e96d4508d22f42b760f70eb5d4e7b1dc9.tar.gz, https://github.com/llvm/llvm-project/archive/1f06398e96d4508d22f42b760f70eb5d4e7b1dc9.tar.gz] to /home/lvhang/.cache/bazel/_bazel_lvhang/ce216ca3f79222179e2eb092d40384de/external/llvm-raw/temp14410369499355690125/1f06398e96d4508d22f42b760f70eb5d4e7b1dc9.tar.gz: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
ERROR: /home/lvhang/tensorflow/tensorflow/WORKSPACE:11:14: fetching _tf_http_archive rule //external:llvm-raw: Traceback (most recent call last):
        File ""/home/lvhang/tensorflow/tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl
                ctx.download_and_extract(
Error in download_and_extract: java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/1f06398e96d4508d22f42b760f70eb5d4e7b1dc9.tar.gz, https://github.com/llvm/llvm-project/archive/1f06398e96d4508d22f42b760f70eb5d4e7b1dc9.tar.gz] to /home/lvhang/.cache/bazel/_bazel_lvhang/ce216ca3f79222179e2eb092d40384de/external/llvm-raw/temp14410369499355690125/1f06398e96d4508d22f42b760f70eb5d4e7b1dc9.tar.gz: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
ERROR: no such package '@llvm-raw//utils/bazel': java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/1f06398e96d4508d22f42b760f70eb5d4e7b1dc9.tar.gz, https://github.com/llvm/llvm-project/archive/1f06398e96d4508d22f42b760f70eb5d4e7b1dc9.tar.gz] to /home/lvhang/.cache/bazel/_bazel_lvhang/ce216ca3f79222179e2eb092d40384de/external/llvm-raw/temp14410369499355690125/1f06398e96d4508d22f42b760f70eb5d4e7b1dc9.tar.gz: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
```
</details>"
56283,The _get_tensor_details() method was crashed while trying to read a tflite model.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1 (v2.9.0-18-gd8ce9f9c301)

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04.5 LTS

### Mobile device

Ubuntu 18.04.5 LTS

### Python version

3.8.5

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!

After we construct an tf.lite.Interpreter() with the specified tflite model, we tried to call _get_tensor_details() method to get some tensor details. Just found out it will crash with some specified tflite models.

Please use the code below to reproduce the issue.
```


### Standalone code to reproduce the issue

```shell
import os
import tensorflow as tf

from tf2onnx.tflite.Model import Model

# face_detection_full_range_sparse.tflite could be downloaded from: https://github.com/google/mediapipe/tree/master/mediapipe/modules/face_detection/face_detection_full_range_sparse.tflite
# tflite_path = os.path.join(os.path.dirname(__file__), ""face_detection_full_range_sparse.tflite"")

# pose_detection.tflite could be downloaded from: https://github.com/google/mediapipe/blob/master/mediapipe/modules/pose_detection/pose_detection.tflite
tflite_path = os.path.join(os.path.dirname(__file__), ""pose_detection.tflite"")

with open(tflite_path, 'rb') as f:
    buf = f.read()
buf = bytearray(buf)
model = Model.GetRootAsModel(buf, 0)

tensor_shapes = {}
interpreter = tf.lite.Interpreter(tflite_path)
interpreter.allocate_tensors()
tensor_cnt = model.Subgraphs(0).TensorsLength()

try:
    for i in range(tensor_cnt):
        name = model.Subgraphs(0).Tensors(i).Name().decode()
        details = interpreter._get_tensor_details(i)
        print(""==== name: "", name)
        if ""shape_signature"" in details:
            tensor_shapes[name] = details[""shape_signature""].tolist()
        elif ""shape"" in details:
            tensor_shapes[name] = details[""shape""].tolist()
except Exception as e:
    print(""Error loading model into tflite interpreter: %s"", e)

print(""End"")
```


### Relevant log output

```shell
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
==== name:  input_1
==== name:  model_1/model/zero_padding2d/Pad/paddings
==== name:  model_1/model/zero_padding2d/Pad
==== name:  model_1/model/batch_normalization/FusedBatchNormV3
==== name:  model_1/model/batch_normalization/FusedBatchNormV3_dequantize
==== name:  model_1/model/conv2d/Conv2D
==== name:  model_1/model/conv2d/Conv2D_dequantize
==== name:  model_1/model/re_lu/Relu6;model_1/model/batch_normalization/FusedBatchNormV3;model_1/model/batch_normalization_1/FusedBatchNormV3;model_1/model/depthwise_conv2d/depthwise;model_1/model/regressor_person_16_NO_PRUNING/Conv2D;model_1/model/conv2d/Conv2D
==== name:  model_1/model/batch_normalization_1/FusedBatchNormV3
==== name:  model_1/model/batch_normalization_1/FusedBatchNormV3_dequantize
==== name:  model_1/model/batch_normalization_1/FusedBatchNormV3;model_1/model/depthwise_conv2d/depthwise;model_1/model/regressor_person_16_NO_PRUNING/Conv2D
==== name:  model_1/model/batch_normalization_1/FusedBatchNormV3;model_1/model/depthwise_conv2d/depthwise;model_1/model/regressor_person_16_NO_PRUNING/Conv2D_dequantize
==== name:  model_1/model/re_lu_1/Relu6;model_1/model/batch_normalization_1/FusedBatchNormV3;model_1/model/depthwise_conv2d/depthwise;model_1/model/regressor_person_16_NO_PRUNING/Conv2D
==== name:  model_1/model/batch_normalization_2/FusedBatchNormV3;model_1/model/conv2d_1/Conv2D
==== name:  model_1/model/batch_normalization_2/FusedBatchNormV3;model_1/model/conv2d_1/Conv2D_dequantize
Segmentation fault (core dumped)
```
</details>"
56282,Gets stuck in tensorflow.test.is_gpu_available(),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tensorflow_gpu-1.4.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04.5 LTS

### Mobile device

_No response_

### Python version

2.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
# python
Python 2.7.12 (default, Nov 19 2016, 06:48:10)
[GCC 5.4.0 20160609] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> tf.test.is_gpu_available()
2022-05-27 04:09:29.325229: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA

GET STUCK...

Normally it should return msg like ""Found device 0 with properties:""
```


### Standalone code to reproduce the issue

```shell
# python
Python 2.7.12 (default, Nov 19 2016, 06:48:10)
[GCC 5.4.0 20160609] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> tf.test.is_gpu_available()
```


### Relevant log output

```shell
Fri May 27 04:15:13 2022
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla M60           Off  | 00000001:00:00.0 Off |                  Off |
| N/A   41C    P8    22W / 150W |      8MiB /  8129MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  Tesla M60           Off  | 00000002:00:00.0 Off |                  Off |
| N/A   34C    P8    15W / 150W |      3MiB /  8129MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A     50650      C   python                              2MiB |
|    0   N/A  N/A    100924      C   python                              2MiB |
+-----------------------------------------------------------------------------+
```
</details>"
56280,Potential race condition in nsync library that causes SIGSEGV or deadlock,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.4

### Custom Code

No

### OS Platform and Distribution

Linux Redhat 7.6

### Mobile device

_No response_

### Python version

3.7

### Bazel version

3.1.0

### GCC/Compiler version

7

### CUDA/cuDNN version

11.0/8.0.2

### GPU model and memory

_No response_

### Current Behaviour?
Our TensorFlow application which uses [nsync](https://github.com/google/nsync) for mutex/cv implementation failed because of SIGSEGV. The issue can be reproduced during thread pool destruction with thread-local [TraceMeRecorder::ThreadLocalRecorderWrapper](https://github.com/tensorflow/tensorflow/blob/efa65ea9f1fa3d0baac82366f4405f2470797e04/tensorflow/core/profiler/backends/cpu/traceme_recorder.cc#L282) used in each thread. After adding logs into nsync library, we are able to identify the root cause.

#### Root cause
Destruction order of thread-local variables is not deterministic in [POSIX](https://linux.die.net/man/3/pthread_key_create). Mutex or conditional variable in TensorFlow is a thread local object. If another thread local object e.g. per-thread TraceMe recorder use a lock in its destructor, the lock can already be destructed and the behavior is undefined.

#### Details
For example, the following logs show two threads(0x7f4118748700 and 0x7f4100718700) share the same nsync waiter object (0x7f465c2c2290) and cause SIGSEGV. A nsync waiter object is a thread local object used by nsync to implement a lock. First, the waiter_destroy i.e. lock destructor was called on thread 0x7f4118748700 and the waiter 0x7f465c2c2290 had been recycled into free_waiters. At this moment, the other thread 0x7f4100718700 enter and take the waiter from free_waiters. Then, the old thread 0x7f4118748700 tries to take a lock again as the other thread-local object use a lock in its destructor. Because the thread-local state is not cleaned up in the old thread 0x7f4118748700, it assumes the waiter object is still reserved by itself. It ends up with two threads hold the same waiter object. The object was freed twice and failed the assertion in nsync_waiter_free_.

```shell
[nsync] waiter_destroy(internal/common.c#L150) tid 0x7f4118748700, w 0x7f465c2c2290
[nsync] nsync_waiter_new_(internal/common.c#L182) tid 0x7f4100718700, w 0x7f465c2c2290
[nsync] nsync_waiter_new_(internal/common.c#L171) tid 0x7f4118748700, tw 0x7f465c2c2290
[nsync] nsync_waiter_new_((internal/common.c#L206) tid 0x7f4100718700, w 0x7f465c2c2290
[nsync] nsync_mu_lock_slow_(internal/mu.c#L102) tid 0x7f4118748700, mu 0x12482b40, waiter 0x7f465c2c2290
[nsync] nsync_mu_lock_slow_(internal/mu.c#L71) tid 0x7f4100718700, mu 0x7f4ae4593f30, waiter 0x7f465c2c2290
[nsync] nsync_mu_lock_slow_(internal/mu.c#L71) tid 0x7f4118748700, mu 0x12482b40, waiter 0x7f465c2c2290
# A fatal error has been detected by the Java Runtime Environment:
#
# SIGSEGV (0xb) at pc=0x00007f4af0e74eda, pid=6, tid=0x00007f4118748700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_172-b11) (build 1.8.0_172-b11)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.172-b11 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C [pywrap_tensorflow_internal.so+0xc8cbeda] nsync::nsync_waiter_free(nsync::waiter*)+0xa
#
# Core dump written. Default location: /opt/code-fetcher-system/src/core or core.6
#
# An error report file with more information is saved as:
# /opt/code-fetcher-system/src/hs_err_pid6.log
```
Deadlock can also happen when the waiter is moved to another mutex before it got notified. For example, two threads above were locked on two different mutex (0x12482b40 and 0x7f4ae4593f30). In this case, the waiter semaphore is decremented twice but only incremented once.

P.S. The thread-local variable that access locks in its destructor in TensorFlow is the per-thread TraceMeRecorder::ThreadLocalRecorderWrapper used in TensorFlow profiler.

### Standalone code to reproduce the issue

N/A

The issue was fixed in nsync upstream. See [PR](https://github.com/google/nsync/pull/12) for more details. We need help from Google to release a new nsync version and upgrade nsync version in TensorFlow. Thank you so much!

### Relevant log output

_No response_</details>"
56279,Bad file descriptor error during cleanup of MirroredStrategy model,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.8.0

### Custom Code

No

### OS Platform and Distribution

Linux Red Hat

### Mobile device

_No response_

### Python version

3.9.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.1 / cuDNN 8.2.1.32

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Creating a model within the scope of a MirroredStrategy results in an OSError during memory cleanup at the end of the program. I would expect it to run without an error.

Others seem to have encountered a similar issue:
- https://stackoverflow.com/questions/66954494/oserror-errno-9-bad-file-descriptor-in-tensorflow-estimater-when-deploying-mo
- https://sm-stackoverflow.azurefd.net/questions/71965552/problem-when-saving-model-after-mirroredstrategy-in-keras

I encountered this error following the example at
https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_keras_modelfit.
After getting the error, I trimmed it down to the included minimal example.
```


### Standalone code to reproduce the issue

```shell
'''
Environment setup
>> conda create -n test_tf -c conda-forge python=3.9.12 tensorflow=2.8.0 cudnn=8.2.1.32
>> export CUDA_VISIBLE_DEVICES=""""
'''
import os
os.environ['CUDA_VISIBLE_DEVICES'] = ""0""

import tensorflow as tf
from tensorflow import keras

def main():
    print('GPUs:', tf.config.list_physical_devices('GPU'))
    with tf.distribute.MirroredStrategy().scope():
        model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])
    print('Output shape:', model.output_shape)

if __name__ == '__main__':
    main()
```


### Relevant log output

```shell
GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
2022-05-21 14:16:41.133359: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-21 14:16:41.633403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0
Output shape: (None, 1)
Exception ignored in: <function Pool.__del__ at 0x2aabb1d85670>
Traceback (most recent call last):
  File ""~/.conda/envs/test_tf/lib/python3.9/multiprocessing/pool.py"", line 268, in __del__
    self._change_notifier.put(None)
  File ""~/.conda/envs/test_tf/lib/python3.9/multiprocessing/queues.py"", line 378, in put
    self._writer.send_bytes(obj)
  File ""~/.conda/envs/test_tf/lib/python3.9/multiprocessing/connection.py"", line 205, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File ""~/.conda/envs/test_tf/lib/python3.9/multiprocessing/connection.py"", line 416, in _send_bytes
    self._send(header + buf)
  File ""~/.conda/envs/test_tf/lib/python3.9/multiprocessing/connection.py"", line 373, in _send
    n = write(self._handle, buf)
OSError: [Errno 9] Bad file descriptor
```
</details>"
56278,Create a rugged tensor in golang,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

latest or any

### Custom Code

No

### OS Platform and Distribution

any

### Mobile device

any

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I'm currently not able to create a rugged tensor from golang. I wonder if there's a way to do it or if there's gonna be
```


### Standalone code to reproduce the issue

```shell
if you create a newTensor right now, all elements in the lists needs to be the same size
```


### Relevant log output

_No response_</details>"
56277,Is it possible to view or dump full tensor values in Debugger V2 GUI (in TensorBoard)?,"I am trying to dump/view intermediate tensor values in a model created using the [TF2 ObjectDetector API](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2.md) 

I am trying to use the `tf.debugging.experimental.enable_dump_debug_info` for this as follows:
```
import argparse
import sys
import os
import pathlib

import absl
import tensorflow as tf
import numpy as np

from object_detection.builders import model_builder
from object_detection.utils import config_util

tf.debugging.experimental.enable_dump_debug_info(
        './dumpdir',
        tensor_debug_mode='SHAPE',
        circular_buffer_size=-1)
  
model_name = 'ssd_mobilenet_v2_320x320_coco17_tpu-8'
pipeline_config = os.path.join('./TFObjDet/models/research/object_detection/configs/tf2/',
                              model_name + '.config')
model_dir = './TFObjDet/models/research/object_detection/test_data/checkpoint/'
configs = config_util.get_configs_from_pipeline_file(pipeline_config)
model_config = configs['model']
detection_model = model_builder.build(model_config=model_config, is_training=False)

  @tf.function
  def detect_fn(model, image):
    """"""Detect objects in image.""""""
    
    image, shapes = model.preprocess(image)
    prediction_dict = model.predict(image, shapes)
    detections = model.postprocess(prediction_dict, shapes)
    
    return detections, prediction_dict, tf.reshape(shapes, [-1])

# Evaluation Step
X = np.ones([1, 320, 320, 3])
image = tf.convert_to_tensor(X, dtype=tf.float32)
detections, predictions_dict, shapes = detect_fn(detection_model, image)
print(""Model finished executing"")
```

I understand that none of the 5 `tensor_debug_mode` options allow for displaying or dumping tensors produced during the Graph execution. At most we can see the datatype, dimesions, shape and size of these tensors. For example, in the image below, we can see these 4 attributes of the tensor `Postprocessor/Decode/get_center_coordinates_and_sizes/sub_1:0`, (which are: float32 1D shape:[1917]  size:1917) :

![image](https://user-images.githubusercontent.com/4872638/170533762-d4a5f3be-efe4-454c-bac8-ec79bde9706e.png)

I am wondering if it is possible to somehow view or dump the actual value of these tensors, i.e.:

1. Is there a way to view or dump the 1917 numeric values present in the tensor: `Postprocessor/Decode/get_center_coordinates_and_sizes/sub_1:0` ? 
2. Is it  possible to do this using the Debugger V2 and TensorBoard?
3. Is it possible to do this in some other way, without modifying the `object_detection` API code?

https://github.com/tensorflow/tensorflow/blob/d8ce9f9c301d021a69953134185ab728c1c248d3/tensorflow/python/debug/lib/dumping_callback.py#L683-L871"
56273,"i can use interpreter.invoke() a tflite well, but edgetpu_compiler show errors about dynamic-sized tensor.","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.3

### Custom Code

Yes

### OS Platform and Distribution

windows 10

### Mobile device

edge-tpu

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
i have a tflite file(yolov4.tflite) that runs well in PC with the interpreter invoke(),


but when i edgetpu-compiler the tflite file~~
it shows that the model has graph with dynamic-sized tensor...
i already know the limit of edge-tpu not support dynamic-sized tensor, but how do i know where the dynamic-sized tensor is~~~

`edgetpu_compiler  yolov4.tflite`

it shows

does it mean that even interpreter invoke runs well, the edge-tpu hardware may encounter other problem? like graph with  dynamic-sized tensors~~

my edgetpu runtime version is 14, edgetpu-compiler version is 16 , they match well.

MY QUESTION IS how do i know where the dynamic-sized tensor is~~~ why the interpreter invoke() runs well, but the compiler suddently shows error during compiling and gives no information where the bug is~~
```


### Standalone code to reproduce the issue

```shell
you can download my yolov4.tflite here:
https://drive.google.com/file/d/16qkplaBXlHS9oy1qsq3peFadoc_VmuHB/view

the python code runs well in PC with the interpreter invoke() is :
# ===========================================================
interpreter = tf.lite.Interpreter(model_path=tflite_model_filename)
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Explore the model input output shape.
print(input_details)
print(output_details)
input_shape = input_details[0]['shape']
print(""input_shape is "",input_shape)

for i, output_detail in enumerate(output_details):
    output_shape = output_detail['shape']
    print(""No {} output_shape is {}"".format(i,output_shape))

# Test the model on random input data.
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)
print(""feeding model with input_shape is "",input_data.shape)

interpreter.invoke()
# The function `get_tensor()` returns a copy of the tensor data.
# Use `tensor()` in order to get a pointer to the tensor.
for i in range(len(output_details)):
    output_data_tmp = interpreter.get_tensor(output_details[i]['index'])
    output_shape_tmp = output_data_tmp.shape
    print(""inference done! No {} output_shape is {}"".format(i,output_shape_tmp))
# ===========================================================

the invoke python code gives the output, everything seems ok, the print output is :
# ===========================================================
[{'name': 'input_5', 'index': 0, 'shape': array([  1, 512, 512,   3]), 'shape_signature': array([ -1, 512, 512,   3]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
[{'name': 'Identity', 'index': 804, 'shape': array([ 1, 64, 64,  3, 97]), 'shape_signature': array([-1, 64, 64,  3, 97]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'Identity_1', 'index': 898, 'shape': array([ 1, 16, 16,  3, 97]), 'shape_signature': array([-1, 16, 16,  3, 97]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'Identity_2', 'index': 851, 'shape': array([ 1, 32, 32,  3, 97]), 'shape_signature': array([-1, 32, 32,  3, 97]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
input_shape is  [  1 512 512   3]
No 0 output_shape is [ 1 64 64  3 97]
No 1 output_shape is [ 1 16 16  3 97]
No 2 output_shape is [ 1 32 32  3 97]
feeding model with input_shape is  (1, 512, 512, 3)
inference done! No 0 output_shape is (1, 64, 64, 3, 97)
inference done! No 1 output_shape is (1, 16, 16, 3, 97)
inference done! No 2 output_shape is (1, 32, 32, 3, 97)
# ===========================================================

but when i compile the tflite file as edge-tpu ask, the error shows the tflite file has graph-sized tensor and the compiler stops~~~ the compiler command is below
# ===========================================================
`edgetpu_compiler  yolov4.tflite`
you can use the colab to run the edgetpu_compiler command line as below:
https://colab.research.google.com/drive/1xqLIs5tbfHmguze_9tXDjekvAleFQtyZ
# ===========================================================


the compiler give the following errors:
# ===========================================================

Edge TPU Compiler version 16.0.384591198
Started a compilation timeout timer of 180 seconds.
ERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.
Compilation failed: Model failed in Tflite interpreter. Please ensure model can be loaded/run in Tflite interpreter.
Compilation child process completed within timeout period.
Compilation failed!
```
# ===========================================================
```


### Relevant log output

```shell
tflite file:
https://drive.google.com/file/d/16qkplaBXlHS9oy1qsq3peFadoc_VmuHB/view

edgetpu_compiler colabe:
https://colab.research.google.com/drive/1xqLIs5tbfHmguze_9tXDjekvAleFQtyZ
```
</details>"
56272,TensorArray documentation is misleading,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Source

binary

### Tensorflow Version

tf 2.8

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

The [documentation for TensorArray](https://www.tensorflow.org/api_docs/python/tf/TensorArray) describes it as ""write-once"", but this is incorrect. For example, the code below executes fine.

The documentation should be clearer about whether TensorArray is write-once at all, or if this constraint only applies in certain contexts, such as when the user wishes to backprop through it.


### Standalone code to reproduce the issue

```python
array = tf.TensorArray(tf.float32, dynamic_size=False, size=16)
for i in range(16):
    array = array.write(i, 5.)
array = array.write(15, 6.)  # Write to already-written position
print(array.concat())  # Overwriting the last position completed successfully
```


### Relevant log output

_No response_</details>"
56271,no flatbuffer module errors in source of tensorflow,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf2.8

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When i use the tf-lite c-api to inference my code, it happened!

.../tensorflow/lite/interpreter_builder.h:26: error: flatbuffers/flatbuffers.h: No such file or directory
 #include ""flatbuffers/flatbuffers.h""  // from @flatbuffers
          ^~~~~~~~~~~~~~~~~~~~~~~~~~~
```


### Standalone code to reproduce the issue

```shell
I build the tf-lite.so successfully by cmake, but I make sure there is no flatbuffer module in tf2.8 source code only a cmakeLists.txt, 
as the snapshot
```
![file_image](https://user-images.githubusercontent.com/52693361/170504990-730982ec-e3a7-438a-a361-d38fa8acd4dd.png)


### Relevant log output

_No response_</details>"
56270,"TensorFlow core operator ExtractImagePatches , not supported","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.6.2 /cuDNN 8.4.0

### GPU model and memory

RTX 1660Ti and 16 GB

### Current Behaviour?

```shell
I made a custom  layer which uses ExtractImagePatches TensorFlow operator, but it was not supported by tflite (but listed as supported). I tried to enable the op manually but was unable to find .cc and .h file. It would be great if there is a solution to this problem, since my research depends on it. I am encountering the error when I am converting the model from tf to tflite.
```


### Standalone code to reproduce the issue

```shell
pip install tensorflow_model_optimization

import tensorflow as tf
import tensorflow_model_optimization as tfmot
import matplotlib.pyplot as plt
import numpy as np

from keras.utils import np_utils
from tensorflow.python.keras import activations
from tensorflow.keras.callbacks import EarlyStopping

def myconv2d(ix, w, padding):
   # filter shape: [filter_height, filter_width, in_channels, out_channels]
   # flatten filters
   filter_height = int(w.shape[0])
   filter_width = int(w.shape[1])
   in_channels = int(w.shape[2])
   out_channels = int(w.shape[3])
   ix_height = int(ix.shape[1])
   ix_width = int(ix.shape[2])
   ix_channels = int(ix.shape[3])
   filter_shape = [filter_height, filter_width, in_channels, out_channels]
   flat_w = tf.reshape(w, [filter_height * filter_width * in_channels, out_channels])
   patches = tf.image.extract_patches(
       ix,
       sizes=[1, filter_height, filter_width, 1],
       strides=[1, 1, 1, 1],
       rates=[1, 1, 1, 1],
       padding=padding
   )
   patches_reshaped = tf.reshape(patches, [-1, ix_height, ix_width, filter_height * filter_width * ix_channels])
   feature_maps = []
   for i in range(out_channels):
       feature_map = tf.reduce_sum(tf.multiply(flat_w[:, i], patches_reshaped), axis=3, keepdims=True)
       feature_maps.append(feature_map)
   features = tf.concat(feature_maps, axis=3)
   return features

class MyConv2D(tf.keras.layers.Layer):
    def __init__(self, filters, kernel_size, padding='SAME', **kwargs):
        self.filters = filters
        self.kernel_size = kernel_size
        self.padding = padding
        #self.units= units
        super(MyConv2D, self).__init__(**kwargs)

    def get_config(self):
        config = super().get_config()
        config.update({
            ""filters"": self.filters,
            ""kernel_size"": self.kernel_size,
            ""padding"" : self.padding,
        })
        return config

    def build(self, input_shape):
        # only have a 3x3 kernel
        shape = self.kernel_size + (input_shape[-1], self.filters)
        self.kernel = self.add_weight(name='kernel', shape=shape,
                                      initializer='glorot_uniform', trainable=True)
        self.b = self.add_weight(
            name=""bias"", shape=(self.filters,), initializer=""random_normal"", trainable=True
        )
        #super((MyConv2D, self).build(input_shape))

    def call(self, inputs):
        result = myconv2d(inputs, self.kernel, self.padding) + self.b
        return result

    def compute_output_shape(self, input_shape):
        return input_shape[:-1] + (self.filters,)

def load_dataset():
	# load dataset
	(trainX, trainY), (testX, testY) = tf.keras.datasets.mnist.load_data()
	# reshape dataset to have a single channel
	trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))
	testX = testX.reshape((testX.shape[0], 28, 28, 1))
	# one hot encode target values
	trainY = np_utils.to_categorical(trainY)
	testY = np_utils.to_categorical(testY)
  # convert from integers to floats
	train_norm = trainX.astype('float32')
	test_norm = testX.astype('float32')
	# normalize to range 0-1
	trainX = train_norm / 255.0
	testX = test_norm / 255.0
	# return normalized images
	return trainX, trainY, testX, testY

def create_model():
  # creating a sequantial model
  model = tf.keras.Sequential()
  # adding convolution2D layer to the model of 32 filters of size 3x3
  model.add(MyConv2D(filters=32, kernel_size=(3,3)))
  model.add(tf.keras.layers.Activation(activations.relu))
  # adding a maxpooling 2D layer of size 2x2
  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))
  # adding a Dropout layer
  model.add(tf.keras.layers.Dropout(0.2))
  # adding a Flatten layer
  model.add(tf.keras.layers.Flatten())
  # adding Dense layer with 'relu' activation
  model.add(tf.keras.layers.Dense(100, activation='relu'))
  # adding Dense layer with 'softmax' activation for output
  model.add(tf.keras.layers.Dense(10, activation='softmax'))
  return model

train_images, train_labels,test_images, test_labels= load_dataset()

model = create_model()
# compile model
opt = tf.keras.optimizers.Adam()
model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)

# fit model
history = model.fit(train_images, train_labels, epochs=5, batch_size=32, validation_data=(test_images, test_labels),callbacks=[es])
# evaluate model
scores = model.evaluate(test_images, test_labels, verbose=0)
print(""Accuracy: %.2f%%"" % (scores[1]*100))
# stores scores

quantize_annotate_model = tfmot.quantization.keras.quantize_annotate_model

def representative_dataset():
  for data in tf.data.Dataset.from_tensor_slices((train_images)).batch(1).take(100):
    yield [tf.dtypes.cast(data, tf.float32)]

quant_model = quantize_annotate_model(model)
quant_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
converter = tf.lite.TFLiteConverter.from_keras_model(quant_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset
converter.allow_custom_ops = True
converter.experimental_new_converter =True
quantized_tflite_model = converter.convert()
```


### Relevant log output

```shell
RuntimeError: Failed to initialize op resolver for calibration:
There are unresolved custom ops: [ExtractImagePatches]Encountered unresolved custom op: ExtractImagePatches.
See instructions: https://www.tensorflow.org/lite/guide/ops_customNode number 0 (ExtractImagePatches) failed to prepare.
```
</details>"
56266,New released protobuf +v3.20 causes an error when importing ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.7.1

### Custom Code

No

### OS Platform and Distribution

Linux 20.04

### Mobile device

_No response_

### Python version

3.8.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
After the new protobuf release and installing tensorflow an error appears when importing it.
https://pypi.org/project/protobuf/4.21.0/
```


### Standalone code to reproduce the issue

```shell
After installing tensorflow 2.7.1 running a simple `import tensorflow`
```


### Relevant log output

```shell
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).
```
</details>"
56265,is it possible to calculate Body degree angle using pose estimation,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

pod 'TensorFlowLiteSwift', '~> 0.0.1-nightly', :subspecs => ['CoreML', 'Metal']

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
How to calculate degree of angles using body keyPoints? & Pose angle. ie. 45 degree, 90 degree.
```


### Standalone code to reproduce the issue

```shell
I want to detect human body pose angle is it possible?
```


### Relevant log output

_No response_</details>"
56264,When we will  publish the  rnn_cell dynamic_rnn  lstm build method  in C api and Java api? eager to use,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

2.8

### Custom Code

Yes

### OS Platform and Distribution

mac

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
want to use  lstm in java ,but we not have example
```


### Relevant log output

_No response_</details>"
56262,Getting the bounding box coordinates on tensorflow lite ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

Android

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am trying to get the bounding box coordinates of each detected object.
```


### Standalone code to reproduce the issue

```shell
I have created a Hashmap.
From hashmap I am reading the count of the objects.

classCount.put(recognition.title, classCount.get(recognition.title) + 1);

How could I get the bounding box coordinates ?
```


### Relevant log output

_No response_</details>"
56261,Detecting objects while android phone is locked/sleep,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

Android

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I want to implement a feature where tensorflow lite is still running and detecting objects while the phone is locked, screen is off. I am using ForeGround service but when I lock the phone, the app is not running.
In the Log it shows that the service is running. But whenever I lock the phone the app does not continue to work.
```


### Standalone code to reproduce the issue

```shell
In the examples app I have created a class MyForegroundService.java
public class MyForegroundService extends Service {
@RequiresApi(api = Build.VERSION_CODES.O)
@Override
public int onStartCommand(Intent intent, int flags, int startId) {
    new Thread(
            new Runnable() {
                @Override
                public void run() {
                    while (true) {
                        Log.e(""Service"", ""Service is running..."");
                        try {
                            Thread.sleep(2000);
                        } catch (InterruptedException e) {
                            e.printStackTrace();
                        }
                    }
                }
            }
    ).start();

    final String CHANNELID = ""Foreground Service ID"";
    NotificationChannel channel = new NotificationChannel(
            CHANNELID,
            CHANNELID,
            NotificationManager.IMPORTANCE_LOW
    );

    getSystemService(NotificationManager.class).createNotificationChannel(channel);
    Notification.Builder notification = new Notification.Builder(this, CHANNELID)
            .setContentText(""Service is running"")
            .setContentTitle(""Service is enabled"")
            .setSmallIcon(R.drawable.ic_launcher_background);

    startForeground(1001, notification.build());

    return super.onStartCommand(intent, flags, startId);
}

@Nullable
@Override
public IBinder onBind(Intent intent) {
    return null;
}
}

Then in the file CameraActivity.java, I have added
if(!foregroundServiceRunning()){
  Intent serviceIntent = new Intent(this, MyForegroundService.class); 
  startForegroundService(serviceIntent); 
}
AND
public boolean foregroundServiceRunning(){
ActivityManager activityManager = (ActivityManager) getSystemService(Context.ACTIVITY_SERVICE);
for(ActivityManager.RunningServiceInfo service: activityManager.getRunningServices(Integer.MAX_VALUE)){
    if(MyForegroundService.class.getName().equals(service.service.getClassName())){
      return true;
    }
}
return false;
}

And in the androidmanifest.xml

<uses-permission android:name=""android.permission.FOREGROUND_SERVICE""/>
<service android:name="".MyForegroundService""></service>
```


### Relevant log output

_No response_</details>"
56260,Using front camera for android with tensorflow lite,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

Android

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I want to use my android phones front camera (Samsung galaxy S9). I have toggled the camera to be on the front and also I have removed the mirror effect but still. It does not detect the objects. 
1) It does not detect all of the objects;
2) If it detects some object then it like floats away.

The funny thing is whenever I switch to the back camera. Everything works like a charm, so it is not the models problem that is for sure I think.
```


### Standalone code to reproduce the issue

```shell
So to make it happen.

In CameraActivity.java, line 427
if (facing != null && facing == CameraCharacteristics.LENS_FACING_BACK); //To toggle front camera

In LegacyCameraConnectionFragment.java, line 217
if (ci.facing == CameraInfo.CAMERA_FACING_FRONT) //to toggle front camera

In LegacyCameraConnectionFragment.java, under line 181
Matrix matrix = new Matrix();                         
matrix.setScale(-1, 1);                        
matrix.postTranslate(textureView.getWidth(), 0);  
textureView.setTransform(matrix);   

The whole matrix is to remove the mirror efect on the front camera. 
Now that I have made these changes in the code, the front camera does not want to detect all of the objects, if it detects, then the objects just float. Am I missing something ?
```


### Relevant log output

_No response_</details>"
56259,HtoD copy not fully overlapping with he execute,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

5.0.0

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.5/8.3

### GPU model and memory

V100 32GB

### Current Behaviour?

```shell
Despite the recent work on moving the staging logic to TF from the driver (commits 9a8772d, ad60111, b4b294f, 2803dfc, 4111779 and ecc3db7), the blocking calls seem to still be existent and the execute could not be fully synchronized. This behavior is not consistent and seems to be more frequent in the elater steps.
```


### Standalone code to reproduce the issue

```shell
import os
import datetime
from tqdm import tqdm
import numpy as np
import tensorflow as tf

def main(argv):
  del argv
  @tf.function
  def do_stuff(wmat, tf_var):
    result = tf.matmul(wmat + 1, tf.transpose(wmat))
    error = tf.reduce_mean(tf_var)
    return error, result

  dataset = tf.data.Dataset.range(1000)
  dataset = dataset.map(
      lambda _: tf.random.uniform([10000,1024]),
      num_parallel_calls=tf.data.AUTOTUNE)
  dataset = dataset.apply(tf.data.experimental.prefetch_to_device('/gpu:0'))

  tf_var = tf.Variable(np.zeros(3))
  adam = tf.keras.optimizers.Adam(1e-4)

  for _, wmat in tqdm(enumerate(dataset)):
    with tf.GradientTape() as tape:
      result = do_stuff(wmat, tf_var)
      grads = tape.gradient(result[0], [tf_var])
      adam.apply_gradients(zip(grads, [tf_var]))

if __name__ == ""__main__"":
    main("""")
```


### Relevant log output

_No response_</details>"
56258,"Dataset from generator is far slower than from tensor slices, anything I can improve?","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

binary

### Tensorflow Version

tf 1.15.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When I use `tf.data.Dataset.from_tensor_slices` and `tf.data.Dataset.from_generator` to create the same dataset, I observe when fetching batches, from_generator is much slower than from_tensor_slices (more than 10x slower).
Also the performance of from_generator greatly drops when the number of yield elements increases. (See the details below).

I am using tf1.15, but I tested tf2.8, seems the performance of generator is even worse...

Wondering is it reasonable that Dataset from generator is far slower than from tensor slices? And want some help on how to improve the performance using the generator. If I want to have a dataset with 10 elements for each records, how can I get better performance?

Thanks so much in advance!
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
import time

tf.compat.v1.enable_eager_execution(
    config=None, device_policy=None, execution_mode=None
)

size = 100000
data = np.random.rand(size)

def get_one():
    i = 0
    while i < size:
        yield tuple([data[i]]*10)
        i += 1

# dataset = tf.data.Dataset.from_generator(get_one, output_types=tuple([tf.float32]*10))
dataset = tf.data.Dataset.from_tensor_slices(tuple([data]*10))
dataset = dataset.batch(512)

i = 0
total_time = 0
start = time.time()
for sample in dataset:
    # Performing a training step
    end = time.time()
    used = end - start
    total_time += used
    print(""Get batch time: "", used)
    i += 1
    start = time.time()
print(""Average get batch time: "", total_time / i)
```


### Relevant log output

When using from_tensor_slices, average get batch time:  0.001715s
When using from_generator, average get batch time:  0.10747s [0.14395s for tf2.8]

If I change the generator function as follows and re-run the above program:
```shell
def get_one():
    i = 0
    while i < size:
        for j in range(10):
            res = data[i]
        yield res
        i += 1
```
Namely if I get the value 10 times in the generator, but only yield one element, the performance is much faster compared with yield 10 features: average get batch time:  0.02014s, even though it is still much slower than from_tensor_slices. Why is that? I suppose the computation is the same, the only difference is how many elements I output? (BTW if I use tf2.8, in this case, it is 0.3935s, unreasonable... don't know why...)
"
56257,parameters in XML file comparison,"
 ### Issue Type

Support

### Source

binary

### Tensorflow Version

1.15

### Custom Code

No

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.7.9

### Bazel version

_No response_

### GCC/Compiler version

gcc (MinGW.org GCC-6.3.0-1) 6.3.0

### CUDA/cuDNN version

10.0, V10.0.130

### GPU model and memory

NVIDIA GeForce RTX 2060 6GB


Hello,

i am a bit unsure about the new xml file i created with a labelling tool. the first example is a tool that doesn't support object tracking, meaning you have to mark each object yourself. in the second example (i use) you can see that more parameters are added. now i wanted to know if there can be any problems with the creation of the tfrecord files?

the normal process is:

xml file -> csv file -> tfrecord

the following scripts are used for this:

https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/blob/master/xml_to_csv.py /
https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/blob/master/generate_tfrecord.py

are the parameters ignored by the python script and only read which are defined?


standard xml file:

```
<annotation>
	<folder>train</folder>
	<filename>IMG_2385.JPG</filename>
	<path>C:\tensorflow_cards\train\IMG_2385.JPG</path>
	<source>
		<database>Unknown</database>
	</source>
	<size>
		<width>378</width>
		<height>504</height>
		<depth>3</depth>
	</size>
	<segmented>0</segmented>
	<object>
		<name>king</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>92</xmin>
			<ymin>183</ymin>
			<xmax>223</xmax>
			<ymax>348</ymax>
		</bndbox>
	</object>
</annotation>
```

```
new xml file:

<annotation>
  <folder>image</folder>
  <filename>image_14.jpg</filename>
  <source>
    <database>Unknown</database>
    <annotation>Unknown</annotation>
    <image>Unknown</image>
  </source>
  <size>
    <width>720</width>
    <height>720</height>
    <depth></depth>
  </size>
  <segmented>0</segmented>
  <object>
    <name>person</name>
    <truncated>0</truncated>
    <occluded>0</occluded>
    <difficult>0</difficult>
    <bndbox>
      <xmin>281.22</xmin>
      <ymin>688.92</ymin>
      <xmax>323.7</xmax>
      <ymax>718.45</ymax>
    </bndbox>
    <attributes>
      <attribute>
        <name>rotation</name>
        <value>0.0</value>
      </attribute>
      <attribute>
        <name>track_id</name>
        <value>63</value>
      </attribute>
      <attribute>
        <name>keyframe</name>
        <value>True</value>
      </attribute>
    </attributes>
  </object>
</annotation>
```"
56256,Remove fake quant node,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

binary

### Tensorflow Version

tf2.6

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I got a pb with fake quant nodes. I want to remove it. I just want to use the fp32 model. I found 'drop_fake_quant' flag was removed from TOCO.
What should i do?
```


### Standalone code to reproduce the issue

```shell
I tried use TOCO by tf1.13.1 but it can't work. Here is the log.
```


### Relevant log output

```shell
$ toco --graph_def_file=tflite_graph.pb --output_file=test.tflite --input_format=TENSORFLOW_GRAPHDEF   --output_format=TFLITE   --inference_type=FLOAT --input_shape=1,512,512,3 --input_array=normalized_input_image_tensor --output_arrays=convert_scores,Squeeze --drop_fake_quant
/home/curio/env/tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/curio/env/tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/curio/env/tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/curio/env/tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/curio/env/tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/curio/env/tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
2022-05-25 17:44:43.220138: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2022-05-25 17:44:43.242294: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3699850000 Hz
2022-05-25 17:44:43.242535: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x171d5d0 executing computations on platform Host. Devices:
2022-05-25 17:44:43.242550: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
Traceback (most recent call last):
  File ""/home/curio/env/tf1/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 426, in import_graph_def
    graph._c_graph, serialized, options)  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef mentions attr 'explicit_paddings' not in Op<name=Conv2D; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[""SAME"", ""VALID""]; attr=data_format:string,default=""NHWC"",allowed=[""NHWC"", ""NCHW""]; attr=dilations:list(int),default=[1, 1, 1, 1]>; NodeDef: {{node FeatureExtractor/MobilenetV2/Conv/Conv2D_Fold}}. (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/curio/env/tf1/bin/toco"", line 8, in <module>
    sys.exit(main())
  File ""/home/curio/env/tf1/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py"", line 442, in main    app.run(main=run_main, argv=sys.argv[:1])
  File ""/home/curio/env/tf1/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run        
    _sys.exit(main(argv))
  File ""/home/curio/env/tf1/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py"", line 438, in run_main
    _convert_model(tflite_flags)
  File ""/home/curio/env/tf1/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py"", line 122, in _convert_model
    converter = _get_toco_converter(flags)
  File ""/home/curio/env/tf1/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py"", line 109, in _get_toco_converter
    return converter_fn(**converter_kwargs)
  File ""/home/curio/env/tf1/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 274, in from_frozen_graph
    _import_graph_def(graph_def, name="""")
  File ""/home/curio/env/tf1/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/home/curio/env/tf1/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 430, in import_graph_def
    raise ValueError(str(e))
ValueError: NodeDef mentions attr 'explicit_paddings' not in Op<name=Conv2D; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[""SAME"", ""VALID""]; attr=data_format:string,default=""NHWC"",allowed=[""NHWC"", ""NCHW""]; attr=dilations:list(int),default=[1, 1, 1, 1]>; NodeDef: {{node FeatureExtractor/MobilenetV2/Conv/Conv2D_Fold}}. (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
```
</details>"
56255,tflite nightly/2.9.1 on iOS (CocoaPods) linker/build issue," ### Issue Type

Build/Install

### Source

binary

### Tensorflow Version

2.9.1 and 0.0.1-nightly.20220524

### Custom Code

No

### OS Platform and Distribution

MacOS 12.1, XCode 13.4, CocoaPod 1.11.3

### Mobile device

iOS 15

### Python version

n/a

### Bazel version

n/a

### GCC/Compiler version

n/a

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?


Compiling a simple iOS project with this podfile:

```shell
platform :ios, '15.5'

target 'my-app' do
  use_frameworks!

  # Pods for my-app
  pod 'TensorFlowLiteSwift', '~> 0.0.1-nightly'
  pod 'TensorFlowLiteSelectTfOps', '~> 0.0.1-nightly'
end
```

and adding `-force_load ""$(PROJECT_DIR)Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.xcframework/ios-arm64/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps""`

results in linker error (see below).


### Standalone code to reproduce the issue
n/a


### Relevant log output

```shell
Ld /Users/me/Library/Developer/Xcode/DerivedData/my-app-hfikcxuyugearxghudrfwmrazkdp/Build/Products/Debug-iphoneos/my-app.app/my-app normal (in target 'my-app' from project 'my-app')
    cd /Users/me/dev/cmc/mobile-prototypes/my-app
    /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/clang -target arm64-apple-ios15.5 -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS15.5.sdk -L/Users/me/Library/Developer/Xcode/DerivedData/my-app-hfikcxuyugearxghudrfwmrazkdp/Build/Products/Debug-iphoneos -L/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/swift/iphoneos -L/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS15.5.sdk/usr/lib/swift -F/Users/me/Library/Developer/Xcode/DerivedData/my-app-hfikcxuyugearxghudrfwmrazkdp/Build/Products/Debug-iphoneos -F/Users/me/Library/Developer/Xcode/DerivedData/my-app-hfikcxuyugearxghudrfwmrazkdp/Build/Products/Debug-iphoneos/TensorFlowLiteSwift -F/Users/me/dev/cmc/mobile-prototypes/my-app/Pods/TensorFlowLiteC/Frameworks -F/Users/me/dev/cmc/mobile-prototypes/my-app/Pods/TensorFlowLiteSelectTfOps/Frameworks -F/Users/me/Library/Developer/Xcode/DerivedData/my-app-hfikcxuyugearxghudrfwmrazkdp/Build/Products/Debug-iphoneos/XCFrameworkIntermediates/TensorFlowLiteC/Core -F/Users/me/Library/Developer/Xcode/DerivedData/my-app-hfikcxuyugearxghudrfwmrazkdp/Build/Products/Debug-iphoneos/XCFrameworkIntermediates/TensorFlowLiteSelectTfOps -filelist /Users/me/Library/Developer/Xcode/DerivedData/my-app-hfikcxuyugearxghudrfwmrazkdp/Build/Intermediates.noindex/my-app.build/Debug-iphoneos/my-app.build/Objects-normal/arm64/my-app.LinkFileList -Xlinker -rpath -Xlinker /usr/lib/swift -Xlinker -rpath -Xlinker @executable_path/Frameworks -Xlinker -rpath -Xlinker @loader_path/Frameworks -Xlinker -rpath -Xlinker @executable_path/Frameworks -dead_strip -Xlinker -object_path_lto -Xlinker /Users/me/Library/Developer/Xcode/DerivedData/my-app-hfikcxuyugearxghudrfwmrazkdp/Build/Intermediates.noindex/my-app.build/Debug-iphoneos/my-app.build/Objects-normal/arm64/my-app_lto.o -Xlinker -export_dynamic -Xlinker -no_deduplicate -fembed-bitcode-marker -fobjc-link-runtime -L/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/swift/iphoneos -L/usr/lib/swift -Xlinker -add_ast_path -Xlinker /Users/me/Library/Developer/Xcode/DerivedData/my-app-hfikcxuyugearxghudrfwmrazkdp/Build/Intermediates.noindex/my-app.build/Debug-iphoneos/my-app.build/Objects-normal/arm64/beaker_proto2.swiftmodule -force_load /Users/me/dev/cmc/mobile-prototypes/my-app/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.xcframework/ios-arm64/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps -framework Pods_beaker_proto2 -Xlinker -no_adhoc_codesign -Xlinker -dependency_info -Xlinker /Users/me/Library/Developer/Xcode/DerivedData/my-app-hfikcxuyugearxghudrfwmrazkdp/Build/Intermediates.noindex/my-app.build/Debug-iphoneos/my-app.build/Objects-normal/arm64/my-app_dependency_info.dat -o /Users/me/Library/Developer/Xcode/DerivedData/my-app-hfikcxuyugearxghudrfwmrazkdp/Build/Products/Debug-iphoneos/my-app.app/my-app

ld: warning: Could not find or use auto-linked framework 'TensorFlowLiteC'
Undefined symbols for architecture arm64:
  ""_TfLiteModelCreateFromFile"", referenced from:
      TensorFlowLite.Model.init(filePath: Swift.String) -> TensorFlowLite.Model? in TensorFlowLite(Model.o)
  ""_TfLiteSignatureRunnerGetOutputTensor"", referenced from:
      TensorFlowLite.SignatureRunner.(tensor in _FF0433A885FC97E4F565F3E81A9383E1)(named: Swift.String, withType: TensorFlowLite.TensorType) throws -> TensorFlowLite.Tensor in TensorFlowLite(SignatureRunner.o)
  ""_TfLiteSignatureRunnerGetInputTensor"", referenced from:
      TensorFlowLite.SignatureRunner.copy(_: Foundation.Data, toInputNamed: Swift.String) throws -> () in TensorFlowLite(SignatureRunner.o)
      TensorFlowLite.SignatureRunner.(tensor in _FF0433A885FC97E4F565F3E81A9383E1)(named: Swift.String, withType: TensorFlowLite.TensorType) throws -> TensorFlowLite.Tensor in TensorFlowLite(SignatureRunner.o)

< MANY MORE LINKER ERRORS FOLLOW >
```


This seems similar to: https://github.com/tensorflow/tensorflow/issues/52042#issuecomment-1098076930"
56253,"converter.convert() to a tflite file sucessfully, but interpreter.invoke() raise RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 5 (FlexSize) failed to prepare.","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.3

### Custom Code

Yes

### OS Platform and Distribution

windows 10

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
i have a model contains codes like this:
    grid_size = tf.shape(conv_output)[1:3] # shape=(13,13)
    x_grid,y_grid = tf.meshgrid(tf.range(grid_size[1]), tf.range(grid_size[0]))  # shape=(13,13)
i can generate tflite file sucessfully
but when i open this tflite file, and use interpreter.invoke(), the runtime error occurs:

  File ""D:\Anaconda3\envs\CV_TF23_py37\lib\site-packages\tensorflow\lite\python\interpreter.py"", line 524, in invoke
    self._interpreter.Invoke()

RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 5 (FlexSize) failed to prepare.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
class CFGGG:
    SAVED_MODEL_DIR='./zz_tflite_op_support/demo'
    TFLITE_MODEL_FILENAME='./zz_tflite_op_support/demo.tflite'

inputs=tf.keras.layers.Input((5,5,2))
def pre_process(x):
    grid_x,grid_y=tf.shape(x)[1:3]
    # range_x=tf.range(5) # when replace 5 with grid_xruntime error occurs when invoke
    # range_y=tf.range(5) # when replace 5 with grid_yruntime error occurs when invoke
    range_x=tf.range(grid_x) 
    range_y=tf.range(grid_y) 
    x_grid,y_grid=tf.meshgrid(range_y,range_x)
    
    b=tf.stack([y_grid,x_grid],axis=-1)
    b=tf.cast(b,tf.float32)
    y=x+b
    return y
    
x=pre_process(inputs)
outputs=tf.keras.layers.Dense(10)(x)

model_debug=tf.keras.Model(inputs=inputs,outputs=x)
model_debug.summary()


model_debug.save(CFGGG.SAVED_MODEL_DIR)

converter = tf.lite.TFLiteConverter.from_saved_model(CFGGG.SAVED_MODEL_DIR)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_types = [tf.compat.v1.lite.constants.FLOAT16]
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,
    tf.lite.OpsSet.SELECT_TF_OPS]
converter.allow_custom_ops = True


tflite_model = converter.convert()

with open(CFGGG.TFLITE_MODEL_FILENAME, 'wb') as f:
    f.write(tflite_model)
    
interpreter = tf.lite.Interpreter(model_path=CFGGG.TFLITE_MODEL_FILENAME)
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Explore the model input output shape.
print(input_details)
print(output_details)

input_shape = input_details[0]['shape']
print(""input shape"",input_shape)
for i, output_detail in enumerate(output_details):
    output_shape = output_detail['shape']
    print(""No {} output shape {}"".format(i,output_shape))


# Test the model on random input data.
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)
print(""input data complete: "",input_data.shape)
interpreter.invoke()
# The function `get_tensor()` returns a copy of the tensor data.
# Use `tensor()` in order to get a pointer to the tensor.
for i in range(len(output_details)):
    output_data_tmp = interpreter.get_tensor(output_details[i]['index'])
    output_shape_tmp = output_data_tmp.shape
    print(""invoke() completeNo {} output shape {}"".format(i,output_shape_tmp))
```


### Relevant log output

```shell
INFO:tensorflow:Assets written to: ./zz_tflite_op_support/demo\assets
INFO:tensorflow:Assets written to: ./zz_tflite_op_support/demo\assets
[{'name': 'input_27', 'index': 0, 'shape': array([1, 5, 5, 2]), 'shape_signature': array([-1,  5,  5,  2]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
[{'name': 'Identity', 'index': 25, 'shape': array([1, 5, 5, 2]), 'shape_signature': array([-1,  5,  5,  2]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
input shape [1 5 5 2]
No 0 output shape [1 5 5 2]
intpu data complete: (1, 5, 5, 2)

Traceback (most recent call last):

  File ""D:\OneDrive\AI_Working_Directory\prj_quickstart\zzz_debug_tflite_op_support.py"", line 103, in <module>
    interpreter.invoke()

  File ""D:\Anaconda3\envs\CV_TF23_py37\lib\site-packages\tensorflow\lite\python\interpreter.py"", line 524, in invoke
    self._interpreter.Invoke()

RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 5 (FlexSize) failed to prepare.
```
</details>"
56252,convert to TF-Lite ValueError: Cannot iterate over a shape with unknown rank,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf2.3.1

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
when I convert the savedmodel to tflite model, error happens

ValueError: Cannot iterate over a shape with unknown rank.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model('model') 
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,
    tf.lite.OpsSet.SELECT_TF_OPS
    ]
tflite_model = converter.convert()
with open('rvm.tflite', 'wb') as f:
     f.write(tflite_model)
```


### Relevant log output

```shell
2022-05-25 12:59:38.001545: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory
2022-05-25 12:59:38.001571: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2022-05-25 12:59:39.165540: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2022-05-25 12:59:39.165560: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)
2022-05-25 12:59:39.165578: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (wudi-QiTianM620-N000): /proc/driver/nvidia/version does not exist
2022-05-25 12:59:39.165701: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-25 12:59:39.190199: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2899885000 Hz
2022-05-25 12:59:39.190662: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55dc4ea19680 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-05-25 12:59:39.190693: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-05-25 12:59:46.493553: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2022-05-25 12:59:46.493624: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2022-05-25 12:59:46.608109: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize
2022-05-25 12:59:46.608136: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: Graph size after: 2419 nodes (2082), 6014 edges (5672), time = 50.604ms.
2022-05-25 12:59:46.608141: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: Graph size after: 2419 nodes (0), 6014 edges (0), time = 26.182ms.
2022-05-25 12:59:46.608144: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: __inference_cond_1_true_10408_9212
2022-05-25 12:59:46.608147: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2022-05-25 12:59:46.608150: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.
2022-05-25 12:59:46.608153: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: __inference_cond_true_8440_26709
2022-05-25 12:59:46.608156: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.
2022-05-25 12:59:46.608158: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.
2022-05-25 12:59:46.608161: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: __inference_cond_false_8441_36559
2022-05-25 12:59:46.608164: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2022-05-25 12:59:46.608167: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.
2022-05-25 12:59:46.608170: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: __inference_cond_1_false_10409_21697
2022-05-25 12:59:46.608173: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2022-05-25 12:59:46.608176: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.
Traceback (most recent call last):
  File ""/home/wudi/project/project_rvm/Matting(tensorflow)/convert1.py"", line 11, in <module>
    tflite_model = converter.convert()
  File ""/home/wudi/Software/yes/envs/onnx2tf/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 1076, in convert
    return super(TFLiteConverterV2, self).convert()
  File ""/home/wudi/Software/yes/envs/onnx2tf/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 900, in convert
    self).convert(graph_def, input_tensors, output_tensors)
  File ""/home/wudi/Software/yes/envs/onnx2tf/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 633, in convert
    **converter_kwargs)
  File ""/home/wudi/Software/yes/envs/onnx2tf/lib/python3.6/site-packages/tensorflow/lite/python/convert.py"", line 567, in toco_convert_impl
    input_tensors, output_tensors, *args, **kwargs)
  File ""/home/wudi/Software/yes/envs/onnx2tf/lib/python3.6/site-packages/tensorflow/lite/python/convert.py"", line 458, in build_toco_convert_protos
    for dim in shape:
  File ""/home/wudi/Software/yes/envs/onnx2tf/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py"", line 859, in __iter__
    raise ValueError(""Cannot iterate over a shape with unknown rank."")
ValueError: Cannot iterate over a shape with unknown rank.
```
</details>"
56251,TF Lite Ondevice Training - Training Problem With Fashion MNIST,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

4.2.1

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

-

### GPU model and memory

-

### Current Behaviour?

refer site : https://www.tensorflow.org/lite/examples/on_device_training/overview

I'm Converting model to Tensorflow Lite and Testing on Linux Ubuntu 20.04. Trying Re-Training with signature('train'), Fashion MNIST

I remade the model several times, checked the shape of the data, but it was the same as in Python. However, proceed with the training, I can see the ""nan"" value, incorrect data as shown in the log below. What's the problem?

Here is my Build Command

```shell
bazel build --config=monolithic --config=noaws --config=nogcp --config=nohdfs --config=nonccl  --fat_apk_cpu=x86_64 --experimental_ui_max_stdouterr_bytes=1073741819 -c opt --cxxopt=--std=c++14 //tensorflow/lite:libtensorflowlite.so
```

ADD below to tensorflow/lite/BUILD

```shell
""//tensorflow/lite/kernels:builtin_ops"",
        ""//tensorflow/lite/delegates/flex:delegate"",
        ""//tensorflow/lite/delegates/flex:exported_symbols.lds"",
        ""//tensorflow/lite/delegates/flex:version_script.lds"",
""
```


### Standalone code to reproduce the issue

```shell
#include <cstdio>
#include <string>
#include <cstddef>
#include <cstdint>
#include <iostream>
#include <fstream>
#include <stdio.h>
#include <iomanip>
#include <getopt.h>
#include <ctime>
#include <typeinfo>
#include <algorithm>

#include ""tensorflow/lite/interpreter.h""
#include ""tensorflow/lite/interpreter_builder.h""
#include ""tensorflow/lite/kernels/register.h""
#include ""tensorflow/lite/model.h""
#include ""tensorflow/lite/optional_debug_tools.h""
#include ""tensorflow/lite/signature_runner.h""

#include ""mnist/mnist_reader.hpp""

#define BATCH_SIZE 100
#define EPOCH 100

inline bool is_little_indian() {
	int val = 1;
	return *reinterpret_cast<char *>(&val) != 0;
}

uint32_t reverse_indian(uint32_t val) {
	val = ((val << 8) & 0xFF00FF00) | ((val >> 8) & 0xFF00FF);
	return (val << 16) | (val >> 16);
}

int main(int argc, char* argv[]) {

	const char* model_filename = argv[1];
    const int num_epochs = EPOCH;
    
    mnist::MNIST_dataset<std::vector, std::vector<uint8_t>, uint8_t> dataset =
        mnist::read_dataset<std::vector, std::vector, uint8_t, uint8_t>(MNIST_FASHION_DATA_LOCATION);

	//Loading the tflite retrainable model
	std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(model_filename);
	
	tflite::ops::builtin::BuiltinOpResolver resolver;
	tflite::InterpreterBuilder builder(*model, resolver);

	//Building the interpreter
	std::unique_ptr<tflite::Interpreter> interpreter;
	builder(&interpreter);

	//Getting the list of signatures
	auto signature_defs = interpreter->signature_keys();
		
    tflite::SignatureRunner* train_runner = interpreter->GetSignatureRunner(""train"");
    
    //Getting the input and output tensors names
    const std::vector<const char*>& input_names = train_runner->input_names();
    const std::vector<const char*>& output_names = train_runner->output_names();

    TfLiteTensor* train_input_x = train_runner->input_tensor((std::string(input_names[0])).c_str()); // x
    TfLiteTensor* train_input_y = train_runner->input_tensor((std::string(input_names[1])).c_str()); // y

    //Allocating tensors
    interpreter->AllocateTensors();
    train_runner->AllocateTensors();

    const int num_pixels = 28 * 28;
    const int num_batches = 60000 / BATCH_SIZE;

    float* input_x = train_input_x->data.f;
    float* input_y = train_input_y->data.f;

    std::cout << ""[DEBUG][METADATA] Number of batches: "" << num_batches << std::endl;
    std::cout << ""[DEBUG][TRAINING] Training launched ... "" << std::endl;
    
    
    // tflite::PrintInterpreterState(interpreter.get());
    for (int epoch = 0; epoch < num_epochs; ++epoch)
    {
        auto image_it = dataset.training_images.begin();
        auto label_it = dataset.training_labels.begin();
        float current_loss = 0;
        std::cout << ""[DEBUG][TRAINING] Current epoch: "" << epoch << std::endl;

        // float *fpixels{ new float[num_pixels * BATCH_SIZE]{}};
        // float labels[10 * BATCH_SIZE] = {0.0};
        float *fpixels{ new float[num_pixels]{}};
        float labels[10] = {0.0};

        for (uint32_t batch_index = 0; batch_index < num_batches; ++batch_index)
        {
            for (int index = 0; index < BATCH_SIZE; ++index)
            {   
                auto it = image_it->begin();

                // labels[int(*label_it) + (10 * index)] = 1.0;
                labels[int(*label_it)] = 1.0;
                for (int i=0; i<num_pixels; ++i)
                {   
                    // fpixels[i + (num_pixels * index)] = float(int(*it)/255.0);
                    fpixels[i] = float(float(*it)/255.0);
                    it++;
                }
                
                label_it++;
                image_it++;
                
                input_x = fpixels;
                input_y = labels;
                train_runner->Invoke();
                
            }
        }
        delete[] fpixels;
        const TfLiteTensor* output_tensor = train_runner->output_tensor((std::string(output_names[0])).c_str());
        float* output = output_tensor->data.f;
        current_loss = *(output);
        std::cout << ""[DEBUG][TRAINING] Current Loss: "" << current_loss << std::endl;
        
    }
    std::cout << ""[DEBUG][TRAINING] Training finished successfully."" << std::endl;
	
	return 0;
}
```


### Relevant log output

```shell
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
[DEBUG][METADATA] Number of batches: 600
[DEBUG][TRAINING] Training launched ... 
[DEBUG][TRAINING] Current epoch: 0
[DEBUG][TRAINING] Current Loss: -3.96266e+22
[DEBUG][TRAINING] Current epoch: 1
[DEBUG][TRAINING] Current Loss: -3.96266e+22
[DEBUG][TRAINING] Current epoch: 2
[DEBUG][TRAINING] Current Loss: -3.96266e+22
[DEBUG][TRAINING] Current epoch: 3
```
</details>"
56250,Multiple problems building TFLite 2.9.1,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

iOS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
1. tensorflow/lite/c/CMakeLists.txt references common.c. Actual file at the referenced location is common.cc
2. With default TFL_MINIMUM_OS_VERSION=9.0 in tensorflow/lite/ios/ios.bzl, there is a compile-time error in Eigen related to thread local storage. Need that value bumped to 10.0 or 11.0 by default.
3. Upon the successful completion of `bazel build --config=ios_fat -c opt //tensorflow/lite/ios:TensorFlowLiteC_static_framework`, the `bazel-bin` folder is empty. The output can be found in `bazel-out/applebin_ios-ios_armv7-opt-ST-5b7531beec20/bin/tensorflow/lite/ios/TensorFlowLiteC_static_framework.zip` instead
4. Why is the hard dependency on Bazel version? Why does it have to be 5.0.0 and can not be 5.1.1? This makes it very unfriendly for CI environment.
```


### Standalone code to reproduce the issue

```shell
Just build it out of the box.
```


### Relevant log output

```shell
INFO: Found 1 target...
Target //tensorflow/lite/ios:TensorFlowLiteC_static_framework up-to-date:
  bazel-out/applebin_ios-ios_armv7-opt-ST-5b7531beec20/bin/tensorflow/lite/ios/TensorFlowLiteC_static_framework.zip
INFO: Elapsed time: 0.280s, Critical Path: 0.00s
INFO: 1 process: 1 internal.
INFO: Build completed successfully, 1 total action
```

Compared to 2.7.1:
```
Target //tensorflow/lite/ios:TensorFlowLiteC_static_framework up-to-date:
  bazel-bin/tensorflow/lite/ios/TensorFlowLiteC_static_framework.zip
INFO: Elapsed time: 411.210s, Critical Path: 80.19s
INFO: 3901 processes: 227 internal, 3674 local.
INFO: Build completed successfully, 3901 total actions
INFO: Build completed successfully, 3901 total actions
```
```
</details>"
56248,Unable to automatically fuse nested loop,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

binary

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.7

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am running the following code with all Grappler optimizations enabled. It should be possible to re-write the code in a way that does a single matrix add for the entire tensor instead of looping - however, I am unable to force Grappler to vectorize the loops. Is there any way to do this in TensorFlow?
```


### Standalone code to reproduce the issue

```shell
@tf.function()
    def somefunc():
        tmp = tf.zeros(shape=(10, 10), dtype=tf.double, name=""tmp"")
        for i in range(0, 10):
            for j in range(0, 10):
                tmp = tf.tensor_scatter_nd_update(
                    tmp, [[i, j]], [tmp[i][j] + 1.0], name=f""scatter_{i}_{j}""
                )

        return {""output"": tmp}
```


### Relevant log output

_No response_</details>"
56247,ImportError: /home/smartcane/anaconda3/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: PyCMethod_New,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Error while running the code
```


### Standalone code to reproduce the issue

```shell
even though TensorFlow is installed and the location is added to PYTHONPATH it throws the below error
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/home/smartcane/anaconda3/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /home/smartcane/anaconda3/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: PyCMethod_New

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/smartcane/Desktop/Hand_track/SKU110K_CVPR19/object_detector_retinanet/keras_retinanet/bin/predict.py"", line 23, in <module>
    from tensorflow import keras
  File ""/home/smartcane/anaconda3/lib/python3.9/site-packages/tensorflow/__init__.py"", line 37, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/home/smartcane/anaconda3/lib/python3.9/site-packages/tensorflow/python/__init__.py"", line 36, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""/home/smartcane/anaconda3/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 77, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""/home/smartcane/anaconda3/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /home/smartcane/anaconda3/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: PyCMethod_New
```
</details>"
56245,tf.keras.initializers.TruncatedNormal is not available when using tf.keras.models.load_model,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

tensorflow/tensorflow Docker image

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When using `tf.keras.initializers.TruncatedNormal` in a custom model, the model can be saved, but fails to load.
Adding `custom_objects={""TruncatedNormal"": tf.keras.initializers.TruncatedNormal})` to `tf.keras.models.load_model` prevents the error.
I expected that passing TF classes as custom objects would not be necessary.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

@tf.keras.utils.register_keras_serializable()
class MyModel(tf.keras.Model):
    def __init__(self, initializer):
        self._config = {""initializer"": initializer}
        super().__init__(inputs=[], outputs=[])
        
    def get_config(self):
        return self._config
    
    @classmethod
    def from_config(cls, config, custom_objects=None):
        return cls(**config)
    
mymodel = MyModel(initializer=tf.keras.initializers.TruncatedNormal())
tf.keras.models.save_model(mymodel, ""mymodel.sm"", overwrite=True)
loaded_mymodel = tf.keras.models.load_model(""mymodel.sm"")
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""tfbug.py"", line 18, in <module>
    loaded_mymodel = tf.keras.models.load_model(""mymodel.sm"")
  File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None    
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/tracking/base.py"", line 587, in _method_wrapper 
    result = method(self, *args, **kwargs)    
TypeError: __init__() missing 2 required positional arguments: 'inputs' and 'outputs'
```
</details>"
56244,Remove or bump upper bound for `gast` dependency,"In https://github.com/tensorflow/tensorflow/commit/c762c4501ca017994c1fa5554c3c8e47b7c80b66 @angerson said:
> I also know that gast 0.5.2 breaks one of our tests, so I've pinned the gast
version to 0.4.0 or below, which is what our CI currently installs for testing.

and pinned gast to 0.4.0, which is by now [almost](https://github.com/serge-sans-paille/gast/tags) two years old.

The added `TODO(angerson): File a bug for these incompatible tests and the limitation` eventually got removed in https://github.com/tensorflow/tensorflow/commit/0c8b4f2bed6e22e797f576fb553a9a9a9145711e, but I cannot find an issue for it, hence opening this one."
56243,Extract some of the weights in the saved model and assign them to another,"Tensorflow Version

2.5

How to extract the weights of part of the specified network from the saved model, and assign the extracted weights to another saved model with the same structure.

Also, the point is, how to write the extracted weight into the [variables.data-00000-of-00001] and [variables.index] files.

Thank you very much."
56242,Model trains extremely slow after upgrade to tf 2.9,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Windows

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.2 cuDNN 8.1

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Model training is extremely slower than tf 2.8. And bunch of warnings showed up.
First, I thought GPU isn't been using so I checked it by ""print(tf.config.list_physical_devices('GPU'))"".
Everything was fine. And nothing changed even with ""with tf.device('/device:gpu:0'):""
When I downgrade tf to 2.8, training speed became normal suddenly.
```


### Standalone code to reproduce the issue

```shell
Model from [here](https://keras.io/examples/vision/vit_small_ds/#build-the-vit).
```


### Relevant log output

```shell
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3
Epoch 1/2000
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3
3/3 [==============================] - 14s 1s/step - loss: 4.9424 - accuracy: 0.3194
Epoch 2/2000
3/3 [==============================] - 5s 1s/step - loss: 5.1579 - accuracy: 0.2903
Epoch 3/2000
3/3 [==============================] - 5s 1s/step - loss: 2.8383 - accuracy: 0.2806
Epoch 4/2000
3/3 [==============================] - 5s 2s/step - loss: 2.0193 - accuracy: 0.3032
Epoch 5/2000
3/3 [==============================] - 5s 1s/step - loss: 1.7683 - accuracy: 0.2677
Epoch 6/2000
3/3 [==============================] - 6s 2s/step - loss: 1.6382 - accuracy: 0.2484
Epoch 7/2000
3/3 [==============================] - 6s 2s/step - loss: 1.5515 - accuracy: 0.3129
Epoch 8/2000
3/3 [==============================] - 6s 1s/step - loss: 1.4848 - accuracy: 0.3290
Epoch 9/2000
3/3 [==============================] - 5s 2s/step - loss: 1.4735 - accuracy: 0.2871
Epoch 10/2000
3/3 [==============================] - 6s 2s/step - loss: 1.4442 - accuracy: 0.3194
Epoch 11/2000
3/3 [==============================] - 7s 2s/step - loss: 1.5043 - accuracy: 0.2806
Epoch 12/2000
3/3 [==============================] - 9s 2s/step - loss: 1.4329 - accuracy: 0.3032
Epoch 13/2000
3/3 [==============================] - 7s 2s/step - loss: 1.4456 - accuracy: 0.3194
Epoch 14/2000
3/3 [==============================] - 6s 2s/step - loss: 1.3863 - accuracy: 0.3323
Epoch 15/2000
3/3 [==============================] - 7s 2s/step - loss: 1.3881 - accuracy: 0.3387
Epoch 16/2000
3/3 [==============================] - 7s 2s/step - loss: 1.3851 - accuracy: 0.3452
Epoch 17/2000
3/3 [==============================] - 6s 1s/step - loss: 1.4353 - accuracy: 0.2935
Epoch 18/2000
3/3 [==============================] - 5s 1s/step - loss: 1.4537 - accuracy: 0.3097
Epoch 19/2000
3/3 [==============================] - 5s 1s/step - loss: 1.3740 - accuracy: 0.3452
Epoch 20/2000
3/3 [==============================] - 5s 1s/step - loss: 1.3845 - accuracy: 0.3387
Epoch 21/2000
3/3 [==============================] - 5s 1s/step - loss: 1.3990 - accuracy: 0.3065
Epoch 22/2000
3/3 [==============================] - 5s 1s/step - loss: 1.4265 - accuracy: 0.3065
Epoch 23/2000
3/3 [==============================] - 5s 1s/step - loss: 1.3932 - accuracy: 0.3290
Epoch 24/2000
3/3 [==============================] - 5s 1s/step - loss: 1.4290 - accuracy: 0.3000
Epoch 25/2000
3/3 [==============================] - 5s 1s/step - loss: 1.3516 - accuracy: 0.3161
Epoch 26/2000
3/3 [==============================] - 6s 2s/step - loss: 1.4167 - accuracy: 0.2581
Epoch 27/2000
3/3 [==============================] - 6s 2s/step - loss: 1.3771 - accuracy: 0.3161
Epoch 28/2000
3/3 [==============================] - 7s 2s/step - loss: 1.3567 - accuracy: 0.3484
Epoch 29/2000
3/3 [==============================] - 7s 2s/step - loss: 1.3472 - accuracy: 0.2968
Epoch 30/2000
3/3 [==============================] - 6s 2s/step - loss: 1.3214 - accuracy: 0.3323
Epoch 31/2000
3/3 [==============================] - 5s 1s/step - loss: 1.3627 - accuracy: 0.3323
Epoch 32/2000
3/3 [==============================] - 5s 1s/step - loss: 1.3597 - accuracy: 0.3290
Epoch 33/2000
3/3 [==============================] - 5s 1s/step - loss: 1.3425 - accuracy: 0.3581
Epoch 34/2000
3/3 [==============================] - 5s 1s/step - loss: 1.3398 - accuracy: 0.3548
Epoch 35/2000
3/3 [==============================] - 6s 2s/step - loss: 1.3045 - accuracy: 0.3581
Epoch 36/2000
3/3 [==============================] - 6s 2s/step - loss: 1.3544 - accuracy: 0.3323
Epoch 37/2000
3/3 [==============================] - 6s 2s/step - loss: 1.3534 - accuracy: 0.3226
Epoch 38/2000
3/3 [==============================] - 6s 2s/step - loss: 1.3467 - accuracy: 0.3129
Epoch 39/2000
3/3 [==============================] - 5s 1s/step - loss: 1.3483 - accuracy: 0.3484
Epoch 40/2000
3/3 [==============================] - 5s 1s/step - loss: 1.3432 - accuracy: 0.3387
Epoch 41/2000
3/3 [==============================] - 6s 2s/step - loss: 1.3118 - accuracy: 0.3677
Epoch 42/2000
3/3 [==============================] - 6s 2s/step - loss: 1.3486 - accuracy: 0.3677
Epoch 43/2000
3/3 [==============================] - 7s 2s/step - loss: 1.3060 - accuracy: 0.3710
Epoch 44/2000
3/3 [==============================] - 6s 2s/step - loss: 1.3532 - accuracy: 0.3323
Epoch 45/2000
3/3 [==============================] - 6s 1s/step - loss: 1.3577 - accuracy: 0.3194
Epoch 46/2000
3/3 [==============================] - 5s 1s/step - loss: 1.3188 - accuracy: 0.3710
Epoch 47/2000
3/3 [==============================] - 5s 2s/step - loss: 1.3294 - accuracy: 0.3613
Epoch 48/2000
3/3 [==============================] - 5s 1s/step - loss: 1.3581 - accuracy: 0.3097
Epoch 49/2000
3/3 [==============================] - 5s 1s/step - loss: 1.3758 - accuracy: 0.3516
Epoch 50/2000
3/3 [==============================] - 5s 1s/step - loss: 1.3494 - accuracy: 0.3032
Epoch 51/2000
3/3 [==============================] - 5s 1s/step - loss: 1.3446 - accuracy: 0.3290
Epoch 52/2000
3/3 [==============================] - 5s 1s/step - loss: 1.3520 - accuracy: 0.3290
Epoch 53/2000
3/3 [==============================] - 5s 1s/step - loss: 1.2753 - accuracy: 0.3742
Epoch 54/2000
3/3 [==============================] - 5s 1s/step - loss: 1.3350 - accuracy: 0.3323
```
</details>"
56241,parameters in XML file comparison,"Hello,

i am a bit unsure about the new xml file i created with a labelling tool. the first example is a tool that doesn't support object tracking, meaning you have to mark each object yourself. in the second example (i use) you can see that more parameters are added. now i wanted to know if there can be any problems with the creation of the tfrecord files?

the normal process is:

xml file -> csv file -> tfrecord

the following scripts are used for this: 

https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/blob/master/xml_to_csv.py / 
https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/blob/master/generate_tfrecord.py

are the parameters ignored by the python script and only read which are defined? 

standard xml file:
```
<annotation>
	<folder>train</folder>
	<filename>IMG_2385.JPG</filename>
	<path>C:\tensorflow_cards\train\IMG_2385.JPG</path>
	<source>
		<database>Unknown</database>
	</source>
	<size>
		<width>378</width>
		<height>504</height>
		<depth>3</depth>
	</size>
	<segmented>0</segmented>
	<object>
		<name>king</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>92</xmin>
			<ymin>183</ymin>
			<xmax>223</xmax>
			<ymax>348</ymax>
		</bndbox>
	</object>
</annotation>
```
new xml file:

```
<annotation>
  <folder>image</folder>
  <filename>image_14.jpg</filename>
  <source>
    <database>Unknown</database>
    <annotation>Unknown</annotation>
    <image>Unknown</image>
  </source>
  <size>
    <width>720</width>
    <height>720</height>
    <depth></depth>
  </size>
  <segmented>0</segmented>
  <object>
    <name>person</name>
    <truncated>0</truncated>
    <occluded>0</occluded>
    <difficult>0</difficult>
    <bndbox>
      <xmin>281.22</xmin>
      <ymin>688.92</ymin>
      <xmax>323.7</xmax>
      <ymax>718.45</ymax>
    </bndbox>
    <attributes>
      <attribute>
        <name>rotation</name>
        <value>0.0</value>
      </attribute>
      <attribute>
        <name>track_id</name>
        <value>63</value>
      </attribute>
      <attribute>
        <name>keyframe</name>
        <value>True</value>
      </attribute>
    </attributes>
  </object>
</annotation>
```"
56240,"Invalid argument: No OpKernel was registered to support Op 'PyFunc' used by {{node PyFunc}}with these attrs: [Tin=[DT_FLOAT], Tout=[DT_INT32], token=""pyfunc_0""] Registered devices: [CPU, GPU] Registered kernels:   <no registered kernels>  	 [[PyFunc]]","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

1.14

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.6

### Bazel version

0.26.0

### GCC/Compiler version

7.5

### CUDA/cuDNN version

10.0

### GPU model and memory

1050ti 4GB

### Current Behaviour?

```shell
2022-05-24 17:04:39.195755: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2022-05-24 17:04:39.220670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-05-24 17:04:39.220885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: NVIDIA GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2022-05-24 17:04:39.270904: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2022-05-24 17:04:39.500409: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2022-05-24 17:04:39.611684: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2022-05-24 17:04:39.662292: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2022-05-24 17:04:39.919741: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2022-05-24 17:04:40.068422: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2022-05-24 17:04:40.553116: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2022-05-24 17:04:40.553460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-05-24 17:04:40.554410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-05-24 17:04:40.555188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2022-05-24 17:04:40.591205: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2022-05-24 17:04:40.705261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-05-24 17:04:40.705335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2022-05-24 17:04:40.705356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2022-05-24 17:04:40.716567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-05-24 17:04:40.717333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-05-24 17:04:40.717987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-05-24 17:04:40.718531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2732 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2022-05-24 17:04:40.779337: E /home/zhj/lidarDetect/pointnet_cpp/fpointnet.cpp:106] Invalid argument: No OpKernel was registered to support Op 'PyFunc' used by {{node PyFunc}}with these attrs: [Tin=[DT_FLOAT], Tout=[DT_INT32], token=""pyfunc_0""]
Registered devices: [CPU, GPU]
Registered kernels:
  <no registered kernels>

	 [[PyFunc]]
------------------------------------------------------------------------------------------------------------------------------------------------------------------------
I use freeze_graph to generate pb from cpkt which include tf.py_func.Then I use LoadGraph to load the pb model to infernece by c++.And it make the above bug.
```


### Standalone code to reproduce the issue

```shell
############################################
The following code is the function which using tf.py_func.
##############################################
def tf_gather_object_pc(point_cloud, mask, npoints=512):
    ''' Gather object point clouds according to predicted masks.
    Input:
        point_cloud: TF tensor in shape (B,N,C)
        mask: TF tensor in shape (B,N) of 0 (not pick) or 1 (pick)
        npoints: int scalar, maximum number of points to keep (default: 512)
    Output:
        object_pc: TF tensor in shape (B,npoint,C)
        indices: TF int tensor in shape (B,npoint,2)
    '''
    def mask_to_indices(mask):
        indices = np.zeros((mask.shape[0], npoints, 2), dtype=np.int32)
        for i in range(mask.shape[0]):
            pos_indices = np.where(mask[i,:]>0.5)[0]
            # skip cases when pos_indices is empty
            if len(pos_indices) > 0: 
                if len(pos_indices) > npoints:
                    choice = np.random.choice(len(pos_indices),
                        npoints, replace=False)
                else:
                    choice = np.random.choice(len(pos_indices),
                        npoints-len(pos_indices), replace=True)
                    choice = np.concatenate((np.arange(len(pos_indices)), choice))
                np.random.shuffle(choice)
                indices[i,:,1] = pos_indices[choice]
            indices[i,:,0] = i
        return indices

    indices = tf.py_func(mask_to_indices, [mask], tf.int32)  
    object_pc = tf.gather_nd(point_cloud, indices)
    return object_pc, indices
```


### Relevant log output

```shell
node name: ""PyFunc""
op: ""PyFunc""
input: ""Squeeze_1""
attr {
  key: ""Tin""
  value {
    list {
      type: DT_FLOAT
    }
  }
}
attr {
  key: ""Tout""
  value {
    list {
      type: DT_INT32
    }
  }
}
attr {
  key: ""token""
  value {
    s: ""pyfunc_0""
  }
}
```
</details>"
56239,Memory leak in TfLiteModelCreateFromFile() in tflite v2.8.0 GNU Linux x86-64,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.8.0

### Custom Code

No

### OS Platform and Distribution

Linux ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

gcc version 8.3.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
I see memory leak from TfLiteModelCreateFromFile().
```


### Standalone code to reproduce the issue

```shell
When I compile below code
#############################
#include <stdio.h>
#include ""c_api.h""

int main()
{
    TfLiteModel* model = NULL;
    model = TfLiteModelCreateFromFile(""model.tflite"");
    if (model == NULL)
    {
        printf(""Model opening failed\n"");
    }
    TfLiteModelDelete(model);
        return 0;
}

#############################

using 
g++ app.c -I <path to c_api.h> -L <path to libtensorflowlite_c.so >-ltensorflowlite_c
```


### Relevant log output

```shell
valgrind --leak-check=full --track-origins=yes ./a.out

HEAP SUMMARY:
==24315==     in use at exit: 8 bytes in 1 blocks
==24315==   total heap usage: 6 allocs, 5 frees, 72,824 bytes allocated
==24315==
==24315== LEAK SUMMARY:
==24315==    definitely lost: 0 bytes in 0 blocks
==24315==    indirectly lost: 0 bytes in 0 blocks
==24315==      possibly lost: 0 bytes in 0 blocks
==24315==    still reachable: 8 bytes in 1 blocks
==24315==         suppressed: 0 bytes in 0 blocks


It looks like object StderrReporter is not freed.

 ErrorReporter* DefaultErrorReporter() {
  static StderrReporter* error_reporter = new StderrReporter;
  return error_reporter;
}
```
</details>"
56238,BUG: Memory leak bugs due to returned new reference is not decreased on failure (static analyzer reports),"Returning a new reference here:
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/python/client/tf_session_wrapper.cc#L285
Function returns without decreasing the refcnt.
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/python/client/tf_session_wrapper.cc#L296
Internal Report ID: cd62b9


---
Returning a new reference here:
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L1390
Function returns without decreasing the refcnt.
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L1397
Internal Report ID: 471b1c 


---
Returning a new reference here:
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/util/nest.cc#L93
Function returns without decreasing the refcnt.
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/util/nest.cc#L136 
Internal Report ID: 86960d
"
56237,BUG: Memory leak due to returned new reference is not assigned (static analyzer report),"`PyObject_CallFunctionObjArgs` returns a new reference. If the return value is not assigned to a variable and the refcnt is not decreased, the returned PyObject will get leaked.

https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/framework/op_def_util.cc#L179

Internal Report ID: 8566d6
"
56236,BUG: Memory leak bugs due to new reference passed to non-stealing APIs (static analyzer reports),"## Function `PyDict_SetItemString` and `PyDict_SetItem`

`PyDict_SetItemString` does not steal a reference from its third argument.
`PyDict_SetItem` does not steal a reference from its last two arguments.

* Internal Report ID: 7311bd 
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L145

* Internal Report ID: 0bf563
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L147

* Internal Report ID: 107a6c
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L149

* Internal Report ID: 336e4b
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L659

* Internal Report ID: f40c5f 
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L663

* Internal Report ID: 253804
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/toco/python/toco_python_api.cc#L233

* Internal Report ID: 9b7aa7 
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L1057

* Internal Report ID: 66dcb9
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L1058

* Internal Report ID: 327452 
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L1093

* Internal Report ID: c80650
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L1094

* Internal Report ID: 271ec8 
Returning a new reference here:
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L141
Passed to a non-stealing API as the sink:
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L161

* Internal Report ID: c41b64
Returning a new reference here:
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L650
Other usages as the `self` PyObject:
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L666
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L667
Passed to a non-stealing API as the sink:
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L668

* Internal Report ID: 7193e8 
Returning a new reference here:
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L651
Other usages as the `self` PyObject:
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L658
Passed to a non-stealing API as the sink:
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L666

* Internal Report ID: 9dcdd9 
Returning a new reference here:
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L652
Other usages as the `self` PyObject:
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L662
Passed to a non-stealing API as the sink:
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L667

## Other functions

* Internal Report ID: 6a12cf (`PyObject_GetAttr`)
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L1018

* Internal Report ID: 918a9a (`PyBytes_AsString`)
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/lib/core/ndarray_tensor.cc#L90

* Internal Report ID: fffdc2 (`PyCapsule_GetPointer`)
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/lib/core/py_func.cc#L201

* Internal Report ID: 34df8e (`PyCapsule_GetPointer`)
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/lib/core/py_func.cc#L240 

* Internal Report ID: 506234 (`PyCapsule_GetPointer`)
Returning a new reference here:
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L3663
Passed to a non-stealing API as the sink:
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L3666

* Internal Report ID: 9ede7a (`PyType_FromSpecWithBases`)
Returning a new reference here:
https://github.com/tensorflow/tensorflow/blob/eee70a365f5deb9c78e63945756f856fb96847cb/tensorflow/python/eager/pywrap_tensor.cc#L1054
Other usages as the `self` PyObject:
https://github.com/tensorflow/tensorflow/blob/eee70a365f5deb9c78e63945756f856fb96847cb/tensorflow/python/eager/pywrap_tensor.cc#L1055
Passed to a non-stealing API as the sink:
https://github.com/tensorflow/tensorflow/blob/eee70a365f5deb9c78e63945756f856fb96847cb/tensorflow/python/eager/pywrap_tensor.cc#L1085


## Many usages as the `self` PyObject or a C pointer.

* Internal Report ID: 8235b2 
Returning a new reference here:
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/toco/python/toco_python_api.cc#L276
All other usages of this PyObject:
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/toco/python/toco_python_api.cc#L277
https://github.com/tensorflow/tensorflow/blob/4c1572c8cd91ac2bf757189e0d533b1dbecdad01/tensorflow/lite/toco/python/toco_python_api.cc#L278


* Internal Report ID: bf3201
Returning a new reference here:
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/framework/python_api_parameter_converter_wrapper.cc#L33
All other usages of this PyObject:
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/framework/python_api_parameter_converter_wrapper.cc#L34
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/framework/python_api_parameter_converter_wrapper.cc#L35

* Internal Report ID: b665b7
Returning a new reference here:
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L505 
All other usages of this PyObject:
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L507
Other usages in the callee function `ParseStringValue`:
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L324
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L327
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L332
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L334 
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L343

* Internal Report ID: 3eadfd
Returning a new reference here:
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L715
All other usages of this PyObject:
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L717
Other usages in the callee function `ParseStringValue`: 
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L324
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L327
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L332
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L334
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/eager/pywrap_tfe_src.cc#L343

* Internal Report ID: 9e7c5b
Returning a new reference here:
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/util/fast_module_type.cc#L197
All other usages of this PyObject:
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/util/fast_module_type.cc#L201
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/util/fast_module_type.cc#L215
Other usages in the callee function `CallFunc`:
https://github.com/tensorflow/tensorflow/blob/15c26be8f79834ae5a6123939b4885e4d6a112af/tensorflow/python/util/fast_module_type.cc#L158
"
56235,GPU can not be identified when using Tensorflow 2.4./2.5 with CUDA11.6,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.4

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.6/

### GPU model and memory

RTX 3080   

### Current Behaviour?

```shell
GPU can not be identified under the tensorflow2.4.0 environment, and the code can only running using CPU. I am wandering whether Tensorflow with higher version (>=2.5) supports GPU, or whether TensorFlow 2.4/2.5 is compatible with CUDA 11.6.

Note that two RTX 3080 GPUs were installed on the machine, and the CUDA and CUDNN are installed properly, since there is no bugs when running other programs written based on Pytorch and GPU can be identified.
```


### Standalone code to reproduce the issue

```shell
This is an environmental problem. The GPU cannot be recognized using tensorflow 2.4 / 2.5 framework, but it can be recognized using pytorch.
```


### Relevant log output

_No response_</details>"
56233,Mapping a dataset squashes class_names from image_dataset_from_directory,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.4.4

### Custom Code

Yes

### OS Platform and Distribution

n/a

### Mobile device

n/a

### Python version

3.8

### Bazel version

n/a

### GCC/Compiler version

n/a

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

Using [`image_dataset_from_directory`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory) will correctly create a `Dataset` that contains the labels as `class_names`.

However, when you transform that dataset (e.g. via `.map`) the `class_names` get clobbered.

Ideally, `.map`'ing or `.batch`'ing a `Dataset` won't clobber `class_names`.

### Standalone code to reproduce the issue

Here's how to get the dataset (from [Image classification from scratch](https://keras.io/examples/vision/image_classification_from_scratch/)).

```shell
curl -O https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip
unzip kagglecatsanddogs_5340.zip
```

Then running this:

```python
import tensorflow as tf

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    ""PetImages"",
    batch_size=32,
)
assert hasattr(train_ds, ""class_names"")  # Passes
mapped_train_ds = train_ds.map(lambda x, y: (x, y))
assert hasattr(mapped_train_ds, ""class_names"")  # Doesn't pass
```

### Relevant log output

_No response_</details>"
56232,tensorflow 1 is much slower than 2 with tf.data.Dataset.from_tensor_slices,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

tf 1.15.0 vs tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 16

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
It was very slow when I used tf.data.Dataset.from_tensor_slices() in a tf1 environment to create a dataset, but very quickly with tf2. Whether this is an optimization of tf2. If you have to be in a tf1 environment, is there a good alternative?
```


### Standalone code to reproduce the issue

```shell
df = pd.read_csv(train_file, header=None, names=TRAIN_DATA_COLUMNS)
features = collections.OrderedDict()
for col in CONTINUOUS_COLUMNS:
    features[col] = df[col].to_numpy().astype(np.float32)
    print(features[col].dtype)

for col in CATEGORICAL_COLUMNS:
    features[col] = df[col].to_numpy()
    print(features[col].dtype)
labels = df[LABEL_COLUMN].to_numpy()
dataset = tf.data.Dataset.from_tensor_slices((features, labels))
```


### Relevant log output

_No response_</details>"
56231,tensorflow.keras IDE auto-completion fails,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.8 2.9

### Custom Code

Yes

### OS Platform and Distribution

win mac

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
tensorflow.keras IDE auto-completion fails.
Using ""from tensorflow."" does not pop up keras, pycharm will report red but it works.
Also, tf 2.5 is normal.
I see there are other people who have had this problem
```


### Standalone code to reproduce the issue

```shell
from tensorflow.keras import layers
```


### Relevant log output

_No response_</details>"
56230,non-DMA-copy attempted of tensor type: string error when trying to run roberta-base example from tfhub,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Source

source

### Tensorflow Version

2.5

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04.5

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


I am trying to run the following example for roberta-base, copied from the documentation on tfhub. However, it seems unhappy with how the input is being fed into the model. I am getting `Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string`. 

```
# define a text embedding model
text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)
preprocessor = hub.KerasLayer(""https://tfhub.dev/jeongukjae/roberta_en_cased_preprocess/1"")
encoder_inputs = preprocessor(text_input)

encoder = hub.KerasLayer(""https://tfhub.dev/jeongukjae/roberta_en_cased_L-12_H-768_A-12/1"", trainable=True)
encoder_outputs = encoder(encoder_inputs)
pooled_output = encoder_outputs[""pooled_output""]      # [batch_size, 768].
sequence_output = encoder_outputs[""sequence_output""]  # [batch_size, seq_length, 768].

model = tf.keras.Model(text_input, pooled_output)

# You can embed your sentences as follows
sentences = tf.constant([""(your text here)""])
print(model(sentences))
```

If I instead use the preprocessor and encoder from BERT instead of RoBERTa, the code above works. 
```
preprocessor = hub.KerasLayer(""https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3"")
encoder = hub.KerasLayer(""https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4"", trainable=True)
```
In addition, the code above with the RoBERTa preprocessor/encoder seems to work if I use CPU instead of GPU (adding `with tf.device('/cpu:0')`), but this is not feasible because I need to fine-tune a model on lots of data.  

### Standalone code to reproduce the issue
I am using tensorflow version 2.5 and tensoflow_hub version 0.12. Here is a link to the model on tfhub: https://tfhub.dev/jeongukjae/roberta_en_cased_L-12_H-768_A-12/1.



### Relevant log output

```shell
InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  2 root error(s) found.
  (0) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
  (1) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
0 successful operations.
0 derived errors ignored.
	 [[{{node StatefulPartitionedCall/StatefulPartitionedCall/bpe_sentencepiece_tokenizer/StatefulPartitionedCall/map/TensorArrayUnstack/TensorListFromTensor/_100}}]]
  (1) Invalid argument:  2 root error(s) found.
  (0) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
  (1) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
0 successful operations.
0 derived errors ignored.
	 [[{{node StatefulPartitionedCall/StatefulPartitionedCall/bpe_sentencepiece_tokenizer/StatefulPartitionedCall/map/TensorArrayUnstack/TensorListFromTensor/_100}}]]
	 [[StatefulPartitionedCall/StatefulPartitionedCall/bpe_sentencepiece_tokenizer/StatefulPartitionedCall/RaggedFromRowSplits_1/RowPartitionFromRowSplits/control_dependency/_157]]
0 successful operations.
0 derived errors ignored. [Op:__inference_restored_function_body_376768]

Function call stack:
restored_function_body -> restored_function_body
```
</details>"
56229,Dataset has no attribute load,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Mac

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hello, I just upgraded to tensorflow v2.9.1 expecting to be able to use the new dataset methods load and save. But they are not available.
```


### Standalone code to reproduce the issue

```shell
# Load doesn't work
tf.data.Dataset.load('path')
AttributeError: type object 'DatasetV2' has no attribute 'load'

# Save doesn't work
dataset = tf.data.Dataset.range(2)
tf.data.Dataset.save(dataset, path)

AttributeError: type object 'DatasetV2' has no attribute 'save'
```


### Relevant log output

_No response_</details>"
56228,Unexpected behavior when using DatasetCreator with respect to sharding,"### Issue Type

Documentation Bug

### Source

binary

### Tensorflow Version

2.8.0

### Custom Code

No

### OS Platform and Distribution

Google Colab, tfx training using `gcr.io/tfx-oss-public/tfx:1.7.1`

### Current Behaviour?


I have an autoencoder training under the default `model.fit` using a `DatasetCreator`. My setup is 8 workers, 4 accelerator each, 8 ps.

When I add sharding to the `dataset_fn` in `DatasetCreator`, the loss MSE changes dramatically. Namely, calling (see code below):

```python
train_dataset = tf.keras.utils.experimental.DatasetCreator(
		lambda input_context: get_dataset(
				fn_args.train_files,
				transformed_feature_spec,
				preprocessor,
				batch_size=256,
				# remove sharding dramatically changes the loss mse
				shard_num=None,
				shard_index=None))
```

Gives me loss in the thousands (see logs), while calling:

```python
train_dataset = tf.keras.utils.experimental.DatasetCreator(
		lambda input_context: get_dataset(
				fn_args.train_files,
				transformed_feature_spec,
				preprocessor,
				batch_size=256,
				# remove sharding dramatically changes the loss mse
				shard_num=input_context.num_input_pipelines,
				shard_index=input_context.input_pipeline_id))
```

Gives me normal MSE less than 1 (see logs).

To debug, I added `tf.print()` to `get_dataset`, the printed logs values printed by the workers are:

```
num_input_pipelines == 1
input_pipeline_id == 0
num_replicas_in_sync == 4
```

Which tells me the `InputContext` given to each worker are unaware of other workers. And this makes sense in the context of the `ParameterServerStrategy` [tutorial guide][1], which states:

>Keras Model.fit with parameter server training assumes that each worker receives the same dataset, except when it is shuffled differently.

So I'm not sure how sharding the dataset with `dataset.shard(1, 0)` could possibly make such a difference. My only guess is that perhaps the debug `tf.print()` was triggered during tracing and the actual `InputContext` is different when training. **But even then, whether the data is sharded or not shouldn't affect the loss?** There's some unexpected behaviors here that's not documented?


[1]: https://www.tensorflow.org/tutorials/distribute/parameter_server_training#input_data


### Standalone code to reproduce the issue

```python
def get_dataset(
		file_pattern: List[Text],
		feature_spec: Dict[str, common_types.FeatureSpecType],
		preprocessing_layer: keras.Model,
		batch_size: int = 256,
		shard_num: int = None,
		shard_index: int = None) -> tf.data.Dataset:

	file_pattern = tf.convert_to_tensor(file_pattern)

	compression_type = dataset_util.detect_compression_type(file_pattern)

	dataset = tf.data.Dataset.list_files(
		file_pattern,
		shuffle=False)

	options = tf.data.Options()
	options.deterministic = False
	options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF
	dataset = dataset.with_options(options)
	
	dataset = dataset.interleave(
		lambda filename: tf.data.TFRecordDataset(
			filename, 
			compression_type=compression_type),
		cycle_length=tf.data.AUTOTUNE,
		num_parallel_calls=tf.data.AUTOTUNE,
		deterministic=False)

	def _apply_preprocessing(tfrecord):
		parsed_features = tf.io.parse_example(tfrecord, feature_spec)
		preprocessed_features = tf.squeeze(preprocessing_layer(parsed_features))
		return preprocessed_features, preprocessed_features

	dataset = dataset.map(
		_apply_preprocessing,
		num_parallel_calls=tf.data.AUTOTUNE,
		deterministic=False)

	dataset = dataset.cache()

	dataset = dataset.shuffle(buffer_size=1000000)

	dataset = dataset.repeat()

	if shard_num and shard_index:
		dataset = dataset.shard(shard_num, shard_index)
	
	dataset = dataset.batch(
		batch_size,
		num_parallel_calls=tf.data.AUTOTUNE,
		deterministic=False)

	return dataset.prefetch(tf.data.AUTOTUNE)



# using a tfx pipeline
# TF_CONFIG is auto provisioned to the cluster
# 8 worker, 4 accelerator each, 8 ps, no eval workers
cluster_resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()

if cluster_resolver.task_type == 'chief':
	transformed_feature_spec = tf_transform_output.transformed_feature_spec()

	variable_partitioner = tf.distribute.experimental.partitioners.FixedShardsPartitioner(
			num_shards=8)

	strategy = tf.distribute.ParameterServerStrategy(
			cluster_resolver=cluster_resolver,
			variable_partitioner=variable_partitioner)


	with strategy.scope():
		preprocessor, autoencoder = create_model(
				feature_spec=transformed_feature_spec,
				learning_rate=0.004,
				)

		train_dataset = tf.keras.utils.experimental.DatasetCreator(
				lambda input_context: get_dataset(
						fn_args.train_files,
						transformed_feature_spec,
						preprocessor,
						batch_size=256,
						##############################################
						# Adding sharding dramatically reduce loss!? #
						##############################################
						shard_num=input_context.num_input_pipelines,
						shard_index=input_context.input_pipeline_id))

	autoencoder.fit(
			train_dataset,
			epochs=1024,
			steps_per_epoch=200)

	# save model...

elif cluster_resolver.task_type in ('worker', 'ps'):

	server = tf.distribute.Server(
			cluster_resolver.cluster_spec(),
			job_name=cluster_resolver.task_type,
			task_index=cluster_resolver.task_id,
			protocol='grpc',
			start=True)
	server.join()
```


### Relevant log output


---Without sharding:

```
workerpool0-0
Epoch 32/1024

workerpool0-0
200/200 - 4s - loss: 12659.2314 - mse: 12659.2275 - mae: 9.0156 - msle: 0.0395 - 4s/epoch - 19ms/step
```

---With sharding:

```
workerpool0-0
Epoch 32/1024

workerpool0-0
200/200 - 4s - loss: 0.3014 - mse: 0.2985 - mae: 0.3113 - msle: 0.0571 - 4s/epoch - 20ms/step
```
"
56226,Cholesky decomposition half precision either does not work or wrong documentation.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

Tesla T4

### Current Behaviour?

```shell
Cholesky crashes for half precision while documentation says otherwise. 
Quote from documentation: input	A Tensor. Must be one of the following types: float64, float32, half, complex64, complex128. Shape is [..., M, M].
https://www.tensorflow.org/api_docs/python/tf/linalg/cholesky
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
A = tf.random.normal((32,32), dtype=tf.dtypes.float16) # same for tf.dtypes.half
U = A @ tf.transpose(A)
tf.linalg.cholesky(U)
```


### Relevant log output

```shell
Could not find device for node: {{node Cholesky}} = Cholesky[T=DT_HALF]
All kernels registered for op Cholesky:
  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_COMPLEX128]
  device='GPU'; T in [DT_COMPLEX64]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_FLOAT]
 [Op:Cholesky]
```
</details>"
56225,Function similar to torch.nn.functional.grid_sample,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

2.9.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.7.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I want to convert a model containing torch.nn.functional.grid_sample function from pytorch format to tensorflow format.
I was able to convert from pytorch format to onnx format (see https://github.com/microsoft/onnxruntime/issues/10232).
To convert from onnx format to tensorflow format, I need to implement this function in the onnx_tf converter.
Is there an analogue of such a function in tensorflow?
If not, are there plans to implement it?
```


### Standalone code to reproduce the issue

```shell
torch.nn.functional.grid_sample
```


### Relevant log output

_No response_</details>"
56222,Ragged reduce_variance returns negative value & reduce_std -> NaN,"<details><summary>Click to expand!</summary> 

This is not a contribution.
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9

### Custom Code

No

### OS Platform and Distribution

Linux, macOS

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The implementation of `tf.math.reduce_variance` for ragged tensors is not constrained to be non-negative. Also as this subroutine is used in `tf.math.reduce_std`, this causes NaNs when trying to `sqrt` the negative value.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
c = tf.ragged.constant([[[0.214441], [0.214441], [0.214441], [0.214441], [0.214441], [0.214441], [0.214441]]], inner_shape=(1,))
tf.math.reduce_variance(c, axis=1)
>>> <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-1.1175871e-08]], dtype=float32)>
tf.math.reduce_std(c, axis=1)
>>> <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[nan]], dtype=float32)>
```


### Relevant log output

_No response_</details>"
56217,Multiple variable contraint,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

2.4.1

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu

### Mobile device

_No response_

### Python version

3.8.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Based on this [Stackoverflow question](https://stackoverflow.com/questions/68353001/keras-weight-contraints-including-both-bias-and-kernel-matrix-at-the-same-time).

It would be nice to have the possiblilty to apply contraints to multiple variables at the same time. For example, one could apply a maxnorm contraint to a Dense layer that takes into account weigths and biases all together.
```


### Standalone code to reproduce the issue

```shell
There are many UX options here. One possibility (but less scalable for other use cases) is to add the parameter `kernel_bias_contraint` to the Dense layer.
A better one would be to let `tf.keras.constraints.Constraint` to receive multiple variables.
Both options are non exclusive.
```


### Relevant log output

_No response_</details>"
56216,pod install command can't install TensorFlowLiteSwift 2.9.0 on MacBook Air M1,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

No

### OS Platform and Distribution

macOS 12.3.1M1 arm64CPU)

### Mobile device

iPhone 12

### Python version

3.9.12

### Bazel version

5.0.0

### GCC/Compiler version

Apple clang version 13.1.6

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
pod install command can't install TensorFlowLiteSwift 2.9.0
```


### Standalone code to reproduce the issue

```shell
Using Xcode to create TFLiteSwift project
cd TFLiteSwift
pod init
Adding the following command to Podfile
pod 'TensorFlowLiteSwift' '~> 2.9.0'
pod install
```


### Relevant log output

```shell
pod install
Analyzing dependencies
[!] Unable to find a specification for `TensorFlowLiteSwift~> 2.9.0`

You have either:
 * out-of-date source repos which you can update with `pod repo update` or with `pod install --repo-update`.
 * mistyped the name or version.
 * not added the source repo that hosts the Podspec to your Podfile.
```
</details>"
56215,Document Dataset.repeat(0),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Source

binary

### Tensorflow Version

2.4

### Custom Code

No

### OS Platform and Distribution

n/a

### Mobile device

_No response_

### Python version

3.8

### Bazel version

n/a

### GCC/Compiler version

n/a

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

When using Keras `model.fit(validation_data=val_ds)` where `val_ds` is a `Dataset` created via `val_ds.repeat(0)`, the below exception will be raised:

```python
96/96 [==============================] - ETA: 0s - loss: 2.8148 - accuracy: 0.2789Traceback (most recent call last):
  File ""/home/ubuntu/.pyenv/versions/3.8.13/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/home/ubuntu/.pyenv/versions/3.8.13/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/home/ubuntu/path/to/training/vgg16_train.py"", line 68, in <module>
    history: tf.keras.callbacks.History = model.fit(
  File ""/home/ubuntu/path/to/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 1113, in fit
    self._eval_data_handler = data_adapter.DataHandler(
  File ""/home/ubuntu/path/to/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 1100, in __init__
    self._adapter = adapter_cls(
  File ""/home/ubuntu/path/to/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 779, in __init__
    peek, x = self._peek_and_restore(x)
  File ""/home/ubuntu/path/to/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 836, in _peek_and_restore
    peek = next(x)
StopIteration
```

I had wanted to basically go through the list once, so I thought I should set `count=0`.  In other words, `ds.repeat` will not work when `count=0`, but does work when `count=1`.

The documentation currently documents two special cases:

> The default behavior (if count is `None` or `-1`) is for the dataset be repeated indefinitely.

Can we add information to the docs when `count=0`?  It's unclear what happens in this case.

### Standalone code to reproduce the issue

```shell
n/a
```


### Relevant log output

```shell
n/a
```
</details>"
56210,'Unable to map file to memory buffer' - audioClassifier runtimeError,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

2.9

### Custom Code

Yes

### OS Platform and Distribution

Mac OSX Monterey 12.2.1

### Mobile device

_No response_

### Python version

3.9.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
After following an example TF guide, exporting the bird calls model (https://www.tensorflow.org/lite/tutorials/model_maker_audio_classification), I wish to run the model against the audio classifier with a random bird call to test (https://www.tensorflow.org/lite/inference_with_metadata/task_library/audio_classifier).

The issue I am having in is step 2 (https://www.tensorflow.org/lite/inference_with_metadata/task_library/audio_classifier#step_2_using_the_model_2) where I get a runtime error (see Relevant log output).

Any help would be greatly appreciated!
```


### Standalone code to reproduce the issue

(`classifier` towards the end fails to get defined)

```
import tensorflow as tf
import tflite_model_maker as mm
from tflite_support.task import core
from tflite_support.task import processor
from tflite_support.task import audio
from tflite_model_maker import audio_classifier
import os

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import itertools
import glob
import random

# from IPython.display import Audio, Image
from IPython import display
from scipy.io import wavfile

print(f""TensorFlow Version: {tf.__version__}"")
print(f""Model Maker Version: {mm.__version__}"")

birds_dataset_folder = tf.keras.utils.get_file('birds_dataset.zip',
                                               'https://storage.googleapis.com/laurencemoroney-blog.appspot.com/birds_dataset.zip',
                                               cache_dir='./',
                                               cache_subdir='dataset',
                                               extract=True)

# # @title [Run this] Util functions and data structures.

data_dir = './dataset/small_birds_dataset'

bird_code_to_name = {
    'wbwwre1': 'White-breasted Wood-Wren',
    'houspa': 'House Sparrow',
    'redcro': 'Red Crossbill',
    'chcant2': 'Chestnut-crowned Antpitta',
    'azaspi1': ""Azara's Spinetail"",
}

birds_images = {
    'wbwwre1': 'https://upload.wikimedia.org/wikipedia/commons/thumb/2/22/Henicorhina_leucosticta_%28Cucarachero_pechiblanco%29_-_Juvenil_%2814037225664%29.jpg/640px-Henicorhina_leucosticta_%28Cucarachero_pechiblanco%29_-_Juvenil_%2814037225664%29.jpg',  # Alejandro Bayer Tamayo from Armenia, Colombia
    'houspa': 'https://upload.wikimedia.org/wikipedia/commons/thumb/5/52/House_Sparrow%2C_England_-_May_09.jpg/571px-House_Sparrow%2C_England_-_May_09.jpg',  # Diliff
    # Elaine R. Wilson, www.naturespicsonline.com
    'redcro': 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Red_Crossbills_%28Male%29.jpg/640px-Red_Crossbills_%28Male%29.jpg',
    # Mike's Birds from Riverside, CA, US
    'chcant2': 'https://upload.wikimedia.org/wikipedia/commons/thumb/6/67/Chestnut-crowned_antpitta_%2846933264335%29.jpg/640px-Chestnut-crowned_antpitta_%2846933264335%29.jpg',
    # https://www.inaturalist.org/photos/76608368
    'azaspi1': 'https://upload.wikimedia.org/wikipedia/commons/thumb/b/b2/Synallaxis_azarae_76608368.jpg/640px-Synallaxis_azarae_76608368.jpg',
}

test_files = os.path.abspath(os.path.join(data_dir, 'test/*/*.wav'))


def get_random_audio_file():
    test_list = glob.glob(test_files)
    random_audio_path = random.choice(test_list)
    return random_audio_path


def show_bird_data(audio_path):
    sample_rate, audio_data = wavfile.read(audio_path, 'rb')

    bird_code = audio_path.split('/')[-2]
    print(f'Bird name: {bird_code_to_name[bird_code]}')
    print(f'Bird code: {bird_code}')
    display.display(display.Image(birds_images[bird_code]))

    plttitle = f'{bird_code_to_name[bird_code]} ({bird_code})'
    plt.title(plttitle)
    plt.plot(audio_data)
    display.display(display.Audio(audio_data, rate=sample_rate))


print('functions and data structures created')

random_audio = get_random_audio_file()
show_bird_data(random_audio)

spec = audio_classifier.YamNetSpec(
    keep_yamnet_and_custom_heads=True,
    frame_step=3 * audio_classifier.YamNetSpec.EXPECTED_WAVEFORM_LENGTH,
    frame_length=6 * audio_classifier.YamNetSpec.EXPECTED_WAVEFORM_LENGTH)

train_data = audio_classifier.DataLoader.from_folder(
    spec, os.path.join(data_dir, 'train'), cache=True)
train_data, validation_data = train_data.split(0.8)
test_data = audio_classifier.DataLoader.from_folder(
    spec, os.path.join(data_dir, 'test'), cache=True)

batch_size = 128
epochs = 100

print('Training the model')
model = audio_classifier.create(
    train_data,
    spec,
    validation_data,
    batch_size=batch_size,
    epochs=epochs)

print('Evaluating the model')
model.evaluate(test_data)

models_path = './birds_models'
print(f'Exporing the TFLite model to {models_path}')

model.export(models_path, tflite_filename='my_birds_model.tflite')

model.export(models_path, export_format=[
             mm.ExportFormat.SAVED_MODEL, mm.ExportFormat.LABEL])


# Initialization
base_options = core.BaseOptions(file_name=models_path)
classification_options = processor.ClassificationOptions(max_results=2)
options = audio.AudioClassifierOptions(
    base_options=base_options, classification_options=classification_options)
classifier = audio.AudioClassifier.create_from_options(options)

# Alternatively, you can create an audio classifier in the following manner:
# classifier = audio.AudioClassifier.create_from_file(models_path)

# Run inferencee
audio_file = audio.TensorAudio.create_from_wav_file(
    random_audio, classifier.required_input_buffer_size)
audio_result = classifier.classify(audio_file)
```


### Relevant log output

```shell
venv/lib/python3.9/site-packages/tensorflow_lite_support/python/task/audio/audio_classifier.py"", line 85, in create_from_options
    classifier = _CppAudioClassifier.create_from_options(
RuntimeError: Unable to map file to memory buffer, errno=22
```
</details>"
56208,Model training error,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

2.6.2

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.8.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Sometimes my model works fine it goes up to 10 epochs without any problems. Sometimes it ends up by crashing the program. Even the crash occurred after a few epochs and sometimes in the middle of an epoch. My Model architecture looks like this-

Layer (type)                 Output Shape              Param #   
=================================================================
masking_2 (Masking)          (None, 200, 16)           0         
_________________________________________________________________
bidirectional_4 (Bidirection (None, 200, 512)          419328    
_________________________________________________________________
dropout_4 (Dropout)          (None, 200, 512)          0         
_________________________________________________________________
bidirectional_5 (Bidirection (None, 512)               1181184   
_________________________________________________________________
dropout_5 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 513       
=================================================================
Total params: 1,601,025
Trainable params: 1,601,025
Non-trainable params: 0
_________________________________________________________________


Here I am attaching one example where the program stops in the middle of an epoch:

1493/3750 [==========>...................] - ETA: 32:06 - loss: 2.2078 - true_positives_2: 550.0000 - false_positives_2: 2011.0000 - true_negatives_2: 7292.0000 - false_negatives_2: 2091.0000 - precision_2: 0.2148 - recall_2: 0.2083
1494/3750 [==========>...................] - ETA: 32:05 - loss: 2.2072 - true_positives_2: 550.0000 - false_positives_2: 2011.0000 - true_negatives_2: 7298.0000 - false_negatives_2: 2093.0000 - precision_2: 0.2148 - recall_2: 0.2081
1495/3750 [==========>...................] - ETA: 32:04 - loss: 2.2071 - true_positives_2: 550.0000 - false_positives_2: 2015.0000 - true_negatives_2: 7300.0000 - false_negatives_2: 2095.0000 - precision_2: 0.2144 - recall_2: 0.2079
```


### Standalone code to reproduce the issue

```shell
I am attaching my model code:


    model = Sequential()
    
    model.add(Masking(mask_value=0.0, input_shape=(maxlen, vector_dim)))
    
    for i in range(1, layers):
        model.add(Bidirectional(GRU(units=256, activation='tanh', recurrent_activation='hard_sigmoid', return_sequences=True)))
        model.add(Dropout(dropout))
        
    model.add(Bidirectional(GRU(units=256, activation='tanh', recurrent_activation='hard_sigmoid')))
    model.add(Dropout(dropout))
    
    model.add(Dense(1, activation='sigmoid'))

    if optimizer == ""adam"":
        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)

    elif optimizer == ""adamax"":
        optimizer = tf.keras.optimizers.Adamax(learning_rate=lr)

    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[
        tf.keras.metrics.TruePositives(),
        tf.keras.metrics.FalsePositives(),
        tf.keras.metrics.TrueNegatives(),
        tf.keras.metrics.FalseNegatives(),
        tf.keras.metrics.Precision(),
        tf.keras.metrics.Recall()]
    )
 
    return model
```
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""bgru_graph_grid_search.py"", line 579, in <module>
    perform_grid_search()
  File ""bgru_graph_grid_search.py"", line 502, in perform_grid_search
    main(raw_dataset, labels, traindataSetPath, testdataSetPath, realtestdataSetPath, weightPath, resultPath,
  File ""bgru_graph_grid_search.py"", line 180, in main
    model.fit(train_generator, validation_steps=1, steps_per_epoch=steps_epoch, epochs=epoch, verbose=1, callbacks=[checkpoint])
  File ""/home/faysal-gpu/anaconda3/envs/py37_maml/lib/python3.8/site-packages/keras/engine/training.py"", line 1184, in fit
    tmp_logs = self.train_function(iterator)
  File ""/home/faysal-gpu/anaconda3/envs/py37_maml/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 885, in __call__
    result = self._call(*args, **kwds)
  File ""/home/faysal-gpu/anaconda3/envs/py37_maml/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 917, in _call
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
  File ""/home/faysal-gpu/anaconda3/envs/py37_maml/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 3039, in __call__
    return graph_function._call_flat(
  File ""/home/faysal-gpu/anaconda3/envs/py37_maml/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1963, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""/home/faysal-gpu/anaconda3/envs/py37_maml/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 591, in call
    outputs = execute.execute(
  File ""/home/faysal-gpu/anaconda3/envs/py37_maml/lib/python3.8/site-packages/tensorflow/python/eager/execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (sequential_2/dense_2/Sigmoid:0) = ] [[nan][nan][nan]...] [y (Cast_3/x:0) = ] [0]
	 [[{{node assert_greater_equal/Assert/AssertGuard/else/_2663/assert_greater_equal/Assert/AssertGuard/Assert}}]]
	 [[assert_greater_equal_4/Assert/AssertGuard/pivot_f/_2745/_651]]
  (1) Invalid argument:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (sequential_2/dense_2/Sigmoid:0) = ] [[nan][nan][nan]...] [y (Cast_3/x:0) = ] [0]
	 [[{{node assert_greater_equal/Assert/AssertGuard/else/_2663/assert_greater_equal/Assert/AssertGuard/Assert}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_16916]

Function call stack:
train_function -> train_function

2022-05-22 09:13:12.319808: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
```
</details>"
56207,LLVM ERROR: IMAGE_REL_AMD64_ADDR32NB relocation requires an ordered section layout with using XLA with tf.keras.applications.densenet.preprocess_input(),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Windows 10 19043

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.3/8.2.1

### GPU model and memory

RTX 3080Ti 12GB

### Current Behaviour?

```shell
LLVM ERROR: IMAGE_REL_AMD64_ADDR32NB relocation requires an ordered section layout
Happens when using tf.keras.applications.densenet.preprocess_input() together with XLA. Other models' preprocessors might also have the issue but I did not have time to test them yet. Without this preprocessing, everything runs properly. If I remove the XLA flag below, it also works properly.
Using XLA --tf_xla_auto_jit=2 --tf_xla_cpu_global_jit
```


### Standalone code to reproduce the issue

```shell
tf.keras.applications.densenet.preprocess_input()
```


### Relevant log output

```shell
LLVM ERROR: IMAGE_REL_AMD64_ADDR32NB relocation requires an ordered section layout
```
</details>"
56206,Restore non max suppression (nms) support for Tensorflow Lite GPU Delegate.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

v2.9.0-rc2-42-g8a20d54a3c1 2.9.0

### Custom Code

Yes

### OS Platform and Distribution

Google Colab

### Mobile device

_No response_

### Python version

3.7.13 (Google Colab)

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
TL;DR: 
Please restore support for NonMaxSuppression for Tensorflow Lite GPU Delegate.  
It was supported before (TF 2.3) and is not supported now.

Background:
Back in Tensorflow version 2.3 one could take and run Object Detection model (e.g. from
Tensorflow Object Detection API) and run it fully on the TFLite with GPU via delegate.

Right now it is impossible - operations that were previously supported (like `NonMaxSuppression`) are now unsupported.

By dropping support to these operations user's are forced-locked to use older Tensorflow version. Please bring back support for `NonMaxSuppression` as it was previously supported.
```


### Standalone code to reproduce the issue

```shell
# Colab link:
# https://colab.research.google.com/drive/1orm0B-5US7Tn5EaAKJl4xGKbHegLtNYm?usp=sharing

# Let's verify support with TFLite's Authoring Tool
import tensorflow as tf

@tf.function(input_signature = [
    tf.TensorSpec(shape=(1, 100, 4), dtype=tf.float32),  # boxes
    tf.TensorSpec(shape=(1, 100, 1), dtype=tf.float32)  # scores
])
def nms(boxes, scores):
    return tf.image.non_max_suppression_padded(
        boxes,
        scores,
        max_output_size=15,
        pad_to_max_output_size=True,
    )

# Verify this works:
boxes = tf.ones(shape=(1, 100, 4))
scores = tf.ones(shape=(1, 100, 1))
selected_indices_padded, num_valid = nms(boxes, scores)

# Verify crashes TFLite GPU Delegate (authoring tool)
target_spec = tf.lite.TargetSpec()
target_spec.experimental_supported_backends = [""GPU""]
func_to_test = tf.lite.experimental.authoring.compatible(
    nms, converter_target_spec=target_spec, raise_exception=True
)
func_to_test(boxes, scores)
```


### Relevant log output

```shell
# The log output is rather long and covers multiple sub ops. I'm only pasting the lower part:

During handling of the above exception, another exception occurred:

CompatibilityError                        Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/authoring/authoring.py in _decode_error(self, err)
    244 
    245     if self._raise_exception and self._log_messages:
--> 246       raise CompatibilityError(f""CompatibilityException at {repr(self._func)}"")
    247 
    248   def _log(self, message):

CompatibilityError: CompatibilityException at <tensorflow.python.eager.def_function.Function object at 0x7f70f5780590>
```
</details>"
56205,'only supports NHWC tensor format' in Conv3D on M1 Mac,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

2.8

### Custom Code

No

### OS Platform and Distribution

MacOS Monterey

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am trying to run tensorflow on M1 Mac(installed via miniforge) and while fitting the model I am getting error for Invalid argument.
Can some one please help with that is wrong?
Below is the code which I am trying to run:

Code: To fit data using below.

model_1.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=20, verbose=1, callbacks=callbacks_list, validation_data=val_generator, validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)

The input format I am using is as below:

model_1 = Sequential() model_1.add(Conv3D(8,kernel_size=(3,3,3),padding='same',input_shape=(30, 120, 120, 3))) model_1.add(BatchNormalization()) model_1.add(Activation('relu'))
```


### Standalone code to reproduce the issue

```shell
The error is as below:

`File ~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/engine/training_generator_v1.py:202, in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)
199 break
201 # Setup work for each epoch.
--> 202 model.reset_metrics()
203 epoch_logs = {}
204 if mode == ModeKeys.TRAIN:

File ~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/engine/training_v1.py:1003, in Model.reset_metrics(self)
1001 metrics = self._get_training_eval_metrics()
1002 for m in metrics:
-> 1003 m.reset_state()
1005 # Reset metrics on all the distributed (cloned) models.
1006 if self._distribution_strategy:

File ~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/metrics.py:260, in Metric.reset_state(self)
258 return self.reset_states()
259 else:
--> 260 backend.batch_set_value([(v, 0) for v in self.variables])

File ~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206, in add_dispatch_support..wrapper(*args, **kwargs)
204 """"""Call target, and fall back on dispatchers if there is a TypeError.""""""
205 try:
--> 206 return target(*args, **kwargs)
207 except (TypeError, ValueError):
208 # Note: convert_to_eager_tensor currently raises a ValueError, not a
209 # TypeError, when given unexpected types. So we need to catch both.
210 result = dispatch(wrapper, args, kwargs)

File ~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/backend.py:3829, in batch_set_value(tuples)
3827 assign_ops.append(assign_op)
3828 feed_dict[assign_placeholder] = value
-> 3829 get_session().run(assign_ops, feed_dict=feed_dict)

File ~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/backend.py:745, in get_session(op_input_list)
743 if not _MANUAL_VAR_INIT:
744 with session.graph.as_default():
--> 745 _initialize_variables(session)
746 return session

File ~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/backend.py:1193, in _initialize_variables(session)
1189 candidate_vars.append(v)
1190 if candidate_vars:
1191 # This step is expensive, so we only run it on variables not already
1192 # marked as initialized.
-> 1193 is_initialized = session.run(
1194 [variables_module.is_variable_initialized(v) for v in candidate_vars])
1195 # TODO(kathywu): Some metric variables loaded from SavedModel are never
1196 # actually used, and do not have an initializer.
1197 should_be_initialized = [
1198 (not is_initialized[n]) and v.initializer is not None
1199 for n, v in enumerate(candidate_vars)]

File ~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/client/session.py:967, in BaseSession.run(self, fetches, feed_dict, options, run_metadata)
964 run_metadata_ptr = tf_session.TF_NewBuffer() if run_metadata else None
966 try:
--> 967 result = self._run(None, fetches, feed_dict, options_ptr,
968 run_metadata_ptr)
969 if run_metadata:
970 proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

File ~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/client/session.py:1190, in BaseSession._run(self, handle, fetches, feed_dict, options, run_metadata)
1187 # We only want to really perform the run if fetches or targets are provided,
1188 # or if the call is a partial run that specifies feeds.
1189 if final_fetches or final_targets or (handle and feed_dict_tensor):
-> 1190 results = self._do_run(handle, final_targets, final_fetches,
1191 feed_dict_tensor, options, run_metadata)
1192 else:
1193 results = []

File ~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/client/session.py:1368, in BaseSession._do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
1365 return self._call_tf_sessionprun(handle, feed_dict, fetch_list)
1367 if handle is None:
-> 1368 return self._do_call(_run_fn, feeds, fetches, targets, options,
1369 run_metadata)
1370 else:
1371 return self._do_call(_prun_fn, handle, feeds, fetches)

File ~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/client/session.py:1394, in BaseSession._do_call(self, fn, *args)
1389 if 'only supports NHWC tensor format' in message:
1390 message += ('\nA possible workaround: Try disabling Grappler optimizer'
1391 '\nby modifying the config for creating the session eg.'
1392 '\nsession_config.graph_options.rewrite_options.'
1393 'disable_meta_optimizer = True')
-> 1394 raise type(e)(node_def, op, message)

InvalidArgumentError: Node 'training/Adam/gradients/gradients/dropout_1/cond_grad/StatelessIf': Connecting to invalid output 1 of source node dropout_1/cond which has 1 outputs. Try using tf.compat.v1.experimental.output_all_intermediates(True).`
```


### Relevant log output

_No response_</details>"
56204,Error happend in 'tf.linalg.svd'.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04 

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```
* When I train a deep learning model that contains a operator of `tf.linalg.svd`, an error is sometimes happening.
* Sometimes, it happens to be
```
Segmentation fault (core dumped)
```
and the training process is stopped.
* Sometimes, it happens to be
```
2022-05-21 09:15:22.151612: E ./tensorflow/core/kernels/linalg/svd_op_impl.h:110] Eigen::BDCSVD failed with error code 3
2022-05-21 09:15:22.151656: E ./tensorflow/core/kernels/linalg/svd_op_impl.h:110] Eigen::BDCSVD failed with error code 3
Epoch 6 / 5 Step 1578 loss = nan nan time 1.8127152919769287 lr =  0.0007 param_num 259244.0
2022-05-21 09:15:23.531004: E ./tensorflow/core/kernels/linalg/svd_op_impl.h:110] Eigen::BDCSVD failed with error code 3
```
The error info is never stopped, and the training loss and gradients are `nan`.
* The following issues from other developers of the BDCSVD may help
        * https://gitlab.com/libeigen/eigen/-/issues/1723
        * https://stackoverflow.com/questions/48404575/nan-in-eigens-bdcsvd
```


### Standalone code to reproduce the issue

```shell
The error happens sometimes not always, so there's no codes.
```


### Relevant log output

_No response_</details>"
56185,java.lang.IncompatibleClassChangeError,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

binary

### Tensorflow Version

tf 2.8.0

### Custom Code

No

### OS Platform and Distribution

Android

### Mobile device

Android 12

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
An Android App is crashing.

I have Android App installed on a device that uses third-party SDK which is internally using tfLite. When I update the tfLite version from 2.5.0 to 2.8.0 in my app and try to install it, I am getting a crash.

Caused by: java.lang.IncompatibleClassChangeError: Found interface org.tensorflow.lite.Tensor, but class was expected (declaration of 'org.tensorflow.lite.Tensor' appears in /data/data/com.m.f.dev/code_cache/.overlay/base.apk/classes3.dex)
at com.spl.sdk.ondevicefull.k.a(SourceFile:2031)
        at com.spl.sdk.ondevicefull.k.b(SourceFile:43)
        at com.spl.sdk.ondevicefull.k.a(SourceFile:24)
        at com.spl.sdk.ondevicefull.g.c(SourceFile:2284)
        at com.spl.sdk.ondevicefull.g.d(SourceFile:382)
        at com.spl.sdk.ondevicefull.g$a.a(SourceFile:404)
        at com.spl.sdk.events.d$4.run(SourceFile:502)
        at ....

I was just wondering if you could provide me with some pointers. If there is any change happening in the source code that might be producing this crash.
```


### Standalone code to reproduce the issue

```shell
I don't know how I can provide a steps to reproduce this crash, since I don't have access to the third party library code.
```


### Relevant log output

```shell
Caused by: java.lang.IncompatibleClassChangeError: Found interface org.tensorflow.lite.Tensor, but class was expected (declaration of 'org.tensorflow.lite.Tensor' appears in /data/data/com.m.f.dev/code_cache/.overlay/base.apk/classes3.dex)
at com.spl.sdk.ondevicefull.k.a(SourceFile:2031)
        at com.spl.sdk.ondevicefull.k.b(SourceFile:43)
        at com.spl.sdk.ondevicefull.k.a(SourceFile:24)
        at com.spl.sdk.ondevicefull.g.c(SourceFile:2284)
        at com.spl.sdk.ondevicefull.g.d(SourceFile:382)
        at com.spl.sdk.ondevicefull.g$a.a(SourceFile:404)
        at com.spl.sdk.events.d$4.run(SourceFile:502)
        at ....
```
</details>"
56184,tensorflow.experimental.dtensor.relayout segfaults going from sharded layout to replicated layout," ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.9.0-rc2-42-g8a20d54a3c1 2.9.0

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04.4

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.6

### GPU model and memory

GPU A100

### Current Behaviour?


`tensorflow.experimental.dtensor.relayout` crashes (segfaults) going from sharded layout to replicated layout on GPUs.
See output and stack trace.



### Standalone code to reproduce the issue


Script to reproduce issue on a system with 2 or more GPUs:

```
import tensorflow as tf
from tensorflow.experimental import dtensor

size=2
devices = [f'GPU:{i}' for i in range(size)]
mesh_1d = dtensor.create_mesh([('x', size)], devices=devices)
layout = dtensor.Layout(['x', dtensor.UNSHARDED], mesh_1d)

components=[]
for i, dev in enumerate(devices):
    with tf.device(dev):
        local_component = tf.range(i*2, 2 + i * 2)
        local_component = tf.reshape(local_component, [1, 2])
        print('local_component', local_component)
    components.append(local_component)
my_dtensor = dtensor.pack(components,layout)
print('my_dtensor', my_dtensor)

fully_replicated_layout = dtensor.Layout([dtensor.UNSHARDED, dtensor.UNSHARDED], mesh_1d)
replicated_dtensor = dtensor.relayout(my_dtensor, fully_replicated_layout)
print('fully_replicated_dtensor', replicated_dtensor)
```


### Relevant log output

Output:

```
2022-05-20 09:20:53.653386: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-20 09:20:56.910871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79133 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:07:00.0, compute capability: 8.0
2022-05-20 09:20:56.912562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 79133 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:0f:00.0, compute capability: 8.0
local_component tf.Tensor([[0 1]], shape=(1, 2), dtype=int32)
local_component tf.Tensor([[2 3]], shape=(1, 2), dtype=int32)
my_dtensor tf.Tensor({""GPU:0"": [[0 1]], ""GPU:1"": [[2 3]]}, layout=""sharding_specs:x,unsharded, mesh:|x=2|0,1|0,1|/job:localhost/replica:0/task:0/device:GPU:0,/job:localhost/replica:0/task:0/device:GPU:1"", shape=(2, 2), dtype=int32)
Segmentation fault (core dumped)
```

Stack trace from core dump:

```
#0  0x00007f8e6acea66c in __memmove_chk_avx_unaligned () at ../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S:123
#1  0x00007f8e44e0cf86 in tensorflow::Status tensorflow::functor::DoCopy<Eigen::ThreadPoolDevice>(Eigen::ThreadPoolDevice const&, tensorflow::Tensor const&, tensorflow::Tensor*) () from /usr/local/lib/python3.8/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00007f8e42bfdf59 in tensorflow::StridedSliceAssignOp<Eigen::ThreadPoolDevice, int, true>::Compute(tensorflow::OpKernelContext*) () from /usr/local/lib/python3.8/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00007f8e38308aac in tensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) () from /usr/local/lib/python3.8/dist-packages/tensorflow/python/../libtensorflow_framework.so.2
#4  0x00007f8e3840db7d in tensorflow::(anonymous namespace)::ExecutorState<tensorflow::PropagatorState>::Process(tensorflow::PropagatorState::TaggedNode, long) () from /usr/local/lib/python3.8/dist-packages/tensorflow/python/../libtensorflow_framework.so.2
#5  0x00007f8e3840ed48 in std::_Function_handler<void (), tensorflow::(anonymous namespace)::ExecutorState<tensorflow::PropagatorState>::RunTask<tensorflow::(anonymous namespace)::ExecutorState<tensorflow::PropagatorState>::ScheduleReady(absl::lts_20211102::InlinedVector<tensorflow::PropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::PropagatorState::TaggedNode> >*, tensorflow::PropagatorState::TaggedNodeReadyQueue*)::{lambda()#2}>(tensorflow::(anonymous namespace)::ExecutorState<tensorflow::PropagatorState>::ScheduleReady(absl::lts_20211102::InlinedVector<tensorflow::PropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::PropagatorState::TaggedNode> >*, tensorflow::PropagatorState::TaggedNodeReadyQueue*)::{lambda()#2}&&)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()
   from /usr/local/lib/python3.8/dist-packages/tensorflow/python/../libtensorflow_framework.so.2
#6  0x00007f8e4613db95 in Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) () from /usr/local/lib/python3.8/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#7  0x00007f8e4613b8c3 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /usr/local/lib/python3.8/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#8  0x00007f8e38b574fb in tensorflow::(anonymous namespace)::PThread::ThreadFn(void*) () from /usr/local/lib/python3.8/dist-packages/tensorflow/python/../libtensorflow_framework.so.2
#9  0x00007f8e6ab44609 in start_thread (arg=<optimized out>) at pthread_create.c:477
#10 0x00007f8e6ac7e163 in umount2 () at ../sysdeps/unix/sysv/linux/umount2.S:8
#11 0x0000000000000000 in ?? ()
```"
56183,DenseNet PB and TFLite models produce inaccurate results,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

binary

### Tensorflow Version

TF 2.8.0

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04 4LTS

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

Intel UHD Graphics 620

### Current Behaviour?

```shell
I am using the slim DenseNet models provided [here](https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/densenet_2018_04_27.tgz), building and loading them using [Apache TVM](https://tvm.apache.org/).

Although I applied the [indicated preprocessing](https://github.com/tensorflow/models/tree/master/research/slim/preprocessing), it seems that the models produce results much different from the ground truths expected in a small validation dataset I use, and they are consistent to this across each other.

Also, they tend to predict similar ImageNet labels (e.g., IDs 1000, 340 and 341 are common part to almost all top-5 classification predictions they produce).

I suspect this is related to preprocessing somehow, but I have followed the process step by step and the results are the same.
```


### Standalone code to reproduce the issue

```shell
1) Build model in TVM modules (one for PB and one for TFLite)
2) Execute the modules, applying preprocessing.
3) Compare top-5 predictions with dataset ground truths.
```


### Relevant log output

_No response_</details>"
56182,tf.keras.datasets.cifar10.load_data(path='cifar-10-python.tar.gz'),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

2.8.0

### Custom Code

No

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The tf.keras.datasets.mnist.load_data(path='/user/.keras/datasets/mnist.npz') works fine, but the tf.keras.datasets.cifar10.load_data() have no path param, which means everytime everyone runs his/her jupyter scripts that requires cifar10 datasets, he/she had to download the datasets from source again, it's really boring and time-consuming :)
```


### Standalone code to reproduce the issue

```shell
import autokeras as ak
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import cifar10

(x_train, y_train), (x_test, y_test) = cifar10.load_data()
```


### Relevant log output

_No response_</details>"
56181,`tf.image.resize` different result when inside a `tf.function`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.9.0-rc2-42-g8a20d54a3c1 2.9.0

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When you put a `tf.image.resize` op inside a `tf.function` whilst using `tf.RaggedTensor`s, the result changes.
```


### Standalone code to reproduce the issue

```python

import numpy as np
import tensorflow as tf
np.random.seed(0)
batch1 = tf.cast(tf.ragged.constant([255*np.random.uniform(size=(2000, 2000))]), tf.uint8)
batch1 = tf.expand_dims(batch1, axis=-1)
batch1 = tf.concat([batch1, batch1, batch1], axis=-1)

sign = tf.RaggedTensorSpec((1, None, None, 3), tf.uint8, 2, tf.int64)

@tf.function(input_signature=(sign,))
def resize_tf(images):
  return tf.image.resize(images, (50, 50)) / 255.
  
def resize_non_tf(images):
  return tf.image.resize(images, (50, 50)) / 255.
  
print(tf.reduce_mean(resize_tf(batch1)))
print(tf.reduce_mean(resize_non_tf(batch1)))

and then run `python3 test.py`
```


### Relevant log output

```shell
tf.Tensor(0.49723607, shape=(), dtype=float32)
tf.Tensor(0.497236, shape=(), dtype=float32)
```
</details>"
56180,"GPU and NNAPI Delegate not available due to ""dynamic-sized tensors ""","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

binary

### Tensorflow Version

tf 2.5.0

### Custom Code

Yes

### OS Platform and Distribution

Windows 11

### Mobile device

Android 10

### Python version

3.6.0

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

Not used. 

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am developing an RNN to process some time sequences. I am using both LSTM and GRU and need to convert the trained model to Tflite to use it under Android (version 10).
The problem is that despite my best efforts, I am unable to use the GPU and NNAPI delegates under the android device I am using due to the exception that is thrown:

java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.

I state that I am not using CUDA / cuDNN since the laptop I am using does not have a GPU and the model I train is relatively simple.

I don't know if this depends on the batch size set to None, on the tensorflow version, on the tf.lite.OpsSet.SELECT_TF_OPS option being converted, or simply on the fact that this type of layers (LSTM, GRU) cannot support these types of delegates.
```


### Standalone code to reproduce the issue

```shell
model = tf.keras.models.Sequential([                            
            tf.keras.layers.InputLayer(input_shape=(21 * 3 * 2 * 21, )),
            tf.keras.layers.Reshape((21, 3 * 2 * 21)),        
            tf.keras.layers.LSTM(32, return_sequences=True),
            tf.keras.layers.Dropout(0.40, seed=42),
            tf.keras.layers.LSTM(32),        
            tf.keras.layers.Dropout(0.50, seed=42),             
            tf.keras.layers.Dense(27, activation='softmax')])    
           
    # Model Compile
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
        history = model.fit(
            X_train,
            y_train,
            epochs=2000,
            batch_size=256,
            validation_data=(X_val, y_val),
            callbacks=callbacks,
            shuffle=True,
        )

    model.save(model_save_path, include_optimizer=False)   
    model = tf.keras.models.load_model(model_save_path)

    converter = tf.lite.TFLiteConverter.from_keras_model(model)  
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.experimental_new_converter=True
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS]

    tflite_quantized_model = converter.convert()

    open(tflite_save_path, 'wb').write(tflite_quantized_model)

    interpreter = tf.lite.Interpreter(model_path=tflite_save_path)
    interpreter.allocate_tensors()
```


### Relevant log output

```shell
java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.
```
</details>"
56179,C++ compilation of rule '//tensorflow/core/kernels/image:extract_image_patches_op' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf2.7

### Custom Code

Yes

### OS Platform and Distribution

linux

### Mobile device

_No response_

### Python version

python3.7.12

### Bazel version

3.7.2

### GCC/Compiler version

7.3.1

### CUDA/cuDNN version

rocm5.1

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!  when i compile TF2.7 form source code
```


### Standalone code to reproduce the issue

```shell
bazel build -c opt --config=rocm //tensorflow/tools/pip_package:build_pip_package --verbose_failures
```


### Relevant log output

```shell
ERROR: /data/jenkins_workspace/workspace/tensorflow2x_release_bak/tensorflow/core/kernels/image/BUILD:241:18: C++ compilation of rule '//tensorflow/core/kernels/image:extract_image_patches_op' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /root/.cache/bazel/_bazel_root/13db7a7ceaa19d7f3d62a2ad5999e2b1/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/opt/dtk-22.04/hip/lib:/opt/dtk-22.04/llvm/lib:/opt/dtk-22.04/lib:/opt/dtk-22.04/lib64: \
    PATH=/data/jenkins_workspace/workspace/tensorflow2x_release_bak/Depend/bin:/opt/dtk-22.04/bin:/opt/dtk-22.04/llvm/bin:/opt/dtk-22.04/hip/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/local/python3.7.12/lib/python3.7/site-packages \
    ROCBLAS_TENSILE_LIBPATH=/opt/dtk-22.04/lib/library \
    ROCM_PATH=/opt/dtk-22.04 \
    TF2_BEHAVIOR=1 \
  external/local_config_rocm/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++14' -MD -MF bazel-out/k8-opt/bin/tensorflow/core/kernels/image/_objs/extract_image_patches_op/extract_image_patches_op.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/core/kernels/image/_objs/extract_image_patches_op/extract_image_patches_op.pic.o' -fPIC -DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL -DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL '-DEIGEN_ALTIVEC_USE_CUSTOM_PACK=0' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -iquote . -iquote bazel-out/k8-opt/bin -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/gif -iquote bazel-out/k8-opt/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/k8-opt/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/k8-opt/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/k8-opt/bin/external/zlib -iquote external/local_config_rocm -iquote bazel-out/k8-opt/bin/external/local_config_rocm -iquote external/png -iquote bazel-out/k8-opt/bin/external/png -iquote external/mkl_dnn_v1 -iquote bazel-out/k8-opt/bin/external/mkl_dnn_v1 -iquote external/double_conversion -iquote bazel-out/k8-opt/bin/external/double_conversion -iquote external/local_config_cuda -iquote bazel-out/k8-opt/bin/external/local_config_cuda -iquote external/local_config_tensorrt -iquote bazel-out/k8-opt/bin/external/local_config_tensorrt -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -isystem external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/k8-opt/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -isystem external/local_config_rocm/rocm -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm -isystem external/local_config_rocm/rocm/rocm/include -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include -isystem external/local_config_rocm/rocm/rocm/include/rocrand -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include/rocrand -isystem external/local_config_rocm/rocm/rocm/include/roctracer -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include/roctracer -isystem external/png -isystem bazel-out/k8-opt/bin/external/png -isystem external/mkl_dnn_v1/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/include -isystem external/mkl_dnn_v1/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src -isystem external/mkl_dnn_v1/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/common -isystem external/mkl_dnn_v1/src/common/ittnotify -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/common/ittnotify -isystem external/mkl_dnn_v1/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/cpu -isystem external/mkl_dnn_v1/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/cpu/gemm -isystem external/mkl_dnn_v1/src/cpu/x64/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/cpu/x64/xbyak -isystem external/double_conversion -isystem bazel-out/k8-opt/bin/external/double_conversion -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include -w -DAUTOLOAD_DYNAMIC_KERNELS '-std=c++14' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DTENSORFLOW_USE_XLA=1' '-DTENSORFLOW_USE_ROCM=1' -DINTEL_MKL -msse3 -pthread '-DTENSORFLOW_USE_ROCM=1' '-DTENSORFLOW_USE_XLA=1' '-DINTEL_MKL=1' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' '-DTENSORFLOW_USE_ROCM=1' -D__HIP_PLATFORM_HCC__ -DEIGEN_USE_HIP -no-canonical-prefixes -fno-canonical-system-headers -c tensorflow/core/kernels/image/extract_image_patches_op.cc -o bazel-out/k8-opt/bin/tensorflow/core/kernels/image/_objs/extract_image_patches_op/extract_image_patches_op.pic.o)
Execution platform: @local_execution_config_platform//:platform
In file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:97:0,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/kernels/image/extract_image_patches_op.h:19,
                 from tensorflow/core/kernels/image/extract_image_patches_op.cc:21:
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorImagePatch.h: In static member function 'static void Eigen::internal::EvalRange<Evaluator, StorageIndex, true>::run(Evaluator*, StorageIndex, StorageIndex) [with Evaluator = Eigen::TensorEvaluator<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 4, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 4>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const std::complex<float>, 4, 1, long int>, 16, Eigen::MakePointer> > > >, Eigen::ThreadPoolDevice>; StorageIndex = long int]':
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorImagePatch.h:546:7: internal compiler error: in emit_move_insn, at expr.c:3698
       values[i] = coeff(index+i);
       ^~~~~~
Please submit a full bug report,
with preprocessed source if appropriate.
See <https://gcc.gnu.org/bugs/> for instructions.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 4326.567s, Critical Path: 534.32s
INFO: 11449 processes: 1314 internal, 10135 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
```
</details>"
56178,In Pose detection when I use front camera it display opposite mirror how to resolve it? ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

pod 'TensorFlowLiteSwift', '~> 0.0.1-nightly', :subspecs => ['CoreML', 'Metal']

### Custom Code

No

### OS Platform and Distribution

MacOS

### Mobile device

iPhone X

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
In Pose detection when I use front camera it display opposite mirror how to resolve it? also camera resolution also need to clear.
```


### Standalone code to reproduce the issue

```shell
Must display proper mirror frame & resolution when front camera start ie. iPhone default camera app front camera mirror & resolution.
```


### Relevant log output

_No response_</details>"
56177,Memory Leak with Tensorflow Experimental Save,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I have a loop where I am creating tensorflow datasets and then saving to directories for later use using tf.data.experimental.save.

I found that as the loop progresses, the memory being used greatly increases, until the process eventually crashes.

In this first example below, I do not save the file, and the memory stays at the same level throughout. However, if I save each dataset, the memory used increases each time.

Adding tf.keras.backend.clear_session() after each save appears to slow down the memory growth but doesn't fully stop it.

Thank you in advance for any help.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from humanize import naturalsize
import psutil

# First example, no memory increase
for i in range(10000):
    if i in range(1, 10001, 100):
        print(naturalsize(psutil.Process().memory_info().rss))
    data = tf.data.Dataset.from_tensors(np.array([0, 1, 2]))

# Second example, memory increase
for i in range(10000):
    if not i % 100:
        print(naturalsize(psutil.Process().memory_info().rss))
    data = tf.data.Dataset.from_tensors(np.array([0, 1, 2]))
    tf.data.experimental.save(data, path='~/Desktop/1')
# Grows by about 5 MB each 100 runs.
```


### Relevant log output

_No response_</details>"
56176,Xcode 13.3.1 build simple project failed on MacBook Air M1,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

No

### OS Platform and Distribution

macOS 

### Mobile device

iPhone 12

### Python version

 3.9.12

### Bazel version

5.0.0

### GCC/Compiler version

Apple clang version 13.1.6

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Xcode 13.3.1 build simple project failed on MacBook Air M1.
```


### Standalone code to reproduce the issue

```shell
git clone https://github.com/tensorflow/tensorflow tensorflow-2.9.0_ios
git checkout r2.9
cd tensorflow-2.9.0_ios/tensorflow/lite/examples/iOS/simple
pod install
open simple.xcworkspace
open simple.xcodeproj
Using Xcode to build simple project. But the following error occurred.
Use of undeclared identifier 'RegisterSelectedOps'
```


### Relevant log output

```shell
Use of undeclared identifier 'RegisterSelectedOps'
```
</details>"
56175,Xcode 13.3.1 builds ImageClassification iOS TensorFlow Lite example failed,"git clone https://github.com/tensorflow/examples
cd examples/lite/examples/image_classification/ios
pod install
open open ImageClassification.xcodeproj 
Using Xcode to build ImageClassification project. But the following error occurred.
No such module 'TensorFlowLite'"
56174,Xcode 13.3.1 build digit_classifier iOS TensorFlow Lite example failed,"git clone https://github.com/tensorflow/examples
cd examples/lite/examples/digit_classifier/iOS
pod install
open DigitClassifier.xcodeproj
Using Xcode to build digit_classifier project. But the following error occurred.
No such module 'Sketch'"
56173,[XLA] xla seems to optimise fast loops poorly,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.6

### GPU model and memory

Titan X

### Current Behaviour?

```shell
When vectorizing over an inner loop, XLA lifts the loop to the top level and runs many sequential vector operations. 
It would be much faster to sink the loop, with each thread running the full loop on a narrow slice of data.

Also, despite the body of the loop being very lightweight, XLA fails to perform effective fusion. It builds 4 fused kernels for the body of the loop, so the execution time is entirely dominated by kernel dispatch.

Also, despite the loop body being very light, XLA fails to fuse the loop, so there are a total of N * 4 kernels dispatched for N loop iterations.
```


### Standalone code to reproduce the issue

```shell
# Colab is here:
#   https://colab.research.google.com/drive/1KJhitFonQB0hdwJ8Tted2KVyC5vTu9ZA
# That colab will generate and dump the XLA intermediate files.

import jax
import jax.numpy as jnp

def run_model(model, xs, hidden):
  u, vf, bf, vr, br = model
  seq_len = xs.shape[-1]
  u = jnp.einsum('n b d, o k d -> k n b o', xs, u)
  u0, u1, u2 = u[0], u[1], u[2]
  def do_step(c, l):
    (u0, u1, u2, xs) = l
    f = jax.nn.sigmoid(u1 + vf*c + bf)
    c = f * c + (1. - f) * u0
    r = jax.nn.sigmoid(u2 + vr*c + br)
    h = r * c + (1. - r) * xs
    return c, h

  return jax.lax.scan(do_step, hidden, (u0, u1, u2, xs), unroll=1) # Loops over the 'L' dimension.

B, D, L = 64, 512, 128
h = jnp.ones( (B, D))
x = jnp.ones( (L, B, D))

key = jax.random.PRNGKey(1)
ks = jax.random.split(key, 5)
shapes = [ (D,3,D), (D,), (D,), (D,), (D,) ]
model = [ jnp.ones(shape) for shape in shapes ]

f = jax.jit(run_model)
o = f(model, x, h) # <---- generates L*4 kernel dispatches.
```


### Relevant log output

_No response_</details>"
56171,LLVM 15 MLIR compilation bug,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.9.0

### Custom Code

No

### OS Platform and Distribution

ArchLinux

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

GCC 12.1 & LLVM 15

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
ERROR: ./tensorflow/BUILD:1116:21: Linking tensorflow/libtensorflow_cc.so.2.10.0 failed: (Exit 1): gcc failed: error executing command /usr/bin/gcc @bazel-out/k8-opt/bin/tensorflow/libtensorflow_cc.so.2.10.0-2.params
ld.lld: error: undefined symbol: mlir::MLIRContext::getOrLoadDialect(llvm::StringRef, mlir::TypeID, llvm::function_ref<std::unique_ptr<mlir::Dialect, std::default_delete<mlir::Dialect>> ()>)
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(std::_Function_handler<mlir::Dialect* (mlir::MLIRContext*), void mlir::DialectRegistry::insert<mlir::arith::ArithmeticDialect>()::'lambda'(mlir::MLIRContext*)>::_M_invoke(std::_Any_data const&, mlir::MLIRContext*&&))
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(std::_Function_handler<mlir::Dialect* (mlir::MLIRContext*), void mlir::DialectRegistry::insert<mlir::func::FuncDialect>()::'lambda'(mlir::MLIRContext*)>::_M_invoke(std::_Any_data const&, mlir::MLIRContext*&&))
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(std::_Function_handler<mlir::Dialect* (mlir::MLIRContext*), void mlir::DialectRegistry::insert<mlir::scf::SCFDialect>()::'lambda'(mlir::MLIRContext*)>::_M_invoke(std::_Any_data const&, mlir::MLIRContext*&&))
>>> referenced 66 more times

ld.lld: error: undefined symbol: mlir::PassManager::~PassManager()
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(std::default_delete<tensorflow::tfr::TFRDecomposeContext>::operator()(tensorflow::tfr::TFRDecomposeContext*) const (.part.0))
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(tensorflow::tfr::TFRDecomposeContext::TFRDecomposeContext(mlir::ModuleOp) (.cold))
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(tensorflow::internal_statusor::StatusOrData<std::unique_ptr<tensorflow::tfr::TFRDecomposeContext, std::default_delete<tensorflow::tfr::TFRDecomposeContext>>>::~StatusOrData())
>>> referenced 21 more times

ld.lld: error: undefined symbol: mlir::detail::FallbackTypeIDResolver::registerImplicitTypeID(llvm::StringRef)
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(std::_Function_handler<mlir::Dialect* (mlir::MLIRContext*), void mlir::DialectRegistry::insert<mlir::tf_executor::TensorFlowExecutorDialect>()::'lambda'(mlir::MLIRContext*)>::_M_invoke(std::_Any_data const&, mlir::MLIRContext*&&))
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(std::_Function_handler<mlir::Dialect* (mlir::MLIRContext*), void mlir::DialectRegistry::insert<mlir::TFR::TFRDialect>()::'lambda'(mlir::MLIRContext*)>::_M_invoke(std::_Any_data const&, mlir::MLIRContext*&&))
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(std::_Function_handler<mlir::Dialect* (mlir::MLIRContext*), void mlir::DialectRegistry::insert<mlir::tf_device::TensorFlowDeviceDialect>()::'lambda'(mlir::MLIRContext*)>::_M_invoke(std::_Any_data const&, mlir::MLIRContext*&&))
>>> referenced 2498 more times

ld.lld: error: undefined symbol: mlir::Region::~Region()
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(mlir::OperationState::~OperationState())
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(mlir::func::ReturnOp mlir::OpBuilder::create<mlir::func::ReturnOp, mlir::ResultRange>(mlir::Location, mlir::ResultRange&&))
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(mlir::OwningOpRef<mlir::ModuleOp> mlir::detail::constructContainerOpForParserIfNecessary<mlir::ModuleOp>(mlir::Block*, mlir::MLIRContext*, mlir::Location))
>>> referenced 1768 more times

ld.lld: error: undefined symbol: mlir::Operation::erase()
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(tensorflow::tfr::TFRDecomposeContext::Destroy())
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(mlir::OwningOpRef<mlir::ModuleOp> mlir::detail::constructContainerOpForParserIfNecessary<mlir::ModuleOp>(mlir::Block*, mlir::MLIRContext*, mlir::Location))
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(mlir::OwningOpRef<mlir::ModuleOp> mlir::detail::constructContainerOpForParserIfNecessary<mlir::ModuleOp>(mlir::Block*, mlir::MLIRContext*, mlir::Location))
>>> referenced 309 more times

ld.lld: error: undefined symbol: mlir::Attribute::getContext() const
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(tensorflow::tfr::TFRDecomposeContext::TFRDecomposeContext(mlir::ModuleOp))
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(mlir::func::ReturnOp mlir::OpBuilder::create<mlir::func::ReturnOp, mlir::ResultRange>(mlir::Location, mlir::ResultRange&&))
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(mlir::OwningOpRef<mlir::ModuleOp> mlir::detail::constructContainerOpForParserIfNecessary<mlir::ModuleOp>(mlir::Block*, mlir::MLIRContext*, mlir::Location))
>>> referenced 5339 more times

ld.lld: error: undefined symbol: mlir::PassManager::PassManager(mlir::MLIRContext*, mlir::OpPassManager::Nesting, llvm::StringRef)
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(tensorflow::tfr::TFRDecomposeContext::TFRDecomposeContext(mlir::ModuleOp))
>>> referenced by cpu_compiler.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/cpu_compiler/cpu_compiler.pic.o:(xla::cpu::(anonymous namespace)::LowerMLIRModule(mlir::ModuleOp, mlir::MLIRContext&))
>>> referenced by dot_op_emitter.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/dot_op_emitter/dot_op_emitter.pic.o:(xla::cpu::(anonymous namespace)::DotOpEmitter::EmitLinalgMatmul()::'lambda'(mlir::OpBuilder*, mlir::func::FuncOp)::operator()(mlir::OpBuilder*, mlir::func::FuncOp) const)
>>> referenced 6 more times

ld.lld: error: undefined symbol: mlir::OpPassManager::nest(llvm::StringRef)
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(tensorflow::tfr::TFRDecomposeContext::TFRDecomposeContext(mlir::ModuleOp))
>>> referenced by cpu_compiler.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/cpu_compiler/cpu_compiler.pic.o:(void mlir::OpPassManager::addNestedPass<mlir::func::FuncOp>(std::unique_ptr<mlir::Pass, std::default_delete<mlir::Pass>>))
>>> referenced by compile_mlir_util.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tensorflow/_objs/compile_mlir_util_no_tf_dialect_passes/compile_mlir_util.pic.o:(tensorflow::CreateConvertMlirToXlaHloPipeline(mlir::OpPassManager&, llvm::StringRef, bool, llvm::MutableArrayRef<std::unique_ptr<mlir::Pass, std::default_delete<mlir::Pass>>>, bool))
>>> referenced 37 more times

ld.lld: error: undefined symbol: mlir::OpPassManager::addPass(std::unique_ptr<mlir::Pass, std::default_delete<mlir::Pass>>)
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(tensorflow::tfr::TFRDecomposeContext::TFRDecomposeContext(mlir::ModuleOp))
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(tensorflow::tfr::TFRDecomposeContext::TFRDecomposeContext(mlir::ModuleOp))
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(tensorflow::tfr::TFRDecomposeContext::TFRDecomposeContext(mlir::ModuleOp))
>>> referenced 156 more times

ld.lld: error: undefined symbol: mlir::RegisteredOperationName::lookup(llvm::StringRef, mlir::MLIRContext*)
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(mlir::func::ReturnOp mlir::OpBuilder::create<mlir::func::ReturnOp, mlir::ResultRange>(mlir::Location, mlir::ResultRange&&))
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(mlir::OwningOpRef<mlir::ModuleOp> mlir::detail::constructContainerOpForParserIfNecessary<mlir::ModuleOp>(mlir::Block*, mlir::MLIRContext*, mlir::Location))
>>> referenced by canonicalize.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/passes/canonicalize.pic.o:(mlir::arith::MulIOp mlir::OpBuilder::create<mlir::arith::MulIOp, mlir::Value&, mlir::Value&>(mlir::Location, mlir::Value&, mlir::Value&))
>>> referenced 2864 more times

ld.lld: error: undefined symbol: mlir::OperationState::OperationState(mlir::Location, mlir::OperationName)
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(mlir::func::ReturnOp mlir::OpBuilder::create<mlir::func::ReturnOp, mlir::ResultRange>(mlir::Location, mlir::ResultRange&&))
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(mlir::OwningOpRef<mlir::ModuleOp> mlir::detail::constructContainerOpForParserIfNecessary<mlir::ModuleOp>(mlir::Block*, mlir::MLIRContext*, mlir::Location))
>>> referenced by canonicalize.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/passes/canonicalize.pic.o:(mlir::arith::MulIOp mlir::OpBuilder::create<mlir::arith::MulIOp, mlir::Value&, mlir::Value&>(mlir::Location, mlir::Value&, mlir::Value&))
>>> referenced 2896 more times

ld.lld: error: undefined symbol: mlir::ValueRange::ValueRange(mlir::ResultRange)
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(mlir::func::ReturnOp mlir::OpBuilder::create<mlir::func::ReturnOp, mlir::ResultRange>(mlir::Location, mlir::ResultRange&&))
>>> referenced by canonicalize.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/passes/canonicalize.pic.o:(mlir::TFR::(anonymous namespace)::SimplifySCFIfOp::InlineRegion(mlir::Location, mlir::PatternRewriter&, mlir::Operation*, mlir::Region*) const (.constprop.0))
>>> referenced by AffineToStandard.cpp
>>>               bazel-out/k8-opt/bin/external/llvm-project/mlir/_objs/AffineToStandard/AffineToStandard.pic.o:((anonymous namespace)::AffineForLowering::matchAndRewrite(mlir::AffineForOp, mlir::PatternRewriter&) const)
>>> referenced 191 more times

ld.lld: error: undefined symbol: mlir::OpBuilder::create(mlir::OperationState const&)
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(mlir::func::ReturnOp mlir::OpBuilder::create<mlir::func::ReturnOp, mlir::ResultRange>(mlir::Location, mlir::ResultRange&&))
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(mlir::OwningOpRef<mlir::ModuleOp> mlir::detail::constructContainerOpForParserIfNecessary<mlir::ModuleOp>(mlir::Block*, mlir::MLIRContext*, mlir::Location))
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(tensorflow::tfr::TFRDecomposeContext::ExpandNode(tensorflow::NodeDef const&, absl::lts_20211102::string_view))
>>> referenced 2960 more times

ld.lld: error: undefined symbol: mlir::DialectRegistry::insert(mlir::TypeID, llvm::StringRef, std::function<mlir::Dialect* (mlir::MLIRContext*)> const&)
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(void mlir::DialectRegistry::insert<mlir::func::FuncDialect, mlir::scf::SCFDialect, mlir::shape::ShapeDialect, mlir::TF::TensorFlowDialect, mlir::tf_device::TensorFlowDeviceDialect, mlir::tf_executor::TensorFlowExecutorDialect, mlir::TFR::TFRDialect>())
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(void mlir::DialectRegistry::insert<mlir::func::FuncDialect, mlir::scf::SCFDialect, mlir::shape::ShapeDialect, mlir::TF::TensorFlowDialect, mlir::tf_device::TensorFlowDeviceDialect, mlir::tf_executor::TensorFlowExecutorDialect, mlir::TFR::TFRDialect>())
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(void mlir::DialectRegistry::insert<mlir::func::FuncDialect, mlir::scf::SCFDialect, mlir::shape::ShapeDialect, mlir::TF::TensorFlowDialect, mlir::tf_device::TensorFlowDeviceDialect, mlir::tf_executor::TensorFlowExecutorDialect, mlir::TFR::TFRDialect>())
>>> referenced 132 more times

ld.lld: error: undefined symbol: mlir::ModuleOp::build(mlir::OpBuilder&, mlir::OperationState&, llvm::Optional<llvm::StringRef>)
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(mlir::OwningOpRef<mlir::ModuleOp> mlir::detail::constructContainerOpForParserIfNecessary<mlir::ModuleOp>(mlir::Block*, mlir::MLIRContext*, mlir::Location))
>>> referenced by cpu_compiler.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/cpu_compiler/cpu_compiler.pic.o:(xla::cpu::CpuCompiler::CompileAheadOfTime(std::unique_ptr<xla::HloModuleGroup, std::default_delete<xla::HloModuleGroup>>, xla::AotCompilationOptions const&))

ld.lld: error: undefined symbol: mlir::detail::TypeIDResolver<mlir::ModuleOp, void>::id
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(mlir::OwningOpRef<mlir::ModuleOp> mlir::detail::constructContainerOpForParserIfNecessary<mlir::ModuleOp>(mlir::Block*, mlir::MLIRContext*, mlir::Location))
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(mlir::OwningOpRef<mlir::ModuleOp> mlir::detail::constructContainerOpForParserIfNecessary<mlir::ModuleOp>(mlir::Block*, mlir::MLIRContext*, mlir::Location))
>>> referenced by decompose.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/passes/decompose.pic.o:(mlir::TFR::(anonymous namespace)::DecomposeTFOpsPass::runOnOperation())
>>> referenced 70 more times

ld.lld: error: undefined symbol: llvm::ilist_traits<mlir::Operation>::transferNodesFromList(llvm::ilist_traits<mlir::Operation>&, llvm::ilist_iterator<llvm::ilist_detail::node_options<mlir::Operation, false, false, void>, false, false>, llvm::ilist_iterator<llvm::ilist_detail::node_options<mlir::Operation, false, false, void>, false, false>)
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(mlir::OwningOpRef<mlir::ModuleOp> mlir::detail::constructContainerOpForParserIfNecessary<mlir::ModuleOp>(mlir::Block*, mlir::MLIRContext*, mlir::Location))
>>> referenced by Utils.cpp
>>>               bazel-out/k8-opt/bin/external/llvm-project/mlir/_objs/SCFUtils/Utils.pic.o:(mlir::promoteIfSingleIteration(mlir::scf::ForOp))
>>> referenced by Utils.cpp
>>>               bazel-out/k8-opt/bin/external/llvm-project/mlir/_objs/SCFUtils/Utils.pic.o:(mlir::replaceLoopWithNewYields(mlir::OpBuilder&, mlir::scf::ForOp, mlir::ValueRange, std::function<llvm::SmallVector<mlir::Value, 6u> (mlir::OpBuilder&, mlir::Location, llvm::ArrayRef<mlir::BlockArgument>)>))
>>> referenced 12 more times

ld.lld: error: undefined symbol: mlir::ModuleOp::verifyInvariants()
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(mlir::OwningOpRef<mlir::ModuleOp> mlir::detail::constructContainerOpForParserIfNecessary<mlir::ModuleOp>(mlir::Block*, mlir::MLIRContext*, mlir::Location))

ld.lld: error: undefined symbol: llvm::ilist_detail::SpecificNodeAccess<llvm::ilist_detail::node_options<mlir::Operation, false, false, void>>::getValuePtr(llvm::ilist_node_impl<llvm::ilist_detail::node_options<mlir::Operation, false, false, void>>*)
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(mlir::OwningOpRef<mlir::ModuleOp> mlir::detail::constructContainerOpForParserIfNecessary<mlir::ModuleOp>(mlir::Block*, mlir::MLIRContext*, mlir::Location))
>>> referenced by canonicalize.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/passes/canonicalize.pic.o:(mlir::TFR::(anonymous namespace)::UnrollSCFForOp::matchAndRewrite(mlir::scf::ForOp, mlir::PatternRewriter&) const)
>>> referenced by decompose.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/passes/decompose.pic.o:(_ZN4llvm12function_refIFN4mlir10WalkResultEPNS1_9OperationEEE11callback_fnIZNS1_6detail4walkILNS1_9WalkOrderE1EZNS1_3TFR12_GLOBAL__N_118DecomposeTFOpsPass18InlineTFRFuncCallsEvEUlNSB_6CallOpEE_SE_S2_EENSt9enable_ifIXaantsrNS_11disjunctionIJSt7is_sameIT1_S4_ESI_ISJ_PNS1_6RegionEESI_ISJ_PNS1_5BlockEEEEE5valuesrSI_IT2_S2_E5valueESS_E4typeES4_OT0_EUlS4_E_EES2_lS4_)
>>> referenced 541 more times

ld.lld: error: undefined symbol: mlir::Operation::remove()
>>> referenced by tfr_decompose_ctx.cc
>>>               bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tfr/_objs/tfr_decompose_ctx/tfr_decompose_ctx.pic.o:(mlir::OwningOpRef<mlir::ModuleOp> mlir::detail::constructContainerOpForParserIfNecessary<mlir::ModuleOp>(mlir::Block*, mlir::MLIRContext*, mlir::Location))

ld.lld: error: too many errors emitted, stopping now (use --error-limit=0 to see all errors)
collect2: error: ld returned 1 exit status
```


### Standalone code to reproduce the issue

```shell
./configure && bazel build
```


### Relevant log output

_No response_</details>"
56170,"Variable constant folding is failed. Please consider using enabling `experimental_enable_resource_variables` flag in the TFLite converter object. For example, converter.experimental_enable_resource_variables = True","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.8

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am trying to export a generator model (part of GAN) with TFLite which fails. I provide a Collab to reproduce the error.
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1SPUX9b5hHw0mBeU11y1fOE6WigK3CrgU?usp=sharing


# example of defining a u-net encoder-decoder generator model
from tensorflow.keras.initializers import RandomNormal
from tensorflow.keras.layers import Input, Activation, BatchNormalization, Concatenate, Conv2D, Conv2DTranspose, Dropout, LeakyReLU
from tensorflow.keras.models import Model
from tensorflow.keras.utils import plot_model

# define an encoder block
def define_encoder_block(layer_in, n_filters, batchnorm=True):
    # weight initialization
    init = RandomNormal(stddev=0.02)
    # add downsampling layer
    g = Conv2D(n_filters, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init)(layer_in)
    # conditionally add batch normalization
    if batchnorm:
        g = BatchNormalization()(g, training=True)
    # leaky relu activation
    g = LeakyReLU(alpha=0.2)(g)
    return g

# define a decoder block
def decoder_block(layer_in, skip_in, n_filters, dropout=True):
    # weight initialization
    init = RandomNormal(stddev=0.02)
    # add upsampling layer
    g = Conv2DTranspose(n_filters, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init)(layer_in)
    # add batch normalization
    g = BatchNormalization()(g, training=True)
    # conditionally add dropout
    if dropout:
        g = Dropout(0.5)(g, training=True)
    # merge with skip connection
    g = Concatenate()([g, skip_in])
    # relu activation
    g = Activation('relu')(g)
    return g

# define the standalone generator model
def define_generator(image_shape=(256, 256, 3)):
    # weight initialization
    init = RandomNormal(stddev=0.02)
    # image input
    in_image = Input(shape=image_shape)
    # encoder model: C64-C128-C256-C512-C512-C512-C512-C512
    e1 = define_encoder_block(in_image, 64, batchnorm=False)
    e2 = define_encoder_block(e1, 128)
    e3 = define_encoder_block(e2, 256)
    e4 = define_encoder_block(e3, 512)
    e5 = define_encoder_block(e4, 512)
    e6 = define_encoder_block(e5, 512)
    e7 = define_encoder_block(e6, 512)
    # bottleneck, no batch norm and relu
    b = Conv2D(512, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init)(e7)
    b = Activation('relu')(b)
    # decoder model: CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128
    d1 = decoder_block(b, e7, 512)
    d2 = decoder_block(d1, e6, 512)
    d3 = decoder_block(d2, e5, 512)
    d4 = decoder_block(d3, e4, 512, dropout=False)
    d5 = decoder_block(d4, e3, 256, dropout=False)
    d6 = decoder_block(d5, e2, 128, dropout=False)
    d7 = decoder_block(d6, e1, 64, dropout=False)
    # output
    g = Conv2DTranspose(3, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init)(d7)
    out_image = Activation('tanh')(g)
    # define model
    model = Model(in_image, out_image)
    return model

# define image shape
image_shape = (256, 256, 3)
# create the model
model = define_generator(image_shape)
# Convert the model
converter = tf.lite.TFLiteConverter.from_keras_model(model) # path to the SavedModel directory
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]
converter.experimental_enable_resource_variables = True
tflite_model = converter.convert()

# Save the model.
with open('model.tflite', 'wb') as f:
    f.write(tflite_model)
```
```


### Relevant log output

```shell
---------------------------------------------------------------------------
ConverterError                            Traceback (most recent call last)
Input In [4], in <cell line: 10>()
      5 converter.target_spec.supported_ops = [
      6   tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
      7   tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
      8 ]
      9 converter.experimental_enable_resource_variables = True
---> 10 tflite_model = converter.convert()
     12 # Save the model.
     13 with open('model.tflite', 'wb') as f:

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\lite\python\lite.py:803, in _export_metrics.<locals>.wrapper(self, *args, **kwargs)
    800 @functools.wraps(convert_func)
    801 def wrapper(self, *args, **kwargs):
    802   # pylint: disable=protected-access
--> 803   return self._convert_and_export_metrics(convert_func, *args, **kwargs)

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\lite\python\lite.py:789, in TFLiteConverterBase._convert_and_export_metrics(self, convert_func, *args, **kwargs)
    787 self._save_conversion_params_metric()
    788 start_time = time.process_time()
--> 789 result = convert_func(self, *args, **kwargs)
    790 elapsed_time_ms = (time.process_time() - start_time) * 1000
    791 if result:

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\lite\python\lite.py:1210, in TFLiteKerasModelConverterV2.convert(self)
   1197 @_export_metrics
   1198 def convert(self):
   1199   """"""Converts a keras model based on instance variables.
   1200 
   1201   Returns:
   (...)
   1208       Invalid quantization parameters.
   1209   """"""
-> 1210   saved_model_convert_result = self._convert_as_saved_model()
   1211   if saved_model_convert_result:
   1212     return saved_model_convert_result

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\lite\python\lite.py:1192, in TFLiteKerasModelConverterV2._convert_as_saved_model(self)
   1189   graph_def, input_tensors, output_tensors = (
   1190       self._convert_keras_to_saved_model(temp_dir))
   1191   if self.saved_model_dir:
-> 1192     return super(TFLiteKerasModelConverterV2,
   1193                  self).convert(graph_def, input_tensors, output_tensors)
   1194 finally:
   1195   shutil.rmtree(temp_dir, True)

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\lite\python\lite.py:1003, in TFLiteConverterBaseV2.convert(self, graph_def, input_tensors, output_tensors)
    998   logging.info(""Using new converter: If you encounter a problem ""
    999                ""please file a bug. You can opt-out ""
   1000                ""by setting experimental_new_converter=False"")
   1002 # Converts model.
-> 1003 result = _convert_graphdef(
   1004     input_data=graph_def,
   1005     input_tensors=input_tensors,
   1006     output_tensors=output_tensors,
   1007     **converter_kwargs)
   1009 return self._optimize_tflite_model(
   1010     result, self._quant_mode, quant_io=self.experimental_new_quantizer)

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\lite\python\convert_phase.py:213, in convert_phase.<locals>.actual_decorator.<locals>.wrapper(*args, **kwargs)
    211   else:
    212     report_error_message(str(converter_error))
--> 213   raise converter_error from None  # Re-throws the exception.
    214 except Exception as error:
    215   report_error_message(str(error))

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\lite\python\convert_phase.py:206, in convert_phase.<locals>.actual_decorator.<locals>.wrapper(*args, **kwargs)
    203 @functools.wraps(func)
    204 def wrapper(*args, **kwargs):
    205   try:
--> 206     return func(*args, **kwargs)
    207   except ConverterError as converter_error:
    208     if converter_error.errors:

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\lite\python\convert.py:774, in convert_graphdef(input_data, input_tensors, output_tensors, **kwargs)
    771   else:
    772     model_flags.output_arrays.append(util.get_tensor_name(output_tensor))
--> 774 data = convert(
    775     model_flags.SerializeToString(),
    776     conversion_flags.SerializeToString(),
    777     input_data.SerializeToString(),
    778     debug_info_str=debug_info.SerializeToString() if debug_info else None,
    779     enable_mlir_converter=enable_mlir_converter)
    780 return data

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\lite\python\convert.py:306, in convert(model_flags_str, conversion_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    304     for error_data in _metrics_wrapper.retrieve_collected_errors():
    305       converter_error.append_error(error_data)
--> 306     raise converter_error
    308 return _run_deprecated_conversion_binary(
    309     model_flags_str, conversion_flags_str, input_data_str, debug_info_str)

ConverterError: Variable constant folding is failed. Please consider using enabling `experimental_enable_resource_variables` flag in the TFLite converter object. For example, converter.experimental_enable_resource_variables = True
```
</details>"
56169,Unexpected output when lambda layers are used in a for loop,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9

### Custom Code

Yes

### OS Platform and Distribution

Windows 10 x64

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The attached code should produce an output that is equal to the input.  All it does is take the input vector, split it, and reconcatenate it.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.keras import layers, models

test_input_data = tf.expand_dims(tf.linspace(0.0, 1.0, 6), axis=0)

input_layer = layers.Input(batch_shape=(1, 6))

outputs = []
for i_channel in range(6):
    print(f'Initialising layer that should extract element #{i_channel} - (expecting {test_input_data[0, i_channel]})')
    outputs.append(layers.Lambda(lambda c: c[..., i_channel])(input_layer))
cat_outputs = layers.Concatenate()(outputs)

model = models.Model(inputs=input_layer, outputs=cat_outputs, name='repro_model')

y = model(test_input_data)

print(y)
```


### Relevant log output

```shell
Initialising layer that should extract element #0 - (expecting 0.0)
Initialising layer that should extract element #1 - (expecting 0.20000000298023224)
Initialising layer that should extract element #2 - (expecting 0.4000000059604645)
Initialising layer that should extract element #3 - (expecting 0.6000000238418579)
Initialising layer that should extract element #4 - (expecting 0.800000011920929)
Initialising layer that should extract element #5 - (expecting 1.0)
tf.Tensor([1. 1. 1. 1. 1. 1.], shape=(6,), dtype=float32)
```
</details>"
56168,Compile without JAVA,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.9.0

### Custom Code

No

### OS Platform and Distribution

ArchLinux

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

5.1.1

### GCC/Compiler version

12.1

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When there is any trace of a JDK present on your system but you _don't need any Java support_ or bindings, TensorFlow nevertheless refuses to compile and complains:
_FATAL: Could not find system javabase. Ensure JAVA_HOME is set, or javac is on your PATH._
```


### Standalone code to reproduce the issue

```shell
https://github.com/archlinux/svntogit-community/blob/packages/tensorflow/trunk/PKGBUILD :

  bazel build \
    //tensorflow:libtensorflow.so \
    //tensorflow:libtensorflow_cc.so \
    //tensorflow:install_headers \
    //tensorflow/tools/pip_package:build_pip_package
  bazel-bin/tensorflow/tools/pip_package/build_pip_package ""${srcdir}""/tmp
```


### Relevant log output

_No response_</details>"
56166,benchmark_model tool get Failed to apply XNNPACK delegate error.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.8

### Custom Code

No

### OS Platform and Distribution

Android 11

### Mobile device

android_arm64

### Python version

_No response_

### Bazel version

4.2.1

### GCC/Compiler version

7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

[Following this guide](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark),I successfully compiled benchmark_model tool.
It's ok when I run this cmd `adb shell /data/local/tmp/benchmark/benchmark_model --graph=/data/local/tmp/benchmark/model.tflite --num_threads=1 --num_runs=1000`. But when I add `--use_xnnpack=true`, I getting shuch error:
```
STARTING!
Log parameter values verbosely: [0]
Graph: [pruning_model.tflite]
Use xnnpack: [1]
Loaded model pruning_model.tflite
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
XNNPACK delegate created.
Failed to apply XNNPACK delegate.
Benchmarking failed.
```

I tried to compile again with the following command, but it didn't work.
```
bazel build -c opt --config=android_arm64 --define tflite_with_xnnpack=true tensorflow/lite/tools/benchmark:benchmark_model
or
bazel build -c opt --config=android_arm64 --define tflite_with_xnnpack=true --define xnn_eable_qs8=true tensorflow/lite/tools/benchmark:benchmark_model
```


### Standalone code to reproduce the issue

```shell
Post-training quantization cod

convert = tf.lite.TFLiteConverter.from_keras_model(model)
convert.optimizations = [tf.lite.Optimize.EXPERIMENTAL_SPARSITY]
convert.representative_dataset = representative_dataset
tflite_quant_model = convert.convert()
```


### Relevant log output

```shell
STARTING!
Log parameter values verbosely: [0]
Graph: [pruning_model.tflite]
Use xnnpack: [1]
Loaded model pruning_model.tflite
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
XNNPACK delegate created.
Failed to apply XNNPACK delegate.
Benchmarking failed.
```
</details>"
56164,Image name supported by generate_tf_record.py script,"Hello,

There are now many scripts on the Internet that can be used to create TFRecord files. However, I think I have read from time to time that certain names such as `image_001.jpg `or` image001.jpg` are more likely to lead to errors than `001.jpg`. I have a large data set and want to get everything right when I take the picture. 

So I wonder whether it generally doesn't matter as long as the label files (XML files) are correct, i.e. that the image endings (.jpg or .png) in the CSV files are correct everywhere?

Many thanks for clarifying this mystery

"
56163,tfl.mul to tosa lowering in mlir,"I have tfl dialect as:

%1 = tfl.mul(%0, %0) {fused_activation_function = NONE} : (tensor<1x2x!quant.uniform<i8:f32, 0.0043027559295296669:-128>>, tensor<1x2x!quant.uniform<i8:f32, 0.0043027559295296669:-128>>)  tensor<1x2x!quant.uniform<i8:f32, 0.0047209961339831352:-128>>

But when convert this to tosa :
%1 = tosa.rescale(%0) {double_round = false, input_zp = -128 : i32, multiplier = [1073741824 : i32], output_zp = 0 : i32, per_channel = false, scale32 = true, shift = [30 : i32]} : (tensor<1x2xi8>)  tensor<1x2xi32>
%2 = tosa.rescale(%0) {double_round = false, input_zp = -128 : i32, multiplier = [1073741824 : i32], output_zp = 0 : i32, per_channel = false, scale32 = true, shift = [30 : i32]} : (tensor<1x2xi8>)  tensor<1x2xi32>
%3 = tosa.mul(%1, %2) {shift = 0 : i32} : (tensor<1x2xi32>, tensor<1x2xi32>)  tensor<1x2xi32>
%4 = tosa.rescale(%3) {double_round = true, input_zp = 0 : i32, multiplier = [1077952512 : i32], output_zp = -128 : i32, per_channel = false, scale32 = true, shift = [38 : i32]} : (tensor<1x2xi32>)  tensor<1x2xi8>

Here I wanted to understand why rescaling i8 to i32 is done before passing inputs to tosa. mul

Thanks
"
56162,"Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

binary

### Tensorflow Version

tf 2.6.2

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select.
When I convert model from tf to tflite, a question occur like this, and I add ""--target_ops=TFLITE_BUILTINS,SELECT_TF_OPS"", it still can not fix this question, so how do i fix the question.
```


### Standalone code to reproduce the issue

```shell
I convert model command like this:

tflite_convert --target_ops=TFLITE_BUILTINS,SELECT_TF_OPS --saved_model_dir=tensorflow_model/ssdlite_mobilenet_v3_large/ --output_file=ssdlite_mobilenet_v3_large.tflite
```


### Relevant log output

```shell
tensorflow.lite.python.convert_phase.ConverterError: <unknown>:0: error: loc(callsite(callsite(""truediv/Cast@__inference___call___4559"" at ""StatefulPartitionedCall@__inference_signature_wrapper_4568"") at ""StatefulPartitionedCall"")): 'tf.Cast' op is neither a custom op nor a flex op
<unknown>:0: note: loc(""StatefulPartitionedCall""): called from
<unknown>:0: note: loc(callsite(callsite(""truediv/Cast@__inference___call___4559"" at ""StatefulPartitionedCall@__inference_signature_wrapper_4568"") at ""StatefulPartitionedCall"")): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: loc(callsite(callsite(""truediv@__inference___call___4559"" at ""StatefulPartitionedCall@__inference_signature_wrapper_4568"") at ""StatefulPartitionedCall"")): 'tf.RealDiv' op is neither a custom op nor a flex op
<unknown>:0: note: loc(""StatefulPartitionedCall""): called from
<unknown>:0: note: loc(callsite(callsite(""truediv@__inference___call___4559"" at ""StatefulPartitionedCall@__inference_signature_wrapper_4568"") at ""StatefulPartitionedCall"")): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: loc(callsite(callsite(""Cast_40@__inference___call___4559"" at ""StatefulPartitionedCall@__inference_signature_wrapper_4568"") at ""StatefulPartitionedCall"")): 'tf.Cast' op is neither a custom op nor a flex op
<unknown>:0: note: loc(""StatefulPartitionedCall""): called from
<unknown>:0: note: loc(callsite(callsite(""Cast_40@__inference___call___4559"" at ""StatefulPartitionedCall@__inference_signature_wrapper_4568"") at ""StatefulPartitionedCall"")): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: failed while converting: 'main':
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select
TF Select ops: Cast, RealDiv
Details:
	tf.Cast(tensor<f64>) -> (tensor<i32>) : {Truncate = false, device = """"}
	tf.Cast(tensor<i64>) -> (tensor<f64>) : {Truncate = false, device = """"}
	tf.RealDiv(tensor<f64>, tensor<f64>) -> (tensor<f64>) : {device = """"}
```
</details>"
56159,cufft plan fails with tf.signal.fft2d,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.1.0 gpu_py37h7db9008_0

### Custom Code

Yes

### OS Platform and Distribution

Windows

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

cudatoolkit=10.1.243 cudnn=7.6.5

### GPU model and memory

NVIDIA Quatro RTX 8000 48GB

### Current Behaviour?

```shell
I'm trying to take the 2D FFT of a float32 tensor of shape (100000, 1, 128, 128)

I've tried several suggested solutions on related issues; enabling GPU memory growth, and configuring the tf1 Session

```


### Standalone code to reproduce the issue

```shell
x = tf.random.normal((100000, 1, 128, 128))
out = tf.signal.fft2d(tf.cast(x, tf.complex64))
```


### Relevant log output

```shell
2022-05-18 17:49:40.628964: F tensorflow/stream_executor/cuda/cuda_fft.cc:435] failed to initialize batched cufft plan with customized allocator: 

Process finished with exit code -1073740791 (0xC0000409)
```
</details>"
56157,TFLite forcing to run on CPU,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf 2.8

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Hello! 
Can I somehow force the TFLite interpreter to run on my CPU instead of GPU? Now it is running on GPU automatically. 

Another question is also if I can use TFLite runtime on GPU? It automatically uses the CPU when I start right now even if it is exactly the same code and computer as when I use TensorFlow complete library. 

Best regards
Sara Larsson



### Standalone code to reproduce the issue

```shell
import tensorflow as tf
interpreter = tf.lite.Interpreter(model_path)

import tflite_runtime.interpreter as tflite
interpreter = tflite.Interpreter(model_path)
```


### Relevant log output

_No response_</details>"
56153,Failing //tensorflow/core/grappler/optimizers tests with oneDNN on non-AVX512 cores,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

master

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

5.1.1

### GCC/Compiler version

10.3.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

In TF build with `TF_ENABLE_ONEDNN_OPTS=1` when running tests `//tensorflow/core/grappler/optimizers:auto_mixed_precision_test_cpu`
and `//tensorflow/core/grappler/optimizers:remapper_test_cpu` on AArch64 or x86_64 without AVX512 support tests such as `AutoMixedPrecisionMklTest.InferFollowUpStreamAllow` fail because after optimisation convolution layer has source, destination and weights as`bf16`. This in turns tries to find oneDNN primitive in this list: https://github.com/oneapi-src/oneDNN/blob/master/src/cpu/cpu_convolution_list.cpp#L127-L142 and if it returns `true` from here: https://github.com/oneapi-src/oneDNN/blob/master/src/cpu/platform.cpp#L105-L114 it will at least pick up `ref_convolution_fwd_t otherwise` it will return `nullptr` and error that primitive cannot be created making test to fail


### Standalone code to reproduce the issue

```shell
bazel test --config=mkl_aarch64 --test_env=TF_ENABLE_ONEDNN_OPTS=1 --jobs=64 --test_timeout=300,500,1200,3600 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --copt=""-mtune=generic"" --copt=""-march=armv8-a"" --copt=""-O3""  --copt=""-fopenmp"" --linkopt=""-lgomp"" --verbose_failures --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu //tensorflow/core/grappler/optimizers:auto_mixed_precision_test_cpu
```


### Relevant log output

```shell
[ RUN      ] AutoMixedPrecisionMklTest.InferFollowUpStreamAllow
2022-05-05 13:17:13.161401: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:2185] Converted 3/7 nodes to bfloat16 precision using 0 cast(s) to bfloat16 (excluding Const and Variable casts)
2022-05-05 13:17:13.171854: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at mkl_conv_ops.cc:917 : ABORTED: Operation received an exception:Status: 3, message: could not create a primitive descriptor iterator, in file tensorflow/core/kernels/mkl/mkl_conv_ops.cc:916
2022-05-05 13:17:13.175121: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at mkl_conv_ops.cc:917 : ABORTED: Operation received an exception:Status: 3, message: could not create a primitive descriptor iterator, in file tensorflow/core/kernels/mkl/mkl_conv_ops.cc:916
2022-05-05 13:17:13.176238: F tensorflow/core/grappler/utils/grappler_test.cc:123] Non-OK-status: session->Run(run_options, inputs, node_names, node_names, &output_tensors, nullptr) status: ABORTED: Operation received an exception:Status: 3, message: could not create a primitive descriptor iterator, in file tensorflow/core/kernels/mkl/mkl_conv_ops.cc:916
	 [[{{node infer}}]]
*** Received signal 6 ***
*** BEGIN MANGLED STACK TRACE ***
/root/.cache/bazel/_bazel_root/016b2a9c3533caae0eb68665b3373f9e/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/grappler/optimizers/auto_mixed_precision_test_cpu.runfiles/org_tensorflow/tensorflow/core/grappler/optimizers/auto_mixed_precision_test_cpu[0x7f6b5c]
[0xffff7f9807a0]
/lib64/libc.so.6(gsignal+0x4c)[0xffff72335238]
/lib64/libc.so.6(abort+0x11c)[0xffff723368b0]
/root/.cache/bazel/_bazel_root/016b2a9c3533caae0eb68665b3373f9e/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/grappler/optimizers/../../../../_solib_aarch64/_U_S_Stensorflow_Score_Sgrappler_Soptimizers_Cauto_Umixed_Uprecision_Utest_Ucpu___Utensorflow/libtensorflow_framework.so.2(+0x16f21a8)[0xffff7f3221a8]
/root/.cache/bazel/_bazel_root/016b2a9c3533caae0eb68665b3373f9e/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/grappler/optimizers/../../../../_solib_aarch64/libtensorflow_Score_Sgrappler_Sutils_Slibgrappler_Utest.so(_ZNK10tensorflow8grappler12GrapplerTest13EvaluateNodesERKNS_8GraphDefERKSt6vectorISsSaISsEERKS5_ISt4pairISsNS_6TensorEESaISC_EE+0x750)[0xffff7d42a130]
/root/.cache/bazel/_bazel_root/016b2a9c3533caae0eb68665b3373f9e/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/grappler/optimizers/../../../../_solib_aarch64/libtensorflow_Score_Sgrappler_Sutils_Slibgrappler_Utest.so(_ZNK10tensorflow8grappler12GrapplerTest13EvaluateNodesERKNS_8GraphDefERKSt6vectorISsSaISsEE+0x28)[0xffff7d42a1e8]
/root/.cache/bazel/_bazel_root/016b2a9c3533caae0eb68665b3373f9e/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/grappler/optimizers/auto_mixed_precision_test_cpu.runfiles/org_tensorflow/tensorflow/core/grappler/optimizers/auto_mixed_precision_test_cpu[0x7f42e0]
/root/.cache/bazel/_bazel_root/016b2a9c3533caae0eb68665b3373f9e/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/grappler/optimizers/../../../../_solib_aarch64/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so(_ZN7testing8internal35HandleExceptionsInMethodIfSupportedINS_4TestEvEET0_PT_MS4_FS3_vEPKc+0x48)[0xffff7336c768]
/root/.cache/bazel/_bazel_root/016b2a9c3533caae0eb68665b3373f9e/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/grappler/optimizers/../../../../_solib_aarch64/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so(_ZN7testing4Test3RunEv+0xf8)[0xffff7336c9f8]
/root/.cache/bazel/_bazel_root/016b2a9c3533caae0eb68665b3373f9e/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/grappler/optimizers/../../../../_solib_aarch64/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so(_ZN7testing8TestInfo3RunEv+0x134)[0xffff7336cd34]
/root/.cache/bazel/_bazel_root/016b2a9c3533caae0eb68665b3373f9e/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/grappler/optimizers/../../../../_solib_aarch64/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so(_ZN7testing9TestSuite3RunEv+0xc8)[0xffff7336cff8]
/root/.cache/bazel/_bazel_root/016b2a9c3533caae0eb68665b3373f9e/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/grappler/optimizers/../../../../_solib_aarch64/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so(_ZN7testing8internal12UnitTestImpl11RunAllTestsEv+0x4d4)[0xffff7336d584]
/root/.cache/bazel/_bazel_root/016b2a9c3533caae0eb68665b3373f9e/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/grappler/optimizers/../../../../_solib_aarch64/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so(_ZN7testing8UnitTest3RunEv+0xb8)[0xffff7336d818]
/root/.cache/bazel/_bazel_root/016b2a9c3533caae0eb68665b3373f9e/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/grappler/optimizers/../../../../_solib_aarch64/libtensorflow_Score_Splatform_Slibtest_Umain.so(main+0xa4)[0xffff7d6e0af4]
/lib64/libc.so.6(__libc_start_main+0xf0)[0xffff72321724]
/root/.cache/bazel/_bazel_root/016b2a9c3533caae0eb68665b3373f9e/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/grappler/optimizers/auto_mixed_precision_test_cpu.runfiles/org_tensorflow/tensorflow/core/grappler/optimizers/auto_mixed_precision_test_cpu[0x7e5754]
*** END MANGLED STACK TRACE ***

*** Begin stack trace ***
	tensorflow::CurrentStackTrace()
	
	__kernel_rt_sigreturn
	gsignal
	abort
	
	tensorflow::grappler::GrapplerTest::EvaluateNodes(tensorflow::GraphDef const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&) const
	tensorflow::grappler::GrapplerTest::EvaluateNodes(tensorflow::GraphDef const&, std::vector<std::string, std::allocator<std::string> > const&) const
	
	void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*)
	testing::Test::Run()
	testing::TestInfo::Run()
	testing::TestSuite::Run()
	testing::internal::UnitTestImpl::RunAllTests()
	testing::UnitTest::Run()
	main
	__libc_start_main
	
*** End stack trace ***
================================================================================
```
</details>"
56152,[TFLite] Dynamic batch size with GPU delegate,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

2.10.0-dev20220427

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

Linux Ubuntu 18.04

### Python version

3.10

### Bazel version

5.1.1

### GCC/Compiler version

7.5.0

### CUDA/cuDNN version

CUDA 11.6

### GPU model and memory

NVIDIA GeForce GTX 1650 4BG

### Current Behaviour?

Considering these two assertions (please tell me if they are wrong):
* It is not possible to use dynamic-sized tensors with the GPU delegate.
* Nor is it possible to use a batch size greater than one with the GPU delegate.

EDIT: It seems that when I saw the bug with batch size greater than one, the GPU delegate switched to OpenGL instead of OpenCL (I do not know the reason), and now that it is working with OpenCL (after rebooting the computer), the inference can run on the GPU with any batch size. Thus it would still mean that this feature should be available with OpenGL...

However it would very convenient to be able to modify the batch size before creating the network on the GPU, or (if requested by the user) to recreate the network at any time with a different batch size.
This would allow to convert and store a single model with dynamic-sized tensors, the size of which would actually be known (i.e., batch size would be set) before making use of the network.

Then it would be possible to perform [on-device training](https://www.tensorflow.org/lite/examples/on_device_training/overview#build_a_model_for_on-device_training) on the GPU with a batch of inputs, once issue #56151 is solved.


### Standalone code to reproduce the issue

Try running [this on-device training tutorial](https://www.tensorflow.org/lite/examples/on_device_training/overview#build_a_model_for_on-device_training) with both Flex and GPU delegates using the experimental C API.


### Relevant log output

_No response_</details>"
56151,[TFLite] On-device training with GPU delegate,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

2.10.0-dev20220427

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

Linux Ubuntu 18.04

### Python version

3.10

### Bazel version

5.1.1

### GCC/Compiler version

7.5.0

### CUDA/cuDNN version

CUDA 11.6

### GPU model and memory

NVIDIA GeForce GTX 1650 4BG

### Current Behaviour?

Would it be possible to perform on-device training with TFLite experimental C API using the GPU delegate for inference?
Following [this tutorial](https://www.tensorflow.org/lite/examples/on_device_training/overview#build_a_model_for_on-device_training), after converting the model with the four signatures `infer`, `train`, `save` and `restore`, the interpreter can only be created without any delegates.

When putting only the `infer` signature in the model without the softmax operation, however, it is possible to run inference on the GPU either through this signature or the subgraph 0, and the results are identical as expected.
When adding other operations that require the Flex delegate, it becomes impossible to use the GPU delegate.

Consider for instance putting only the `train` signature in the model, then delegates other than Flex cannot be used, while one would expect the inference part `prediction = model(x)` to be able to run on the GPU and the gradient part to run on the CPU (maybe the loss could also be computed on the GPU, I am not sure of what operators exactly are not supported by the GPU delegate).
Indeed, if the inference can normally be handled by the GPU delegate (e.g. when there is no signatures in the model), then why is it impossible to use it in a signature that contains both an inference part and something else?

It would very great to always be able to run the inference on the GPU since it highly reduces the computation time.
What, do you think, would need to be changed in the code for achieving that?
I personally tried some modifications but was not able to make it work.

Some references indicating that it is already supposed to work:
* [Select Ops](https://www.tensorflow.org/lite/guide/ops_select#updates): Compatibility with hardware accelerated delegates has improved in version 2.4.
* [Issue 42190](https://github.com/tensorflow/tensorflow/issues/42190#issuecomment-697004884): Flex + GPU combination is now working at tf-nightly version if the graph has only static shapes.

The breaking change seems to be the structure with signatures.


### Standalone code to reproduce the issue

Compile the C API with both Flex and GPU delegates using this compilation rule added in `//tensorflow/lite/c/BUILD` (CUDA enabled in `./configure`):
```
tflite_cc_shared_object(
    name = ""tensorflowlite_c_gpu_flex"",
    per_os_targets = True,
    deps = [
        ""//tensorflow/lite/c:c_api_experimental"",
        ""//tensorflow/lite/delegates/gpu:delegate"",
        ""//tensorflow/lite/delegates/flex:delegate_symbol"",
    ],
)
```

with this bazel command:
```
bazel --output_user_root=build build -c opt --copt -Os --copt -DTFLITE_GPU_BINARY_RELEASE --copt -DMESA_EGL_NO_X11_HEADERS --copt -DEGL_NO_X11 --linkopt -lEGL --linkopt -lGLESv2 --linkopt -s --strip always //tensorflow/lite/c:tensorflowlite_c_gpu_flex --config monolithic --config linux
```


### Relevant log output

_No response_</details>"
56149," tflite_convert --saved_model_dir command throws error ""SavedModel file does not exist {saved_model.pbtxt|saved_model.pb}"" ","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.9.0

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.7.13

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am trying to convert the tf model to tflite model. I have downloaded the tf model ""spaghettinet_edgetpu_s"" from GitHub and extracted it. The extracted folder contains all te required files(.pb, .pbtxt, and others). 
Run the ""tflite_convert --saved_model_dir  --output_file"" command and expect to generate the tflite model file.
```


### Standalone code to reproduce the issue

```shell
1)  Downalod the model ""spaghettinet_edgetpu_s""  (https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md#pixel-6-edge-tpu-models)
2) extract it.
3) Folde contains these files: ~/data/sample/model/spaghettinet_edgetpu_s$ ls
checkpoint                 graph.pbtxt                            model.ckpt-400000.index  model.tflite
frozen_inference_graph.pb  model.ckpt-400000.data-00000-of-00001  model.ckpt-400000.meta   pipeline.config
4) Run: tflite_convert   --saved_model_dir=/home/gaurav/data/sample/model/spaghettinet_edgetpu_m  --output_file=/home/gaurav/data/sample/model/model_m.tflite
```


### Relevant log output

```shell
~/data/sample/model$ tflite_convert   --saved_model_dir=/home/gaurav/data/sample/model/spaghettinet_edgetpu_m  --output_file=/home/gaurav/data/sample/model/model_m.tflite
2022-05-18 14:04:23.552150: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
Traceback (most recent call last):
  File ""/home/gaurav/.local/bin/tflite_convert"", line 8, in <module>
    sys.exit(main())
  File ""/home/gaurav/.local/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py"", line 692, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/home/gaurav/.local/lib/python3.7/site-packages/absl/app.py"", line 312, in run
    _run_main(main, args)
  File ""/home/gaurav/.local/lib/python3.7/site-packages/absl/app.py"", line 258, in _run_main
    sys.exit(main(argv))
  File ""/home/gaurav/.local/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py"", line 675, in run_main
    _convert_tf2_model(tflite_flags)
  File ""/home/gaurav/.local/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py"", line 279, in _convert_tf2_model
    tags=_parse_set(flags.saved_model_tag_set))
  File ""/home/gaurav/.local/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 1786, in from_saved_model
    saved_model = _load(saved_model_dir, tags)
  File ""/home/gaurav/.local/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py"", line 782, in load
    result = load_partial(export_dir, None, tags, options)[""root""]
  File ""/home/gaurav/.local/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py"", line 887, in load_partial
    loader_impl.parse_saved_model_with_debug_info(export_dir))
  File ""/home/gaurav/.local/lib/python3.7/site-packages/tensorflow/python/saved_model/loader_impl.py"", line 57, in parse_saved_model_with_debug_info
    saved_model = parse_saved_model(export_dir)
  File ""/home/gaurav/.local/lib/python3.7/site-packages/tensorflow/python/saved_model/loader_impl.py"", line 116, in parse_saved_model
    f""SavedModel file does not exist at: {export_dir}{os.path.sep}""
OSError: SavedModel file does not exist at: /home/gaurav/data/sample/model/spaghettinet_edgetpu_m/{saved_model.pbtxt|saved_model.pb}
```
</details>"
56148,How to reduce CPU usage of TensorFlowLite,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

tflite 2.6.1

### Custom Code

No

### OS Platform and Distribution

yocto Linux

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

3.7.1

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hello, I have a question about how to reduce CPU usage of TensorFlowLite.

I have 100% CPU utilization when running on ARM.
I want to lower it to about 50% because other processes will not work.
Is there a solution to this problem?
It is already running on 1 core, so the method of reducing the number of cores used will not solve the problem.
```


### Standalone code to reproduce the issue

```shell
-
```


### Relevant log output

_No response_</details>"
56146,compatible Issue between CUDA version and tensorflow version,"Hello, I have a question about the CUDA version and tensor flow version.
I have looked the compatible from https://github.com/tensorflow/docs/blob/master/site/en/install/source.md#gpu
The setup for my GPU is:
![image](https://user-images.githubusercontent.com/58043128/168935667-c9d4cd11-8854-4281-9247-7c69db206477.png)
with CUDA version: 10.2

When I try to run the ML code with tensor flow version **1.14.0**, it is Find. But when I run the code with tensor flow version of **1.15.1**, it raise error like this:

> tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.
>   (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
> 	 [[{{node conv1d_1/convolution}}]]
> 	 [[loss/mul/_131]]
>   (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
> 	 [[{{node conv1d_1/convolution}}]]
> 0 successful operations.
> 0 derived errors ignored.
> E0517 07:32:37.654887 140167361226560 logger.py:55] 2 root error(s) found.
>   (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
> 	 [[{{node conv1d_1/convolution}}]]
> 	 [[loss/mul/_131]]
>   (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
> 	 [[{{node conv1d_1/convolution}}]]
> 0 successful operations.
> 0 derived errors ignored.
> Traceback (most recent call last):
>   File ""/home/timhon/amlml2/aml_prediction/project/classes/Evaluator/trainer.py"", line 99, in make_prediction
>     best_model, best_param_df = self.train_model(**train_params)
>   File ""/home/timhon/amlml2/aml_prediction/project/classes/Evaluator/trainer.py"", line 166, in train_model
>     mclass.getoptimized(paramslist, traindata, evaldata)
>   File ""/home/timhon/amlml2/aml_prediction/project/classes/Modelling/nlpmodels.py"", line 248, in getoptimized
>     return super().getoptimized(paramslist, traindata, evaldata)
>   File ""/home/timhon/amlml2/aml_prediction/project/classes/Modelling/basemodel.py"", line 82, in getoptimized
>     results = self.validate(params, traindata, evaldata)
>   File ""/home/timhon/amlml2/aml_prediction/project/classes/Modelling/nlpmodels.py"", line 235, in validate
>     callbacks=[early_stopping, history], verbose=1, validation_split=0.2)
>   File ""/home/timhon/.local/lib/python3.6/site-packages/keras/engine/training.py"", line 1039, in fit
>     validation_steps=validation_steps)
>   File ""/home/timhon/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py"", line 199, in fit_loop
>     outs = f(ins_batch)
>   File ""/home/timhon/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 2715, in __call__
>     return self._call(inputs)
>   File ""/home/timhon/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 2675, in _call
>     fetched = self._callable_fn(*array_vals)
>   File ""/home/timhon/.local/lib/python3.6/site-packages/tensorflow_core/python/client/session.py"", line 1472, in __call__
>     run_metadata_ptr)
> tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.
>   (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
> 	 [[{{node conv1d_1/convolution}}]]
> 	 [[loss/mul/_131]]
>   (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
> 	 [[{{node conv1d_1/convolution}}]]
> 0 successful operations.
> 0 derived errors ignored.

is there are anyone know the error? 
Remarks:
to test if the problem is coming from my Python-scripts, I also try to run the **SAME** script in different computer, with  CUDA version of 11.2, and it works!!!! Hence, I think we can ignore the issue from my scripts...at less it is working in other machines"
56144,Data init API for TFLite Swift,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

2.8+

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The current Swift API only has `init` functions from files on disk unlike the Java (Android) API which has a byte buffer initializer. It'd be convenient if the Swift API could initialize `Interpreters` from `Data`.
```


### Standalone code to reproduce the issue

```shell
No code. This is a feature request
```


### Relevant log output

_No response_</details>"
56143,"TensorFlow core operator ExtractImagePatches , not included and supported ","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

RTX 1660Ti and 16 GB

### Current Behaviour?

```shell
I made a custom  layer which uses ExtractImagePatches TensorFlow operator, but it was not supported by tflite (but listed as supported). I tried to enable the op manually but was unable to find .cc and .h file. It would be great if there is a solution to this problem, since my research depends on it.

RuntimeError: Failed to initialize op resolver for calibration:
There are unresolved custom ops: [ExtractImagePatches]Encountered unresolved custom op: ExtractImagePatches.
See instructions: https://www.tensorflow.org/lite/guide/ops_customNode number 0 (ExtractImagePatches) failed to prepare.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
from keras.utils import np_utils
from tensorflow.python.keras import activations
from sklearn.model_selection import KFold
from tensorflow.python.keras.callbacks import EarlyStopping
def myconv2d(ix, w, padding):
   # filter shape: [filter_height, filter_width, in_channels, out_channels]
   # flatten filters
   filter_height = int(w.shape[0])
   filter_width = int(w.shape[1])
   in_channels = int(w.shape[2])
   out_channels = int(w.shape[3])
   ix_height = int(ix.shape[1])
   ix_width = int(ix.shape[2])
   ix_channels = int(ix.shape[3])
   filter_shape = [filter_height, filter_width, in_channels, out_channels]
   flat_w = tf.reshape(w, [filter_height * filter_width * in_channels, out_channels])
   patches = tf.image.extract_patches(
       ix,
       sizes=[1, filter_height, filter_width, 1],
       strides=[1, 1, 1, 1],
       rates=[1, 1, 1, 1],
       padding= padding
   )
   patches_reshaped = tf.reshape(patches, [-1, ix_height, ix_width, filter_height * filter_width * ix_channels])
   feature_maps = []
   for i in range(out_channels):
       feature_map = tf.reduce_sum(tf.multiply(flat_w[:, i], patches_reshaped), axis=3, keepdims=True)
       feature_maps.append(feature_map)
   features = tf.concat(feature_maps, axis=3)
   return features

class MyConv2D(tf.keras.layers.Layer):
    def __init__(self, filters, kernel_size,padding='SAME', **kwargs):
        self.filters = filters
        self.kernel_size = kernel_size
        self.padding = padding
        #self.units= units
        super(MyConv2D, self).__init__(**kwargs)

    def get_config(self):
        config = super().get_config()
        config.update({
            ""filters"": self.filters,
            ""kernel_size"": self.kernel_size,
            ""padding"" : self.padding,
        })
        return config

    def build(self, input_shape):
        # only have a 3x3 kernel
        shape = self.kernel_size + (input_shape[-1], self.filters)
        self.kernel = self.add_weight(name='kernel', shape=shape,
                                      initializer='glorot_uniform', trainable=True)
        self.b = self.add_weight(
            name=""bias"", shape=(self.filters,), initializer=""random_normal"", trainable=True
        )
        super((MyConv2D, self).build(input_shape))

    def call(self, inputs):
        result = myconv2d(inputs, self.kernel, self.padding) + self.b
        return result

    def compute_output_shape(self, input_shape):
        return input_shape[:-1] + (self.filters,)


def load_dataset():
	# load dataset
	(trainX, trainY), (testX, testY) = tf.keras.datasets.mnist.load_data()
	# reshape dataset to have a single channel
	trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))
	testX = testX.reshape((testX.shape[0], 28, 28, 1))
	# one hot encode target values
	trainY = np_utils.to_categorical(trainY)
	testY = np_utils.to_categorical(testY)
	# convert from integers to floats
	train_norm = trainX.astype('float32')
	test_norm = testX.astype('float32')
	# normalize to range 0-1
	trainX = train_norm / 255.0
	testX = test_norm / 255.0
	# return normalized images
	return trainX, trainY, testX, testY


def create_model():
	# creating a sequantial model
	model = tf.keras.Sequential()
	# adding convolution2D layer to the model of 32 filters of size 3x3
	model.add(MyConv2D(filters=32, kernel_size=(3, 3), input_shape=(28, 28, 1)))
	model.add(tf.keras.layers.Activation(activations.relu))
	# adding a maxpooling 2D layer of size 2x2
	model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))
	# adding a Flatten layer
	model.add(tf.keras.layers.Flatten())
	# adding Dense layer with 'relu' activation
	model.add(tf.keras.layers.Dense(100, activation='relu'))
	# adding Dense layer with 'softmax' activation for output
	model.add(tf.keras.layers.Dense(10, activation='softmax'))
	return model


def define_model(model):
	# compile model
	opt = tf.keras.optimizers.Adam()
	model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
	return model


if __name__ == ""__main__"":
	train_images, train_labels, test_images, test_labels = load_dataset()

	model = create_model()

	# compile model
	opt = tf.keras.optimizers.Adam()
	model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
	es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)
	# fit model
	history = model.fit(train_images, train_labels, epochs=50, batch_size=32,
						validation_data=(test_images, test_labels), callbacks=[es])
	# evaluate model
	scores = model.evaluate(test_images, test_labels, verbose=0)
	print(""Accuracy: %.2f%%"" % (scores[1] * 100))
	# stores scores
```


### Relevant log output

_No response_</details>"
56142,Mix precision leads to nan loss,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

pip install

### Tensorflow Version

tf2.8

### Custom Code

Yes

### OS Platform and Distribution

windows

### Mobile device

windows 21h2 (19044, 1706)

### Python version

python 3.9.7

### Bazel version

unkonwn

### GCC/Compiler version

unknown

### CUDA/cuDNN version

11.2

### GPU model and memory

RTX3070/8Gb

### Problem Discription?

Loss is nan when using mix precision API even though use (get_scaled_loss, get_unscaled_gradients) or scale loss manually.

- Model works well in float32.
- Mix precision API with [tensorflow guide](https://www.tensorflow.org/guide/mixed_precision#training_the_model_with_a_custom_training_loop) works well in my computer.
- My model input start from Cond2D -> ... -> Resblock -> ... -> Dense -> SoftMax, it's to solve a classification problem.


### Standalone code to reproduce the issue

```shell
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import numpy as np
import tensorflow as tf
from utils._history import fig
from utils.network_float16 import MyModel, CosineAnnealingScheduler, CCE, Scale_Gradient
from utils.system import args, Yie, _Plot, _Predict, MyTrain
tf.keras.mixed_precision.set_global_policy('mixed_float16')

test_SNR = np.arange(0, 22, 2) if args.fading_type == 1 else np.arange(10, 32, 2)
a,b = MyTrain(args.K, args.m, args.Nd, args.B, args.p, args.Ka, args.snr, args.tap, args.fading_type)   # training data generation

train_dataset = tf.data.Dataset.from_tensor_slices((a, b)) 
train_dataset = train_dataset.shuffle(buffer_size=2048).batch(args.B)
optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)
optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)
loss_metric = tf.keras.metrics.CategoricalCrossentropy()
acc = tf.keras.metrics.Accuracy()

model = MyModel(args)
model = model.Model(is_summary=0, is_plot_model=1)   # compile model


@tf.function
def train_step(x_train, y_train):
    scale=2048
    with tf.GradientTape() as tape:
        y_pred = model(x_train,training = True)
        loss = tf.keras.losses.categorical_crossentropy(y_train, y_pred)
        loss *= scale
    grads = tape.gradient(loss, model.trainable_weights)
    grads = [x*scale for x in grads]
    optimizer.apply_gradients(zip(grads, model.trainable_weights))
    return loss


def decision(y_pred):
    y_pred = np.squeeze(np.argsort(-y_pred)[:, :args.Ka])
    temp = np.zeros([args.B, args.K], dtype='int')
    for j in range(args.Ka):
        temp[np.arange(args.B), y_pred[:, j]] = 1
    return temp

for epoch in range(args.epoch):
    epoch_time = time.time()
    for step, (x_train, y_train) in enumerate(train_dataset):
        loss = train_step(x_train, y_train)
    y_pred = model(x_train)
    y_pred = decision(y_pred)
    acc.update_state(y_pred, y_train)
    print(""Epoch %d - %ds - loss: %.4f - acc: %4f"" % (epoch+1, time.time()-epoch_time, loss_record[epoch], acc.result().numpy()))

_Plot.Simulation(args, _Predict(args, model, test_SNR), test_SNR, fig, save_txt=1)
##### Loss is nan from first epoch #####



##### Next, I tried two types of train_step, and the 'Revelant log output' is the print output #####
##### First I set the size of x_train to batch size, so the network will only update weight ones #####
##### Compile the model and run the following code #####
model = MyModel(args)
model = model.Model(is_summary=0, is_plot_model=1)   # compile model
# type 1 (follow tensorflow guide)
for scale in range(1,3000,30):   # to check which scale is the best
    with tf.GradientTape() as tape:
        y_pred = model(x_train,training = True)
        loss = tf.keras.losses.categorical_crossentropy(y_train, y_pred)
        print(tf.reduce_mean(loss))    # see if CCE loss works normally
        loss = optimizer.get_scaled_loss(loss)
        print(tf.reduce_mean(loss))   # see if get_scaled_loss works normally
    grads = tape.gradient(loss, model.trainable_weights)
    grads = optimizer.get_unscaled_gradients(grads)
    optimizer.apply_gradients(zip(grads, model.trainable_weights))
# -----------------------------------------------------------------
# type 2 (scale loss manually)
for scale in range(1,3000,30):   # to check which scale is the best
    with tf.GradientTape() as tape:
        y_pred = model(x_train,training = True)
        loss = tf.keras.losses.categorical_crossentropy(y_train, y_pred)
        loss *= scale
        print(tf.reduce_mean(loss))   # see if loss scaled properly
    grads = tape.gradient(loss, model.trainable_weights)
    print(tf.reduce_mean(grads[0]))  #  _ in grads[ _ ] can be arbitrary number (layer)
    grads = [x*scale for x in grads]
    optimizer.apply_gradients(zip(grads, model.trainable_weights))

```


### Relevant log output

```shell
type 1

tf.Tensor(9.516, shape=(), dtype=float16)   # unscaled loss
tf.Tensor(inf, shape=(), dtype=float16)     # scaled loss
tf.Tensor(9.516, shape=(), dtype=float16)   # unscaled loss
tf.Tensor(inf, shape=(), dtype=float16)     # scaled loss
...
... # same for other scales
# -------------------------------------------------------------------------
type 2

tf.Tensor(9.516, shape=(), dtype=float16)    # scaled loss
tf.Tensor(nan, shape=(), dtype=float32 )     # gradients
tf.Tensor(961.0, shape=(), dtype=float16)    # scaled loss
tf.Tensor(nan, shape=(), dtype=float32)      # gradients
tf.Tensor(1913.0, shape=(), dtype=float16)   # scaled loss
tf.Tensor(nan, shape=(), dtype=float32)      # gradients
...
...  # gradients are all nan for all scales

##### All gradients are nan from first step, even w/wo kernel initializer (random normal initializer) #####
```
</details>"
56138,tape.jacobian with chain rule cannot compute the gradient correctly,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf2.7

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 21.10

### Mobile device

_No response_

### Python version

3.9.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.3.1/8.1.0.77

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I want to use the jacobian matrix and chain-rule to compute the gradient. When the composed functions are basic arithmetics, the gradient is right. But when the composed function is a convolution, the gradient  calculated by the chain-rule is not consistent with the one outputed by 'tape.gradient'.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

layer = tf.keras.layers.Conv2D(1, 3, activation='relu', padding=""same"")
x = tf.random.normal([2,7, 7,1])
xx = tf.random.normal([2,7, 7,1])

with tf.GradientTape(persistent=True) as tape1:
    tape1.watch(x)
    y1 = layer(x)
    y2 = layer(y1)
    tape1.watch(y2)
    loss=tf.keras.losses.MeanSquaredError()(y2,xx)

j1=tape1.jacobian(y2,x)
[g_x,g_y2]=tape1.gradient(loss,[x,y2])
j1_vec=tf.reshape(j1,[x.shape[0]*x.shape[1]*x.shape[2]*x.shape[3],
                     y2.shape[0]*y2.shape[1]*y2.shape[2]*y2.shape[3]])
g_y2_vec=tf.reshape(g_y2,[y2.shape[0]*y2.shape[1]*y2.shape[2]*y2.shape[3],1])
G=tf.matmul(j1_vec,g_y2_vec)
G_x=tf.reshape(G,[x.shape[0],x.shape[1],x.shape[2],x.shape[3]])
e1=G_x-g_x
print('error value of e1:',tf.reduce_max(abs(e1)).numpy())

with tf.GradientTape(persistent=True) as tape2:
    tape2.watch(x)
    y1 = x**3+100
    y2 = tf.math.cos(y1)+10
    tape2.watch(y2)
    loss=tf.keras.losses.MeanSquaredError()(y2,xx)

j2=tape2.jacobian(y2,x)
[g_x,g_y2]=tape2.gradient(loss,[x,y2])
j2_vec=tf.reshape(j2,[x.shape[0]*x.shape[1]*x.shape[2]*x.shape[3],
                     y2.shape[0]*y2.shape[1]*y2.shape[2]*y2.shape[3]])
g_y2_vec=tf.reshape(g_y2,[y2.shape[0]*y2.shape[1]*y2.shape[2]*y2.shape[3],1])
G=tf.matmul(j2_vec,g_y2_vec)
G_x=tf.reshape(G,[x.shape[0],x.shape[1],x.shape[2],x.shape[3]])
e2=G_x-g_x
print('error value of e2:',tf.reduce_max(abs(e2)).numpy())
```


### Relevant log output

```shell
error value of e1: 0.10533138
error value of e2: 2.3841858e-07
```
</details>"
56137,Build/release Python 3.10 tflite-runtime wheels to PyPI,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

binary

### Tensorflow Version

2.9.0

### Custom Code

No

### OS Platform and Distribution

Linux & macOS

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

[Tensorflow 2.9.0 was released with Python 3.10 support](https://pypi.org/project/tensorflow/2.9.0/#files).

We would like to support Python 3.10 in production, but in order to do this we need all of the following packages to support Python 3.10:

- `tensorflow`
- `tensorflow-macos` (for ARM macOS)
- `tflite-runtime` (for stripped down production on Linux)

Neither [`tensorflow-macos`](https://pypi.org/project/tensorflow-macos/#history) or [`tflite-runtime`](https://pypi.org/project/tflite-runtime/) have 2.9.0 releases on PyPI yet.


### Standalone code to reproduce the issue

Create a Python 3.10 virtual environment on Linux:

```bash
(venv) $ python -m pip install tflite-runtime
```

Create a Python 3.10 virtual environment on macOS ARM:

```bash
(venv) $ python -m pip install tensorflow-macos
```


### Relevant log output

_No response_</details>"
56135,COLAB : DNN library is not found. ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.8

### Custom Code

Yes

### OS Platform and Distribution

Colab

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Unable to train custom tflite model using tensorflow 2.8 version.
```


### Standalone code to reproduce the issue

```shell
https://www.tensorflow.org/lite/tutorials/model_maker_object_detection
https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/model_maker_object_detection.ipynb

While trying to train a custom tf lite model on Colab. Training fails with above error.(DNN library is not found.)

Currently the official notebook is not accessible. Getting Notebook not found
```


### Relevant log output

```shell
UnimplementedError                        Traceback (most recent call last)
<ipython-input-10-187f39c1697e> in <module>()
----> 1 model = object_detector.create(train_data, model_spec=spec, batch_size=8, train_whole_model=True, validation_data=validation_data)

4 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     53     ctx.ensure_initialized()
     54     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---> 55                                         inputs, attrs, num_outputs)
     56   except core._NotOkStatusException as e:
     57     if name is not None:

UnimplementedError: Graph execution error:

DNN library is not found.
	 [[{{node stem/conv2d/Conv2D}}]] [Op:__inference_train_function_96447]
```
</details>"
56134,GpuCudaMallocAsyncAllocator fails if the same GPU device is initialised multiple times,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.7.1

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.2, cuDNN 8.1.1

### GPU model and memory

NVIDIA GTX 1080

### Current Behaviour?

When [https://github.com/tensorflow/tensorflow/blob/v2.7.1/tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc#L391](GpuCudaMallocAsyncAllocator) is used instead of the default one (either via environment variable or config setting), and the same GPU device is initialised more than once (e.g. if there are multiple TensorFlow sessions within a single process) it fails with the [""Trying to set the stream twice. This isn't supported.""](https://github.com/tensorflow/tensorflow/blob/v2.7.1/tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc#L391) error.

Apparently this is happening because the `GpuCudaMallocAsyncAllocator` is global and its `cuda_stream_` field is filled on the first `SetStreamAndPreallocateMemory`. I was able to fix this by making some changes in the code, and I am going to submit a PR containing those.

### Standalone code to reproduce the issue

```cpp
#include <tensorflow/core/public/session.h>

tensorflow::GraphDef graph_def2;

auto options = tensorflow::SessionOptions();
options.config.mutable_gpu_options()->mutable_experimental()->set_use_cuda_malloc_async(true);

tensorflow::GraphDef graph_def1;
// Load graph_def1
tensorflow::Session* session1 = nullptr;
tensorflow::NewSession(options, &session1);
session1->Create(graph_def1);

tensorflow::GraphDef graph_def2;
// Load graph_def2
tensorflow::Session* session2 = nullptr;
tensorflow::NewSession(options, &session2);
session2->Create(graph_def2);
```


### Relevant log output

```shell
2022-05-17 14:17:05.253418: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:214] Using CUDA malloc Async allocator for GPU: 0
2022-05-17 14:17:05.253515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7234 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1
2022-05-17 14:17:05.422828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7234 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1
2022-05-17 14:17:05.423011: F tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:390] Trying to set the stream twice. This isn't supported.
```
</details>"
56133,TF 2.9 - CombinedNonMaxSuppression conversion to TFLite not working anymore,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04.4 LTS

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2/8.1

### GPU model and memory

_No response_

### Current Behaviour?

```shell
In tensorflow 2.8 the provided code didn't raise any error. In tensorflow 2.9 ERROR_NEEDS_FLEX_OPS is raised.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf


class NMSLayer(tf.keras.layers.Layer):

    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def call(self, inputs, **kwargs):
        boxes, scores = inputs[0], inputs[1]
        boxes, scores, classes, valid_detections = tf.image.combined_non_max_suppression(
            boxes=boxes,
            scores=scores,
            max_output_size_per_class=8,
            max_total_size=8,
            iou_threshold=0.5,
            score_threshold=0.5,
            pad_per_class=False,
            clip_boxes=True,
            name=f'{self.name}/NMS_op'
        )

        return boxes, scores, tf.cast(classes, dtype=tf.int32), valid_detections


def representative_dataset_generator():
    for batch in range(10):
        yield [tf.zeros((1, 10, 1, 4)), tf.zeros((1, 10, 5))]


num_boxes = 10
num_classes = 5

boxes = tf.keras.Input((num_boxes, 1, 4), batch_size=1)
scores = tf.keras.Input((num_boxes, num_classes))

model = tf.keras.Model([boxes, scores], NMSLayer()([boxes, scores]))
# test that the model can run
model(next(representative_dataset_generator()))

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.SELECT_TF_OPS, tf.lite.OpsSet.TFLITE_BUILTINS_INT8
]
converter.representative_dataset = representative_dataset_generator
converter.target_spec.supported_types = [tf.int8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
tflite_model = converter.convert()
```


### Relevant log output

```shell
Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: failed while converting: 'main': 
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select 
TF Select ops: CombinedNonMaxSuppression
Details:
	tf.CombinedNonMaxSuppression(tensor<1x10x1x4xf32>, tensor<?x10x5xf32>, tensor<i32>, tensor<i32>, tensor<f32>, tensor<f32>) -> (tensor<1x8x4xf32>, tensor<1x8xf32>, tensor<1x8xf32>, tensor<1xi32>) : {clip_boxes = true, device = """", pad_per_class = false}
```
</details>"
56132,SGD with momentum optimizer update fails for variables with dynamic shape,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.8.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Using SGD with momentum optimizer on variables with dynamic shapes throws an error, while without momentum the updates work well.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

x = tf.Variable(tf.random.normal((32,3)), shape=[None,3])

with tf.GradientTape() as tape:
  x.assign(tf.random.normal((20,3)))
  y = tf.reduce_sum(x)

grads = tape.gradient(y, x)
opt = tf.keras.optimizers.SGD(0.01, momentum = 0.9)
opt.apply_gradients([[grads, x]])
```


### Relevant log output

```shell
<ipython-input-8-dddd56b45b9e> in <module>()
     10 opt = tf.keras.optimizers.SGD(0.01, momentum = 0.9)
     11 print(x)
---> 12 opt.apply_gradients([[grads, x]])

5 frames
/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py in apply_gradients(self, grads_and_vars, name, experimental_aggregate_gradients)
    637       # Create iteration if necessary.
    638       with tf.init_scope():
--> 639         self._create_all_weights(var_list)
    640 
    641       if not grads_and_vars:

/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py in _create_all_weights(self, var_list)
    823     _ = self.iterations
    824     self._create_hypers()
--> 825     self._create_slots(var_list)
    826 
    827   def __getattribute__(self, name):

/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py in _create_slots(self, var_list)
    117     if self._momentum:
    118       for var in var_list:
--> 119         self.add_slot(var, ""momentum"")
    120 
    121   def _prepare_local(self, var_device, var_dtype, apply_state):

/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py in add_slot(self, var, slot_name, initializer, shape)
    913               dtype=var.dtype,
    914               trainable=False,
--> 915               initial_value=initial_value)
    916       backend.track_variable(weight)
    917       slot_dict[slot_name] = weight

/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs)
    151     except Exception as e:
    152       filtered_tb = _process_traceback_frames(e.__traceback__)
--> 153       raise e.with_traceback(filtered_tb) from None
    154     finally:
    155       del filtered_tb

/usr/local/lib/python3.7/dist-packages/keras/initializers/initializers_v2.py in __call__(self, shape, dtype, **kwargs)
    143     if _PARTITION_SHAPE in kwargs:
    144       shape = kwargs[_PARTITION_SHAPE]
--> 145     return tf.zeros(shape, dtype)
    146 
    147 

ValueError: Cannot convert a partially known TensorShape (None, 3) to a Tensor.
```
</details>"
56131,TensorFlow 2.9 ImportError: cannot import name 'keras' from 'tensorflow',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

No

### OS Platform and Distribution

MacOS 12.3.1M1 arm64CPU)

### Mobile device

_No response_

### Python version

3.9.12

### Bazel version

5.0.0

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
bazel build -c opt //tensorflow/tools/pip_package:build_pip_package
bazel-bin/tensorflow/tools/pip_package/build_pip_package .
pip3 install tensorflow-2.9.0-cp39-cp39-macosx_12_0_arm64.whl 
python3
from tensorflow import keras
```


### Relevant log output

```shell
python3
Python 3.9.12 (main, May  8 2022, 17:57:49) 
[Clang 13.1.6 (clang-1316.0.21.2)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from tensorflow import keras
Traceback (most recent call last):
  File ""/Users/andyueng/samba/workspace_TensorFlow2/tensorflow-2.9.0_pip_package/tensorflow/python/platform/self_check.py"", line 23, in <module>
    from tensorflow.python.platform import build_info
ImportError: cannot import name 'build_info' from 'tensorflow.python.platform' (/Users/andyueng/samba/workspace_TensorFlow2/tensorflow-2.9.0_pip_package/tensorflow/python/platform/__init__.py)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/andyueng/samba/workspace_TensorFlow2/tensorflow-2.9.0_pip_package/tensorflow/__init__.py"", line 20, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/Users/andyueng/samba/workspace_TensorFlow2/tensorflow-2.9.0_pip_package/tensorflow/python/__init__.py"", line 36, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""/Users/andyueng/samba/workspace_TensorFlow2/tensorflow-2.9.0_pip_package/tensorflow/python/pywrap_tensorflow.py"", line 21, in <module>
    from tensorflow.python.platform import self_check
  File ""/Users/andyueng/samba/workspace_TensorFlow2/tensorflow-2.9.0_pip_package/tensorflow/python/platform/self_check.py"", line 25, in <module>
    raise ImportError(""Could not import tensorflow. Do not import tensorflow ""
ImportError: Could not import tensorflow. Do not import tensorflow from its source directory; change directory to outside the TensorFlow source tree, and relaunch your Python interpreter from there.
```
</details>"
56130,math floordiv function gives incorrect result,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.8

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The code 

import tensorflow as tf
x = tf.constant([1e-37], dtype=tf.float32)
y = tf.constant([-2.0], dtype=tf.float32)
tf.math.floordiv(x, y)
```
works as expected, giving 
```<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>```.

However
```
import tensorflow as tf
x = tf.constant([1e-38], dtype=tf.float32)
y = tf.constant([-2.0], dtype=tf.float32)
tf.math.floordiv(x, y)
```
gives 
```<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-0.], dtype=float32)>```.

Any idea why when x gets small enough it starts to output `-0.` rather than `-1.`. `1e-38` is still in range for float32 I believe so I think it should still work?
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1J583b0h11hee3UItK6l4VyaJ_hG7J4AT#scrollTo=c4-4DQ72QPZy
```


### Relevant log output

_No response_</details>"
56129,Bazel build TensorFlow Lite 2.8.0 for iOS failed,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.8

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

macOS 12.3.1 M1 (ARM64)

### Python version

3.9.12

### Bazel version

4.2.1

### GCC/Compiler version

Apple clang version 13.1.6

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Can't build TensorFlow Lite 2.8 for iOS on MacBook Air M1(ARM64 CPU).
```


### Standalone code to reproduce the issue

```shell
bazel build --verbose_failures --config=ios --ios_multi_cpus=armv7,arm64 -c opt //tensorflow/lite/ios:TensorFlowLiteC_framework
```


### Relevant log output

```shell
INFO: Found 1 target...
ERROR: /Users/andyueng/samba/workspace_TensorFlow2/tensorflow-2.8.0_ios/tensorflow/lite/ios/BUILD:104:21: Bundling Preprocessed_TensorFlowLiteC_framework failed: (Exit 127): bundletool failed: error executing command 
  (cd /private/var/tmp/_bazel_andyueng/b359781934b2fbee0aeeaf947e072230/execroot/org_tensorflow && \
  exec env - \
  bazel-out/host/bin/external/build_bazel_rules_apple/tools/bundletool/bundletool bazel-out/applebin_ios-ios_armv7-opt-ST-36ad1e8d6baf/bin/tensorflow/lite/ios/Preprocessed_TensorFlowLiteC_framework-intermediates/bundletool_control.json)
Execution platform: @local_execution_config_platform//:platform
env: python: No such file or directory
Target //tensorflow/lite/ios:TensorFlowLiteC_framework failed to build
INFO: Elapsed time: 111.590s, Critical Path: 17.23s
INFO: 2219 processes: 359 internal, 1860 local.
FAILED: Build did NOT complete successfully
```
</details>"
56128,Can't install TensorFlow 2.9 python package,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

No

### OS Platform and Distribution

macOS 12.3.1M1 arm64CPU)

### Mobile device

_No response_

### Python version

 3.9.12

### Bazel version

5.0.0

### GCC/Compiler version

Apple clang version 13.1.6 

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Can't install TensorFlow 2.9 python package on MacBook Air M1 platform.
```


### Standalone code to reproduce the issue

```shell
bazel build -c opt //tensorflow/tools/pip_package:build_pip_package
bazel-bin/tensorflow/tools/pip_package/build_pip_package .
pip3 install tensorflow-2.9.0-cp39-cp39-macosx_12_0_arm64.whl
```


### Relevant log output

```shell
pip3 install tensorflow-2.9.0-cp39-cp39-macosx_12_0_arm64.whl
DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621
Processing ./tensorflow-2.9.0-cp39-cp39-macosx_12_0_arm64.whl
Collecting gast<=0.4.0,>=0.2.1
  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)
Collecting typing-extensions>=3.6.6
  Using cached typing_extensions-4.2.0-py3-none-any.whl (24 kB)
Collecting tensorboard<2.10,>=2.9
  Using cached tensorboard-2.9.0-py3-none-any.whl (5.8 MB)
Collecting grpcio<2.0,>=1.24.3
  Using cached grpcio-1.46.1-cp39-cp39-macosx_11_0_arm64.whl
Collecting astunparse>=1.6.0
  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)
Requirement already satisfied: six>=1.12.0 in /opt/homebrew/lib/python3.9/site-packages/six-1.16.0-py3.9.egg (from tensorflow==2.9.0) (1.16.0)
Collecting libclang>=13.0.0
  Using cached libclang-14.0.1-py2.py3-none-macosx_11_0_arm64.whl (11.8 MB)
Collecting opt-einsum>=2.3.2
  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)
ERROR: Could not find a version that satisfies the requirement tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow) (from versions: none)
ERROR: No matching distribution found for tensorflow-io-gcs-filesystem>=0.23.1
```
</details>"
56125,TensorFlow Lite 2.9 ARM cross-compilation failed: Cannot find source file: common.c,"**System information**

* Linux Ubuntu 20.04
* TensorFlow 2.9
* CMake 3.16.3
* gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf ([here](https://www.tensorflow.org/lite/guide/build_cmake_arm#download_toolchain_2))

**Describe the problem**

I trying to cross-compile TensorFlow Lite 2.9 for ARM using CMake. I got error ""Cannot find source file: common.c"":

```txt
CMake Error at CMakeLists.txt:63 (add_library):
  Cannot find source file:

    common.c

  Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm
  .hpp .hxx .in .txx

CMake Error at CMakeLists.txt:63 (add_library):
  No SOURCES given to target: tensorflowlite_c

CMake Generate step failed.  Build files cannot be regenerated correctly.
make: *** No targets specified and no makefile found.  Stop.
```

I seet that `.c` files has been renamed to `.cc` but CMakeLists.txt file left unchanged: [CMakeLists.txt](https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/lite/c/CMakeLists.txt#L66)

I using build instructions provided [here](https://www.tensorflow.org/lite/guide/build_cmake_arm#build_for_armv7_neon_enabled)
"
56124,Is there a way to get the current learning rate from an Estimator,"TF version: tf 1.15

I have spent so much time to find the solution .... BUT FIND NOTHING...

A similar problem was raised from StackOverflow, but no one can answer it.... 
https://stackoverflow.com/questions/60244149/is-there-a-way-to-get-the-current-learning-rate-from-an-estimator

PLEASE HELP ME....

IT IS  REALLY UPSET FOR ME to use TensorFlow
"
56122,`tf.random.uniform()` always picking the same value when metal plugin is installed,"### System information

* OS: macOS Montererey 12.4
* Tensorflow installed from: `pip` commands inside a conda environment
* Tensorflow version: `tensorflow-macos` 2.8.0
* Metal plugin version: `tensorflow-metal` 0.4.0
* Python version: 3.9.12
* GPU model and memory: Apple M1 Pro with 16 GB memory
* Exact command to reproduce: see below

### Describe the problem

When the `tensorflow-metal` plugin is installed, the `tf.random.uniform()` function always picks the same value on every call. Other random functions like `tf.random.normal()` do not have this problem. Neither does this problem occur when the Metal plugin is not installed.

### Source code / logs

```text
yunhao@Yunhaos-MBP ~ % conda create --name tf-rand-test python=3.9 six=1.15                      
Collecting package metadata (current_repodata.json): done
Solving environment: done

## Package Plan ##

  environment location: /Users/yunhao/miniforge3/envs/tf-rand-test

  added / updated specs:
    - python=3.9
    - six=1.15


The following NEW packages will be INSTALLED:

  bzip2              conda-forge/osx-arm64::bzip2-1.0.8-h3422bc3_4
  ca-certificates    conda-forge/osx-arm64::ca-certificates-2021.10.8-h4653dfc_0
  libffi             conda-forge/osx-arm64::libffi-3.4.2-h3422bc3_5
  libzlib            conda-forge/osx-arm64::libzlib-1.2.11-h90dfc92_1014
  ncurses            conda-forge/osx-arm64::ncurses-6.3-h07bb92c_1
  openssl            conda-forge/osx-arm64::openssl-3.0.3-ha287fd2_0
  pip                conda-forge/noarch::pip-22.1-pyhd8ed1ab_0
  python             conda-forge/osx-arm64::python-3.9.12-h14b404e_1_cpython
  python_abi         conda-forge/osx-arm64::python_abi-3.9-2_cp39
  readline           conda-forge/osx-arm64::readline-8.1-hedafd6a_0
  setuptools         conda-forge/osx-arm64::setuptools-62.2.0-py39h2804cbe_0
  six                conda-forge/noarch::six-1.15.0-pyh9f0ad1d_0
  sqlite             conda-forge/osx-arm64::sqlite-3.38.5-h40dfcc0_0
  tk                 conda-forge/osx-arm64::tk-8.6.12-he1e0b03_0
  tzdata             conda-forge/noarch::tzdata-2022a-h191b570_0
  wheel              conda-forge/noarch::wheel-0.37.1-pyhd8ed1ab_0
  xz                 conda-forge/osx-arm64::xz-5.2.5-h642e427_1
  zlib               conda-forge/osx-arm64::zlib-1.2.11-h90dfc92_1014


Proceed ([y]/n)? 

Preparing transaction: done
Verifying transaction: done
Executing transaction: done
#
# To activate this environment, use
#
#     $ conda activate tf-rand-test
#
# To deactivate an active environment, use
#
#     $ conda deactivate

yunhao@Yunhaos-MBP ~ % conda activate tf-rand-test                         
(tf-rand-test) yunhao@Yunhaos-MBP ~ % conda install -c apple tensorflow-deps
Collecting package metadata (current_repodata.json): done
Solving environment: done

## Package Plan ##

  environment location: /Users/yunhao/miniforge3/envs/tf-rand-test

  added / updated specs:
    - tensorflow-deps


The following NEW packages will be INSTALLED:

  c-ares             conda-forge/osx-arm64::c-ares-1.18.1-h3422bc3_0
  cached-property    conda-forge/noarch::cached-property-1.5.2-hd8ed1ab_1
  cached_property    conda-forge/noarch::cached_property-1.5.2-pyha770c72_1
  grpcio             conda-forge/osx-arm64::grpcio-1.46.1-py39h365d37b_0
  h5py               conda-forge/osx-arm64::h5py-3.6.0-nompi_py39hd982b79_100
  hdf5               conda-forge/osx-arm64::hdf5-1.12.1-nompi_hd9dbc9e_104
  krb5               conda-forge/osx-arm64::krb5-1.19.3-he492e65_0
  libblas            conda-forge/osx-arm64::libblas-3.9.0-14_osxarm64_openblas
  libcblas           conda-forge/osx-arm64::libcblas-3.9.0-14_osxarm64_openblas
  libcurl            conda-forge/osx-arm64::libcurl-7.83.1-h7965298_0
  libcxx             conda-forge/osx-arm64::libcxx-14.0.3-h6a5c8ee_0
  libedit            conda-forge/osx-arm64::libedit-3.1.20191231-hc8eb9b7_2
  libev              conda-forge/osx-arm64::libev-4.33-h642e427_1
  libgfortran        conda-forge/osx-arm64::libgfortran-5.0.0.dev0-11_0_1_hf114ba7_23
  libgfortran5       conda-forge/osx-arm64::libgfortran5-11.0.1.dev0-hf114ba7_23
  liblapack          conda-forge/osx-arm64::liblapack-3.9.0-14_osxarm64_openblas
  libnghttp2         conda-forge/osx-arm64::libnghttp2-1.47.0-hf30690b_0
  libopenblas        conda-forge/osx-arm64::libopenblas-0.3.20-openmp_h2209c59_0
  libssh2            conda-forge/osx-arm64::libssh2-1.10.0-h7a5bd25_2
  llvm-openmp        conda-forge/osx-arm64::llvm-openmp-14.0.3-hd125106_0
  numpy              conda-forge/osx-arm64::numpy-1.21.6-py39h690d673_0
  tensorflow-deps    apple/osx-arm64::tensorflow-deps-2.8.0-0


Proceed ([y]/n)? 

Preparing transaction: done
Verifying transaction: done
Executing transaction: done
(tf-rand-test) yunhao@Yunhaos-MBP ~ % python -m pip install tensorflow-macos                      
Collecting tensorflow-macos
  Using cached tensorflow_macos-2.8.0-cp39-cp39-macosx_11_0_arm64.whl (190.1 MB)
Collecting tensorboard<2.9,>=2.8
  Using cached tensorboard-2.8.0-py3-none-any.whl (5.8 MB)
Collecting absl-py>=0.4.0
  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)
Collecting libclang>=9.0.1
  Using cached libclang-14.0.1-py2.py3-none-macosx_11_0_arm64.whl (11.8 MB)
Collecting termcolor>=1.1.0
  Using cached termcolor-1.1.0-py3-none-any.whl
Collecting protobuf>=3.9.2
  Using cached protobuf-3.20.1-py2.py3-none-any.whl (162 kB)
Collecting opt-einsum>=2.3.2
  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)
Collecting keras-preprocessing>=1.1.1
  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)
Collecting flatbuffers>=1.12
  Using cached flatbuffers-2.0-py2.py3-none-any.whl (26 kB)
Requirement already satisfied: six>=1.12.0 in ./miniforge3/envs/tf-rand-test/lib/python3.9/site-packages (from tensorflow-macos) (1.15.0)
Collecting keras<2.9,>=2.8.0rc0
  Using cached keras-2.8.0-py2.py3-none-any.whl (1.4 MB)
Requirement already satisfied: setuptools in ./miniforge3/envs/tf-rand-test/lib/python3.9/site-packages (from tensorflow-macos) (62.2.0)
Collecting astunparse>=1.6.0
  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)
Collecting wrapt>=1.11.0
  Using cached wrapt-1.14.1-cp39-cp39-macosx_11_0_arm64.whl (35 kB)
Collecting gast>=0.2.1
  Using cached gast-0.5.3-py3-none-any.whl (19 kB)
Collecting google-pasta>=0.1.1
  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)
Collecting typing-extensions>=3.6.6
  Using cached typing_extensions-4.2.0-py3-none-any.whl (24 kB)
Requirement already satisfied: numpy>=1.20 in ./miniforge3/envs/tf-rand-test/lib/python3.9/site-packages (from tensorflow-macos) (1.21.6)
Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./miniforge3/envs/tf-rand-test/lib/python3.9/site-packages (from tensorflow-macos) (1.46.1)
Collecting tf-estimator-nightly==2.8.0.dev2021122109
  Using cached tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)
Requirement already satisfied: h5py>=2.9.0 in ./miniforge3/envs/tf-rand-test/lib/python3.9/site-packages (from tensorflow-macos) (3.6.0)
Requirement already satisfied: wheel<1.0,>=0.23.0 in ./miniforge3/envs/tf-rand-test/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow-macos) (0.37.1)
Collecting google-auth<3,>=1.6.3
  Using cached google_auth-2.6.6-py2.py3-none-any.whl (156 kB)
Collecting requests<3,>=2.21.0
  Using cached requests-2.27.1-py2.py3-none-any.whl (63 kB)
Collecting tensorboard-plugin-wit>=1.6.0
  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)
Collecting werkzeug>=0.11.15
  Using cached Werkzeug-2.1.2-py3-none-any.whl (224 kB)
Collecting markdown>=2.6.8
  Using cached Markdown-3.3.7-py3-none-any.whl (97 kB)
Collecting tensorboard-data-server<0.7.0,>=0.6.0
  Using cached tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)
Collecting google-auth-oauthlib<0.5,>=0.4.1
  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)
Collecting pyasn1-modules>=0.2.1
  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)
Collecting cachetools<6.0,>=2.0.0
  Using cached cachetools-5.1.0-py3-none-any.whl (9.2 kB)
Collecting rsa<5,>=3.1.4
  Using cached rsa-4.8-py3-none-any.whl (39 kB)
Collecting requests-oauthlib>=0.7.0
  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)
Collecting importlib-metadata>=4.4
  Using cached importlib_metadata-4.11.3-py3-none-any.whl (18 kB)
Collecting idna<4,>=2.5
  Using cached idna-3.3-py3-none-any.whl (61 kB)
Collecting certifi>=2017.4.17
  Using cached certifi-2021.10.8-py2.py3-none-any.whl (149 kB)
Collecting urllib3<1.27,>=1.21.1
  Using cached urllib3-1.26.9-py2.py3-none-any.whl (138 kB)
Collecting charset-normalizer~=2.0.0
  Using cached charset_normalizer-2.0.12-py3-none-any.whl (39 kB)
Collecting zipp>=0.5
  Using cached zipp-3.8.0-py3-none-any.whl (5.4 kB)
Collecting pyasn1<0.5.0,>=0.4.6
  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)
Collecting oauthlib>=3.0.0
  Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB)
Installing collected packages: tf-estimator-nightly, termcolor, tensorboard-plugin-wit, pyasn1, libclang, keras, flatbuffers, certifi, zipp, wrapt, werkzeug, urllib3, typing-extensions, tensorboard-data-server, rsa, pyasn1-modules, protobuf, opt-einsum, oauthlib, keras-preprocessing, idna, google-pasta, gast, charset-normalizer, cachetools, astunparse, absl-py, requests, importlib-metadata, google-auth, requests-oauthlib, markdown, google-auth-oauthlib, tensorboard, tensorflow-macos
Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-5.1.0 certifi-2021.10.8 charset-normalizer-2.0.12 flatbuffers-2.0 gast-0.5.3 google-auth-2.6.6 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 idna-3.3 importlib-metadata-4.11.3 keras-2.8.0 keras-preprocessing-1.1.2 libclang-14.0.1 markdown-3.3.7 oauthlib-3.2.0 opt-einsum-3.3.0 protobuf-3.20.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.27.1 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-macos-2.8.0 termcolor-1.1.0 tf-estimator-nightly-2.8.0.dev2021122109 typing-extensions-4.2.0 urllib3-1.26.9 werkzeug-2.1.2 wrapt-1.14.1 zipp-3.8.0
(tf-rand-test) yunhao@Yunhaos-MBP ~ % python                                
Python 3.9.12 | packaged by conda-forge | (main, Mar 24 2022, 23:25:14) 
[Clang 12.0.1 ] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> for _ in range(10):
...     print(tf.random.uniform([]))
... 
tf.Tensor(0.43182516, shape=(), dtype=float32)
tf.Tensor(0.73127687, shape=(), dtype=float32)
tf.Tensor(0.8234819, shape=(), dtype=float32)
tf.Tensor(0.7472857, shape=(), dtype=float32)
tf.Tensor(0.06519365, shape=(), dtype=float32)
tf.Tensor(0.46567404, shape=(), dtype=float32)
tf.Tensor(0.081846, shape=(), dtype=float32)
tf.Tensor(0.26130438, shape=(), dtype=float32)
tf.Tensor(0.53803635, shape=(), dtype=float32)
tf.Tensor(0.98602235, shape=(), dtype=float32)
>>> exit()
(tf-rand-test) yunhao@Yunhaos-MBP ~ % python -m pip install tensorflow-metal                      
Collecting tensorflow-metal
  Using cached tensorflow_metal-0.4.0-cp39-cp39-macosx_11_0_arm64.whl (1.2 MB)
Requirement already satisfied: wheel~=0.35 in ./miniforge3/envs/tf-rand-test/lib/python3.9/site-packages (from tensorflow-metal) (0.37.1)
Requirement already satisfied: six~=1.15.0 in ./miniforge3/envs/tf-rand-test/lib/python3.9/site-packages (from tensorflow-metal) (1.15.0)
Installing collected packages: tensorflow-metal
Successfully installed tensorflow-metal-0.4.0
(tf-rand-test) yunhao@Yunhaos-MBP ~ % python                                
Python 3.9.12 | packaged by conda-forge | (main, Mar 24 2022, 23:25:14) 
[Clang 12.0.1 ] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> for _ in range(10):
...     print(tf.random.uniform([]))
... 
Metal device set to: Apple M1 Pro

systemMemory: 16.00 GB
maxCacheSize: 5.33 GB

2022-05-16 15:21:55.560387: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2022-05-16 15:21:55.560711: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)
tf.Tensor(0.9149647, shape=(), dtype=float32)
tf.Tensor(0.9149647, shape=(), dtype=float32)
tf.Tensor(0.9149647, shape=(), dtype=float32)
tf.Tensor(0.9149647, shape=(), dtype=float32)
tf.Tensor(0.9149647, shape=(), dtype=float32)
tf.Tensor(0.9149647, shape=(), dtype=float32)
tf.Tensor(0.9149647, shape=(), dtype=float32)
tf.Tensor(0.9149647, shape=(), dtype=float32)
tf.Tensor(0.9149647, shape=(), dtype=float32)
tf.Tensor(0.9149647, shape=(), dtype=float32)
>>> for _ in range(10):
...     print(tf.random.uniform([]))
... 
tf.Tensor(0.9149647, shape=(), dtype=float32)
tf.Tensor(0.9149647, shape=(), dtype=float32)
tf.Tensor(0.9149647, shape=(), dtype=float32)
tf.Tensor(0.9149647, shape=(), dtype=float32)
tf.Tensor(0.9149647, shape=(), dtype=float32)
tf.Tensor(0.9149647, shape=(), dtype=float32)
tf.Tensor(0.9149647, shape=(), dtype=float32)
tf.Tensor(0.9149647, shape=(), dtype=float32)
tf.Tensor(0.9149647, shape=(), dtype=float32)
tf.Tensor(0.9149647, shape=(), dtype=float32)
>>> exit()
(tf-rand-test) yunhao@Yunhaos-MBP ~ % 
```"
56121,problem with batch production,"Hi All,
Inspiring by the tutorial called Text generation with an RNN (https://www.tensorflow.org/text/tutorials/text_generation), I managed to train a model to generate new molecules in SMILES string format. It has been trained very well and I could generate new molecules either consecutively (one after another) and batch production. However, when I saved the trained model and loaded it again, the batch production fails and ends up with an error. could someone help please?"
56118,Tensorflow is failing on Bazel CI (release Bazel + HEAD TF),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

HEAD

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

5.1.1

### GCC/Compiler version

7.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
https://buildkite.com/bazel/tensorflow/builds/7777#1132f1e2-efac-44fc-b57b-b1068aa08662

AttributeError: module 'typing' has no attribute '_SpecialForm'
```


### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

_No response_</details>"
56117,Tflite checkpoint restore ,"Checkpoints saved sucessfully, but can't load checkpoint after interruption 
![Screenshot_20220516-112630](https://user-images.githubusercontent.com/86015315/168553539-cd972228-29a9-471c-bf9a-941c1248dc4f.jpg)
 
Why detection variable is not defined , how to restore saved checkpoint "
56116,Import tensorflow module error and abort to execute on Ubuntu 1804,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf-nightly==2.10.0.dev20220515

### Custom Code

No

### OS Platform and Distribution

Ubuntu 1804 lts

### Mobile device

_No response_

### Python version

3.7.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
After run pip install tf-nightly==2.10.0.dev20220515. 
Directly import tensorflow as tf, will get error message ""getInferLibVersion symbol not found."" and abort.
```


### Standalone code to reproduce the issue

```shell
1. Run pip install tf-nightly==2.10.0.dev20220515
2. Run python -c 'import tensorflow as tf'
```


### Relevant log output

```shell
(py376_test) azureuser@anubis-StandardD32sv4-linux-9:~/tmp/mlperf_python$ python
Python 3.7.7 (default, May  7 2020, 21:25:33)
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
2022-05-16 04:06:41.555118: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-16 04:06:41.665272: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-05-16 04:06:41.669465: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2022-05-16 04:06:41.669490: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base has been moved to tensorflow.python.trackable.base. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.checkpoint_management has been moved to tensorflow.python.checkpoint.checkpoint_management. The old module will be deleted in version 2.9.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.resource has been moved to tensorflow.python.trackable.resource. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.util has been moved to tensorflow.python.checkpoint.checkpoint. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base_delegate has been moved to tensorflow.python.trackable.base_delegate. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.graph_view has been moved to tensorflow.python.checkpoint.graph_view. The old module will be deleted in version 2.11.
2022-05-16 04:06:42.334465: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2022-05-16 04:06:42.334493: F tensorflow/compiler/tf2tensorrt/stub/nvinfer_stub.cc:49] getInferLibVersion symbol not found.
Aborted (core dumped)
```
</details>"
56115,[Feature Request] Default Keras callback for timing the training loop,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

2.8

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The current Keras progress bar callback displays training latency in time/step. The current recommendation is to disable the progress bar in production workflows.

This poses 2 problems:

1. The metric time/step is not batch size agnostic. While tuning batch size to fit the H/W, metrics represented in terms of samples allow for better head to head comparison. Currently, an additional step of converting step to samples is required.

2. Production workflows that require continuous training will benefit greatly from being able to monitor training latency. A deterioration in training speed could lead to spike in training costs.

I propose:

1. Reporting an additional metric for training latency based on samples.

2. A new verbosity setting that allows for reporting the training latency metrics without the progress bar. This could even be part of an existing verbosity.

I would be happy to create a PR for these changes.
```


### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

_No response_</details>"
56114,bazel build TensorFlow Lite 2.7.0 for iOS failed,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.7

### Custom Code

No

### OS Platform and Distribution

macOS 12.3.1

### Mobile device

iPhone

### Python version

3.9.12

### Bazel version

3.7.2

### GCC/Compiler version

Apple clang version 13.1.6

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
bazel build TensorFlow Lite 2.7.0 for iOS successfully.
```


### Standalone code to reproduce the issue

```shell
bazel build --config=ios_fat -c opt //tensorflow/lite/ios:TensorFlowLiteC_framework
```


### Relevant log output

```shell
ERROR: Skipping '//tensorflow/lite/ios:TensorFlowLiteC_framework': no such package 'tensorflow/lite/ios': BUILD file not found in any of the following directories. Add a BUILD file to a directory to mark it as a package.
 - /Users/andyueng/samba/workspace_TensorFlow2/tensorflow-2.7.0_ios/tensorflow/lite/ios
WARNING: Target pattern parsing failed.
ERROR: no such package 'tensorflow/lite/ios': BUILD file not found in any of the following directories. Add a BUILD file to a directory to mark it as a package.
 - /Users/andyueng/samba/workspace_TensorFlow2/tensorflow-2.7.0_ios/tensorflow/lite/ios
INFO: Elapsed time: 5.298s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
```
</details>"
56113,bazel build TensorFlow Lite 2.7.0 with Select TensorFlow operators for iOS failed,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.7

### Custom Code

No

### OS Platform and Distribution

macOS 12.3.1

### Mobile device

iPhone

### Python version

3.9.10

### Bazel version

3.7.2

### GCC/Compiler version

Apple clang version 13.1.6

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
bazel build TensorFlow Lite 2.7.0 with Select TensorFlow operators for iOS successfully.
```


### Standalone code to reproduce the issue

```shell
bazel build -c opt --config=ios --ios_multi_cpus=armv7,arm64 //tensorflow/lite/ios:TensorFlowLiteSelectTfOps_framework
```


### Relevant log output

```shell
ERROR: Skipping '//tensorflow/lite/ios:TensorFlowLiteSelectTfOps_framework': no such package 'tensorflow/lite/ios': BUILD file not found in any of the following directories. Add a BUILD file to a directory to mark it as a package.
 - /Users/andyueng/samba/workspace_TensorFlow2/tensorflow-2.7.0_selops_ios/tensorflow/lite/ios
WARNING: Target pattern parsing failed.
ERROR: no such package 'tensorflow/lite/ios': BUILD file not found in any of the following directories. Add a BUILD file to a directory to mark it as a package.
 - /Users/andyueng/samba/workspace_TensorFlow2/tensorflow-2.7.0_selops_ios/tensorflow/lite/ios
INFO: Elapsed time: 0.284s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
```
</details>"
56112,No module named model.recommendation_model_launcher_keras,"I followed all previous steps in [ondevice_recommendation.ipynb](https://github.com/tensorflow/examples/blob/master/lite/examples/recommendation/ml/ondevice_recommendation.ipynb)

I can't find either `model.recommendation_model_launcher_keras` in examples repo
but error still exists when I execute this block
```
!python -m model.recommendation_model_launcher_keras \
  --run_mode ""train_and_eval"" \
  --encoder_type ""cnn"" \
  --training_data_filepattern ""data/examples/train_movielens_1m.tfrecord"" \
  --testing_data_filepattern ""data/examples/test_movielens_1m.tfrecord"" \
  --model_dir ""model/model_dir"" \
  --params_path ""model/sample_config.json""\
  --batch_size 64 \
  --learning_rate 0.1 \
  --steps_per_epoch 1000 \
  --num_epochs 10 \
  --num_eval_steps 1000 \
  --gradient_clip_norm 1.0 \
  --max_history_length 10
  ```"
56111,"TFLite Example Build Fails with error ""undefined reference to `max'""","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.8

### Custom Code

No

### OS Platform and Distribution

Windows 10 MSYS2

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

10.3.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Followed these docs to build a TFLite project with CMake: https://www.tensorflow.org/lite/guide/build_cmake#step_1_install_cmake_tool
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/README.md

Unfortunately, building a project with CMake fails during linking with the error ""undefined reference to max"" in cpuinfo/src/x86/windows/init.c. The expected behavior is that the CMake build show finish with no errors.
```


### Standalone code to reproduce the issue

```shell
$ git clone https://github.com/tensorflow/tensorflow.git tensorflow_src
$ mkdir tf_example
$ cd tf_example
$ cmake ../tensorflow_src/tensorflow/lite/example/minimal -G ""MinGW Makefiles""
$ cmake --build . -j 8
```


### Relevant log output

```shell
[100%] Linking CXX static library libtensorflow-lite.a
[100%] Built target tensorflow-lite
[100%] Building CXX object CMakeFiles/minimal.dir/minimal.cc.obj
[100%] Linking CXX executable minimal.exe
C:/msys64/mingw64/bin/../lib/gcc/x86_64-w64-mingw32/10.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: _deps/cpuinfo-build/libcpuinfo.a(init.c.obj):init.c:(.text+0x117): undefined reference to `max'
C:/msys64/mingw64/bin/../lib/gcc/x86_64-w64-mingw32/10.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: _deps/cpuinfo-build/libcpuinfo.a(init.c.obj):init.c:(.text+0x219): undefined reference to `max'
C:/msys64/mingw64/bin/../lib/gcc/x86_64-w64-mingw32/10.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: _deps/cpuinfo-build/libcpuinfo.a(init.c.obj):init.c:(.text+0x34d): undefined reference to `max'
C:/msys64/mingw64/bin/../lib/gcc/x86_64-w64-mingw32/10.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: _deps/cpuinfo-build/libcpuinfo.a(init.c.obj):init.c:(.text+0x549): undefined reference to `max'
collect2.exe: error: ld returned 1 exit status
mingw32-make[2]: *** [CMakeFiles\minimal.dir\build.make:179: minimal.exe] Error 1
mingw32-make[1]: *** [CMakeFiles\Makefile2:1195: CMakeFiles/minimal.dir/all] Error 2
mingw32-make: *** [Makefile:135: all] Error 2
```
</details>"
